<html><body><head><link rel="stylesheet" type="text/css" href="style.css" /><script src="map.js"></script><script src="jquery-1.7.1.min.js"></script></head>
<div class="dstPaperData">
W02-1011 <div class="dstPaperTitle">Thumbs Up? Sentiment Classification Using Machine Learning Techniques</div><div class="dstPaperAuthors">Pang, Bo;Lee, Lillian;Vaithyanathan, Shivakumar;</div>
</div>
<table cellspacing="0" cellpadding="0"><tr>
	<td class="srcData" >Source Paper</td>
	<td class="pp legend" ><input type="checkbox" id="cbIPositive" checked="true"/><label for="cbIPositive">Informal +<label></td>
	<td class="nn legend" ><input type="checkbox" id="cbINegative" checked="true"/><label for="cbINegative">Informal -<label></td>
	<td class="oo legend" ><input type="checkbox" id="cbIObjective" checked="true"/><label for="cbIObjective">Informal Neutral<label></td>
	<td class="ppc legend" ><input type="checkbox" id="cbEPositive" checked="true"/><label for="cbEPositive">Formal +</label></td>
	<td class="nnc legend" ><input type="checkbox" id="cbENegative" checked="true"/><label for="cbENegative">Formal -</label></td>
	<td class="ooc legend" ><input type="checkbox" id="cbEObjective" checked="true"/><label for="cbEObjective">Formal Neutral</label></td>
	<td class="lb"><input type="checkbox" id="cbSentenceBoundary"/><label for="cbSentenceBoundary">Sentence Boundary</label></td>
</tr></table>
<div class="dstPaper">
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W03-0404
Learning Subjective Nouns Using Extraction Pattern Bootstrapping
Riloff, Ellen;Wiebe, Janyce M.;Wilson, Theresa;"></td>
	<td class="line x" title="1:226	Learning Subjective Nouns using Extraction Pattern Bootstrapping Ellen Riloff School of Computing University of Utah Salt Lake City, UT 84112 riloff@cs.utah.edu Janyce Wiebe Department of Computer Science University of Pittsburgh Pittsburgh, PA 15260 wiebe@cs.pitt.edu Theresa Wilson Intelligent Systems Program University of Pittsburgh Pittsburgh, PA 15260 twilson@cs.pitt.edu Abstract We explore the idea of creating a subjectivity classifier that uses lists of subjective nouns learned by bootstrapping algorithms." ></td>
	<td class="line x" title="2:226	The goal of our research is to develop a system that can distinguish subjective sentences from objective sentences." ></td>
	<td class="line x" title="3:226	First, we use two bootstrapping algorithms that exploit extraction patterns to learn sets of subjective nouns." ></td>
	<td class="line x" title="4:226	Then we train a Naive Bayes classifier using the subjective nouns, discourse features, and subjectivity clues identified in prior research." ></td>
	<td class="line x" title="5:226	The bootstrapping algorithms learned over 1000 subjective nouns, and the subjectivity classifier performed well, achieving 77% recall with 81% precision." ></td>
	<td class="line x" title="6:226	1 Introduction Many natural language processing applications could benefit from being able to distinguish between factual and subjective information." ></td>
	<td class="line x" title="7:226	Subjective remarks come in a variety of forms, including opinions, rants, allegations, accusations, suspicions, and speculation." ></td>
	<td class="line x" title="8:226	Ideally, information extraction systems should be able to distinguish between factual information (which should be extracted) and non-factual information (which should be discarded or labeled as uncertain)." ></td>
	<td class="line x" title="9:226	Question answering systems should distinguish between factual and speculative answers." ></td>
	<td class="line x" title="10:226	Multi-perspective question answering aims to present multiple answers to the user based upon speculation or opinions derived from different sources." ></td>
	<td class="line x" title="11:226	MultiThis work was supported in part by the National Science Foundation under grants IIS-0208798 and IRI-9704240." ></td>
	<td class="line x" title="12:226	The data preparation was performed in support of the Northeast Regional Reseach Center (NRRC) which is sponsored by the Advanced Research and Development Activity (ARDA), a U.S. Government entity which sponsors and promotes research of import to the Intelligence Community which includes but is not limited to the CIA, DIA, NSA, NIMA, and NRO." ></td>
	<td class="line x" title="13:226	document summarization systems need to summarize different opinions and perspectives." ></td>
	<td class="line x" title="14:226	Spam filtering systems must recognize rants and emotional tirades, among other things." ></td>
	<td class="line x" title="15:226	In general, nearly any system that seeks to identify information could benefit from being able to separate factual and subjective information." ></td>
	<td class="line x" title="16:226	Subjective language has been previously studied in fields such as linguistics, literary theory, psychology, and content analysis." ></td>
	<td class="line x" title="17:226	Some manually-developed knowledge resources exist, but there is no comprehensive dictionary of subjective language." ></td>
	<td class="line x" title="18:226	Meta-Bootstrapping (Riloff and Jones, 1999) and Basilisk (Thelen and Riloff, 2002) are bootstrapping algorithms that use automatically generated extraction patterns to identify words belonging to a semantic category." ></td>
	<td class="line x" title="19:226	We hypothesized that extraction patterns could also identify subjective words." ></td>
	<td class="line x" title="20:226	For example, the pattern expressed <direct object> often extracts subjective nouns, such as concern, hope, and support." ></td>
	<td class="line x" title="21:226	Furthermore, these bootstrapping algorithms require only a handful of seed words and unannotated texts for training; no annotated data is needed at all." ></td>
	<td class="line x" title="22:226	In this paper, we use the Meta-Bootstrapping and Basilisk algorithms to learn lists of subjective nouns from a large collection of unannotated texts." ></td>
	<td class="line x" title="23:226	Then we train a subjectivity classifier on a small set of annotated data, using the subjective nouns as features along with some other previously identified subjectivity features." ></td>
	<td class="line x" title="24:226	Our experimental results show that the subjectivity classifier performs well (77% recall with 81% precision) and that the learned nouns improve upon previous state-of-the-art subjectivity results (Wiebe et al. , 1999)." ></td>
	<td class="line x" title="25:226	2 Subjectivity Data 2.1 The Annotation Scheme In 2002, an annotation scheme was developed for a U.S. government-sponsored project with a team of 10 researchers (the annotation instructions and project reports are available on the Web at http://www.cs.pitt.edu/wiebe/pubs/ardasummer02/)." ></td>
	<td class="line x" title="26:226	Edmonton, May-June 2003 held at HLT-NAACL 2003, pp." ></td>
	<td class="line x" title="27:226	25-32 Proceeings of the Seventh CoNLL conference The scheme was inspired by work in linguistics and literary theory on subjectivity, which focuses on how opinions, emotions, etc. are expressed linguistically in context (Banfield, 1982)." ></td>
	<td class="line x" title="28:226	The scheme is more detailed and comprehensive than previous ones." ></td>
	<td class="line x" title="29:226	We mention only those aspects of the annotation scheme relevant to this paper." ></td>
	<td class="line x" title="30:226	The goal of the annotation scheme is to identify and characterize expressions of private states in a sentence." ></td>
	<td class="line x" title="31:226	Private state is a general covering term for opinions, evaluations, emotions, and speculations (Quirk et al. , 1985)." ></td>
	<td class="line x" title="32:226	For example, in sentence (1) the writer is expressing a negative evaluation." ></td>
	<td class="line x" title="33:226	(1) The time has come, gentlemen, for Sharon, the assassin, to realize that injustice cannot last long. Sentence (2) reflects the private state of Western countries." ></td>
	<td class="line x" title="34:226	Mugabes use of overwhelmingly also reflects a private state, his positive reaction to and characterization of his victory." ></td>
	<td class="line x" title="35:226	(2) Western countries were left frustrated and impotent after Robert Mugabe formally declared that he had overwhelmingly won Zimbabwes presidential election. Annotators are also asked to judge the strength of each private state." ></td>
	<td class="line x" title="36:226	A private state can have low, medium, high or extreme strength." ></td>
	<td class="line x" title="37:226	2.2 Corpus and Agreement Results Our data consists of English-language versions of foreign news documents from FBIS, the U.S. Foreign Broadcast Information Service." ></td>
	<td class="line x" title="38:226	The data is from a variety of publications and countries." ></td>
	<td class="line x" title="39:226	The annotated corpus used to train and test our subjectivity classifiers (the experiment corpus) consists of 109 documents with a total of 2197 sentences." ></td>
	<td class="line x" title="40:226	We used a separate, annotated tuning corpus of 33 documents with a total of 698 sentences to establish some experimental parameters.1 Each document was annotated by one or both of two annotators, A and T. To allow us to measure interannotator agreement, the annotators independently annotated the same 12 documents with a total of 178 sentences." ></td>
	<td class="line x" title="41:226	We began with a strict measure of agreement at the sentence level by first considering whether the annotator marked any private-state expression, of any strength, anywhere in the sentence." ></td>
	<td class="line x" title="42:226	If so, the sentence should be subjective." ></td>
	<td class="line x" title="43:226	Otherwise, it is objective." ></td>
	<td class="line x" title="44:226	Table 1 shows the contingency table." ></td>
	<td class="line x" title="45:226	The percentage agreement is 88%, and the value is 0.71." ></td>
	<td class="line x" title="46:226	1The annotated data will be available to U.S. government contractors this summer." ></td>
	<td class="line x" title="47:226	We are working to resolve copyright issues to make it available to the wider research community." ></td>
	<td class="line x" title="48:226	Tagger T Subj Obj Tagger A Subj nyy = 112 nyn = 16 Obj nny = 6 nnn = 44 Table 1: Agreement for sentence-level annotations Tagger T Subj Obj Tagger A Subj nyy = 106 nyn = 9 Obj nny = 0 nnn = 44 Table 2: Agreement for sentence-level annotations, lowstrength cases removed One would expect that there are clear cases of objective sentences, clear cases of subjective sentences, and borderline sentences in between." ></td>
	<td class="line x" title="49:226	The agreement study supports this." ></td>
	<td class="line x" title="50:226	In terms of our annotations, we define a sentence as borderline if it has at least one privatestate expression identified by at least one annotator, and all strength ratings of private-state expressions are low." ></td>
	<td class="line x" title="51:226	Table 2 shows the agreement results when such borderline sentences are removed (19 sentences, or 11% of the agreement test corpus)." ></td>
	<td class="line x" title="52:226	The percentage agreement increases to 94% and the value increases to 0.87." ></td>
	<td class="line x" title="53:226	As expected, the majority of disagreement cases involve low-strength subjectivity." ></td>
	<td class="line x" title="54:226	The annotators consistently agree about which are the clear cases of subjective sentences." ></td>
	<td class="line x" title="55:226	This leads us to define the gold-standard that we use in our experiments." ></td>
	<td class="line x" title="56:226	A sentence is subjective if it contains at least one private-state expression of medium or higher strength." ></td>
	<td class="line x" title="57:226	The second class, which we call objective, consists of everything else." ></td>
	<td class="line x" title="58:226	Thus, sentences with only mild traces of subjectivity are tossed into the objective category, making the systems goal to find the clearly subjective sentences." ></td>
	<td class="line x" title="59:226	3 Using Extraction Patterns to Learn Subjective Nouns In the last few years, two bootstrapping algorithms have been developed to create semantic dictionaries by exploiting extraction patterns: Meta-Bootstrapping (Riloff and Jones, 1999) and Basilisk (Thelen and Riloff, 2002)." ></td>
	<td class="line x" title="60:226	Extraction patterns were originally developed for information extraction tasks (Cardie, 1997)." ></td>
	<td class="line x" title="61:226	They represent lexico-syntactic expressions that typically rely on shallow parsing and syntactic role assignment." ></td>
	<td class="line x" title="62:226	For example, the pattern <subject> was hired would apply to sentences that contain the verb hired in the passive voice." ></td>
	<td class="line x" title="63:226	The subject would be extracted as the hiree." ></td>
	<td class="line x" title="64:226	Meta-Bootstrapping and Basilisk were designed to learn words that belong to a semantic category (e.g. , truck is a VEHICLE and seashore is a LOCATION)." ></td>
	<td class="line x" title="65:226	Both algorithms begin with unannotated texts and seed words that represent a semantic category." ></td>
	<td class="line x" title="66:226	A bootstrapping process looks for words that appear in the same extraction patterns as the seeds and hypothesizes that those words belong to the same semantic class." ></td>
	<td class="line x" title="67:226	The principle behind this approach is that words of the same semantic class appear in similar pattern contexts." ></td>
	<td class="line x" title="68:226	For example, the phrases lived in and traveled to will co-occur with many noun phrases that represent LOCATIONS." ></td>
	<td class="line x" title="69:226	In our research, we want to automatically identify words that are subjective." ></td>
	<td class="line x" title="70:226	Subjective terms have many different semantic meanings, but we believe that the same contextual principle applies to subjectivity." ></td>
	<td class="line x" title="71:226	In this section, we briefly overview these bootstrapping algorithms and explain how we used them to generate lists of subjective nouns." ></td>
	<td class="line x" title="72:226	3.1 Meta-Bootstrapping The Meta-Bootstrapping (MetaBoot) process (Riloff and Jones, 1999) begins with a small set of seed words that represent a targeted semantic category (e.g. , 10 words that represent LOCATIONS) and an unannotated corpus." ></td>
	<td class="line x" title="73:226	First, MetaBoot automatically creates a set of extraction patterns for the corpus by applying and instantiating syntactic templates." ></td>
	<td class="line x" title="74:226	This process literally produces thousands of extraction patterns that, collectively, will extract every noun phrase in the corpus." ></td>
	<td class="line x" title="75:226	Next, MetaBoot computes a score for each pattern based upon the number of seed words among its extractions." ></td>
	<td class="line x" title="76:226	The best pattern is saved and all of its extracted noun phrases are automatically labeled as the targeted semantic category.2 MetaBoot then re-scores the extraction patterns, using the original seed words as well as the newly labeled words, and the process repeats." ></td>
	<td class="line x" title="77:226	This procedure is called mutual bootstrapping." ></td>
	<td class="line x" title="78:226	A second level of bootstrapping (the meta- bootstrapping part) makes the algorithm more robust." ></td>
	<td class="line x" title="79:226	When the mutual bootstrapping process is finished, all nouns that were put into the semantic dictionary are reevaluated." ></td>
	<td class="line x" title="80:226	Each noun is assigned a score based on how many different patterns extracted it." ></td>
	<td class="line x" title="81:226	Only the five best nouns are allowed to remain in the dictionary." ></td>
	<td class="line x" title="82:226	The other entries are discarded, and the mutual bootstrapping process starts over again using the revised semantic dictionary." ></td>
	<td class="line x" title="83:226	3.2 Basilisk Basilisk (Thelen and Riloff, 2002) is a more recent bootstrapping algorithm that also utilizes extraction patterns to create a semantic dictionary." ></td>
	<td class="line x" title="84:226	Similarly, Basilisk begins with an unannotated text corpus and a small set of 2Our implementation of Meta-Bootstrapping learns individual nouns (vs. noun phrases) and discards capitalized words." ></td>
	<td class="line x" title="85:226	seed words for a semantic category." ></td>
	<td class="line x" title="86:226	The bootstrapping process involves three steps." ></td>
	<td class="line x" title="87:226	(1) Basilisk automatically generates a set of extraction patterns for the corpus and scores each pattern based upon the number of seed words among its extractions." ></td>
	<td class="line x" title="88:226	This step is identical to the first step of Meta-Bootstrapping." ></td>
	<td class="line x" title="89:226	Basilisk then puts the best patterns into a Pattern Pool." ></td>
	<td class="line x" title="90:226	(2) All nouns3 extracted by a pattern in the Pattern Pool are put into a Candidate Word Pool." ></td>
	<td class="line x" title="91:226	Basilisk scores each noun based upon the set of patterns that extracted it and their collective association with the seed words." ></td>
	<td class="line x" title="92:226	(3) The top 10 nouns are labeled as the targeted semantic class and are added to the dictionary." ></td>
	<td class="line x" title="93:226	The bootstrapping process then repeats, using the original seeds and the newly labeled words." ></td>
	<td class="line x" title="94:226	The main difference between Basilisk and MetaBootstrapping is that Basilisk scores each noun based on collective information gathered from all patterns that extracted it." ></td>
	<td class="line x" title="95:226	In contrast, Meta-Bootstrapping identifies a single best pattern and assumes that everything it extracted belongs to the same semantic class." ></td>
	<td class="line x" title="96:226	The second level of bootstrapping smoothes over some of the problems caused by this assumption." ></td>
	<td class="line x" title="97:226	In comparative experiments (Thelen and Riloff, 2002), Basilisk outperformed Meta-Bootstrapping." ></td>
	<td class="line x" title="98:226	But since our goal of learning subjective nouns is different from the original intent of the algorithms, we tried them both." ></td>
	<td class="line x" title="99:226	We also suspected they might learn different words, in which case using both algorithms could be worthwhile." ></td>
	<td class="line x" title="100:226	3.3 Experimental Results The Meta-Bootstrapping and Basilisk algorithms need seed words and an unannotated text corpus as input." ></td>
	<td class="line x" title="101:226	Since we did not need annotated texts, we created a much larger training corpus, the bootstrapping corpus, by gathering 950 new texts from the FBIS source mentioned in Section 2.2." ></td>
	<td class="line x" title="102:226	To find candidate seed words, we automatically identified 850 nouns that were positively correlated with subjective sentences in another data set." ></td>
	<td class="line x" title="103:226	However, it is crucial that the seed words occur frequently in our FBIS texts or the bootstrapping process will not get off the ground." ></td>
	<td class="line x" title="104:226	So we searched for each of the 850 nouns in the bootstrapping corpus, sorted them by frequency, and manually selected 20 high-frequency words that we judged to be strongly subjective." ></td>
	<td class="line x" title="105:226	Table 3 shows the 20 seed words used for both Meta-Bootstrapping and Basilisk." ></td>
	<td class="line x" title="106:226	We ran each bootstrapping algorithm for 400 iterations, generating 5 words per iteration." ></td>
	<td class="line x" title="107:226	Basilisk generated 2000 nouns and Meta-Bootstrapping generated 1996 nouns.4 Table 4 shows some examples of extraction pat3Technically, each head noun of an extracted noun phrase." ></td>
	<td class="line x" title="108:226	4Meta-Bootstrapping will sometimes produce fewer than 5 words per iteration if it has low confidence in its judgements." ></td>
	<td class="line x" title="109:226	cowardice embarrassment hatred outrage crap fool hell slander delight gloom hypocrisy sigh disdain grievance love twit dismay happiness nonsense virtue Table 3: Subjective Seed Words Extraction Patterns Examples of Extracted Nouns expressed <dobj> condolences, hope, grief, views, worries, recognition indicative of <np> compromise, desire, thinking inject <dobj> vitality, hatred reaffirmed <dobj> resolve, position, commitment voiced <dobj> outrage, support, skepticism, disagreement, opposition, concerns, gratitude, indignation show of <np> support, strength, goodwill, solidarity, feeling <subject> was shared anxiety, view, niceties, feeling Table 4: Extraction Pattern Examples terns that were discovered to be associated with subjective nouns." ></td>
	<td class="line x" title="110:226	Meta-Bootstrapping and Basilisk are semi-automatic lexicon generation tools because, although the bootstrapping process is 100% automatic, the resulting lexicons need to be reviewed by a human.5 So we manually reviewed the 3996 words proposed by the algorithms." ></td>
	<td class="line x" title="111:226	This process is very fast; it takes only a few seconds to classify each word." ></td>
	<td class="line x" title="112:226	The entire review process took approximately 3-4 hours." ></td>
	<td class="line x" title="113:226	One author did this labeling; this person did not look at or run tests on the experiment corpus." ></td>
	<td class="line x" title="114:226	Strong Subjective Weak Subjective tyranny scum aberration plague smokescreen bully allusion risk apologist devil apprehensions drama barbarian liar beneficiary trick belligerence pariah resistant promise condemnation venom credence intrigue sanctimonious diatribe distortion unity exaggeration mockery eyebrows failures repudiation anguish inclination tolerance insinuation fallacies liability persistent antagonism evil assault trust atrocities genius benefit success denunciation goodwill blood spirit exploitation injustice controversy slump humiliation innuendo likelihood sincerity ill-treatment revenge peaceful eternity sympathy rogue pressure rejection Table 5: Examples of Learned Subjective Nouns 5This is because NLP systems expect dictionaries to have high integrity." ></td>
	<td class="line x" title="115:226	Even if the algorithms could achieve 90% accuracy, a dictionary in which 1 of every 10 words is defined incorrectly would probably not be desirable." ></td>
	<td class="line x" title="116:226	B M B \ M B [ M StrongSubj 372 192 110 454 WeakSubj 453 330 185 598 Total 825 522 295 1052 Table 6: Subjective Word Lexicons after Manual Review (B=Basilisk, M=MetaBootstrapping) 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 200 400 600 800 1000 1200 1400 1600 1800 2000 % of Words Subjective Number of Words Generated Basilisk MetaBoot Figure 1: Accuracy during Bootstrapping We classified the words as StrongSubjective, WeakSubjective, or Objective." ></td>
	<td class="line x" title="117:226	Objective terms are not subjective at all (e.g. , chair or city)." ></td>
	<td class="line x" title="118:226	StrongSubjective terms have strong, unambiguously subjective connotations, such as bully or barbarian." ></td>
	<td class="line x" title="119:226	WeakSubjective was used for three situations: (1) words that have weak subjective connotations, such as aberration which implies something out of the ordinary but does not evoke a strong sense of judgement, (2) words that have multiple senses or uses, where one is subjective but the other is not." ></td>
	<td class="line x" title="120:226	For example, the word plague can refer to a disease (objective) or an onslaught of something negative (subjective), (3) words that are objective by themselves but appear in idiomatic expressions that are subjective." ></td>
	<td class="line x" title="121:226	For example, the word eyebrows was labeled WeakSubjective because the expression raised eyebrows probably occurs more often in our corpus than literal references to eyebrows." ></td>
	<td class="line x" title="122:226	Table 5 shows examples of learned words that were classified as StrongSubjective or WeakSubjective." ></td>
	<td class="line x" title="123:226	Once the words had been manually classified, we could go back and measure the effectiveness of the algorithms." ></td>
	<td class="line x" title="124:226	The graph in Figure 1 tracks their accuracy as the bootstrapping progressed." ></td>
	<td class="line x" title="125:226	The X-axis shows the number of words generated so far." ></td>
	<td class="line x" title="126:226	The Y-axis shows the percentage of those words that were manually classified as subjective." ></td>
	<td class="line x" title="127:226	As is typical of bootstrapping algorithms, accuracy was high during the initial iterations but tapered off as the bootstrapping continued." ></td>
	<td class="line x" title="128:226	After 20 words, both algorithms were 95% accurate." ></td>
	<td class="line x" title="129:226	After 100 words Basilisk was 75% accurate and MetaBoot was 81% accurate." ></td>
	<td class="line x" title="130:226	After 1000 words, accuracy dropped to about 28% for MetaBoot, but Basilisk was still performing reasonably well at 53%." ></td>
	<td class="line x" title="131:226	Although 53% accuracy is not high for a fully automatic process, Basilisk depends on a human to review the words so 53% accuracy means that the human is accepting every other word, on average." ></td>
	<td class="line x" title="132:226	Thus, the reviewers time was still being spent productively even after 1000 words had been hypothesized." ></td>
	<td class="line x" title="133:226	Table 6 shows the size of the final lexicons created by the bootstrapping algorithms." ></td>
	<td class="line x" title="134:226	The first two columns show the number of subjective terms learned by Basilisk and Meta-Bootstrapping." ></td>
	<td class="line x" title="135:226	Basilisk was more prolific, generating 825 subjective terms compared to 522 for MetaBootstrapping." ></td>
	<td class="line x" title="136:226	The third column shows the intersection between their word lists." ></td>
	<td class="line x" title="137:226	There was substantial overlap, but both algorithms produced many words that the other did not." ></td>
	<td class="line x" title="138:226	The last column shows the results of merging their lists." ></td>
	<td class="line x" title="139:226	In total, the bootstrapping algorithms produced 1052 subjective nouns." ></td>
	<td class="line x" title="140:226	4 Creating Subjectivity Classifiers To evaluate the subjective nouns, we trained a Naive Bayes classifier using the nouns as features." ></td>
	<td class="line x" title="141:226	We also incorporated previously established subjectivity clues, and added some new discourse features." ></td>
	<td class="line x" title="142:226	In this section, we describe all the feature sets and present performance results for subjectivity classifiers trained on different combinations of these features." ></td>
	<td class="line x" title="143:226	The threshold values and feature representations used in this section are the ones that produced the best results on our separate tuning corpus." ></td>
	<td class="line x" title="144:226	4.1 Subjective Noun Features We defined four features to represent the sets of subjective nouns produced by the bootstrapping algorithms." ></td>
	<td class="line x" title="145:226	BA-Strong: the set of StrongSubjective nouns generated by Basilisk BA-Weak: the set of WeakSubjective nouns generated by Basilisk MB-Strong: the set of StrongSubjective nouns generated by Meta-Bootstrapping MB-Weak: the set of WeakSubjective nouns generated by Meta-Bootstrapping For each set, we created a three-valued feature based on the presence of 0, 1, or 2 words from that set." ></td>
	<td class="line x" title="146:226	We used the nouns as feature sets, rather than define a separate feature for each word, so the classifier could generalize over the set to minimize sparse data problems." ></td>
	<td class="line x" title="147:226	We will refer to these as the SubjNoun features." ></td>
	<td class="line x" title="148:226	4.2 Previously Established Features Wiebe, Bruce, & OHara (1999) developed a machine learning system to classify subjective sentences." ></td>
	<td class="line x" title="149:226	We experimented with the features that they used, both to compare their results to ours and to see if we could benefit from their features." ></td>
	<td class="line x" title="150:226	We will refer to these as the WBO features." ></td>
	<td class="line x" title="151:226	WBO includes a set of stems positively correlated with the subjective training examples (subjStems) and a set of stems positively correlated with the objective training examples (objStems)." ></td>
	<td class="line x" title="152:226	We defined a three-valued feature for the presence of 0, 1, or 2 members of subjStems in a sentence, and likewise for objStems." ></td>
	<td class="line x" title="153:226	For our experiments, subjStems includes stems that appear 7 times in the training set, and for which the precision is 1.25 times the baseline word precision for that training set." ></td>
	<td class="line x" title="154:226	objStems contains the stems that appear 7 times and for which at least 50% of their occurrences in the training set are in objective sentences." ></td>
	<td class="line x" title="155:226	WBO also includes a binary feature for each of the following: the presence in the sentence of a pronoun, an adjective, a cardinal number, a modal other than will, and an adverb other than not." ></td>
	<td class="line x" title="156:226	We also added manually-developed features found by other researchers." ></td>
	<td class="line x" title="157:226	We created 14 feature sets representing some classes from (Levin, 1993; Ballmer and Brennenstuhl, 1981), some Framenet lemmas with frame element experiencer (Baker et al. , 1998), adjectives manually annotated for polarity (Hatzivassiloglou and McKeown, 1997), and some subjectivity clues listed in (Wiebe, 1990)." ></td>
	<td class="line x" title="158:226	We represented each set as a three-valued feature based on the presence of 0, 1, or 2 members of the set." ></td>
	<td class="line x" title="159:226	We will refer to these as the manual features." ></td>
	<td class="line x" title="160:226	4.3 Discourse Features We created discourse features to capture the density of clues in the text surrounding a sentence." ></td>
	<td class="line x" title="161:226	First, we computed the average number of subjective clues and objective clues per sentence, normalized by sentence length." ></td>
	<td class="line x" title="162:226	The subjective clues, subjClues, are all sets for which 3-valued features were defined above (except objStems)." ></td>
	<td class="line x" title="163:226	The objective clues consist only of objStems." ></td>
	<td class="line x" title="164:226	For sentence S, let ClueRatesubj(S) = jsubjClues in SjjSj and ClueRateobj(S) = jobjStems in SjjSj." ></td>
	<td class="line x" title="165:226	Then we define AvgClueRatesubj to be the average of ClueRate(S) over all sentences S and similarly for AvgClueRateobj." ></td>
	<td class="line x" title="166:226	Next, we characterize the number of subjective and objective clues in the previous and next sentences as: higher-than-expected (high), lower-than-expected (low), or expected (medium)." ></td>
	<td class="line x" title="167:226	The value for ClueRatesubj(S) is high if ClueRatesubj(S) AvgClueRatesubj 1:3; low if ClueRatesubj(S) AvgClueRatesubj=1:3; otherwise it is medium." ></td>
	<td class="line x" title="168:226	The values for ClueRateobj(S) are defined similarly." ></td>
	<td class="line x" title="169:226	Using these definitions we created four features: ClueRatesubj for the previous and following sentences, and ClueRateobj for the previous and following sentences." ></td>
	<td class="line x" title="170:226	We also defined a feature for sentence length." ></td>
	<td class="line x" title="171:226	Let AvgSentLen be the average sentence length." ></td>
	<td class="line x" title="172:226	SentLen(S) is high if length(S) AvgSentLen 1:3; low if length(S) AvgSentLen=1:3; and medium otherwise." ></td>
	<td class="line x" title="173:226	4.4 Classification Results We conducted experiments to evaluate the performance of the feature sets, both individually and in various combinations." ></td>
	<td class="line x" title="174:226	Unless otherwise noted, all experiments involved training a Naive Bayes classifier using a particular set of features." ></td>
	<td class="line x" title="175:226	We evaluated each classifier using 25fold cross validation on the experiment corpus and used paired t-tests to measure significance at the 95% confidence level." ></td>
	<td class="line x" title="176:226	As our evaluation metrics, we computed accuracy (Acc) as the percentage of the systems classifications that match the gold-standard, and precision (Prec) and recall (Rec) with respect to subjective sentences." ></td>
	<td class="line x" title="177:226	Acc Prec Rec (1) Bag-Of-Words 73.3 81.7 70.9 (2) WBO 72.1 76.0 77.4 (3) Most-Frequent 59.0 59.0 100.0 Table 7: Baselines for Comparison Table 7 shows three baseline experiments." ></td>
	<td class="line x" title="178:226	Row (3) represents the common baseline of assigning every sentence to the most frequent class." ></td>
	<td class="line x" title="179:226	The Most-Frequent baseline achieves 59% accuracy because 59% of the sentences in the gold-standard are subjective." ></td>
	<td class="line x" title="180:226	Row (2) is a Naive Bayes classifier that uses the WBO features, which performed well in prior research on sentence-level subjectivity classification (Wiebe et al. , 1999)." ></td>
	<td class="line x" title="181:226	Row (1) shows a Naive Bayes classifier that uses unigram bag-ofwords features, with one binary feature for the absence or presence in the sentence of each word that appeared during training." ></td>
	<td class="line oc" title="182:226	Pang et al.(2002) reported that a similar experiment produced their best results on a related classification task." ></td>
	<td class="line x" title="184:226	The difference in accuracy between Rows (1) and (2) is not statistically significant (Bag-of-Words higher precision is balanced by WBOs higher recall)." ></td>
	<td class="line x" title="185:226	Next, we trained a Naive Bayes classifier using only the SubjNoun features." ></td>
	<td class="line x" title="186:226	This classifier achieved good precision (77%) but only moderate recall (64%)." ></td>
	<td class="line x" title="187:226	Upon further inspection, we discovered that the subjective nouns are good subjectivity indicators when they appear, but not every subjective sentence contains one of them." ></td>
	<td class="line x" title="188:226	And, relatively few sentences contain more than one, making it difficult to recognize contextual effects (i.e. , multiple clues in a region)." ></td>
	<td class="line x" title="189:226	We concluded that the appropriate way to benefit from the subjective nouns is to use them in tandem with other subjectivity clues." ></td>
	<td class="line x" title="190:226	Acc Prec Rec (1) 76.1 81.3 77.4 WBO+SubjNoun+ manual+discourse (2) 74.3 78.6 77.8 WBO+SubjNoun (3) 72.1 76.0 77.4 WBO Table 8: Results with New Features Table 8 shows the results of Naive Bayes classifiers trained with different combinations of features." ></td>
	<td class="line x" title="191:226	The accuracy differences between all pairs of experiments in Table 8 are statistically significant." ></td>
	<td class="line x" title="192:226	Row (3) uses only the WBO features (also shown in Table 7 as a baseline)." ></td>
	<td class="line x" title="193:226	Row (2) uses the WBO features as well as the SubjNoun features." ></td>
	<td class="line x" title="194:226	There is a synergy between these feature sets: using both types of features achieves better performance than either one alone." ></td>
	<td class="line x" title="195:226	The difference is mainly precision, presumably because the classifier found more and better combinations of features." ></td>
	<td class="line x" title="196:226	In Row (1), we also added the manual and discourse features." ></td>
	<td class="line x" title="197:226	The discourse features explicitly identify contexts in which multiple clues are found." ></td>
	<td class="line x" title="198:226	This classifier produced even better performance, achieving 81.3% precision with 77.4% recall." ></td>
	<td class="line x" title="199:226	The 76.1% accuracy result is significantly higher than the accuracy results for all of the other classifiers (in both Table 8 and Table 7)." ></td>
	<td class="line x" title="200:226	Finally, higher precision classification can be obtained by simply classifying a sentence as subjective if it contains any of the StrongSubjective nouns." ></td>
	<td class="line x" title="201:226	On our data, this method produces 87% precision with 26% recall." ></td>
	<td class="line x" title="202:226	This approach could support applications for which precision is paramount." ></td>
	<td class="line x" title="203:226	5 Related Work Several types of research have involved document-level subjectivity classification." ></td>
	<td class="line oc" title="204:226	Some work identifies inflammatory texts (e.g. , (Spertus, 1997)) or classifies reviews as positive or negative ((Turney, 2002; Pang et al. , 2002))." ></td>
	<td class="line x" title="205:226	Tongs system (Tong, 2001) generates sentiment timelines, tracking online discussions and creating graphs of positive and negative opinion messages over time." ></td>
	<td class="line x" title="206:226	Research in genre classification may include recognition of subjective genres such as editorials (e.g. , (Karlgren and Cutting, 1994; Kessler et al. , 1997; Wiebe et al. , 2001))." ></td>
	<td class="line x" title="207:226	In contrast, our work classifies individual sentences, as does the research in (Wiebe et al. , 1999)." ></td>
	<td class="line x" title="208:226	Sentence-level subjectivity classification is useful because most documents contain a mix of subjective and objective sentences." ></td>
	<td class="line x" title="209:226	For example, newspaper articles are typically thought to be relatively objective, but (Wiebe et al. , 2001) reported that, in their corpus, 44% of sentences (in articles that are not editorials or reviews) were subjective." ></td>
	<td class="line x" title="210:226	Some previous work has focused explicitly on learning subjective words and phrases." ></td>
	<td class="line x" title="211:226	(Hatzivassiloglou and McKeown, 1997) describes a method for identifying the semantic orientation of words, for example that beautiful expresses positive sentiments." ></td>
	<td class="line x" title="212:226	Researchers have focused on learning adjectives or adjectival phrases (Turney, 2002; Hatzivassiloglou and McKeown, 1997; Wiebe, 2000) and verbs (Wiebe et al. , 2001), but no previous work has focused on learning nouns." ></td>
	<td class="line x" title="213:226	A unique aspect of our work is the use of bootstrapping methods that exploit extraction patterns." ></td>
	<td class="line x" title="214:226	(Turney, 2002) used patterns representing part-of-speech sequences, (Hatzivassiloglou and McKeown, 1997) recognized adjectival phrases, and (Wiebe et al. , 2001) learned N-grams." ></td>
	<td class="line x" title="215:226	The extraction patterns used in our research are linguistically richer patterns, requiring shallow parsing and syntactic role assignment." ></td>
	<td class="line x" title="216:226	In recent years several techniques have been developed for semantic lexicon creation (e.g. , (Hearst, 1992; Riloff and Shepherd, 1997; Roark and Charniak, 1998; Caraballo, 1999))." ></td>
	<td class="line x" title="217:226	Semantic word learning is different from subjective word learning, but we have shown that MetaBootstrapping and Basilisk could be successfully applied to subjectivity learning." ></td>
	<td class="line x" title="218:226	Perhaps some of these other methods could also be used to learn subjective words." ></td>
	<td class="line x" title="219:226	6 Conclusions This research produced interesting insights as well as performance results." ></td>
	<td class="line x" title="220:226	First, we demonstrated that weakly supervised bootstrapping techniques can learn subjective terms from unannotated texts." ></td>
	<td class="line x" title="221:226	Subjective features learned from unannotated documents can augment or enhance features learned from annotated training data using more traditional supervised learning techniques." ></td>
	<td class="line x" title="222:226	Second, Basilisk and Meta-Bootstrapping proved to be useful for a different task than they were originally intended." ></td>
	<td class="line x" title="223:226	By seeding the algorithms with subjective words, the extraction patterns identified expressions that are associated with subjective nouns." ></td>
	<td class="line x" title="224:226	This suggests that the bootstrapping algorithms should be able to learn not only general semantic categories, but any category for which words appear in similar linguistic phrases." ></td>
	<td class="line x" title="225:226	Third, our best subjectivity classifier used a wide variety of features." ></td>
	<td class="line x" title="226:226	Subjectivity is a complex linguistic phenomenon and our evidence suggests that reliable subjectivity classification requires a broad array of features." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W03-1014
Learning Extraction Patterns For Subjective Expressions
Riloff, Ellen;Wiebe, Janyce M.;"></td>
	<td class="line x" title="1:210	Learning Extraction Patterns for Subjective Expressions Ellen Riloff School of Computing University of Utah Salt Lake City, UT 84112 riloff@cs.utah.edu Janyce Wiebe Department of Computer Science University of Pittsburgh Pittsburgh, PA 15260 wiebe@cs.pitt.edu Abstract This paper presents a bootstrapping process that learns linguistically rich extraction patterns for subjective (opinionated) expressions." ></td>
	<td class="line x" title="2:210	High-precision classifiers label unannotated data to automatically create a large training set, which is then given to an extraction pattern learning algorithm." ></td>
	<td class="line x" title="3:210	The learned patterns are then used to identify more subjective sentences." ></td>
	<td class="line x" title="4:210	The bootstrapping process learns many subjective patterns and increases recall while maintaining high precision." ></td>
	<td class="line x" title="5:210	1 Introduction Many natural language processing applications could benefit from being able to distinguish between factual and subjective information." ></td>
	<td class="line x" title="6:210	Subjective remarks come in a variety of forms, including opinions, rants, allegations, accusations, suspicions, and speculations." ></td>
	<td class="line x" title="7:210	Ideally, information extraction systems should be able to distinguish between factual information (which should be extracted) and non-factual information (which should be discarded or labeled as uncertain)." ></td>
	<td class="line x" title="8:210	Question answering systems should distinguish between factual and speculative answers." ></td>
	<td class="line x" title="9:210	Multi-perspective question answering aims to present multiple answers to the user based upon speculation or opinions derived from different sources." ></td>
	<td class="line x" title="10:210	Multidocument summarization systems need to summarize different opinions and perspectives." ></td>
	<td class="line x" title="11:210	Spam filtering systems This work was supported by the National Science Foundation under grants IIS-0208798, IIS-0208985, and IRI-9704240." ></td>
	<td class="line x" title="12:210	The data preparation was performed in support of the Northeast Regional Research Center (NRRC) which is sponsored by the Advanced Research and Development Activity (ARDA), a U.S. Government entity which sponsors and promotes research of import to the Intelligence Community which includes but is not limited to the CIA, DIA, NSA, NIMA, and NRO." ></td>
	<td class="line x" title="13:210	must recognize rants and emotional tirades, among other things." ></td>
	<td class="line x" title="14:210	In general, nearly any system that seeks to identify information could benefit from being able to separate factual and subjective information." ></td>
	<td class="line x" title="15:210	Some existing resources contain lists of subjective words (e.g. , Levins desire verbs (1993)), and some empirical methods in NLP have automatically identified adjectives, verbs, and N-grams that are statistically associated with subjective language (e.g. , (Turney, 2002; Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Wiebe et al. , 2001))." ></td>
	<td class="line x" title="16:210	However, subjective language can be exhibited by a staggering variety of words and phrases." ></td>
	<td class="line x" title="17:210	In addition, many subjective terms occur infrequently, such as strongly subjective adjectives (e.g. , preposterous, unseemly) and metaphorical or idiomatic phrases (e.g. , dealt a blow, swept off ones feet)." ></td>
	<td class="line x" title="18:210	Consequently, we believe that subjectivity learning systems must be trained on extremely large text collections before they will acquire a subjective vocabulary that is truly broad and comprehensive in scope." ></td>
	<td class="line x" title="19:210	To address this issue, we have been exploring the use of bootstrapping methods to allow subjectivity classifiers to learn from a collection of unannotated texts." ></td>
	<td class="line x" title="20:210	Our research uses high-precision subjectivity classifiers to automatically identify subjective and objective sentences in unannotated texts." ></td>
	<td class="line x" title="21:210	This process allows us to generate a large set of labeled sentences automatically." ></td>
	<td class="line x" title="22:210	The second emphasis of our research is using extraction patterns to represent subjective expressions." ></td>
	<td class="line x" title="23:210	These patterns are linguistically richer and more flexible than single words or N-grams." ></td>
	<td class="line x" title="24:210	Using the (automatically) labeled sentences as training data, we apply an extraction pattern learning algorithm to automatically generate patterns representing subjective expressions." ></td>
	<td class="line x" title="25:210	The learned patterns can be used to automatically identify more subjective sentences, which grows the training set, and the entire process can then be bootstrapped." ></td>
	<td class="line x" title="26:210	Our experimental results show that this bootstrapping process increases the recall of the highprecision subjective sentence classifier with little loss in precision." ></td>
	<td class="line x" title="27:210	We also find that the learned extraction patterns capture subtle connotations that are more expressive than the individual words by themselves." ></td>
	<td class="line x" title="28:210	This paper is organized as follows." ></td>
	<td class="line x" title="29:210	Section 2 discusses previous work on subjectivity analysis and extraction pattern learning." ></td>
	<td class="line x" title="30:210	Section 3 overviews our general approach, describes the high-precision subjectivity classifiers, and explains the algorithm for learning extraction patterns associated with subjectivity." ></td>
	<td class="line x" title="31:210	Section 4 describes the data that we use, presents our experimental results, and shows examples of patterns that are learned." ></td>
	<td class="line x" title="32:210	Finally, Section 5 summarizes our findings and conclusions." ></td>
	<td class="line x" title="33:210	2 Background 2.1 Subjectivity Analysis Much previous work on subjectivity recognition has focused on document-level classification." ></td>
	<td class="line oc" title="34:210	For example, (Spertus, 1997) developed a system to identify inflammatory texts and (Turney, 2002; Pang et al. , 2002) developed methods for classifying reviews as positive or negative." ></td>
	<td class="line x" title="35:210	Some research in genre classification has included the recognition of subjective genres such as editorials (e.g. , (Karlgren and Cutting, 1994; Kessler et al. , 1997; Wiebe et al. , 2001))." ></td>
	<td class="line x" title="36:210	In contrast, the goal of our work is to classify individual sentences as subjective or objective." ></td>
	<td class="line x" title="37:210	Document-level classification can distinguish between subjective texts, such as editorials and reviews, and objective texts, such as newspaper articles." ></td>
	<td class="line x" title="38:210	But in reality, most documents contain a mix of both subjective and objective sentences." ></td>
	<td class="line x" title="39:210	Subjective texts often include some factual information." ></td>
	<td class="line x" title="40:210	For example, editorial articles frequently contain factual information to back up the arguments being made, and movie reviews often mention the actors and plot of a movie as well as the theatres where its currently playing." ></td>
	<td class="line x" title="41:210	Even if one is willing to discard subjective texts in their entirety, the objective texts usually contain a great deal of subjective information in addition to facts." ></td>
	<td class="line x" title="42:210	For example, newspaper articles are generally considered to be relatively objective documents, but in a recent study (Wiebe et al. , 2001) 44% of sentences in a news collection were found to be subjective (after editorial and review articles were removed)." ></td>
	<td class="line x" title="43:210	One of the main obstacles to producing a sentencelevel subjectivity classifier is a lack of training data." ></td>
	<td class="line x" title="44:210	To train a document-level classifier, one can easily find collections of subjective texts, such as editorials and reviews." ></td>
	<td class="line oc" title="45:210	For example, (Pang et al. , 2002) collected reviews from a movie database and rated them as positive, negative, or neutral based on the rating (e.g. , number of stars) given by the reviewer." ></td>
	<td class="line x" title="46:210	It is much harder to obtain collections of individual sentences that can be easily identified as subjective or objective." ></td>
	<td class="line x" title="47:210	Previous work on sentence-level subjectivity classification (Wiebe et al. , 1999) used training corpora that had been manually annotated for subjectivity." ></td>
	<td class="line x" title="48:210	Manually producing annotations is time consuming, so the amount of available annotated sentence data is relatively small." ></td>
	<td class="line x" title="49:210	The goal of our research is to use high-precision subjectivity classifiers to automatically identify subjective and objective sentences in unannotated text corpora." ></td>
	<td class="line x" title="50:210	The high-precision classifiers label a sentence as subjective or objective when they are confident about the classification, and they leave a sentence unlabeled otherwise." ></td>
	<td class="line x" title="51:210	Unannotated texts are easy to come by, so even if the classifiers can label only 30% of the sentences as subjective or objective, they will still produce a large collection of labeled sentences." ></td>
	<td class="line x" title="52:210	Most importantly, the high-precision classifiers can generate a much larger set of labeled sentences than are currently available in manually created data sets." ></td>
	<td class="line x" title="53:210	2.2 Extraction Patterns Information extraction (IE) systems typically use lexicosyntactic patterns to identify relevant information." ></td>
	<td class="line x" title="54:210	The specific representation of these patterns varies across systems, but most patterns represent role relationships surrounding noun and verb phrases." ></td>
	<td class="line x" title="55:210	For example, an IE system designed to extract information about hijackings might use the pattern hijacking of <x>, which looks for the noun hijacking and extracts the object of the preposition of as the hijacked vehicle." ></td>
	<td class="line x" title="56:210	The pattern <x> was hijacked would extract the hijacked vehicle when it finds the verb hijacked in the passive voice, and the pattern <x> hijacked would extract the hijacker when it finds the verb hijacked in the active voice." ></td>
	<td class="line x" title="57:210	One of our hypotheses was that extraction patterns would be able to represent subjective expressions that have noncompositional meanings." ></td>
	<td class="line x" title="58:210	For example, consider the common expression drives (someone) up the wall, which expresses the feeling of being annoyed with something." ></td>
	<td class="line x" title="59:210	The meaning of this expression is quite different from the meanings of its individual words (drives, up, wall)." ></td>
	<td class="line x" title="60:210	Furthermore, this expression is not a fixed word sequence that could easily be captured by N-grams." ></td>
	<td class="line x" title="61:210	It is a relatively flexible construction that may be more generally represented as <x> drives <y> up the wall, where x and y may be arbitrary noun phrases." ></td>
	<td class="line x" title="62:210	This pattern would match many different sentences, such as George drives me up the wall, She drives the mayor up the wall, or The nosy old man drives his quiet neighbors up the wall. We also wondered whether the extraction pattern representation might reveal slight variations of the same verb or noun phrase that have different connotations." ></td>
	<td class="line x" title="63:210	For example, you can say that a comedian bombed last night, which is a subjective statement, but you cant express this sentiment with the passive voice of bombed." ></td>
	<td class="line x" title="64:210	In Section 3.2, we will show examples of extraction patterns representing subjective expressions which do in fact exhibit both of these phenomena." ></td>
	<td class="line x" title="65:210	A variety of algorithms have been developed to automatically learn extraction patterns." ></td>
	<td class="line x" title="66:210	Most of these algorithms require special training resources, such as texts annotated with domain-specific tags (e.g. , AutoSlog (Riloff, 1993), CRYSTAL (Soderland et al. , 1995), RAPIER (Califf, 1998), SRV (Freitag, 1998), WHISK (Soderland, 1999)) or manually defined keywords, frames, or object recognizers (e.g. , PALKA (Kim and Moldovan, 1993) and LIEP (Huffman, 1996))." ></td>
	<td class="line x" title="67:210	AutoSlog-TS (Riloff, 1996) takes a different approach, requiring only a corpus of unannotated texts that have been separated into those that are related to the target domain (the relevant texts) and those that are not (the irrelevant texts)." ></td>
	<td class="line x" title="68:210	Most recently, two bootstrapping algorithms have been used to learn extraction patterns." ></td>
	<td class="line x" title="69:210	Metabootstrapping (Riloff and Jones, 1999) learns both extraction patterns and a semantic lexicon using unannotated texts and seed words as input." ></td>
	<td class="line x" title="70:210	ExDisco (Yangarber et al. , 2000) uses a bootstrapping mechanism to find new extraction patterns using unannotated texts and some seed patterns as the initial input." ></td>
	<td class="line x" title="71:210	For our research, we adopted a learning process very similar to that used by AutoSlog-TS, which requires only relevant texts and irrelevant texts as its input." ></td>
	<td class="line x" title="72:210	We describe this learning process in more detail in the next section." ></td>
	<td class="line x" title="73:210	3 Learning and Bootstrapping Extraction Patterns for Subjectivity We have developed a bootstrapping process for subjectivity classification that explores three ideas: (1) highprecision classifiers can be used to automatically identify subjective and objective sentences from unannotated texts, (2) this data can be used as a training set to automatically learn extraction patterns associated with subjectivity, and (3) the learned patterns can be used to grow the training set, allowing this entire process to be bootstrapped." ></td>
	<td class="line x" title="74:210	Figure 1 shows the components and layout of the bootstrapping process." ></td>
	<td class="line x" title="75:210	The process begins with a large collection of unannotated text and two high precision subjectivity classifiers." ></td>
	<td class="line x" title="76:210	One classifier searches the unannotated corpus for sentences that can be labeled as subjective with high confidence, and the other classifier searches for sentences that can be labeled as objective with high confidence." ></td>
	<td class="line x" title="77:210	All other sentences in the corpus are left unlabeled." ></td>
	<td class="line x" title="78:210	The labeled sentences are then fed to an extraction pattern learner, which produces a set of extraction patterns that are statistically correlated with the subjective sentences (we will call these the subjective patterns)." ></td>
	<td class="line x" title="79:210	These patterns are then used to identify more sentences within the unannotated texts that can be classified as subjective." ></td>
	<td class="line x" title="80:210	The extraction pattern learner can then retrain using the larger training set and the process repeats." ></td>
	<td class="line x" title="81:210	The subjective patterns can also be added to the highprecision subjective sentence classifier as new features to improve its performance." ></td>
	<td class="line x" title="82:210	The dashed lines in Figure 1 represent the parts of the process that are bootstrapped." ></td>
	<td class="line x" title="83:210	In this section, we will describe the high-precision sentence classifiers, the extraction pattern learning process, and the details of the bootstrapping process." ></td>
	<td class="line x" title="84:210	3.1 High-Precision Subjectivity Classifiers The high-precision classifiers (HP-Subj and HP-Obj) use lists of lexical items that have been shown in previous work to be good subjectivity clues." ></td>
	<td class="line x" title="85:210	Most of the items are single words, some are N-grams, but none involve syntactic generalizations as in the extraction patterns." ></td>
	<td class="line x" title="86:210	Any data used to develop this vocabulary does not overlap with the test sets or the unannotated data used in this paper." ></td>
	<td class="line x" title="87:210	Many of the subjective clues are from manually developed resources, including entries from (Levin, 1993; Ballmer and Brennenstuhl, 1981), Framenet lemmas with frame element experiencer (Baker et al. , 1998), adjectives manually annotated for polarity (Hatzivassiloglou and McKeown, 1997), and subjectivity clues listed in (Wiebe, 1990)." ></td>
	<td class="line x" title="88:210	Others were derived from corpora, including subjective nouns learned from unannotated data using bootstrapping (Riloff et al. , 2003)." ></td>
	<td class="line x" title="89:210	The subjectivity clues are divided into those that are strongly subjective and those that are weakly subjective, using a combination of manual review and empirical results on a small training set of manually annotated data." ></td>
	<td class="line x" title="90:210	As the terms are used here, a strongly subjective clue is one that is seldom used without a subjective meaning, whereas a weakly subjective clue is one that commonly has both subjective and objective uses." ></td>
	<td class="line x" title="91:210	The high-precision subjective classifier classifies a sentence as subjective if it contains two or more of the strongly subjective clues." ></td>
	<td class="line x" title="92:210	On a manually annotated test set, this classifier achieves 91.5% precision and 31.9% recall (that is, 91.5% of the sentences that it selected are subjective, and it found 31.9% of the subjective sentences in the test set)." ></td>
	<td class="line x" title="93:210	This test set consists of 2197 sentences, 59% of which are subjective." ></td>
	<td class="line x" title="94:210	The high-precision objective classifier takes a different approach." ></td>
	<td class="line x" title="95:210	Rather than looking for the presence of lexical items, it looks for their absence." ></td>
	<td class="line x" title="96:210	It classifies a sentence as objective if there are no strongly subjective clues and at most one weakly subjective clue in the current, previous, and next sentence combined." ></td>
	<td class="line x" title="97:210	Why doesnt the objective classifier mirror the subjective classifier, and consult its own list of strongly objective clues?" ></td>
	<td class="line x" title="98:210	There are certainly lexical items that are statistically correlated with the obKnown Subjective Vocabulary HighPrecision Objective Sentence Classifier (HPObj) HighPrecision Subjective Sentence Classifier (HPSubj) Unannotated Text Collection unlabeled sentences unlabeled sentences unlabeled sentences Patternbased Subjective Sentence Classifier Extraction Pattern Learner subjective sentences subjective sentences objective sentences subjective patterns subjective patterns Figure 1: Bootstrapping Process jective class (examples are cardinal numbers (Wiebe et al. , 1999), and words such as per, case, market, and total), but the presence of such clues does not readily lead to high precision objective classification." ></td>
	<td class="line x" title="99:210	Add sarcasm or a negative evaluation to a sentence about a dry topic such as stock prices, and the sentence becomes subjective." ></td>
	<td class="line x" title="100:210	Conversely, add objective topics to a sentence containing two strongly subjective words such as odious and scumbag, and the sentence remains subjective." ></td>
	<td class="line x" title="101:210	The performance of the high-precision objective classifier is a bit lower than the subjective classifier: 82.6% precision and 16.4% recall on the test set mentioned above (that is, 82.6% of the sentences selected by the objective classifier are objective, and the objective classifier found 16.4% of the objective sentences in the test set)." ></td>
	<td class="line x" title="102:210	Although there is room for improvement, the performance proved to be good enough for our purposes." ></td>
	<td class="line x" title="103:210	3.2 Learning Subjective Extraction Patterns To automatically learn extraction patterns that are associated with subjectivity, we use a learning algorithm similar to AutoSlog-TS (Riloff, 1996)." ></td>
	<td class="line x" title="104:210	For training, AutoSlogTS uses a text corpus consisting of two distinct sets of texts: relevant texts (in our case, subjective sentences) and irrelevant texts (in our case, objective sentences)." ></td>
	<td class="line x" title="105:210	A set of syntactic templates represents the space of possible extraction patterns." ></td>
	<td class="line x" title="106:210	The learning process has two steps." ></td>
	<td class="line x" title="107:210	First, the syntactic templates are applied to the training corpus in an exhaustive fashion, so that extraction patterns are generated for (literally) every possible instantiation of the templates that appears in the corpus." ></td>
	<td class="line x" title="108:210	The left column of Figure 2 shows the syntactic templates used by AutoSlog-TS." ></td>
	<td class="line x" title="109:210	The right column shows a specific extraction pattern that was learned during our subjectivity experiments as an instantiation of the syntactic form on the left." ></td>
	<td class="line x" title="110:210	For example, the pattern <subj> was satisfied1 will match any sentence where the verb satisfied appears in the passive voice." ></td>
	<td class="line x" title="111:210	The pattern <subj> dealt blow represents a more complex expression that will match any sentence that contains a verb phrase with head=dealt followed by a direct object with head=blow." ></td>
	<td class="line x" title="112:210	This would match sentences such as The experience dealt a stiff blow to his pride. It is important to recognize that these patterns look for specific syntactic constructions produced by a (shallow) parser, rather than exact word sequences." ></td>
	<td class="line x" title="113:210	SYNTACTIC FORM EXAMPLE PATTERN <subj> passive-verb <subj> was satisfied <subj> active-verb <subj> complained <subj> active-verb dobj <subj> dealt blow <subj> verb infinitive <subj> appear to be <subj> aux noun <subj> has position active-verb <dobj> endorsed <dobj> infinitive <dobj> to condemn <dobj> verb infinitive <dobj> get to know <dobj> noun aux <dobj> fact is <dobj> noun prep <np> opinion on <np> active-verb prep <np> agrees with <np> passive-verb prep <np> was worried about <np> infinitive prep <np> to resort to <np> Figure 2: Syntactic Templates and Examples of Patterns that were Learned 1This is a shorthand notation for the internal representation." ></td>
	<td class="line x" title="114:210	PATTERN FREQ %SUBJ <subj> was asked 11 100% <subj> asked 128 63% <subj> is talk 5 100% talk of <np> 10 90% <subj> will talk 28 71% <subj> put an end 10 90% <subj> put 187 67% <subj> is going to be 11 82% <subj> is going 182 67% was expected from <np> 5 100% <subj> was expected 45 42% <subj> is fact 38 100% fact is <dobj> 12 100% Figure 3: Patterns with Interesting Behavior The second step of AutoSlog-TSs learning process applies all of the learned extraction patterns to the training corpus and gathers statistics for how often each pattern occurs in subjective versus objective sentences." ></td>
	<td class="line x" title="115:210	AutoSlog-TS then ranks the extraction patterns using a metric called RlogF (Riloff, 1996) and asks a human to review the ranked list and make the final decision about which patterns to keep." ></td>
	<td class="line x" title="116:210	In contrast, for this work we wanted a fully automatic process that does not depend on a human reviewer, and we were most interested in finding patterns that can identify subjective expressions with high precision." ></td>
	<td class="line x" title="117:210	So we ranked the extraction patterns using a conditional probability measure: the probability that a sentence is subjective given that a specific extraction pattern appears in it." ></td>
	<td class="line x" title="118:210	The exact formula is: Pr(subjective j patterni) = subjfreq(patterni)freq(patterni) where subjfreq(patterni) is the frequency of patterni in subjective training sentences, and freq(patterni) is the frequency of patterni in all training sentences." ></td>
	<td class="line x" title="119:210	(This may also be viewed as the precision of the pattern on the training data)." ></td>
	<td class="line x" title="120:210	Finally, we use two thresholds to select extraction patterns that are strongly associated with subjectivity in the training data." ></td>
	<td class="line x" title="121:210	We choose extraction patterns for which freq(patterni) 1 and Pr(subjective j patterni) 2." ></td>
	<td class="line x" title="122:210	Figure 3 shows some patterns learned by our system, the frequency with which they occur in the training data (FREQ) and the percentage of times they occur in subjective sentences (%SUBJ)." ></td>
	<td class="line x" title="123:210	For example, the first two rows show the behavior of two similar expressions using the verb asked." ></td>
	<td class="line x" title="124:210	100% of the sentences that contain asked in the passive voice are subjective, but only 63% of the sentences that contain asked in the active voice are subjective." ></td>
	<td class="line x" title="125:210	A human would probably not expect the active and passive voices to behave so differently." ></td>
	<td class="line x" title="126:210	To understand why this is so, we looked in the training data and found that the passive voice is often used to query someone about a specific opinion." ></td>
	<td class="line x" title="127:210	For example, here is one such sentence from our training set: Ernest Bai Koroma of RITCORP was asked to address his supporters on his views relating to full blooded Temne to head APC. In contrast, many of the sentences containing asked in the active voice are more general in nature, such as The mayor asked a newly formed JR about his petition. Figure 3 also shows that expressions using talk as a noun (e.g. , Fred is the talk of the town) are highly correlated with subjective sentences, while talk as a verb (e.g. , The mayor will talk about) are found in a mix of subjective and objective sentences." ></td>
	<td class="line x" title="128:210	Not surprisingly, longer expressions tend to be more idiomatic (and subjective) than shorter expressions (e.g. , put an end (to) vs. put; is going to be vs. is going; was expected from vs. was expected)." ></td>
	<td class="line x" title="129:210	Finally, the last two rows of Figure 3 show that expressions involving the noun fact are highly correlated with subjective expressions!" ></td>
	<td class="line x" title="130:210	These patterns match sentences such as The fact is and is a fact, which apparently are often used in subjective contexts." ></td>
	<td class="line x" title="131:210	This example illustrates that the corpus-based learning method can find phrases that might not seem subjective to a person intuitively, but that are reliable indicators of subjectivity." ></td>
	<td class="line x" title="132:210	4 Experimental Results 4.1 Subjectivity Data The text collection that we used consists of Englishlanguage versions of foreign news documents from FBIS, the U.S. Foreign Broadcast Information Service." ></td>
	<td class="line x" title="133:210	The data is from a variety of countries." ></td>
	<td class="line x" title="134:210	Our system takes unannotated data as input, but we needed annotated data to evaluate its performance." ></td>
	<td class="line x" title="135:210	We briefly describe the manual annotation scheme used to create the gold-standard, and give interannotator agreement results." ></td>
	<td class="line x" title="136:210	In 2002, a detailed annotation scheme (Wilson and Wiebe, 2003) was developed for a government-sponsored project." ></td>
	<td class="line x" title="137:210	We only mention aspects of the annotation scheme relevant to this paper." ></td>
	<td class="line x" title="138:210	The scheme was inspired by work in linguistics and literary theory on subjectivity, which focuses on how opinions, emotions, etc. are expressed linguistically in context (Banfield, 1982)." ></td>
	<td class="line x" title="139:210	The goal is to identify and characterize expressions of private states in a sentence." ></td>
	<td class="line x" title="140:210	Private state is a general covering term for opinions, evaluations, emotions, and speculations (Quirk et al. , 1985)." ></td>
	<td class="line x" title="141:210	For example, in sentence (1) the writer is expressing a negative evaluation." ></td>
	<td class="line x" title="142:210	(1) The time has come, gentlemen, for Sharon, the assassin, to realize that injustice cannot last long. Sentence (2) reflects the private state of Western countries." ></td>
	<td class="line x" title="143:210	Mugabes use of overwhelmingly also reflects a private state, his positive reaction to and characterization of his victory." ></td>
	<td class="line x" title="144:210	(2) Western countries were left frustrated and impotent after Robert Mugabe formally declared that he had overwhelmingly won Zimbabwes presidential election. Annotators are also asked to judge the strength of each private state." ></td>
	<td class="line x" title="145:210	A private state may have low, medium, high or extreme strength." ></td>
	<td class="line x" title="146:210	To allow us to measure interannotator agreement, three annotators (who are not authors of this paper) independently annotated the same 13 documents with a total of 210 sentences." ></td>
	<td class="line x" title="147:210	We begin with a strict measure of agreement at the sentence level by first considering whether the annotator marked any private-state expression, of any strength, anywhere in the sentence." ></td>
	<td class="line x" title="148:210	If so, the sentence is subjective." ></td>
	<td class="line x" title="149:210	Otherwise, it is objective." ></td>
	<td class="line x" title="150:210	The average pairwise percentage agreement is 90% and the average pairwise value is 0.77." ></td>
	<td class="line x" title="151:210	One would expect that there are clear cases of objective sentences, clear cases of subjective sentences, and borderline sentences in between." ></td>
	<td class="line x" title="152:210	The agreement study supports this." ></td>
	<td class="line x" title="153:210	In terms of our annotations, we define a sentence as borderline if it has at least one private-state expression identified by at least one annotator, and all strength ratings of private-state expressions are low." ></td>
	<td class="line x" title="154:210	On average, 11% of the corpus is borderline under this definition." ></td>
	<td class="line x" title="155:210	When those sentences are removed, the average pairwise percentage agreement increases to 95% and the average pairwise value increases to 0.89." ></td>
	<td class="line x" title="156:210	As expected, the majority of disagreement cases involve low-strength subjectivity." ></td>
	<td class="line x" title="157:210	The annotators consistently agree about which are the clear cases of subjective sentences." ></td>
	<td class="line x" title="158:210	This leads us to define the gold-standard that we use when evaluating our results." ></td>
	<td class="line x" title="159:210	A sentence is subjective if it contains at least one private-state expression of medium or higher strength." ></td>
	<td class="line x" title="160:210	The second class, which we call objective, consists of everything else." ></td>
	<td class="line x" title="161:210	4.2 Evaluation of the Learned Patterns Our pool of unannotated texts consists of 302,163 individual sentences." ></td>
	<td class="line x" title="162:210	The HP-Subj classifier initially labeled roughly 44,300 of these sentences as subjective, and the HP-Obj classifier initially labeled roughly 17,000 sentences as objective." ></td>
	<td class="line x" title="163:210	In order to keep the training set relatively balanced, we used all 17,000 objective sentences and 17,000 of the subjective sentences as training data for the extraction pattern learner." ></td>
	<td class="line x" title="164:210	17,073 extraction patterns were learned that have frequency 2 and Pr(subjective j patterni) .60 on the training data." ></td>
	<td class="line x" title="165:210	We then wanted to determine whether the extraction patterns are, in fact, good indicators of subjectivity." ></td>
	<td class="line x" title="166:210	To evaluate the patterns, we applied different subsets of them to a test set to see if they consistently occur in subjective sentences." ></td>
	<td class="line x" title="167:210	This test set consists of 3947 Figure 4: Evaluating the Learned Patterns on Test Data sentences, 54% of which are subjective." ></td>
	<td class="line x" title="168:210	Figure 4 shows sentence recall and pattern (instancelevel) precision for the learned extraction patterns on the test set." ></td>
	<td class="line x" title="169:210	In this figure, precision is the proportion of pattern instances found in the test set that are in subjective sentences, and recall is the proportion of subjective sentences that contain at least one pattern instance." ></td>
	<td class="line x" title="170:210	We evaluated 18 different subsets of the patterns, by selecting the patterns that pass certain thresholds in the training data." ></td>
	<td class="line x" title="171:210	We tried all combinations of 1 = f2,10g and 2 = f.60,.65,.70,.75,.80,.85,.90,.95,1.0g." ></td>
	<td class="line x" title="172:210	The data points corresponding to 1=2 are shown on the upper line in Figure 4, and those corresponding to 1=10 are shown on the lower line." ></td>
	<td class="line x" title="173:210	For example, the data point corresponding to 1=10 and 2=.90 evaluates only the extraction patterns that occur at least 10 times in the training data and with a probability .90 (i.e. , at least 90% of its occurrences are in subjective training sentences)." ></td>
	<td class="line x" title="174:210	Overall, the extraction patterns perform quite well." ></td>
	<td class="line x" title="175:210	The precision ranges from 71% to 85%, with the expected tradeoff between precision and recall." ></td>
	<td class="line x" title="176:210	This experiment confirms that the extraction patterns are effective at recognizing subjective expressions." ></td>
	<td class="line x" title="177:210	4.3 Evaluation of the Bootstrapping Process In our second experiment, we used the learned extraction patterns to classify previously unlabeled sentences from the unannotated text collection." ></td>
	<td class="line x" title="178:210	The new subjective sentences were then fed back into the Extraction Pattern Learner to complete the bootstrapping cycle depicted by the rightmost dashed line in Figure 1." ></td>
	<td class="line x" title="179:210	The Patternbased Subjective Sentence Classifier classifies a sentence as subjective if it contains at least one extraction pattern with 1 5 and 2 1.0 on the training data." ></td>
	<td class="line x" title="180:210	This process produced approximately 9,500 new subjective sentences that were previously unlabeled." ></td>
	<td class="line x" title="181:210	Since our bootstrapping process does not learn new objective sentences, we did not want to simply add the new subjective sentences to the training set, or it would become increasingly skewed toward subjective sentences." ></td>
	<td class="line x" title="182:210	Since HP-Obj had produced roughly 17,000 objective sentences used for training, we used the 9,500 new subjective sentences along with 7,500 of the previously identified subjective sentences as our new training set." ></td>
	<td class="line x" title="183:210	In other words, the training set that we used during the second bootstrapping cycle contained exactly the same objective sentences as the first cycle, half of the same subjective sentences as the first cycle, and 9,500 brand new subjective sentences." ></td>
	<td class="line x" title="184:210	On this second cycle of bootstrapping, the extraction pattern learner generated many new patterns that were not discovered during the first cycle." ></td>
	<td class="line x" title="185:210	4,248 new patterns were found that have 1 2 and 2 .60." ></td>
	<td class="line x" title="186:210	If we consider only the strongest (most subjective) extraction patterns, 308 new patterns were found that had 1 10 and 2 1.0." ></td>
	<td class="line x" title="187:210	This is a substantial set of new extraction patterns that seem to be very highly correlated with subjectivity." ></td>
	<td class="line x" title="188:210	An open question was whether the new patterns provide additional coverage." ></td>
	<td class="line x" title="189:210	To assess this, we did a simple test: we added the 4,248 new patterns to the original set of patterns learned during the first bootstrapping cycle." ></td>
	<td class="line x" title="190:210	Then we repeated the same analysis that we depict in Figure 4." ></td>
	<td class="line x" title="191:210	In general, the recall numbers increased by about 2-4% while the precision numbers decreased by less, from 0.5-2%." ></td>
	<td class="line x" title="192:210	In our third experiment, we evaluated whether the learned patterns can improve the coverage of the highprecision subjectivity classifier (HP-Subj), to complete the bootstrapping loop depicted in the top-most dashed line of Figure 1." ></td>
	<td class="line x" title="193:210	Our hope was that the patterns would allow more sentences from the unannotated text collection to be labeled as subjective, without a substantial drop in precision." ></td>
	<td class="line x" title="194:210	For this experiment, we selected the learned extraction patterns that had 1 10 and 2 1.0 on the training set, since these seemed likely to be the most reliable (high precision) indicators of subjectivity." ></td>
	<td class="line x" title="195:210	We modified the HP-Subj classifier to use extraction patterns as follows." ></td>
	<td class="line x" title="196:210	All sentences labeled as subjective by the original HP-Subj classifier are also labeled as subjective by the new version." ></td>
	<td class="line x" title="197:210	For previously unlabeled sentences, the new version classifies a sentence as subjective if (1) it contains two or more of the learned patterns, or (2) it contains one of the clues used by the original HPSubj classifier and at least one learned pattern." ></td>
	<td class="line x" title="198:210	Table 1 shows the performance results on the test set mentioned in Section 3.1 (2197 sentences) for both the original HPSubj classifier and the new version that uses the learned extraction patterns." ></td>
	<td class="line x" title="199:210	The extraction patterns produce a 7.2 percentage point gain in coverage, and only a 1.1 percentage point drop in precision." ></td>
	<td class="line x" title="200:210	This result shows that the learned extraction patterns do improve the performance of the high-precision subjective sentence classifier, allowing it to classify more sentences as subjective with nearly the same high reliability." ></td>
	<td class="line x" title="201:210	HP-Subj HP-Subj w/Patterns Recall Precision Recall Precision 32.9 91.3 40.1 90.2 Table 1: Bootstrapping the Learned Patterns into the High-Precision Sentence Classifier Table 2 gives examples of patterns used to augment the HP-Subj classifier which do not overlap in non-function words with any of the clues already known by the original system." ></td>
	<td class="line x" title="202:210	For each pattern, we show an example sentence from our corpus that matches the pattern." ></td>
	<td class="line x" title="203:210	5 Conclusions This research explored several avenues for improving the state-of-the-art in subjectivity analysis." ></td>
	<td class="line x" title="204:210	First, we demonstrated that high-precision subjectivity classification can be used to generate a large amount of labeled training data for subsequent learning algorithms to exploit." ></td>
	<td class="line x" title="205:210	Second, we showed that an extraction pattern learning technique can learn subjective expressions that are linguistically richer than individual words or fixed phrases." ></td>
	<td class="line x" title="206:210	We found that similar expressions may behave very differently, so that one expression may be strongly indicative of subjectivity but the other may not." ></td>
	<td class="line x" title="207:210	Third, we augmented our original high-precision subjective classifier with these newly learned extraction patterns." ></td>
	<td class="line x" title="208:210	This bootstrapping process resulted in substantially higher recall with a minimal loss in precision." ></td>
	<td class="line x" title="209:210	In future work, we plan to experiment with different configurations of these classifiers, add new subjective language learners in the bootstrapping process, and address the problem of how to identify new objective sentences during bootstrapping." ></td>
	<td class="line x" title="210:210	6 Acknowledgments We are very grateful to Theresa Wilson for her invaluable programming support and help with data preparation." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W03-1017
Towards Answering Opinion Questions: Separating Facts From Opinions And Identifying The Polarity Of Opinion Sentences
Yu, Hong;Hatzivassiloglou, Vasileios;"></td>
	<td class="line x" title="1:165	Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences Hong Yu Department of Computer Science Columbia University New York, NY 10027, USA hongyu@cs.columbia.edu Vasileios Hatzivassiloglou Department of Computer Science Columbia University New York, NY 10027, USA vh@cs.columbia.edu Abstract Opinion question answering is a challenging task for natural language processing." ></td>
	<td class="line x" title="2:165	In this paper, we discuss a necessary component for an opinion question answering system: separating opinions from fact, at both the document and sentence level." ></td>
	<td class="line x" title="3:165	We present a Bayesian classifier for discriminating between documents with a preponderance of opinions such as editorials from regular news stories, and describe three unsupervised, statistical techniques for the significantly harder task of detecting opinions at the sentence level." ></td>
	<td class="line x" title="4:165	We also present a first model for classifying opinion sentences as positive or negative in terms of the main perspective being expressed in the opinion." ></td>
	<td class="line x" title="5:165	Results from a large collection of news stories and a human evaluation of 400 sentences are reported, indicating that we achieve very high performance in document classification (upwards of 97% precision and recall), and respectable performance in detecting opinions and classifying them at the sentence level as positive, negative, or neutral (up to 91% accuracy)." ></td>
	<td class="line x" title="6:165	1 Introduction Newswire articles include those that mainly present opinions or ideas, such as editorials and letters to the editor, and those that mainly report facts such as daily news articles." ></td>
	<td class="line x" title="7:165	Text materials from many other sources also contain mixed facts and opinions." ></td>
	<td class="line x" title="8:165	For many natural language processing applications, the ability to detect and classify factual and opinion sentences offers distinct advantages in deciding what information to extract and how to organize and present this information." ></td>
	<td class="line x" title="9:165	For example, information extraction applications may target factual statements rather than subjective opinions, and summarization systems may list separately factual information and aggregate opinions according to distinct perspectives." ></td>
	<td class="line x" title="10:165	At the document level, information retrieval systems can target particular types of articles and even utilize perspectives in focusing queries (e.g. , filtering or retrieving only editorials in favor of a particular policy decision)." ></td>
	<td class="line x" title="11:165	Our motivation for building the opinion detection and classification system described in this paper is the need for organizing information in the context of question answering for complex questions." ></td>
	<td class="line x" title="12:165	Unlike questions like Who was the first man on the moon? which can be answered with a simple phrase, more intricate questions such as What are the reasons for the US-Iraq war? require long answers that must be constructed from multiple sources." ></td>
	<td class="line x" title="13:165	In such a context, it is imperative that the question answering system can discriminate between opinions and facts, and either use the appropriate type depending on the question or combine them in a meaningful presentation." ></td>
	<td class="line x" title="14:165	Perspective information can also help highlight contrasts and contradictions between different sourcesthere will be significant disparity in the material collected for the question mentioned above between Fox News and the Independent, for example." ></td>
	<td class="line x" title="15:165	Fully analyzing and classifying opinions involves tasks that relate to some fairly deep semantic and syntactic analysis of the text." ></td>
	<td class="line x" title="16:165	These include not only recognizing that the text is subjective, but also determining who the holder of the opinion is, what the opinion is about, and which of many possible positions the holder of the opinion expresses regarding that subject." ></td>
	<td class="line x" title="17:165	In this paper, we are presenting three of the components of our opinion detection and organization subsystem, which have already been integrated into our larger question-answering system." ></td>
	<td class="line x" title="18:165	These components deal with the initial tasks of classifying articles as mostly subjective or objective, finding opinion sentences in both kinds of articles, and determining, in general terms and without reference to a specific subject, if the opinions are positive or negative." ></td>
	<td class="line x" title="19:165	The three modules of the system discussed here provide the basis for ongoing work for further classification of opinions according to subject and opinion holder and for refining the original positive/negative attitude determination." ></td>
	<td class="line x" title="20:165	We review related work in Section 2, and then present our document-level classifier for opinion or factual articles (Section 3), three implemented techniques for detecting opinions at the sentence level (Section 4), and our approach for rating an opinion as positive or negative (Section 5)." ></td>
	<td class="line x" title="21:165	We have evaluated these methods using a large collection of news articles without additional annotation (Section 6) and an evaluation corpus of 400 sentences annotated for opinion classifications (Section 7)." ></td>
	<td class="line x" title="22:165	The results, presented in Section 8, indicate that we achieve very high performance (more than 97%) at document-level classification and respectable performance (8691%) at detecting opinion sentences and classifying them according to orientation." ></td>
	<td class="line x" title="23:165	2 Related Work Much of the earlier research in automated opinion detection has been performed by Wiebe and colleagues (Bruce and Wiebe, 1999; Wiebe et al. , 1999; Hatzivassiloglou and Wiebe, 2000; Wiebe, 2000; Wiebe et al. , 2002), who proposed methods for discriminating between subjective and objective text at the document, sentence, and phrase levels." ></td>
	<td class="line x" title="24:165	Bruce and Wiebe (1999) annotated 1,001 sentences as subjective or objective, and Wiebe et al.(1999) described a sentence-level Naive Bayes classifier using as features the presence or absence of particular syntactic classes (pronouns, adjectives, cardinal numbers, modal verbs, adverbs), punctuation, and sentence position." ></td>
	<td class="line x" title="26:165	Subsequently, Hatzivassiloglou and Wiebe (2000) showed that automatically detected gradable adjectives are a useful feature for subjectivity classification, while Wiebe (2000) introduced lexical features in addition to the presence/absence of syntactic categories." ></td>
	<td class="line x" title="27:165	More recently, Wiebe et al.(2002) report on document-level subjectivity classification, using a k-nearest neighbor algorithm based on the total count of subjective words and phrases within each document." ></td>
	<td class="line x" title="29:165	Psychological studies (Bradley and Lang, 1999) found measurable associations between words and human emotions." ></td>
	<td class="line x" title="30:165	Hatzivassiloglou and McKeown (1997) described an unsupervised learning method for obtaining positively and negatively oriented adjectives with accuracy over 90%, and demonstrated that this semantic orientation, or polarity, is a consistent lexical property with high inter-rater agreement." ></td>
	<td class="line x" title="31:165	Turney (2002) showed that it is possible to use only a few of those semantically oriented words (namely, excellent and poor) to label other phrases co-occuring with them as positive or negative." ></td>
	<td class="line x" title="32:165	He then used these phrases to automatically separate positive and negative movie and product reviews, with accuracy of 6684%." ></td>
	<td class="line oc" title="33:165	Pang et al.(2002) adopted a more direct approach, using supervised machine learning with words and n-grams as features to predict orientation at the document level with up to 83% precision." ></td>
	<td class="line x" title="35:165	Our approach to document and sentence classification of opinions builds upon the earlier work by using extended lexical models with additional features." ></td>
	<td class="line x" title="36:165	Unlike the work cited above, we do not rely on human annotations for training but only on weak metadata provided at the document level." ></td>
	<td class="line x" title="37:165	Our sentence-level classifiers introduce additional criteria for detecting subjective material (opinions), including methods based on sentence similarity within a topic and an approach that relies on multiple classifiers." ></td>
	<td class="line x" title="38:165	At the document level, our classifier uses the same document labels that the method of (Wiebe et al. , 2002) does, but automatically detects the words and phrases of importance without further analysis of the text." ></td>
	<td class="line x" title="39:165	For determining whether an opinion sentence is positive or negative, we have used seed words similar to those produced by (Hatzivassiloglou and McKeown, 1997) and extended them to construct a much larger set of semantically oriented words with a method similar to that proposed by (Turney, 2002)." ></td>
	<td class="line oc" title="40:165	Our focus is on the sentence level, unlike (Pang et al. , 2002) and (Turney, 2002); we employ a significantly larger set of seed words, and we explore as indicators of orientation words from syntactic classes other than adjectives (nouns, verbs, and adverbs)." ></td>
	<td class="line x" title="41:165	3 Document Classification To separate documents that contain primarily opinions from documents that report mainly facts, we applied Naive Bayes1, a commonly used supervised machine-learning algorithm." ></td>
	<td class="line x" title="42:165	This approach presupposes the availability of at least a collection of articles with pre-assigned opinion and fact labels at the document level; fortunately, Wall Street Journal articles contain such metadata by identifying the type of each article as Editorial, Letter to editor, Business and News." ></td>
	<td class="line x" title="43:165	These labels are used only to provide the correct classification labels during training and evaluation, and are not included in the feature space." ></td>
	<td class="line x" title="44:165	We used as features single words, without stemming or stopword removal." ></td>
	<td class="line x" title="45:165	Naive Bayes assigns a document a0 to the class a1 that maximizes a2a4a3a5a1a7a6 a0a9a8 by applying Bayes rule a2a4a3a5a1a7a6 a0a9a8a11a10a13a12a15a14a17a16a19a18a20a12a15a14a17a21a23a22a16a19a18 a12a15a14a17a21a24a18 and assuming conditional independence of the features." ></td>
	<td class="line oc" title="46:165	Although Naive Bayes can be outperformed in text classification tasks by more complex methods such as SVMs, Pang et al.(2002) report similar performance for Naive Bayes and other machine learning techniques for a similar task, that of distinguishing between positive and negative reviews at the document level." ></td>
	<td class="line x" title="48:165	Further, we achieved such high performance with Naive Bayes (see Section 8) that exploring additional techniques for this task seemed unnecessary." ></td>
	<td class="line x" title="49:165	4 Finding Opinion Sentences We developed three different approaches to classify opinions from facts at the sentence level." ></td>
	<td class="line x" title="50:165	To avoid the need for obtaining individual sentence annotations for training and evaluation, we rely instead on the expectation that documents classified as opinion on the whole (e.g. , editorials) will tend to have mostly opinion sentences, and conversely documents placed in the factual category will tend to have mostly factual sentences." ></td>
	<td class="line x" title="51:165	Wiebe et al.(2002) report that this expectation is borne out 75% of the time for opinion documents and 56% of the time for factual documents." ></td>
	<td class="line x" title="53:165	4.1 Similarity Approach Our first approach to classifying sentences as opinions or facts explores the hypothesis that, within a given topic, opinion sentences will be more similar to other opinion sentences than to factual sen1Using the Rainbow implementation, available from www." ></td>
	<td class="line x" title="54:165	cs.cmu.edu/mccallum/bow/rainbow." ></td>
	<td class="line x" title="55:165	tences." ></td>
	<td class="line x" title="56:165	We used SIMFINDER (Hatzivassiloglou et al. , 2001), a state-of-the-art system for measuring sentence similarity based on shared words, phrases, and WordNet synsets." ></td>
	<td class="line x" title="57:165	To measure the overall similarity of a sentence to the opinion or fact documents, we first select the documents that are on the same topic as the sentence in question." ></td>
	<td class="line x" title="58:165	We obtain topics as the results of IR queries (for example, by searching our document collection for welfare reform)." ></td>
	<td class="line x" title="59:165	We then average its SIMFINDER-provided similarities with each sentence in those documents." ></td>
	<td class="line x" title="60:165	Then we assign the sentence to the category for which the average is higher (we call this approach the score variant)." ></td>
	<td class="line x" title="61:165	Alternatively, for the frequency variant, we do not use the similarity scores themselves but instead we count how many of them, for each category, exceed a predetermined threshold (empirically set to 0.65)." ></td>
	<td class="line x" title="62:165	4.2 Naive Bayes Classifier Our second method trains a Naive Bayes classifier (see Section 3), using the sentences in opinion and fact documents as the examples of the two categories." ></td>
	<td class="line x" title="63:165	The features include words, bigrams, and trigrams, as well as the parts of speech in each sentence." ></td>
	<td class="line x" title="64:165	In addition, the presence of semantically oriented (positive and negative) words in a sentence is an indicator that the sentence is subjective (Hatzivassiloglou and Wiebe, 2000)." ></td>
	<td class="line x" title="65:165	Therefore, we include in our features the counts of positive and negative words in the sentence (which are obtained with the method of Section 5.1), as well as counts of the polarities of sequences of semantically oriented words (e.g. , ++ for two consecutive positively oriented words)." ></td>
	<td class="line x" title="66:165	We also include the counts of parts of speech combined with polarity information (e.g. , JJ+ for positive adjectives), as well as features encoding the polarity (if any) of the head verb, the main subject, and their immediate modifiers." ></td>
	<td class="line x" title="67:165	Syntactic structure was obtained with Charniaks statistical parser (Charniak, 2000)." ></td>
	<td class="line x" title="68:165	Finally, we used as one of the features the average semantic orientation score of the words in the sentence." ></td>
	<td class="line x" title="69:165	4.3 Multiple Naive Bayes Classifiers Our designation of all sentences in opinion or factual articles as opinion or fact sentences is an approximation." ></td>
	<td class="line x" title="70:165	To address this, we apply an algorithm using multiple classifiers, each relying on a different subset of our features." ></td>
	<td class="line x" title="71:165	The goal is to reduce the training set to the sentences that are most likely to be correctly labeled, thus boosting classification accuracy." ></td>
	<td class="line x" title="72:165	Given separate sets of features a0a2a1a4a3a5a0a7a6a8a3a4a9a8a9a4a9a10a3a5a0a12a11, we train separate Naive Bayes classifiers a13a14a1a15a3a16a13a17a6, a9a4a9a8a9a18a3a16a13a19a11 corresponding to each feature set." ></td>
	<td class="line x" title="73:165	Assuming as ground truth the information provided by the document labels and that all sentences inherit the status of their document as opinions or facts, we first traina13 a1 on the entire training set, then use the resulting classifier to predict labels for the training set." ></td>
	<td class="line x" title="74:165	The sentences that receive a label different from the assumed truth are then removed, and we train a13 a6 on the remaining sentences." ></td>
	<td class="line x" title="75:165	This process is repeated iteratively until no more sentences can be removed." ></td>
	<td class="line x" title="76:165	We report results using five feature sets, starting from words alone and adding in bigrams, trigrams, part-of-speech, and polarity." ></td>
	<td class="line x" title="77:165	5 Identifying the Polarity of Opinion Sentences Having distinguished whether a sentence is a fact or opinion, we separate positive, negative, and neutral opinions into three classes." ></td>
	<td class="line x" title="78:165	We base this decision on the number and strength of semantically oriented words (either positive or negative) in the sentence." ></td>
	<td class="line x" title="79:165	We first discuss how such words are automatically found by our system, and then describe the method by which we aggregate this information across the sentence." ></td>
	<td class="line x" title="80:165	5.1 Semantically Oriented Words To determine which words are semantically oriented, in what direction, and the strength of their orientation, we measured their co-occurrence with words from a known seed set of semantically oriented words." ></td>
	<td class="line x" title="81:165	The approach is based on the hypothesis that positive words co-occur more than expected by chance, and so do negative words; this hypothesis was validated, at least for strong positive/negative words, in (Turney, 2002)." ></td>
	<td class="line x" title="82:165	As seed words, we used subsets of the 1,336 adjectives that were manually classified as positive (657) or negative (679) by Hatzivassiloglou and McKeown (1997)." ></td>
	<td class="line x" title="83:165	In earlier work (Turney, 2002) only singletons were used as seed words; varying their number allows us to test whether multiple seed words have a positive effect in detection performance." ></td>
	<td class="line x" title="84:165	We experimented with seed sets containing 1, 20, 100 and over 600 positive and negative pairs of adjectives." ></td>
	<td class="line x" title="85:165	For a given seed set size, we denote the set of positive seeds as ADJa20 and the set of negative seeds as ADJa21." ></td>
	<td class="line x" title="86:165	We then calculate a modified log-likelihood ratio a22 a3a24a23a26a25a27a3 POSa28 a8 for a word a23 a25 with part of speech POSa28 (a29 can be adjective, adverb, noun or verb) as the ratio of its collocation frequency with ADJa20 and ADJa21 within a sentence, a22 a3a30a23 a25a3 POSa28 a8 a10a32a31a34a33a36a35 a37a38 Freq a14a40a39a19a41a43a42POS a44 a42ADJ a45 a18a47a46a49a48 Freqa14a50a39 alla42POSa44a42ADJa45 a18 Freqa14a50a39a51a41a30a42POSa44a42ADJa52 a18a53a46a54a48 Freqa14a50a39 alla42POSa44a42ADJa52 a18 a55a56 where Freqa3a30a23 alla3 POSa28a3 ADJa20 a8 represents the collocation frequency of all wordsa23 all of part of speech POSa28 with ADJa20 and a57 is a smoothing constant (a57 a10a59a58 a9a61a60 in our case)." ></td>
	<td class="line x" title="87:165	We used Brills tagger (Brill, 1995) to obtain part-of-speech information." ></td>
	<td class="line x" title="88:165	5.2 Sentence Polarity Tagging As our measure of semantic orientation across an entire sentence we used the average per word loglikelihood scores defined in the preceding section." ></td>
	<td class="line x" title="89:165	To determine the orientation of an opinion sentence, all that remains is to specify cutoffs a62a47a20 and a62a63a21 so that sentences for which the average log-likelihood score exceeds a62a53a20 are classified as positive opinions, sentences with scores lower than a62a63a21 are classified as negative opinions, and sentences with in-between scores are treated as neutral opinions." ></td>
	<td class="line x" title="90:165	Optimal values fora62a20 anda62a21 are obtained from the training data via density estimationusing a small, hand-labeled subset of sentences we estimate the proportion of sentences that are positive or negative." ></td>
	<td class="line x" title="91:165	The values of the average log-likelihood score that correspond to the appropriate tails of the score distribution are then determined via Monte Carlo analysis of a much larger sample of unlabeled training data." ></td>
	<td class="line x" title="92:165	6 Data We used the TREC2 8, 9, and 11 collections, which consist of more than 1.7 million newswire articles." ></td>
	<td class="line x" title="93:165	The aggregate collection covers six different newswire sources including 173,252 Wall Street 2http://trec.nist.gov/." ></td>
	<td class="line x" title="94:165	Journal (WSJ) articles from 1987 to 1992." ></td>
	<td class="line x" title="95:165	Some of the WSJ articles have structured headings that include Editorial, Letter to editor, Business, and News (2,877, 1,695, 2,009 and 3,714 articles, respectively)." ></td>
	<td class="line x" title="96:165	We randomly selected 2,000 articles3 from each category so that our data set was approximate evenly divided between fact and opinion articles." ></td>
	<td class="line x" title="97:165	Those articles were used for both document and sentence level opinion/fact classification." ></td>
	<td class="line x" title="98:165	7 Evaluation Metrics and Gold Standard For classification tasks (i.e. , classifying between facts and opinions and identifying the semantic orientation of sentences), we measured our systems performance by standard recall and precision." ></td>
	<td class="line x" title="99:165	We evaluated the quality of semantically oriented words by mapping the extracted words and labels to an external gold standard." ></td>
	<td class="line x" title="100:165	We took the subset of our output containing words that appear in the standard, and measured the accuracy of our output as the portion of that subset that was assigned the correct label." ></td>
	<td class="line x" title="101:165	A gold standard for document-level classification is readily available, since each article in our Wall Street Journal collection comes with an article type label (see Section 6)." ></td>
	<td class="line x" title="102:165	We mapped article types News and Business to facts, and article types Editorial and Letter to the Editor to opinions." ></td>
	<td class="line x" title="103:165	We cannot automatically select a sentence-level gold standard discriminating between facts and opinions, or between positive and negative opinions." ></td>
	<td class="line x" title="104:165	We therefore asked human evaluators to classify a set of sentences between facts and opinions as well as determine the type of opinions." ></td>
	<td class="line x" title="105:165	Since we have implemented our methods in an opinion question answering system, we selected four different topics (gun control, illegal aliens, social security, and welfare reform)." ></td>
	<td class="line x" title="106:165	For each topic, we randomly selected 25 articles from the entire combined TREC corpus (not just the WSJ portion); these were articles matching the corresponding topical phrase given above as determined by the Lucene search engine.4 From each of these documents we randomly selected four sentences." ></td>
	<td class="line x" title="107:165	If a document happened to have less than four sentences, additional 3Except for Letters to Editor, for which we included all 1,695 articles available." ></td>
	<td class="line x" title="108:165	4http://www.jguru.com/faq/Lucene." ></td>
	<td class="line x" title="109:165	Label A B Agreement Fact 123 16 46% Opinion 258 65 77% Uncertain 19 1 33% Breakdown of opinion labels Positive 33 4 29% Negative 131 27 51% No orientation 45 6 26% Mixed orientation 8 0 0% Uncertain orientation 41 1 7% Table 1: Statistics of gold standards A and B. documents from the same topic were retrieved to supply the missing sentences." ></td>
	<td class="line x" title="110:165	The resulting a0a2a1 a3 a60a4a1a5a0 a10 a0 a58a36a58 sentences were then interleaved so that successive sentences came from different topics and documents and divided into ten 50-sentence blocks." ></td>
	<td class="line x" title="111:165	Each block shares ten sentences with the preceding and following block (the last block is considered to precede the first one), so that 100 of the 400 sentences appear in two blocks." ></td>
	<td class="line x" title="112:165	Each of ten human evaluators (all with graduate training in computational linguistics) was presented with one block and asked to select a label for each sentence among the following: fact, positive opinion, negative opinion, neutral opinion, sentence contains both positive and negative opinions, opinion but cannot determine orientation, and uncertain.5 Since we have one judgment for 300 sentences and two judgments for 100 sentences, we created two gold standards for sentence classification." ></td>
	<td class="line x" title="113:165	The first (Standard A) includes the 300 sentences with one judgment and a single judgment for the remaining 100 sentences.6 The second standard (Standard B) contains the subset of the 100 sentences for which we obtained identical labels." ></td>
	<td class="line x" title="114:165	Statistics of these two standards are given in Table 1." ></td>
	<td class="line x" title="115:165	We measured the pairwise agreement among the 100 sentences that were judged by two evaluators, as the ratio of sentences that receive a label a6 from both evaluators divided by the total number of sentences receiving label a6 from any evaluator." ></td>
	<td class="line x" title="116:165	The agreement across 5The full instructions can be viewed online at http: //www1.cs.columbia.edu/hongyu/research/ emnlp03/opinion-eval-instructions.html." ></td>
	<td class="line x" title="117:165	6In order to assign a unique label, we arbitrarily chose the first evaluator for those sentences." ></td>
	<td class="line x" title="118:165	F-measure News vs. Editorial 0.96 News+Business vs. Editorial+Letter 0.97 Table 2: Document-level fact/opinion classification by Naive Bayes algorithm." ></td>
	<td class="line x" title="119:165	the 100 sentences for all seven choices was 55%; if we group together the five subtypes of opinion sentences, the overall agreement rises to 82%." ></td>
	<td class="line x" title="120:165	The low agreement for some labels was not surprising because there is much ambiguity between facts and opinions." ></td>
	<td class="line x" title="121:165	An example of an arguable sentence is A lethal guerrilla war between poachers and wardens now rages in central and eastern Africa, which one rater classified as fact and another rater classified as opinion." ></td>
	<td class="line x" title="122:165	Finally, for evaluating the quality of extracted words with semantic orientation labels, we used two distinct manually labeled collections as gold standards." ></td>
	<td class="line x" title="123:165	One set consists of the previously described 657 positive and 679 negative adjectives (Hatzivassiloglou and McKeown, 1997)." ></td>
	<td class="line x" title="124:165	We also used the ANEW list which was constructed during psycholinguistic experiments (Bradley and Lang, 1999) and contains 1,031 words of all four open classes." ></td>
	<td class="line x" title="125:165	As described in (Bradley and Lang, 1999), humans assigned valence scores to each score according to dimensions such as pleasure, arousal, and dominance; following heuristics proposed in psycholinguistics7 we obtained 284 positive and 272 negative words from the valence scores." ></td>
	<td class="line x" title="126:165	8 Results and Discussion Document Classification We trained our Bayes classifier for documents on 4,000 articles from the WSJ portion of our combined TREC collection, and evaluated on 4,000 other articles also from the WSJ part." ></td>
	<td class="line x" title="127:165	Table 2 lists the F-measure scores (the harmonic mean of precision and recall) of our Bayesian classifier for document-level opinion/fact classification." ></td>
	<td class="line x" title="128:165	The results show the classifier achieved 97% F-measure, which is comparable or higher than the 93% accuracy reported by (Wiebe et al. , 2002), who evaluated their work based on a similar set of WSJ articles." ></td>
	<td class="line x" title="129:165	The high classification performance 7http://www.sci.sdsu.edu/CAL/wordlist/." ></td>
	<td class="line x" title="130:165	Variant Class Standard A Standard B Score Fact a0 0.61,0.34a1 a0 1.00,0.27 a1Opinion a0 0.30,0.49a1 a0 0.16,0.64 a1 Frequency Fact a0 0.82,0.32a1 a0 0.89,0.19 a1Opinion a0 0.17,0.55a1 a0 0.28,0.55 a1 Table 3: a0 Recall, precision a1 of similarity classifier." ></td>
	<td class="line x" title="131:165	is also consistent with a high inter-rater agreement (kappa=0.95) for document-level fact/opinion annotation (Wiebe et al. , 2002)." ></td>
	<td class="line x" title="132:165	Note that we trained and evaluated only on WSJ articles for which we can obtain article class metadata, so the classifier may perform less accurately when used for other newswire articles." ></td>
	<td class="line x" title="133:165	Sentence Classification Table 3 shows the recall and precision of the similarity-based approach, while Table 4 lists the recall and precision of naive Bayes (single and multiple classifiers) for sentencelevel opinion/fact classification." ></td>
	<td class="line x" title="134:165	In both cases, the results are better when we evaluate against Standard B, containing the sentences for which two humans assign the same label; obviously, it is easier for the automatic system to produce the correct label in these more clear-cut cases." ></td>
	<td class="line x" title="135:165	Our Naive Bayes classifier has a higher recall and precision (8090%) for detecting opinions than for facts (around 50%)." ></td>
	<td class="line x" title="136:165	While words and n-grams had little performance effect for the opinion class, they increased the recall for the fact class around five fold compared to the approach by Wiebe et al.(1999)." ></td>
	<td class="line x" title="138:165	In general, the additional features helped the classifier; the best performance is achieved when words, bigrams, trigrams, part-of-speech, and polarity are included in the feature set." ></td>
	<td class="line x" title="139:165	Further, using multiple classifiers to automatically identify an appropriate subset of the data for training slightly increases performance." ></td>
	<td class="line x" title="140:165	Polarity Classification Using the method of Section 5.1, we automatically identified a total of 39,652 (65,773), 3,128 (4,426), 144,238 (195,984), and 22,279 (30,609) positive (negative) adjectives, adverbs, nouns, and verbs, respectively." ></td>
	<td class="line x" title="141:165	Extracted positive words include inspirational, truly, luck, and achieve." ></td>
	<td class="line x" title="142:165	Negative ones include depraved, disastrously, problem, and depress." ></td>
	<td class="line x" title="143:165	Figure 1 plots the Features Class Standard A Standard BSingle Multiple Single Multiple Features from (Wiebe et al. , 1999) Fact a0 0.03,0.38 a1 a0 0.03,0.38 a1 a0 0.06,1.00a1 a0 0.06,1.00 a1Opinion a0 0.97,0.69 a1 a0 0.97,0.69 a1 a0 1.00,0.80a1 a0 1.00,0.80 a1 Words only Fact a0 0.14,0.39 a1 a0 0.12,0.42 a1 a0 0.28,0.42a1 a0 0.28,0.45 a1Opinion a0 0.90,0.69 a1 a0 0.92,0.69 a1 a0 0.90,0.82a1 a0 0.91,0.83 a1 Words and Bigrams Fact a0 0.15,0.39 a1 a0 0.12,0.43 a1 a0 0.16,0.25a1 a0 0.16,0.25 a1Opinion a0 0.89,0.69 a1 a0 0.92,0.69 a1 a0 0.87,0.79a1 a0 0.87,0.79 a1 Words, Bigrams, and Trigrams Fact a0 0.18,0.44 a1 a0 0.13,0.41 a1 a0 0.26,0.50a1 a0 0.26,0.50 a1Opinion a0 0.89,0.70 a1 a0 0.91,0.69 a1 a0 0.93,0.82a1 a0 0.93,0.82 a1 Words, Bigrams, Trigrams, and Part-of-Speech Fact a0 0.17,0.42 a1 a0 0.13,0.40 a1 a0 0.18,0.49a1 a0 0.27,0.44 a1 Opinion a0 0.89,0.70 a1 a0 0.91,0.69 a1 a0 0.92,0.70a1 a0 0.85,0.84 a1 Words, Bigrams, Trigrams, Part-of-Speech, and Polarity Fact a0 0.15,0.43 a1 a0 0.13,0.42 a1 a0 0.44,0.50a1 a0 0.44,0.53 a1 Opinion a0 0.91,0.69 a1 a0 0.92,0.70 a1 a0 0.88,0.86a1 a0 0.91,0.86 a1 Table 4: a0 Recall, precisiona1 of opinion/fact sentence classification using different features and either a single or multiple (data cleaning) classifiers." ></td>
	<td class="line x" title="144:165	Figure 1: Recall and precision (1,336 manually labeled positive and negative adjectives as gold standard) of extracted adjectives using 1, 20, and 100 positive and negative adjective pairs as seeds." ></td>
	<td class="line x" title="145:165	recall and precision of extracted adjectives by using randomly selected seed sets of 1, 20, and 100 pairs of positive and negative adjectives from the list of (Hatzivassiloglou and McKeown, 1997)." ></td>
	<td class="line x" title="146:165	Both recall and precision increase as the seed set becomes larger." ></td>
	<td class="line x" title="147:165	We obtained similar results with the ANEW list of adjectives (Section 7)." ></td>
	<td class="line x" title="148:165	As an additional experiment, we tested the effect of ignoring sentences with negative particles, obtaining a small increase in precision and recall." ></td>
	<td class="line x" title="149:165	We subsequently used the automatically extracted polarity score for each word to assign an aggregate Parts-of-speech Used A B Adjectives 0.49 0.55 Adverbs 0.37 0.46 Nouns 0.54 0.52 Verbs 0.54 0.52 Adjectives and Adverbs 0.55 0.84 Adjectives, Adverbs, and Verbs 0.68 0.90 Adjectives, Adverbs, Nouns, and Verbs 0.62 0.74 Table 5: Accuracy of sentence polarity tagging on gold standards A and B for different sets of parts-ofspeech." ></td>
	<td class="line x" title="150:165	polarity to opinion sentences." ></td>
	<td class="line x" title="151:165	Table 5 lists the accuracy of our sentence-level tagging process." ></td>
	<td class="line x" title="152:165	We experimented with different combinations of part-ofspeech classes for calculating the aggregate polarity scores, and found that the combined evidence from adjectives, adverbs, and verbs achieves the highest accuracy (90% over a baseline of 48%)." ></td>
	<td class="line x" title="153:165	As in the case of sentence-level classification between opinion and fact, we also found the performance to be higher on Standard B, for which humans exhibited consistent agreement." ></td>
	<td class="line x" title="154:165	9 Conclusions We presented several models for distinguishing between opinions and facts, and between positive and negative opinions." ></td>
	<td class="line x" title="155:165	At the document level, a fairly straightforward Bayesian classifier using lexical information can distinguish between mostly factual and mostly opinion documents with very high precision and recall (F-measure of 97%)." ></td>
	<td class="line x" title="156:165	The task is much harder at the sentence level." ></td>
	<td class="line x" title="157:165	For that case, we described three novel techniques for opinion/fact classification achieving up to 91% precision and recall on the detection of opinion sentences." ></td>
	<td class="line x" title="158:165	We also examined an automatic method for assigning polarity information to single words and sentences, accurately discriminating between positive, negative, and neutral opinions in 90% of the cases." ></td>
	<td class="line x" title="159:165	Our work so far has focused on characterizing opinions and facts in a generic manner, without examining who the opinion holder is or what the opinion is about." ></td>
	<td class="line x" title="160:165	While we have found presenting information organized in separate opinion and fact classes useful, our goal is to introduce further analysis of each sentence so that opinion sentences can be linked to particular perspectives on a specific subject." ></td>
	<td class="line x" title="161:165	We intend to cluster together sentences from the same perspective and present them in summary form as answers to subjective questions." ></td>
	<td class="line x" title="162:165	Acknowledgments We wish to thank Eugene Agichtein, Sasha BlairGoldensohn, Roy Byrd, John Chen, Noemie Elhadad, Kathy McKeown, Becky Passonneau, and the anonymous reviewers for valuable input on earlier versions of this paper." ></td>
	<td class="line x" title="163:165	We are grateful to the graduate students at Columbia University who participated in our evaluation of sentence-level opinions." ></td>
	<td class="line x" title="164:165	This work was supported by ARDA under AQUAINT project MDA908-02-C-0008." ></td>
	<td class="line x" title="165:165	Any opinions, findings, or recommendations are those of the authors and do not necessarily reflect ARDAs views." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C04-1071
Deeper Sentiment Analysis Using Machine Translation Technology
Kanayama, Hiroshi;Nasukawa, Tetsuya;Watanabe, Hideo;"></td>
	<td class="line x" title="1:218	Deeper Sentiment Analysis Using Machine Translation Technology KANAYAMA Hiroshi NASUKAWA Tetsuya WATANABE Hideo Tokyo Research Laboratory, IBM Japan, Ltd. 1623-14 Shimotsuruma, Yamato-shi, Kanagawa-ken, 242-8502 Japan fhkana,nasukawa,hiwatg@jp.ibm.com Abstract This paper proposes a new paradigm for sentiment analysis: translation from text documents to a set of sentiment units." ></td>
	<td class="line x" title="2:218	The techniques of deep language analysis for machine translation are applicable also to this kind of text mining task." ></td>
	<td class="line x" title="3:218	We developed a high-precision sentiment analysis system at a low development cost, by making use of an existing transfer-based machine translation engine." ></td>
	<td class="line x" title="4:218	1 Introduction Sentiment analysis (SA) (Nasukawa and Yi, 2003; Yi et al. , 2003) is a task to obtain writers feelings as expressed in positive or negative comments, questions, and requests, by analyzing large numbers of documents." ></td>
	<td class="line x" title="5:218	SA is becoming a useful tool for the commercial activities of both companies and individual consumers, because they want to sort out opinions about products, services, or brands that are scattered in online texts such as product review articles, replies given to questionnaires, and messages in bulletin boards on the WWW." ></td>
	<td class="line x" title="6:218	This paper describes a method to extract a set of sentiment units from sentences, which is the key component of SA." ></td>
	<td class="line x" title="7:218	A sentiment unit is a tuple of a sentiment1, a predicate, and its arguments." ></td>
	<td class="line x" title="8:218	For example, these sentences in a customers review of a digital camera (1) contained three sentiment units (1a), (1b), and (1c)." ></td>
	<td class="line x" title="9:218	Apparently these units indicate that the camera has good features in its lens and recharger, and a bad feature in its price." ></td>
	<td class="line x" title="10:218	It has excellent lens, but the price is too high." ></td>
	<td class="line x" title="11:218	I dont think the quality of the recharger has any problem." ></td>
	<td class="line x" title="12:218	9= ; (1) [favorable] excellent (lens) (1a) [unfavorable] high (price) (1b) [favorable] problematic+neg (recharger) (1c) The extraction of these sentiment units is not a trivial task because many syntactic and semantic operations are required." ></td>
	<td class="line x" title="13:218	First, the structure of a predicate and its arguments may be changed from the 1Possible values of a sentiment are favorable, unfavorable, question, and request." ></td>
	<td class="line x" title="14:218	In this paper the discussion is mostly focused on the first two values." ></td>
	<td class="line x" title="15:218	syntactic form as in (1a) and (1c)." ></td>
	<td class="line x" title="16:218	Also modal, aspectual, and negation information must be handled, as in (1c)." ></td>
	<td class="line x" title="17:218	Second, a sentiment unit should be constructed as the smallest possible informative unit so that it is easy to handle for the organizing processes after extraction." ></td>
	<td class="line x" title="18:218	In (1b) the degree adverb too is omitted to normalize the expression." ></td>
	<td class="line x" title="19:218	For (1c), the predicate problematic has the argument recharger instead of the head word of the noun phrase the quality of the recharger, because just using quality is not informative to describe the sentiment of the attribute of a real-world object." ></td>
	<td class="line x" title="20:218	Moreover, disambiguation of sentiments is necessary: in (1b) the adjective high has the unfavorable feature, but high can be treated as favorable in the expression resolution is high." ></td>
	<td class="line x" title="21:218	We regard this task as translation from text to sentiment units, because we noticed that the deep language analysis techniques which are required for the extraction of sentiment units are analogous to those which have been studied for the purpose of language translation." ></td>
	<td class="line x" title="22:218	We implemented an accurate sentiment analyzer by making use of an existing transfer-based machine translation engine (Watanabe, 1992), replacing the translation patterns and bilingual lexicons with sentiment patterns and a sentiment polarity lexicon." ></td>
	<td class="line x" title="23:218	Although we used many techniques for deep language analysis, the system was implemented at a surprisingly low development cost because the techniques for machine translation could be reused in the architecture described in this paper." ></td>
	<td class="line x" title="24:218	We aimed at the high precision extraction of sentiment units." ></td>
	<td class="line x" title="25:218	In other words, our SA system attaches importance to each individual sentiment expression, rather than to the quantitative tendencies of reputation." ></td>
	<td class="line x" title="26:218	This is in order to meet the requirement of the SA users who want to know not only the overall goodness of an object, but also the breakdown of opinions." ></td>
	<td class="line x" title="27:218	For example, when there are many positive opinions and only one negative opinion, the negative one should not be ignored because of its low percentage, but should be investigated thoroughly since valuable knowledge is often found in such a minority opinion." ></td>
	<td class="line x" title="28:218	Figure 1 illustrates an image of the SA output." ></td>
	<td class="line x" title="29:218	The outliner organizes positive and negative opinions by topic words, and provides references to the original text." ></td>
	<td class="line x" title="30:218	Favorable Unfavorable battery long life battery (3) good battery (2) : not good battery (1) s lens : nice lens (2) : (original document) When I bought this camera, I thought the battery was not good, but the problem was solved after I replaced it with new one." ></td>
	<td class="line x" title="31:218	Figure 1: An image of an outliner which uses SA output." ></td>
	<td class="line x" title="32:218	Users can refer to the original text by clicking on the document icons." ></td>
	<td class="line x" title="33:218	MT SA Japanese sentence ?parser Japanese tree structure  transfer j English tree structure Sentiment fragments ? generator ?English sentence Sentiment units Transfer patterns Fragment patterns Bilingual lexicon Polarity lexicon Figure 2: The concept of the machine translation engine and the sentiment analyzer." ></td>
	<td class="line x" title="34:218	Some components are shared between them." ></td>
	<td class="line x" title="35:218	Also other components are similar between MT and SA." ></td>
	<td class="line x" title="36:218	This means that the approach for SA should be switched from the rather shallow analysis techniques used for text mining (Hearst, 1999; Nasukawa and Nagano, 2001), where some errors can be treated as noise, into deep analysis techniques such as those used for machine translation (MT) where all of the syntactic and semantic phenomena must be handled." ></td>
	<td class="line x" title="37:218	We implemented a Japanese SA system using a Japanese to English translation engine." ></td>
	<td class="line x" title="38:218	Figure 2 illustrates our SA system, which utilizes a MT engine, where techniques for parsing and pattern matching on the tree structures are shared between MT and SA." ></td>
	<td class="line x" title="39:218	Section 2 reviews previous studies of sentiment analysis." ></td>
	<td class="line x" title="40:218	In Section 3 we define the sentiment unit to be extracted for sentiment analysis." ></td>
	<td class="line x" title="41:218	Section 4 presents the implementation of our system, comparing the operations and resources with those used for machine translation." ></td>
	<td class="line x" title="42:218	Our system is evaluated in Section 5." ></td>
	<td class="line x" title="43:218	In the rest of paper we mainly use Japanese examples because some of the operations depend on the Japanese language, but we also use English examples to express the sentiment units and some language-independent issues, for understandability." ></td>
	<td class="line nc" title="44:218	2 Previous work on Sentiment Analysis Some prior studies on sentiment analysis focused on the document-level classification of sentiment (Turney, 2002; Pang et al. , 2002) where a document is assumed to have only a single sentiment, thus these studies are not applicable to our goal." ></td>
	<td class="line x" title="45:218	Other work (Subasic and Huettner, 2001; Morinaga et al. , 2002) assigned sentiment to words, but they relied on quantitative information such as the frequencies of word associations or statistical predictions of favorability." ></td>
	<td class="line x" title="46:218	Automatic acquisition of sentiment expressions have also been studied (Hatzivassiloglou and McKeown, 1997), but limited to adjectives, and only one sentiment could be assigned to each word." ></td>
	<td class="line x" title="47:218	Yi et al.(2003) pointed out that the multiple sentiment aspects in a document should be extracted." ></td>
	<td class="line x" title="49:218	This paper follows that approach, but exploits deeper analysis in order to avoid the analytic failures reported by Nasukawa and Yi (2003), which occurred when they used a shallow parser and only addressed a limited number of syntactic phenomena." ></td>
	<td class="line x" title="50:218	In our in-depth approach described in the next section, two types of errors out of the four reported by Nasukawa and Yi (2003) were easily removed2." ></td>
	<td class="line x" title="51:218	3 Sentiment Unit This section describes the sentiment units which are extracted from text, and their roles in the sentiment analysis and its applications." ></td>
	<td class="line x" title="52:218	A sentiment unit consists of a sentiment, a predicate, its one or more arguments, and a surface form." ></td>
	<td class="line x" title="53:218	Formally it is expressed as in Figure 3." ></td>
	<td class="line x" title="54:218	The sentiment feature categorizes a sentiment unit into four types: favorable [fav], unfavorable [unf], question [qst], and request [req]." ></td>
	<td class="line x" title="55:218	A predicate is a word, typically a verb or an adjective, which conveys the main notion of the sentiment unit." ></td>
	<td class="line x" title="56:218	An argument is also a word, typically a noun, which modifies the predicate with a case postpositional in Japanese." ></td>
	<td class="line x" title="57:218	They roughly correspond to a subject and an object of the predicate in English." ></td>
	<td class="line x" title="58:218	For example, from the sentence (2)3, the extracted sentiment unit is (2a)." ></td>
	<td class="line x" title="59:218	ABC123-ha renzu-ga subarashii." ></td>
	<td class="line x" title="60:218	ABC123-TOPIC lens-NOM excellent ABC123 has an excellent lens. (2) [fav] excellent h ABC123, lens i (2a) The sentiment unit (2a) stands for the sentiment is favorable, the predicate is excellent and its arguments are ABC123 and lens." ></td>
	<td class="line x" title="61:218	In this case, both ABC123 and lens are counted as words which are associated with a favorable sentiment." ></td>
	<td class="line x" title="62:218	Arguments are used as the keywords in the outliner, as in the leftmost column in Figure 1." ></td>
	<td class="line x" title="63:218	Predicates with no argument are ignored, because they have no effects on the view and often become noise." ></td>
	<td class="line x" title="64:218	2Though this paper handles Japanese SA, we also implemented an English version of SA using English-French translation techniques, and that system solved the problems which were mentioned in Nasukawa and Yis paper." ></td>
	<td class="line x" title="65:218	3ABC123 is a fictitious product name." ></td>
	<td class="line x" title="66:218	<sentiment unit> ::= <sentiment> <predicate> <argument>+ <surface> <sentiment> ::= favorable | unfavorable | question | request <predicate> ::= <word> <feature>* <argument> ::= <word> <feature>* <surface> ::= <string> Figure 3: The definition of a sentiment unit." ></td>
	<td class="line x" title="67:218	The predicate and its arguments can be different from the surface form in the original text." ></td>
	<td class="line x" title="68:218	Semantically similar representations should be aggregated to organize extracted sentiments, so the examples in this paper use English canonical forms to represent predicates and arguments, while the actual implementation uses Japanese expressions." ></td>
	<td class="line x" title="69:218	Predicates may have features, such as negation, facility, difficulty, etc. For example, ABC123 doesnt have an excellent lens. brings a sentiment unit [unf] excellent+neg h ABC123, lens i." ></td>
	<td class="line x" title="70:218	Also the facility/difficulty feature affects the sentiments such as [unf] break+facil for easy to break and [unf] learn+diff difficult to learn." ></td>
	<td class="line x" title="71:218	The surface string is the corresponding part in the original text." ></td>
	<td class="line x" title="72:218	It is used for reference in the view of the output of SA, because the surface string is the most understandable notation of each sentiment unit for humans." ></td>
	<td class="line x" title="73:218	We use the term sentiment polarity for the selection of the two sentiments [fav] and [unf]." ></td>
	<td class="line x" title="74:218	The other two sentiments, [qst] and [req] are important in applications, e.g. the automatic creation of FAQ." ></td>
	<td class="line x" title="75:218	Roughly speaking, [qst] is extracted from an interrogative sentence, and [req] is used for imperative sentences or expressions such as I want and Id like you to ." ></td>
	<td class="line x" title="76:218	From a pragmatic point of view it is difficult to distinguish between them4, but we classify them using simple rules." ></td>
	<td class="line x" title="77:218	4 Implementation This section describes operations and resources designed for the extraction of sentiment units." ></td>
	<td class="line x" title="78:218	There are many techniques analogous to those for machine translation, so first we show the architecture of the transfer-based machine translation engine which is used as the basis of the extraction of sentiment units." ></td>
	<td class="line x" title="79:218	4.1 Transfer-based Machine Translation Engine As illustrated on the left side of Figure 2, the transfer-based machine translation system consists of three parts: a source language syntactic parser, a bilingual transfer which handles the syntactic tree structures, and a target language generator." ></td>
	<td class="line x" title="80:218	Here the flow of the Japanese to English translation is shown with the following example sentence (3)." ></td>
	<td class="line x" title="81:218	4For example, the interrogative sentence Would you read it? implies a request." ></td>
	<td class="line x" title="82:218	kare hon ki iru watashi ha wo ni no Figure 4: The Japanese syntactic tree for the sentence (3)." ></td>
	<td class="line x" title="83:218	Kare-ha watashi-no He-TOPIC I-GEN hon-wo ki-ni iru." ></td>
	<td class="line x" title="84:218	book-ACC mind-DAT enter He likes my book. (3) First the syntactic parser parses the sentence (3) to create the tree structure as shown in Figure 4." ></td>
	<td class="line x" title="85:218	Next, the transfer converts this Japanese parse tree into an English one by applying the translation patterns as in Figure 5." ></td>
	<td class="line x" title="86:218	A translation pattern consists of a tree of the source language, a tree of the target language, and the word correspondences between both languages." ></td>
	<td class="line x" title="87:218	The patterns (a) and (b) in Figure 5 match with the subtrees in Figure 4, as Figure 6 illustrates." ></td>
	<td class="line x" title="88:218	Thismatchingoperationisverycomplicatedbecause there can be an enormous number of possible combinations of patterns." ></td>
	<td class="line x" title="89:218	The fitness of the pattern combinations is calculated according to the similarity of the source tree and the left side of the translation pattern, the specificity of the translation pattern, and so on." ></td>
	<td class="line x" title="90:218	This example also shows the process of matching the Japanese case markers (postpositional particles)." ></td>
	<td class="line x" title="91:218	The source tree and the pattern (a) match even though the postpositional particles are different (ha and ga)." ></td>
	<td class="line x" title="92:218	This process may be much more complicated when a verb is transformed into special forms e.g. passive or causative." ></td>
	<td class="line x" title="93:218	Besides this there are many operations to handle syntactic and semantic phenomena, but here we take them for granted because of space constraints." ></td>
	<td class="line x" title="94:218	Now the target fragments have been created as in Figure 6, using the right side of the matched translation patterns as in Figure 5." ></td>
	<td class="line x" title="95:218	The two fragments are attached at the shared node  noun2 , and lexicalized by using the bilingual lexicon." ></td>
	<td class="line x" title="96:218	Finally the target sentence He likes my book. is generated by the target language generator." ></td>
	<td class="line x" title="97:218	iru noun noun ki ga wo ni like noun noun SUBJ OBJ (a) noun no watashi noun my (b) Figure 5: Two examples of Japanese-English translation patterns." ></td>
	<td class="line x" title="98:218	The left side and the right side are Japanese and English syntactic trees, respectively." ></td>
	<td class="line x" title="99:218	The  noun  works as a wildcard which matches with any noun." ></td>
	<td class="line x" title="100:218	Curves stand for correspondences between Japanese and English words." ></td>
	<td class="line x" title="101:218	kare hon ki iru watashi ha wo ni no (a) (b) like noun1 noun2 SUBJ OBJ noun2 my Figure 6: Transferring the Japanese tree in Figure 4 intotheEnglishtree." ></td>
	<td class="line x" title="102:218	ThepatternsinFigure5create two English fragments, and they are attached at the nodes  noun2  which share the same correspondent node in the source language tree." ></td>
	<td class="line x" title="103:218	4.2 Techniques Required for Sentiment Analysis Our aim is to extract sentiment units with high precision." ></td>
	<td class="line x" title="104:218	Moreover, the set of arguments of each predicate should be selected necessarily and sufficiently." ></td>
	<td class="line x" title="105:218	Here we show that the techniques to meet these requirements are analogous to the techniques for machine translation which have been reviewed in Section 4.1." ></td>
	<td class="line x" title="106:218	4.2.1 Full parsing and top-down tree matching Full syntactic parsing plays an important role to extract sentiments correctly, because the local structures obtained by a shallow parser are not always reliable." ></td>
	<td class="line x" title="107:218	For example, expressions such as I dont think X is good, I hope that X is good are not favorable opinions about X, even though X is good appears on the surface." ></td>
	<td class="line x" title="108:218	Therefore we use top-down pattern matching on the tree structures from the full parsing in order to find each sentiment fragment, that is potentially a part of a sentiment unit." ></td>
	<td class="line x" title="109:218	In our method, initially the top node is examined to see whether or not the node and its combination of children nodes match with one of the patterns in the pattern repository." ></td>
	<td class="line x" title="110:218	In this top-down manner, the nodes dont think and hope in the above examples are examined before X is good, and thus the above expressions wont be misunderstood to express favorable sentiments." ></td>
	<td class="line x" title="111:218	There are three types of patterns: principal patterns, auxiliary patterns, and nominal patterns." ></td>
	<td class="line x" title="112:218	Figure 7 illustrates examples of principal patterns: the noun warui ga [unf] badh noun i (c) noun iru ki wo ni [fav] likeh noun i (d) Figure 7: Examples of principal patterns." ></td>
	<td class="line x" title="113:218	declinable to omowa nai unit +neg (e) declinable monono declinable unit unit (f) Figure 8: Examples of auxiliary patterns." ></td>
	<td class="line x" title="114:218	 declinable  denotes a verb or an adjective in Japanese." ></td>
	<td class="line x" title="115:218	Note that the two unit s on the right side of (f) are not connected." ></td>
	<td class="line x" title="116:218	This means two separated sentiment units can be obtained." ></td>
	<td class="line x" title="117:218	pattern (c) converts a Japanese expression  noun ga warui to a sentiment unit [unf] bad h noun i." ></td>
	<td class="line x" title="118:218	The pattern (d) converts an expression  noun -wo ki-ni iru to a sentiment unit [fav] like h noun i, where the subject (the noun preceding the postpositional ga) is excluded from the arguments because the subject of like is usually the author, who is not the target of sentiment analysis." ></td>
	<td class="line x" title="119:218	Another type is the auxiliary pattern, which expands the scope of matching." ></td>
	<td class="line x" title="120:218	Figure 8 has two examples." ></td>
	<td class="line x" title="121:218	The pattern (e) matches with phrases such as X-wa yoi-to omowa-nai." ></td>
	<td class="line x" title="122:218	((I) dont think X is good.) and produces a sentiment unit with the negation feature." ></td>
	<td class="line x" title="123:218	When this pattern is attached to a principal pattern, its favorability is inverted." ></td>
	<td class="line x" title="124:218	The pattern (f) allows us to obtain two separate sentiment units from sentences such as Dezain-ga waruimonono, sousasei-ha yoi." ></td>
	<td class="line x" title="125:218	(The design is bad, but the usability is good.)." ></td>
	<td class="line x" title="126:218	4.2.2 Informative noun phrase The third type of pattern is a nominal pattern." ></td>
	<td class="line x" title="127:218	Figure 9 shows three examples." ></td>
	<td class="line x" title="128:218	The pattern (g) is used to avoid a formal noun (nominalizer) being an argument." ></td>
	<td class="line x" title="129:218	Using this pattern, from the sentence Kawaii no-ga suki-da." ></td>
	<td class="line x" title="130:218	((I) like pretty things), [fav] like h pretty i can be extracted instead of [fav] like h thing i." ></td>
	<td class="line x" title="131:218	The pattern (h) is used to convert a noun phrase renzu-no shitsu (quality of the lens) into just lens." ></td>
	<td class="line x" title="132:218	Due to this operation, from Sentence (4), an informative sentiment unit (4a) can be obtained instead of a less informative one (4b)." ></td>
	<td class="line x" title="133:218	Renzu-no shitsu-ga yoi." ></td>
	<td class="line x" title="134:218	lens-GEN quality-NOM good The quality of the lens is good. (4) [fav] good h lens i (4a) ? [fav] good h quality i (4b) adj no adj (g) noun no shitsu noun (h) noun noun noun noun (i) Figure 9: Examples of nominal patterns." ></td>
	<td class="line x" title="135:218	The pattern (i) is for compound nouns such as juuden jikan (recharging time)." ></td>
	<td class="line x" title="136:218	A sentiment unit long h time i is not informative, but long h recharging time i can be regarded as a [unf] sentiment." ></td>
	<td class="line x" title="137:218	4.2.3 Disambiguation of sentiment polarity Some adjectives and verbs may be used for both favorable and unfavorable predicates." ></td>
	<td class="line x" title="138:218	This variation of sentiment polarity can be disambiguated naturally in the same manner as the word sense disambiguation in machine translation." ></td>
	<td class="line x" title="139:218	The adjective takai (high) is a typical example, as in (5a) and (5b)." ></td>
	<td class="line x" title="140:218	In this case the sentiment polarity depends on the noun preceding the postpositional particle ga: favorable if the noun is kaizoudo (resolution), unfavorable if the noun is a product name." ></td>
	<td class="line x" title="141:218	The semantic category assigned to a noun holds the information used for this type of disambiguation." ></td>
	<td class="line x" title="142:218	Kaizoudo-ga takai." ></td>
	<td class="line x" title="143:218	resolution-NOM high The resolution is high. ! [fav] (5a) ABC123-ga takai." ></td>
	<td class="line x" title="144:218	ABC123-NOM high (price) ABC123 is expensive. ! [unf] (5b) 4.2.4 Aggregation of synonymous expressions In contrast to disambiguation, aggregation of synonymous expressions is important to organize extracted sentiment units." ></td>
	<td class="line x" title="145:218	If the different expressions which convey the same (or similar) meanings are aggregated into a canonical one, the frequency increases and one can easily find frequently mentioned opinions." ></td>
	<td class="line x" title="146:218	Using the translation architecture, any forms can be chosen as the predicates and arguments by adjusting the patterns and lexicons." ></td>
	<td class="line x" title="147:218	That is, monolingual word translation is done in our method." ></td>
	<td class="line x" title="148:218	4.3 Resources for Sentiment Analysis We prepared the following resources for sentiment analysis: Principal patterns: The verbal and adjectival patterns for machine translation were converted to principal patterns for sentiment analysis." ></td>
	<td class="line x" title="149:218	The left sides of the patterns are compatible with the source language parts of the original patterns, so we just assigned a sentiment polarity to each word." ></td>
	<td class="line x" title="150:218	A total of 3752 principal patterns were created." ></td>
	<td class="line x" title="151:218	Auxiliary/Nominal patterns: A total of 95 auxiliary patterns and 36 nominal patterns were created manually." ></td>
	<td class="line x" title="152:218	Polarity lexicon: Some nouns were assigned sentiment polarity, e.g. [unf] for noise." ></td>
	<td class="line x" title="153:218	This polarity is used in expressions such as  ga ooi." ></td>
	<td class="line x" title="154:218	(There are many )." ></td>
	<td class="line x" title="155:218	This lexicon is also used for the aggregation of words." ></td>
	<td class="line x" title="156:218	Some patterns and lexicons are domaindependent." ></td>
	<td class="line x" title="157:218	The situation is the same as in machine translation." ></td>
	<td class="line x" title="158:218	Fortunately the translation engine used here has a function to selectively use domain-dependent dictionaries, and thus we can prepare patterns which are especially suited for the messages on bulletin boards, or for the domain of digital cameras." ></td>
	<td class="line x" title="159:218	For example, The size is small. is a desirable feature of a digital camera." ></td>
	<td class="line x" title="160:218	We can assign the appropriate sentiment (in this case, [fav]) by using a domain-specific principal pattern." ></td>
	<td class="line x" title="161:218	5 Evaluation We conducted two experiments on the extraction of sentiment units from bulletin boards on the WWW that are discussing digital cameras." ></td>
	<td class="line x" title="162:218	A total of 200 randomly selected sentences were analyzed by our system." ></td>
	<td class="line x" title="163:218	The resources were created by looking at other parts of the same domain texts, and therefore this experiment is an open test." ></td>
	<td class="line x" title="164:218	Experiment 1 measured the precision of the sentiment polarity, and Experiment 2 evaluated the informativeness of the sentiment units." ></td>
	<td class="line x" title="165:218	In this section we handled only the sentiments [fav] and [unf] sentiments, thus the other two sentiments [qst] and [req] were not evaluated." ></td>
	<td class="line x" title="166:218	5.1 Experiment 1: Precision and Recall In order to see the reliability of the extracted sentiment polarities, we evaluated the following three metrics: Weak precision: The coincidence rate of the sentiment polarity between the systems output and manual output when both the system and the human evaluators assigned either a favorable or unfavorable sentiment." ></td>
	<td class="line x" title="167:218	Strong precision: The coincidence rate of the sentiment polarity between the systems output and manual output when the system assigned either a favorable or unfavorable sentiment." ></td>
	<td class="line x" title="168:218	Recall: The detection rate of sentiment units within the manual output." ></td>
	<td class="line x" title="169:218	These metrics are measured by using two methods: (A) our proposed method based on the machine translation engine, and (B) the lexicon-only method, which emulates the shallow parsing approach." ></td>
	<td class="line x" title="170:218	The latter method used the simple polarity lexicon of adjectives and verbs, where an adjective or a verb had only one sentiment polarity, then no disambiguation was done." ></td>
	<td class="line x" title="171:218	Except for the direct negation of (A) MT (B) Lexicon only Weak prec." ></td>
	<td class="line x" title="172:218	100% (31/31) 80% (41/51) Strong prec." ></td>
	<td class="line x" title="173:218	89% (31/35) 44% (41/93) Recall 43% (31/72) 57% (41/72) Table 1: Precision and recall for the extraction of sentiment units from 200 sentences." ></td>
	<td class="line x" title="174:218	(A) MT Manual f n u f 20 3 0 System n 27 14 u 0 1 11 (B) Lexicon only Manual f n u f 26 19 6 System n 14 7 u 4 23 15 Table 2: The breakdown of the results of Experiment 1." ></td>
	<td class="line x" title="175:218	The columns and rows show the manual output and the system output, respectively (f: favorable, n: non-sentiment, u: unfavorable)." ></td>
	<td class="line x" title="176:218	The sum of the bold numbers equals the numerators of the precision and recall." ></td>
	<td class="line x" title="177:218	an adjective or a verb5, no translation patterns were used." ></td>
	<td class="line x" title="178:218	Instead of the top-down pattern matching, sentiment units were extracted from any part of the tree structures (the results of full-parsing were used also here)." ></td>
	<td class="line x" title="179:218	Table 1 shows the results." ></td>
	<td class="line x" title="180:218	With the MT framework, the weak precision was perfect, and also the strong precision was much higher, while the recall was lower than for the lexicon-only method." ></td>
	<td class="line x" title="181:218	Their breakdowns in the two parts of Table 2 indicate that most of errors where the system wrongly assigned either of sentiments (i.e. human regarded an expression as non-sentiment) have been reduced with the MT framework." ></td>
	<td class="line x" title="182:218	All of the above results are consistent with intuition." ></td>
	<td class="line x" title="183:218	The MT method outputs a sentiment unit only when the expression is reachable from the root node of the syntactic tree through the combination of sentiment fragments, while the lexicon-only method picks up sentiment units from any node in the syntactic tree." ></td>
	<td class="line x" title="184:218	The sentence (6) is an example where the lexicon-only method output the wrong sentiment unit (6a)." ></td>
	<td class="line x" title="185:218	The MT method did not output this sentiment unit, thus the precision values of the MT method did not suffer from this example." ></td>
	<td class="line x" title="186:218	gashitsu-ga kirei-da-to iu hyouka-ha uke-masen-deshi-ta." ></td>
	<td class="line x" title="187:218	(6) There was no opinion that the picture was sharp.  [fav] clear h picture i (6a) In the lexicon-only method, some errors occurred due to the ambiguity in sentiment polarity of an adjective or a verb, e.g. Kanousei-ga takai." ></td>
	<td class="line x" title="188:218	(Capabilities are high.) since takai (high/expensive) is always assigned the [unf] feature." ></td>
	<td class="line x" title="189:218	5He doesnt like it. is regarded as negation, but I dont think it is good. is not." ></td>
	<td class="line x" title="190:218	declinable noun noun noun ga wo ni Figure 10: A nave predicate-argument structure used by the system (C)." ></td>
	<td class="line x" title="191:218	Nouns preceding three major postpositional particles ga, wo, and ni are supported as the slots of arguments." ></td>
	<td class="line x" title="192:218	On the other hand, in the system (A), there are over 3,000 principal patterns that have information on appropriate combinations for each verb and adjective." ></td>
	<td class="line x" title="193:218	(A) MT (C) Nave Less redundant 2/35 0/35 More informative 13/35 1/35 Both 1/35 0/35 Table 3: Comparison of scope of sentiment units." ></td>
	<td class="line x" title="194:218	The numbers mean the counts of the better output for each system among 35 sentiment units." ></td>
	<td class="line x" title="195:218	The remainder is the outputs that were the same in both systems." ></td>
	<td class="line x" title="196:218	The recall was not so high, especially in the MT method, but according to our error analysis the recall can be increased by adding auxiliary patterns." ></td>
	<td class="line x" title="197:218	Ontheotherhand, itisalmostimpossibletoincrease the precision without our deep analysis techniques." ></td>
	<td class="line x" title="198:218	Consequently, ourproposedmethodoutperformsthe shallow (lexicon-only) approach." ></td>
	<td class="line x" title="199:218	5.2 Experiment 2: Scope of Sentiment Unit We also compared the appropriateness of the scope of the extracted sentiment units between (A) the proposed method with the MT framework and (C) a method that supports only nave predicateargument structures as in Figure 10 and doesnt use any nominal patterns." ></td>
	<td class="line x" title="200:218	According to the results shown in Table 3, the MT method produced less redundant or more informative sentiment units than did relying on the nave predicate-argument structures in about half of the cases among the 35 extracted sentiment units." ></td>
	<td class="line x" title="201:218	The following example (7) is a case where the sentiment unit output by the MT method (7a) was less redundant than that output by the nave method (7b)." ></td>
	<td class="line x" title="202:218	The translation engine understood that the phrase kyonen-no 5-gatsu-ni (last May) held temporal information, therefore it was excluded from the arguments of the predicate enhance, while both functionandMayweretheargumentsofenhance in (7b)." ></td>
	<td class="line x" title="203:218	Apparently the argument May is not necessary here." ></td>
	<td class="line x" title="204:218	kyonen-no 5-gatsu-ni kinou-ga kairyou-sare-ta you-desu." ></td>
	<td class="line x" title="205:218	(7) It seems the function was enhanced last May. [fav] enhance h function i (7a) ? [fav] enhance h function, May i (7b) Example (8) is another case where the sentiment unit output by the MT method (8a) was more informative than that output by the nave method (8b)." ></td>
	<td class="line x" title="206:218	Than the Japanese functional noun hou, its modifier zoom was more informative." ></td>
	<td class="line x" title="207:218	The MT method successfully selected the noun zoom as the argument of desirable." ></td>
	<td class="line x" title="208:218	zuum-no hou-ga nozomashii." ></td>
	<td class="line x" title="209:218	(8) A zoom is more desirable. [fav] desirable h zoom i (8a) ? [fav] desirable h hou i (8b) The only one case we encountered where the MT method extracted a less informative sentiment unit was the sentence Botan-ga satsuei-ni pittaridesu (The shutter is suitable for taking photos)." ></td>
	<td class="line x" title="210:218	The nave method could produce the sentiment unit [fav] suitable h shutter, photo i, but the MT method created [fav] suitable h shutter i." ></td>
	<td class="line x" title="211:218	This is due to the lack of a noun phrase preceding the postpositional particle ni in the principal pattern." ></td>
	<td class="line x" title="212:218	Such problems can be avoided by modifying the patterns, and thus the effect of the combination of patterns for SA has been shown here." ></td>
	<td class="line x" title="213:218	6 Conclusion This paper has proposed a new approach to sentiment analysis: the translation from text to a set of semantic fragments." ></td>
	<td class="line x" title="214:218	We have shown that the deep syntactic and semantic analysis makes possible the reliable extraction of sentiment units, and the outlining of sentiments became useful because of the aggregation of the variations in expressions, and the informative outputs of the arguments." ></td>
	<td class="line x" title="215:218	The experimental results have shown that the precision of the sentiment polarity was much higher than for the conventional methods, and the sentiment units created by our system were less redundant and more informative than when using nave predicate-argument structures." ></td>
	<td class="line x" title="216:218	Even though we exploited many advantages of deep analysis, we could create a sentiment analysis system at a very low development cost, becausemanyofthetechniquesformachinetranslation can be reused naturally when we regard the extraction of sentiment units as a kind of translation." ></td>
	<td class="line x" title="217:218	Many techniques which have been studied for the purpose of machine translation, such as word sense disambiguation (Dagan and Itai, 1994; Yarowsky, 1995), anaphora resolution (Mitamura et al. , 2002), and automatic pattern extraction from corpora (Watanabe et al. , 2003), can accelerate the further enhancement of sentiment analysis, or other NLP tasks." ></td>
	<td class="line x" title="218:218	Therefore this work is the first step towards the integration of shallow and wide NLP, with deep NLP." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C04-1200
Determining The Sentiment Of Opinions
Kim, Soo-Min;Hovy, Eduard H.;"></td>
	<td class="line x" title="1:153	Determining the Sentiment of Opinions Soo-Min Kim Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292-6695 skim@isi.edu Eduard Hovy Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292-6695 hovy@isi.edu Abstract Identifying sentiments (the affective parts of opinions) is a challenging problem." ></td>
	<td class="line x" title="2:153	We present a system that, given a topic, automatically finds the people who hold opinions about that topic and the sentiment of each opinion." ></td>
	<td class="line x" title="3:153	The system contains a module for determining word sentiment and another for combining sentiments within a sentence." ></td>
	<td class="line x" title="4:153	We experiment with various models of classifying and combining sentiment at word and sentence levels, with promising results." ></td>
	<td class="line x" title="5:153	1 Introduction What is an opinion?" ></td>
	<td class="line x" title="6:153	The many opinions on opinions are reflected in a considerable literature (Aristotle 1954; Perelman 1970; Toulmin et al. 1979; Wallace 1975; Toulmin 2003)." ></td>
	<td class="line oc" title="7:153	Recent computational work either focuses on sentence subjectivity (Wiebe et al. 2002; Riloff et al. 2003), concentrates just on explicit statements of evaluation, such as of films (Turney 2002; Pang et al. 2002), or focuses on just one aspect of opinion, e.g., (Hatzivassiloglou and McKeown 1997) on adjectives." ></td>
	<td class="line x" title="8:153	We wish to study opinion in general; our work most closely resembles that of (Yu and Hatzivassiloglou 2003)." ></td>
	<td class="line x" title="9:153	Since an analytic definition of opinion is probably impossible anyway, we will not summarize past discussion or try to define formally what is and what is not an opinion." ></td>
	<td class="line x" title="10:153	For our purposes, we describe an opinion as a quadruple [Topic, Holder, Claim, Sentiment] in which the Holder believes a Claim about the Topic, and in many cases associates a Sentiment, such as good or bad, with the belief." ></td>
	<td class="line x" title="11:153	For example, the following opinions contain Claims but no Sentiments: I believe the world is flat The Gap is likely to go bankrupt Bin Laden is hiding in Pakistan Water always flushes anti-clockwise in the southern hemisphere Like Yu and Hatzivassiloglou (2003), we want to automatically identify Sentiments, which in this work we define as an explicit or implicit expression in text of the Holders positive, negative, or neutral regard toward the Claim about the Topic." ></td>
	<td class="line x" title="12:153	(Other sentiments we plan to study later)." ></td>
	<td class="line x" title="13:153	Sentiments always involve the Holders emotions or desires, and may be present explicitly or only implicitly: I think that attacking Iraq would put the US in a difficult position (implicit) The US attack on Iraq is wrong (explicit) I like Ike (explicit) We should decrease our dependence on oil (implicit) Reps. Tom Petri and William F. Goodling asserted that counting illegal aliens violates citizens basic right to equal representation (implicit) In this paper we address the following challenge problem." ></td>
	<td class="line x" title="14:153	Given a Topic (e.g. , Should abortion be banned?) and a set of texts about the topic, find the Sentiments expressed about (claims about) the Topic (but not its supporting subtopics) in each text, and identify the people who hold each sentiment." ></td>
	<td class="line x" title="15:153	To avoid the problem of differentiating between shades of sentiments, we simplify the problem to: identify just expressions of positive, negative, or neutral sentiments, together with their holders." ></td>
	<td class="line x" title="16:153	In addition, for sentences that do not express a sentiment but simply state that some sentiment(s) exist(s), return these sentences in a separate set." ></td>
	<td class="line x" title="17:153	For example, given the topic What should be done with Medicare? the sentence After years of empty promises, Congress has rolled out two Medicare prescription plans, one from House Republicans and the other from the Democratic Sentence POS Tagger verbs nounsAdjectives Adjective Sentiment classifier sentiment sentiment Sentence sentiment classifier Opinion region + polarity + holder Holder finder Named Entity Tagger Sentence Sentence texts + topic sentiment sentiment sentiment Verbs Verb Sentiment classifier Nouns Noun Sentiment classifier WordNet Sentence : Figure 1: System architecture." ></td>
	<td class="line x" title="18:153	Sens. Bob Graham of Florida and Zell Miller of Georgia should be returned in the separate set." ></td>
	<td class="line x" title="19:153	We approach the problem in stages, starting with words and moving on to sentences." ></td>
	<td class="line x" title="20:153	We take as unit sentiment carrier a single word, and first classify each adjective, verb, and noun by its sentiment." ></td>
	<td class="line x" title="21:153	We experimented with several classifier models." ></td>
	<td class="line x" title="22:153	But combining sentiments requires additional care, as Table 1 shows." ></td>
	<td class="line x" title="23:153	California Supreme Court agreed that the states new term-limit law was constitutional." ></td>
	<td class="line x" title="24:153	California Supreme Court disagreed that the states new term-limit law was constitutional." ></td>
	<td class="line x" title="25:153	California Supreme Court agreed that the states new term-limit law was unconstitutional." ></td>
	<td class="line x" title="26:153	California Supreme Court disagreed that the states new term-limit law was unconstitutional." ></td>
	<td class="line x" title="27:153	Table 1: Combining sentiments." ></td>
	<td class="line x" title="28:153	A sentence might even express opinions of different people." ></td>
	<td class="line x" title="29:153	When combining word-level sentiments, we therefore first determine for each Holder a relevant region within the sentence and then experiment with various models for combining word sentiments." ></td>
	<td class="line x" title="30:153	We describe our models and algorithm in Section 2, system experiments and discussion in Section 3, and conclude in Section 4." ></td>
	<td class="line x" title="31:153	2 Algorithm Given a topic and a set of texts, the system operates in four steps." ></td>
	<td class="line x" title="32:153	First it selects sentences that contain both the topic phrase and holder candidates." ></td>
	<td class="line x" title="33:153	Next, the holder-based regions of opinion are delimited." ></td>
	<td class="line x" title="34:153	Then the sentence sentiment classifier calculates the polarity of all sentiment-bearing words individually." ></td>
	<td class="line x" title="35:153	Finally, the system combines them to produce the holders sentiment for the whole sentence." ></td>
	<td class="line x" title="36:153	Figure 1 shows the overall system architecture." ></td>
	<td class="line x" title="37:153	Section 2.1 describes the word sentiment classifier and Section 2.2 describes the sentence sentiment classifier." ></td>
	<td class="line x" title="38:153	2.1 Word Sentiment Classifier 2.1.1 Word Classification Models For word sentiment classification we developed two models." ></td>
	<td class="line x" title="39:153	The basic approach is to assemble a small amount of seed words by hand, sorted by polarity into two listspositive and negativeand then to grow this by adding words obtained from WordNet (Miller et al. 1993; Fellbaum et al. 1993)." ></td>
	<td class="line x" title="40:153	We assume synonyms of positive words are mostly positive and antonyms mostly negative, e.g., the positive word good has synonyms virtuous, honorable, righteous and antonyms evil, disreputable, unrighteous." ></td>
	<td class="line x" title="41:153	Antonyms of negative words are added to the positive list, and synonyms to the negative one." ></td>
	<td class="line x" title="42:153	To start the seed lists we selected verbs (23 positive and 21 negative) and adjectives (15 positive and 19 negative), adding nouns later." ></td>
	<td class="line x" title="43:153	Since adjectives and verbs are structured differently in WordNet, we obtained from it synonyms and antonyms for adjectives but only synonyms for verbs." ></td>
	<td class="line x" title="44:153	For each seed word, we extracted from WordNet its expansions and added them back into the appropriate seed lists." ></td>
	<td class="line x" title="45:153	Using these expanded lists, we extracted an additional cycle of words from WordNet, to obtain finally 5880 positive adjectives, 6233 negative adjectives, 2840 positive verbs, and 3239 negative verbs." ></td>
	<td class="line x" title="46:153	However, not all synonyms and antonyms could be used: some had opposite sentiment or were neutral." ></td>
	<td class="line x" title="47:153	In addition, some common words such as great, strong, take, and get occurred many times in both positive and negative categories." ></td>
	<td class="line x" title="48:153	This indicated the need to develop a measure of strength of sentiment polarity (the alternative was simply to discard such ambiguous words)to determine how strongly a word is positive and also how strongly it is negative." ></td>
	<td class="line x" title="49:153	This would enable us to discard sentiment-ambiguous words but retain those with strengths over some threshold." ></td>
	<td class="line x" title="50:153	Armed with such a measure, we can also assign strength of sentiment polarity to as yet unseen words." ></td>
	<td class="line x" title="51:153	Given a new word, we use WordNet again to obtain a synonym set of the unseen word to determine how it interacts with our sentiment seed lists." ></td>
	<td class="line x" title="52:153	That is, we compute (1) ),|(maxarg )|(maxarg 21 n c c synsynsyncP wcP  where c is a sentiment category (positive or negative), w is the unseen word, and syn n are the WordNet synonyms of w. To compute Equation (1), we tried two different models: (2) )|()(maxarg )|()(maxarg )|()(maxarg)|(maxarg 1 ))(,(3 2 1  = = = = m k wsynsetfcount k c n c cc k cfPcP csynsynsynsynPcP cwPcPwcP where f k is the k th feature (list word) of sentiment class c which is also a member of the synonym set of w, and count(f k,synset(w)) is the total number of occurrences of f k in the synonym set of w. P(c) is the number of words in class c divided by the total number of words considered." ></td>
	<td class="line x" title="53:153	This model derives from document classification." ></td>
	<td class="line x" title="54:153	We used the synonym and antonym lists obtained from Wordnet instead of learning word sets from a corpus, since the former is simpler and does not require manually annotated data for training." ></td>
	<td class="line x" title="55:153	Equation (3) shows the second model for a word sentiment classifier." ></td>
	<td class="line x" title="56:153	(3) )( ),( )(maxarg )|()(maxarg)|(maxarg 1 ccount csyncount cP cwPcPwcP n i i c cc  = = = To compute the probability P(w|c) of word w given a sentiment class c, we count the occurrence of ws synonyms in the list of c. The intuition is that the more synonyms occuring in c, the more likely the word belongs." ></td>
	<td class="line x" title="57:153	We computed both positive and negative sentiment strengths for each word and compared their relative magnitudes." ></td>
	<td class="line x" title="58:153	Table 2 shows several examples of the system output, computed with Equation (2), in which + represents positive category strength and - negative." ></td>
	<td class="line x" title="59:153	The word amusing, for example, was classified as carrying primarily positive sentiment, and blame as primarily negative." ></td>
	<td class="line x" title="60:153	The absolute value of each category represents the strength of its sentiment polarity." ></td>
	<td class="line x" title="61:153	For instance, afraid with strength -0.99 represents strong negavitity while abysmal with strength -0.61 represents weaker negativity." ></td>
	<td class="line x" title="62:153	abysmal : NEGATIVE [+ : 0.3811][: 0.6188] adequate : POSITIVE [+ : 0.9999][: 0.0484e-11] afraid : NEGATIVE [+ : 0.0212e-04][: 0.9999] ailing : NEGATIVE [+ : 0.0467e-8][: 0.9999] amusing : POSITIVE [+ : 0.9999][: 0.0593e-07] answerable : POSITIVE [+ : 0.8655][: 0.1344] apprehensible: POSITIVE [+ : 0.9999][: 0.0227e-07] averse : NEGATIVE [+ : 0.0454e-05][: 0.9999] blame : NEGATIVE [+ : 0.2530][: 0.7469] Table 2: Sample output of word sentiment classifier." ></td>
	<td class="line x" title="63:153	2.2 Sentence Sentiment Classifier As shows in Table 1, combining sentiments in a sentence can be tricky." ></td>
	<td class="line x" title="64:153	We are interested in the sentiments of the Holder about the Claim." ></td>
	<td class="line x" title="65:153	Manual analysis showed that such sentiments can be found most reliably close to the Holder; without either Holder or Topic/Claim nearby as anchor points, even humans sometimes have trouble reliably determining the source of a sentiment." ></td>
	<td class="line x" title="66:153	We therefore included in the algorithm steps to identify the Topic (through direct matching, since we took it as given) and any likely opinion Holders (see Section 2.2.1)." ></td>
	<td class="line x" title="67:153	Near each Holder we then identified a region in which sentiments would be considered; any sentiments outside such a region we take to be of undetermined origin and ignore (Section 2.2.2)." ></td>
	<td class="line x" title="68:153	We then defined several models for combining the sentiments expressed within a region (Section 2.2.3)." ></td>
	<td class="line x" title="69:153	2.2.1 Holder Identification We used BBNs named entity tagger IdentiFinder to identify potential holders of an opinion." ></td>
	<td class="line x" title="70:153	We considered PERSON and ORGANIZATION as the only possible opinion holders." ></td>
	<td class="line x" title="71:153	For sentences with more than one Holder, we chose the one closest to the Topic phrase, for simplicity." ></td>
	<td class="line x" title="72:153	This is a very crude step." ></td>
	<td class="line x" title="73:153	A more sophisticated approach would employ a parser to identify syntactic relationships between each Holder and all dependent expressions of sentiment." ></td>
	<td class="line x" title="74:153	2.2.2 Sentiment Region Lacking a parse of the sentence, we were faced with a dilemma: How large should a region be?" ></td>
	<td class="line x" title="75:153	We therefore defined the sentiment region in various ways (see Table 3) and experimented with their effectiveness, as reported in Section 3." ></td>
	<td class="line x" title="76:153	Window1: full sentence Window2: words between Holder and Topic Window3: window2  2 words Window4: window2 to the end of sentence Table 3: Four variations of region size." ></td>
	<td class="line x" title="77:153	2.2.3 Classification Models We built three models to assign a sentiment category to a given sentence, each combining the individual sentiments of sentiment-bearing words, as described above, in a different way." ></td>
	<td class="line x" title="78:153	Model 0 simply considers the polarities of the sentiments, not the strengths: Model 0:  (signs in region) The intuition here is something like negatives cancel one another out." ></td>
	<td class="line x" title="79:153	Here the system assigns the same sentiment to both the California Supreme Court agreed that the states new term-limit law was constitutional and the California Supreme Court disagreed that the states new term-limit law was unconstitutional." ></td>
	<td class="line x" title="80:153	For this model, we also included negation words such as not and never to reverse the sentiment polarity." ></td>
	<td class="line x" title="81:153	Model 1 is the harmonic mean (average) of the sentiment strengths in the region: Model 1: cwcp wcp cn scP ij n i i = =  = )|(argmax if,)|( )( 1 )|( j 1 Here n(c) is the number of words in the region whose sentiment category is c. If a region contains more and stronger positive than negative words, the sentiment will be positive." ></td>
	<td class="line x" title="82:153	Model 2 is the geometric mean: Model 2: cwcpif wcpscP ij n i i cn = =  =  )|(argmax,)|(10)|( j 1 1)( 2.2.4 Examples The following are two example outputs." ></td>
	<td class="line x" title="83:153	Public officials throughout California have condemned a U.S. Senate vote Thursday to exclude illegal aliens from the 1990 census, saying the action will shortchange California in Congress and possibly deprive the state of millions of dollars of federal aid for medical emergency services and other programs for poor people." ></td>
	<td class="line x" title="84:153	TOPIC : illegal alien HOLDER : U.S. Senate OPINION REGION: vote/NN Thursday/NNP to/TO exclude/VB illegal/JJ aliens/NNS from/IN the/DT 1990/CD census,/NN SENTIMENT_POLARITY: negative For that reason and others, the Constitutional Convention unanimously rejected term limits and the First Congress soundly defeated two subsequent term-limit proposals." ></td>
	<td class="line x" title="85:153	TOPIC : term limit HOLDER : First Congress OPINION REGION: soundly/RB defeated/VBD two/CD subsequent/JJ term-limit/JJ proposals./NN SENTIMENT_POLARITY: negative 3 Experiments The first experiment examines the two word sentiment classifier models and the second the three sentence sentiment classifier models." ></td>
	<td class="line x" title="86:153	3.1 Word Sentiment Classifier For test material, we asked three humans to classify data." ></td>
	<td class="line x" title="87:153	We started with a basic English word list for foreign students preparing for the TOEFL test and intersected it with an adjective list containing 19748 English adjectives and a verb list of 8011 verbs to obtain common adjectives and verbs." ></td>
	<td class="line x" title="88:153	From this we randomly selected 462 adjectives and 502 verbs for human classification." ></td>
	<td class="line x" title="89:153	Human1 and human2 each classified 462 adjectives, and human2 and human3 502 verbs." ></td>
	<td class="line x" title="90:153	The classification task is defined as assigning each word to one of three categories: positive, negative, and neutral." ></td>
	<td class="line x" title="91:153	3.1.1 HumanHuman Agreement Adjectives Verbs Human1 : Human2 Human1 : Human3 Strict 76.19% 62.35% Lenient 88.96% 85.06% Table 4: Inter-human classification agreement." ></td>
	<td class="line x" title="92:153	Table 4 shows inter-human agreement." ></td>
	<td class="line x" title="93:153	The strict measure is defined over all three categories, whereas the lenient measure is taken over only two categories, where positive and neutral have been merged, should we choose to focus only on differentiating words of negative sentiment." ></td>
	<td class="line x" title="94:153	3.1.2 HumanMachine Agreement Table 5 shows results, using Equation (2) of Section 2.1.1, compared against a baseline that randomly assigns a sentiment category to each word (averaged over 10 iterations)." ></td>
	<td class="line x" title="95:153	The system achieves lower agreement than humans but higher than the random process." ></td>
	<td class="line x" title="96:153	Of the test data, the algorithm classified 93.07% of adjectives and 83.27% of verbs as either positive and negative." ></td>
	<td class="line x" title="97:153	The remainder of adjectives and verbs failed to be classified, since they did not overlap with the synonym set of adjectives and verbs." ></td>
	<td class="line x" title="98:153	In Table 5, the seed list included just a few manually selected seed words (23 positive and 21 negative verbs and 15 and 19 adjectives, repectively)." ></td>
	<td class="line x" title="99:153	We decided to investigate the effect of more seed words." ></td>
	<td class="line x" title="100:153	After collecting the annotated data, we added half of it (231 adjectives and 251 verbs) to the training set, retaining the other half for the test." ></td>
	<td class="line x" title="101:153	As Table 6 shows, agreement of both adjectives and verbs with humans improves." ></td>
	<td class="line x" title="102:153	Recall is also improved." ></td>
	<td class="line x" title="103:153	Adjective (Train: 231 Test : 231) Verb (Train: 251 Test : 251) Lenient agreement Lenient agreement H1:M H2:M recall H1:M H3:M recall 75.66% 77.88% 97.84% 81.20% 79.06% 93.23% Table 6: Results including manual data." ></td>
	<td class="line x" title="104:153	3.2 Sentence Sentiment Classifier 3.2.1 Data 100 sentences were selected from the DUC 2001 corpus with the topics illegal alien, term limits, gun control, and NAFTA." ></td>
	<td class="line x" title="105:153	Two humans annotated the 100 sentences with three categories (positive, negative, and N/A)." ></td>
	<td class="line x" title="106:153	To measure the agreement between humans, we used the Kappa statistic (Siegel and Castellan Jr. 1988)." ></td>
	<td class="line x" title="107:153	The Kappa value for the annotation task of 100 sentences was 0.91, which is considered to be reliable." ></td>
	<td class="line x" title="108:153	3.2.2 Test on Human Annotated Data We experimented on Section 2.2.3s 3 models of sentiment classifiers, using the 4 different window definitions and 4 variations of word-level classifiers (the two word sentiment equations introduced in Section 2.1.1, first with and then without normalization, to compare performance)." ></td>
	<td class="line x" title="109:153	Since Model 0 considers not probabilities of words but only their polarities, the two wordlevel classifier equations yield the same results." ></td>
	<td class="line x" title="110:153	Consequently, Model 0 has 8 combinations and Models 1 and 2 have 16 each." ></td>
	<td class="line x" title="111:153	To test the identification of opinion Holder, we first ran models with holders that were annotated by humans then ran the same models with the automatic holder finding strategies." ></td>
	<td class="line x" title="112:153	The results appear in Figures 2 and 3." ></td>
	<td class="line x" title="113:153	The models are numbered as follows: m0 through m4 represent 4 sentence classifier models, Table 5." ></td>
	<td class="line x" title="114:153	Agreement between humans and system." ></td>
	<td class="line x" title="115:153	Adjective (test: 231 adjectives) Verb (test : 251 verbs) Lenient agreement Lenient agreement H1:M H2:M recall H1:M H3:M recall Random selection (average of 10 iterations) 59.35% 57.81% 100% 59.02% 56.59% 100% Basic method 68.37% 68.60% 93.07% 75.84% 72.72% 83.27% p1/p2 and p3/p4 represent the word classifier models in Equation (2) and Equation (3) with normalization and without normalization respectively." ></td>
	<td class="line x" title="116:153	0.3 0.4 0.5 0.6 0.7 0.8 0.9 m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4 ac cu r acy Window1 Window2 Window3 Window4 0.3 0.4 0.5 0.6 0.7 0.8 0.9 m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4 ac cu r a c y Window 1 Window 2 Window 3 Window 4 Human 1 : Machine Human 2 : Machine Figure 2: Results with manually annotated Holder." ></td>
	<td class="line x" title="117:153	0.3 0.4 0.5 0.6 0.7 0.8 0.9 m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4 accu r acy Window 1 Window 2 Window 3 Window 4 0.3 0.4 0.5 0.6 0.7 0.8 0.9 m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4 accu r acy Window 1 Window 2 Window 3 Window 4 Human 1 : Machine Human 2 : Machi ne Figure 3: Results with automatic Holder detection." ></td>
	<td class="line x" title="118:153	Correctness of an opinion is determined when the system finds both a correct holder and the appropriate sentiment within the sentence." ></td>
	<td class="line x" title="119:153	Since human1 classified 33 sentences positive and 33 negative, random classification gives 33 out of 66 sentences." ></td>
	<td class="line x" title="120:153	Similarly, since human2 classified 29 positive and 34 negative, random classification gives 34 out of 63 when the system blindly marks all sentences as negative and 29 out of 63 when it marks all as positive." ></td>
	<td class="line x" title="121:153	The systems best model performed at 81% accuracy with the manually provided holder and at 67% accuracy with automatic holder detection." ></td>
	<td class="line x" title="122:153	3.3 Problems 3.3.1 Word Sentiment Classification As mentioned, some words have both strong positive and negative sentiment." ></td>
	<td class="line x" title="123:153	For these words, it is difficult to pick one sentiment category without considering context." ></td>
	<td class="line x" title="124:153	Second, a unigram model is not sufficient: common words without much sentiment alone can combine to produce reliable sentiment." ></td>
	<td class="line x" title="125:153	For example, in Term limits really hit at democracy, says Prof. Fenno, the common and multi-meaning word hit was used to express a negative point of view about term limits." ></td>
	<td class="line x" title="126:153	If such combinations occur adjacently, we can use bigrams or trigrams in the seed word list." ></td>
	<td class="line x" title="127:153	When they occur at a distance, however, it is more difficult to identify the sentiment correctly, especially if one of the words falls outside the sentiment region." ></td>
	<td class="line x" title="128:153	3.3.2 Sentence Sentiment Classification Even in a single sentence, a holder might express two different opinions." ></td>
	<td class="line x" title="129:153	Our system only detects the closest one." ></td>
	<td class="line x" title="130:153	Another difficult problem is that the models cannot infer sentiments from facts in a sentence." ></td>
	<td class="line x" title="131:153	She thinks term limits will give women more opportunities in politics expresses a positive opinion about term limits but the absence of adjective, verb, and noun sentiment-words prevents a classification." ></td>
	<td class="line x" title="132:153	Although relatively easy task for people, detecting an opinion holder is not simple either." ></td>
	<td class="line x" title="133:153	As a result, our system sometimes picks a wrong holder when there are multiple plausible opinion holder candidates present." ></td>
	<td class="line x" title="134:153	Employing a parser to delimit opinion regions and more accurately associate them with potential holders should help." ></td>
	<td class="line x" title="135:153	3.4 Discussion Which combination of models is best?" ></td>
	<td class="line x" title="136:153	The best overall performance is provided by Model 0." ></td>
	<td class="line x" title="137:153	Apparently, the mere presence of negative words is more important than sentiment strength." ></td>
	<td class="line x" title="138:153	For manually tagged holder and topic, Model 0 has the highest single performance, though Model 1 averages best." ></td>
	<td class="line x" title="139:153	Which is better, a sentence or a region?" ></td>
	<td class="line x" title="140:153	With manually identified topic and holder, the region window4 (from Holder to sentence end) performs better than other regions." ></td>
	<td class="line x" title="141:153	How do scores differ from manual to automatic holder identification?" ></td>
	<td class="line x" title="142:153	Table 7 compares the average results with automatic holder identification to manually annotated holders in 40 different models." ></td>
	<td class="line x" title="143:153	Around 7 more sentences (around 11%) were misclassified by the automatic detection method." ></td>
	<td class="line x" title="144:153	positive negative total Human1 5.394 1.667 7.060 Human2 4.984 1.714 6.698 Table 7: Average difference between manual and automatic holder detection." ></td>
	<td class="line x" title="145:153	How does adding the neutral sentiment as a separate category affect the score?" ></td>
	<td class="line x" title="146:153	It is very confusing even for humans to distinguish between a neutral opinion and nonopinion bearing sentences." ></td>
	<td class="line x" title="147:153	In previous research, we built a sentence subjectivity classifier." ></td>
	<td class="line x" title="148:153	Unfortunately, in most cases it classifies neutral and weak sentiment sentences as non-opinion bearing sentences." ></td>
	<td class="line x" title="149:153	4 Conclusion Sentiment recognition is a challenging and difficult part of understanding opinions." ></td>
	<td class="line x" title="150:153	We plan to extend our work to more difficult cases such as sentences with weak-opinion-bearing words or sentences with multiple opinions about a topic." ></td>
	<td class="line x" title="151:153	To improve identification of the Holder, we plan to use a parser to associate regions more reliably with holders." ></td>
	<td class="line x" title="152:153	We plan to explore other learning techniques, such as decision lists or SVMs." ></td>
	<td class="line x" title="153:153	Nonetheless, as the experiments show, encouraging results can be obtained even with relatively simple models and only a small amount of manual seeding effort." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="J04-3002
Learning Subjective Language
Wiebe, Janyce M.;Wilson, Theresa;Bruce, Rebecca F.;Bell, Matthew;Martin, Melanie J.;"></td>
	<td class="line x" title="1:617	c 2004 Association for Computational Linguistics Learning Subjective Language Janyce Wiebe  Theresa Wilson  University of Pittsburgh University of Pittsburgh Rebecca Bruce  Matthew Bell  University of North Carolina University of Pittsburgh at Asheville Melanie Martin  New Mexico State University Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations." ></td>
	<td class="line x" title="2:617	There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization." ></td>
	<td class="line x" title="3:617	The goal of this work is learning subjective language from corpora." ></td>
	<td class="line x" title="4:617	Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity." ></td>
	<td class="line x" title="5:617	The features are also examined working together in concert." ></td>
	<td class="line x" title="6:617	The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets." ></td>
	<td class="line x" title="7:617	In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features." ></td>
	<td class="line x" title="8:617	Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article." ></td>
	<td class="line x" title="9:617	1." ></td>
	<td class="line x" title="10:617	Introduction Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations (Banfield 1982; Wiebe 1994)." ></td>
	<td class="line x" title="11:617	Many natural language processing (NLP) applications could benefit from being able to distinguish subjective language from language used to objectively present factual information." ></td>
	<td class="line x" title="12:617	Current extraction and retrieval technology focuses almost exclusively on the subject matter of documents." ></td>
	<td class="line x" title="13:617	However, additional aspects of a document influence its relevance, including evidential status and attitude (Kessler, Nunberg, Sch utze 1997)." ></td>
	<td class="line x" title="14:617	Information extraction systems should be able to distinguish between factual information (which should be extracted) and nonfactual information (which should be  Department of Computer Science, University of Pittsburgh, Pittsburgh, PA 15260." ></td>
	<td class="line x" title="15:617	E-mailwiebe,mbell}@cs.pitt.edu." ></td>
	<td class="line x" title="16:617	 Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA 15260." ></td>
	<td class="line x" title="17:617	Email: twilson@cs.pitt.edu." ></td>
	<td class="line x" title="18:617	 Department of Computer Science, University of North Carolina at Asheville, Asheville, NC 28804." ></td>
	<td class="line x" title="19:617	E-mail: bruce@cs.unca.edu  Department of Computer Science, New Mexico State University, Las Cruces, NM 88003." ></td>
	<td class="line x" title="20:617	E-mail: mmartin@cs.nmsu.edu." ></td>
	<td class="line x" title="21:617	Submission received: 20 March 2002; Revised submission received: 30 September 2003; Accepted for publication: 23 January 2004 278 Computational Linguistics Volume 30, Number 3 discarded or labeled as uncertain)." ></td>
	<td class="line x" title="22:617	Question-answering systems should distinguish between factual and speculative answers." ></td>
	<td class="line x" title="23:617	Multi-perspective question answering aims to present multiple answers to the user based upon speculation or opinions derived from different sources (Carbonell 1979; Wiebe et al. 2003)." ></td>
	<td class="line x" title="24:617	Multidocument summarization systems should summarize different opinions and perspectives." ></td>
	<td class="line oc" title="25:617	Automatic subjectivity analysis would also be useful to perform flame recognition (Spertus 1997; Kaufer 2000), e-mail classification (Aone, Ramos-Santacruze, and Niehaus 2000), intellectual attribution in text (Teufel and Moens 2000), recognition of speaker role in radio broadcasts (Barzialy et al. 2000), review mining (Terveen et al. 1997), review classification (Turney 2002; Pang, Lee, and Vaithyanathan 2002), style in generation (Hovy 1987), and clustering documents by ideological point of view (Sack 1995)." ></td>
	<td class="line x" title="26:617	In general, nearly any information-seeking system could benefit from knowledge of how opinionated a text is and whether or not the writer purports to objectively present factual material." ></td>
	<td class="line x" title="27:617	To perform automatic subjectivity analysis, good clues must be found." ></td>
	<td class="line x" title="28:617	A huge variety of words and phrases have subjective usages, and while some manually developed resources exist, such as dictionaries of affective language (General-Inquirer 2000; Heise 2000) and subjective features in general-purpose lexicons (e.g. , the attitude adverb features in Comlex [Macleod, Grishman, and Meyers 1998]), there is no comprehensive dictionary of subjective language." ></td>
	<td class="line x" title="29:617	In addition, many expressions with subjective usages have objective usages as well, so a dictionary alone would not suffice." ></td>
	<td class="line x" title="30:617	An NLP system must disambiguate these expressions in context." ></td>
	<td class="line x" title="31:617	The goal of our work is learning subjective language from corpora." ></td>
	<td class="line x" title="32:617	In this article, we generate and test subjectivity clues and contextual features and use the knowledge we gain to recognize subjective sentences and opinionated documents." ></td>
	<td class="line x" title="33:617	Two kinds of data are available to us: a relatively small amount of data manually annotated at the expression level (i.e. , labels on individual words and phrases) of Wall Street Journal and newsgroup data and a large amount of data with existing documentlevel annotations from the Wall Street Journal (opinion pieces, such as editorials and reviews, versus nonopinion pieces)." ></td>
	<td class="line x" title="34:617	Both are used as training data to identify clues of subjectivity." ></td>
	<td class="line x" title="35:617	In addition, we cross-validate the results between the two types of annotation: The clues learned from the expression-level data are evaluated against the document-level annotations, and those learned using the document-level annotations are evaluated against the expression-level annotations." ></td>
	<td class="line x" title="36:617	There were a number of motivations behind our decision to use document-level annotations, in addition to our manual annotations, to identify and evaluate clues of subjectivity." ></td>
	<td class="line x" title="37:617	The document-level annotations were not produced according to our annotation scheme and were not produced for the purpose of training and evaluating an NLP system." ></td>
	<td class="line x" title="38:617	Thus, they are an external influence from outside the laboratory." ></td>
	<td class="line x" title="39:617	In addition, there are a great number of these data, enabling us to evaluate the results on a larger scale, using multiple large test sets." ></td>
	<td class="line x" title="40:617	This and cross-training between the two types of annotations allows us to assess consistency in performance of the various identification procedures." ></td>
	<td class="line x" title="41:617	Good performance in cross-validation experiments between different types of annotations is evidence that the results are not brittle." ></td>
	<td class="line x" title="42:617	We focus on three types of subjectivity clues." ></td>
	<td class="line x" title="43:617	The first are hapax legomena, the set of words that appear just once in the corpus." ></td>
	<td class="line x" title="44:617	We refer to them here as unique words." ></td>
	<td class="line x" title="45:617	The set of all unique words is a feature with high frequency and significantly higher precision than baseline (Section 3.2)." ></td>
	<td class="line x" title="46:617	The second are collocations (Section 3.3)." ></td>
	<td class="line x" title="47:617	We demonstrate a straightforward method for automatically identifying collocational clues of subjectivity in texts." ></td>
	<td class="line x" title="48:617	The method is first used to identify fixed n-grams, such as of the century and get out of here." ></td>
	<td class="line x" title="49:617	Interest279 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language ingly, many include noncontent words that are typically on stop lists of NLP systems (e.g. , of, the, get, out, here in the above examples)." ></td>
	<td class="line x" title="50:617	The method is then used to identify an unusual form of collocation: One or more positions in the collocation may be filled by any word (of an appropriate part of speech) that is unique in the test data." ></td>
	<td class="line x" title="51:617	The third type of subjectivity clue we examine here are adjective and verb features identified using the results of a method for clustering words according to distributional similarity (Lin 1998) (Section 3.4)." ></td>
	<td class="line x" title="52:617	We hypothesized that two words may be distributionally similar because they are both potentially subjective (e.g. , tragic, sad, and poignant are identified from bizarre)." ></td>
	<td class="line x" title="53:617	In addition, we use distributional similarity to improve estimates of unseen events: A word is selected or discarded based on the precision of it together with its n most similar neighbors." ></td>
	<td class="line x" title="54:617	We show that the various subjectivity clues perform better and worse on the same data sets, exhibiting an important consistency in performance (Section 4.2)." ></td>
	<td class="line x" title="55:617	In addition to learning and evaluating clues associated with subjectivity, we address disambiguating them in context, that is, identifying instances of clues that are subjective in context (Sections 4.3 and 4.4)." ></td>
	<td class="line x" title="56:617	We find that the density of clues in the surrounding context is an important influence." ></td>
	<td class="line x" title="57:617	Using two types of annotations serves us well here, too." ></td>
	<td class="line x" title="58:617	It enables us to use manual judgments to identify parameters for disambiguating instances of automatically identified clues." ></td>
	<td class="line x" title="59:617	High-density clues are high precision in both the expression-level and document-level data." ></td>
	<td class="line x" title="60:617	In addition, we give the results of a new annotation study showing that most high-density clues are in subjective text spans (Section 4.5)." ></td>
	<td class="line x" title="61:617	Finally, we use the clues together to perform documentlevel classification, to further demonstrate the utility of the acquired knowledge (Section 4.6)." ></td>
	<td class="line x" title="62:617	At the end of the article, we discuss related work (Section 5) and conclusions (Section 6)." ></td>
	<td class="line x" title="63:617	2." ></td>
	<td class="line x" title="64:617	Subjectivity Subjective language is language used to express private states in the context of a text or conversation." ></td>
	<td class="line x" title="65:617	Private state is a general covering term for opinions, evaluations, emotions, and speculations (Quirk et al. 1985)." ></td>
	<td class="line x" title="66:617	The following are examples of subjective sentences from a variety of document types." ></td>
	<td class="line x" title="67:617	The first two examples are from Usenet newsgroup messages: (1) I had in mind your facts, buddy, not hers." ></td>
	<td class="line x" title="68:617	(2) Nice touch." ></td>
	<td class="line x" title="69:617	Alleges whenever facts posted are not in your persona of what is real. The next one is from an editorial: (3) We stand in awe of the Woodstock generations ability to be unceasingly fascinated by the subject of itself." ></td>
	<td class="line x" title="70:617	(Bad Acid, Wall Street Journal, August 17, 1989) The next example is from a book review: (4) At several different layers, its a fascinating tale." ></td>
	<td class="line x" title="71:617	(George Melloan, Whose Spying on Our Computers? Wall Street Journal, November 1, 1989) 280 Computational Linguistics Volume 30, Number 3 The last one is from a news story: (5) The cost of health care is eroding our standard of living and sapping industrial strength, complains Walter Maher, a Chrysler health-and-benefits specialist." ></td>
	<td class="line x" title="72:617	(Kenneth H. Bacon, Business and Labor Reach a Consensus on Need to Overhaul Health-Care System, Wall Street Journal, November 1, 1989) In contrast, the following are examples of objective sentences, sentences without significant expressions of subjectivity: (6) Bell Industries Inc. increased its quarterly to 10 cents from 7 cents a share." ></td>
	<td class="line x" title="73:617	(7) Northwest Airlines settled the remaining lawsuits filed on behalf of 156 people killed in a 1987 crash, but claims against the jetliners maker are being pursued, a federal judge said." ></td>
	<td class="line x" title="74:617	(Northwest Airlines Settles Rest of Suits, Wall Street Journal, November 1, 1989) A particular model of linguistic subjectivity underlies the current and past research in this area by Wiebe and colleagues." ></td>
	<td class="line x" title="75:617	It is most fully presented in Wiebe and Rapaport (1986, 1988, 1991) and Wiebe (1990, 1994)." ></td>
	<td class="line x" title="76:617	It was developed to support NLP research and combines ideas from several sources in fields outside NLP, especially linguistics and literary theory." ></td>
	<td class="line x" title="77:617	The most direct influences on the model were Dolezel (1973) (types of subjectivity clues), Uspensky (1973) (types of point of view), Kuroda (1973, 1976) (pragmatics of point of view), Chatman (1978) (story versus discourse), Cohn (1978) (linguistic styles for presenting consciousness), Fodor (1979) (linguistic description of opaque contexts), and especially Banfield (1982) (theory of subjectivity versus communication)." ></td>
	<td class="line x" title="78:617	1 The remainder of this section sketches our conceptualization of subjectivity and describes the annotation projects it underlies." ></td>
	<td class="line x" title="79:617	Subjective elements are linguistic expressions of private states in context." ></td>
	<td class="line x" title="80:617	Subjective elements are often lexical (examples are stand in awe, unceasingly, fascinated in (3) and eroding, sapping, and complains in (5))." ></td>
	<td class="line x" title="81:617	They may be single words (e.g. , complains) or more complex expressions (e.g. , stand in awe, what a NP)." ></td>
	<td class="line x" title="82:617	Purely syntactic or morphological devices may also be subjective elements (e.g. , fronting, parallelism, changes in aspect)." ></td>
	<td class="line x" title="83:617	A subjective element expresses the subjectivity of a source, who may be the writer or someone mentioned in the text." ></td>
	<td class="line x" title="84:617	For example, the source of fascinating in (4) is the writer, while the source of the subjective elements in (5) is Maher (according to the writer)." ></td>
	<td class="line x" title="85:617	In addition, a subjective element usually has a target, that is, what the subjectivity is about or directed toward." ></td>
	<td class="line x" title="86:617	In (4), the target is a tale; in (5), the target of Mahers subjectivity is the cost of health care." ></td>
	<td class="line x" title="87:617	Note our parenthetical aboveaccording to the writerconcerning Mahers subjectivity." ></td>
	<td class="line x" title="88:617	Maher is not directly speaking to us but is being quoted by the writer." ></td>
	<td class="line x" title="89:617	Thus, the source is a nested source, which we notate (writer, Maher); this represents the fact that the subjectivity is being attributed to Maher by the writer." ></td>
	<td class="line x" title="90:617	Since sources 1 For additional citations to relevant work from outside NLP, please see Banfield (1982), Fludernik (1993), Wiebe (1994), and Stein and Wright (1995)." ></td>
	<td class="line x" title="91:617	281 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language are not directly addressed by the experiments presented in this article, we merely illustrate the idea here with an example, to give the reader an idea: The Foreign Ministry said Thursday that it was surprised, to put it mildly by the U.S. State Departments criticism of Russias human rights record and objected in particular to the odious section on Chechnya." ></td>
	<td class="line x" title="92:617	(Moscow Times, March 8, 2002] Let us consider some of the subjective elements in this sentence, along with their sources: surprised, to put it mildly: (writer, Foreign Ministry, Foreign Ministry) to put it mildly: (writer, Foreign Ministry) criticism: (writer, Foreign Ministry, Foreign Ministry, U.S. State Department) objected: (writer, Foreign Ministry) odious: (writer, Foreign Ministry) Consider surprised, to put it mildly." ></td>
	<td class="line x" title="93:617	This refers to a private state of the Foreign Ministry (i.e. , it is very surprised)." ></td>
	<td class="line x" title="94:617	This is in the context of The Foreign Ministry said, which is in a sentence written by the writer." ></td>
	<td class="line x" title="95:617	This gives us the three-level source (writer, Foreign Ministry, Foreign Ministry)." ></td>
	<td class="line x" title="96:617	The phrase to put it mildly, which expresses sarcasm, is attributed to the Foreign Ministry by the writer (i.e. , according to the writer, the Foreign Ministry said this)." ></td>
	<td class="line x" title="97:617	So its source is (writer, Foreign Ministry)." ></td>
	<td class="line x" title="98:617	The subjective element criticism has a deeply nested source: According to the writer, the Foreign Ministry said it is surprised by the U.S. State Departments criticism." ></td>
	<td class="line x" title="99:617	The nested-source representation allows us to pinpoint the subjectivity in a sentence." ></td>
	<td class="line x" title="100:617	For example, there is no subjectivity attributed directly to the writer in the above sentence: At the level of the writer, the sentence merely says that someone said something and objected to something (without evaluating or questioning this)." ></td>
	<td class="line x" title="101:617	If the sentence started The magnificent Foreign Ministry said, then we would have an additional subjective element, magnificent, with source (writer)." ></td>
	<td class="line x" title="102:617	Note that subjective does not mean not true." ></td>
	<td class="line x" title="103:617	Consider the sentence John criticized Mary for smoking." ></td>
	<td class="line x" title="104:617	The verb criticized is a subjective element, expressing negative evaluation, with nested source (writer, John)." ></td>
	<td class="line x" title="105:617	But this does not mean that John does not believe that Mary smokes." ></td>
	<td class="line x" title="106:617	(In addition, the fact that John criticized Mary is being presented as true by the writer.)" ></td>
	<td class="line x" title="107:617	Similarly, objective does not mean true." ></td>
	<td class="line x" title="108:617	A sentence is objective if the language used to convey the information suggests that facts are being presented; in the context of the discourse, material is objectively presented as if it were true." ></td>
	<td class="line x" title="109:617	Whether or not the source truly believes the information, and whether or not the information is in fact true, are considerations outside the purview of a theory of linguistic subjectivity." ></td>
	<td class="line x" title="110:617	An aspect of subjectivity highlighted when we are working with NLP applications is ambiguity." ></td>
	<td class="line x" title="111:617	Many words with subjective usages may be used objectively." ></td>
	<td class="line x" title="112:617	Examples are sapping and eroding." ></td>
	<td class="line x" title="113:617	In (5), they are used subjectively, but one can easily imagine objective usages, in a scientific domain, for example." ></td>
	<td class="line x" title="114:617	Thus, an NLP system may not merely consult a list of lexical items to accurately identify subjective language but must disambiguate words, phrases, and sentences in context." ></td>
	<td class="line x" title="115:617	In our terminology, a potential subjective element (PSE) is a linguistic element that may be used to express 282 Computational Linguistics Volume 30, Number 3 Table 1 Data Sets and Annotations used in Experiments." ></td>
	<td class="line x" title="116:617	Annotators M, MM, and T are co-authors of this paper." ></td>
	<td class="line x" title="117:617	D and R are not." ></td>
	<td class="line x" title="118:617	Name Source Number of Words Annotators Type of annotation WSJ-SE Wall Street Journal 18,341 D,M Subjective elements NG-SE Newsgroup 15,413 M Subjective elements NG-FE Newsgroup 88,210 MM,R Flame elements OP1 Wall Street Journal 640,975 M,T Documents Composed of 4 data sets: W9-4,W9-10,W9-22,W-33 OP2 Wall Street Journal 629,690 M,T Documents Composed of 4 data sets: W9-2,W9-20,W9-21,W-23 subjectivity." ></td>
	<td class="line x" title="119:617	A subjective element is an instance of a potential subjective element, in a particular context, that is indeed subjective in that context (Wiebe 1994)." ></td>
	<td class="line x" title="120:617	In this article, we focus on learning lexical items that are associated with subjectivity (i.e. , PSEs) and then using them in concert to disambiguate instances of them (i.e. , to determine whether the instances are subjective elements)." ></td>
	<td class="line x" title="121:617	2.1 Manual Annotations In our subjectivity annotation projects, we do not give the annotators lists of particular words and phrases to look for." ></td>
	<td class="line x" title="122:617	Rather, we ask them to label sentences according to their interpretations in context." ></td>
	<td class="line x" title="123:617	As a result, the annotators consider a large variety of expressions when performing annotations." ></td>
	<td class="line x" title="124:617	We use data that have been manually annotated at the expression level, the sentence level, and the document level." ></td>
	<td class="line x" title="125:617	For diversity, we use data from the Wall Street Journal Treebank as well as data from a corpus of Usenet newsgroup messages." ></td>
	<td class="line x" title="126:617	Table 1 summarizes the data sets and annotations used in this article." ></td>
	<td class="line x" title="127:617	None of the datasets overlap." ></td>
	<td class="line x" title="128:617	The annotation types listed in the table are those used in the experiments presented in this article." ></td>
	<td class="line x" title="129:617	In our first subjectivity annotation project (Wiebe, Bruce, and OHara 1999; Bruce and Wiebe 1999), a corpus of sentences from the Wall Street Journal Treebank Corpus (Marcus, Santorini, and Marcinkiewicz 1993) (corpus WSJ-SE in Table 1) was annotated at the sentence level by multiple judges." ></td>
	<td class="line x" title="130:617	The judges were instructed to classify a sentence as subjective if it contained any significant expressions of subjectivity, attributed to either the writer or someone mentioned in the text, and to classify the sentence as objective, otherwise." ></td>
	<td class="line x" title="131:617	After multiple rounds of training, the annotators independently annotated a fresh test set of 500 sentences from WSJ-SE." ></td>
	<td class="line x" title="132:617	They achieved an average pairwise kappa score of 0.70 over the entire test set, an average pairwise kappa score of 0.80 for the 85% of the test set for which the annotators were somewhat sure of their judgments, and an average pairwise kappa score of 0.88 for the 70% of the test set for which the annotators were very sure of their judgments." ></td>
	<td class="line x" title="133:617	We later asked the same annotators to identify the subjective elements in WSJSE." ></td>
	<td class="line x" title="134:617	Specifically, each annotator was given the subjective sentences he identified in 283 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language the previous study and asked to put brackets around the words he believed caused the sentence to be classified as subjective." ></td>
	<td class="line x" title="135:617	2 For example (subjective elements are in parentheses): They paid (yet) more for (really good stuff)." ></td>
	<td class="line x" title="136:617	(Perhaps youll forgive me) for reposting his response." ></td>
	<td class="line x" title="137:617	No other instructions were given to the annotators and no training was performed for the expression-level task." ></td>
	<td class="line x" title="138:617	A single round of tagging was performed, with no communication between annotators." ></td>
	<td class="line x" title="139:617	There are techniques for analyzing agreement when annotations involve segment boundaries (Litman and Passonneau 1995; Marcu, Romera, and Amorortu 1999), but our focus in this article is on words." ></td>
	<td class="line x" title="140:617	Thus, our analyses are at the word level: Each word is classified as either appearing in a subjective element or not." ></td>
	<td class="line x" title="141:617	Punctuation and numbers are excluded from the analyses." ></td>
	<td class="line x" title="142:617	The kappa value for word agreement in this study is 0.42." ></td>
	<td class="line x" title="143:617	Another two-level annotation project was performed in Wiebe et al.(2001), this time involving document-level and expression-level annotations of newsgroup data (NG-FE in Table 1)." ></td>
	<td class="line x" title="145:617	In that project, we were interested in annotating flames, inflammatory messages in newsgroups or listservs." ></td>
	<td class="line x" title="146:617	Note that inflammatory language is a kind of subjective language." ></td>
	<td class="line x" title="147:617	The annotators were instructed to mark a message as a flame if the main intention of the message is a personal attack and the message contains insulting or abusive language." ></td>
	<td class="line x" title="148:617	After multiple rounds of training, three annotators independently annotated a fresh test set of 88 messages from NG-FE." ></td>
	<td class="line x" title="149:617	The average pairwise percentage agreement is 92% and the average pairwise kappa value is 0.78." ></td>
	<td class="line x" title="150:617	These results are comparable to those of Spertus (1997), who reports 98% agreement on noninflammatory messages and 64% agreement on inflammatory messages." ></td>
	<td class="line x" title="151:617	Two of the annotators were then asked to identify the flame elements in the entire corpus NG-FE." ></td>
	<td class="line x" title="152:617	Flame elements are the subset of subjective elements that are perceived to be inflammatory." ></td>
	<td class="line x" title="153:617	The two annotators were asked to do this in the entire corpus, even those messages not identified as flames, because messages that were not judged to be flames at the document level may contain some individual inflammatory phrases." ></td>
	<td class="line x" title="154:617	As above, no training was performed for the expression-level task, and a single round of tagging was performed, without communication between annotators." ></td>
	<td class="line x" title="155:617	Agreement was measured in the same way as in the subjective-element study above." ></td>
	<td class="line x" title="156:617	The kappa value for flame element annotations in corpus NG-FE is 0.46." ></td>
	<td class="line x" title="157:617	An additional annotation project involved a single annotator, who performed subjective-element annotations on the newsgroup corpus NG-SE." ></td>
	<td class="line x" title="158:617	The agreement results above suggest that good levels of agreement can be achieved at higher levels of classification (sentence and document), but agreement at the expression level is more challenging." ></td>
	<td class="line x" title="159:617	The agreement values are lower for the expression-level annotations but are still much higher than that expected by chance." ></td>
	<td class="line x" title="160:617	Note that our word-based analysis of agreement is a tough measure, because it requires that exactly the same words be identified by both annotators." ></td>
	<td class="line x" title="161:617	Consider the following example from WSJ-SE: D: (played the role well) (obligatory ragged jeans a thicket of long hair and rejection of all things conventional) 2 We are grateful to Aravind Joshi for suggesting this level of annotation." ></td>
	<td class="line x" title="162:617	284 Computational Linguistics Volume 30, Number 3 M: played the role (well) (obligatory) (ragged) jeans a (thicket) of long hair and (rejection) of (all things conventional) Judge D in the example consistently identifies entire phrases as subjective, while judge M prefers to select discrete lexical items." ></td>
	<td class="line x" title="163:617	Despite such differences between annotators, the expression-level annotations proved very useful for exploring hypotheses and generating features, as described below." ></td>
	<td class="line x" title="164:617	Since this article was written, a new annotation project has been completed." ></td>
	<td class="line x" title="165:617	A 10,000-sentence corpus of English-language versions of world news articles has been annotated with detailed subjectivity information as part of a project investigating multiple-perspective question answering (Wiebe et al. 2003)." ></td>
	<td class="line x" title="166:617	These annotations are much more detailed than the annotations used in this article (including, for example, the source of each private state)." ></td>
	<td class="line x" title="167:617	The interannotator agreement scores for the new corpus are high and are improvements over the results of the studies described above (Wilson and Wiebe 2003)." ></td>
	<td class="line x" title="168:617	The current article uses existing document-level subjective classes, namely editorials, letters to the editor, Arts & Leisure reviews, and Viewpoints in the Wall Street Journal." ></td>
	<td class="line x" title="169:617	These are subjective classes in the sense that they are text categories for which subjectivity is a key aspect." ></td>
	<td class="line x" title="170:617	We refer to them collectively as opinion pieces." ></td>
	<td class="line x" title="171:617	All other types of documents in the Wall Street Journal are collectively referred to as nonopinion pieces." ></td>
	<td class="line x" title="172:617	Note that opinion pieces are not 100% subjective." ></td>
	<td class="line x" title="173:617	For example, editorials contain objective sentences presenting facts supporting the writers argument, and reviews contain sentences objectively presenting facts about the product beign reviewed." ></td>
	<td class="line x" title="174:617	Similarly, nonopinion pieces are not 100% objective." ></td>
	<td class="line x" title="175:617	News reports present opinions and reactions to reported events (van Dijk 1988); they often contain segments starting with expressions such as critics claim and supporters argue." ></td>
	<td class="line x" title="176:617	In addition, quoted-speech sentences in which individuals express their subjectivity are often included (Barzilay et al. 2000)." ></td>
	<td class="line x" title="177:617	For concreteness, let us consider WSJ-SE, which, recall, has been manually annotated at the sentence level." ></td>
	<td class="line x" title="178:617	In WSJ-SE, 70% of the sentences in opinion pieces are subjective and 30% are objective." ></td>
	<td class="line x" title="179:617	In nonopinion pieces, 44% of the sentences are subjective and only 56% are objective." ></td>
	<td class="line x" title="180:617	Thus, while there is a higher concentration of subjective sentences in opinion versus nonopinion pieces, there are many subjective sentences in nonopinion pieces and objective sentences in opinion pieces." ></td>
	<td class="line x" title="181:617	An inspection of some data reveals that some editorial and review articles are not marked as such by the Wall Street Journal." ></td>
	<td class="line x" title="182:617	For example, there are articles whose purpose is to present an argument rather than cover a news story, but they are not explicitly labeled as editorials by the Wall Street Journal." ></td>
	<td class="line x" title="183:617	Thus, the opinion piece annotations of data sets OP1 and OP2 in Table 1 have been manually refined." ></td>
	<td class="line x" title="184:617	The annotation instructions were simply to identify any additional opinion pieces that were not marked as such." ></td>
	<td class="line x" title="185:617	To test the reliability of this annotation, two judges independently annotated two Wall Street Journal files, W9-22 and W9-33, each containing approximately 160,000 words." ></td>
	<td class="line x" title="186:617	This is an annotation lite task: With no training, the annotators achieved kappa values of 0.94 and 0.95, and each spent an average of three hours per Wall Street Journal file." ></td>
	<td class="line x" title="187:617	3." ></td>
	<td class="line x" title="188:617	Generating and Testing Subjective Features 3.1 Introduction The goal in this section is to learn lexical subjectivity clues of various types, single words as well as collocations." ></td>
	<td class="line x" title="189:617	Some require no training data, some are learned us285 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language ing the expression-level subjective-element annotations as training data, and some are learned using the document-level opinion piece annotations as training data (i.e. , opinion piece versus nonopinion piece)." ></td>
	<td class="line x" title="190:617	All of the clues are evaluated with respect to the document-level opinion piece annotations." ></td>
	<td class="line x" title="191:617	While these evaluations are our focus, because many more opinion piece than subjective-element data exist, we do evaluate the clues learned from the opinion piece data on the subjective-element data as well." ></td>
	<td class="line x" title="192:617	Thus, we cross-validate the results both ways between the two types of annotations." ></td>
	<td class="line x" title="193:617	Throughout this section, we evaluate sets of clues directly, by measuring the proportion of clues that appear in subjective documents or expressions, seeking those that appear more often than expected." ></td>
	<td class="line x" title="194:617	In later sections, the clues are used together to find subjective sentences and to perform text categorization." ></td>
	<td class="line x" title="195:617	The following paragraphs give details of the evaluation and experimental design used in this section." ></td>
	<td class="line x" title="196:617	The proportion of clues in subjective documents or expressions is their precision." ></td>
	<td class="line x" title="197:617	Specifically, the precision of a set S with respect to opinion pieces is prec(S)= number of instances of members of S in opinion pieces total number of instances of members of S in the data The precision of a set S with respect to subjective elements is prec(S)= number of instances of members of S in subjective elements total number of instances of members of S in the data In the above, S is a set of types (not tokens)." ></td>
	<td class="line x" title="198:617	The counts are of tokens (i.e. , instances or occurrences) of members of S. Why use a set rather than individual items?" ></td>
	<td class="line x" title="199:617	Many good clues of subjectivity occur with low frequency (Wiebe, McKeever, and Bruce 1998)." ></td>
	<td class="line x" title="200:617	In fact, as we shall see below, uniqueness in the corpus is an informative feature for subjectivity classification." ></td>
	<td class="line x" title="201:617	Thus, we do not want to discard low-frequency clues, because they are a valuable source of information, and we do not want to evaluate individual low-frequency lexical items, because the results would be unreliable." ></td>
	<td class="line x" title="202:617	Our strategy is thus to identify and evaluate sets of words and phrases, rather than individual items." ></td>
	<td class="line x" title="203:617	What kinds of results may we expect?" ></td>
	<td class="line x" title="204:617	We cannot expect absolutely high precision with respect to the opinion piece classifications, even for strong clues, for three reasons." ></td>
	<td class="line x" title="205:617	First, for our purposes, the data are noisy." ></td>
	<td class="line x" title="206:617	As mentioned above, while the proportion of subjective sentences is higher in opinion than in nonopinion pieces, the proportions are not 100 and 0: Opinion pieces contain objective sentences, and nonopinion pieces contain subjective sentences." ></td>
	<td class="line x" title="207:617	Second, we are trying to learn lexical items associated with subjectivity, that is, PSEs." ></td>
	<td class="line x" title="208:617	As discussed above, many words and phrases with subjective usages have objective usages as well." ></td>
	<td class="line x" title="209:617	Thus, even in perfect data with no noise, we would not expect 100% precision." ></td>
	<td class="line x" title="210:617	(This is the motivation for the work on density presented in section 4.4.) Third, the distribution of opinions and nonopinions is highly skewed in favor of nonopinions: Only 9% of the articles in the combination of OP1 and OP2 are opinion pieces." ></td>
	<td class="line x" title="211:617	In this work, increases in precision over a baseline precision are used as evidence that promising sets of PSEs have been found." ></td>
	<td class="line x" title="212:617	Our main baseline for comparison is the number of word instances in opinion pieces, divided by the total number of word instances: Baseline Precision = number of word instances in opinion pieces total number of word instances 286 Computational Linguistics Volume 30, Number 3 Table 2 Frequencies and increases in precision of unique words in subjective-element data." ></td>
	<td class="line x" title="213:617	Baseline frequency is the total number of words, and baseline precision is the proportion of words in subjective elements." ></td>
	<td class="line x" title="214:617	WSJ-SE D M freq +prec +prec Unique words 2,615 +.07 +.12 Baseline 18,341.07 .08 Words and phrases with higher proportions than this appear more than expected in opinion pieces." ></td>
	<td class="line x" title="215:617	To further evaluate the quality of a set of PSEs, we also perform the following significance test." ></td>
	<td class="line x" title="216:617	For a set of PSEs in a given data set, we test the significance of the difference between (1) the proportion of words in opinion pieces that are PSEs and (2) the proportion of words in nonopinion pieces that are PSEs, using the z-significance test for two proportions." ></td>
	<td class="line x" title="217:617	Before we continue, there are a few more technical items to mention concerning the data preparation and experimental design:  All of the data sets are stemmed using Karps morphological analyzer (Karp et al. 1994) and part-of-speech tagged using Brills (1992) tagger." ></td>
	<td class="line x" title="218:617	 When the opinion piece classifications are used for training, the existing classifications, assigned by the Wall Street Journal, are used." ></td>
	<td class="line x" title="219:617	Thus, the processes using them as training data may be applied to more data to learn more clues, without requiring additional manual annotation." ></td>
	<td class="line x" title="220:617	 When the opinion piece data are used for testing, the manually refined classifications (described at the end of Section 2.1) are used." ></td>
	<td class="line x" title="221:617	 OP1 and OP2 together comprise eight treebank files." ></td>
	<td class="line x" title="222:617	Below, we often give results separately for the component files, allowing us to assess the consistency of results for the various types of clues." ></td>
	<td class="line x" title="223:617	3.2 Unique Words In this section, we show that low-frequency words are associated with subjectivity in both the subjective-element and opinion piece data." ></td>
	<td class="line x" title="224:617	Apparently, people are creative when they are being opinionated." ></td>
	<td class="line x" title="225:617	Table 2 gives results for unique words in subjective-element data." ></td>
	<td class="line x" title="226:617	Recall that unique words are those that appear just once in the corpus, that is, hapax legomena." ></td>
	<td class="line x" title="227:617	The first row of Table 2 gives the frequency of unique words in WSJ-SE, followed by the percentage-point improvements in precision over baseline for unique words in subjective elements marked by two annotators (denoted as D and M in the table)." ></td>
	<td class="line x" title="228:617	The second row gives baseline frequency and precisions." ></td>
	<td class="line x" title="229:617	Baseline frequency is the total number of words in WSJ-SE." ></td>
	<td class="line x" title="230:617	Baseline precision for an annotator is the proportion of words included in subjective elements by that annotator." ></td>
	<td class="line x" title="231:617	Specifically, consider annotator M. The baseline precision of words in subjective elements marked by M is 0.08, 287 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language Table 3 Frequencies and increases in precision for words that appear exactly once in the data sets composing OP1." ></td>
	<td class="line x" title="232:617	For each data set, baseline frequency is the total number of words, and baseline precision is the proportion of words in opinion pieces." ></td>
	<td class="line x" title="233:617	W9-04 W9-10 W9-22 W9-33 freq +prec freq +prec freq +prec freq +prec Unique words 4,794 +.15 4,763 +.16 4,274 +.11 4,567 +.11 Baseline 156,421 .19 156,334 .18 155,135 .13 153,634 .14 but the precision of unique words in these same annotations is 0.20, 0.12 points higher than the baseline." ></td>
	<td class="line x" title="234:617	This is a 150% improvement over the baseline." ></td>
	<td class="line x" title="235:617	The number of unique words in opinion pieces is also higher than expected." ></td>
	<td class="line x" title="236:617	Table 3 compares the precision of the set of unique words to the baseline precision (i.e. , the precision of the set of all words that appear in the corpus) in the four WSJ files composing OP1." ></td>
	<td class="line x" title="237:617	Before this analysis was performed, numbers were removed from the data (we are not interested in the fact that, say, the number 163,213.01 appears just once in the corpus)." ></td>
	<td class="line x" title="238:617	The number of words in each data set and baseline precisions are listed at the bottom of the table." ></td>
	<td class="line x" title="239:617	The freq columns give total frequencies." ></td>
	<td class="line x" title="240:617	The +prec columns show the percentage-point improvements in precision over baseline." ></td>
	<td class="line x" title="241:617	For example, in W9-10, unique words have precision 0.34: 0.18 baseline plus an improvement over baseline of 0.16." ></td>
	<td class="line x" title="242:617	The difference in the proportion of words that are unique in opinion pieces and the proportion of words that are unique in nonopinion pieces is highly significant, with p < 0.001 (z  22) for all of the data sets." ></td>
	<td class="line x" title="243:617	Note that not only does the set of unique words have higher than baseline precision, the set is a frequent feature." ></td>
	<td class="line x" title="244:617	The question arises, how does corpus size affect the precision of the set of unique words?" ></td>
	<td class="line x" title="245:617	Presumably, uniqueness in a larger corpus is more meaningful than uniqueness in a smaller one." ></td>
	<td class="line x" title="246:617	The results in Figure 1 provide evidence that it is. The y-axis in Figure 1 represents increase in precision over baseline and the x-axis represents corpus size." ></td>
	<td class="line x" title="247:617	Five graphs are plotted, one for the set of words that appear exactly once (uniques), one for the set of words that appear exactly twice ( freq2), one for the set of words that appear exactly three times ( freq3), etc. In Figure 1, increases in precision are given for corpora of size n, where n = 20, 40,, 2420, 2440 documents." ></td>
	<td class="line x" title="248:617	Each data point is an average over 25 sample corpora of size n. The sample corpora were chosen from the concatenation of OP1 and OP2, in which 9% of the documents are opinion pieces." ></td>
	<td class="line x" title="249:617	The sample corpora were created by randomly selecting documents from the large corpus, preserving the 9% distribution of opinion pieces." ></td>
	<td class="line x" title="250:617	At the smallest corpus size (containing 20 documents), the average number of words is 9,617." ></td>
	<td class="line x" title="251:617	At the largest corpus size (containing 2440 documents), the average is 1,225,186 words." ></td>
	<td class="line x" title="252:617	As can be seen in the figure, the precision of unique and other low-frequency words increases with corpus size, with increases tapering off at the largest corpus size tested." ></td>
	<td class="line x" title="253:617	Words with frequency 2 also realize a nice increase, although one that is not as dramatic, in precision over baseline." ></td>
	<td class="line x" title="254:617	Even words of frequency 3, 4, and 5 show modest increases." ></td>
	<td class="line x" title="255:617	To help us understand the importance of low-frequency words in large as opposed to small data sets, we can consider the following analogy." ></td>
	<td class="line x" title="256:617	With collectible trading cards, rare cards are the most valuable." ></td>
	<td class="line x" title="257:617	However, if we have some cards and are trying to determine thier value, looking in only a few packs of cards will not tell us if 288 Computational Linguistics Volume 30, Number 3 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16 0.18 0.20 20 620 1220 1820 2420 Corpus Size (documents) Increase in Precision uniques freq2 freq3 freq4 freq5 Figure 1 Precision of low-frequency words as corpus size increases." ></td>
	<td class="line x" title="258:617	any of our cards are valuable." ></td>
	<td class="line x" title="259:617	Only by looking at many packs of cards can we make a determination as to which are the rare ones." ></td>
	<td class="line x" title="260:617	Only in samples of sufficient size is uniqueness informative." ></td>
	<td class="line x" title="261:617	The results in this section suggest that an NLP system using uniqueness features to recognize subjectivity should determine uniqueness with respect to the test data augmented with an additional store of (unannotated) data." ></td>
	<td class="line x" title="262:617	3.3 Identifying Potentially Subjective Collocations from Subjective-Element and Flame-Element Annotations In this section, we describe experiments in identifying potentially subjective collocations." ></td>
	<td class="line x" title="263:617	Collocations are selected from the subjective-element data (i.e. , NG-SE, NG-FE, and WSJ-SE), using the union of the annotators tags for the data sets tagged by multiple taggers." ></td>
	<td class="line x" title="264:617	The results are then evaluated on opinion piece data." ></td>
	<td class="line x" title="265:617	The selection procedure is as follows." ></td>
	<td class="line x" title="266:617	First, all 1-grams, 2-grams, 3-grams, and 4-grams are extracted from the data." ></td>
	<td class="line x" title="267:617	In this work, each constituent of an n-gram is a word-stem, part-of-speech pair." ></td>
	<td class="line x" title="268:617	For example, (in-prep the-det can-noun) is a 3-gram that matches trigrams consisting of preposition in, followed by determiner the, and ending with noun can." ></td>
	<td class="line x" title="269:617	A subset of the n-grams are then selected based on precision." ></td>
	<td class="line x" title="270:617	The precision of an n-gram is the number of subjective instances of that n-gram in the data divided by the total number of instances of that n-gram in the data." ></td>
	<td class="line x" title="271:617	An instance of an n-gram is subjective if each word occurs in a subjective element in the data." ></td>
	<td class="line x" title="272:617	n-grams are selected based on two criteria." ></td>
	<td class="line x" title="273:617	First, the precision of the n-gram must be greater than the baseline precision (i.e. , the proportion of all word instances that 289 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language are in subjective elements)." ></td>
	<td class="line x" title="274:617	Second, the precision of the n-gram must be greater than the maximum precision of its constituents." ></td>
	<td class="line x" title="275:617	This criterion is used to avoid selecting unnecessarily long collocations." ></td>
	<td class="line x" title="276:617	For example, scumbag is a strongly subjective clue." ></td>
	<td class="line x" title="277:617	If be a scumbag does not have higher precision than scumbag alone, we do not want to select it." ></td>
	<td class="line x" title="278:617	Specifically, let (W1, W2) be a bigram consisting of consecutive words W1 and W2." ></td>
	<td class="line x" title="279:617	(W1,W2) is identified as a potential subjective element if prec(W1, W2)  0.1 and: prec(W1, W2) > max(prec(W1), prec(W2)) For trigrams, we extend the second condition as follows." ></td>
	<td class="line x" title="280:617	Let (W1, W2, W3) be a trigram consisting of consecutive words W1, W2, and W3." ></td>
	<td class="line x" title="281:617	The condition is then prec(W1, W2, W3) > max(prec(W1, W2), prec(W3)) or prec(W1, W2, W3) > max(prec(W1), prec(W2, W3)) The selection of 4-grams is similar to the selection of 3-grams, comparing the 4-gram first with the maximum of the precisions of word W1 and trigram (W2, W3, W4) and then with the maximum of the precisions of trigram (W1,W2,W3) and word W4." ></td>
	<td class="line x" title="282:617	We call the n-gram collocations identified as above fixed-n-grams." ></td>
	<td class="line x" title="283:617	We also define a type of collocation called a unique generalized n-gram (ugen-ngram)." ></td>
	<td class="line x" title="284:617	Such collocations have placeholders for unique words." ></td>
	<td class="line x" title="285:617	As will be seen below, these are our highest-precision features." ></td>
	<td class="line x" title="286:617	To find and select such generalized collocations, we first find every word that appears just once in the corpus and replace it with a new word, UNIQUE (but remembering the part of speech of the original word)." ></td>
	<td class="line x" title="287:617	In essence, we treat the set of single-instance words as a single, frequently occurring word (which occurs with various parts of speech)." ></td>
	<td class="line x" title="288:617	Precisely the same method used for extracting and selecting n-grams above is used to obtain the potentially subjective collocations with one or more positions filled by a UNIQUE, part-of-speech pair." ></td>
	<td class="line x" title="289:617	To test the ugen-n-grams extracted from the subjective-element training data using the method outlined above, we assess their precision with respect to opinion piece data." ></td>
	<td class="line x" title="290:617	As with the training data, all unique words in the test data are replaced by UNIQUE." ></td>
	<td class="line x" title="291:617	When a ugen-n-gram is matched against the test data, the UNIQUE fillers match words (of the appropriate parts of speech) that are unique in the test data." ></td>
	<td class="line x" title="292:617	Table 4 shows the results of testing the fixed-n-gram and the ugen-n-gram patterns identified as described above on the four data sets composing OP1." ></td>
	<td class="line x" title="293:617	The freq columns give total frequencies, and the +prec columns show the improvements in precision from the baseline." ></td>
	<td class="line x" title="294:617	The number of words in each data set and baseline precisions are given at the bottom of the table." ></td>
	<td class="line x" title="295:617	For all n-gram features besides the fixed-4-grams and ugen-4-grams, the proportion of features in opinion pieces is significantly greater than the proportion of features in nonopinion pieces." ></td>
	<td class="line x" title="296:617	3 The question arises, how much overlap is there between instances of fixed-n-grams and instances of ugen-n-grams?" ></td>
	<td class="line x" title="297:617	In the test data of Table 4, there are a total of 8,577 fixed-n-grams instances." ></td>
	<td class="line x" title="298:617	Only 59 of these, fewer than 1% are contained (wholly or in part) in ugen-n-gram instances." ></td>
	<td class="line x" title="299:617	This small intersection set shows that two different types of potentially subjective collocations are being recognized." ></td>
	<td class="line x" title="300:617	3 Specifically, the difference between (1) the number of feature instances in opinion pieces divided by the number of words in opinion pieces and (2) the number of feature instances in nonopinion pieces divided by the number of words in nonopinion pieces is significant (p < 0.05) for all data sets." ></td>
	<td class="line x" title="301:617	290 Computational Linguistics Volume 30, Number 3 Table 4 Frequencies and increases in precision of fixed-n-gram and ugen-n-gram collocations learned from the subjective-element data." ></td>
	<td class="line x" title="302:617	For each data set, baseline frequency is the total number of words, and baseline precision is the proportion of words in opinion pieces." ></td>
	<td class="line x" title="303:617	W9-04 W9-10 W9-22 W9-33 freq +prec freq +prec freq +prec freq +prec fixed-2-grams 1,840 +.07 1,972 +.07 1,933 +.04 1,839 +.05 ugen-2-grams 281 +.21 256 +.26 261 +.17 254 +.17 fixed-3-grams 213 +.08 243 +.09 214 +.05 238 +.05 ugen-3-grams 148 +.29 133 +.27 147 +.16 133 +.15 fixed-4-grams 18 +.15 17 +.06 12 +.29 14 .07 ugen-4-grams 13 +.12 3 +.82 15 +.27 13 +.25 baseline 156,421 .19 156,334 .18 155,135 .13 153,634 .14 Randomly selected examples of our learned collocations that appear in the test data are given in Tables 5 and 6." ></td>
	<td class="line x" title="304:617	It is interesting to note that the unique generalized collocations were learned from the training data by their matching different unique words from the ones they match in the test data." ></td>
	<td class="line x" title="305:617	3.4 Generating Features from Document-Level Annotations Using Distributional Similarity In this section, we identify adjective and verb PSEs using distributional similarity." ></td>
	<td class="line x" title="306:617	Opinion-piece data are used for training, and (a different set of) opinion-piece data and the subjective-element data are used for testing." ></td>
	<td class="line x" title="307:617	With distributional similarity, words are judged to be more or less similar based on their distributional patterning in text (Lee 1999; Lee and Pereira 1999)." ></td>
	<td class="line x" title="308:617	Our Table 5 Random sample of fixed-3-gram collocations in OP1." ></td>
	<td class="line x" title="309:617	one-noun of-prep his-det worst-adj of-prep all-det quality-noun of-prep the-det to-prep do-verb so-adverb in-prep the-det company-noun you-pronoun and-conj your-pronoun have-verb taken-verb the-det rest-noun of-prep us-pronoun are-verb at-prep least-adj but-conj if-prep you-pronoun as-prep a-det weapon-noun continue-verb to-to do-verb purpose-noun of-prep the-det could-modal have-verb be-verb it-pronoun seem-verb to-prep to-pronoun continue-verb to-prep have-verb be-verb the-det do-verb something-noun about-prep cause-verb you-pronoun to-to evidence-noun to-to back-adverb that-prep you-pronoun are-verb i-pronoun be-verb not-adverb of-prep the-det century-noun of-prep money-noun be-prep 291 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language Table 6 Random sample of unique generalized collocations in OP1." ></td>
	<td class="line x" title="310:617	U: UNIQUE." ></td>
	<td class="line x" title="311:617	Pattern Instances U-adj as-prep: drastic as; perverse as; predatory as U-adj in-prep: perk in; unsatisfying in; unwise in U-adverb U-verb: adroitly dodge; crossly butter; unceasingly fascinate U-noun back-adverb: cutting back; hearken back U-verb U-adverb: coexist harmoniously; flouncing tiresomely ad-noun U-noun: ad hoc; ad valorem any-det U-noun: any over-payment; any tapings; any write-off are-verb U-noun: are escapist; are lowbrow; are resonance but-conj U-noun: but belch; but cirrus; but ssa different-adj U-noun: different ambience; different subconferences like-prep U-noun: like hoffmann; like manute; like woodchuck national-adj U-noun: national commonplace; national yonhap particularly-adverb U-adj: particularly galling; particularly noteworthy so-adverb U-adj: so monochromatic; so overbroad; so permissive this-det U-adj: this biennial; this inexcusable; this scurrilous your-pronoun U-noun: your forehead; your manuscript; your popcorn U-adj and-conj U-adj: arduous and raucous; obstreperous and abstemious U-noun be-verb a-det: acyclovir be a; siberia be a U-noun of-prep its-pronoun: outgrowth of its; repulsion of its U-verb and-conj U-verb: wax and brushed; womanize and booze U-verb to-to a-det: cling to a; trek to a are-verb U-adj to-to: are opaque to; are subject to a-det U-noun and-conj: a blindfold and; a rhododendron and a-det U-verb U-noun: a jaundice ipo; a smoulder sofa it-pronoun be-verb U-adverb: it be humanly; it be sooo than-prep a-det U-noun: than a boob; than a menace the-det U-adj and-conj: the convoluted and; the secretive and the-det U-noun that-prep: the baloney that; the cachet that to-to a-det U-adj: to a gory; to a trappist to-to their-pronoun U-noun: to their arsenal; to their subsistence with-prep an-det U-noun: with an alias; with an avalanche 292 Computational Linguistics Volume 30, Number 3 trainingPrec(s) is the precision of s in the training data validationPrec(s) is the precision of s in the validation data testPrec(s) is the precision of s in the test data (similarly for trainingFreq, validationFreq, and testFreq) S = the set of all adjectives (verbs) in the training data for T in [0.01,0.04,,0.70]: for n in [2,3,,40]: retained = {} For s i in S: if trainingPrec({s i }C i,n ) > T: retained = retained {s i }C i,n R T,n = retained ADJ pses = {} (VERB pses = {}) for T in [0.01,0.04,,0.70]: for n in [2,3,,40]: if validationPrec(R T,n )  0.28 (0.23 for verbs) and validationFreq(R T,n )  100: ADJ pses = ADJ pses  R T,n (VERB pses = VERB pses  R T,n ) Results in Table 7 show testPrec(ADJ pses ) and testFreq(ADJ pses )." ></td>
	<td class="line x" title="312:617	Figure 2 Algorithm for selecting adjective and verb features using distributional similarity." ></td>
	<td class="line x" title="313:617	motivation for experimenting with it to identify PSEs was twofold." ></td>
	<td class="line x" title="314:617	First, we hypothesized that words might be distributionally similar because they share pragmatic usages, such as expressing subjectivity, even if they are not close synonyms." ></td>
	<td class="line x" title="315:617	Second, as shown above, low-frequency words appear more often in subjective texts than expected." ></td>
	<td class="line x" title="316:617	We did not want to discard all low-frequency words from consideration but cannot effectively judge the suitability of individual words." ></td>
	<td class="line x" title="317:617	Thus, to decide whether to retain a word as a PSE, we consider the precision not of the individual word, but of the word together with a cluster of words similar to it." ></td>
	<td class="line x" title="318:617	Many variants of distributional similarity have been used in NLP (Lee 1999; Lee and Pereira 1999)." ></td>
	<td class="line x" title="319:617	Dekang Lins (1998) method is used here." ></td>
	<td class="line x" title="320:617	In contrast to many implementations, which focus exclusively on verb-noun relationships, Lins method incorporates a variety of syntactic relations." ></td>
	<td class="line x" title="321:617	This is important for subjectivity recognition, because PSEs are not limited to verb-noun relationships." ></td>
	<td class="line x" title="322:617	In addition, Lins results are freely available." ></td>
	<td class="line x" title="323:617	A set of seed words begins the process." ></td>
	<td class="line x" title="324:617	For each seed s i, the precision of the set {s i }C i,n in the training data is calculated, where C i,n is the set of n words most similar to s i, according to Lins (1998) method." ></td>
	<td class="line x" title="325:617	If the precision of {s i }C i,n is greater than a threshold T, then the words in this set are retained as PSEs." ></td>
	<td class="line x" title="326:617	If it is not, neither s i nor the words in C i,n are retained." ></td>
	<td class="line x" title="327:617	The union of the retained sets will be denoted R T,n, that is, the union of all sets {s i }C i,n with precision on the training set > T. In Wiebe (2000), the seeds (the s i s) were extracted from the subjective-element annotations in corpus WSJ-SE." ></td>
	<td class="line x" title="328:617	Specifically, the seeds were the adjectives that appear at least once in a subjective element in WSJ-SE." ></td>
	<td class="line x" title="329:617	In this article, the opinion piece corpus is used to move beyond the manual annotations and small corpus of the earlier work, and a much looser criterion is used to choose the initial seeds: All of the adjectives (verbs) in the training data are used." ></td>
	<td class="line x" title="330:617	The algorithm for the process is given in Figure 2." ></td>
	<td class="line x" title="331:617	There is one small difference for adjectives and verbs noted in the figure, that is, the precision threshold of 0.28 for 293 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language Table 7 Frequencies and increases in precision for adjective and verb features identified using distributional similarity with filtering." ></td>
	<td class="line x" title="332:617	For each test data set, baseline frequency is the total number of words, and baseline precision is the proportion of words in opinion pieces." ></td>
	<td class="line x" title="333:617	Baseline ADJ pses VERB pses Training Validation Test freq prec freq +prec freq +prec W9-10 W9-22 W9-22 W9-10 W9-33 153,634 .14 1,576 +.12 1,490 +.11 W9-10 W9-33 W9-33 W9-10 W9-22 155,135 .13 859 +.15 535 +.11 W9-22 W9-33 W9-33 W9-22 W9-10 156,334 .18 249 +.22 224 +.10 All pairings of W9-10, W9-22,W9-33 W9-4 156,421 .19 1,872 +.17 1,777 +.15 adjectives versus 0.23 for verbs." ></td>
	<td class="line x" title="334:617	These thresholds were determined using validation data." ></td>
	<td class="line x" title="335:617	Seeds and their clusters are assessed on a training set for many parameter settings (cluster size n from 2 through 40, and precision threshold T from 0.01 through 0.70 by .03)." ></td>
	<td class="line x" title="336:617	As mentioned above, each (n, T) parameter pair yields a set of adjectives R T,n, that is, the union of all sets {s i }C i,n with precision on the training set > T. A subset, ADJ pses, of those sets is chosen based on precision and frequency in a validation set." ></td>
	<td class="line x" title="337:617	Finally, the ADJ pses are tested on the test set." ></td>
	<td class="line x" title="338:617	Table 7 shows the results for four opinion piece test sets." ></td>
	<td class="line x" title="339:617	Multiple trainingvalidation data set pairs are used for each test set, as given in Table 7." ></td>
	<td class="line x" title="340:617	The results are for the union of the adjectives (verbs) chosen for each pair." ></td>
	<td class="line x" title="341:617	The freq columns give total frequencies, and the +prec columns show the improvements in precision from the baseline." ></td>
	<td class="line x" title="342:617	For each data set, the difference between the proportion of instances of ADJ pses in opinion pieces and the proportion in nonopinion pieces is significant (p < 0.001, z  9.2)." ></td>
	<td class="line x" title="343:617	The same is true for VERB pses (p < 0.001, z  4.1)." ></td>
	<td class="line x" title="344:617	In the interests of testing consistency, Table 8 shows the results of assessing the adjective and verb features generated from opinion piece data (ADJ pses and VERB pses Table 8 Average frequencies and increases in precision in subjective-element data of the sets tested in Table 7." ></td>
	<td class="line x" title="345:617	The baselines are the precisions of adjectives/verbs that appear in subjective elements in the subjective-element data." ></td>
	<td class="line x" title="346:617	Adj baseline Verb baseline ADJ pses VERB pses freq prec freq prec freq +prec freq +prec WSJ-SE-D 1,632 .13 2,980 .15 136 +.16 151 +.10 WSJ-SE-M 1,632 .19 2,980 .12 136 +.24 151 +.13 NG-SE 1,104 .37 2,629 .15 185 +.25 275 +.08 294 Computational Linguistics Volume 30, Number 3 Table 9 Frequencies and increases in precision for all features." ></td>
	<td class="line x" title="347:617	For each data set, baseline frequency is the total number of words, and baseline precision is the proportion of words in opinion pieces." ></td>
	<td class="line x" title="348:617	freq: total frequency; +prec: increase in precision over baseline." ></td>
	<td class="line x" title="349:617	W9-04 W9-10 W9-22 W9-33 freq +prec freq +prec freq +prec freq +prec Unique words 4794 +.15 4763 +.16 4274 +.11 4567 +.11 Fixed-2-grams 1840 +.07 1972 +.07 1933 +.04 1839 +.05 ugen-2-grams 281 +.21 256 +.26 261 +.17 254 +.17 Fixed-3-grams 213 +.08 243 +.09 214 +.05 238 +.05 ugen-3-grams 148 +.29 133 +.27 147 +.16 133 +.15 Fixed-4-grams 18 +.15 17 +.06 12 +.29 14 .07 ugen-4-grams 13 +.12 3 +.82 15 +.27 13 +.25 Adjectives 1872 +.17 249 +.22 859 +.15 1576 +.12 Verbs 1777 +.15 224 +.10 535 +.11 1490 +.11 Baseline 156421 .19 156334 .18 155135 .13 153634 .14 in Table 7) on the subjective-element data." ></td>
	<td class="line x" title="350:617	The left side of the table gives baseline figures for each set of subjective-element annotations." ></td>
	<td class="line x" title="351:617	The right side of the table gives the average frequencies and increases in precision over baseline for the ADJ pses and VERB pses sets on the subjective-element data." ></td>
	<td class="line x" title="352:617	The baseline figures in the table are the frequencies and precisions of the sets of adjectives and verbs that appear at least once in a subjective element." ></td>
	<td class="line x" title="353:617	Since these sets include words that appear just once in the corpus (and thus have 100% precision), the baseline precision is a challenging one." ></td>
	<td class="line x" title="354:617	Testing the VERB pses and ADJ pses on the subjective-element data reveals some interesting consistencies for these subjectivity clues." ></td>
	<td class="line x" title="355:617	The precision increases of the VERB pses on the subjective-element data are comparable to their increases on the opinion piece data." ></td>
	<td class="line x" title="356:617	Similarly, the precision increases of the ADJ pses on the subjective-element data are as good as or better than the performance of this set of PSEs on the opinion piece data." ></td>
	<td class="line x" title="357:617	Finally, the precisions increases for the ADJ pses are higher than for the VERB pses on all data sets." ></td>
	<td class="line x" title="358:617	This is again consistent with the higher performance of the ADJ pses sets in the opinion piece data sets." ></td>
	<td class="line x" title="359:617	4." ></td>
	<td class="line x" title="360:617	Features Used in Concert 4.1 Introduction In this section, we examine the various types of clues used together." ></td>
	<td class="line x" title="361:617	In preparation for this work, all instances in OP1 and OP2 of all of the PSEs identified as described in Section 3 have been automatically identified." ></td>
	<td class="line x" title="362:617	All training to define the PSE instances in OP1 was performed on data separate from OP1, and all training to define the PSE instances in OP2 was performed on data separate from OP2." ></td>
	<td class="line x" title="363:617	4.2 Consistency in Precision among Data Sets Table 9 summarizes the results from previous sections in which the opinion piece data are used for testing." ></td>
	<td class="line x" title="364:617	The performance of the various features is consistently good or bad on the same data sets: the performance is better for all features on W9-10 and W9-04 than on W9-22 and W9-33 (except for the ugen-4-grams, which occur with very low frequency, and the verbs, which have low frequency in W9-10)." ></td>
	<td class="line x" title="365:617	This is so despite the fact that the features were generated using different procedures and data: The 295 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language 0." ></td>
	<td class="line x" title="366:617	PSEs = all adjs, verbs, modals, nouns, and adverbs that appear at least once in an SE (except not, will, be, have)." ></td>
	<td class="line x" title="367:617	1." ></td>
	<td class="line x" title="368:617	PSEinsts = the set of all instances of PSEs 2." ></td>
	<td class="line x" title="369:617	HiDensity = {} 3." ></td>
	<td class="line x" title="370:617	For P in PSEinsts: 4." ></td>
	<td class="line x" title="371:617	leftWin(P) = the W words before P 5." ></td>
	<td class="line x" title="372:617	rightWin(P) = the W words after P 6." ></td>
	<td class="line x" title="373:617	density(P) = number of SEs whose first or last word is in leftWin(P) or rightWin(P) 7." ></td>
	<td class="line x" title="374:617	if density(P)  T: HiDensity = HiDensity {P} 8." ></td>
	<td class="line x" title="375:617	prec(PSEinsts)= number of PSEinsts in subject elements |PSEinsts| 9." ></td>
	<td class="line x" title="376:617	prec(HiDensity)= number of HiDensity in subject elements |HiDensity| Figure 3 Algorithm for calculating density in subjective-element data." ></td>
	<td class="line x" title="377:617	adjectives and verbs were generated from WSJ document-level opinion piece classifications; the n-gram features were generated from newsgroup and WSJ expression-level subjective-element classifications; and the unique unigram feature requires no training." ></td>
	<td class="line x" title="378:617	This consistency in performance suggests that the results are not brittle." ></td>
	<td class="line x" title="379:617	4.3 Choosing Density Parameters from Subjective-Element Data In Wiebe (1994), whether a PSE is interpreted to be subjective depends, in part, on how subjective the surrounding context is. We explore this idea in the current work, assessing whether PSEs are more likely to be subjective if they are surrounded by subjective elements." ></td>
	<td class="line x" title="380:617	In particular, we experiment with a density feature to decide whether or not a PSE instance is subjective: If a sufficient number of subjective elements are nearby, then the PSE instance is considered to be subjective; otherwise, it is discarded." ></td>
	<td class="line x" title="381:617	The density parameters are a window size W and a frequency threshold T. In this section, we explore the density of manually annotated PSEs in subjectiveelement data and choose density parameters to use in Section 4.4, in which we apply them to automatically identified PSEs in opinion piece data." ></td>
	<td class="line x" title="382:617	The process for calculating density in the subjective-element data is given in Figure 3." ></td>
	<td class="line x" title="383:617	The PSEs are defined to be all adjectives, verbs, modals, nouns, and adverbs that appear at least once in a subjective element, with the exception of some stop words (line 0 of Figure 3)." ></td>
	<td class="line x" title="384:617	Note that these PSEs depend only on the subjective-element manual annotations, not on the automatically identified features used elsewhere in the article or on the document-level opinion piece classes." ></td>
	<td class="line x" title="385:617	PSEinsts is the set of PSE instances to be disambiguated (line 1)." ></td>
	<td class="line x" title="386:617	HiDensity (initialized on line 2) will be the subset of PSEinsts that are retained." ></td>
	<td class="line x" title="387:617	In the loop, the density of each PSE instance P is calculated." ></td>
	<td class="line x" title="388:617	This is the number of subjective elements that begin or end in the W words preceding or following P (line 6)." ></td>
	<td class="line x" title="389:617	P is retained if its density is at least T (line 7)." ></td>
	<td class="line x" title="390:617	Lines 89 of the algorithm assess the precision of the original (PSEinsts) and new (HiDensity) sets of PSE instances." ></td>
	<td class="line x" title="391:617	If prec(HiDensity) is greater than prec(PSEinsts), then 296 Computational Linguistics Volume 30, Number 3 Table 10 Most frequent entry in the top three precision intervals for each subjective-element data set." ></td>
	<td class="line x" title="392:617	WSJ-SE1-M WSJ-SE1-D WSJ-SE2-M WSJ-SE2-D NG-SE Baseline freq 1,566 1,245 1,167 1,108 3,303 Baseline prec .49 .47 .41 .36 .51 Range .87.92 .951.0 .951.0 .951.0 .951.0 T, W 10, 20 12, 50 20, 50 14, 100 10, 10 freq 76 12 1 1 3 prec .89 1.0 1.0 1.0 1.0 Range .82.87 .90.95 .73.78 .51.56 .67.72 T, W 6, 10 12, 60 46, 190 22, 370 26, 90 freq 63 22 53 221 664 prec .84 .91 .78 .51 .67 Range .77.82 .84.89 .66.71 .46.51 .63.67 T, W 12, 40 12, 80 18, 60 16, 310 8, 30 freq 292 42 53 358 1504 prec .78 .88 .68 .47 .63 there is evidence that the number of subjective elements near a PSE instance is related to its subjectivity in context." ></td>
	<td class="line x" title="393:617	To create more data points for this analysis, WSJ-SE was split into two (WSJ-SE1 and WSJ-SE2) and annotations of the two judges are considered separately." ></td>
	<td class="line x" title="394:617	WSJ-SE2-D, for example, refers to Ds annotations of WSJ-SE2." ></td>
	<td class="line x" title="395:617	The process in Figure 3 was repeated for different parameter settings (T in [1, 2, 4,,48] and W in [1, 10, 20,, 490]) on each of the SE data sets." ></td>
	<td class="line x" title="396:617	To find good parameter settings, the results for each data set were sorted into five-point precision intervals and then sorted by frequency within each interval." ></td>
	<td class="line x" title="397:617	Information for the top three precision intervals for each data set are shown in Table 10, specifically, the parameter values (i.e. , T and W) and the frequency and precision of the most frequent result in each interval." ></td>
	<td class="line x" title="398:617	The intervals are in the rows labeled Range." ></td>
	<td class="line x" title="399:617	For example, the top three precision intervals for WSJ-SE1-M, 0.87-0.92, 0.82-0.87, and 0.77-0.82 (no parameter values yield higher precision than 0.92)." ></td>
	<td class="line x" title="400:617	The top of Table 10 gives baseline frequencies and precisions, which are |PSEinsts| and prec(PSEinsts), respectively, in line 8 of Figure 3." ></td>
	<td class="line x" title="401:617	The parameter values exhibit a range of frequencies and precisions, with the expected trade-off between precision and frequency." ></td>
	<td class="line x" title="402:617	We choose the following parameters to test in Section 4.4: For each data set, for each precision interval whose lower bound is at least 10 percentage points higher than the baseline for that data set, the top two (T, W) pairs yielding the highest frequencies in that interval are chosen." ></td>
	<td class="line x" title="403:617	Among the five data sets, a total of 45 parameter pairs were so selected." ></td>
	<td class="line x" title="404:617	This exercise was completed once, without experimenting with different parameter settings." ></td>
	<td class="line x" title="405:617	4.4 Density for Disambiguation In this section, density is exploited to find subjective instances of automatically identified PSEs." ></td>
	<td class="line x" title="406:617	The process is shown in Figure 4." ></td>
	<td class="line x" title="407:617	There are only two differences between the algorithms in Figures 3 and 4." ></td>
	<td class="line x" title="408:617	First, in Figure 3, density is defined in terms of the number of subjective elements nearby." ></td>
	<td class="line x" title="409:617	However, subjective-element annotations are not available in test data." ></td>
	<td class="line x" title="410:617	Thus in Figure 4, density is defined in terms of the 297 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language 0." ></td>
	<td class="line x" title="411:617	PSEinsts = the set of instances in the test data of all PSEs described in Section 3 1." ></td>
	<td class="line x" title="412:617	HiDensity = {} 2." ></td>
	<td class="line x" title="413:617	For P in PSEinsts: 3." ></td>
	<td class="line x" title="414:617	leftWin(P) = the W words before P 4." ></td>
	<td class="line x" title="415:617	rightWin(P) = the W words after P 5." ></td>
	<td class="line x" title="416:617	density(P) = number of PSEinsts whose first or last word is in leftWin(P) or rightWin(P) 6." ></td>
	<td class="line x" title="417:617	if density(P)  T: HiDensity = HiDensity {P} 7." ></td>
	<td class="line x" title="418:617	prec(PSEinsts)= #ofPSEinsts in OPs |PSEinsts| 8." ></td>
	<td class="line x" title="419:617	prec(HiDensity)= #ofHiDensity in OPs |HiDensity| Figure 4 Algorithm for calculating density in opinion piece (OP) data number of other PSE instances nearby, where PSEinsts consists of all instances of the automatically identified PSEs described in Section 3, for which results are given in Table 9." ></td>
	<td class="line x" title="420:617	Second, in Figure 4, we assess precision with respect to the document-level classes (lines 78)." ></td>
	<td class="line x" title="421:617	The test data are OP1." ></td>
	<td class="line x" title="422:617	An interesting question arose when we were defining the PSE instances: What should be done with words that are identified to be PSEs (or parts of PSEs) according to multiple criteria?" ></td>
	<td class="line x" title="423:617	For example, sunny, radiant, and exhilarating are all unique in corpus OP1, and are all members of the adjective PSE feature defined for testing on OP1." ></td>
	<td class="line x" title="424:617	Collocations add additional complexity." ></td>
	<td class="line x" title="425:617	For example, consider the sequence and splendidly, which appears in the test data." ></td>
	<td class="line x" title="426:617	The sequence and splendidly matches the ugen-2-gram (and-conj U-adj), and the word splendidly is unique." ></td>
	<td class="line x" title="427:617	In addition, a sequence may match more than one n-gram feature." ></td>
	<td class="line x" title="428:617	For example, is it that matches three fixed-n-gram features: is it, is it that, and it that." ></td>
	<td class="line x" title="429:617	In the current experiments, the more PSEs a word matches, the more weight it is given." ></td>
	<td class="line x" title="430:617	The hypothesis behind this treatment is that additional matches represent additional evidence that a PSE instance is subjective." ></td>
	<td class="line x" title="431:617	This hypothesis is realized as follows: Each match of each member of each type of PSE is considered to be a PSE instance." ></td>
	<td class="line x" title="432:617	Thus, among them, there are 11 members in PSEinsts for the five phrases sunny, radiant, exhilarating, and splendidly, and is it that, one for each of the matches mentioned above." ></td>
	<td class="line x" title="433:617	The process in Figure 4 was conducted with the 45 parameter pair values (T and W) chosen from the subjective-element data as described in Section 4.3." ></td>
	<td class="line x" title="434:617	Table 11 shows results for a subset of the 45 parameters, namely, the most frequent parameter pair chosen from the top three precision intervals for each training set." ></td>
	<td class="line x" title="435:617	The bottom of the table gives a baseline frequency and a baseline precision in OP1, defined as |PSEinsts| and prec(PSEinsts), respectively, in line 7 of Figure 4." ></td>
	<td class="line x" title="436:617	The density features result in substantial increases in precision." ></td>
	<td class="line x" title="437:617	Of the 45 parameter pairs, the minimum percentage increase over baseline is 22%." ></td>
	<td class="line x" title="438:617	Fully 24% of the 45 parameter pairs yield increases of 200% or more; 38% yield increases between 100% 298 Computational Linguistics Volume 30, Number 3 Table 11 Results for high-density PSEs in test data OP1 using parameters chosen from subjective-element data." ></td>
	<td class="line x" title="439:617	WSJ-SE1-M WSJ-SE1-D WSJ-SE2-M WSJ-SE2-D NG-SE T, W 10, 20 12, 50 20, 50 14, 100 10, 10 freq 237 3,176 170 10,510 8 prec .87 .72 .97 .57 1.0 T, W 6, 10 12, 60 46, 190 22, 370 26, 90 freq 459 5,289 1,323 21,916 787 prec .68 .68 .95 .37 .92 T, W 12, 40 12, 80 18, 60 16, 310 8, 30 freq 1,398 9,662 906 24,454 3,239 prec .79 .58 .87 .34 .67 PSE baseline: freq = 30,938, prec = .28 and 199%, and 38% yield increases between 22% and 99%." ></td>
	<td class="line x" title="440:617	In addition, the increases are significant." ></td>
	<td class="line x" title="441:617	Using the set of high-density PSEs defined by the parameter pair with the least increase over baseline, we tested the difference in the proportion of PSEs in opinion pieces that are high-density and the proportion of PSEs in nonopinion pieces that are high-density." ></td>
	<td class="line x" title="442:617	The difference between these two proportions is highly significant (z = 46.2, p < 0.0001)." ></td>
	<td class="line x" title="443:617	Notice that, except for one blip (T, W = 6, 10 under WSJ-SE-M), the precisions decrease and the frequencies increase as we go down each column in Table 11." ></td>
	<td class="line x" title="444:617	The same pattern can be observed with all 45 parameter pairs (results not included here because of space considerations)." ></td>
	<td class="line x" title="445:617	But the parameter pairs are ordered in Table 11 based on performance in the manually annotated subjective-element data, not based on performance in the test data." ></td>
	<td class="line x" title="446:617	For example, the entry in the first row, first column (T, W = 10, 20) is the parameter pair giving the highest frequency in the top precision interval of WSJ-SE-M (frequency and precision in WSJ-SE-M, using the process of Figure 3)." ></td>
	<td class="line x" title="447:617	Thus, the relative precisions and frequencies of the parameter pairs are carried over from the training to the test data." ></td>
	<td class="line x" title="448:617	This is quite a strong result, given that the PSEs in the training data are from manual annotations, while the PSEs in the test data are our automatically identified features." ></td>
	<td class="line x" title="449:617	4.5 High-Density Sentence Annotations To assess the subjectivity of sentences with high-density PSEs, we extracted the 133 sentences in corpus OP2 that contain at least one high-density PSE and manually annotated them." ></td>
	<td class="line x" title="450:617	We refer to these sentences as the system-identified sentences." ></td>
	<td class="line x" title="451:617	We chose the density-parameter pair (T, W = 12, 30), based on its precision and frequency in OP1." ></td>
	<td class="line x" title="452:617	This parameter setting yields results that have relatively high precision and low frequency." ></td>
	<td class="line x" title="453:617	We chose a low-frequency setting to make the annotation study feasible." ></td>
	<td class="line x" title="454:617	The extracted sentences were independently annotated by two judges." ></td>
	<td class="line x" title="455:617	One is a coauthor of this article (judge 1), and the other has performed subjectivity annotation before, but is not otherwise involved in this research (judge 2)." ></td>
	<td class="line x" title="456:617	Sentences were annotated according to the coding instructions of Wiebe, Bruce, and OHara (1999) which, recall, are to classify a sentence as subjective if there is a significant expression of subjectivity of either the writer or someone mentioned in the text, in the sentence." ></td>
	<td class="line x" title="457:617	299 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language Table 12 Examples of system-identified sentences." ></td>
	<td class="line x" title="458:617	(1) The outburst of shooting came nearly two weeks after clashes between Moslem worshippers and oo Somali soldiers." ></td>
	<td class="line x" title="459:617	(2.a) But now the refugees are streaming across the border and alarming the world." ></td>
	<td class="line x" title="460:617	ss (2.b) In the middle of the crisis, Erich Honecker was hospitalized with a gall stone operation." ></td>
	<td class="line x" title="461:617	oo (2.c) It is becoming more and more obvious that his gallstone-age communism is dying with him:  ss (3.a) Not brilliantly, because, after all, this was a performer who was collecting paychecks from lounges ss at Hiltons and Holiday Inns, but creditably and with the air of someone for whom Ten Cents a Dance was more than a bit autobiographical." ></td>
	<td class="line x" title="462:617	(3.b) It was an exercise of blending Michelles singing with Susies singing, explained Ms. Stevens." ></td>
	<td class="line x" title="463:617	oo (4) Enlisted men and lower-grade officers were meat thrown into a grinder." ></td>
	<td class="line x" title="464:617	ss (5) If you believe in God and you believe in miracles, theres nothing particularly crazy about that. ss (6) He was much too eager to create something very weird and dynamic, ss catastrophic and jolly like this great and coily thing Lolita. (7) The Bush approach of mixing confrontation with conciliation strikes some people as sensible, perhaps ss even inevitable, because Mr. Bush faces a Congress firmly in the hands of the opposition." ></td>
	<td class="line x" title="465:617	(8) Still, despite their efforts to convince the world that we are indeed alone, the visitors do seem to keep ss coming and, like the recent sightings, theres often a detail or two that suggests they may actually be a little on the dumb side." ></td>
	<td class="line x" title="466:617	(9) As for the women, theyre pathetic." ></td>
	<td class="line x" title="467:617	ss (10) At this point, the truce between feminism and sensationalism gets might uneasy." ></td>
	<td class="line x" title="468:617	ss (11) MMPIs publishers say the test shouldnt be used alone to diagnose ss psychological problems or in hiring; it should be given in conjunction with other tests." ></td>
	<td class="line x" title="469:617	(12) While recognizing that professional environmentalists may feel threatened, ss I intend to urge that UV-B be monitored whenever I can." ></td>
	<td class="line x" title="470:617	Table 13 Sentence annotation contingency table; judge 1 counts are in rows and judge 2 counts are in columns." ></td>
	<td class="line x" title="471:617	Subjective Objective Unsure Subjective 98 2 3 Objective 2 14 0 Unsure 2 11 1 In addition to the subjective and objective classes, a judge can tag a sentence as unsure if he or she is unsure of his or her rating or considers the sentence to be borderline." ></td>
	<td class="line x" title="472:617	An equal number (133) of other sentences were randomly selected from the corpus to serve as controls." ></td>
	<td class="line x" title="473:617	The 133 system-identified sentences and the 133 control sentences were randomly mixed together." ></td>
	<td class="line x" title="474:617	The judges were asked to annotate all 266 sentences, not knowing which were system-identified and which were control." ></td>
	<td class="line x" title="475:617	Each sentence was presented with the sentence that precedes it and the sentence that follows it in the corpus, to provide some context for interpretation." ></td>
	<td class="line x" title="476:617	Table 12 shows examples of the system-identified sentences." ></td>
	<td class="line x" title="477:617	Sentences classified by both judges as objective are marked oo and those classified by both judges as subjective are marked ss." ></td>
	<td class="line x" title="478:617	300 Computational Linguistics Volume 30, Number 3 Table 14 Examples of subjective sentences adjacent to system-identified sentences." ></td>
	<td class="line x" title="479:617	Bathed in cold sweat, I watched these Dantesque scenes, holding tightly the damp hand of Edek or Waldeck who, like me, were convinced that there was no God." ></td>
	<td class="line x" title="480:617	The Japanese are amazed that a company like this exists in Japan, says Kimindo Kusaka, head of the Softnomics Center, a Japanese management-research organization." ></td>
	<td class="line x" title="481:617	And even if drugs were legal, what evidence do you have that the habitual drug user wouldnt continue to rob and steal to get money for clothes, food or shelter?" ></td>
	<td class="line x" title="482:617	The moral cost of legalizing drugs is great, but it is a cost that apparently lies outside the narrow scope of libertarian policy prescriptions." ></td>
	<td class="line x" title="483:617	I doubt that one exists." ></td>
	<td class="line x" title="484:617	They were upset at his committees attempt to pacify the program critics by cutting the surtax paid by the more affluent elderly and making up the loss by shifting more of the burden to the elderly poor and by delaying some benefits by a year." ></td>
	<td class="line x" title="485:617	Judge 1 classified 103 of the system-identified sentences as subjective, 16 as objective, and 14 as unsure." ></td>
	<td class="line x" title="486:617	Judge 2 classified 102 of the system-identified sentences as subjective, 27 as objective; and 4 as unsure." ></td>
	<td class="line x" title="487:617	The contingency table is given in Table 13." ></td>
	<td class="line x" title="488:617	4 The kappa value using all three classes is 0.60, reflecting the highly skewed distribution in favor of subjective sentences, and the disagreement on the lower-frequency classes (unsure and objective)." ></td>
	<td class="line x" title="489:617	Consistent with the findings in Wiebe, Bruce, and OHara (1999), the kappa value for agreement on the sentences for which neither judge is unsure is very high: 0.86." ></td>
	<td class="line x" title="490:617	A different breakdown of the sentences is illuminating." ></td>
	<td class="line x" title="491:617	For 98 of the sentences (call them SS), judges 1 and 2 tag the sentence as subjective." ></td>
	<td class="line x" title="492:617	Among the other sentences, 20 appear in a block of contiguous system-identified sentences that includes a member of SS." ></td>
	<td class="line x" title="493:617	For example, in Table 12, (2.a) and (2.c) are in SS and (2.b) is in the same block of subjective sentences as they are." ></td>
	<td class="line x" title="494:617	Similarly, (3.a) is in SS and (3.b) is in the same block." ></td>
	<td class="line x" title="495:617	Among the remaining 15 sentences, 6 are adjacent to subjective sentences that were not identified by our system (so were not annotated by the judges)." ></td>
	<td class="line x" title="496:617	All of those sentences contain significant expressions of subjectivity of the writer or someone mentioned in the text, the criterion used in this work for classifying a sentence as subjective." ></td>
	<td class="line x" title="497:617	Samples are shown in Table 14." ></td>
	<td class="line x" title="498:617	Thus, 93% of the sentences identified by the system are subjective or are near subjective sentences." ></td>
	<td class="line x" title="499:617	All the sentences, together with their tags and the sentences adjacent to them, are available on the Web at www.cs.pitt.edu/wiebe." ></td>
	<td class="line x" title="500:617	4.6 Using Features for Opinion Piece Recognition In this section, we assess the usefulness of the PSEs identified in Section 3 and listed in Table 9 by using them to perform document-level classification of opinion pieces." ></td>
	<td class="line x" title="501:617	Opinion-piece classification is a difficult task for two reasons." ></td>
	<td class="line x" title="502:617	First, as discussed in Section 2.1, both opinionated and factual documents tend to be composed of a mixture of subjective and objective language." ></td>
	<td class="line x" title="503:617	Second, the natural distribution of documents in our data is heavily skewed toward nonopinion pieces." ></td>
	<td class="line x" title="504:617	Despite these hurdles, using only 4 In contrast, Judge 1 classified only 53 (45%) of the control sentences as subjective, and Judge 2 classified only 47 (36%) of them as subjective." ></td>
	<td class="line x" title="505:617	301 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language our PSEs, we achieve positive results in opinion-piece classification using the basic knearest-neighbor (KNN) algorithm with leave-one-out cross-validation (Mitchell 1997)." ></td>
	<td class="line x" title="506:617	Given a document, the basic KNN algorithm classifies the document according to the majority classification of the documents k closest neighbors." ></td>
	<td class="line x" title="507:617	For our purposes, each document is characterized by one feature, the count of all PSE instances (regardless of type) in the document, normalized by document length in words." ></td>
	<td class="line x" title="508:617	The distance between two documents is simply the absolute value of the difference between the normalized PSE counts for the two documents." ></td>
	<td class="line x" title="509:617	With leave-one-out cross-validation, the set of n documents to be classified is divided into a training set of size n1 and a validation set of size 1." ></td>
	<td class="line x" title="510:617	The one document in the validation set is then classified according to the majority classification of its k closest-neighbor documents in the training set." ></td>
	<td class="line x" title="511:617	This process is repeated until every document is classified." ></td>
	<td class="line x" title="512:617	Which value to use for k is chosen during a preprocessing phase." ></td>
	<td class="line x" title="513:617	During the preprocessing phase, we run the KNN algorithm with leave-one-out cross-validation on a separate training set, for odd values of k from 1 to 15." ></td>
	<td class="line x" title="514:617	The value of k that results in the best classification during the preprocessing phase is the one used for later KNN classification." ></td>
	<td class="line x" title="515:617	For the classification experiment, the data set OP1 was used in the preprocessing phase to select the value of k, and then classification was performed on the 1,222 documents in OP2." ></td>
	<td class="line x" title="516:617	During training on OP1, k equal to 15 resulted in the best classification." ></td>
	<td class="line x" title="517:617	On the test set, OP2, we achieved a classification accuracy of 0.939; the baseline accuracy for choosing the most frequent class (nonopinion pieces) was 0.915." ></td>
	<td class="line x" title="518:617	Our classification accuracy represents a 28% reduction in error and is significantly better than baseline according to McNemars test (Everitt 1997)." ></td>
	<td class="line x" title="519:617	The positive results from the opinion piece classification show the usefulness of the various PSE features when used together." ></td>
	<td class="line x" title="520:617	5." ></td>
	<td class="line x" title="521:617	Relation to Other Work There has been much work in other fields, including linguistics, literary theory, psychology, philosophy, and content analysis, involving subjective language." ></td>
	<td class="line x" title="522:617	As mentioned in Section 2, the conceptualization underlying our manual annotations is based on work in literary theory and linguistics, most directly Dolezel (1973), Uspensky (1973), Kuroda (1973, 1976), Chatman (1978), Cohn (1978), Fodor (1979), and Banfield (1982)." ></td>
	<td class="line x" title="523:617	We also mentioned existing knowledge resources such as affective lexicons (General-Inquirer 2000; Heise 2000) and annotations in more general-purpose lexicons (e.g. , the attitude adverb features in Comlex [Macleod, Grishman, and Meyers 1998])." ></td>
	<td class="line x" title="524:617	Such knowledge may be used in future work to complement the work presented in this article, for example, to seed the distributional-similarity process described in Section 3.4." ></td>
	<td class="line x" title="525:617	There is also work in fields such as content analysis and psychology on statistically characterizing texts in terms of word lists manually developed for distinctions related to subjectivity." ></td>
	<td class="line x" title="526:617	For example, Hart (1984) performs counts on a manually developed list of words and rhetorical devices (e.g. , sacred terms such as freedom) in political speeches to explore potential reasons for public reactions." ></td>
	<td class="line x" title="527:617	Anderson and McMaster (1998) use fixed sets of high-frequency words to assign connotative scores to documents and sections of documents along dimensions such as how pleasant, acrimonious, pious, or confident, the text is. What distinguishes our work from work on subjectivity in other fields is that we focus on (1) automatically learning knowledge from corpora, (2) automatically 302 Computational Linguistics Volume 30, Number 3 performing contextual disambiguation, and (3) using knowledge of subjectivity in NLP applications." ></td>
	<td class="line x" title="528:617	This article expands and integrates the work reported in Wiebe and Wilson (2002), Wiebe, Wilson, and Bell (2001), Wiebe et al.(2001) and Wiebe (2000)." ></td>
	<td class="line x" title="530:617	Previous work in NLP on the same or related tasks includes sentence-level and document-level subjectivity classifications." ></td>
	<td class="line x" title="531:617	At the sentence level, Wiebe, Bruce, and OHara (1999) developed a machine learning system to classify sentences as subjective or objective." ></td>
	<td class="line x" title="532:617	The accuracy of the system was more than 20 percentage points higher than a baseline accuracy." ></td>
	<td class="line x" title="533:617	Five part-of-speech features, two lexical features, and a paragraph feature were used." ></td>
	<td class="line x" title="534:617	These results suggested to us that there are clues to subjectivity that might be learned automatically from text and motivated the work reported in the current article." ></td>
	<td class="line x" title="535:617	The system was tested in 10-fold cross validation experiments using corpus WSJ-SE, a small corpus of only 1,001 sentences." ></td>
	<td class="line x" title="536:617	As discussed in Section 1, a main goal of our current work is to exploit existing document-level annotations, because they enable us to use much larger data sets, they were created outside our research group, and they allow us to assess consistency of performance by cross-validating between our manual annotations and the existing document-level annotations." ></td>
	<td class="line x" title="537:617	Because the document-level data are not annotated at the sentence level, sentence-level classification is not highlighted in this article." ></td>
	<td class="line x" title="538:617	The new sentence annotation study to evaluate sentences with high-density features (Section 4.5) uses different data from WSJ-SE, because some of the features (n-grams and density parameters) were identified using WSJ-SE as training data." ></td>
	<td class="line x" title="539:617	Other previous work in NLP has addressed related document-level classifications." ></td>
	<td class="line x" title="540:617	Spertus (1997) developed a system for recognizing inflammatory messages." ></td>
	<td class="line x" title="541:617	As mentioned earlier in the article, inflammatory language is a type of subjective language, so the task she addresses is closely related to ours." ></td>
	<td class="line x" title="542:617	She uses machine learning to select among manually developed features." ></td>
	<td class="line x" title="543:617	In contrast, the focus in our work is on automatically identifying features from the data." ></td>
	<td class="line x" title="544:617	A number of projects investigating genre detection include editorials as one of the targeted genres." ></td>
	<td class="line x" title="545:617	For example, in Karlgren and Cutting (1994), editorials are one of fifteen categories, and in Kessler, Nunberg, and Sch utze (1997), editorials are one of six." ></td>
	<td class="line x" title="546:617	Given the goal of these works to perform genre detection in general, they use low-level features that are not specific to editorials." ></td>
	<td class="line x" title="547:617	Neither shows significant improvements for editorial recognition." ></td>
	<td class="line x" title="548:617	Argamon, Koppel, and Avneri (1998) address a slightly different task, though it does involve editorials." ></td>
	<td class="line x" title="549:617	Their goal is to distinguish not only, for example, news from editorials, but also these categories in different publications." ></td>
	<td class="line x" title="550:617	Their best results are distinguishing among the news categories of different publications; their lowest results involve editorials." ></td>
	<td class="line x" title="551:617	Because we focus specifically on distinguishing opinion pieces from nonopinion pieces, our results are better than theirs for those categories." ></td>
	<td class="line x" title="552:617	In addition, in contrast to the above studies, the focus of our work is on learning features of subjectivity." ></td>
	<td class="line x" title="553:617	We perform opinion piece recognition in order to assess the usefulness of the various features when used together." ></td>
	<td class="line x" title="554:617	Other previous NLP research has used features similar to ours for other NLP tasks." ></td>
	<td class="line x" title="555:617	Low-frequency words have been used as features in information extraction (Weeber, Vos, and Baayen 2000) and text categorization (Copeck et al. 2000)." ></td>
	<td class="line x" title="556:617	A number of researchers have worked on mining collocations from text to extend lexicographic resources for machine translation and word sense disambiguation (e.g. , Smajda 1993; Lin 1999; Biber 1993)." ></td>
	<td class="line x" title="557:617	In Samuel, Carberry, and Vijay-Shankers (1998) work on identifying collocations for dialog-act recognition, a filter similar to ours was used to eliminate redundant n-gram features: n-grams were eliminated if they contained substrings with the same entropy score as or a better entropy score than the n-gram." ></td>
	<td class="line x" title="558:617	303 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language While it is common in studies of collocations to omit low-frequency words and expressions from analysis, because they give rise to invalid or unrealistic statistical measures (Church and Hanks, 1990), we are able to identify higher-precision collocations by including placeholders for unique words (i.e. , the ugen-n-grams)." ></td>
	<td class="line x" title="559:617	We are not aware of other work that uses such collocations as we do." ></td>
	<td class="line x" title="560:617	Features identified using distributional similarity have previously been used for syntactic and semantic disambiguation (Hindle 1990; Dagan, Pereira, and Lee 1994) and to develop lexical resources from corpora (Lin 1998; Riloff and Jones 1999)." ></td>
	<td class="line x" title="561:617	We are not aware of other work identifying and using density parameters as described in this article." ></td>
	<td class="line x" title="562:617	Since our experiments, other related work in NLP has been performed." ></td>
	<td class="line x" title="563:617	Some of this work addresses related but different classification tasks." ></td>
	<td class="line oc" title="564:617	Three studies classify reviews as positive or negative (Turney 2002; Pang, Lee, and Vaithyanathan 2002; Dave, Lawrence, Pennock 2003)." ></td>
	<td class="line x" title="565:617	The input is assumed to be a review, so this task does not include finding subjective documents in the first place." ></td>
	<td class="line x" title="566:617	The first study listed above (Turney 2002) uses a variation of the semantic similarity procedure presented in Wiebe (2000) (Section 3.4)." ></td>
	<td class="line x" title="567:617	The third (Dave, Lawrence, and Pennock 2003) uses ngram features identified with a variation of the procedure presented in Wiebe, Wilson, and Bell (2001) (Section 3.3)." ></td>
	<td class="line x" title="568:617	Tong (2001) addresses finding sentiment timelines, that is, tracking sentiments over time in multiple documents." ></td>
	<td class="line x" title="569:617	For clues of subjectivity, he uses manually developed lexical rules, rather than automatically learning them from corpora." ></td>
	<td class="line x" title="570:617	Similarly, Gordon et al.(2003) use manually developed grammars to detect some types of subjective language." ></td>
	<td class="line x" title="572:617	Agrawal et al.(2003) partition newsgroup authors into camps based on quotation links." ></td>
	<td class="line x" title="574:617	They do not attempt to recognize subjective language." ></td>
	<td class="line x" title="575:617	The most closely related new work is Riloff, Wiebe, and Wilson (2003), Riloff and Wiebe (2003) and Yu and Hatzivassiloglou (2003)." ></td>
	<td class="line x" title="576:617	The first two focus on finding additional types of subjective clues (nouns and extraction patterns identified using extraction pattern bootstrapping)." ></td>
	<td class="line x" title="577:617	Yu and Hatzivassiloglou (2003) perform opinion text classification." ></td>
	<td class="line x" title="578:617	They also use existing WSJ document classes for training and testing, but they do not include the entire corpus in their experiments, as we do." ></td>
	<td class="line x" title="579:617	Their opinion piece class consists only of editorials and letters to the editor, and their nonopinion class consists only of business and news." ></td>
	<td class="line x" title="580:617	They report an average F-measure of 96.5%." ></td>
	<td class="line x" title="581:617	Our result of 94% accuracy on document level classification is almost comparable." ></td>
	<td class="line x" title="582:617	They also perform sentence-level classification." ></td>
	<td class="line x" title="583:617	We anticipate that knowledge of subjective language may be usefully exploited in a number of NLP application areas and hope that the work presented in this article will encourage others to experiment with subjective language in their applications." ></td>
	<td class="line x" title="584:617	More generally, there are many types of artificial intelligence systems for which state-ofaffairs types such as beliefs and desires are central, including systems that perform plan recognition for understanding narratives (Dyer 1982; Lehnert et al. 1983), for argument understanding (Alvarado, Dyer, and Flowers 1986), for understanding stories from different perspectives (Carbonell 1979), and for generating language under different pragmatic constraints (Hovy 1987)." ></td>
	<td class="line x" title="585:617	Knowledge of linguistic subjectivity could enhance the abilities of such systems to recognize and generate expressions referring to such states of affairs in natural text." ></td>
	<td class="line x" title="586:617	6." ></td>
	<td class="line x" title="587:617	Conclusions Knowledge of subjective language promises to be beneficial for many NLP applications including information extraction, question answering, text categorization, and 304 Computational Linguistics Volume 30, Number 3 summarization." ></td>
	<td class="line x" title="588:617	This article has presented the results of an empirical study in acquiring knowledge of subjective language from corpora in which a number of feature types were learned and evaluated on different types of data with positive results." ></td>
	<td class="line x" title="589:617	We showed that unique words are subjective more often than expected and that unique words are valuable clues to subjectivity." ></td>
	<td class="line x" title="590:617	We also presented a procedure for automatically identifying potentially subjective collocations, including fixed collocations and collocations with placeholders for unique words." ></td>
	<td class="line x" title="591:617	In addition, we used the results of a method for clustering words according to distributional similarity (Lin 1998) to identify adjectival and verbal clues of subjectivity." ></td>
	<td class="line x" title="592:617	Table 9 summarizes the results of testing all of the above types of PSEs." ></td>
	<td class="line x" title="593:617	All show increased precision in the evaluations." ></td>
	<td class="line x" title="594:617	Together, they show consistency in performance." ></td>
	<td class="line x" title="595:617	In almost all cases they perform better or worse on the same data sets, despite the fact that different kinds of data and procedures are used to learn them." ></td>
	<td class="line x" title="596:617	In addition, PSEs learned using expression-level subjective-element data have precisions higher than baseline on document-level opinion piece data, and vice versa." ></td>
	<td class="line x" title="597:617	Having a large stable of PSEs, it was important to disambiguate whether or not PSE instances are subjective in the contexts in which they appear." ></td>
	<td class="line x" title="598:617	We discovered that the density of other potentially subjective expressions in the surrounding context is important." ></td>
	<td class="line x" title="599:617	If a clue is surrounded by a sufficient number of other clues, then it is more likely to be subjective than if there were not." ></td>
	<td class="line x" title="600:617	Parameter values were selected using training data manually annotated at the expression level for subjective elements and then tested on data annotated at the document level for opinion pieces." ></td>
	<td class="line x" title="601:617	All of the selected parameters led to increases in precision on the test data, and most lead to increases over 100%." ></td>
	<td class="line x" title="602:617	Once again we found consistency between expression-level and document-level annotations." ></td>
	<td class="line x" title="603:617	PSE sets defined by density have high precision in both the subjective-element data and the opinion piece data." ></td>
	<td class="line x" title="604:617	The large differences between training and testing suggest that our results are not brittle." ></td>
	<td class="line x" title="605:617	Using a density feature selected from a training set, sentences containing highdensity PSEs were extracted from a separate test set, and manually annotated by two judges." ></td>
	<td class="line x" title="606:617	Fully 93% of the sentences extracted were found to be subjective or to be near subjective sentences." ></td>
	<td class="line x" title="607:617	Admittedly, the chosen density feature is a high-precision, lowfrequency one." ></td>
	<td class="line x" title="608:617	But since the process is fully automatic, the feature could be applied to more unannotated text to identify regions containing subjective sentences." ></td>
	<td class="line x" title="609:617	In addition, because the precision and frequency of the density features are stable across data sets, lower-precision but higher-frequency options are available." ></td>
	<td class="line x" title="610:617	Finally, the value of the various types of PSEs was demonstrated with the task of opinion piece classification." ></td>
	<td class="line x" title="611:617	Using the k-nearest-neighbor classification algorithm with leave-one-out cross-validation, a classification accuracy of 94% was achieved on a large test set, with a reduction in error of 28% from the baseline." ></td>
	<td class="line x" title="612:617	Future work is required to determine how to exploit density features to improve the performance of text categorization algorithms." ></td>
	<td class="line x" title="613:617	Another area of future work is searching for clues to objectivity, such as the politeness features used by Spertus (1997)." ></td>
	<td class="line x" title="614:617	Still another is identifying the type of a subjective expression (e.g. , positive or negative evaluative), extending work such as Hatzivassiloglou and McKeown (1997) on classifying lexemes to the classification of instances in context (compare, e.g., great! and oh great.) In addition, it would be illuminating to apply our system to data annotated with discourse trees (Carlson, Marcu, and Okurowski 2001)." ></td>
	<td class="line x" title="615:617	We hypothesize that most objective sentences identified by our system are dominated in the discourse by subjective sentences and that we are moving toward identifying subjective discourse segments." ></td>
	<td class="line x" title="616:617	305 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language Acknowledgments We thank the anonymous reviewers for their helpful and constructive comments." ></td>
	<td class="line x" title="617:617	This research was supported in part by the Office of Naval Research under grants N00014-95-1-0776 and N00014-01-1-0381." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P04-1034
The Sentimental Factor: Improving Review Classification Via Human-Provided Information
Beineke, Philip;Hastie, Trevor;Vaithyanathan, Shivakumar;"></td>
	<td class="line x" title="1:179	The Sentimental Factor: Improving Review Classification via Human-Provided Information Philip Beineke and Trevor Hastie Dept. of Statistics Stanford University Stanford, CA 94305 Shivakumar Vaithyanathan IBM Almaden Research Center 650 Harry Rd. San Jose, CA 95120-6099 Abstract Sentiment classification is the task of labeling a review document according to the polarity of its prevailing opinion (favorable or unfavorable)." ></td>
	<td class="line x" title="2:179	In approaching this problem, a model builder often has three sources of information available: a small collection of labeled documents, a large collection of unlabeled documents, and human understanding of language." ></td>
	<td class="line x" title="3:179	Ideally, a learning method will utilize all three sources." ></td>
	<td class="line x" title="4:179	To accomplish this goal, we generalize an existing procedure that uses the latter two." ></td>
	<td class="line x" title="5:179	We extend this procedure by re-interpreting it as a Naive Bayes model for document sentiment." ></td>
	<td class="line x" title="6:179	Viewed as such, it can also be seen to extract a pair of derived features that are linearly combined to predict sentiment." ></td>
	<td class="line x" title="7:179	This perspective allows us to improve upon previous methods, primarily through two strategies: incorporating additional derived features into the model and, where possible, using labeled data to estimate their relative influence." ></td>
	<td class="line x" title="8:179	1 Introduction Text documents are available in ever-increasing numbers, making automated techniques for information extraction increasingly useful." ></td>
	<td class="line x" title="9:179	Traditionally, most research effort has been directed towards objective information, such as classification according to topic; however, interest is growing in producing information about the opinions that a document contains; for instance, Morinaga et al.(2002)." ></td>
	<td class="line x" title="11:179	In March, 2004, the American Association for Artificial Intelligence held a symposium in this area, entitled Exploring Affect and Attitude in Text. One task in opinion extraction is to label a review document d according to its prevailing sentiment s 2 f 1; 1g (unfavorable or favorable)." ></td>
	<td class="line oc" title="12:179	Several previous papers have addressed this problem by building models that rely exclusively upon labeled documents, e.g. Pang et al.(2002), Dave et al.(2003)." ></td>
	<td class="line n" title="15:179	By learning models from labeled data, one can apply familiar, powerful techniques directly; however, in practice it may be difficult to obtain enough labeled reviews to learn model parameters accurately." ></td>
	<td class="line x" title="16:179	A contrasting approach (Turney, 2002) relies only upon documents whose labels are unknown." ></td>
	<td class="line x" title="17:179	This makes it possible to use a large underlying corpus  in this case, the entire Internet as seen through the AltaVista search engine." ></td>
	<td class="line x" title="18:179	As a result, estimates for model parameters are subject to a relatively small amount of random variation." ></td>
	<td class="line x" title="19:179	The corresponding drawback to such an approach is that its predictions are not validated on actual documents." ></td>
	<td class="line x" title="20:179	In machine learning, it has often been effective to use labeled and unlabeled examples in tandem, e.g. Nigam et al.(2000)." ></td>
	<td class="line x" title="22:179	Turneys model introduces the further consideration of incorporating human-provided knowledge about language." ></td>
	<td class="line x" title="23:179	In this paper we build models that utilize all three sources: labeled documents, unlabeled documents, and human-provided information." ></td>
	<td class="line x" title="24:179	The basic concept behind Turneys model is quite simple." ></td>
	<td class="line x" title="25:179	The sentiment orientation (Hatzivassiloglou and McKeown, 1997) of a pair of words is taken to be known." ></td>
	<td class="line x" title="26:179	These words serve as anchors for positive and negative sentiment." ></td>
	<td class="line x" title="27:179	Words that co-occur more frequently with one anchor than the other are themselves taken to be predictive of sentiment." ></td>
	<td class="line x" title="28:179	As a result, information about a pair of words is generalized to many words, and then to documents." ></td>
	<td class="line x" title="29:179	In the following section, we relate this model with Naive Bayes classification, showing that Turneys classifier is a pseudo-supervised approach: it effectively generates a new corpus of labeled documents, upon which it fits a Naive Bayes classifier." ></td>
	<td class="line x" title="30:179	This insight allows the procedure to be represented as a probability model that is linear on the logistic scale, which in turn suggests generalizations that are developed in subsequent sections." ></td>
	<td class="line x" title="31:179	2 A Logistic Model for Sentiment 2.1 Turneys Sentiment Classifier In Turneys model, the sentiment orientation of word w is estimated as follows." ></td>
	<td class="line x" title="32:179	^ (w) = log N(w;excellent)=NexcellentN (w;poor)=Npoor (1) Here, Na is the total number of sites on the Internet that contain an occurrence of a  a feature that can be a word type or a phrase." ></td>
	<td class="line x" title="33:179	N(w;a) is the number of sites in which features w and a appear near each other, i.e. in the same passage of text, within a span of ten words." ></td>
	<td class="line x" title="34:179	Both numbers are obtained from the hit count that results from a query of the AltaVista search engine." ></td>
	<td class="line x" title="35:179	The rationale for this estimate is that words that express similar sentiment often co-occur, while words that express conflicting sentiment cooccur more rarely." ></td>
	<td class="line x" title="36:179	Thus, a word that co-occurs more frequently with excellent than poor is estimated to have a positive sentiment orientation." ></td>
	<td class="line x" title="37:179	To extrapolate from words to documents, the estimated sentiment ^s 2 f 1; 1g of a review document d is the sign of the average sentiment orientation of its constituent features.1 To represent this estimate formally, we introduce the following notation: W is a dictionary of features: (w1;::: ;wp)." ></td>
	<td class="line x" title="38:179	Each features respective sentiment orientation is represented as an entry in the vector ^ of length p: ^ j = ^ (wj) (2) Given a collection of n review documents, the i-th each di is also represented as a vector of length p, with dij equal to the number of times that feature wj occurs in di." ></td>
	<td class="line x" title="39:179	The length of a document is its total number of features, jdij = Ppj=1 dij." ></td>
	<td class="line x" title="40:179	Turneys classifier for the i-th documents sentiment si can now be written: ^si = sign Pp j=1 ^ jdij jdij !" ></td>
	<td class="line x" title="41:179	(3) Using a carefully chosen collection of features, this classifier produces correct results on 65.8% of a collection of 120 movie reviews, where 60 are labeled positive and 60 negative." ></td>
	<td class="line x" title="42:179	Although this is not a particularly encouraging result, movie reviews tend to be a difficult domain." ></td>
	<td class="line x" title="43:179	Accuracy on sentiment classification in other domains exceeds 80% (Turney, 2002)." ></td>
	<td class="line x" title="44:179	1Note that not all words or phrases need to be considered as features." ></td>
	<td class="line x" title="45:179	In Turney (2002), features are selected according to part-of-speech labels." ></td>
	<td class="line x" title="46:179	2.2 Naive Bayes Classification Bayes Theorem provides a convenient framework for predicting a binary response s 2 f 1; 1g from a feature vector x: Pr(s = 1jx) = Pr(xjs = 1) 1P k2f 1;1g Pr(xjs = k) k (4) For a labeled sample of data (xi;si);i = 1;:::;n, a classs marginal probability k can be estimated trivially as the proportion of training samples belonging to the class." ></td>
	<td class="line x" title="47:179	Thus the critical aspect of classification by Bayes Theorem is to estimate the conditional distribution of x given s. Naive Bayes simplifies this problem by making a naive assumption: within a class, the different feature values are taken to be independent of one another." ></td>
	<td class="line x" title="48:179	Pr(xjs) = Y j Pr(xjjs) (5) As a result, the estimation problem is reduced to univariate distributions." ></td>
	<td class="line x" title="49:179	Naive Bayes for a Multinomial Distribution We consider a bag of words model for a document that belongs to class k, where features are assumed to result from a sequence of jdij independent multinomial draws with outcome probability vector qk = (qk1;::: ;qkp)." ></td>
	<td class="line x" title="50:179	Given a collection of documents with labels, (di;si);i = 1;::: ;n, a natural estimate for qkj is the fraction of all features in documents of class k that equal wj: ^qkj = P i:si=k dijP i:si=k jdij (6) In the two-class case, the logit transformation provides a revealing representation of the class posterior probabilities of the Naive Bayes model." ></td>
	<td class="line x" title="51:179	dlogit(sjd), log cPr(s = 1jd)c Pr(s = 1jd) (7) = log ^ 1^ 1 + pX j=1 dj log ^q1j^q 1j (8) = ^ 0 + pX j=1 dj ^ j (9) where ^ 0 = log ^ 1^ 1 (10) ^ j = log ^q1j^q 1j (11) Observe that the estimate for the logit in Equation 9 has a simple structure: it is a linear function of d. Models that take this form are commonplace in classification." ></td>
	<td class="line x" title="52:179	2.3 Turneys Classifier as Naive Bayes Although Naive Bayes classification requires a labeled corpus of documents, we show in this section that Turneys approach corresponds to a Naive Bayes model." ></td>
	<td class="line x" title="53:179	The necessary documents and their corresponding labels are built from the spans of text that surround the anchor words excellent and poor." ></td>
	<td class="line x" title="54:179	More formally, a labeled corpus may be produced by the following procedure: 1." ></td>
	<td class="line x" title="55:179	For a particular anchor ak, locate all of the sites on the Internet where it occurs." ></td>
	<td class="line x" title="56:179	2." ></td>
	<td class="line x" title="57:179	From all of the pages within a site, gather the features that occur within ten words of an occurrence of ak, with any particular feature included at most once." ></td>
	<td class="line x" title="58:179	This list comprises a new document, representing that site.2 3." ></td>
	<td class="line x" title="59:179	Label this document +1 if ak = excellent, -1 if ak = poor." ></td>
	<td class="line x" title="60:179	When a Naive Bayes model is fit to the corpus described above, it results in a vector ^ of length p, consisting of coefficient estimates for all features." ></td>
	<td class="line x" title="61:179	In Propositions 1 and 2 below, we show that Turneys estimates of sentiment orientation ^ are closely related to ^, and that both estimates produce identical classifiers." ></td>
	<td class="line x" title="62:179	Proposition 1 ^ = C1^ (12) where C1 = Nexc:= P i:si=1 jdij Npoor=Pi:si= 1 jdij (13) Proof: Because a feature is restricted to at most one occurrence in a document, X i:si=k dij = N(w;ak) (14) Then from Equations 6 and 11: ^ j = log ^q1j^q 1j (15) = log N(w;exc:)= P i:si=1 jdij N(w;poor)=Pi:si= 1 jdij (16) = C1^ j (17) 2 2If both anchors occur on a site, then there will actually be two documents, one for each sentiment Proposition 2 Turneys classifier is identical to a Naive Bayes classifier fit on this corpus, with 1 = 1 = 0:5." ></td>
	<td class="line x" title="63:179	Proof: A Naive Bayes classifier typically assigns an observation to its most probable class." ></td>
	<td class="line x" title="64:179	This is equivalent to classifying according to the sign of the estimated logit." ></td>
	<td class="line x" title="65:179	So for any document, we must show that both the logit estimate and the average sentiment orientation are identical in sign." ></td>
	<td class="line x" title="66:179	When 1 = 0:5, 0 = 0." ></td>
	<td class="line x" title="67:179	Thus the estimated logit is dlogit(sjd) = pX j=1 ^ jdj (18) = C1 pX j=1 ^ jdj (19) This is a positive multiple of Turneys classifier (Equation 3), so they clearly match in sign." ></td>
	<td class="line x" title="68:179	2 3 A More Versatile Model 3.1 Desired Extensions By understanding Turneys model within a Naive Bayes framework, we are able to interpret its output as a probability model for document classes." ></td>
	<td class="line x" title="69:179	In the presence of labeled examples, this insight also makes it possible to estimate the intercept term 0." ></td>
	<td class="line x" title="70:179	Further, we are able to view this model as a member of a broad class: linear estimates for the logit." ></td>
	<td class="line x" title="71:179	This understanding facilitates further extensions, in particular, utilizing the following: 1." ></td>
	<td class="line x" title="72:179	Labeled documents 2." ></td>
	<td class="line x" title="73:179	More anchor words The reason for using labeled documents is straightforward; labels offer validation for any chosen model." ></td>
	<td class="line x" title="74:179	Using additional anchors is desirable in part because it is inexpensive to produce lists of words that are believed to reflect positive sentiment, perhaps by reference to a thesaurus." ></td>
	<td class="line x" title="75:179	In addition, a single anchor may be at once too general and too specific." ></td>
	<td class="line x" title="76:179	An anchor may be too general in the sense that many common words have multiple meanings, and not all of them reflect a chosen sentiment orientation." ></td>
	<td class="line x" title="77:179	For example, poor can refer to an objective economic state that does not necessarily express negative sentiment." ></td>
	<td class="line x" title="78:179	As a result, a word such as income appears 4.18 times as frequently with poor as excellent, even though it does not convey negative sentiment." ></td>
	<td class="line x" title="79:179	Similarly, excellent has a technical meaning in antiquity trading, which causes it to appear 3.34 times as frequently with furniture." ></td>
	<td class="line x" title="80:179	An anchor may also be too specific, in the sense that there are a variety of different ways to express sentiment, and a single anchor may not capture them all." ></td>
	<td class="line x" title="81:179	So a word like pretentious carries a strong negative sentiment but co-occurs only slightly more frequently (1.23 times) with excellent than poor." ></td>
	<td class="line x" title="82:179	Likewise, fascination generally reflects a positive sentiment, yet it appears slightly more frequently (1.06 times) with poor than excellent." ></td>
	<td class="line x" title="83:179	3.2 Other Sources of Unlabeled Data The use of additional anchors has a drawback in terms of being resource-intensive." ></td>
	<td class="line x" title="84:179	A feature set may contain many words and phrases, and each of them requires a separate AltaVista query for every chosen anchor word." ></td>
	<td class="line x" title="85:179	In the case of 30,000 features and ten queries per minute, downloads for a single anchor word require over two days of data collection." ></td>
	<td class="line x" title="86:179	An alternative approach is to access a large collection of documents directly." ></td>
	<td class="line x" title="87:179	Then all cooccurrences can be counted in a single pass." ></td>
	<td class="line x" title="88:179	Although this approach dramatically reduces the amount of data available, it does offer several advantages." ></td>
	<td class="line x" title="89:179	Increased Query Options Search engine queries of the form phrase NEAR anchor may not produce all of the desired cooccurrence counts." ></td>
	<td class="line x" title="90:179	For instance, one may wish to run queries that use stemmed words, hyphenated words, or punctuation marks." ></td>
	<td class="line x" title="91:179	One may also wish to modify the definition of NEAR, or to count individual co-occurrences, rather than counting sites that contain at least one co-occurrence." ></td>
	<td class="line x" title="92:179	Topic Matching Across the Internet as a whole, features may not exhibit the same correlation structure as they do within a specific domain." ></td>
	<td class="line x" title="93:179	By restricting attention to documents within a domain, one may hope to avoid cooccurrences that are primarily relevant to other subjects." ></td>
	<td class="line x" title="94:179	Reproducibility On a fixed corpus, counts of word occurrences produce consistent results." ></td>
	<td class="line x" title="95:179	Due to the dynamic nature of the Internet, numbers may fluctuate." ></td>
	<td class="line x" title="96:179	3.3 Co-Occurrences and Derived Features The Naive Bayes coefficient estimate ^ j may itself be interpreted as an intercept term plus a linear combination of features of the form log N(wj;ak)." ></td>
	<td class="line x" title="97:179	Num." ></td>
	<td class="line x" title="98:179	of Labeled Occurrences Correlation 1 5 0.022 6 10 0.082 11 25 0.113 26 50 0.183 51 75 0.283 76 100 0.316 Figure 1: Correlation between Supervised and Unsupervised Coefficient Estimates ^ j = log N(j;exc:)= P i:si=1 jdij N(j;pr:)=Pi:si= 1 jdij (20) = log C1 + log N(j;exc:) log N(j;pr:) (21) We generalize this estimate as follows: for a collection of K different anchor words, we consider a general linear combination of logged co-occurrence counts." ></td>
	<td class="line x" title="99:179	^ j = KX k=1 k log N(wj;ak) (22) In the special case of a Naive Bayes model, k = 1 when the k-th anchor word ak conveys positive sentiment, 1 when it conveys negative sentiment." ></td>
	<td class="line x" title="100:179	Replacing the logit estimate in Equation 9 with an estimate of this form, the model becomes: dlogit(sjd) = ^ 0 + pX j=1 dj ^ j (23) = ^ 0 + pX j=1 KX k=1 dj k log N(wj;ak) (24) = 0 + KX k=1 k pX j=1 dj log N(wj;ak) (25) (26) This model has only K + 1 parameters: 0; 1;::: ; K. These can be learned straightforwardly from labeled documents by a method such as logistic regression." ></td>
	<td class="line x" title="101:179	Observe that a document receives a score for each anchor word Ppj=1 dj log N(wj;ak)." ></td>
	<td class="line x" title="102:179	Effectively, the predictor variables in this model are no longer counts of the original features dj." ></td>
	<td class="line x" title="103:179	Rather, they are 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 3 2 1 0 1 2 3 4 Traditional Naive Bayes Coefs." ></td>
	<td class="line x" title="104:179	Turney Naive Bayes Coefs." ></td>
	<td class="line x" title="105:179	Unsupervised vs. Supervised Coefficients Figure 2: Unsupervised versus Supervised Coefficient Estimates inner products between the entire feature vector d and the logged co-occurence vector N(w;ak)." ></td>
	<td class="line x" title="106:179	In this respect, the vector of logged co-occurrences is used to produce derived feature." ></td>
	<td class="line o" title="107:179	4 Data Analysis 4.1 Accuracy of Unsupervised Coefficients By means of a Perl script that uses the Lynx browser, Version 2.8.3rel.1, we download AltaVista hit counts for queries of the form target NEAR anchor. The initial list of targets consists of 44,321 word types extracted from the Pang corpus of 1400 labeled movie reviews." ></td>
	<td class="line x" title="108:179	After preprocessing, this number is reduced to 28,629.3 In Figure 1, we compare estimates produced by two Naive Bayes procedures." ></td>
	<td class="line x" title="109:179	For each feature wj, we estimate j by using Turneys procedure, and by fitting a traditional Naive Bayes model to the labeled documents." ></td>
	<td class="line x" title="110:179	The traditional estimates are smoothed by assuming a Beta prior distribution that is equivalent to having four previous observations of wj in documents of each class." ></td>
	<td class="line x" title="111:179	^q1j ^q 1j = C2 4 +Pi:si=1 dij 4 +Pi:si= 1 dij (27) where C2 = 4p + P i:si=1 jdij 4p +Pi:si= 1 jdij (28) Here, dij is used to indicate feature presence: dij = 1 if w j appears in di 0 otherwise (29) 3We eliminate extremely rare words by requiring each target to co-occur at least once with each anchor." ></td>
	<td class="line x" title="112:179	In addition, certain types, such as words containing hyphens, apostrophes, or other punctuation marks, do not appear to produce valid counts, so they are discarded." ></td>
	<td class="line x" title="113:179	Positive Negative best awful brilliant bad excellent pathetic spectacular poor wonderful worst Figure 3: Selected Anchor Words We choose this fitting procedure among several candidates because it performs well in classifying test documents." ></td>
	<td class="line x" title="114:179	In Figure 1, each entry in the right-hand column is the observed correlation between these two estimates over a subset of features." ></td>
	<td class="line x" title="115:179	For features that occur in five documents or fewer, the correlation is very weak (0.022)." ></td>
	<td class="line x" title="116:179	This is not surprising, as it is difficult to estimate a coefficient from such a small number of labeled examples." ></td>
	<td class="line x" title="117:179	Correlations are stronger for more common features, but never strong." ></td>
	<td class="line x" title="118:179	As a baseline for comparison, Naive Bayes coefficients can be estimated using a subset of their labeled occurrences." ></td>
	<td class="line x" title="119:179	With two independent sets of 51-75 occurrences, Naive Bayes coefficient estimates had a correlation of 0.475." ></td>
	<td class="line x" title="120:179	Figure 2 is a scatterplot of the same coefficient estimates for word types that appear in 51 to 100 documents." ></td>
	<td class="line x" title="121:179	The great majority of features do not have large coefficients, but even for the ones that do, there is not a tight correlation." ></td>
	<td class="line x" title="122:179	4.2 Additional Anchors We wish to learn how our model performance depends on the choice and number of anchor words." ></td>
	<td class="line x" title="123:179	Selecting from WordNet synonym lists (Fellbaum, 1998), we choose five positive anchor words and five negative (Figure 3)." ></td>
	<td class="line x" title="124:179	This produces a total of 25 different possible pairs for use in producing coefficient estimates." ></td>
	<td class="line o" title="125:179	Figure 4 shows the classification performance of unsupervised procedures using the 1400 labeled Pang documents as test data." ></td>
	<td class="line x" title="126:179	Coefficients ^ j are estimated as described in Equation 22." ></td>
	<td class="line x" title="127:179	Several different experimental conditions are applied." ></td>
	<td class="line x" title="128:179	The methods labeled Count use the original un-normalized coefficients, while those labeled Norm. have been normalized so that the number of co-occurrences with each anchor have identical variance." ></td>
	<td class="line x" title="129:179	Results are shown when rare words (with three or fewer occurrences in the labeled corpus) are included and omitted." ></td>
	<td class="line x" title="130:179	The methods pair and 10 describe whether all ten anchor coefficients are used at once, or just the ones that correspond to a single pair of Method Feat." ></td>
	<td class="line x" title="131:179	Misclass." ></td>
	<td class="line x" title="132:179	St.Dev Count Pair >3 39.6% 2.9% Norm." ></td>
	<td class="line x" title="133:179	Pair >3 38.4% 3.0% Count Pair all 37.4% 3.1% Norm." ></td>
	<td class="line x" title="134:179	Pair all 37.3% 3.0% Count 10 > 3 36.4%  Norm." ></td>
	<td class="line x" title="135:179	10 > 3 35.4%  Count 10 all 34.6%  Norm." ></td>
	<td class="line x" title="136:179	10 all 34.1%  Figure 4: Classification Error Rates for Different Unsupervised Approaches anchor words." ></td>
	<td class="line x" title="137:179	For anchor pairs, the mean error across all 25 pairs is reported, along with its standard deviation." ></td>
	<td class="line x" title="138:179	Patterns are consistent across the different conditions." ></td>
	<td class="line x" title="139:179	A relatively large improvement comes from using all ten anchor words." ></td>
	<td class="line x" title="140:179	Smaller benefits arise from including rare words and from normalizing model coefficients." ></td>
	<td class="line x" title="141:179	Models that use the original pair of anchor words, excellent and poor, perform slightly better than the average pair." ></td>
	<td class="line x" title="142:179	Whereas mean performance ranges from 37.3% to 39.6%, misclassification rates for this pair of anchors ranges from 37.4% to 38.1%." ></td>
	<td class="line x" title="143:179	4.3 A Smaller Unlabeled Corpus As described in Section 3.2, there are several reasons to explore the use of a smaller unlabeled corpus, rather than the entire Internet." ></td>
	<td class="line x" title="144:179	In our experiments, we use additional movie reviews as our documents." ></td>
	<td class="line x" title="145:179	For this domain, Pang makes available 27,886 reviews.4 Because this corpus offers dramatically fewer instances of anchor words, we modify our estimation procedure." ></td>
	<td class="line x" title="146:179	Rather than discarding words that rarely co-occur with anchors, we use the same feature set as before and regularize estimates by the same procedure used in the Naive Bayes procedure described earlier." ></td>
	<td class="line x" title="147:179	Using all features, and ten anchor words with normalized scores, test error is 35.0%." ></td>
	<td class="line x" title="148:179	This suggests that comparable results can be attained while referring to a considerably smaller unlabeled corpus." ></td>
	<td class="line x" title="149:179	Rather than requiring several days of downloads, the count of nearby co-occurrences was completed in under ten minutes." ></td>
	<td class="line x" title="150:179	Because this procedure enables fast access to counts, we explore the possibility of dramatically enlarging our collection of anchor words." ></td>
	<td class="line x" title="151:179	We col4This corpus is freely available on the following website: http://www.cs.cornell.edu/people/pabo/movie-review-data/." ></td>
	<td class="line x" title="152:179	100 200 300 400 500 600 0.30 0.32 0.34 0.36 0.38 0.40 Num." ></td>
	<td class="line x" title="153:179	of Labeled Documents Classif." ></td>
	<td class="line x" title="154:179	Error Misclassification versus Sample Size Figure 5: Misclassification with Labeled Documents." ></td>
	<td class="line x" title="155:179	The solid curve represents a latent factor model with estimated coefficients." ></td>
	<td class="line x" title="156:179	The dashed curve uses a Naive Bayes classifier." ></td>
	<td class="line x" title="157:179	The two horizontal lines represent unsupervised estimates; the upper one is for the original unsupervised classifier, and the lower is for the most successful unsupervised method." ></td>
	<td class="line x" title="158:179	lect data for the complete set of WordNet synonyms for the words good, best, bad, boring, and dreadful." ></td>
	<td class="line x" title="159:179	This yields a total of 83 anchor words, 35 positive and 48 negative." ></td>
	<td class="line x" title="160:179	When all of these anchors are used in conjunction, test error increases to 38.3%." ></td>
	<td class="line x" title="161:179	One possible difficulty in using this automated procedure is that some synonyms for a word do not carry the same sentiment orientation." ></td>
	<td class="line x" title="162:179	For instance, intense is listed as a synonym for bad, even though its presence in a movie review is a strongly positive indication.5 4.4 Methods with Supervision As demonstrated in Section 3.3, each anchor word ak is associated with a coefficient k. In unsupervised models, these coefficients are assumed to be known." ></td>
	<td class="line x" title="163:179	However, when labeled documents are available, it may be advantageous to estimate them." ></td>
	<td class="line x" title="164:179	Figure 5 compares the performance of a model with estimated coefficient vector, as opposed to unsupervised models and a traditional supervised approach." ></td>
	<td class="line x" title="165:179	When a moderate number of labeled documents are available, it offers a noticeable improvement." ></td>
	<td class="line x" title="166:179	The supervised method used for reference in this case is the Naive Bayes model that is described in section 4.1." ></td>
	<td class="line x" title="167:179	Naive Bayes classification is of particular interest here because it converges faster to its asymptotic optimum than do discriminative methods (Ng, A. Y. and Jordan, M. , 2002)." ></td>
	<td class="line o" title="168:179	Further, with 5In the labeled Pang corpus, intense appears in 38 positive reviews and only 6 negative ones." ></td>
	<td class="line oc" title="169:179	a larger number of labeled documents, its performance on this corpus is comparable to that of Support Vector Machines and Maximum Entropy models (Pang et al. , 2002)." ></td>
	<td class="line x" title="170:179	The coefficient vector is estimated by regularized logistic regression." ></td>
	<td class="line x" title="171:179	This method has been used in other text classification problems, as in Zhang and Yang (2003)." ></td>
	<td class="line x" title="172:179	In our case, the regularization6 is introduced in order to enforce the beliefs that: 1 2, if a1, a2 synonyms (30) 1 2, if a1, a2 antonyms (31) For further information on regularized model fitting, see for instance, Hastie et al.(2001)." ></td>
	<td class="line x" title="174:179	5 Conclusion In business settings, there is growing interest in learning product reputations from the Internet." ></td>
	<td class="line x" title="175:179	For such problems, it is often difficult or expensive to obtain labeled data." ></td>
	<td class="line x" title="176:179	As a result, a change in modeling strategies is needed, towards approaches that require less supervision." ></td>
	<td class="line x" title="177:179	In this paper we provide a framework for allowing human-provided information to be combined with unlabeled documents and labeled documents." ></td>
	<td class="line x" title="178:179	We have found that this framework enables improvements over existing techniques, both in terms of the speed of model estimation and in classification accuracy." ></td>
	<td class="line x" title="179:179	As a result, we believe that this is a promising new approach to problems of practical importance." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P04-3025
Incorporating Topic Information Into Semantic Analysis Models
Mullen, Tony;Collier, Nigel;"></td>
	<td class="line x" title="1:77	Incorporating topic information into sentiment analysis models Tony Mullen National Institute of Informatics (NII) Hitotsubashi 2-1-2, Chiyoda-ku Tokyo 101-8430, Japan, mullen@nii.ac.jp Nigel Collier National Institute of Informatics (NII) Hitotsubashi 2-1-2, Chiyoda-ku Tokyo 101-8430, Japan, collier@nii.ac.jp Abstract This paper reports experiments in classifying texts based upon their favorability towards the subject of the text using a feature set enriched with topic information on a small dataset of music reviews hand-annotated for topic." ></td>
	<td class="line x" title="2:77	The results of these experiments suggest ways in which incorporating topic information into such models may yield improvement over models which do not use topic information." ></td>
	<td class="line x" title="3:77	1 Introduction There are a number of challenging aspects in recognizing the favorability of opinion-based texts, the task known as sentiment analysis." ></td>
	<td class="line x" title="4:77	Opinions in natural language are very often expressed in subtle and complex ways, presenting challenges which may not be easily addressed by simple text categorization approaches such as n-gram or keyword identification approaches." ></td>
	<td class="line nc" title="5:77	Although such approaches have been employed effectively (Pang et al. , 2002), there appears to remain considerable room for improvement." ></td>
	<td class="line x" title="6:77	Moving beyond these approaches can involve addressing the task at several levels." ></td>
	<td class="line x" title="7:77	Negative reviews may contain many apparently positive phrases even while maintaining a strongly negative tone, and the opposite is also common." ></td>
	<td class="line x" title="8:77	This paper attempts to address this issue using Support Vector Machines (SVMs), a well-known and powerful tool for classification of vectors of real-valued features (Vapnik, 1998)." ></td>
	<td class="line x" title="9:77	The present approach emphasizes the use of a variety of diverse information sources." ></td>
	<td class="line x" title="10:77	In particular, several classes of features based upon the proximity of the topic with phrases which have been assigned favorability values are described in order to take advantage of situations in which the topic of the text may be explicitly identified." ></td>
	<td class="line x" title="11:77	2 Motivation In the past, work has been done in the area of characterizing words and phrases according to their emotive tone (Turney and Littman, 2003; Turney, 2002; Kamps et al. , 2002; Hatzivassiloglou and Wiebe, 2000; Hatzivassiloglou and McKeown, 2002; Wiebe, 2000), but in many domains of text, the values of individual phrases may bear little relation to the overall sentiment expressed by the text." ></td>
	<td class="line oc" title="12:77	Pang et al.(2002)s treatment of the task as analogous to topic-classification underscores the difference between the two tasks." ></td>
	<td class="line x" title="14:77	A number of rhetorical devices, such as the drawing of contrasts between the reviewed entity and other entities or expectations, sarcasm, understatement, and digressions, all of which are used in abundance in many discourse domains, create challenges for these approaches." ></td>
	<td class="line x" title="15:77	It is hoped that incorporating topic information along the lines suggested in this paper will be a step towards solving some of these problems." ></td>
	<td class="line x" title="16:77	3 Methods 3.1 Semantic orientation with PMI Here, the term semantic orientation (SO) (Hatzivassiloglou and McKeown, 2002) refers to a real number measure of the positive or negative sentiment expressed by a word or phrase." ></td>
	<td class="line x" title="17:77	In the present work, the approach taken by Turney (2002) is used to derive such values for selected phrases in the text." ></td>
	<td class="line x" title="18:77	For the purposes of this paper, these phrases will be referred to as value phrases, since they will be the sources of SO values." ></td>
	<td class="line x" title="19:77	Once the desired value phrases have been extracted from the text, each one is assigned an SO value." ></td>
	<td class="line x" title="20:77	The SO of a phrase is determined based upon the phrases pointwise mutual information (PMI) with the words excellent and poor." ></td>
	<td class="line x" title="21:77	PMI is defined by Church and Hanks (1989) as follows: a0a2a1a4a3a6a5a8a7a10a9a12a11a13a7a15a14a17a16a19a18a21a20a23a22a25a24 a14a27a26a29a28 a5a8a7a10a9a19a30a31a7a15a14a17a16 a28 a5a8a7 a9 a16 a28 a5a8a7 a14 a16a33a32 (1) where a28 a5a8a7a10a9a19a30a31a7a15a14a12a16 is the probability that a7a34a9 and a7a35a14 co-occur." ></td>
	<td class="line x" title="22:77	The SO for a a28a37a36a39a38a41a40a29a42a44a43 is the difference between its PMI with the word excellent and its PMI with the word poor. The method used to derive these values takes advantage of the possibility of using the World Wide Web as a corpus, similarly to work such as (Keller and Lapata, 2003)." ></td>
	<td class="line x" title="23:77	The probabilities are estimated by querying the AltaVista Advanced Search engine1 for counts." ></td>
	<td class="line x" title="24:77	The search engines NEAR operator, representing occurrences of the two queried words within ten words of each other in a text, is used to define co-occurrence." ></td>
	<td class="line x" title="25:77	The final SO equation is a45a39a46 a5 a28a37a36a47a38a25a40a29a42a33a43 a16a19a18 a48a50a49a33a51 a14a35a52a54a53a56a55 a57a59a58a61a60a62a17a63a6a64a13a65a67a66a61a68a70a69a72a71a74a73a37a75a77a76a8a78a80a79a67a81a82a78a84a83a85a83a85a78a84a86a87a57a8a88a88a90a89a90a53a91a55 a57a8a58a82a60a80a76a8a92a12a93a94a93a13a95a8a88a88a23a89 a53a56a55 a57a59a58a61a60a62a17a63a6a64a13a65a67a66a61a68a70a69a72a71a74a73a37a75a77a76a8a92a12a93a94a93a13a95 a88a88 a89a90a53a91a55 a57a8a58a82a60a80a76a8a78a80a79a67a81a82a78a84a83a85a83a85a78a84a86a87a57 a88a88 a89a67a96 Intuitively, this yields values above zero for phrases with greater PMI with the word excellent and below zero for greater PMI with poor." ></td>
	<td class="line x" title="26:77	A SO value of zero would indicate a completely neutral semantic orientation." ></td>
	<td class="line x" title="27:77	3.2 Osgood semantic differentiation with WordNet Further feature types are derived using the method of Kamps and Marx (2002) of using WordNet relationships to derive three values pertinent to the emotive meaning of adjectives." ></td>
	<td class="line x" title="28:77	The three values correspond to the potency (strong or weak), activity (active or passive) and the evaluative (good or bad) factors introduced in Charles Osgoods Theory of Semantic Differentiation (Osgood et al. , 1957)." ></td>
	<td class="line x" title="29:77	These values are derived by measuring the relative minimal path length (MPL) in WordNet between the adjective in question and the pair of words appropriate for the given factor." ></td>
	<td class="line x" title="30:77	In the case of the evaluative factor (EVA) for example, the comparison is between the MPL between the adjective and good and the MPL between the adjective and bad." ></td>
	<td class="line x" title="31:77	Only adjectives connected by synonymy to each of the opposites are considered." ></td>
	<td class="line x" title="32:77	The method results in a list of 5410 adjectives, each of which is given a value for each of the three factors referred to as EVA, POT, and ACT." ></td>
	<td class="line x" title="33:77	Each of these factors values are averaged over all the adjectives in a text, yielding three real-valued feature values for the text, which will be added to the SVM model." ></td>
	<td class="line x" title="34:77	3.3 Topic proximity and syntactic-relation features In some application domains, it is known in advance what the topic is toward which sentiment is to be evaluated." ></td>
	<td class="line x" title="35:77	Incorporating this information is done by creating several classes of features based upon the semantic orientation values of phrases given their position in relation to the topic of the text." ></td>
	<td class="line x" title="36:77	The approach allows secondary information to be incorporated where available, in this case, the primary information is the specific record being reviewed and the secondary information identified is the artist." ></td>
	<td class="line x" title="37:77	Texts were annotated by hand using the Open Ontology Forge annotation tool (Collier et al. , 2003)." ></td>
	<td class="line x" title="38:77	In each record review, references (including co-reference) to the record being reviewed were tagged as THIS WORK and references to the artist under review were tagged as THIS ARTIST." ></td>
	<td class="line x" title="39:77	With these entities tagged, a number of classes of features may be extracted, representing various relationships between topic entities and value phrases similar to those described in section 3.1." ></td>
	<td class="line x" title="40:77	The classes looked at in this work are as follows: Turney Value The average value of all value phrases SO values for the text." ></td>
	<td class="line x" title="41:77	Classification by this feature alone is not the equivalent of Turneys approach, since the present approach involves retraining in a supervised model." ></td>
	<td class="line x" title="42:77	In sentence with THIS WORK The average value of all value phrases which occur in the same sentence as a reference to the work being reviewed." ></td>
	<td class="line x" title="43:77	1www.altavista.com Following THIS WORK The average value of all value phrases which follow a reference to the work being reviewed directly, or separated only by the copula or a preposition." ></td>
	<td class="line x" title="44:77	Preceding THIS WORK The average value of all value phrases which precede a reference to the work being reviewed directly, or separated only by the copula or a preposition." ></td>
	<td class="line x" title="45:77	In sentence with THIS ARTIST As above, but with reference to the artist." ></td>
	<td class="line x" title="46:77	Following THIS ARTIST As above, but with reference to the artist." ></td>
	<td class="line x" title="47:77	Preceding THIS ARTIST As above, but with reference to the artist." ></td>
	<td class="line x" title="48:77	The features used which make use of adjectives with WordNet derived Osgood values include the following: Text-wide EVA The average EVA value of all adjectives in a text." ></td>
	<td class="line x" title="49:77	Text-wide POT The average POT value of all adjectives in a text." ></td>
	<td class="line x" title="50:77	Text-wide ACT The average ACT value of all adjectives in a text." ></td>
	<td class="line x" title="51:77	TOPIC-sentence EVA The average EVA value of all adjectives which share a sentence with the topic of the text." ></td>
	<td class="line x" title="52:77	TOPIC-sentence POT The average POT value of all adjectives which share a sentence with the topic of the text." ></td>
	<td class="line x" title="53:77	TOPIC-sentence ACT The average ACT value of all adjectives which share a sentence with the topic of the text." ></td>
	<td class="line x" title="54:77	The grouping of these classes should reflect some common degree of reliability of features within a given class, but due to data sparseness what might have been more natural class groupingsfor example including value-phrase preposition topic-entity as a distinct classoften had to be conflated in order to get features with enough occurrences to be representative." ></td>
	<td class="line x" title="55:77	4 Experiments The dataset consists of 100 record reviews from the Pitchfork Media online record review publication,2 topic-annotated by hand." ></td>
	<td class="line x" title="56:77	Features used include word unigrams and lemmatized unigrams3 as well as the features described in 3.3 which make use of topic information, namely the broader PMI derived SO values and the topic-sentence Osgood values." ></td>
	<td class="line x" title="57:77	Due to the relatively small size of this dataset, test suites were created using 100, 20, 10, and 5-fold cross validation, to maximize the amount of data available for training and the accuracy of the results." ></td>
	<td class="line x" title="58:77	SVMs were built using Kudos TinySVM software implementation.4 5 Results Experimental results may be seen in figure 1." ></td>
	<td class="line x" title="59:77	It must be noted that this dataset is very small,and although the results are not conclusive they are promising insofar as they suggest that the use of incorporating PMI values towards the topic yields some improvement in modeling." ></td>
	<td class="line x" title="60:77	They also suggest that the best way to incorporate such features is in the form of a separate SVM which may then be combined with the lemma-based model to create a hybrid." ></td>
	<td class="line x" title="61:77	2http://www.pitchforkmedia.com 3We employ the Conexor FDG parser (Tapanainen and Jarvinen, 1997) for POS tagging and lemmatization 4http://cl.aist-nara.ac.jp/taku-ku/software/TinySVM Model 5 folds 10 folds 20 folds 100 folds All (THIS WORK and THIS ARTIST) PMI 70% 70% 68% 69% THIS WORK PMI 72% 69% 70% 71% All Osgood 64% 64% 65% 64% All PMI and Osgood 74% 71% 74% 72% Unigrams 79% 80% 78% 82% Unigrams, PMI, Osgood 81% 80% 82% 82% Lemmas 83% 85% 84% 84% Lemmas and Osgood 83% 84% 84% 84% Lemmas and Turney 84% 85% 84% 84% Lemmas, Turney, text-wide Osgood 84% 85% 84% 84% Lemmas, PMI, Osgood 84% 85% 84% 86% Lemmas and PMI 84% 85% 85% 86% Hybrid SVM (PMI/Osgood and Lemmas) 86% 87% 84% 89% Figure 1: Accuracy results (percent of texts correctly classed) for 5, 10, 20 and 100-fold cross-validation tests with Pitchforkmedia.com record review data, hand-annotated for topic." ></td>
	<td class="line x" title="62:77	5.1 Discussion At the level of the phrasal SO assignment, it would seem that some improvement could be gained by adding domain context to the AltaVista Search." ></td>
	<td class="line x" title="63:77	Manyperhaps mostterms favorability content depends to some extent on their context." ></td>
	<td class="line x" title="64:77	As Turney notes, unpredictable, is generally positive when describing a movie plot, and negative when describing an automobile or a politician." ></td>
	<td class="line x" title="65:77	Likewise, such terms as devastating might be generally negative, but in the context of music or art may imply an emotional engagement which is usually seen as positive." ></td>
	<td class="line x" title="66:77	Likewise, using excellent and poor as the poles in assessing this value seems somewhat arbitrary, especially given the potentially misleading economic meaning of poor. Nevertheless, cursory experiments in adjusting the search have not yielded improvements." ></td>
	<td class="line x" title="67:77	One problem with limiting the domain (such as adding AND music or some disjunction of such constraints to the query) is that the resultant hit count is greatly diminished." ></td>
	<td class="line x" title="68:77	The data sparseness which results from added restrictions appears to cancel out any potential gain." ></td>
	<td class="line x" title="69:77	It is to be hoped that in the future, as search engines continue to improve and the Internet continues to grow, more possibilities will open up in this regard." ></td>
	<td class="line x" title="70:77	As it is, Google returns more hits than AltaVista, but its query syntax lacks a NEAR operator, making it unsuitable for this task." ></td>
	<td class="line x" title="71:77	As to why using excellent and poor works better than, for example good and bad, it is not entirely clear." ></td>
	<td class="line x" title="72:77	Again, cursory investigations have thus far supported Turneys conclusion that the former are the appropriate terms to use for this task." ></td>
	<td class="line x" title="73:77	It also seems likely that the topic-relations aspect of the present research only scratches the surface of what should be possible." ></td>
	<td class="line x" title="74:77	Although performance in the mid-80s is not bad, there is still considerable room for improvement." ></td>
	<td class="line x" title="75:77	The present models may also be further expanded with features representing other information sources, which may include other types of semantic annotation (Wiebe, 2002; Wiebe et al. , 2002), or features based on more sophisticated grammatical or dependency relations, or perhaps upon such things as zoning (e.g. do opinions become more clearly stated towards the end of a text?)." ></td>
	<td class="line x" title="76:77	In any case, it is hoped that the present work may help to indicate how various information sources pertinent to the task may be brought together." ></td>
	<td class="line x" title="77:77	6 Conclusion Further investigation using larger datasets is necessary for the purposes of fully exploiting topic information where it is available, but the present results suggest that this is a worthwhile direction to investigate." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W04-0509
Analysis Of Semantic Classes In Medical Text For Question Answering
Niu, Yun;Hirst, Graeme;"></td>
	<td class="line x" title="1:234	Analysis of Semantic Classes in Medical Text for Question Answering Yun Niu and Graeme Hirst Department of Computer Science University of Toronto Toronto, Ontario M5S 3G4 Canada DDD9D2BSCRD7BAD8D3D6D3D2D8D3BACTCSD9B8 CVCWBSCRD7BAD8D3D6D3D2D8D3BACTCSD9 Abstract To answer questions from clinical-evidence texts, we identify occurrences of the semantic classes  disease, medication, patient outcome  that are candidate elements of the answer, and the relations among them." ></td>
	<td class="line x" title="2:234	Additionally, we determine whether an outcome is positive or negative." ></td>
	<td class="line x" title="3:234	1 Motivation The published medical literature is an important source to help clinicians make decisions in patient treatment (Sackett and Straus, 1998; Straus and Sackett, 1999)." ></td>
	<td class="line x" title="4:234	Clinicians often need to consult literature on the latest information in patient care, such as side effects of a medication, symptoms of a disease, or time constraints in the use of a medication." ></td>
	<td class="line x" title="5:234	For example: 1 Q: In a patient with a suspected MI does thrombolysis decrease the risk of death if it is administered 10 hours after the onset of chest pain?" ></td>
	<td class="line x" title="6:234	The answer to the question can be found in Clinical Evidence (CE) (Barton, 2002), a regularly updated publication that reviews and consolidates experimental results for clinical problems: A: Systematic reviews of RCTs have found that prompt thrombolytic treatment (within 6 hours and perhaps up to 12 hours and longer after the onset of symptoms) reduces mortality in people with AMI and ST elevation or bundle branch block on their presenting ECG." ></td>
	<td class="line x" title="7:234	The goal of the EpoCare project (Evidence at Point of Care) at the University of Toronto is to develop methods for answering questions automatically with CE as the source text." ></td>
	<td class="line x" title="8:234	(We do not look at 1 All the examples in this paper are taken from a collection of questions that arose over a two-week period in August 2001 in a clinical teaching unit at the University of Toronto." ></td>
	<td class="line x" title="9:234	primary medical research text)." ></td>
	<td class="line x" title="10:234	Currently, the system accepts keyword queries in PICO format (Sackett et al. , 2000)." ></td>
	<td class="line x" title="11:234	In this format, a clinical question is represented by a set of four fields that correspond to the basic elements of the question: P: a description of the patient (or the problem); I: an intervention; C: a comparison or control intervention (may be omitted); O: the clinical outcome." ></td>
	<td class="line x" title="12:234	For example, the question shown above can be represented in PICO format as follows: P: myocardial infarction I: thrombolysis C:  O: mortality Our work in the project is to extend the keyword retrieval to a system that can answer questions expressed in natural language." ></td>
	<td class="line x" title="13:234	In our earlier work (Niu et al. , 2003), we showed that current technologies for factoid question answering (QA) are not adequate for clinical questions, whose answers must often be obtained by synthesizing relevant context." ></td>
	<td class="line x" title="14:234	To adapt to this new characteristic of QA in the medical domain, we exploit semantic classes and relations between them in medical text." ></td>
	<td class="line x" title="15:234	Semantic classes are important for our task because the information contained in them is often a good candidate for answering clinical questions." ></td>
	<td class="line x" title="16:234	In the example above, PICO elements correspond to three semantic classes: DISEASE (medical problem of the patient), INTERVENTION (medication applied to the disease) and the CLINICAL OUTCOME." ></td>
	<td class="line x" title="17:234	They together constitute a SCENARIO of treatment." ></td>
	<td class="line x" title="18:234	Similarly, a diagnosis scenario often includes SYMPTOMS, TESTING PROCEDURE,and HYPOTHESIZED DISEASES." ></td>
	<td class="line x" title="19:234	To understand the semantics of medical text and find answers to clinical questions, we need to know how these classes relate to each other in a specific scenario." ></td>
	<td class="line x" title="20:234	For example, is this medication a special type of another one; is this medication applied to this disease?" ></td>
	<td class="line x" title="21:234	These are the kind of relations that we are interested in." ></td>
	<td class="line x" title="22:234	In this work, we use a cue-wordbased approach to identify semantic classes in the treatment scenario and analyze the relations between them." ></td>
	<td class="line x" title="23:234	We also apply an automatic classification process to determine the polarity of an outcome, as it is important in answering clinical questions." ></td>
	<td class="line x" title="24:234	2 Identifying Semantic Classes in Medical Text 2.1 Diseases and Medications The identification of named entities (NEs) in the biomedical area, such as PROTEINS and CELLS,has been extensively explored; e.g., Lee et al.(2003), Shen et al.(2003)." ></td>
	<td class="line x" title="27:234	However, we are not aware of any satisfactory solution that focuses on the recognition of semantic classes such as MEDICATION and DISEASE." ></td>
	<td class="line x" title="28:234	To straightforwardly identify DISEASE and MEDICATION in the text, we use the knowledge base Unified Medical Language System (UMLS) (Lindberg et al. , 1993) and the software MetaMap (Aronson, 2001)." ></td>
	<td class="line x" title="29:234	UMLS contains three knowledge sources: the Metathesaurus, the Semantic Network, and the Specialist Lexicon." ></td>
	<td class="line x" title="30:234	Given an input sentence, MetaMap separates it into phrases, identifies the medical concepts embedded in the phrases, and assigns proper semantic categories to them according to the knowledge in UMLS." ></td>
	<td class="line x" title="31:234	For example, for the phrase immediate systemic anticoagulants, MetaMap identifies immediate as a TEMPORAL CONCEPT, systemic as a FUNCTIONAL CONCEPT,andanticoagulants as a PHARMACOLOGIC SUBSTANCE." ></td>
	<td class="line x" title="32:234	More than one semantic category in UMLS may correspond to MEDICATION or DISEASE." ></td>
	<td class="line x" title="33:234	For example, either a PHARMACOLOGIC SUBSTANCE or a THERAPEUTIC OR PREVENTIVE PROCEDURE can be a MEDICATION; either a DISEASE OR SYNDROME or a PATHOLOGIC FUNCTION can be a DISEASE." ></td>
	<td class="line x" title="34:234	We use some training text to find the mapping between UMLS categories and the two semantic classes in the treatment scenario." ></td>
	<td class="line x" title="35:234	The training text was tagged for us by a clinician to mark DISEASE and MEDICATION." ></td>
	<td class="line x" title="36:234	It was also processed by MetaMap." ></td>
	<td class="line x" title="37:234	After that, the annotated text was compared with the output of MetaMap to find the corresponding UMLS categories." ></td>
	<td class="line x" title="38:234	Medical text containing these categories can then be identified as either MEDICATION or DISEASE." ></td>
	<td class="line x" title="39:234	In the example above, anticoagulants will be taken as a MEDICATION.The problem of identification of medical terminology is still a big challenge in this area." ></td>
	<td class="line x" title="40:234	MetaMap does not provide a full solution to it." ></td>
	<td class="line x" title="41:234	For cases in which the output of MetaMap is not consistent with the judgment of the clinician who annotated our text, our decisions rely on the latter." ></td>
	<td class="line x" title="42:234	2.2 Clinical Outcome The task of identifying clinical outcomes is more complicated." ></td>
	<td class="line x" title="43:234	Outcomes are often not just noun phrases; instead, they usually are expressed in complex syntactic structures." ></td>
	<td class="line x" title="44:234	The following are some examples: (1) Thrombolysis reduces the risk of dependency,butincreases the risk of death." ></td>
	<td class="line x" title="45:234	(2) The median proportion of symptom free days improved more with salmeterol than with placebo." ></td>
	<td class="line x" title="46:234	In our analysis of the text, we found another type of outcome which is also very important: the outcome of clinical trials: (3) Several small comparative RCTs [randomized clinical trials] have found sodium cromoglicate to be less effective than inhaled corticosteroids in improving symptoms and lung function." ></td>
	<td class="line x" title="47:234	(4) In the systematic review of calcium channel antagonists, indirect and limited comparisons of intravenous versus oral administration found no significant difference in adverse events." ></td>
	<td class="line x" title="48:234	We treat these as a special type of clinical outcome." ></td>
	<td class="line x" title="49:234	For convenience, we refer to them as results in the following description when necessary." ></td>
	<td class="line x" title="50:234	A result might contain a clinical outcome within it, as results often involve a comparison of the effects of two (or more) interventions on a disease." ></td>
	<td class="line x" title="51:234	In medical text, the appearance of some words is found often to be a signal of the occurrence of an outcome, and usually several words signal the occurrence of one single outcome." ></td>
	<td class="line x" title="52:234	The combination approach that we applied for identifying outcomes is based on this observation." ></td>
	<td class="line x" title="53:234	Our approach does not extract the whole outcome at once." ></td>
	<td class="line x" title="54:234	Instead, it tries to identify the different parts of an outcome that may be scattered in the sentence, and then combines them to form the complete outcome." ></td>
	<td class="line x" title="55:234	2.2.1 Related work Rule-based methods and machine-learning approaches have been used for similar problems." ></td>
	<td class="line x" title="56:234	Gildea and Jurafsky (2002) used a supervised learning method to learn both the identifier of the semantic roles defined in FrameNet such as theme, target, goal, and the boundaries of the roles (Baker et al. , 2003)." ></td>
	<td class="line x" title="57:234	A set of features were learned from a large training set, and then applied to the unseen data to detect the roles." ></td>
	<td class="line x" title="58:234	The performance of the system was quite good." ></td>
	<td class="line x" title="59:234	However, it requires a large training set for related roles, which is not available in many tasks, including tasks in the medical area." ></td>
	<td class="line x" title="60:234	Rule-based methods are explored in information extraction (IE) to identify roles to fill slots in some pre-defined templates (Catal`a et al. , 2003)." ></td>
	<td class="line x" title="61:234	The rules are represented by a set of patterns, and template role identification is usually conducted by pattern matching." ></td>
	<td class="line x" title="62:234	Slots indicating roles are embedded in these patterns." ></td>
	<td class="line x" title="63:234	Text that satisfies the constraints of a pattern will be identified, and the contents corresponding to the slots are extracted." ></td>
	<td class="line x" title="64:234	This approach has been proved to be effective in many IE tasks." ></td>
	<td class="line x" title="65:234	However, pattern construction is very timeconsuming, especially for complicated phrasings." ></td>
	<td class="line x" title="66:234	In order to select the roles and only the roles, their expression has to be customized specifically in patterns." ></td>
	<td class="line x" title="67:234	This results in increasing difficulties in pattern construction, and reduces the coverage of the patterns." ></td>
	<td class="line x" title="68:234	2.2.2 A combination approach Different pieces of an outcome are identified by various cue words." ></td>
	<td class="line x" title="69:234	Each occurrence of a cue word suggests a portion of the expression of the outcome." ></td>
	<td class="line x" title="70:234	Detecting all of them will increase the chance of obtaining the complete outcome." ></td>
	<td class="line x" title="71:234	Also, different occurrences of cue words provide more evidence of the existence of an outcome." ></td>
	<td class="line x" title="72:234	The first step of the combination approach is to collect the cue words." ></td>
	<td class="line x" title="73:234	Two sections of CE (stroke management, asthma in children) were analyzed for detection of outcome." ></td>
	<td class="line x" title="74:234	The text was annotated by a clinician in the EpoCare project." ></td>
	<td class="line x" title="75:234	About two-thirds of each section (267 sentences in total) was taken as the analysis examples for collecting the cue words, and the rest (156 sentences) as the test set." ></td>
	<td class="line x" title="76:234	Some words we found in the analysis are the following: Nouns: death, benefit, dependency, outcome, evidence, harm, difference." ></td>
	<td class="line x" title="77:234	Verbs: improve, reduce, prevent, produce, increase." ></td>
	<td class="line x" title="78:234	Adjectives: beneficial, harmful, negative, adverse, superior." ></td>
	<td class="line x" title="79:234	After the cue words are identified, the next question is what portion of text each cue word suggests as the outcome, which determines the boundary of the outcome." ></td>
	<td class="line x" title="80:234	The text was pre-processed by the Apple Pie parser (Sekine, 1997) to obtain the partof-speech and phrase information." ></td>
	<td class="line x" title="81:234	We found that for the noun cues, the noun phrase that contains the noun will be part of the outcome." ></td>
	<td class="line x" title="82:234	For the verb cue words, the verb and its object together constitute one portion of the outcome." ></td>
	<td class="line x" title="83:234	For the adjective cue words, often the corresponding adjective phrase or the noun phrase belongs to the outcome." ></td>
	<td class="line x" title="84:234	Cue words for the results of clinical trials are processed in a slightly different way." ></td>
	<td class="line x" title="85:234	For example, for difference and superior, any immediately following prepositional phrase is also included in the results of the trial." ></td>
	<td class="line x" title="86:234	Our approach does not rely on specific patterns, it is more flexible than pattern-matching techniques in IE systems, and it does not need a large training set." ></td>
	<td class="line x" title="87:234	A limitation of this approach is that some connections between different portions of an outcome may be missing." ></td>
	<td class="line x" title="88:234	2.2.3 Evaluation and analysis of results We evaluated the cue word method of detecting the outcome on the remaining one-third of the sections of CE." ></td>
	<td class="line x" title="89:234	(The test set is rather small because of the difficulty in obtaining the annotations)." ></td>
	<td class="line x" title="90:234	The outcome detection task was broken into two sub-tasks, each evaluated separately: to identify the outcome itself and to determine its textual boundary." ></td>
	<td class="line x" title="91:234	The result of identification is shown in Table 1." ></td>
	<td class="line x" title="92:234	Eighty-one sentences in the test set contain either an outcome or result, which is 52% of all the test sentences." ></td>
	<td class="line x" title="93:234	This was taken as the baseline of the evaluation: taking all sentences in the test set as positive (i.e. , containing an outcome or result)." ></td>
	<td class="line x" title="94:234	By contrast, the accuracy of the combination approach is 83%." ></td>
	<td class="line x" title="95:234	There are two main reasons why some outcomes were not identified." ></td>
	<td class="line x" title="96:234	One is that some outcomes do not have any cue word: (5) Gastrointestinal symptoms and headaches have been reported with both montelukast and zafirlukast." ></td>
	<td class="line x" title="97:234	The other reason is that although some outcomes contained words that might be regarded as cue words, we did not include them in our set; for example, fewer and higher." ></td>
	<td class="line x" title="98:234	Adjectives were found to have the most irregular usages." ></td>
	<td class="line x" title="99:234	It is normal for them to modify both medications and outcomes, as shown in the following examples: (6)." ></td>
	<td class="line x" title="100:234	children receiving higher dose inhaled corticosteroids . . ." ></td>
	<td class="line x" title="101:234	Table 1: Results of identifying outcomes in CE False False Method Correct Positives Negatives Precision% Recall% Accuracy% Baseline 81 75 0 52 (81/156) 100 52 Combination approach 67 14 14 83 (67/81) 83 82 Table 2: Results of boundary detection of correctly identified outcomes in CE." ></td>
	<td class="line x" title="102:234	A: Identified fragments; B: true boundary." ></td>
	<td class="line x" title="103:234	Type of Overlap Number Percentage Exact match 26 39 A entirely within B 19 28 B entirely within A 13 19 Each partially within the other 8 12 No match 1 1 (7) . . ." ></td>
	<td class="line x" title="104:234	mean morning PEFR was 4% higher in the salmeterol group." ></td>
	<td class="line x" title="105:234	Other adjectives such as less, more, lower, shorter, longer, and different have similar problems." ></td>
	<td class="line x" title="106:234	If they are taken as identifiers of outcomes then some false positives are very likely to be generated." ></td>
	<td class="line x" title="107:234	However, if they are excluded, some true outcomes will be missed." ></td>
	<td class="line x" title="108:234	There were 14 samples of false positives." ></td>
	<td class="line x" title="109:234	The main cause was sentences containing cue words that did not have any useful information: (8) We found that the balance between benefits and harms has not been clearly established for the evacuation of supratentorial haematomas." ></td>
	<td class="line x" title="110:234	(9) The third systematic review did not evaluate these adverse outcomes." ></td>
	<td class="line x" title="111:234	Table 2 shows the result of boundary detection for those outcomes that were correctly identified." ></td>
	<td class="line x" title="112:234	The true boundary is the boundary of an outcome that was annotated manually." ></td>
	<td class="line x" title="113:234	The no match case means that there is a true outcome in the sentence but the program missed the correct portions of text and marked some other portions as the outcome." ></td>
	<td class="line x" title="114:234	The program identified 39% of the boundaries exactly the same as the true boundaries." ></td>
	<td class="line x" title="115:234	In 19% of the samples, the true boundaries were entirely within the identified fragments." ></td>
	<td class="line x" title="116:234	The spurious text in them (the text that was not in the true boundary) was found to be small in many cases, both in terms of number of words and in terms of the importance of the content." ></td>
	<td class="line x" title="117:234	The average number of words correctly identified was 7 for each outcome and the number of spurious words was 3.4." ></td>
	<td class="line x" title="118:234	The most frequent content in the spurious text was the medication applied to obtain the outcome." ></td>
	<td class="line x" title="119:234	In the following examples, text in CWCX is the outcome (result) identified automatically, and text in CUCV is spurious." ></td>
	<td class="line x" title="120:234	(10) The RCTs found CWno significant adverse effects CUassociated with salmeterolCVCX." ></td>
	<td class="line x" title="121:234	(11) The second RCT . . ." ></td>
	<td class="line x" title="122:234	also found CWno significant difference in mortality at 12 weeks CUwith lubeluzole versus placeboCVCX  Again, adjectives are most problematic." ></td>
	<td class="line x" title="123:234	Even when a true adjective identifier is found, the boundary of the outcome is hard to determine by an unsupervised approach because of the variations in the expression." ></td>
	<td class="line x" title="124:234	In the following examples, the true boundaries of outcomes are indicated by [ ], adjectives are highlighted." ></td>
	<td class="line x" title="125:234	(12) Nebulised . . ., but [CWserious adverse effectsCX are rare]." ></td>
	<td class="line x" title="126:234	(13) Small RCTs . . ." ></td>
	<td class="line x" title="127:234	found that [." ></td>
	<td class="line x" title="128:234	was CWeffectiveCX, with ]." ></td>
	<td class="line x" title="129:234	The correctness of the output of the parser also had an important impact on the performance, as shown in the following example: (14) RCTs found no evidence that lubeluzole improved clinical outcomes in people with acute ischaemic stroke." ></td>
	<td class="line x" title="130:234	(S(NPL(DTthat)(JJlubeluzole) (JJimproved)(JJclinical) (NNSoutcomes))) In this parse, the verb improve was incorrectly assigned to be an adjective in a noun phrase." ></td>
	<td class="line x" title="131:234	Thus improve as a verb cue word was missed in identifying the outcome." ></td>
	<td class="line x" title="132:234	However, another cue word outcomes was matched, so the whole noun phrase of outcomes was identified as the outcome." ></td>
	<td class="line x" title="133:234	On the one hand, the example shows that the wrong parsing output directly affects the identification process." ></td>
	<td class="line x" title="134:234	On the other hand, it also shows that missing one cue word in identifying the outcome can be corrected by the occurrence of other cue words in the combination approach." ></td>
	<td class="line x" title="135:234	3 Analysis of Relations Recognition of individual semantic classes is not enough for text understanding; we also need to know how different entities in the same semantic class are connected, as well as what relations hold between different classes." ></td>
	<td class="line x" title="136:234	Currently, all these relations are considered at the sentence level." ></td>
	<td class="line x" title="137:234	3.1 Relations within the same semantic class Relations between different medications are the focus of this sub-section, as a sentence often mentioned more than one medication." ></td>
	<td class="line x" title="138:234	Relations between diseases can be analyzed in a similar way, although they occur much less often than medications." ></td>
	<td class="line x" title="139:234	Text from CE was analyzed manually to understand what relations are often involved and how they are represented." ></td>
	<td class="line x" title="140:234	The text for the analysis is the same as in the class-identification task discussed above." ></td>
	<td class="line x" title="141:234	As with classes themselves, it was found that these relations can be identified by a group of cue words or symbols." ></td>
	<td class="line x" title="142:234	For example, the word plus refers to the COMBINATION of two or more medications, the word or, as well as a comma, often suggests the ALTERNATIVE relation, and the word versus (or v)usually implies a COMPARISON relation, as shown in the following examples: (15) The combination of aspirin plus streptokinase significantly increased mortality at 3 months." ></td>
	<td class="line x" title="143:234	(16) RCTs found no evidence that calcium channel antagonists, lubeluzole, aminobutyric acid (GABA) agonists, glycine antagonists, or N-methyl-D-aspartate (NMDA) antagonists improve clinical outcomes in people with acute ischaemic stroke." ></td>
	<td class="line x" title="144:234	(17) One systematic review found no short or long term improvement in acute ischaemic stroke with immediate systemic anticoagulants (unfractionated heparin, low molecular weight heparin, heparinoids,orspecific thrombin inhibitors) versus usual care without systemic anticoagulants." ></td>
	<td class="line x" title="145:234	It is worth noting that in CE, the experimental conditions are often explained in the description of the outcomes, for example: (18) . . ." ></td>
	<td class="line x" title="146:234	receiving higher dose inhaled corticosteroids (3.6cm, 95% CI 3.0 to 4.2 with double dose beclometasone v 5.1cm, 95% CI 4.5 to 5.7 with salmeterol v 4.5cm, 95% CI 3.8 to 5.2 with placebo)." ></td>
	<td class="line x" title="147:234	(19) It found that . . ." ></td>
	<td class="line x" title="148:234	oral theophylline . . ." ></td>
	<td class="line x" title="149:234	versus placebo increased the mean number of symptom free days (63% with theophylline v 42% with placebo; P=0.02)." ></td>
	<td class="line x" title="150:234	(20) Studies of . . ." ></td>
	<td class="line x" title="151:234	inhaled steroid (see salmeterol v high dose inhaled corticosteroids under adult asthma)." ></td>
	<td class="line x" title="152:234	These descriptions are usually in parentheses." ></td>
	<td class="line x" title="153:234	They are often phrases and even just fragments of strings that are not represented in a manner that is uniform with the other parts of the sentence." ></td>
	<td class="line x" title="154:234	Their behavior is more difficult to capture and therefore the relations among the concepts in these descriptions are more difficult to identify." ></td>
	<td class="line x" title="155:234	Because they usually are examples and data, omission of them will not affect the understanding of the whole sentence in most cases." ></td>
	<td class="line x" title="156:234	Six common relations and their cue words were found in the text which are shown in Table 3." ></td>
	<td class="line x" title="157:234	Cue words and symbols between medical concepts were first collected from the training text." ></td>
	<td class="line x" title="158:234	Then the relations they signal were analyzed." ></td>
	<td class="line x" title="159:234	Some cue words are ambiguous, for example, or, and,andwith." ></td>
	<td class="line x" title="160:234	Or could also suggest a comparison relation although most of the time it means alternative, and could represent an alternative relation, and with could be a specification relation." ></td>
	<td class="line x" title="161:234	It is interesting to find that and in the text when it connects two medications often suggests an alternative relation rather than a combination relation (e.g. , the second and in example 5)." ></td>
	<td class="line x" title="162:234	Also, compared with versus, plus,etc. ,and and with are weak cues as most of their appearances in the text do not suggest a relation between two medications." ></td>
	<td class="line x" title="163:234	On the basis of this analysis, an automatic relation analysis process was applied to the test set, which was the same as in outcome identification." ></td>
	<td class="line x" title="164:234	The test process was divided into two parts: one took parenthetical descriptions into account (case 1) and the other one did not (case 2)." ></td>
	<td class="line x" title="165:234	In the evaluation, for sentences that contain at least two medications, correct means that the relation that holds between the medications is correctly identified." ></td>
	<td class="line x" title="166:234	We do not evaluate the relation between any two medications in a sentence; instead, we only considered two medications that are related to each other by a cue word or symbol (including those connected by cue words Table 3: Cue words/symbols for relations between medications Relation(s) Cue Words/Symbols comparison superior to, more than, versus, or, comparewith,betweenand alternative or, ,, and combination plus, add to, addition of to, combined use of, and, with, ( specification with, ( substitute substitute, substituted for preference rather than Table 4: Results of relation analysis Correct Wrong Missing False Positive Case 1 49 7 10 9 Case 2 48 7 3 6 other than the set collected from the training text)." ></td>
	<td class="line x" title="167:234	The results of the two cases are shown in Table 4." ></td>
	<td class="line x" title="168:234	Most errors are because of the weak indicators with and and." ></td>
	<td class="line x" title="169:234	As in the outcome identification task, both the training and test sets are rather small, as no standard annotated text is available." ></td>
	<td class="line x" title="170:234	Some of the surface relationships in Table 3 reflect deeper relationships of the semantic classes." ></td>
	<td class="line x" title="171:234	For example, COMPARISON, ALTERNATIVE,and PREFERENCE imply that the two (or more) medications have some common effects on the disease(s) they are applied to." ></td>
	<td class="line x" title="172:234	The SPECIFICATION relation, on the other hand, suggests a hierarchical relation between the first medication and the following ones, in which the first medication is a higher-level concept and the following medications are at a lower level." ></td>
	<td class="line x" title="173:234	For example, in example 17 above, systemic anticoagulants is a higher-level concept, unfractionated heparin, low molecular weight heparin,etc. ,areexamples of it that lie at a lower level." ></td>
	<td class="line x" title="174:234	3.2 Relations between different semantic classes In a specific domain such as medicine, some default relations often hold between semantic classes." ></td>
	<td class="line x" title="175:234	For example, a CAUSEEFFECT relation is strongly embedded in the three semantic classes appearing in a sentence of the form: medication  disease  outcome, even if not in this exact order." ></td>
	<td class="line x" title="176:234	This default relation helps the relation analysis because in most cases we do not need to depend on the text between the classes to understand the whole sentence." ></td>
	<td class="line x" title="177:234	For instance, the CAUSEEFFECT relation is very likely to express the idea that applying the intervention on the disease will have the outcome." ></td>
	<td class="line x" title="178:234	This is another reason that semantic classes are important, especially in a specific domain." ></td>
	<td class="line x" title="179:234	4 The polarity of outcomes Most clinical outcomes and the results of clinical trials are either positive or negative: (21) Positive: Thrombolysis reduced the risk of death or dependency at the end of the studies." ></td>
	<td class="line x" title="180:234	(22) Negative: In the systematic review, thrombolysis increased fatal intracranial haemorrhage compared with placebo." ></td>
	<td class="line x" title="181:234	Polarity information is useful for several reasons." ></td>
	<td class="line x" title="182:234	First of all, it can filter out positive outcomes if the question is about the negative aspects of a medication." ></td>
	<td class="line x" title="183:234	Secondly, negative outcomes may be crucial even if the question does not explicitly ask about them." ></td>
	<td class="line x" title="184:234	Finally, from the number of positive or negative descriptions of the outcome of a medication applying to a disease, clinicians can form a general idea about how good the medication is. As a first step in understanding opposing relations between scenarios in medical text, the polarity of outcomes was determined by an automatic classification process." ></td>
	<td class="line x" title="185:234	We use support vector machines (SVMs) to distinguish positive outcomes from negative ones." ></td>
	<td class="line x" title="186:234	SVMs have been shown to be efficient in text classification tasks (Joachims, 1998)." ></td>
	<td class="line x" title="187:234	Given a training sample, the SVM finds a hyperplane with the maximal margin of separation between the two classes." ></td>
	<td class="line x" title="188:234	The classification is then just to determine which side of the hyperplane the test sample lies in." ></td>
	<td class="line x" title="189:234	We used the SVM light package (Joachims, 2002) in our experiment." ></td>
	<td class="line x" title="190:234	4.1 Training and test examples The training and test sets were built by collecting sentences from different sections in CE; 772 sentences were used, 500 for training (300 positive, 200 negative), and 272 for testing (95 positive, 177 negative)." ></td>
	<td class="line x" title="191:234	All examples were labeled manually." ></td>
	<td class="line x" title="192:234	4.2 Evaluation The classification used four different sets of features." ></td>
	<td class="line x" title="193:234	The first feature set includes every unigram that appears at least three times in the whole training set." ></td>
	<td class="line x" title="194:234	To improve the performance by attenuating the sparse data problem, in the second feature set, all names of diseases were replaced by the same tag disease." ></td>
	<td class="line x" title="195:234	This was done by pre-processing the text using MetaMap to identify all diseases in both the training and the test examples." ></td>
	<td class="line x" title="196:234	Then the identified diseases were replaced by the disease tag automatically." ></td>
	<td class="line x" title="197:234	As medications often are not mentioned in outcomes, they were not generalized in this manner." ></td>
	<td class="line x" title="198:234	The third feature set represents changes described in outcomes." ></td>
	<td class="line x" title="199:234	Our observation is that outcomes often involve the change in a clinical value." ></td>
	<td class="line x" title="200:234	For example, after a medication was applied to a disease, something was increased (enhanced, more, ) or decreased (reduced, less, )." ></td>
	<td class="line x" title="201:234	Thus the polarity of an outcome is often determined by how change happens: if a bad thing (e.g. , mortality) is reduced then it is a positive outcome; if the bad thing is increased, then the outcome is negative." ></td>
	<td class="line x" title="202:234	We try to capture this observation by adding context features to the feature set." ></td>
	<td class="line oc" title="203:234	The way they were added is similar to incorporating the negation effect described by Pang et al.(2002)." ></td>
	<td class="line x" title="205:234	But instead of just finding a negation word (not, isnt, didnt, etc.), we need to find two groups of words: those indicating more and those indicating less." ></td>
	<td class="line x" title="206:234	In the training text, we found 9 words in the first group and 7 words in the second group." ></td>
	<td class="line o" title="207:234	When pre-processing text for classification, following the method of Pang et al. , we attached the tag MORE to all words between the more-words and the following punctuation mark, and the tag LESS to the words after the less-words." ></td>
	<td class="line x" title="208:234	The fourth feature set is the combination of the effects of feature set two and three." ></td>
	<td class="line x" title="209:234	In representing each sentence by a feature vector, we tested both presence (feature appears or not) and frequency (count the number of occurrences of the feature in the sentence)." ></td>
	<td class="line x" title="210:234	The accuracy of the classification is shown in Table 5." ></td>
	<td class="line x" title="211:234	The baseline is to assign a random class (here we use negative, as they are more frequent in the test set) to all test samples." ></td>
	<td class="line x" title="212:234	The presence of features performs better than frequency of features in general." ></td>
	<td class="line x" title="213:234	Using a more general category instead of specific diseases has a positive effect on the presence-based classification." ></td>
	<td class="line x" title="214:234	We speculate that the effect of this generalization will be bigger if a larger test set were used." ></td>
	<td class="line nc" title="215:234	Pang et al.(2002) did not compare the result of using and not using the negation context effect, so it is not clear how much it improved their result." ></td>
	<td class="line x" title="217:234	In our task, it is clear that the MORE/ LESS feature has a significant effect on the performance, especially for the frequency features." ></td>
	<td class="line x" title="218:234	Table 5: Results of outcome polarity classification Presence Frequency Features (%) (%) Baseline 65.07 65.07 Original unigrams 88.97 87.87 Unigrams with disease 90.07 88.24 Unigrams with MORE/ LESS tag 91.54 91.91 Unigrams with disease and MORE/ LESS tag 92.65 92.28 5Conclusion We have described our work in medical text analysis by identifying semantic classes and the relations between them." ></td>
	<td class="line x" title="219:234	Our work suggests that semantic classes in medical scenarios play an important role in understanding medical text." ></td>
	<td class="line x" title="220:234	The scenario view may be extended to a framework that acts as a guideline for further semantic analysis." ></td>
	<td class="line x" title="221:234	Semantic classes and their relations have direct applications in medical question answering and query refinement in information retrieval." ></td>
	<td class="line x" title="222:234	In question answering, the question and answer candidates will contain some semantic classes." ></td>
	<td class="line x" title="223:234	After identifying them on both sides, the question can be compared with the answer to find whether there is a match." ></td>
	<td class="line x" title="224:234	In information retrieval, relations between semantic classes can be added to the index." ></td>
	<td class="line x" title="225:234	If the query posed by the user is too general, the system will ask the user to refine the query by adding more concepts and even relations so that it will be more pertinent according to the content of the source." ></td>
	<td class="line x" title="226:234	For example, a user may search for a document describing the comparison of aspirin and placebo." ></td>
	<td class="line x" title="227:234	Instead of just using aspirin and placebo as the query terms, the user can specify the comparison relation as well in the query." ></td>
	<td class="line x" title="228:234	We will continue working on the second level of the semantic analysis, to explore the relations on the scenario level." ></td>
	<td class="line x" title="229:234	A complete scenario contains all three semantic classes." ></td>
	<td class="line x" title="230:234	One scenario may be the explanation or justification of the previous scenario(s), or contradictory to the previous scenario(s)." ></td>
	<td class="line x" title="231:234	Detecting these relationships will be of great help for understanding-based tasks, such as context-related question answering, topic-related summarization, etc. As different scenarios might not be adjacent to each other in the texts, classical rhetorical analysis cannot provide a complete solution for this problem." ></td>
	<td class="line x" title="232:234	Acknowledgements The EpoCare project is supported by grants from Bell University Laboratories at the University of Toronto." ></td>
	<td class="line x" title="233:234	Our work is also supported by a grant from the Natural Sciences and Engineering Research Council of Canada and an Ontario Graduate Scholarship." ></td>
	<td class="line x" title="234:234	We are grateful to Sharon Straus, MD, and other members of the EpoCare project for discussion and assistance." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="H05-1043
Extracting Product Features And Opinions From Reviews
Popescu, Ana-Maria;Etzioni, Oren;"></td>
	<td class="line x" title="1:242	Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 339346, Vancouver, October 2005." ></td>
	<td class="line x" title="2:242	c2005 Association for Computational Linguistics Extracting Product Features and Opinions from Reviews Ana-Maria Popescu and Oren Etzioni Department of Computer Science and Engineering University of Washington Seattle, WA 98195-2350 {amp, etzioni}@cs.washington.edu Abstract Consumers are often forced to wade through many on-line reviews in order to make an informed product choice." ></td>
	<td class="line x" title="3:242	This paper introduces OPINE, an unsupervised informationextraction system which mines reviews in order to build a model of important product features, their evaluation by reviewers, and their relative quality across products." ></td>
	<td class="line x" title="4:242	Compared to previous work, OPINE achieves 22% higher precision (with only 3% lower recall) on the feature extraction task." ></td>
	<td class="line x" title="5:242	OPINEs novel use of relaxation labeling for finding the semantic orientation of words in context leads to strong performance on the tasks of finding opinion phrases and their polarity." ></td>
	<td class="line x" title="6:242	1 Introduction The Web contains a wealth of opinions about products, politicians, and more, which are expressed in newsgroup posts, review sites, and elsewhere." ></td>
	<td class="line x" title="7:242	As a result, the problem of opinion mining has seen increasing attention over the last three years from (Turney, 2002; Hu and Liu, 2004) and many others." ></td>
	<td class="line x" title="8:242	This paper focuses on product reviews, though our methods apply to a broader range of opinions." ></td>
	<td class="line x" title="9:242	Product reviews on Web sites such as amazon.com and elsewhere often associate meta-data with each review indicating how positive (or negative) it is using a 5-star scale, and also rank products by how they fare in the reviews at the site." ></td>
	<td class="line x" title="10:242	However, the readers taste may differ from the reviewers." ></td>
	<td class="line x" title="11:242	For example, the reader may feel strongly about the quality of the gym in a hotel, whereas many reviewers may focus on other aspects of the hotel, such as the decor or the location." ></td>
	<td class="line x" title="12:242	Thus, the reader is forced to wade through a large number of reviews looking for information about particular features of interest." ></td>
	<td class="line x" title="13:242	We decompose the problem of review mining into the following main subtasks: I. Identify product features." ></td>
	<td class="line x" title="14:242	II." ></td>
	<td class="line x" title="15:242	Identify opinions regarding product features." ></td>
	<td class="line x" title="16:242	III." ></td>
	<td class="line x" title="17:242	Determine the polarity of opinions." ></td>
	<td class="line x" title="18:242	IV." ></td>
	<td class="line x" title="19:242	Rank opinions based on their strength." ></td>
	<td class="line x" title="20:242	This paper introduces OPINE, an unsupervised information extraction system that embodies a solution to each of the above subtasks." ></td>
	<td class="line x" title="21:242	OPINE is built on top of the KnowItAll Web information-extraction system (Etzioni et al. , 2005) as detailed in Section 3." ></td>
	<td class="line x" title="22:242	Given a particular product and a corresponding set of reviews, OPINE solves the opinion mining tasks outlined above and outputs a set of product features, each accompanied by a list of associated opinions which are ranked based on strength (e.g. , abominable is stronger than bad)." ></td>
	<td class="line x" title="23:242	This output information can then be used to generate various types of opinion summaries." ></td>
	<td class="line x" title="24:242	This paper focuses on the first 3 review mining subtasks and our contributions are as follows: 1." ></td>
	<td class="line x" title="25:242	We introduce OPINE, a review-mining system whose novel components include the use of relaxation labeling to find the semantic orientation of words in the context of given product features and sentences." ></td>
	<td class="line x" title="26:242	2." ></td>
	<td class="line x" title="27:242	We compare OPINE with the most relevant previous review-mining system (Hu and Liu, 2004) and find that OPINEs precision on the feature extraction task is 22% better though its recall is 3% lower on Hus data sets." ></td>
	<td class="line x" title="28:242	We show that 1/3 of this increase in precision comes from using OPINEs feature assessment mechanism on review data while the rest is due to Web PMI statistics." ></td>
	<td class="line x" title="29:242	3." ></td>
	<td class="line x" title="30:242	While many other systems have used extracted opinion phrases in order to determine the polarity of sentences or documents, OPINE is the first to report its precision and recall on the tasks of opinion phrase extraction and opinion phrase polarity determination in the context of known product features and sentences." ></td>
	<td class="line x" title="31:242	On the first task, OPINE has a precision of 79% and a recall of 76%." ></td>
	<td class="line x" title="32:242	On the second task, OPINE has a precision of 86% and a recall of 89%." ></td>
	<td class="line x" title="33:242	339 Input: product class C, reviews R. Output: set of [feature, ranked opinion list] tuples RparseReviews(R); EfindExplicitFeatures(R, C); OfindOpinions(R, E); COclusterOpinions(O); IfindImplicitFeatures(CO, E); ROrankOpinions(CO); {(f, oi,oj)}outputTuples(RO, IE); Figure 1: OPINE Overview." ></td>
	<td class="line x" title="34:242	The remainder of this paper is organized as follows: Section 2 introduces the basic terminology, Section 3 gives an overview of OPINE, describes and evaluates its main components, Section 4 describes related work and Section 5 presents our conclusion." ></td>
	<td class="line x" title="35:242	2 Terminology A product class (e.g. , Scanner) is a set of products (e.g. , Epson1200)." ></td>
	<td class="line x" title="36:242	OPINE extracts the following types of product features: properties, parts, features of product parts, related concepts, parts and properties of related concepts (see Table 1 for examples of such features in the Scanner domains)." ></td>
	<td class="line x" title="37:242	Related concepts are concepts relevant to the customers experience with the main product (e.g. , the company that manufactures a scanner)." ></td>
	<td class="line x" title="38:242	The relationships between the main product and related concepts are typically expressed as verbs (e.g. , Epson manufactures scanners) or prepositions (scanners from Epson)." ></td>
	<td class="line x" title="39:242	Features can be explicit (good scan quality) or implicit (good scans implies good ScanQuality)." ></td>
	<td class="line x" title="40:242	OPINE also extracts opinion phrases, which are adjective, noun, verb or adverb phrases representing customer opinions." ></td>
	<td class="line x" title="41:242	Opinions can be positive or negative and vary in strength (e.g. , fantastic is stronger than good)." ></td>
	<td class="line x" title="42:242	3 OPINE Overview This section gives an overview of OPINE (see Figure 1) and describes its components and their experimental evaluation." ></td>
	<td class="line x" title="43:242	Goal Given product class C with instances I and reviews R, OPINEs goal is to find a set of (feature, opinions) tuples(f,oi,oj)}s.t. f F and oi,oj O, where: a) F is the set of product class features in R. b) O is the set of opinion phrases in R. c) f is a feature of a particular product instance." ></td>
	<td class="line x" title="44:242	d) o is an opinion about f in a particular sentence." ></td>
	<td class="line x" title="45:242	d) the opinions associated with each feature f are ranked based on their strength." ></td>
	<td class="line x" title="46:242	Solution The steps of our solution are outlined in Figure 1 above." ></td>
	<td class="line x" title="47:242	OPINE parses the reviews using MINIPAR (Lin, 1998) and applies a simple pronoun-resolution module to parsed review data." ></td>
	<td class="line x" title="48:242	OPINE then uses the data to find explicit product features (E)." ></td>
	<td class="line x" title="49:242	OPINEs Feature Assessor and its use of Web PMI statistics are vital for the extraction of high-quality features (see 3.2)." ></td>
	<td class="line x" title="50:242	OPINE then identifies opinion phrases associated with features in E and finds their polarity." ></td>
	<td class="line x" title="51:242	OPINEs novel use of relaxationlabeling techniques for determining the semantic orientation of potential opinion words in the context of given features and sentences leads to high precision and recall on the tasks of opinion phrase extraction and opinion phrase polarity extraction (see 3.3)." ></td>
	<td class="line x" title="52:242	In this paper, we only focus on the extraction of explicit features, identifying corresponding customer opinions about these features and determining their polarity." ></td>
	<td class="line x" title="53:242	We omit the descriptions of the opinion clustering, implicit feature generation and opinion ranking algorithms." ></td>
	<td class="line x" title="54:242	3.0.1 The KnowItAll System." ></td>
	<td class="line x" title="55:242	OPINE is built on top of KnowItAll, a Web-based, domain-independent information extraction system (Etzioni et al. , 2005)." ></td>
	<td class="line x" title="56:242	Given a set of relations of interest, KnowItAll instantiates relation-specific generic extraction patterns into extraction rules which find candidate facts." ></td>
	<td class="line x" title="57:242	KnowItAlls Assessor then assigns a probability to each candidate." ></td>
	<td class="line x" title="58:242	The Assessor uses a form of Point-wise Mutual Information (PMI) between phrases that is estimated from Web search engine hit counts (Turney, 2001)." ></td>
	<td class="line x" title="59:242	It computes the PMI between each fact and automatically generated discriminator phrases (e.g. , is a scanner for the isA() relationship in the context of the Scanner class)." ></td>
	<td class="line x" title="60:242	Given fact f and discriminator d, the computed PMI score is: PMI(f,d) = Hits(d+f)Hits(d)Hits(f) The PMI scores are converted to binary features for a Naive Bayes Classifier, which outputs a probability associated with each fact (Etzioni et al. , 2005)." ></td>
	<td class="line x" title="61:242	3.1 Finding Explicit Features OPINE extracts explicit features for the given product class from parsed review data." ></td>
	<td class="line x" title="62:242	First, the system recursively identifies both the parts and the properties of the given product class and their parts and properties, in turn, continuing until no candidates are found." ></td>
	<td class="line x" title="63:242	Then, the system finds related concepts as described in (Popescu et al. , 2004) and extracts their parts and properties." ></td>
	<td class="line x" title="64:242	Table 1 shows that each feature type contributes to the set of final features (averaged over 7 product classes)." ></td>
	<td class="line x" title="65:242	Explicit Features Examples % Total Properties ScannerSize 7% Parts ScannerCover 52% Features of Parts BatteryLife 24% Related Concepts ScannerImage 9% Related Concepts Features ScannerImageSize 8% Table 1: Explicit Feature Information 340 In order to find parts and properties, OPINE first extracts the noun phrases from reviews and retains those with frequency greater than an experimentally set threshold." ></td>
	<td class="line x" title="66:242	OPINEs Feature Assessor, which is an instantiation of KnowItAlls Assessor, evaluates each noun phrase by computing the PMI scores between the phrase and meronymy discriminators associated with the product class (e.g. , of scanner, scanner has, scanner comes with, etc. for the Scanner class)." ></td>
	<td class="line x" title="67:242	OPINE distinguishes parts from properties using WordNets IS-A hierarchy (which enumerates different kinds of properties) and morphological cues (e.g. , -iness, -ity suffixes)." ></td>
	<td class="line x" title="68:242	3.2 Experiments: Explicit Feature Extraction In our experiments we use sets of reviews for 7 product classes (1621 total reviews) which include the publicly available data sets for 5 product classes from (Hu and Liu, 2004)." ></td>
	<td class="line x" title="69:242	Hus system is the review mining system most relevant to our work." ></td>
	<td class="line x" title="70:242	It uses association rule mining to extract frequent review noun phrases as features." ></td>
	<td class="line x" title="71:242	Frequent features are used to find potential opinion words (only adjectives) and the system uses WordNet synonyms/antonyms in conjunction with a set of seed words in order to find actual opinion words." ></td>
	<td class="line x" title="72:242	Finally, opinion words are used to extract associated infrequent features." ></td>
	<td class="line x" title="73:242	The system only extracts explicit features." ></td>
	<td class="line x" title="74:242	On the 5 datasets in (Hu and Liu, 2004), OPINEs precision is 22% higher than Hus at the cost of a 3% recall drop." ></td>
	<td class="line x" title="75:242	There are two important differences between OPINE and Hus system: a) OPINEs Feature Assessor uses PMI assessment to evaluate each candidate feature and b) OPINE incorporates Web PMI statistics in addition to review data in its assessment." ></td>
	<td class="line x" title="76:242	In the following, we quantify the performance gains from a) and b)." ></td>
	<td class="line x" title="77:242	a) In order to quantify the benefits of OPINEs Feature Assessor, we use it to evaluate the features extracted by Hus algorithm on review data (Hu+A/R)." ></td>
	<td class="line x" title="78:242	The Feature Assessor improves Hus precision by 6%." ></td>
	<td class="line x" title="79:242	b) In order to evaluate the impact of using Web PMI statistics, we assess OPINEs features first on reviews (OP/R) and then on reviews in conjunction with the Web (the corresponding methods are Hu+A/R+W and OPINE)." ></td>
	<td class="line x" title="80:242	Web PMI statistics increase precision by an average of 14.5%." ></td>
	<td class="line x" title="81:242	Overall, 1/3 of OPINEs precision increase over Hus system comes from using PMI assessment on reviews and the other 2/3 from the use of the Web PMI statistics." ></td>
	<td class="line x" title="82:242	In order to show that OPINEs performance is robust across multiple product classes, we used two sets of reviews downloaded from tripadvisor.com for Hotels and amazon.com for Scanners." ></td>
	<td class="line x" title="83:242	Two annotators labeled a set of unique 450 OPINE extractions as correct or incorrect." ></td>
	<td class="line x" title="84:242	The inter-annotator agreement was 86%." ></td>
	<td class="line x" title="85:242	The extractions on which the annotators agreed were used to compute OPINEs precision, which was 89%." ></td>
	<td class="line x" title="86:242	FurData Explicit Feature Extraction: Precision Hu Hu+A/R Hu+A/R+W OP/R OPINE D1 0.75 +0.05 +0.17 +0.07 +0.19 D2 0.71 +0.03 +0.19 +0.08 +0.22 D3 0.72 +0.03 +0.25 +0.09 +0.23 D4 0.69 +0.06 +0.22 +0.08 +0.25 D5 0.74 +0.08 +0.19 +0.04 +0.21 Avg 0.72 +0.06 + 0.20 +0.07 +0.22 Table 2: Precision Comparison on the Explicit FeatureExtraction Task." ></td>
	<td class="line x" title="87:242	OPINEs precision is 22% better than Hus precision; Web PMI statistics are responsible for 2/3 of the precision increase." ></td>
	<td class="line x" title="88:242	All results are reported with respect to Hus. Data Explicit Feature Extraction: Recall Hu Hu+A/R Hu+A/R+W OP/R OPINE D1 0.82 -0.16 -0.08 -0.14 -0.02 D2 0.79 -0.17 -0.09 -0.13 -0.06 D3 0.76 -0.12 -0.08 -0.15 -0.03 D4 0.82 -0.19 -0.04 -0.17 -0.03 D5 0.80 -0.16 -0.06 -0.12 -0.02 Avg 0.80 -0.16 -0.07 -0.14 -0.03 Table 3: Recall Comparison on the Explicit FeatureExtraction Task." ></td>
	<td class="line x" title="89:242	OPINEs recall is 3% lower than the recall of Hus original system (precision level = 0.8)." ></td>
	<td class="line x" title="90:242	All results are reported with respect to Hus. thermore, the annotators extracted explicit features from 800 review sentences (400 for each domain)." ></td>
	<td class="line x" title="91:242	The interannotator agreement was 82%." ></td>
	<td class="line x" title="92:242	OPINEs recall on the set of 179 features on which both annotators agreed was 73%." ></td>
	<td class="line x" title="93:242	3.3 Finding Opinion Phrases and Their Polarity This subsection describes how OPINE extracts potential opinion phrases, distinguishes between opinions and nonopinions, and finds the polarity of each opinion in the context of its associated feature in a particular review sentence." ></td>
	<td class="line x" title="94:242	3.3.1 Extracting Potential Opinion Phrases OPINE uses explicit features to identify potential opinion phrases." ></td>
	<td class="line x" title="95:242	Our intuition is that an opinion phrase associated with a product feature will occur in its vicinity." ></td>
	<td class="line x" title="96:242	This idea is similar to that of (Kim and Hovy, 2004) and (Hu and Liu, 2004), but instead of using a window of size k or the output of a noun phrase chunker, OPINE takes advantage of the syntactic dependencies computed by the MINIPAR parser." ></td>
	<td class="line x" title="97:242	Our intuition is embodied by 10 extraction rules, some of which are shown in Table 4." ></td>
	<td class="line x" title="98:242	If an explicit feature is found in a sentence, OPINE applies the extraction rules in order to find the heads of potential opinion phrases." ></td>
	<td class="line x" title="99:242	Each head word together with its modi341 fiers is returned as a potential opinion phrase1." ></td>
	<td class="line x" title="100:242	Extraction Rules Examples if(M,NP = f)po = M (expensive) scanner if(S = f,P,O)po = O lamp has (problems) if(S,P,O = f)po = P I (hate) this scanner if(S = f,P,O)po = P program (crashed) Table 4: Examples of Domain-independent Rules for the Extraction of Potential Opinion Phrases." ></td>
	<td class="line x" title="101:242	Notation: po=potential opinion, M=modifier, NP=noun phrase, S=subject, P=predicate, O=object." ></td>
	<td class="line x" title="102:242	Extracted phrases are enclosed in parentheses." ></td>
	<td class="line x" title="103:242	Features are indicated by the typewriter font." ></td>
	<td class="line x" title="104:242	The equality conditions on the left-hand side use pos head." ></td>
	<td class="line x" title="105:242	Rule Templates Rules dep(w,wprime) m(w,wprime) v s.t. dep(w,v),dep(v,wprime) v s.t. m(w,v),o(v,wprime) v s.t. dep(w,v),dep(wprime,v) v s.t. m(w,v),o(wprime,v) Table 5: Dependency Rule Templates For Finding Words w, w with Related SO Labels." ></td>
	<td class="line x" title="106:242	OPINE instantiates these templates in order to obtain extraction rules." ></td>
	<td class="line x" title="107:242	Notation: dep=dependent, m=modifier, o=object, v,w,w=words." ></td>
	<td class="line x" title="108:242	OPINE examines the potential opinion phrases in order to identify the actual opinions." ></td>
	<td class="line x" title="109:242	First, the system finds the semantic orientation for the lexical head of each potential opinion phrase." ></td>
	<td class="line x" title="110:242	Every phrase whose head word has a positive or negative semantic orientation is then retained as an opinion phrase." ></td>
	<td class="line x" title="111:242	In the following, we describe how OPINE finds the semantic orientation of words." ></td>
	<td class="line x" title="112:242	3.3.2 Word Semantic Orientation OPINE finds the semantic orientation of a word w in the context of an associated feature f and sentence s. We restate this task as follows: Task Given a set of semantic orientation (SO) labels ({positive,negative,neutral}), a set of reviews and a set of tuples (w, f, s), where w is a potential opinion word associated with feature f in sentence s, assign a SO label to each tuple (w, f, s)." ></td>
	<td class="line x" title="113:242	For example, the tuple (sluggish, driver, I am not happy with this sluggish driver) would be assigned a negative SO label." ></td>
	<td class="line x" title="114:242	Note: We use word to refer to a potential opinion word w and feature to refer to the word or phrase which represents the explicit feature f. Solution OPINE uses the 3-step approach below: 1." ></td>
	<td class="line x" title="115:242	Given the set of reviews, OPINE finds a SO label for each word w. 2." ></td>
	<td class="line x" title="116:242	Given the set of reviews and the set of SO labels for words w, OPINE finds a SO label for each (w, f) pair." ></td>
	<td class="line x" title="117:242	1The (S,P,O) tuples in Table 4 are automatically generated from MINIPARs output." ></td>
	<td class="line x" title="118:242	3." ></td>
	<td class="line x" title="119:242	Given the set of SO labels for (w, f) pairs, OPINE finds a SO label for each (w, f, s) input tuple." ></td>
	<td class="line x" title="120:242	Each of these subtasks is cast as an unsupervised collective classification problem and solved using the same mechanism." ></td>
	<td class="line x" title="121:242	In each case, OPINE is given a set of objects (words, pairs or tuples) and a set of labels (SO labels); OPINE then searches for a global assignment of labels to objects." ></td>
	<td class="line x" title="122:242	In each case, OPINE makes use of local constraints on label assignments (e.g. , conjunctions and disjunctions constraining the assignment of SO labels to words (Hatzivassiloglou and McKeown, 1997))." ></td>
	<td class="line x" title="123:242	A key insight in OPINE is that the problem of searching for a global SO label assignment to words, pairs or tuples while trying to satisfy as many local constraints on assignments as possible is analogous to labeling problems in computer vision (e.g. , model-based matching)." ></td>
	<td class="line x" title="124:242	OPINE uses a well-known computer vision technique, relaxation labeling (Hummel and Zucker, 1983), in order to solve the three subtasks described above." ></td>
	<td class="line x" title="125:242	3.3.3 Relaxation Labeling Overview Relaxation labeling is an unsupervised classification technique which takes as input: a) a set of objects (e.g. , words) b) a set of labels (e.g. , SO labels) c) initial probabilities for each objects possible labels d) the definition of an object os neighborhood (a set of other objects which influence the choice of os label) e) the definition of neighborhood features f) the definition of a support function for an object label The influence of an object os neighborhood on its label L is quantified using the support function." ></td>
	<td class="line x" title="126:242	The support function computes the probability of the label L being assigned to o as a function of os neighborhood features." ></td>
	<td class="line x" title="127:242	Examples of features include the fact that a certain local constraint is satisfied (e.g. , the word nice participates in the conjunction and together with some other word whose SO label is estimated to be positive)." ></td>
	<td class="line x" title="128:242	Relaxation labeling is an iterative procedure whose output is an assignment of labels to objects." ></td>
	<td class="line x" title="129:242	At each iteration, the algorithm uses an update equation to reestimate the probability of an object label based on its previous probability estimate and the features of its neighborhood." ></td>
	<td class="line x" title="130:242	The algorithm stops when the global label assignment stays constant over multiple consecutive iterations." ></td>
	<td class="line x" title="131:242	We employ relaxation labeling for the following reasons: a) it has been extensively used in computer-vision with good results b) its formalism allows for many types of constraints on label assignments to be used simultaneously." ></td>
	<td class="line x" title="132:242	As mentioned before, constraints are integrated into the algorithm as neighborhood features which influence the assignment of a particular label to a particular object." ></td>
	<td class="line x" title="133:242	OPINE uses the following sources of constraints: 342 a) conjunctions and disjunctions in the review text b) manually-supplied syntactic dependency rule templates (see Table 5)." ></td>
	<td class="line x" title="134:242	The templates are automatically instantiated by our system with different dependency relationships (premodifier, postmodifier, subject, etc)." ></td>
	<td class="line x" title="135:242	in order to obtain syntactic dependency rules which find words with related SO labels." ></td>
	<td class="line x" title="136:242	c) automatically derived morphological relationships (e.g. , wonderful and wonderfully are likely to have similar SO labels)." ></td>
	<td class="line x" title="137:242	d) WordNet-supplied synonymy, antonymy, IS-A and morphological relationships between words." ></td>
	<td class="line x" title="138:242	For example, clean and neat are synonyms and so they are likely to have similar SO labels." ></td>
	<td class="line x" title="139:242	Each of the SO label assignment subtasks previously identified is solved using a relaxation labeling step." ></td>
	<td class="line x" title="140:242	In the following, we describe in detail how relaxation labeling is used to find SO labels for words in the given review sets." ></td>
	<td class="line x" title="141:242	3.3.4 Finding SO Labels for Words For many words, a word sense or set of senses is used throughout the review corpus with a consistently positive, negative or neutral connotation (e.g. , great, awful, etc.)." ></td>
	<td class="line x" title="142:242	Thus, in many cases, a word ws SO label in the context of a feature f and sentence s will be the same as its SO label in the context of other features and sentences." ></td>
	<td class="line x" title="143:242	In the following, we describe how OPINEs relaxation labeling mechanism is used to find a words dominant SO label in a set of reviews." ></td>
	<td class="line x" title="144:242	For this task, a words neighborhood is defined as the set of words connected to it through conjunctions, disjunctions and all other relationships previously introduced as sources of constraints." ></td>
	<td class="line x" title="145:242	RL uses an update equation to re-estimate the probability of a word label based on its previous probability estimate and the features of its neighborhood (see Neighborhood Features)." ></td>
	<td class="line x" title="146:242	At iteration m, let q(w,L)(m) denote the support function for label L of w and let P(l(w) = L)(m) denote the probability that L is the label of w. P(l(w) = L)(m+1) is computed as follows: RL Update Equation (Rangarajan, 2000) P(l(w) = L)(m+1) = P(l(w) = L)(m)(1+ q(w,L)(m))P Lprime P(l(w) = Lprime)(m)(1+ q(w,Lprime)(m)) where Lprime  {pos,neg,neutral} and  > 0 is an experimentally set constant keeping the numerator and probabilities positive." ></td>
	<td class="line x" title="147:242	RLs output is an assignment of dominant SO labels to words." ></td>
	<td class="line x" title="148:242	In the following, we describe in detail the initialization step, the derivation of the support function formula and the use of neighborhood features." ></td>
	<td class="line x" title="149:242	RL Initialization Step OPINE uses a version of Turneys PMI-based approach (Turney, 2003) in order to derive the initial probability estimates (P(l(w) = L)(0)) for a subset S of the words." ></td>
	<td class="line x" title="150:242	OPINE computes a SO score so(w) for each w in S as the difference between the PMI of w with positive keywords (e.g. , excellent) and the PMI of w with negative keywords (e.g. , awful)." ></td>
	<td class="line x" title="151:242	When so(w) is small, or w rarely co-occurs with the keywords, w is classified as neutral." ></td>
	<td class="line x" title="152:242	If so(w) > 0, then w is positive, otherwise w is negative." ></td>
	<td class="line x" title="153:242	OPINE then uses the labeled S set in order to compute prior probabilities P(l(w) = L), L {pos,neg,neutral} by computing the ratio between the number of words in S labeled L and |S|." ></td>
	<td class="line x" title="154:242	Such probabilities are used as initial probability estimates associated with the labels of the remaining words." ></td>
	<td class="line x" title="155:242	Support Function The support function computes the probability of each label for word w based on the labels of objects in ws neighborhood N. Let Ak = {(wj,Lj)|wj  N}, 0 < k  3|N| represent one of the potential assignments of labels to the words in N. Let P(Ak)(m) denote the probability of this particular assignment at iteration m. The support for label L of word w at iteration m is : q(w,L)(m) = 3|N|X k=1 P(l(w) = L|Ak)(m)  P(Ak)(m) We assume that the labels of ws neighbors are independent of each other and so the formula becomes: q(w,L)(m) = 3|N|X k=1 P(l(w) = L|Ak)(m) |N|Y j=1 P(l(wj) = Lj)(m) Every P(l(wj) = Lj)(m) term is the estimate for the probability that l(wj) = Lj (which was computed at iteration m using the RL update equation)." ></td>
	<td class="line x" title="156:242	The P(l(w) = L|Ak)(m) term quantifies the influence of a particular label assignment to ws neighborhood over ws label." ></td>
	<td class="line x" title="157:242	In the following, we describe how we estimate this term." ></td>
	<td class="line x" title="158:242	Neighborhood Features Each type of word relationship which constrains the assignment of SO labels to words (synonymy, antonymy, etc)." ></td>
	<td class="line x" title="159:242	is mapped by OPINE to a neighborhood feature." ></td>
	<td class="line x" title="160:242	This mapping allows OPINE to use simultaneously use multiple independent sources of constraints on the label of a particular word." ></td>
	<td class="line x" title="161:242	In the following, we formalize this mapping." ></td>
	<td class="line x" title="162:242	Let T denote the type of a word relationship in R (synonym, antonym, etc)." ></td>
	<td class="line x" title="163:242	and let Ak,T represent the labels assigned by Ak to neighbors of a word w which are connected to w through a relationship of type T . We have Ak =uniontextT Ak,T and P(l(w) = L|Ak)(m) = P(l(w) = L| [ T Ak,T)(m) For each relationship type T, OPINE defines a neighborhood feature fT(w,L,Ak,T) which computes P(l(w) = L|Ak,T), the probability that ws label is L given Ak,T (see below)." ></td>
	<td class="line x" title="164:242	P(l(w) = L|uniontextT Ak,T)(m) is estimated combining the information from various features about ws label using the sigmoid function (): 343 P(l(w) = L|Ak)(m) = ( jX i=1 fi(w,L,Ak,i)(m)  ci) where c0,cj are weights whose sum is 1 and which reflect OPINE s confidence in each type of feature." ></td>
	<td class="line x" title="165:242	Given word w, label L, relationship type T and neighborhood label assignment Ak, let NT represent the subset of ws neighbors connected to w through a type T relationship." ></td>
	<td class="line x" title="166:242	The feature fT computes the probability that ws label is L given the labels assigned by Ak to words in NT . Using Bayess Law and assuming that these labels are independent given l(w), we have the following formula for fT at iteration m: fT(w,L,Ak,T)(m) = P(l(w) = L)(m) |NT|Y j=1 P(Lj|l(w) = L) P(Lj|l(w) = L) is the probability that word wj has label Lj if wj and w are linked by a relationship of type T and w has label L. We make the simplifying assumption that this probability is constant and depends only of T, L and Lprime, not of the particular words wj and w. For each tuple (T, L, Lj), L,Lj {pos,neg,neutral}, OPINE builds a probability table using a small set of bootstrapped positive, negative and neutral words." ></td>
	<td class="line x" title="167:242	3.3.5 Finding (Word, Feature) SO Labels This subtask is motivated by the existence of frequent words which change their SO label based on associated features, but whose SO labels in the context of the respective features are consistent throughout the reviews (e.g. , in the Hotel domain, hot water has a consistently positive connotation, whereas hot room has a negative one)." ></td>
	<td class="line x" title="168:242	In order to solve this task, OPINE first assigns each (w,f) pair an initial SO label which is ws SO label." ></td>
	<td class="line x" title="169:242	The system then executes a relaxation labeling step during which syntactic relationships between words and, respectively, between features, are used to update the default SO labels whenever necessary." ></td>
	<td class="line x" title="170:242	For example, (hot, room) appears in the proximity of (broken, fan)." ></td>
	<td class="line x" title="171:242	If roomand fan are conjoined by and, this suggests that hot and broken have similar SO labels in the context of their respective features." ></td>
	<td class="line x" title="172:242	If broken has a strongly negative semantic orientation, this fact contributes to OPINEs belief that hot may also be negative in this context." ></td>
	<td class="line x" title="173:242	Since (hot, room) occurs in the vicinity of other such phrases (e.g. , stifling kitchen), hot acquires a negative SO label in the context of room." ></td>
	<td class="line x" title="174:242	3.3.6 Finding (Word, Feature, Sentence) SO Labels This subtask is motivated by the existence of (w,f) pairs (e.g. , (big, room)) for which ws orientation changes based on the sentence in which the pair appears (e.g. ,  I hated the big, drafty room because I ended up freezing. vs. We had a big, luxurious room.)" ></td>
	<td class="line x" title="175:242	In order to solve this subtask, OPINE first assigns each (w,f,s) tuple an initial label which is simply the SO label for the (w,f) pair." ></td>
	<td class="line x" title="176:242	The system then uses syntactic relationships between words and, respectively, features in order to update the SO labels when necessary." ></td>
	<td class="line x" title="177:242	For example, in the sentence I hated the big, drafty room because I ended up freezing., big and hate satisfy condition 2 in Table 5 and therefore OPINE expects them to have similar SO labels." ></td>
	<td class="line x" title="178:242	Since hate has a strong negative connotation, big acquires a negative SO label in this context." ></td>
	<td class="line x" title="179:242	In order to correctly update SO labels in this last step, OPINE takes into consideration the presence of negation modifiers." ></td>
	<td class="line x" title="180:242	For example, in the sentence I dont like a large scanner either, OPINE first replaces the positive (w,f) pair (like, scanner) with the negative labeled pair (not like, scanner) and then infers that large is likely to have a negative SO label in this context." ></td>
	<td class="line x" title="181:242	3.3.7 Identifying Opinion Phrases After OPINE has computed the most likely SO labels for the head words of each potential opinion phrase in the context of given features and sentences, OPINE can extract opinion phrases and establish their polarity." ></td>
	<td class="line x" title="182:242	Phrases whose head words have been assigned positive or negative labels are retained as opinion phrases." ></td>
	<td class="line x" title="183:242	Furthermore, the polarity of an opinion phrase o in the context of a feature f and sentence s is given by the SO label assigned to the tuple (head(o),f,s) (3.3.6 shows how OPINE takes into account negation modifiers)." ></td>
	<td class="line x" title="184:242	3.4 Experiments In this section we evaluate OPINEs performance on the following tasks: finding SO labels of words in the context of known features and sentences (SO label extraction); distinguishing between opinion and non-opinion phrases in the context of known features and sentences (opinion phrase extraction); finding the correct polarity of extracted opinion phrases in the context of known features and sentences (opinion phrase polarity extraction)." ></td>
	<td class="line x" title="185:242	While other systems, such as (Hu and Liu, 2004; Turney, 2002), have addressed these tasks to some degree, OPINE is the first to report results." ></td>
	<td class="line x" title="186:242	We first ran OPINE on 13841 sentences and 538 previously extracted features." ></td>
	<td class="line x" title="187:242	OPINE searched for a SO label assignment for 1756 different words in the context of the given features and sentences." ></td>
	<td class="line x" title="188:242	We compared OPINE against two baseline methods, PMI++ and Hu++." ></td>
	<td class="line x" title="189:242	PMI++ is an extended version of (Turney, 2002)s method for finding the SO label of a phrase (as an attempt to deal with context-sensitive words)." ></td>
	<td class="line x" title="190:242	For a given (word, feature, sentence) tuple, PMI++ ignores the sentence, generates a phrase based on the word and the feature (e.g. , (clean, room): clean room) and finds its SO label using PMI statistics." ></td>
	<td class="line x" title="191:242	If unsure of the label, PMI++ tries to find the orientation of the potential opinion word instead." ></td>
	<td class="line x" title="192:242	The search engine queries use domain-specific keywords (e.g. , scanner), which are dropped if they 344 lead to low counts." ></td>
	<td class="line x" title="193:242	Hu++ is a WordNet-based method for finding a words context-independent semantic orientation." ></td>
	<td class="line x" title="194:242	It extends Hus adjective labeling method in a number of ways in order to handle nouns, verbs and adverbs in addition to adjectives and in order to improve coverage." ></td>
	<td class="line x" title="195:242	Hus method starts with two sets of positive and negative words and iteratively grows each one by including synonyms and antonyms from WordNet." ></td>
	<td class="line x" title="196:242	The final sets are used to predict the orientation of an incoming word." ></td>
	<td class="line x" title="197:242	Type PMI++ Hu++ OPINE P R P R P R adj 0.73 0.91 +0.02 -0.17 +0.07 -0.03 nn 0.63 0.92 +0.04 -0.24 +0.11 -0.08 vb 0.71 0.88 +0.03 -0.12 +0.01 -0.01 adv 0.82 0.92 +0.02 -0.01 +0.06 +0.01 Avg 0.72 0.91 +0.03 -0.14 +0.06 -0.03 Table 6: Finding SO Labels of Potential Opinion Words in the Context of Given Product Features and Sentences." ></td>
	<td class="line x" title="198:242	OPINEs precision is higher than that of PMI++ and Hu++." ></td>
	<td class="line x" title="199:242	All results are reported with respect to PMI++ . Notation: adj=adjectives, nn=nouns, vb=verbs, adv=adverbs 3.4.1 Experiments: SO Labels On the task of finding SO labels for words in the context of given features and review sentences, OPINE obtains higher precision than both baseline methods at a small loss in recall with respect to PMI++." ></td>
	<td class="line x" title="200:242	As described below, this result is due in large part to OPINEs ability to handle context-sensitive opinion words." ></td>
	<td class="line x" title="201:242	We randomly selected 200 (word, feature, sentence) tuples for each word type (adjective, adverb, etc)." ></td>
	<td class="line x" title="202:242	and obtained a test set containing 800 tuples." ></td>
	<td class="line x" title="203:242	Two annotators assigned positive, negative and neutral labels to each tuple (the inter-annotator agreement was 78%)." ></td>
	<td class="line x" title="204:242	We retained the tuples on which the annotators agreed as the gold standard." ></td>
	<td class="line x" title="205:242	We ran PMI++ and Hu++ on the test data and compared the results against OPINEs results on the same data." ></td>
	<td class="line x" title="206:242	In order to quantify the benefits of each of the three steps of our method for finding SO labels, we also compared OPINE with a version which only finds SO labels for words and a version which finds SO labels for words in the context of given features, but doesnt take into account given sentences." ></td>
	<td class="line x" title="207:242	We have learned from this comparison that OPINEs precision gain over PMI++ and Hu++ is mostly due to to its ability to handle contextsensitive words in a large number of cases." ></td>
	<td class="line x" title="208:242	Although Hu++ does not handle context-sensitive SO label assignment, its average precision was reasonable (75%) and better than that of PMI++." ></td>
	<td class="line x" title="209:242	Finding a words SO label is good enough in the case of strongly positive or negative opinion words, which account for the majority of opinion instances." ></td>
	<td class="line x" title="210:242	The methods loss in recall is due to not recognizing words absent from WordNet (e.g. , depth-adjustable) or not having enough information to classify some words in WordNet." ></td>
	<td class="line x" title="211:242	PMI++ typically does well in the presence of strongly positive or strongly negative words." ></td>
	<td class="line x" title="212:242	Its high recall is correlated with decreased precision, but overall this simple approach does well." ></td>
	<td class="line x" title="213:242	PMI++s main shortcoming is misclassifying terms such as basic or visible which change orientation based on context." ></td>
	<td class="line x" title="214:242	3.4.2 Experiments: Opinion Phrases In order to evaluate OPINE on the tasks of opinion phrase extraction and opinion phrase polarity extraction in the context of known features and sentences, we used a set of 550 sentences containing previously extracted features." ></td>
	<td class="line x" title="215:242	The sentences were annotated with the opinion phrases corresponding to the known features and with the opinion polarity." ></td>
	<td class="line x" title="216:242	We compared OPINE with PMI++ and Hu++ on the tasks of interest." ></td>
	<td class="line x" title="217:242	We found that OPINE had the highest precision on both tasks at a small loss in recall with respect to PMI++." ></td>
	<td class="line x" title="218:242	OPINEs ability to identify a words SO label in the context of a given feature and sentence allows the system to correctly extract opinions expressed by words such as big or small, whose semantic orientation varies based on context." ></td>
	<td class="line x" title="219:242	Measure PMI++ Hu++ OPINE OP Extraction: Precision 0.71 +0.06 +0.08 OP Extraction: Recall 0.78 -0.08 -0.02 OP Polarity: Precision 0.80 -0.04 +0.06 OP Polarity: Recall 0.93 +0.07 -0.04 Table 7: Extracting Opinion Phrases and Opinion Phrase Polarity Corresponding to Known Features and Sentences." ></td>
	<td class="line x" title="220:242	OPINEs precision is higher than that of PMI++ and of Hu++." ></td>
	<td class="line x" title="221:242	All results are reported with respect to PMI++." ></td>
	<td class="line x" title="222:242	4 Related Work The key components of OPINE described in this paper are the PMI feature assessment which leads to high-precision feature extraction and the use of relaxation-labeling in order to find the semantic orientation of potential opinion words." ></td>
	<td class="line x" title="223:242	The review-mining work most relevant to our research is that of (Hu and Liu, 2004) and (Kobayashi et al. , 2004)." ></td>
	<td class="line x" title="224:242	Both identify product features from reviews, but OPINE significantly improves on both." ></td>
	<td class="line x" title="225:242	(Hu and Liu, 2004) doesnt assess candidate features, so its precision is lower than OPINEs." ></td>
	<td class="line x" title="226:242	(Kobayashi et al. , 2004) employs an iterative semi-automatic approach which requires human input at every iteration." ></td>
	<td class="line x" title="227:242	Neither model explicitly addresses composite (feature of feature) or implicit features." ></td>
	<td class="line x" title="228:242	Other systems (Morinaga et al. , 2002; Kushal et al. , 2003) also look at Web product reviews but they do not extract 345 opinions about particular product features." ></td>
	<td class="line x" title="229:242	OPINEs use of meronymy lexico-syntactic patterns is similar to that of many others, from (Berland and Charniak, 1999) to (Almuhareb and Poesio, 2004)." ></td>
	<td class="line x" title="230:242	Recognizing the subjective character and polarity of words, phrases or sentences has been addressed by many authors, including (Turney, 2003; Riloff et al. , 2003; Wiebe, 2000; Hatzivassiloglou and McKeown, 1997)." ></td>
	<td class="line x" title="231:242	Most recently, (Takamura et al. , 2005) reports on the use of spin models to infer the semantic orientation of words." ></td>
	<td class="line x" title="232:242	The papers global optimization approach and use of multiple sources of constraints on a words semantic orientation is similar to ours, but the mechanism differs and they currently omit the use of syntactic information." ></td>
	<td class="line oc" title="233:242	Subjective phrases are used by (Turney, 2002; Pang and Vaithyanathan, 2002; Kushal et al. , 2003; Kim and Hovy, 2004) and others in order to classify reviews or sentences as positive or negative." ></td>
	<td class="line x" title="234:242	So far, OPINEs focus has been on extracting and analyzing opinion phrases corresponding to specific features in specific sentences, rather than on determining sentence or review polarity." ></td>
	<td class="line x" title="235:242	5 Conclusion OPINE is an unsupervised information extraction system which extracts fine-grained features, and associated opinions, from reviews." ></td>
	<td class="line x" title="236:242	OPINEs use of the Web as a corpus helps identify product features with improved precision compared with previous work." ></td>
	<td class="line x" title="237:242	OPINE uses a novel relaxation-labeling technique to determine the semantic orientation of potential opinion words in the context of the extracted product features and specific review sentences; this technique allows the system to identify customer opinions and their polarity with high precision and recall." ></td>
	<td class="line x" title="238:242	6 Acknowledgments We would like to thank the KnowItAll project and the anonymous reviewers for their comments." ></td>
	<td class="line x" title="239:242	Michael Gamon, Costas Boulis and Adam Carlson have also provided valuable feedback." ></td>
	<td class="line x" title="240:242	We thank Minquing Hu and Bing Liu for providing their data sets and for their comments." ></td>
	<td class="line x" title="241:242	Finally, we are grateful to Bernadette Minton and Fetch Technologies for their help in collecting additional reviews." ></td>
	<td class="line x" title="242:242	This research was supported in part by NSF grant IIS-0312988, DARPA contract NBCHD030010, ONR grant N00014-02-1-0324 as well as gifts from Google and the Turing Center." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="H05-1045
Identifying Sources Of Opinions With Conditional Random Fields And Extraction Patterns
Choi, Yejin;Cardie, Claire;Riloff, Ellen;Patwardhan, Siddharth;"></td>
	<td class="line x" title="1:194	Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 355362, Vancouver, October 2005." ></td>
	<td class="line x" title="2:194	c2005 Association for Computational Linguistics Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns Yejin Choi and Claire Cardie Department of Computer Science Cornell University Ithaca, NY 14853 {ychoi,cardie}@cs.cornell.edu Ellen Riloff and Siddharth Patwardhan School of Computing University of Utah Salt Lake City, UT 84112 {riloff,sidd}@cs.utah.edu Abstract Recent systems have been developed for sentiment classification, opinion recognition, and opinion analysis (e.g. , detecting polarity and strength)." ></td>
	<td class="line x" title="3:194	We pursue another aspect of opinion analysis: identifying the sources of opinions, emotions, and sentiments." ></td>
	<td class="line x" title="4:194	We view this problem as an information extraction task and adopt a hybrid approach that combines Conditional Random Fields (Lafferty et al. , 2001) and a variation of AutoSlog (Riloff, 1996a)." ></td>
	<td class="line x" title="5:194	While CRFs model source identification as a sequence tagging task, AutoSlog learns extraction patterns." ></td>
	<td class="line x" title="6:194	Our results show that the combination of these two methods performs better than either one alone." ></td>
	<td class="line x" title="7:194	The resulting system identifies opinion sources with 79.3% precision and 59.5% recall using a head noun matching measure, and 81.2% precision and 60.6% recall using an overlap measure." ></td>
	<td class="line x" title="8:194	1 Introduction In recent years, there has been a great deal of interest in methods for automatically identifying opinions, emotions, and sentiments in text." ></td>
	<td class="line oc" title="9:194	Much of this research explores sentiment classification, a text categorization task in which the goal is to classify a document as having positive or negative polarity (e.g. , Das and Chen (2001), Pang et al.(2002), Turney (2002), Dave et al.(2003), Pang and Lee (2004))." ></td>
	<td class="line x" title="12:194	Other research efforts analyze opinion expressions at the sentence level or below to recognize opinions, their polarity, and their strength (e.g. , Dave et al.(2003), Pang and Lee (2004), Wilson et al.(2004), Yu and Hatzivassiloglou (2003), Wiebe and Riloff (2005))." ></td>
	<td class="line x" title="15:194	Many applications could benefit from these opinion analyzers, including product reputation tracking (e.g. , Morinaga et al.(2002), Yi et al.(2003)), opinion-oriented summarization (e.g. , Cardie et al.(2004)), and question answering (e.g. , Bethard et al.(2004), Yu and Hatzivassiloglou (2003))." ></td>
	<td class="line x" title="20:194	We focus here on another aspect of opinion analysis: automatically identifying the sources of the opinions." ></td>
	<td class="line x" title="21:194	Identifying opinion sources will be especially critical for opinion-oriented questionanswering systems (e.g. , systems that answer questions of the form How does [X] feel about [Y]?) and opinion-oriented summarization systems, both of which need to distinguish the opinions of one source from those of another.1 The goal of our research is to identify direct and indirect sources of opinions, emotions, sentiments, and other private states that are expressed in text." ></td>
	<td class="line x" title="22:194	To illustrate the nature of this problem, consider the examples below: S1: Taiwan-born voters favoring independence 1In related work, we investigate methods to identify the opinion expressions (e.g. , Riloff and Wiebe (2003), Wiebe and Riloff (2005), Wilson et al.(2005)) and the nesting structure of sources (e.g. , Breck and Cardie (2004))." ></td>
	<td class="line x" title="24:194	The target of each opinion, i.e., what the opinion is directed towards, is currently being annotated manually for our corpus." ></td>
	<td class="line x" title="25:194	355 S2: According to the report, the human rights record in China is horrendous." ></td>
	<td class="line x" title="26:194	S3: International officers believe that the EU will prevail." ></td>
	<td class="line x" title="27:194	S4: International officers said US officials want the EU to prevail." ></td>
	<td class="line x" title="28:194	In S1, the phrase Taiwan-born voters is the direct (i.e. , first-hand) source of the favoring sentiment." ></td>
	<td class="line x" title="29:194	In S2, the report is the direct source of the opinion about Chinas human rights record." ></td>
	<td class="line x" title="30:194	In S3, International officers are the direct source of an opinion regarding the EU." ></td>
	<td class="line x" title="31:194	The same phrase in S4, however, denotes an indirect (i.e. , second-hand, third-hand, etc)." ></td>
	<td class="line x" title="32:194	source of an opinion whose direct source is US officials." ></td>
	<td class="line x" title="33:194	In this paper, we view source identification as an information extraction task and tackle the problem using sequence tagging and pattern matching techniques simultaneously." ></td>
	<td class="line x" title="34:194	Using syntactic, semantic, and orthographic lexical features, dependency parse features, and opinion recognition features, we train a linear-chain Conditional Random Field (CRF) (Lafferty et al. , 2001) to identify opinion sources." ></td>
	<td class="line x" title="35:194	In addition, we employ features based on automatically learned extraction patterns and perform feature induction on the CRF model." ></td>
	<td class="line x" title="36:194	We evaluate our hybrid approach using the NRRC corpus (Wiebe et al. , 2005), which is manually annotated with direct and indirect opinion source information." ></td>
	<td class="line x" title="37:194	Experimental results show that the CRF model performs well, and that both the extraction patterns and feature induction produce performance gains." ></td>
	<td class="line x" title="38:194	The resulting system identifies opinion sources with 79.3% precision and 59.5% recall using a head noun matching measure, and 81.2% precision and 60.6% recall using an overlap measure." ></td>
	<td class="line x" title="39:194	2 The Big Picture The goal of information extraction (IE) systems is to extract information about events, including the participants of the events." ></td>
	<td class="line x" title="40:194	This task goes beyond Named Entity recognition (e.g. , Bikel et al.(1997)) because it requires the recognition of role relationships." ></td>
	<td class="line x" title="42:194	For example, an IE system that extracts information about corporate acquisitions must distinguish between the company that is doing the acquiring and the company that is being acquired." ></td>
	<td class="line x" title="43:194	Similarly, an IE system that extracts information about terrorism must distinguish between the person who is the perpetrator and the person who is the victim." ></td>
	<td class="line x" title="44:194	We hypothesized that IE techniques would be wellsuited for source identification because an opinion statement can be viewed as a kind of speech event with the source as the agent." ></td>
	<td class="line x" title="45:194	We investigate two very different learning-based methods from information extraction for the problem of opinion source identification: graphical models and extraction pattern learning." ></td>
	<td class="line x" title="46:194	In particular, we consider Conditional Random Fields (Lafferty et al. , 2001) and a variation of AutoSlog (Riloff, 1996a)." ></td>
	<td class="line x" title="47:194	CRFs have been used successfully for Named Entity recognition (e.g. , McCallum and Li (2003), Sarawagi and Cohen (2004)), and AutoSlog has performed well on information extraction tasks in several domains (Riloff, 1996a)." ></td>
	<td class="line x" title="48:194	While CRFs treat source identification as a sequence tagging task, AutoSlog views the problem as a pattern-matching task, acquiring symbolic patterns that rely on both the syntax and lexical semantics of a sentence." ></td>
	<td class="line x" title="49:194	We hypothesized that a combination of the two techniques would perform better than either one alone." ></td>
	<td class="line x" title="50:194	Section 3 describes the CRF approach to identifying opinion sources and the features that the system uses." ></td>
	<td class="line x" title="51:194	Section 4 then presents a new variation of AutoSlog, AutoSlog-SE, which generates IE patterns to extract sources." ></td>
	<td class="line x" title="52:194	Section 5 describes the hybrid system: we encode the IE patterns as additional features in the CRF model." ></td>
	<td class="line x" title="53:194	Finally, Section 6 presents our experimental results and error analysis." ></td>
	<td class="line x" title="54:194	3 Semantic Tagging via Conditional Random Fields We defined the problem of opinion source identification as a sequence tagging task via CRFs as follows." ></td>
	<td class="line x" title="55:194	Given a sequence of tokens, x = x1x2xn, we need to generate a sequence of tags, or labels, y = y1y2yn." ></td>
	<td class="line x" title="56:194	We define the set of possible label values as S, T, -, where S is the first token (or Start) of a source, T is a non-initial token (i.e. , a conTinuation) of a source, and -is a token that is not part of any source.2 A detailed description of CRFs can be found in 2This is equivalent to the IOB tagging scheme used in syntactic chunkers (Ramshaw and Marcus, 1995)." ></td>
	<td class="line x" title="57:194	356 Lafferty et al.(2001)." ></td>
	<td class="line x" title="59:194	For our sequence tagging problem, we create a linear-chain CRF based on an undirected graph G = (V,E), where V is the set of random variables Y = fYij1 i ng, one for each of n tokens in an input sentence; and E = f(Yi1,Yi)j1 < i ng is the set of n 1 edges forming a linear chain." ></td>
	<td class="line x" title="60:194	For each sentence x, we define a non-negative clique potential exp(summationtextKk=1 kfk(yi1,yi,x)) for each edge, and exp(summationtextKprimek=1 primekfprimek(yi,x)) for each node, where fk() is a binary feature indicator function, k is a weight assigned for each feature function, and K and Kprime are the number of features defined for edges and nodes respectively." ></td>
	<td class="line x" title="61:194	Following Lafferty et al.(2001), the conditional probability of a sequence of labels y given a sequence of tokens x is: P(y|x) = 1Z x exp X i,k k fk(yi1, yi, x)+ X i,k primek fprimek(yi, x)  (1) Zx = X y exp X i,k k fk(yi1, yi, x) + X i,k primek fprimek(yi, x)  (2) where Zx is a normalization constant for each x. Given the training data D, a set of sentences paired with their correct ST- source label sequences, the parameters of the model are trained to maximize the conditional log-likelihoodproducttext (x,y)D P(yjx)." ></td>
	<td class="line x" title="63:194	For inference, given a sentence x in the test data, the tagging sequence y is given by argmaxyprimeP(yprimejx)." ></td>
	<td class="line x" title="64:194	3.1 Features To develop features, we considered three properties of opinion sources." ></td>
	<td class="line x" title="65:194	First, the sources of opinions are mostly noun phrases." ></td>
	<td class="line x" title="66:194	Second, the source phrases should be semantic entities that can bear or express opinions." ></td>
	<td class="line x" title="67:194	Third, the source phrases should be directly related to an opinion expression." ></td>
	<td class="line x" title="68:194	When considering only the first and second criteria, this task reduces to named entity recognition." ></td>
	<td class="line x" title="69:194	Because of the third condition, however, the task requires the recognition of opinion expressions and a more sophisticated encoding of sentence structure to capture relationships between source phrases and opinion expressions." ></td>
	<td class="line x" title="70:194	With these properties in mind, we define the following features for each token/word xi in an input sentence." ></td>
	<td class="line x" title="71:194	For pedagogical reasons, we will describe some of the features as being multi-valued or categorical features." ></td>
	<td class="line x" title="72:194	In practice, however, all features are binarized for the CRF model." ></td>
	<td class="line x" title="73:194	Capitalization features We use two boolean features to represent the capitalization of a word: all-capital, initial-capital." ></td>
	<td class="line o" title="74:194	Part-of-speech features Based on the lexical categories produced by GATE (Cunningham et al. , 2002), each token xi is classified into one of a set of coarse part-of-speech tags: noun, verb, adverb, wh-word, determiner, punctuation, etc. We do the same for neighboring words in a [ 2, +2] window in order to assist noun phrase segmentation." ></td>
	<td class="line x" title="75:194	Opinion lexicon features For each token xi, we include a binary feature that indicates whether or not the word is in our opinion lexicon  a set of words that indicate the presence of an opinion." ></td>
	<td class="line x" title="76:194	We do the same for neighboring words in a [ 1, +1] window." ></td>
	<td class="line x" title="77:194	Additionally, we include for xi a feature that indicates the opinion subclass associated with xi, if available from the lexicon." ></td>
	<td class="line x" title="78:194	(e.g., bless is classified as moderately subjective according to the lexicon, while accuse and berate are classified more specifically as judgments)." ></td>
	<td class="line x" title="79:194	The lexicon is initially populated with approximately 500 opinion words 3 from (Wiebe et al. , 2002), and then augmented with opinion words identified in the training data." ></td>
	<td class="line x" title="80:194	The training data contains manually produced phrase-level annotations for all expressions of opinions, emotions, etc.(Wiebe et al. , 2005)." ></td>
	<td class="line x" title="82:194	We collected all content words that occurred in the training set such that at least 50% of their occurrences were in opinion annotations." ></td>
	<td class="line x" title="83:194	Dependency tree features For each token xi, we create features based on the parse tree produced by the Collins (1999) dependency parser." ></td>
	<td class="line x" title="84:194	The purpose of the features is to (1) encode structural information, and (2) indicate whether xi is involved in any grammatical relations with an opinion word." ></td>
	<td class="line x" title="85:194	Two pre-processing steps are required before features can be constructed: 3Some words are drawn from Levin (1993); others are from Framenet lemmas (Baker et al. 1998) associated with communication verbs." ></td>
	<td class="line x" title="86:194	357 1." ></td>
	<td class="line x" title="87:194	Syntactic chunking." ></td>
	<td class="line x" title="88:194	We traverse the dependency tree using breadth-first search to identify and group syntactically related nodes, producing a flatter, more concise tree." ></td>
	<td class="line x" title="89:194	Each syntactic chunk is also assigned a grammatical role (e.g. , subject, object, verb modifier, time, location, of-pp, by-pp) based on its constituents." ></td>
	<td class="line x" title="90:194	Possessives (e.g. , Clintons idea) and the phrase according to X are handled as special cases in the chunking process." ></td>
	<td class="line x" title="91:194	2." ></td>
	<td class="line x" title="92:194	Opinion word propagation." ></td>
	<td class="line x" title="93:194	Although the opinion lexicon contains only content words and no multi-word phrases, actual opinions often comprise an entire phrase, e.g., is really willing or in my opinion." ></td>
	<td class="line x" title="94:194	As a result, we mark as an opinion the entire chunk that contains an opinion word." ></td>
	<td class="line x" title="95:194	This allows each token in the chunk to act as an opinion word for feature encoding." ></td>
	<td class="line x" title="96:194	After syntactic chunking and opinion word propagation, we create the following dependency tree features for each token xi: the grammatical role of its chunk the grammatical role of xi1s chunk whether the parent chunk includes an opinion word whether xis chunk is in an argument position with respect to the parent chunk whether xi represents a constituent boundary Semantic class features We use 7 binary features to encode the semantic class of each word xi: authority, government, human, media, organizationor company, proper name, and other." ></td>
	<td class="line x" title="97:194	The other class captures 13 semantic classes that cannot be sources, such as vehicle and time." ></td>
	<td class="line x" title="98:194	Semantic class information is derived from named entity and semantic class labels assigned to xi by the Sundance shallow parser (Riloff, 2004)." ></td>
	<td class="line x" title="99:194	Sundance uses named entity recognition rules to label noun phrases as belonging to named entity classes, and assigns semantic tags to individual words based on a semantic dictionary." ></td>
	<td class="line x" title="100:194	Table 1 shows the hierarchy that Sundance uses for semantic classes associated with opinion sources." ></td>
	<td class="line x" title="101:194	Sundance is also used to recognize and instantiate the source extraction patterns PROPER NAMEAUTHORITY LOCATION CITY COUNTRY PLANET PROVINCE PERSON NAME PERSON DESC NATIONALITY TITLE COMPANY GOVERNMENT MEDIA ORGANIZATION HUMAN SOURCE Figure 1: The semantic hierarchy for opinion sources that are learned by AutoSlog-SE, which is described in the next section." ></td>
	<td class="line x" title="102:194	4 Semantic Tagging via Extraction Patterns We also learn patterns to extract opinion sources using a statistical adaptation of the AutoSlog IE learning algorithm." ></td>
	<td class="line x" title="103:194	AutoSlog (Riloff, 1996a) is a supervised extraction pattern learner that takes a training corpus of texts and their associated answer keys as input." ></td>
	<td class="line x" title="104:194	A set of heuristics looks at the context surrounding each answer and proposes a lexicosyntactic pattern to extract that answer from the text." ></td>
	<td class="line x" title="105:194	The heuristics are not perfect, however, so the resulting set of patterns needs to be manually reviewed by a person." ></td>
	<td class="line x" title="106:194	In order to build a fully automatic system that does not depend on manual review, we combined AutoSlogs heuristics with statistics from the annotated training data to create a fully automatic supervised learner." ></td>
	<td class="line x" title="107:194	We will refer to this learner as AutoSlog-SE (Statistically Enhanced variation of AutoSlog)." ></td>
	<td class="line x" title="108:194	AutoSlog-SEs learning process has three steps: Step 1: AutoSlogs heuristics are applied to every noun phrase (NP) in the training corpus." ></td>
	<td class="line x" title="109:194	This generates a set of extraction patterns that, collectively, can extract every NP in the training corpus." ></td>
	<td class="line x" title="110:194	Step 2: The learned patterns are augmented with selectional restrictions that semantically constrain the types of noun phrases that are legitimate extractions for opinion sources." ></td>
	<td class="line x" title="111:194	We used 358 the semantic classes shown in Figure 1 as selectional restrictions." ></td>
	<td class="line x" title="112:194	Step 3: The patterns are applied to the training corpus and statistics are gathered about their extractions." ></td>
	<td class="line x" title="113:194	We count the number of extractions that match annotations in the corpus (correct extractions) and the number of extractions that do not match annotations (incorrect extractions)." ></td>
	<td class="line x" title="114:194	These counts are then used to estimate the probability that the pattern will extract an opinion source in new texts: P(source | patterni) = correct sourcescorrect sources + incorrect sources This learning process generates a set of extraction patterns coupled with probabilities." ></td>
	<td class="line x" title="115:194	In the next section, we explain how these extraction patterns are represented as features in the CRF model." ></td>
	<td class="line x" title="116:194	5 Extraction Pattern Features for the CRF The extraction patterns provide two kinds of information." ></td>
	<td class="line x" title="117:194	SourcePatt indicates whether a word activates any source extraction pattern." ></td>
	<td class="line x" title="118:194	For example, the word complained activates the pattern <subj> complained because it anchors the expression." ></td>
	<td class="line x" title="119:194	SourceExtrindicates whether a word is extracted by any source pattern." ></td>
	<td class="line x" title="120:194	For example, in the sentence President Jacques Chirac frequently complained about Frances economy, the words President, Jacques, and Chirac would all be extracted by the <subj> complained pattern." ></td>
	<td class="line x" title="121:194	Each extraction pattern has frequency and probability values produced by AutoSlog-SE, hence we create four IE pattern-based features for each token xi: SourcePatt-Freq, SourceExtr-Freq, SourcePatt-Prob, and SourceExtr-Prob, where the frequency values are divided into three ranges: f0, 1, 2+g and the probability values are divided into five ranges of equal size." ></td>
	<td class="line x" title="122:194	6 Experiments We used the Multi-Perspective Question Answering (MPQA) corpus4 for our experiments." ></td>
	<td class="line x" title="123:194	This corpus 4The MPQA corpus can be freely obtained at http://nrrc.mitre.org/NRRC/publications.htm." ></td>
	<td class="line x" title="124:194	consists of 535 documents that have been manually annotated with opinion-related information including direct and indirect sources." ></td>
	<td class="line x" title="125:194	We used 135 documents as a tuning set for model development and feature engineering, and used the remaining 400 documents for evaluation, performing 10-fold cross validation." ></td>
	<td class="line x" title="126:194	These texts are English language versions of articles that come from many countries and cover many topics.5 We evaluate performance using 3 measures: overlap match (OL), head match (HM), and exact match (EM)." ></td>
	<td class="line x" title="127:194	OL is a lenient measure that considers an extraction to be correct if it overlaps with any of the annotated words." ></td>
	<td class="line x" title="128:194	HM is a more conservative measure that considers an extraction to be correct if its head matches the head of the annotated source." ></td>
	<td class="line x" title="129:194	We report these somewhat loose measures because the annotators vary in where they place the exact boundaries of a source." ></td>
	<td class="line x" title="130:194	EM is the strictest measure that requires an exact match between the extracted words and the annotated words." ></td>
	<td class="line x" title="131:194	We use three evaluation metrics: recall, precision, and F-measure with recall and precision equally weighted." ></td>
	<td class="line x" title="132:194	6.1 Baselines We developed three baseline systems to assess the difficulty of our task." ></td>
	<td class="line x" title="133:194	Baseline-1 labels as sources all phrases that belong to the semantic categories authority, government, human, media, organizationor company, proper name." ></td>
	<td class="line x" title="134:194	Table 1 shows that the precision is poor, suggesting that the third condition described in Section 3.1 (opinion recognition) does play an important role in source identification." ></td>
	<td class="line x" title="135:194	The recall is much higher but still limited due to sources that fall outside of the semantic categories or are not recognized as belonging to these categories." ></td>
	<td class="line x" title="136:194	Baseline-2 labels a noun phrase as a source if any of the following are true: (1) the NP is the subject of a verb phrase containing an opinion word, (2) the NP follows according to, (3) the NP contains a possessive and is preceded by an opinion word, or (4) the NP follows by and attaches to an opinion word." ></td>
	<td class="line x" title="137:194	Baseline-2s heuristics are designed to address the first and the third conditions in Section 3.1." ></td>
	<td class="line x" title="138:194	Table 1 shows that Baseline-2 is substantially better than Baseline-1." ></td>
	<td class="line x" title="139:194	Baseline-3 5This data was obtained from the Foreign Broadcast Information Service (FBIS), a U.S. government agency." ></td>
	<td class="line x" title="140:194	359 Recall Prec F1 OL 77.3 28.8 42.0 Baseline-1 HM 71.4 28.6 40.8 EM 65.4 20.9 31.7 OL 62.4 60.5 61.4 Baseline-2 HM 59.7 58.2 58.9 EM 50.8 48.9 49.8 OL 49.9 72.6 59.2 Baseline-3 HM 47.4 72.5 57.3 EM 44.3 58.2 50.3 OL 48.5 81.3 60.8 Extraction Patterns HM 46.9 78.5 58.7 EM 41.9 70.2 52.5 CRF: OL 56.1 81.0 66.3 basic features HM 55.1 79.2 65.0 EM 50.0 72.4 59.2 CRF: OL 59.1 82.4 68.9 basic + IE pattern HM 58.1 80.5 67.5 features EM 52.5 73.3 61.2 CRF-FI: OL 57.7 80.7 67.3 basic features HM 56.8 78.8 66.0 EM 51.7 72.4 60.3 CRF-FI: OL 60.6 81.2 69.4 basic + IE pattern HM 59.5 79.3 68.0 features EM 54.1 72.7 62.0 Table 1: Source identification performance table labels a noun phrase as a source if it satisfies both Baseline-1 and Baseline-2s conditions (this should satisfy all three conditions described in Section 3.1)." ></td>
	<td class="line x" title="141:194	As shown in Table 1, the precision of this approach is the best of the three baselines, but the recall is the lowest." ></td>
	<td class="line x" title="142:194	6.2 Extraction Pattern Experiment We evaluated the performance of the learned extraction patterns on the source identification task." ></td>
	<td class="line x" title="143:194	The learned patterns were applied to the test data and the extracted sources were scored against the manual annotations.6 Table 1 shows that the extraction patterns produced lower recall than the baselines, but with considerably higher precision." ></td>
	<td class="line x" title="144:194	These results show that the extraction patterns alone can identify 6These results were obtained using the patterns that had a probability >.50 and frequency > 1." ></td>
	<td class="line x" title="145:194	nearly half of the opinion sources with good accuracy." ></td>
	<td class="line x" title="146:194	6.3 CRF Experiments We developed our CRF model using the MALLET code from McCallum (2002)." ></td>
	<td class="line x" title="147:194	For training, we used a Gaussian prior of 0.25, selected based on the tuning data." ></td>
	<td class="line x" title="148:194	We evaluate the CRF using the basic features from Section 3, both with and without the IE pattern features from Section 5." ></td>
	<td class="line x" title="149:194	Table 1 shows that the CRF with basic features outperforms all of the baselines as well as the extraction patterns, achieving an F-measure of 66.3 using the OL measure, 65.0 using the HM measure, and 59.2 using the EM measure." ></td>
	<td class="line x" title="150:194	Adding the IE pattern features further increases performance, boosting recall by about 3 points for all of the measures and slightly increasing precision as well." ></td>
	<td class="line x" title="151:194	CRF with feature induction." ></td>
	<td class="line x" title="152:194	One limitation of log-linear function models like CRFs is that they cannot form a decision boundary from conjunctions of existing features, unless conjunctions are explicitly given as part of the feature vector." ></td>
	<td class="line x" title="153:194	For the task of identifying opinion sources, we observed that the model could benefit from conjunctive features." ></td>
	<td class="line x" title="154:194	For instance, instead of using two separate features, HUMAN and PARENT-CHUNK-INCLUDESOPINION-EXPRESSION, the conjunction of the two is more informative." ></td>
	<td class="line x" title="155:194	For this reason, we applied the CRF feature induction approach introduced by McCallum (2003)." ></td>
	<td class="line x" title="156:194	As shown in Table 1, where CRF-FI stands for the CRF model with feature induction, we see consistent improvements by automatically generating conjunctive features." ></td>
	<td class="line x" title="157:194	The final system, which combines the basic features, the IE pattern features, and feature induction achieves an F-measure of 69.4 (recall=60.6%, precision=81.2%) for the OL measure, an F-measure of 68.0 (recall=59.5%, precision=79.3%) for the HM measure, and an F-measure of 62.0 (recall=54.1%, precision=72.7%) for the EM measure." ></td>
	<td class="line x" title="158:194	6.4 Error Analysis An analysis of the errors indicated some common mistakes: Some errors resulted from error propagation in 360 our subsystems." ></td>
	<td class="line o" title="159:194	Errors from the sentence boundary detector in GATE (Cunningham et al. , 2002) were especially problematic because they caused the Collins parser to fail, resulting in no dependency tree information." ></td>
	<td class="line x" title="160:194	Some errors were due to complex and unusual sentence structure, which our rather simple feature encoding for CRF could not capture well." ></td>
	<td class="line x" title="161:194	Some errors were due to the limited coverage of the opinion lexicon." ></td>
	<td class="line x" title="162:194	We failed to recognize some cases when idiomatic or vague expressions were used to express opinions." ></td>
	<td class="line x" title="163:194	Below are some examples of errors that we found interesting." ></td>
	<td class="line x" title="164:194	Doubly underlined phrases indicate incorrectly extracted sources (either false positives or false negatives)." ></td>
	<td class="line x" title="165:194	Opinion words are singly underlined." ></td>
	<td class="line x" title="166:194	False positives: (1) Actually, these three countries do have one common denominator, i.e., that their values and policies do not agree with those of the United States and none of them are on good terms with the United States." ></td>
	<td class="line x" title="167:194	(2) Perhaps this is why Fidel Castro has not spoken out against what might go on in Guantanamo." ></td>
	<td class="line x" title="168:194	In (1), their values and policies seems like a reasonable phrase to extract, but the annotation does not mark this as a source, perhaps because it is somewhat abstract." ></td>
	<td class="line x" title="169:194	In (2), spoken out is negated, which means that the verb phrase does not bear an opinion, but our system failed to recognize the negation." ></td>
	<td class="line x" title="170:194	False negatives: (3) And for this reason, too, they have a moral duty to speak out, as Swedish Foreign Minister Anna Lindh, among others, did yesterday." ></td>
	<td class="line x" title="171:194	(4) In particular, Iran and Iraq are at loggerheads with each other to this day." ></td>
	<td class="line x" title="172:194	Example (3) involves a complex sentence structure that our system could not deal with." ></td>
	<td class="line x" title="173:194	(4) involves an uncommon opinion expression that our system did not recognize." ></td>
	<td class="line x" title="174:194	7 Related Work To our knowledge, our research is the first to automatically identify opinion sources using the MPQA opinion annotation scheme." ></td>
	<td class="line x" title="175:194	The most closely related work on opinion analysis is Bethard et al.(2004), who use machine learning techniques to identify propositional opinions and their holders (sources)." ></td>
	<td class="line x" title="177:194	However, their work is more limited in scope than ours in several ways." ></td>
	<td class="line x" title="178:194	Their work only addresses propositional opinions, which are localized in the propositional argument of certain verbs such as believe or realize." ></td>
	<td class="line x" title="179:194	In contrast, our work aims to find sources for all opinions, emotions, and sentiments, including those that are not related to a verb at all." ></td>
	<td class="line x" title="180:194	Furthermore, Berthard et al.s task definition only requires the identification of direct sources, while our task requires the identification of both direct and indirect sources." ></td>
	<td class="line x" title="181:194	Bethard et al. evaluate their system on manually annotated FrameNet (Baker et al. , 1998) and PropBank (Palmer et al. , 2005) sentences and achieve 48% recall with 57% precision." ></td>
	<td class="line x" title="182:194	Our IE pattern learner can be viewed as a cross between AutoSlog (Riloff, 1996a) and AutoSlogTS (Riloff, 1996b)." ></td>
	<td class="line x" title="183:194	AutoSlog is a supervised learner that requires annotated training data but does not compute statistics." ></td>
	<td class="line x" title="184:194	AutoSlog-TS is a weakly supervised learner that does not require annotated data but generates coarse statistics that measure each patterns correlation with relevant and irrelevant documents." ></td>
	<td class="line x" title="185:194	Consequently, the patterns learned by both AutoSlog and AutoSlog-TS need to be manually reviewed by a person to achieve good accuracy." ></td>
	<td class="line x" title="186:194	In contrast, our IE learner, AutoSlog-SE, computes statistics directly from the annotated training data, creating a fully automatic variation of AutoSlog." ></td>
	<td class="line x" title="187:194	8 Conclusion We have described a hybrid approach to the problem of extracting sources of opinions in text." ></td>
	<td class="line x" title="188:194	We cast this problem as an information extraction task, using both CRFs and extraction patterns." ></td>
	<td class="line x" title="189:194	Our research is the first to identify both direct and indirect sources for all types of opinions, emotions, and sentiments." ></td>
	<td class="line x" title="190:194	Directions for future work include trying to increase recall by identifying relationships between opinions and sources that cross sentence boundaries, and relationships between multiple opinion expressions by the same source." ></td>
	<td class="line x" title="191:194	For example, the fact that a coreferring noun phrase was marked as a source in one sentence could be a useful clue for extracting the source from another sentence." ></td>
	<td class="line x" title="192:194	The probability or the strength of an opinion expression may also play a useful role in encouraging or suppressing source extraction." ></td>
	<td class="line x" title="193:194	361 9 Acknowledgments We thank the reviewers for their many helpful comments, and the Cornell NLP group for their advice and suggestions for improvement." ></td>
	<td class="line x" title="194:194	This work was supported by the Advanced Research and Development Activity (ARDA), by NSF Grants IIS-0208028 and IIS-0208985, and by the Xerox Foundation." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W05-0408
Automatic Identification Of Sentiment Vocabulary: Exploiting Low Association With Known Sentiment Terms
Gamon, Michael;Aue, Anthony;"></td>
	<td class="line x" title="1:174	Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 5764, Ann Arbor, June 2005." ></td>
	<td class="line x" title="2:174	c2005 Association for Computational Linguistics Automatic identification of sentiment vocabulary: exploiting low association with known sentiment terms Michael Gamon Anthony Aue Natural Language Processing Group Natural Language Processing Group Microsoft Research Microsoft Research mgamon@microsoft.com anthaue@microsoft.com Abstract We describe an extension to the technique for the automatic identification and labeling of sentiment terms described in Turney (2002) and Turney and Littman (2002)." ></td>
	<td class="line x" title="3:174	Their basic assumption is that sentiment terms of similar orientation tend to co-occur at the document level." ></td>
	<td class="line x" title="4:174	We add a second assumption, namely that sentiment terms of opposite orientation tend not to co-occur at the sentence level." ></td>
	<td class="line x" title="5:174	This additional assumption allows us to identify sentiment-bearing terms very reliably." ></td>
	<td class="line x" title="6:174	We then use these newly identified terms in various scenarios for the sentiment classification of sentences." ></td>
	<td class="line x" title="7:174	We show that our approach outperforms Turneys original approach." ></td>
	<td class="line x" title="8:174	Combining our approach with a Naive Bayes bootstrapping method yields a further small improvement of classifier performance." ></td>
	<td class="line x" title="9:174	We finally compare our results to precision and recall figures that can be obtained on the same data set with labeled data." ></td>
	<td class="line oc" title="10:174	1 Introduction The field of sentiment classification has received considerable attention from researchers in recent years (Pang and Lee 2002, Pang et al. 2004, Turney 2002, Turney and Littman 2002, Wiebe et al. 2001, Bai et al. 2004, Yu and Hatzivassiloglou 2003 and many others)." ></td>
	<td class="line x" title="11:174	The identification and classification of sentiment constitutes a problem that is orthogonal to the usual task of text classification." ></td>
	<td class="line x" title="12:174	Whereas in traditional text classification the focus is on topic identification, in sentiment classification the focus is on the assessment of the writers sentiment toward the topic." ></td>
	<td class="line oc" title="13:174	Movie and product reviews have been the main focus of many of the recent studies in this area (Pang and Lee 2002, Pang et al. 2004, Turney 2002, Turney and Littman 2002)." ></td>
	<td class="line x" title="14:174	Typically, these reviews are classified at the document level, and the class labels are positive and negative." ></td>
	<td class="line x" title="15:174	In this work, in contrast, we narrow the scope of investigation to the sentence level and expand the set of labels, making a threefold distinction between positive, neutral, and negative." ></td>
	<td class="line x" title="16:174	The narrowing of scope is motivated by the fact that for realistic text mining on customer feedback, the document level is too coarse, as described in Gamon et al.(2005)." ></td>
	<td class="line x" title="18:174	The expansion of the label set is also motivated by real-world concerns; while it is a given that review text expresses positive or negative sentiment, in many cases it is necessary to also identify the cases that dont carry strong expressions of sentiment at all." ></td>
	<td class="line x" title="19:174	Traditional approaches to text classification require large amounts of labeled training data." ></td>
	<td class="line x" title="20:174	Acquisition of such data can be costly and timeconsuming." ></td>
	<td class="line x" title="21:174	Due to the highly domain-specific nature of the sentiment classification task, moving from one domain to another typically requires the acquisition of a new set of training data." ></td>
	<td class="line x" title="22:174	For this reason, unsupervised or very weakly supervised methods for sentiment classification are especially 57 desirable." ></td>
	<td class="line x" title="23:174	1 Our focus, therefore, is on methods that require very little data annotation." ></td>
	<td class="line x" title="24:174	We describe a method to automatically identify the sentiment vocabulary in a domain." ></td>
	<td class="line x" title="25:174	This method rests on three special properties of the sentiment domain: 1." ></td>
	<td class="line x" title="26:174	the presence of certain words can serve as a proxy for the class label 2." ></td>
	<td class="line x" title="27:174	sentiment terms of similar orientation tend to co-occur 3." ></td>
	<td class="line x" title="28:174	sentiment terms of opposite orientation tend to not co-occur at the sentence level." ></td>
	<td class="line x" title="29:174	Turney (2002) and Turney and Littman (2002) exploit the first two generalizations for unsupervised sentiment classification of movie reviews." ></td>
	<td class="line x" title="30:174	They use the two terms excellent and poor as seed terms to determine the semantic orientation of other terms." ></td>
	<td class="line x" title="31:174	These seed terms can be viewed as proxies for the class labels positive and negative, allowing for the exploitation of otherwise unlabeled data: Terms that tend to co-occur with excellent in documents tend to be of positive orientation, and vice versa for poor." ></td>
	<td class="line x" title="32:174	Turney (2002) starts from a small (2 word) set of terms with known orientation (excellent and poor)." ></td>
	<td class="line x" title="33:174	Given a set of terms with unknown sentiment orientation, Turney (2002) then uses the PMI-IR algorithm (Turney 2001) to issue queries to the web and determine, for each of these terms, its pointwise mutual information (PMI) with the two seed words across a large set of documents." ></td>
	<td class="line x" title="34:174	Term candidates are constrained to be adjectives, which tend to be the strongest bearers of sentiment." ></td>
	<td class="line x" title="35:174	The sentiment orientation (SO) of a term is then determined by the difference between its association (PMI) with the positive seed term excellent and its association with the negative seed term poor." ></td>
	<td class="line x" title="36:174	The resulting list of terms and associated sentiment orientations can then be used to implement a classifier: semantic orientation of the terms in a document of unknown sentiment is added up, and if the overall score is positive, the document is classified as being of positive sentiment, otherwise it is classified as negative." ></td>
	<td class="line x" title="37:174	Yu and Hatzivassiloglou (2003) extend this approach by (1) applying it at the sentence level (instead of the document-level), (2) taking into account non-adjectival parts-of-speech, and (3) 1 For domain-specificity of sentiment classification see Engstrm (2004) and Aue and Gamon (2005)." ></td>
	<td class="line x" title="38:174	using larger sets of seed words." ></td>
	<td class="line x" title="39:174	Their classification goal also differs from Turneys: it is to distinguish opinion sentences from factual statements." ></td>
	<td class="line x" title="40:174	Turney et al.s approach is based on the assumption that sentiment terms of similar orientation tend to co-occur in documents." ></td>
	<td class="line x" title="41:174	Our approach takes advantage of a second assumption: At the sentence level, sentiment terms of opposite orientation tend not to co-occur." ></td>
	<td class="line x" title="42:174	This is, of course, an assumption that will only hold in general, with exceptions." ></td>
	<td class="line x" title="43:174	Basically, the assumption is that sentences of the following form: I dislike X. I really like X. are more frequent than mixed sentiment sentences such as I dislike X but I really like Y. It has been our experience that this generalization does hold often enough to be useful." ></td>
	<td class="line x" title="44:174	We propose to utilize this assumption to identify a set of sentiment terms in a domain." ></td>
	<td class="line x" title="45:174	We select the terms that have the lowest PMI scores on the sentence level with respect to a set of manually selected seed words." ></td>
	<td class="line x" title="46:174	If our assumption about low association at the sentence level is correct, this set of low-scoring terms will be particularly rich in sentiment terms." ></td>
	<td class="line x" title="47:174	We can then use this newly identified set to: (1) use Turneys method to find the orientation for the terms and employ the terms and their scores in a classifier, and (2) use Turneys method to find the orientation for the terms and add the new terms as additional seed terms for a second iteration As opposed to Turney (2002), we do not use the web as a resource to find associations, rather we apply the method directly to in-domain data." ></td>
	<td class="line x" title="48:174	This has the disadvantage of not being able to apply the classification to any arbitrary domain." ></td>
	<td class="line x" title="49:174	It is worth noting, however, that even in Turney (2002) the choice of seed words is explicitly motivated by domain properties of movie reviews." ></td>
	<td class="line x" title="50:174	In the remainder of the paper we will describe results from various experiments based on this assumption." ></td>
	<td class="line x" title="51:174	We also show how we can combine this method with a Naive Bayes bootstrapping approach that takes further advantage of the unlabeled data (Nigam et al. 2000)." ></td>
	<td class="line x" title="52:174	58 2 Data For our experiments we used a set of car reviews from the MSN Autos web site." ></td>
	<td class="line x" title="53:174	The data consist of 406,818 customer car reviews written over a fouryear period." ></td>
	<td class="line x" title="54:174	Aside from filtering out examples containing profanity, the data was not edited." ></td>
	<td class="line x" title="55:174	The reviews range in length from a single sentence (56% of all cases) to 50 sentences (a single review)." ></td>
	<td class="line x" title="56:174	Less than 1% of reviews contain ten or more sentences." ></td>
	<td class="line x" title="57:174	There are almost 900,000 sentences in total." ></td>
	<td class="line x" title="58:174	When customers submitted reviews to the website, they were asked for a recommendation on a scale of 1 (negative) to 10 (positive)." ></td>
	<td class="line x" title="59:174	The average score was very high, at 8.3, yielding a strong skew in favor of positive class labels." ></td>
	<td class="line x" title="60:174	We annotated a randomlyselected sample of 3,000 sentences for sentiment." ></td>
	<td class="line x" title="61:174	Each sentence was viewed in isolation and classified as positive, negative or neutral." ></td>
	<td class="line x" title="62:174	The neutral category was applied to sentences with no discernible sentiment, as well as to sentences that expressed both positive and negative sentiment." ></td>
	<td class="line x" title="63:174	Three annotators had pair-wise agreement scores (Cohens Kappa score, Cohen 1960) of 70.10%, 71.78% and 79.93%, suggesting that the task of sentiment classification on the sentence level is feasible but difficult even for people." ></td>
	<td class="line x" title="64:174	This set of data was split into a development test set of 400 sentences and a blind test set of 2600 sentences." ></td>
	<td class="line x" title="65:174	Sentences are represented as vectors of binary unigram features." ></td>
	<td class="line x" title="66:174	The total number of observed unigram features is 72988." ></td>
	<td class="line x" title="67:174	In order to restrict the number of features to a manageable size, we disregard features that occur less than 10 times in the corpus." ></td>
	<td class="line x" title="68:174	With this restriction we obtain a reduced feature set of 13317 features." ></td>
	<td class="line x" title="69:174	3 Experimental Setup Our experiments were performed as follows: We started with a small set of manually-selected and annotated seed terms." ></td>
	<td class="line x" title="70:174	We used 4 positive and 6 negative seed terms." ></td>
	<td class="line x" title="71:174	We decided to use a few more negative seed words because of the inherent positive skew in the data that makes the identification of negative sentences particularly hard." ></td>
	<td class="line x" title="72:174	The terms we used are: positive: negative: good bad excellent lousy love terrible happy hate suck unreliable There was no tuning of the set of initial seed terms; the 10 words were originally chosen intuitively, as words that we observed frequently when manually inspecting the data." ></td>
	<td class="line x" title="73:174	We then used these seed terms in two basic ways: (1) We used them as seeds for a Turneystyle determination of the semantic orientation of words in the corpus (semantic orientation, or SO method)." ></td>
	<td class="line x" title="74:174	As mentioned above, this process is based on the assumption that terms of similar orientation tend to co-occur." ></td>
	<td class="line x" title="75:174	(2) We used them to mine sentiment vocabulary from the unlabeled data using the additional assumption that sentiment terms of opposite orientation tend not to co-occur at the sentence level (sentiment mining, or SM method)." ></td>
	<td class="line x" title="76:174	This method yields a set of sentiment terms, but no orientation for that set of terms." ></td>
	<td class="line x" title="77:174	We continue by using the SO method to find the semantic orientation for this set of sentiment terms, effectively using SM as a feature selection method for sentiment terminology." ></td>
	<td class="line x" title="78:174	Pseudo-code for the SO and SM approaches is provided in Figure 1 and Figure 2." ></td>
	<td class="line x" title="79:174	As a first step for both SO and SM methods (not shown in the pseudocode), PMI needs to be calculated for each pair (f, s) of feature f and seed word s over the collection of feature vectors." ></td>
	<td class="line x" title="80:174	Figure 1: SO method for determining semantic orientation 59 Figure 2: SM method for mining sentiment terms In the first scenario (using straightforward SO), features F range over all observed features in the data (modulo the aforementioned count cutoff of 10)." ></td>
	<td class="line x" title="81:174	In the second scenario (SM + SO), features F range over the n% of features with the lowest PMI scores with respect to any of the seed words that were identified using the sentiment mining technique in Figure 2." ></td>
	<td class="line x" title="82:174	The result of both SO and SM+SO is a list of unigram features which have an associated semantic orientation score, indicating their sentiment orientation: the higher the score, the more positive a term, and vice versa." ></td>
	<td class="line x" title="83:174	This list of features and associated scores can be used to construct a simple classifier: for each sentence with unknown sentiment, we take the sum of the semantic orientation scores for all of the unigrams in that sentence." ></td>
	<td class="line x" title="84:174	This overall score determines the classification of the sentence as positive, neutral or negative as shown in Figure 3." ></td>
	<td class="line x" title="85:174	Scoring and classifying sentence vectors: (1) assigning a sentence score: FOREACH feature f in sentence vector v: Score(v) = Score(v) + SO(f) (2) assigning a class label based on the sentence score: IF Score(v) > threshold1: Class(v) = positive ELSE IF Score(v) < threshold1 AND Score(v) > threshold2: Class(v) = neutral ELSE Class(v) = negative Figure 3: Using SO scores for sentence scoring and classification The two thresholds used in classification need to be determined empirically by taking the distribution of class values in the corpus into account." ></td>
	<td class="line x" title="86:174	For our experiments we simply took the distribution of class labels in the 400 sentence development test set as an approximation of the overall class label distribution: we determined that distribution to be 15.5% for negative sentences, 21.5% for neutral sentences, and 63.0% for positive sentences." ></td>
	<td class="line x" title="87:174	Scores for all sentence vectors in the corpus are then collected using the scoring part of the algorithm in Figure 3." ></td>
	<td class="line x" title="88:174	The scores are sorted and the thresholds are determined as the cutoffs for the top 63% and bottom 15.5% of scores respectively." ></td>
	<td class="line x" title="89:174	4 Results 4.1." ></td>
	<td class="line x" title="90:174	Comparing SO and SM+SO In our first set of experiments we manipulated the following parameters: 1." ></td>
	<td class="line x" title="91:174	the choice of SO or SM+SO method 2." ></td>
	<td class="line x" title="92:174	the choice of n when selecting the n% semantic terms with lowest PMI score in the SM method The tables below show the results of classifying sentence vectors using the unigram features and associated scores produced by SO and SO+SM." ></td>
	<td class="line x" title="93:174	We used the 2,600-sentence manually-annotated test set described previously to establish these numbers." ></td>
	<td class="line x" title="94:174	Since the data exhibit a strong skew in favor of the positive class label, we measure performance not in terms of accuracy but in terms of average precision and recall across the three class labels, as suggested in (Manning and Schtze 2002)." ></td>
	<td class="line x" title="95:174	Avg precision Avg recall SO 0.4481 0.4511 Table 1: Using the SO approach." ></td>
	<td class="line x" title="96:174	Table 1 shows results of using the SO method on the data." ></td>
	<td class="line x" title="97:174	Table 2 presents the results of combining the SM and SO methods for different values of n. The best results are shown in boldface." ></td>
	<td class="line x" title="98:174	As a comparison between Table 1 and Table 2 shows, the highest average precision and recall scores were obtained by combining the SM and SO methods." ></td>
	<td class="line x" title="99:174	Using SM as a feature selection mechanism also reduces the number of features significantly." ></td>
	<td class="line x" title="100:174	While the SO method employed on sentence-level vectors uses 13,000 features, the best-performing SM+SO combination uses only 20% of this feature set, indicating that SM is indeed effective in selecting the most important sentiment-bearing terms." ></td>
	<td class="line x" title="101:174	60 We also determined that the positive impact of SM is not just a matter of reducing the number of features." ></td>
	<td class="line x" title="102:174	If SO without the SM feature selection step is reduced to a comparable number of features by taking the top features according to absolute score, average precision is at 0.4445 and average recall at 0.4464." ></td>
	<td class="line x" title="103:174	N=10 N=20 N=30 N=40 N=50 Avg prec Avg rec Avg prec Avg rec Avg prec Avg rec Avg prec Avg rec Avg prec Avg rec SM+SO SO from document level 0.4351 0.4377 0.4568 0.4605 0.4528 0.4557 0.4457 0.4478 0.4451 0.4475 Table 2: combining SM and SO." ></td>
	<td class="line x" title="104:174	Sentiment terms in top 100 SM terms Sentiment terms in top 100 SO terms excellent, terrible, broke, junk, alright, bargain, grin, highest, exceptional, exceeded, horrible, loved, waste, ok, death, leaking, outstanding, cracked, rebate, warped, hooked, sorry, refuses, excellant, satisfying, died, biggest, competitive, delight, avoid, awful, garbage, loud, okay, competent, upscale, dated, mistake, sucks, superior, high, kill, neither excellent, happy, stylish, sporty, smooth, love, quiet, overall, pleased, plenty, dependable, solid, roomy, safe, good, easy, smaller, luxury, comfortable, style, loaded, space, classy, handling, joy, small, comfort, size, perfect, performance, room, choice, recommended, package, compliments, awesome, unique, fun, holds, comfortably, extremely, value, free, satisfied, little, recommend, limited, great, pleasure Non sentiment terms in top 100 SM terms Non sentiment terms in top 100 SO terms alternative, wont, below, surprisingly, maintained, choosing, comparing, legal, vibration, seemed, claim, demands, assistance, knew, engineering, accelleration, ended, salesperson, performed, started, midsize, site, gonna, lets, plugs, industry, alternator, month, told, vette, 180, powertrain, write, mos, walk, causing, lift, es, segment, $250, 300m, wanna, february, mod, $50, nhtsa, suburbans, manufactured, tiburon, $10, f150, 5000, posted, tt, him, saw, jan, condition, very, handles, milage, definitely, definately, far, drives, shape, color, price, provides, options, driving, rides, sports, heated, ride, sport, forward, expected, fairly, anyone, test, fits, storage, range, family, sedan, trunk, young, weve, black, college, suv, midsize, coupe, 30, shopping, kids, player, saturn, bose, truck, town, am, leather, stereo, car, husband Table 3: the top 100 terms identified by SM and SO Table 3 shows the top 100 terms that were identified by each SM and SO methods." ></td>
	<td class="line x" title="105:174	The terms are categorized into sentiment-bearing and nonsentiment bearing terms by human judgment." ></td>
	<td class="line x" title="106:174	The two sets seem to differ in both strength and orientation of the identified terms." ></td>
	<td class="line x" title="107:174	The SM-identified words have a higher density of negative terms (22 out of 43 versus 2 out of 49 for the SO-identified terms)." ></td>
	<td class="line x" title="108:174	The SM-identified terms also express sentiment more strongly, but this conclusion is more tentative since it may be a consequence of the higher density of negative terms." ></td>
	<td class="line x" title="109:174	4.2." ></td>
	<td class="line x" title="110:174	Multiple iterations: increasing the number of seed features by SM+SO In a second set of experiments, we assessed the question of whether it is possible to use multiple iterations of the SM+SO method to gradually build the list of seed words." ></td>
	<td class="line x" title="111:174	We do this by adding the top n% of features selected by SM, along with their orientation as determined by SO, to the initial set of seed words." ></td>
	<td class="line x" title="112:174	The procedure for this round of experiments is as follows:  take the top n% of features identified by SM (we used n=1 for the reported re61 sults, since preliminary experiments with other values for n did not improve results)  perform SO for these features to determine their orientation  take the top 15.5% negative and top 63% positive (according to class label distribution in the development test set) of the features and add them as negative/positive seed features respectively This iteration increases the number of seed features from the original 10 manually-selected features to a total of 111 seed features." ></td>
	<td class="line x" title="113:174	With this enhanced set of seed features we then re-ran a subset of the experiments in Table 2." ></td>
	<td class="line x" title="114:174	Results are shown in Table 4." ></td>
	<td class="line x" title="115:174	Increasing the number of seed features through the SM feature selection method increases precision and recall by several percentage points." ></td>
	<td class="line x" title="116:174	In particular, precision and recall for negative sentences are boosted." ></td>
	<td class="line x" title="117:174	Avg precision Avg recall SM + SO, n=10, SO from document vectors 0.4826 0.48.76 SM + SO, n=30, SO from document vectors 0.4957 0.4995 SM + SO, n=50, SO from document vectors 0.4914 0.4952 Table 4: Using 2 iterations to increase the seed feature set We also confirmed that these results are truly attributable to the use of the SM method for the first iteration." ></td>
	<td class="line x" title="118:174	If we take an equivalent number of features with strongest semantic orientation according to the SO method and add them to the list of seed features, our results degrade significantly (the resulting classifier performance is significantly different at the 99.9% level as established by the McNemar test)." ></td>
	<td class="line x" title="119:174	This is further evidence that SM is indeed an effective method for selecting sentiment terms." ></td>
	<td class="line x" title="120:174	4.3." ></td>
	<td class="line x" title="121:174	Using the SO classifier to bootstrap a Naive Bayes classifier In a third set of experiments, we tried to improve on the results of the SO classifier by combining it with the bootstrapping approach described in (Nigam et al. 2000)." ></td>
	<td class="line x" title="122:174	The basic idea here is to use the SO classifier to label a subset of the data DL." ></td>
	<td class="line x" title="123:174	This labeled subset of the data is then used to bootstrap a Naive Bayes (NB) classifier on the remaining unlabeled data D U using the Expectation Maximization (EM) algorithm: (1) An initial naive Bayes classifier with parameters  is trained on the documents in DL." ></td>
	<td class="line x" title="124:174	(2) This initial classifier is used to estimate a probability distribution over all classes for each of the documents in DU." ></td>
	<td class="line x" title="125:174	(EStep) (3) The labeled and unlabeled data are then used to estimate parameters for a new classifier." ></td>
	<td class="line x" title="126:174	(M-Step) Steps 2 and 3 are repeated until convergence is achieved when the difference in the joint probability of the data and the parameters falls below the configurable threshold  between iterations." ></td>
	<td class="line x" title="127:174	Another free parameter, , can be used to control how much weight is given to the unlabeled data." ></td>
	<td class="line x" title="128:174	For our experiments we used classifiers from the best SM+SO combination (2 iterations at n=30) from Table 4 above to label 30% of the total data." ></td>
	<td class="line x" title="129:174	Table 5 shows the average precision and recall numbers for the converged NB classifier." ></td>
	<td class="line x" title="130:174	2 In addition to improving average precision and recall, the resulting classifier also has the advantage of producing class probabilities instead of simple scores." ></td>
	<td class="line x" title="131:174	3 Avg precision Avg recall Bootstrapped NB classifier 0.5167 0.52 Table 5: Results obtained by bootstrapping a NB classifier 4.4." ></td>
	<td class="line x" title="132:174	Results from supervised learning: using small sets of labeled data Given infinite resources, we can always annotate enough data to train a classifier using a supervised algorithm that will outperform unsupervised or weakly-supervised methods." ></td>
	<td class="line x" title="133:174	Which approach to take depends entirely on how much time and money are available and on the accuracy requirements for the task at hand." ></td>
	<td class="line x" title="134:174	2 In this experiment,  was set to 0.1 and  was set to 0.05." ></td>
	<td class="line x" title="135:174	3 We also experimented with labeling the whole data set with the best of our SO score classifiers, and then training a linear Support Vector Machine classifier on the data." ></td>
	<td class="line x" title="136:174	The results were considerably worse than any of the reported numbers, so they are not included in this paper." ></td>
	<td class="line x" title="137:174	62 To help situate the precision and recall numbers presented in the tables above, we trained Support Vector Machines (SVMs) using small amounts of labeled data." ></td>
	<td class="line x" title="138:174	SVMs were trained with 500, 1000, 2000, and 2500 labeled sentences." ></td>
	<td class="line x" title="139:174	Annotating 2500 sentences represents approximately eight person-hours of work." ></td>
	<td class="line x" title="140:174	The results can be found in Table 5." ></td>
	<td class="line x" title="141:174	We were pleasantly surprised at how well the unsupervised classifiers described above perform in comparison to state-of-the-art supervised methods (albeit trained on small amounts of data)." ></td>
	<td class="line x" title="142:174	Labeled examples Avg." ></td>
	<td class="line x" title="143:174	Precision Avg." ></td>
	<td class="line x" title="144:174	Recall 500.4878 .4967 1000 .5161 .5105 2000 .5297 .5256 2500 .5017 .5083 Table 6: Average precision and recall for SVMs for small numbers of labeled examples 4.5." ></td>
	<td class="line x" title="145:174	Results on the movie domain We also performed a small set of experiments on the movie domain using Pang and Lees 2004 data set." ></td>
	<td class="line x" title="146:174	This set consists of 2000 reviews, 1000 each of very positive and very negative reviews." ></td>
	<td class="line x" title="147:174	Since this data set is balanced and the task is only a two-way classification between positive and negative reviews, we only report accuracy numbers here." ></td>
	<td class="line xc" title="148:174	accuracy Training data Turney (2002) 66% unsupervised Pang & Lee (2004) 87.15% supervised Aue & Gamon (2005) 91.4% supervised SO 73.95% unsupervised SM+SO to increase seed words, then SO 74.85% weakly supervised Table 7: Classification accuracy on the movie review domain Turney (2002) achieves 66% accuracy on the movie review domain using the PMI-IR algorithm to gather association scores from the web." ></td>
	<td class="line x" title="149:174	Pang and Lee (2004) report 87.15% accuracy using a unigram-based SVM classifier combined with subjectivity detection." ></td>
	<td class="line x" title="150:174	Aue and Gamon (2005) use a simple linear SVM classifier based on unigrams, combined with LLR-based feature reduction, to achieve 91.4% accuracy." ></td>
	<td class="line x" title="151:174	Using the Turney SO method on in-domain data instead of web data achieves 73.95% accuracy (using the same two seed words that Turney does)." ></td>
	<td class="line x" title="152:174	Using one iteration of SM+SO to increase the number of seed words, followed by finding SO scores for all words with respect to the enhanced seed word set, yields a slightly higher accuracy of 74.85%." ></td>
	<td class="line x" title="153:174	With additional parameter tuning, this number can be pushed to 76.4%, at which point we achieve statistical significance at the 0.95 level according to the McNemar test, indicating that there is more room here for improvement." ></td>
	<td class="line x" title="154:174	Any reduction of the number of overall features in this domain leads to decreased accuracy, contrary to what we observed in the car review domain." ></td>
	<td class="line x" title="155:174	We attribute this observation to the smaller data set." ></td>
	<td class="line x" title="156:174	5 Discussion 5.1 A note on statistical significance We used the McNemar test to assess whether two classifiers are performing significantly differently." ></td>
	<td class="line x" title="157:174	This test establishes whether the accuracy of two classifiers differs significantly it does not guarantee significance for precision and recall differences." ></td>
	<td class="line x" title="158:174	For the latter, other tests have been proposed (e.g. Chinchor 1995), but time constraints prohibited us from implementing any of those more computationally costly tests." ></td>
	<td class="line x" title="159:174	For the results presented in the previous sections the McNemar test established statistical significance at the 0.99 level over baseline (i.e. the SO results in Table 1) for the multiple iterations results (Table 4) and the bootstrapping approach (Table 5), but not for the SM+SO approach (Table 2)." ></td>
	<td class="line x" title="160:174	5.2 Future work This exploratory set of experiments indicates a number of interesting directions for future work." ></td>
	<td class="line x" title="161:174	A shortcoming of the present work is the manual tuning of cutoff parameters." ></td>
	<td class="line x" title="162:174	This problem could be alleviated in at least two possible ways: First, using a general combination of the ranking of terms according to SM and SO." ></td>
	<td class="line x" title="163:174	In other words, calculate the semantic weight of a term as a combination of SO and its rank in the SM scores." ></td>
	<td class="line x" title="164:174	63 Secondly, following a suggestion by an anonymous reviewer, the Naive Bayes bootstrapping approach could be used in a feedback loop to inform the SO score estimation in the absence of a manually annotated parameter tuning set." ></td>
	<td class="line x" title="165:174	5.3 Summary Our results demonstrate that the SM method can serve as a valid tool to mine sentiment-rich vocabulary in a domain." ></td>
	<td class="line x" title="166:174	SM will yield a list of terms that are likely to have a strong sentiment orientation." ></td>
	<td class="line x" title="167:174	SO can then be used to find the polarity for the selected features by association with the sentiment terms of known polarity in the seed word list." ></td>
	<td class="line x" title="168:174	Performing this process iteratively by first enhancing the set of seed words through SM+SO yields the best results." ></td>
	<td class="line x" title="169:174	While this approach does not compare to the results that can be achieved by supervised learning with large amounts of labeled data, it does improve on results obtained by using SO alone." ></td>
	<td class="line x" title="170:174	We believe that this result is relevant in two respects." ></td>
	<td class="line x" title="171:174	First, by improving average precision and recall on the classification task, we move closer to the goal of unsupervised sentiment classification." ></td>
	<td class="line x" title="172:174	This is a very important goal in itself given the need for out of the box sentiment techniques in business intelligence and the notorious difficulty of rapidly adapting to a new domain (Engstrm 2004, Aue and Gamon 2005)." ></td>
	<td class="line x" title="173:174	Second, the exploratory results reported here may indicate a general source of information for feature selection in natural language tasks: features that have a tendency to be in complementary distribution (especially in smaller linguistic units such as sentences) may often form a class that shares certain properties." ></td>
	<td class="line x" title="174:174	In other words, it is not only the strong association scores that should be exploited but also the particularly weak (negative) associations." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N06-3005
Identifying Perspectives At The Document And Sentence Levels Using Statistical Models
Lin, Wei-Hao;"></td>
	<td class="line x" title="1:72	Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 227230, New York, June 2006." ></td>
	<td class="line x" title="2:72	c2006 Association for Computational Linguistics Identifying Perspectives at the Document and Sentence Levels Using Statistical Models Wei-Hao Lin Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 U.S.A. whlin@cs.cmu.edu Abstract In this paper we investigate the problem of identifying the perspective from which a document was written." ></td>
	<td class="line x" title="3:72	By perspective we mean a point of view, for example, from the perspective of Democrats or Republicans." ></td>
	<td class="line x" title="4:72	Can computers learn to identify the perspective of a document?" ></td>
	<td class="line x" title="5:72	Furthermore, can computers identify which sentences in a document strongly convey a particular perspective?" ></td>
	<td class="line x" title="6:72	We develop statistical models to capture how perspectives are expressed at the document and sentence levels, and evaluate the proposed models on a collection of articles on the Israeli-Palestinian conflict." ></td>
	<td class="line x" title="7:72	The results show that the statistical models can successfully learn how perspectives are reflected in word usage and identify the perspective of a document with very high accuracy." ></td>
	<td class="line x" title="8:72	1 Introduction In this paper we investigate the problem of automatically identifying the perspective from which a document was written." ></td>
	<td class="line x" title="9:72	By perspective, we mean subjective evaluation of relative significance, a point-of-view. For example, documents about the Palestinian-Israeli conflict may appear to be about the same topic, but reveal different perspectives: This is joint work with Theresa Wilson, Janyce Wiebe, and Alexander Hauptmann, and supported by the Advanced Research and Development Activity (ARDA) under contract number NBCHC040037." ></td>
	<td class="line x" title="10:72	(1) The inadvertent killing by Israeli forces of Palestinian civilians  usually in the course of shooting at Palestinian terrorists  is considered no different at the moral and ethical level than the deliberate targeting of Israeli civilians by Palestinian suicide bombers." ></td>
	<td class="line x" title="11:72	(2) In the first weeks of the Intifada, for example, Palestinian public protests and civilian demonstrations were answered brutally by Israel, which killed tens of unarmed protesters." ></td>
	<td class="line x" title="12:72	Example 1 is written from a Israeli perspective; Example 2 is written from a Palestinian perspective." ></td>
	<td class="line x" title="13:72	We aim to address a research question: can computers learn to identify the perspective of a document given a training corpus of documents that are written from different perspectives?" ></td>
	<td class="line x" title="14:72	When an issue is discussed from different perspectives, not every sentence in a document strongly reflects the perspective the author possesses." ></td>
	<td class="line x" title="15:72	For example, the following sentences are written by one Palestinian and one Israeli: (3) The Rhodes agreements of 1949 set them as the ceasefire lines between Israel and the Arab states." ></td>
	<td class="line x" title="16:72	(4) The green line was drawn up at the Rhodes Armistice talks in 1948-49." ></td>
	<td class="line x" title="17:72	Example 3 and 4 both factually introduce the background of the issue of the green line without expressing explicit perspectives." ></td>
	<td class="line x" title="18:72	Can computers automatically discriminate between sentences that strongly express a perspective and sentences that only reflect shared background information?" ></td>
	<td class="line x" title="19:72	227 A system that can automatically identify the perspective from which a document written will be a highly desirable tool for people analyzing huge collections of documents from different perspectives." ></td>
	<td class="line x" title="20:72	An intelligence analyst regularly monitors the positions that foreign countries take on political and diplomatic issues." ></td>
	<td class="line x" title="21:72	A media analyst frequently surveys broadcast news, newspapers, and web blogs for different viewpoints." ></td>
	<td class="line x" title="22:72	What these analysts need in common is that they would like to find evidence of strong statements of differing perspectives, while ignoring statements without strong perspectives as less interesting." ></td>
	<td class="line x" title="23:72	In this paper we approach the problem of learning perspectives in a statistical learning framework." ></td>
	<td class="line x" title="24:72	We develop statistical models to learn how perspectives are reflected in word usage, and evaluate the models by measuring how accurately they can predict the perspectives of unseen documents." ></td>
	<td class="line x" title="25:72	Lacking annotation on how strongly individual sentences convey a particular perspective in our corpus poses a challenge on learning sentence-level perspectives." ></td>
	<td class="line x" title="26:72	We propose a novel statistical model, Latent Sentence Perspective Model, to address the problem." ></td>
	<td class="line x" title="27:72	2 Related Work Identifying the perspective from which a document is written is a subtask in the growing area of automatic opinion recognition and extraction." ></td>
	<td class="line x" title="28:72	Subjective language is used to express opinions, emotions, and sentiments." ></td>
	<td class="line oc" title="29:72	So far research in automatic opinion recognition has primarily addressed learning subjective language (Wiebe et al. , 2004; Riloff et al. , 2003; Riloff and Wiebe, 2003), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Yu and Hatzivassiloglou, 2003; Riloff et al. , 2003; Riloff and Wiebe, 2003), and discriminating between positive and negative language (Yu and Hatzivassiloglou, 2003; Turney and Littman, 2003; Pang et al. , 2002; Dave et al. , 2003; Nasukawa and Yi, 2003; Morinaga et al. , 2002)." ></td>
	<td class="line x" title="30:72	Although by its very nature we expect much of the language of presenting a perspective or pointof-view to be subjective, labeling a document or a sentence as subjective is not enough to identify the perspective from which it is written." ></td>
	<td class="line x" title="31:72	Moreover, the ideology and beliefs authors possess are often expressed in ways more than conspicuous positive or negative language toward specific targets." ></td>
	<td class="line x" title="32:72	3 Corpus Our corpus consists of articles published on the bitterlemons website1." ></td>
	<td class="line x" title="33:72	The website is set up to contribute to mutual understanding [between Palestinians and Israels] through the open exchange of ideas." ></td>
	<td class="line x" title="34:72	Every week an issue about IsraeliPalestinian conflict is selected for discussion, for example, Disengagement: unilateral or coordinated?, and a Palestinian editor and an Israeli editor contribute a article addressing the issue." ></td>
	<td class="line x" title="35:72	In addition, the Israeli and Palestinian editors invite or interview one Israeli and one Palestinian to express their views, resulting in a total of four articles in a weekly edition." ></td>
	<td class="line x" title="36:72	We evaluate the subjectivity of each sentence using the patterns automatically extracted from foreign news documents (Riloff and Wiebe, 2003), and find that 65.6% of Palestinian sentences and 66.2% of Israeli sentences are classified as subjective." ></td>
	<td class="line x" title="37:72	The high but almost equivalent percentages of subjective sentences from two perspectives supports our observation in Section 2 that perspective is largely expressed in subjective language but subjectivity ratio is not necessarily indicative of the perspective of a document." ></td>
	<td class="line x" title="38:72	4 Statistical Modeling of Perspectives We approach the problem of learning perspectives in a statistical learning framework." ></td>
	<td class="line x" title="39:72	Denote a training corpus as pairs of documents Wn and their perspectives labels Dn, n = 1,,N, N is the total number of documents in the corpus." ></td>
	<td class="line x" title="40:72	Given a new document W with a unknown document perspective D, identifying its perspective is to calculate the following conditional probability, P( D| W,{Dn,Wn}Nn=1) (5) We are interested in how strongly each sentence in the document convey perspective." ></td>
	<td class="line x" title="41:72	Denote the intensity of the m-th sentence of the n-th document as a binary random variable Sm,n, m = 1,,Mn, Mn is the total number of sentences of the n-th document." ></td>
	<td class="line x" title="42:72	Evaluating how strongly a sentence conveys 1http://www.bitterlemons.org 228 a particular perspective is to calculate the following conditional probability, P(Sm,n|{Dn,Wn}Nn=1) (6) 4.1 Document Perspective Models The process of generating documents from a particular perspective is modeled as follows, pi  Beta(pi,pi)   Dirichlet() Dn  Binomial(1,pi) Wn  Multinomial(Ln,d) The model is known as nave Bayes models (NB), which has been widely used for NLP tasks such as text categorization (Lewis, 1998)." ></td>
	<td class="line x" title="43:72	To calculate (5) under NB in a full Bayesian manner is, however, complicated, and alternatively we employ Markov Chain Monte Carlo (MCMC) methods to simulate samples from the posterior distributions." ></td>
	<td class="line x" title="44:72	4.2 Latent Sentence Perspective Models We introduce a new binary random variables, S, to model how strongly a perspective is expressed at the sentence level." ></td>
	<td class="line x" title="45:72	The value of S is either s1 or s0, where s1 means the sentence is written strongly from a perspective, and s0 is not." ></td>
	<td class="line x" title="46:72	The whole generative process is modeled as follows, pi  Beta(pi,pi)   Beta(,)   Dirichlet() Dn  Binomial(1,pi) Sm,n  Binomial(1,) Wm,n  Multinomial(Lm,n,) pi and  carry the same semantics as those in NB." ></td>
	<td class="line x" title="47:72	S is naturally modeled as a binary variable, where  is the parameter of S and represents how likely a perspective is strongly expressed at the sentence given on the overall document perspective." ></td>
	<td class="line x" title="48:72	We call this model Latent Sentence Perspective Models (LSPM), because S is never directly observed in either training or testing documents and need to be inferred." ></td>
	<td class="line x" title="49:72	To calculate (6) under LSPM is difficult." ></td>
	<td class="line x" title="50:72	We again resort to MCMC methods to simulate samples from the posterior distributions." ></td>
	<td class="line x" title="51:72	5 Experiments 5.1 Identifying Perspectives at the Document Level To objectively evaluate how well nave Bayes models (NB) learn to identify perspectives expressed at the document level, we train NB against on the bitterlemons corpus, and evaluate how accurately NB predicts the perspective of a unseen document as either Palestinian or Israeli in ten-fold cross-validation manner." ></td>
	<td class="line x" title="52:72	The average classification accuracy over 10 folds is reported." ></td>
	<td class="line x" title="53:72	We compare three different models, including NB with two different inference methods and Support Vector Machines (SVM) (Cristianini and Shawe-Taylor, 2000)." ></td>
	<td class="line x" title="54:72	NB-B uses full Bayesian inference and NB-M uses Maximum a posteriori (MAP)." ></td>
	<td class="line x" title="55:72	Model Data Set Accuracy Reduction Baseline 0.5 SVM Editors 0.9724 NB-M Editors 0.9895 61% NB-B Editors 0.9909 67% SVM Guests 0.8621 NB-M Guests 0.8789 12% NB-B Guests 0.8859 17% Table 1: Results of Identifying Perspectives at the Document Level The results in Table 1 show that both NB and SVM perform surprisingly well on both Editors and Guests subsets of the bitterlemons corpus." ></td>
	<td class="line x" title="56:72	We also see that NBs further reduce classification errors even though SVM already achieves high accuracy." ></td>
	<td class="line x" title="57:72	By considering the full posterior distribution NB-B further improves on NB-M, which performs only point estimation." ></td>
	<td class="line x" title="58:72	The results strongly suggest that the word choices made by authors, either consciously or subconsciously, reflect much of their political perspectives." ></td>
	<td class="line x" title="59:72	5.2 Identifying Perspectives at the Sentence Level In addition to identify the perspectives of a document, we are interested in which sentences in the document strongly convey perspectives." ></td>
	<td class="line x" title="60:72	Although the posterior probability that a sentence 229 covey strongly perspectives in (6) is of our interest, we can not directly evaluate their quality due to the lack of golden truth at the sentence level." ></td>
	<td class="line x" title="61:72	Alternatively we evaluate how accurately LSPM predicts the perspective of a document, in the same way of evaluating SVM and NB in the previous section." ></td>
	<td class="line x" title="62:72	If LSPM does not achieve similar identification accuracy after modeling sentence-level information, we will doubt the quality of predictions on how strongly a sentence convey perspective made by LSPM." ></td>
	<td class="line x" title="63:72	Model Training Testing Accuracy Baseline 0.5 NB-M Guest Editor 0.9327 NB-B Guest Editor 0.9346 LSPM Guest Editor 0.9493 NB-M Editors Guests 0.8485 NB-B Editors Guests 0.8585 LSPM Guest Editor 0.8699 Table 2: Results of Perspective Identification at the Sentence Level The experimental results in Table 2 show that the LSPM achieves similarly or even slightly better accuracy than those of NBs, which is very encouraging and suggests that the proposed LSPM closely match how perspectives are expressed at the document and sentence levels." ></td>
	<td class="line x" title="64:72	If one does not explicitly model the uncertainty at the sentence level, one can train NB directly against the sentences to classify a sentence into Palestinian or Israeli perspective." ></td>
	<td class="line x" title="65:72	We obtain the accuracy of 0.7529, which is much lower than the accuracy previously achieved at the document level." ></td>
	<td class="line x" title="66:72	Therefore identifying perspective at the sentence level is much harder than at that the document level, and the high accuracy of identifying document-level perspectives suggests that LPSM closely captures the perspectives expressed at the document and sentence levels, given individual sentences are very short and much less informative about overall perspective." ></td>
	<td class="line x" title="67:72	6 Summary of Contributions In this paper we study the problem of learning to identify the perspective from which a text was written at the document and sentence levels." ></td>
	<td class="line x" title="68:72	We show that perspectives are expressed in word usage, and statistical learning algorithms such as SVM and nave Bayes models can successfully uncover the word patterns chosen by authors from different perspectives." ></td>
	<td class="line x" title="69:72	Furthermore, we develop a novel statistical model to infer how strongly a sentence convey perspective without any labels." ></td>
	<td class="line x" title="70:72	By introducing latent variables, Latent Sentence Perspective Models are shown to capture well how perspectives are reflected at the document and sentence levels." ></td>
	<td class="line x" title="71:72	The proposed statistical models can help analysts sift through a large collection of documents written from different perspectives." ></td>
	<td class="line x" title="72:72	The unique sentencelevel perspective modeling can automatically identify sentences that are strongly representative of the perspective of interest, and we plan to manually evaluate their quality in the future work." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-1133
Are These Documents Written From Different Perspectives? A Test Of Different Perspectives Based On Statistical Distribution Divergence
Lin, Wei-Hao;Hauptmann, Alexander G.;"></td>
	<td class="line x" title="1:268	Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 10571064, Sydney, July 2006." ></td>
	<td class="line x" title="2:268	c2006 Association for Computational Linguistics Are These Documents Written from Different Perspectives?" ></td>
	<td class="line x" title="3:268	A Test of Different Perspectives Based On Statistical Distribution Divergence Wei-Hao Lin Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 U.S.A. whlin@cs.cmu.edu Alexander Hauptmann Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 U.S.A. alex@cs.cmu.edu Abstract In this paper we investigate how to automatically determine if two document collections are written from different perspectives." ></td>
	<td class="line x" title="4:268	By perspectives we mean a point of view, for example, from the perspective of Democrats or Republicans." ></td>
	<td class="line x" title="5:268	We propose a test of different perspectives based on distribution divergence between the statistical models of two collections." ></td>
	<td class="line x" title="6:268	Experimental results show that the test can successfully distinguish document collections of different perspectives from other types of collections." ></td>
	<td class="line x" title="7:268	1 Introduction Conflicts arise when two groups of people take very different perspectives on political, socioeconomical, or cultural issues." ></td>
	<td class="line x" title="8:268	For example, here are the answers that two presidential candidates, John Kerry and George Bush, gave during the third presidential debate in 2004 in response to a question on abortion: (1) Kerry: What is an article of faith for me is not something that I can legislate on somebody who doesnt share that article of faith." ></td>
	<td class="line x" title="9:268	I believe that choice is a womans choice." ></td>
	<td class="line x" title="10:268	Its between a woman, God and her doctor." ></td>
	<td class="line x" title="11:268	And thats why I support that." ></td>
	<td class="line x" title="12:268	(2) Bush: I believe the ideal world is one in which every child is protected in law and welcomed to life." ></td>
	<td class="line x" title="13:268	I understand theres great differences on this issue of abortion, but I believe reasonable people can come together and put good law in place that will help reduce the number of abortions." ></td>
	<td class="line x" title="14:268	After reading the above transcripts some readers may conclude that one takes a pro-choice perspective while the other takes a pro-life perspective, the two dominant perspectives in the abortion controversy." ></td>
	<td class="line x" title="15:268	Perspectives, however, are not always manifested when two pieces of text together are put together." ></td>
	<td class="line x" title="16:268	For example, the following two sentences are from Reuters newswire: (3) Gold output in the northeast China province of Heilongjiang rose 22.7 pct in 1986 from 1985s level, the New China News Agency said." ></td>
	<td class="line x" title="17:268	(4) Exco Chairman Richard Lacy told Reuters the acquisition was being made from Bank of New York Co Inc, which currently holds a 50.1 pct, and from RMJ partners who hold the remainder." ></td>
	<td class="line x" title="18:268	A reader would not from this pair of examples perceive as strongly contrasting perspectives as the Kerry-Bush answers." ></td>
	<td class="line x" title="19:268	Instead, as the Reuters annotators did, one would label Example 3 as gold and Example 4 as acquisition, that is, as two topics instead of two perspectives." ></td>
	<td class="line x" title="20:268	Why does the contrast between Example 1 and Example 2 convey different perspectives, but the contrast between Example 3 and Example 4 result in different topics?" ></td>
	<td class="line x" title="21:268	How can we define the impalpable different perspectives anyway?" ></td>
	<td class="line x" title="22:268	The definition of perspective in the dictionary is subjective evaluation of relative significance,1 but can we have a computable definition to test the existence of different perspectives?" ></td>
	<td class="line x" title="23:268	1The American Heritage Dictionary of the English Language, 4th ed." ></td>
	<td class="line x" title="24:268	We are interested in identifying ideological perspectives (Verdonk, 2002), not first-person or secondperson perspective in narrative." ></td>
	<td class="line x" title="25:268	1057 The research question about the definition of different perspectives is not only scientifically intriguing, it also enables us to develop important natural language processing applications." ></td>
	<td class="line x" title="26:268	Such a computational definition can be used to detect the emergence of contrasting perspectives." ></td>
	<td class="line x" title="27:268	Media and political analysts regularly monitor broadcast news, magazines, newspapers, and blogs to see if there are public opinion splitting." ></td>
	<td class="line x" title="28:268	The huge number of documents, however, make the task extremely daunting." ></td>
	<td class="line x" title="29:268	Therefore an automated test of different perspectives will be very valuable to information analysts." ></td>
	<td class="line x" title="30:268	We first review the relevant work in Section 2." ></td>
	<td class="line x" title="31:268	We take a model-based approach to develop a computational definition of different perspectives." ></td>
	<td class="line x" title="32:268	We first develop statistical models for the two document collections, A and B, and then measure the degree of contrast by calculating the distance between A and B. How document collections are statistically modeled and how distribution difference is estimated are described in Section 3." ></td>
	<td class="line x" title="33:268	The document corpora are described in Section 4." ></td>
	<td class="line x" title="34:268	In Section 5, we evaluate how effective the proposed test of difference perspectives based on statistical distribution." ></td>
	<td class="line x" title="35:268	The experimental results show that the distribution divergence can successfully separate document collections of different perspectives from other kinds of collection pairs." ></td>
	<td class="line x" title="36:268	We also investigate if the pattern of distribution difference is due to personal writing or speaking styles." ></td>
	<td class="line x" title="37:268	2 Related Work There has been interest in understanding how beliefs and ideologies can be represented in computers since mid-sixties of the last century (Abelson and Carroll, 1965; Schank and Abelson, 1977)." ></td>
	<td class="line x" title="38:268	The Ideology Machine (Abelson, 1973) can simulate a right-wing ideologue, and POLITICS (Carbonell, 1978) can interpret a text from conservative or liberal ideologies." ></td>
	<td class="line x" title="39:268	In this paper we take a statistics-based approach, which is very different from previous work that rely very much on manually-constructed knowledge base." ></td>
	<td class="line x" title="40:268	Note that what we are interested in is to determine if two document collections are written from different perspectives, not to model individual perspectives." ></td>
	<td class="line x" title="41:268	We aim to capture the characteristics, specifically the statistical regularities of any pairs of document collections with opposing perspectives." ></td>
	<td class="line x" title="42:268	Given a pair of document collections A and B, our goal is not to construct classifiers that can predict if a document was written from the perspective of A or B (Lin et al. , 2006), but to determine if the document collection pair (A,B) convey opposing perspectives." ></td>
	<td class="line x" title="43:268	There has been growing interest in subjectivity and sentiment analysis." ></td>
	<td class="line oc" title="44:268	There are studies on learning subjective language (Wiebe et al. , 2004), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Riloff et al. , 2003; Riloff and Wiebe, 2003), and discriminating between positive and negative language (Turney and Littman, 2003; Pang et al. , 2002; Dave et al. , 2003; Nasukawa and Yi, 2003; Morinaga et al. , 2002)." ></td>
	<td class="line x" title="45:268	There are also research work on automatically classifying movie or product reviews as positive or negative (Nasukawa and Yi, 2003; Mullen and Collier, 2004; Beineke et al. , 2004; Pang and Lee, 2004; Hu and Liu, 2004)." ></td>
	<td class="line x" title="46:268	Although we expect by its very nature much of the language used when expressing a perspective to be subjective and opinionated, the task of labeling a document or a sentence as subjective is orthogonal to the test of different perspectives." ></td>
	<td class="line x" title="47:268	A subjectivity classifier may successfully identify all subjective sentences in the document collection pair A and B, but knowing the number of subjective sentences in A and B does not necessarily tell us if they convey opposing perspectives." ></td>
	<td class="line x" title="48:268	We utilize the subjectivity patterns automatically extracted from foreign news documents (Riloff and Wiebe, 2003), and find that the percentages of the subjective sentences in the bitterlemons corpus (see Section 4) are similar (65.6% in the Palestinian documents and 66.2% in the Israeli documents)." ></td>
	<td class="line x" title="49:268	The high but almost equivalent number of subjective sentences in two perspectives suggests that perspective is largely expressed in subjective language but subjectivity ratio is not enough to tell if two document collections are written from the same (Palestinian v.s. Palestinian) or different perspectives (Palestinian v.s. Israeli)2." ></td>
	<td class="line x" title="50:268	3 Statistical Distribution Divergence We take a model-based approach to measure to what degree, if any, two document collections are different." ></td>
	<td class="line x" title="51:268	A document is represented as a point 2However, the close subjectivity ratio doesnt mean that subjectivity can never help identify document collections of opposing perspectives." ></td>
	<td class="line x" title="52:268	For example, the accuracy of the test of different perspectives may be improved by focusing on only subjective sentences." ></td>
	<td class="line x" title="53:268	1058 in a V -dimensional space, where V is vocabulary size." ></td>
	<td class="line x" title="54:268	Each coordinate is the frequency of a word in a document, i.e., term frequency." ></td>
	<td class="line x" title="55:268	Although vector representation, commonly known as a bag of words, is oversimplified and ignores rich syntactic and semantic structures, more sophisticated representation requires more data to obtain reliable models." ></td>
	<td class="line x" title="56:268	Practically, bag-of-word representation has been very effective in many tasks, including text categorization (Sebastiani, 2002) and information retrieval (Lewis, 1998)." ></td>
	<td class="line x" title="57:268	We assume that a collection of N documents, y1,y2,,yN are sampled from the following process,   Dirichlet() yi  Multinomial(ni,)." ></td>
	<td class="line x" title="58:268	We first sample a V -dimensional vector  from a Dirichlet prior distribution with a hyperparameter , and then sample a document yi repeatedly from a Multinomial distribution conditioned on the parameter , where ni is the document length of the ith document in the collection and assumed to be known and fixed." ></td>
	<td class="line x" title="59:268	We are interested in comparing the parameter  after observing document collections A and B: p(|A) = p(A|)p()p(A) = Dirichlet(|+ summationdisplay yiA yi)." ></td>
	<td class="line x" title="60:268	The posterior distribution p(|) is a Dirichlet distribution since a Dirichlet distribution is a conjugate prior for a Multinomial distribution." ></td>
	<td class="line x" title="61:268	How should we measure the difference between two posterior distributions p(|A) and p(|B)?" ></td>
	<td class="line x" title="62:268	One common way to measure the difference between two distributions is Kullback-Leibler (KL) divergence (Kullback and Leibler, 1951), defined as follows, D(p(|A)||p(|B)) = integraldisplay p(|A)log p(|A)p(|B)d." ></td>
	<td class="line x" title="63:268	(5) Directly calculating KL divergence according to (5) involves a difficult high-dimensional integral." ></td>
	<td class="line x" title="64:268	As an alternative, we approximate KL divergence using Monte Carlo methods as follows, 1." ></td>
	<td class="line x" title="65:268	Sample 1,2,,M from Dirichlet(|+summationtext yiAyi)." ></td>
	<td class="line x" title="66:268	2." ></td>
	<td class="line x" title="67:268	Return D = 1M summationtextMi=1 log p(i|A)p(i|B) as a Monte Carlo estimate of D(p(|A)||p(|B))." ></td>
	<td class="line x" title="68:268	Algorithms of sampling from Dirichlet distribution can be found in (Ripley, 1987)." ></td>
	<td class="line x" title="69:268	As M  , the Monte Carlo estimate will converge to true KL divergence by the Law of Large Numbers." ></td>
	<td class="line x" title="70:268	4 Corpora To evaluate how well KL divergence between posterior distributions can discern a document collection pair of different perspectives, we collect two corpora of documents that were written or spoken from different perspectives and one newswire corpus that covers various topics, as summarized in Table 1." ></td>
	<td class="line x" title="71:268	No stemming algorithms is performed; no stopwords are removed." ></td>
	<td class="line x" title="72:268	Corpus Subset |D| |d| V bitterlemons Palestinian 290 748.7 10309 Israeli 303 822.4 11668 Pal." ></td>
	<td class="line x" title="73:268	Editor 144 636.2 6294 Pal." ></td>
	<td class="line x" title="74:268	Guest 146 859.6 8661 Isr." ></td>
	<td class="line x" title="75:268	Editor 152 819.4 8512 Isr." ></td>
	<td class="line x" title="76:268	Guest 151 825.5 8812 2004 Presidential Debate Kerry 178 124.7 2554 Bush 176 107.8 2393 1st Kerry 33 216.3 1274 1st Bush 41 155.3 1195 2nd Kerry 73 103.8 1472 2nd Bush 75 89.0 1333 3rd Kerry 72 104.0 1408 3rd Bush 60 98.8 1281 Reuters21578 ACQ 2448 124.7 14293 CRUDE 634 214.7 9009 EARN 3987 81.0 12430 GRAIN 628 183.0 8236 INTEREST 513 176.3 6056 MONEY-FX 801 197.9 8162 TRADE 551 255.3 8175 Table 1: The number of documents |D|, average document length |d|, and vocabulary size V of the three corpora." ></td>
	<td class="line x" title="77:268	The first perspective corpus consists of articles published on the bitterlemons website3 from late 2001 to early 2005." ></td>
	<td class="line x" title="78:268	The website is set up to contribute to mutual understanding [between Palestinians and Israelis] through the open exchange of ideas4." ></td>
	<td class="line x" title="79:268	Every week an issue about the Israeli-Palestinian conflict is selected for discussion (e.g. , Disengagement: unilateral or coordinated?), and a Palestinian editor and an Israeli editor each contribute one article addressing the 3http://www.bitterlemons.org/ 4http://www.bitterlemons.org/about/ about.html 1059 issue." ></td>
	<td class="line x" title="80:268	In addition, the Israeli and Palestinian editors interview a guest to express their views on the issue, resulting in a total of four articles in a weekly edition." ></td>
	<td class="line x" title="81:268	The perspective from which each article is written is labeled as either Palestinian or Israeli by the editors." ></td>
	<td class="line x" title="82:268	The second perspective corpus consists of the transcripts of the three Bush-Kerry presidential debates in 2004." ></td>
	<td class="line x" title="83:268	The transcripts are from the website of the Commission on Presidential Debates5." ></td>
	<td class="line x" title="84:268	Each spoken document is roughly an answer to a question or a rebuttal." ></td>
	<td class="line x" title="85:268	The transcript are segmented by the speaker tags already in the transcripts." ></td>
	<td class="line x" title="86:268	All words from moderators are discarded." ></td>
	<td class="line x" title="87:268	The topical corpus contains newswire from Reuters in 1987." ></td>
	<td class="line x" title="88:268	Reuters-215786 is one of the most common testbeds for text categorization." ></td>
	<td class="line x" title="89:268	Each document belongs to none, one, or more of the 135 categories (e.g. , Mergers and U.S. Dollars)." ></td>
	<td class="line x" title="90:268	The number of documents in each category is not evenly distributed (median 9.0, mean 105.9)." ></td>
	<td class="line x" title="91:268	To estimate statistics reliably, we only consider categories with more than 500 documents, resulting in a total of seven categories (ACQ, CRUDE, EARN, GRAIN, INTEREST, MONEY-FX, and TRADE)." ></td>
	<td class="line x" title="92:268	5 Experiments A test of different perspectives is acute when it can draw distinctions between document collection pairs of different perspectives and document collection pairs of the same perspective and others." ></td>
	<td class="line x" title="93:268	We thus evaluate the proposed test of different perspectives in the following four types of document collection pairs (A,B): Different Perspectives (DP) A and B are written from different perspectives." ></td>
	<td class="line x" title="94:268	For example, A is written from the Palestinian perspective and B is written from the Israeli perspective in the bitterlemons corpus." ></td>
	<td class="line x" title="95:268	Same Perspective (SP) A and B are written from the same perspective." ></td>
	<td class="line x" title="96:268	For example, A and B consist of the words spoken by Kerry." ></td>
	<td class="line x" title="97:268	Different Topics (DT) A and B are written on different topics." ></td>
	<td class="line x" title="98:268	For example, A is about 5http://www.debates.org/pages/ debtrans.html 6http://www.ics.uci.edu/kdd/ databases/reuters21578/reuters21578.html acquisition (ACQ) and B is about crude oil (CRUDE)." ></td>
	<td class="line x" title="99:268	Same Topic (ST) A and B are written on the same topic." ></td>
	<td class="line x" title="100:268	For example, A and B are both about earnings (EARN)." ></td>
	<td class="line x" title="101:268	The effectiveness of the proposed test of different perspectives can thus be measured by how the distribution divergence of DP document collection pairs is separated from the distribution divergence of SP, DT, and ST document collection pairs." ></td>
	<td class="line x" title="102:268	The little the overlap of the range of distribution divergence, the sharper the test of different perspectives." ></td>
	<td class="line x" title="103:268	To account for large variation in the number of words and vocabulary size across corpora, we normalize the total number of words in a document collection to be the same K, and consider only the top C% frequent words in the document collection pair." ></td>
	<td class="line x" title="104:268	We vary the values of K and C, and find that K changes the absolute scale of KL divergence but does not change the rankings of four conditions." ></td>
	<td class="line x" title="105:268	Rankings among four conditions is consistent when C is small." ></td>
	<td class="line x" title="106:268	We only report results of K = 1000,C = 10 in the paper due to space limit." ></td>
	<td class="line x" title="107:268	There are two kinds of variances in the estimation of divergence between two posterior distribution and should be carefully checked." ></td>
	<td class="line x" title="108:268	The first kind of variance is due to Monte Carlo methods." ></td>
	<td class="line x" title="109:268	We assess the Monte Carlo variance by calculating a 100 percent confidence interval as follows, [ D1(2) M, D + 1(1 2) M] where 2 is the sample variance of 1,2,,M, and ()1 is the inverse of the standard normal cumulative density function." ></td>
	<td class="line x" title="110:268	The second kind of variance is due to the intrinsic uncertainties of data generating processes." ></td>
	<td class="line x" title="111:268	We assess the second kind of variance by collecting 1000 bootstrapped samples, that is, sampling with replacement, from each document collection pair." ></td>
	<td class="line x" title="112:268	5.1 Quality of Monte Carlo Estimates The Monte Carlo estimates of the KL divergence from several document collection pair are listed in Table 2." ></td>
	<td class="line x" title="113:268	A complete list of the results is omitted due to the space limit." ></td>
	<td class="line x" title="114:268	We can see that the 95% confidence interval captures well the Monte Carlo estimates of KL divergence." ></td>
	<td class="line x" title="115:268	Note that KL divergence is not symmetric." ></td>
	<td class="line x" title="116:268	The KL divergence 1060 A B D 95% CI ACQ ACQ 2.76 [2.62, 2.89] Palestinian Palestinian 3.00 [3.54, 3.85] Palestinian Israeli 27.11 [26.64, 27.58] Israeli Palestinian 28.44 [27.97, 28.91] Kerry Bush 58.93 [58.22, 59.64] ACQ EARN 615.75 [610.85, 620.65] Table 2: The Monte Carlo estimate D and 95% confidence interval (CI) of the Kullback-Leibler divergence of several document collection pairs (A,B) with the number of Monte Carlo samples M = 1000." ></td>
	<td class="line x" title="117:268	of the pair (Israeli, Palestinian) is not necessarily the same as (Palestinian, Israeli)." ></td>
	<td class="line x" title="118:268	KL divergence is greater than zero (Cover and Thomas, 1991) and equal to zero only when document collections A and B are exactly the same." ></td>
	<td class="line x" title="119:268	Here (ACQ, ACQ) is close to but not exactly zero because they are different samples of documents in the ACQ category." ></td>
	<td class="line x" title="120:268	Since the CIs of Monte Carlo estimates are reasonably tight, we assume them to be exact and ignore the errors from Monte Carlo methods." ></td>
	<td class="line x" title="121:268	5.2 Test of Different Perspectives We now present the main result of the paper." ></td>
	<td class="line x" title="122:268	We calculate the KL divergence between posterior distributions of document collection pairs in four conditions using Monte Carlo methods, and plot the results in Figure 1." ></td>
	<td class="line x" title="123:268	The test of different perspectives based on statistical distribution divergence is shown to be very acute." ></td>
	<td class="line x" title="124:268	The KL divergence of the document collection pairs in the DP condition fall mostly in the middle range, and is well separated from the high KL divergence of the pairs in DT condition and from the low KL divergence of the pairs in SP and ST conditions." ></td>
	<td class="line x" title="125:268	Therefore, by simply calculating the KL divergence of a document collection pair, we can reliably predict that they are written from different perspectives if the value of KL divergence falls in the middle range, from different topics if the value is very large, from the same topic or perspective if the value is very small." ></td>
	<td class="line x" title="126:268	5.3 Personal Writing Styles or Perspectives?" ></td>
	<td class="line x" title="127:268	One may suspect that the mid-range distribution divergence is attributed to personal speaking or writing styles and has nothing to do with different perspectives." ></td>
	<td class="line x" title="128:268	The doubt is expected because half of the bitterlemons corpus are written by one Palestinian editor and one Israeli editor (see Table 1), and the debate transcripts come from only two candidates." ></td>
	<td class="line x" title="129:268	We test the hypothesis by computing the distribution divergence of the document collection pair (Israeli Guest, Palestinian Guest), that is, a Different Perspectives (DP) pair." ></td>
	<td class="line x" title="130:268	There are more than 200 different authors in the Israeli Guest and Palestinian Guest collection." ></td>
	<td class="line x" title="131:268	If the distribution divergence of the pair with diverse authors falls out of the middle range, it will support that mid-range divergence is due to writing styles." ></td>
	<td class="line x" title="132:268	On the other hand, if the distribution divergence still fall in the middle range, we are more confident the effect is attributed to different perspectives." ></td>
	<td class="line x" title="133:268	We compare the distribution divergence of the pair (Israeli Guest, Palestinian Guest) with others in Figure 2." ></td>
	<td class="line x" title="134:268	ST SP DP Guest DT KL Divergence 1 2 5 10 20 50 200 500 Figure 2: The average KL divergence of document collection pairs in the bitterlemons Guest subset (Israeli Guest vs. Palestinian Guest), ST,SP, DP, DT conditions." ></td>
	<td class="line x" title="135:268	The horizontal lines are the same as those in Figure 1." ></td>
	<td class="line x" title="136:268	The results show that the distribution divergence of the (Israeli Guest, Palestinian Guest) pair, as other pairs in the DP condition, still falls in the middle range, and is well separated from SP and ST in the low range and DT in the high range." ></td>
	<td class="line x" title="137:268	The decrease in KL divergence due to writing or speaking styles is noticeable, and the overall effect due to different perspectives is strong enough to make the test robust." ></td>
	<td class="line x" title="138:268	We thus conclude that the test of different perspectives based on distribution divergence indeed captures different perspectives, not personal writing or speaking styles." ></td>
	<td class="line x" title="139:268	5.4 Origins of Differences While the effectiveness of the test of different perspectives is demonstrated in Figure 1, one may 1061 2 5 10 20 50 100 200 500 1000 0.00 0.05 0.10 0.15 KL Divergence Density SP ST DP DT Figure 1: The KL divergence of the document collection pairs in four conditions: Different Perspectives (DP), Same Perspective (SP), Different Topics (DT), and Same Topic (ST)." ></td>
	<td class="line x" title="140:268	Note that the x axis is in log scale." ></td>
	<td class="line x" title="141:268	The Monte Carlo estimates D of the pairs in DP condition are plotted as rugs." ></td>
	<td class="line x" title="142:268	D of the pairs in other conditions are omitted to avoid clutter and summarized in one-dimensional density using Kernel Density Estimation." ></td>
	<td class="line x" title="143:268	The vertical lines are drawn at the points with equivalent densities." ></td>
	<td class="line x" title="144:268	wonder why the distribution divergence of the document collection pair with different perspectives falls in the middle range and what causes the large and small divergence of the document collection pairs with different topics (DT) and the same topic (ST) or perspective (SP), respectively." ></td>
	<td class="line x" title="145:268	In other words where do the differences result from?" ></td>
	<td class="line x" title="146:268	We answer the question by taking a closer look at the causes of the distribution divergence in our model." ></td>
	<td class="line x" title="147:268	We compare the expected marginal difference of  between two posterior distributions p(|A) and p(|B)." ></td>
	<td class="line x" title="148:268	The marginal distribution of the i-th coordinate of , that is, the i-th word in the vocabulary, is a Beta distribution, and thus the expected value can be easily calculated." ></td>
	<td class="line x" title="149:268	We plot the  = E[i|A]E[i|B] against E[i|A] for each condition in Figure 3." ></td>
	<td class="line x" title="150:268	How  is deviated from zero partially explains different patterns of distribution divergence in Figure 1." ></td>
	<td class="line x" title="151:268	In Figure 3d we see that the  increases as  increases, and the deviance from zero is much greater than those in the Same Perspective (Figure 3b) and Same Topic (Figure 3a) conditions." ></td>
	<td class="line x" title="152:268	The large  not only accounts for large distribution divergence of the document pairs in DT conditions, but also shows that words in different topics that is frequent in one topic are less likely to be frequent in the other topic." ></td>
	<td class="line x" title="153:268	At the other extreme, document collection pairs of the Same Perspective (SP) or Same Topic (ST) show very little difference in , which matches our intuition that documents of the same perspective or the same topic use the same vocabulary in a very similar way." ></td>
	<td class="line x" title="154:268	The manner in which  is varied with the value of  in the Different Perspective (DP) condition is very unique." ></td>
	<td class="line x" title="155:268	The  in Figure 3c is not as small as those in the SP and ST conditions, but at the same time not as large as those in DT conditions, resulting in mid-range distribution divergence in Figure 1." ></td>
	<td class="line x" title="156:268	Why do document collections of different perspectives distribute this way?" ></td>
	<td class="line x" title="157:268	Partly because articles from different perspectives focus on the closely related issues (the PalestinianIsraeli conflict in the bitterlemons corpus, or the political and economical issues in the debate corpus), the authors of different perspectives write or speak in a similar vocabulary, but with emphasis on different words." ></td>
	<td class="line x" title="158:268	6 Conclusions In this paper we develop a computational test of different perspectives based on statistical distribution divergence between the statistical models of document collections." ></td>
	<td class="line x" title="159:268	We show that the pro1062 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.04 0.02 0.00 0.02 0.04 (a) Same Topic (ST) 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.04 0.02 0.00 0.02 0.04 (b) Same Topic (SP) 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.04 0.02 0.00 0.02 0.04 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.04 0.02 0.00 0.02 0.04 (c) Two examples of Different Perspective (DP) Figure 3: The  vs.  plots of the typical document collection pairs in four conditions." ></td>
	<td class="line x" title="160:268	The horizontal line is  = 0." ></td>
	<td class="line x" title="161:268	0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.04 0.02 0.00 0.02 0.04 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.04 0.02 0.00 0.02 0.04 (d) Two examples of Different Topics (DT) Figure 3: Contd posed test can successfully separate document collections of different perspectives from other types of document collection pairs." ></td>
	<td class="line x" title="162:268	The distribution divergence falling in the middle range can not simply be attributed to personal writing or speaking styles." ></td>
	<td class="line x" title="163:268	From the plot of multinomial parameter difference we offer insights into where the different patterns of distribution divergence come from." ></td>
	<td class="line x" title="164:268	Although we validate the test of different perspectives by comparing the DP condition with DT, SP, and ST conditions, the comparisons are by no means exhaustive, and the distribution divergence of some document collection pairs may also fall in the middle range." ></td>
	<td class="line x" title="165:268	We plan to investigate more types of document collections pairs, e.g., the document collections from different text genres (Kessler et al. , 1997)." ></td>
	<td class="line x" title="166:268	Acknowledgment We would like thank the anonymous reviewers for useful comments and suggestions." ></td>
	<td class="line x" title="167:268	This material is based on work supported by the Advanced Research and Development Activity (ARDA) under contract number NBCHC040037." ></td>
	<td class="line x" title="168:268	1063 References Robert P. Abelson and J. Douglas Carroll." ></td>
	<td class="line x" title="169:268	1965." ></td>
	<td class="line x" title="170:268	Computer simulation of individual belief systems." ></td>
	<td class="line x" title="171:268	The American Behavioral Scientist, 8:2430, May. Robert P. Abelson, 1973." ></td>
	<td class="line x" title="172:268	Computer Models of Thought and Language, chapter The Structure of Belief Systems, pages 287339." ></td>
	<td class="line x" title="173:268	W. H. Freeman and Company." ></td>
	<td class="line x" title="174:268	Philip Beineke, Trevor Hastie, and Shivakumar Vaithyanathan." ></td>
	<td class="line x" title="175:268	2004." ></td>
	<td class="line x" title="176:268	The sentimental factor: Improving review classification via human-provided information." ></td>
	<td class="line x" title="177:268	In Proceedings of the Association for Computational Linguistics (ACL-2004)." ></td>
	<td class="line x" title="178:268	Jaime G. Carbonell." ></td>
	<td class="line x" title="179:268	1978." ></td>
	<td class="line x" title="180:268	POLITICS: Automated ideological reasoning." ></td>
	<td class="line x" title="181:268	Cognitive Science, 2(1):27 51." ></td>
	<td class="line x" title="182:268	Thomas M. Cover and Joy A. Thomas." ></td>
	<td class="line x" title="183:268	1991." ></td>
	<td class="line x" title="184:268	Elements of Information Theory." ></td>
	<td class="line x" title="185:268	Wiley-Interscience." ></td>
	<td class="line x" title="186:268	Kushal Dave, Steve Lawrence, and David M. Pennock." ></td>
	<td class="line x" title="187:268	2003." ></td>
	<td class="line x" title="188:268	Mining the peanut gallery: Opinion extraction and semantic classification of product reviews." ></td>
	<td class="line x" title="189:268	In Proceedings of the 12th International World Wide Web Conference (WWW2003)." ></td>
	<td class="line x" title="190:268	Minqing Hu and Bing Liu." ></td>
	<td class="line x" title="191:268	2004." ></td>
	<td class="line x" title="192:268	Mining and summarizing customer reviews." ></td>
	<td class="line x" title="193:268	In Proceedings of the 2004 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining." ></td>
	<td class="line x" title="194:268	Brett Kessler, Geoffrey Nunberg, and Hinrich Schutze." ></td>
	<td class="line x" title="195:268	1997." ></td>
	<td class="line x" title="196:268	Automatic detection of text genre." ></td>
	<td class="line x" title="197:268	In Proceedings of the 35th Conference on Association for Computational Linguistics, pages 3238." ></td>
	<td class="line x" title="198:268	S. Kullback and R. A. Leibler." ></td>
	<td class="line x" title="199:268	1951." ></td>
	<td class="line x" title="200:268	On information and sufficiency." ></td>
	<td class="line x" title="201:268	The Annals of Mathematical Statistics, 22(1):7986, March." ></td>
	<td class="line x" title="202:268	David D. Lewis." ></td>
	<td class="line x" title="203:268	1998." ></td>
	<td class="line x" title="204:268	Naive (Bayes) at forty: The independence assumption in information retrieval." ></td>
	<td class="line x" title="205:268	In Proceedings of the 9th European Conference on Machine Learning (ECML)." ></td>
	<td class="line x" title="206:268	Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and Alexander Hauptmann." ></td>
	<td class="line x" title="207:268	2006." ></td>
	<td class="line x" title="208:268	Which side are you on?" ></td>
	<td class="line x" title="209:268	identifying perspectives at the document and sentence levels." ></td>
	<td class="line x" title="210:268	In Proceedings of Tenth Conference on Natural Language Learning (CoNLL)." ></td>
	<td class="line x" title="211:268	S. Morinaga, K. Yamanishi, K. Tateishi, and T. Fukushima." ></td>
	<td class="line x" title="212:268	2002." ></td>
	<td class="line x" title="213:268	Mining product reputations on the web." ></td>
	<td class="line x" title="214:268	In Proceedings of the 2002 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining." ></td>
	<td class="line x" title="215:268	Tony Mullen and Nigel Collier." ></td>
	<td class="line x" title="216:268	2004." ></td>
	<td class="line x" title="217:268	Sentiment analysis using support vector machines with diverse information sources." ></td>
	<td class="line x" title="218:268	In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2004)." ></td>
	<td class="line x" title="219:268	T. Nasukawa and J. Yi." ></td>
	<td class="line x" title="220:268	2003." ></td>
	<td class="line x" title="221:268	Sentiment analysis: Capturing favorability using natural language processing." ></td>
	<td class="line x" title="222:268	In Proceedings of the 2nd International Conference on Knowledge Capture (K-CAP 2003)." ></td>
	<td class="line x" title="223:268	Bo Pang and Lillian Lee." ></td>
	<td class="line x" title="224:268	2004." ></td>
	<td class="line x" title="225:268	A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts." ></td>
	<td class="line x" title="226:268	In Proceedings of the Association for Computational Linguistics (ACL-2004)." ></td>
	<td class="line x" title="227:268	Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan." ></td>
	<td class="line x" title="228:268	2002." ></td>
	<td class="line x" title="229:268	Thumbs up?" ></td>
	<td class="line x" title="230:268	Sentiment classification using machine learning techniques." ></td>
	<td class="line x" title="231:268	In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2002)." ></td>
	<td class="line x" title="232:268	Ellen Riloff and Janyce Wiebe." ></td>
	<td class="line x" title="233:268	2003." ></td>
	<td class="line x" title="234:268	Learning extraction patterns for subjective expressions." ></td>
	<td class="line x" title="235:268	In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2003)." ></td>
	<td class="line x" title="236:268	Ellen Riloff, Janyce Wiebe, and Theresa Wilson." ></td>
	<td class="line x" title="237:268	2003." ></td>
	<td class="line x" title="238:268	Learning subjective nouns using extraction pattern bootstrapping." ></td>
	<td class="line x" title="239:268	In Proceedings of the 7th Conference on Natural Language Learning (CoNLL-2003)." ></td>
	<td class="line x" title="240:268	B. D. Ripley." ></td>
	<td class="line x" title="241:268	1987." ></td>
	<td class="line x" title="242:268	Stochastic Simulation." ></td>
	<td class="line x" title="243:268	Wiley." ></td>
	<td class="line x" title="244:268	Roger C. Schank and Robert P. Abelson." ></td>
	<td class="line x" title="245:268	1977." ></td>
	<td class="line x" title="246:268	Scripts, plans, goals, and understanding: an inquiry into human knowledge structures." ></td>
	<td class="line x" title="247:268	Lawrene Erlbaum Associates." ></td>
	<td class="line x" title="248:268	Fabrizio Sebastiani." ></td>
	<td class="line x" title="249:268	2002." ></td>
	<td class="line x" title="250:268	Machine learning in automated text categorization." ></td>
	<td class="line x" title="251:268	ACM Computing Surveys, 34(1):147, March." ></td>
	<td class="line x" title="252:268	Peter Turney and Michael L. Littman." ></td>
	<td class="line x" title="253:268	2003." ></td>
	<td class="line x" title="254:268	Measuring praise and criticism: Inference of semantic orientation from association." ></td>
	<td class="line x" title="255:268	ACM Transactions on Information Systems (TOIS), 21(4):315346." ></td>
	<td class="line x" title="256:268	Peter Verdonk." ></td>
	<td class="line x" title="257:268	2002." ></td>
	<td class="line x" title="258:268	Stylistics." ></td>
	<td class="line x" title="259:268	Oxford University Press." ></td>
	<td class="line x" title="260:268	Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell, and Melanie Martin." ></td>
	<td class="line x" title="261:268	2004." ></td>
	<td class="line x" title="262:268	Learning subjective language." ></td>
	<td class="line x" title="263:268	Computational Linguistics, 30(3)." ></td>
	<td class="line x" title="264:268	Hong Yu and Vasileios Hatzivassiloglou." ></td>
	<td class="line x" title="265:268	2003." ></td>
	<td class="line x" title="266:268	Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences." ></td>
	<td class="line x" title="267:268	In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2003)." ></td>
	<td class="line x" title="268:268	1064" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-2059
Automatic Construction Of Polarity-Tagged Corpus From HTML Documents
Kaji, Nobuhiro;Kitsuregawa, Masaru;"></td>
	<td class="line x" title="1:288	Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 452459, Sydney, July 2006." ></td>
	<td class="line x" title="2:288	c2006 Association for Computational Linguistics Automatic Construction of Polarity-tagged Corpus from HTML Documents Nobuhiro Kaji and Masaru Kitsuregawa Institute of Industrial Science the University of Tokyo 4-6-1 Komaba, Meguro-ku, Tokyo 153-8505 Japan CUkaji,kitsureCV@tkl.iis.u-tokyo.ac.jp Abstract This paper proposes a novel method of building polarity-tagged corpus from HTML documents." ></td>
	<td class="line x" title="3:288	The characteristics of this method is that it is fully automatic and can be applied to arbitrary HTML documents." ></td>
	<td class="line x" title="4:288	The idea behind our method is to utilize certain layout structures and linguistic pattern." ></td>
	<td class="line x" title="5:288	By using them, we can automatically extract such sentences that express opinion." ></td>
	<td class="line x" title="6:288	In our experiment, the method could construct a corpus consisting of 126,610 sentences." ></td>
	<td class="line x" title="7:288	1 Introduction Recently, there has been an increasing interest in such applications that deal with opinions (a.k.a. sentiment, reputation etc.)." ></td>
	<td class="line x" title="8:288	For instance, Morinaga et al. developed a system that extracts and analyzes reputations on the Internet (Morinaga et al. , 2002)." ></td>
	<td class="line oc" title="9:288	Pang et al. proposed a method of classifying movie reviews into positive and negative ones (Pang et al. , 2002)." ></td>
	<td class="line o" title="10:288	In these applications, one of the most important issue is how to determine the polarity (or semantic orientation) of a given text." ></td>
	<td class="line x" title="11:288	In other words, it is necessary to decide whether a given text conveys positive or negative content." ></td>
	<td class="line x" title="12:288	In order to solve this problem, we intend to take statistical approach." ></td>
	<td class="line x" title="13:288	More specifically, we plan to learn the polarity of texts from a corpus in which phrases, sentences or documents are tagged with labels expressing the polarity (polarity-tagged corpus)." ></td>
	<td class="line oc" title="14:288	So far, this approach has been taken by a lot of researchers (Pang et al. , 2002; Dave et al. , 2003; Wilson et al. , 2005)." ></td>
	<td class="line o" title="15:288	In these previous works, polarity-tagged corpus was built in either of the following two ways." ></td>
	<td class="line x" title="16:288	It is built manually, or created from review sites such as AMAZON.COM." ></td>
	<td class="line x" title="17:288	In some review sites, the review is associated with metadata indicating its polarity." ></td>
	<td class="line x" title="18:288	Those reviews can be used as polarity-tagged corpus." ></td>
	<td class="line x" title="19:288	In case of AMAZON.COM, the reviews polarity is represented by using 5-star scale." ></td>
	<td class="line n" title="20:288	However, both of the two approaches are not appropriate for building large polarity-tagged corpus." ></td>
	<td class="line x" title="21:288	Since manual construction of tagged corpus is time-consuming and expensive, it is difficult to build large polarity-tagged corpus." ></td>
	<td class="line n" title="22:288	The method that relies on review sites can not be applied to domains in which large amount of reviews are not available." ></td>
	<td class="line x" title="23:288	In addition, the corpus created from reviews is often noisy as we discuss in Section 2." ></td>
	<td class="line x" title="24:288	This paper proposes a novel method of building polarity-tagged corpus from HTML documents." ></td>
	<td class="line x" title="25:288	The idea behind our method is to utilize certain layout structures and linguistic pattern." ></td>
	<td class="line x" title="26:288	By using them, we can automatically extract sentences that express opinion (opinion sentences) from HTML documents." ></td>
	<td class="line x" title="27:288	Because this method is fully automatic and can be applied to arbitrary HTML documents, it does not suffer from the same problems as the previous methods." ></td>
	<td class="line x" title="28:288	In the experiment, we could construct a corpus consisting of 126,610 sentences." ></td>
	<td class="line x" title="29:288	To validate the quality of the corpus, two human judges assessed a part of the corpus and found that 92% opinion sentences are appropriate ones." ></td>
	<td class="line x" title="30:288	Furthermore, we applied our corpus to opinion sentence classification task." ></td>
	<td class="line x" title="31:288	Naive Bayes classifier was trained on our corpus and tested on three data sets." ></td>
	<td class="line x" title="32:288	The result demonstrated that the classifier achieved more than 80% accuracy in each data set." ></td>
	<td class="line x" title="33:288	The following of this paper is organized as fol452 lows." ></td>
	<td class="line x" title="34:288	Section 2 shows the design of the corpus constructed by our method." ></td>
	<td class="line x" title="35:288	Section 3 gives an overview of our method, and the detail follows in Section 4." ></td>
	<td class="line x" title="36:288	In Section 5, we discuss experimental results, and in Section 6 we examine related works." ></td>
	<td class="line x" title="37:288	Finally we conclude in Section 7." ></td>
	<td class="line x" title="38:288	2 Corpus Design This Section explains the design of our corpus that is built automatically." ></td>
	<td class="line x" title="39:288	Table 1 represents a part of our corpus that was actually constructed in the experiment." ></td>
	<td class="line x" title="40:288	Note that this paper treats Japanese." ></td>
	<td class="line x" title="41:288	The sentences in the Table are translations, and the original sentences are in Japanese." ></td>
	<td class="line x" title="42:288	The followings are characteristics of our corpus: AF Our corpus uses two labels, B7 and A0." ></td>
	<td class="line x" title="43:288	They denote positive and negative sentences respectively." ></td>
	<td class="line x" title="44:288	Other labels such as neutral are not used." ></td>
	<td class="line x" title="45:288	AF Since we do not use neutral label, such sentence that does not convey opinion is not stored in our corpus." ></td>
	<td class="line x" title="46:288	AF The label is assigned to not multiple sentences (or document) but single sentence." ></td>
	<td class="line x" title="47:288	Namely, our corpus is tagged at sentence level rather than document level." ></td>
	<td class="line x" title="48:288	It is important to discuss the reason that we intend to build a corpus tagged at sentence level rather than document level." ></td>
	<td class="line x" title="49:288	The reason is that one document often includes both positive and negative sentences, and hence it is difficult to learn the polarity from the corpus tagged at document level." ></td>
	<td class="line oc" title="50:288	Consider the following example (Pang et al. , 2002): This film should be brilliant." ></td>
	<td class="line x" title="51:288	It sounds like a great plot, the actors are first grade, and the supporting cast is good as well, and Stallone is attempting to deliver a good performance." ></td>
	<td class="line x" title="52:288	However, it cant hold up." ></td>
	<td class="line x" title="53:288	This document as a whole expresses negative opinion, and should be labeled negative if it is tagged at document level." ></td>
	<td class="line x" title="54:288	However, it includes several sentences that represent positive attitude." ></td>
	<td class="line x" title="55:288	We would like to point out that polarity-tagged corpus created from reviews prone to be tagged at document-level." ></td>
	<td class="line x" title="56:288	This is because meta-data (e.g. stars in AMAZON.COM) is usually associated with one review rather than individual sentences in a review." ></td>
	<td class="line x" title="57:288	This is one serious problem in previous works." ></td>
	<td class="line x" title="58:288	Table 1: A part of automatically constructed polarity-tagged corpus." ></td>
	<td class="line x" title="59:288	label opinion sentence B7 It has high adaptability." ></td>
	<td class="line x" title="60:288	A0 The cost is expensive." ></td>
	<td class="line x" title="61:288	A0 The engine is powerless and noisy." ></td>
	<td class="line x" title="62:288	B7 The usage is easy to understand." ></td>
	<td class="line x" title="63:288	B7 Above all, the price is reasonable." ></td>
	<td class="line x" title="64:288	3 The Idea This Section briefly explains our basic idea, and the detail of our corpus construction method is represented in the next Section." ></td>
	<td class="line x" title="65:288	Our idea is to use certain layout structures and linguistic pattern in order to extract opinion sentences from HTML documents." ></td>
	<td class="line x" title="66:288	More specifically, we used two kinds of layout structures: the itemization and the table." ></td>
	<td class="line x" title="67:288	In what follows, we explain examples where opinion sentences can be extracted by using the itemization, table and linguistic pattern." ></td>
	<td class="line x" title="68:288	3.1 Itemization The first idea is to extract opinion sentences from the itemization (Figure 1)." ></td>
	<td class="line x" title="69:288	In this Figure, opinions about a music player are itemized and these itemizations have headers such as pros and cons." ></td>
	<td class="line x" title="70:288	By using the headers, we can recognize that opinion sentences are described in these itemizations." ></td>
	<td class="line x" title="71:288	Pros: AF The sound is natural." ></td>
	<td class="line x" title="72:288	AF Music is easy to find." ></td>
	<td class="line x" title="73:288	AF Can enjoy creating my favorite play-lists." ></td>
	<td class="line x" title="74:288	Cons: AF The remote controller does not have an LCD display." ></td>
	<td class="line x" title="75:288	AF The body gets scratched and fingerprinted easily." ></td>
	<td class="line x" title="76:288	AF The battery drains quickly when using the backlight." ></td>
	<td class="line x" title="77:288	Figure 1: Opinion sentences in itemization." ></td>
	<td class="line x" title="78:288	Hereafter, such phrases that indicate the pres453 ence of opinion sentences are called indicators." ></td>
	<td class="line x" title="79:288	Indicators for positive sentences are called positive indicators." ></td>
	<td class="line x" title="80:288	Pros is an example of positive indicator." ></td>
	<td class="line x" title="81:288	Similarly, indicators for negative sentences are called negative indicators." ></td>
	<td class="line x" title="82:288	3.2 Table The second idea is to use the table structure (Figure 2)." ></td>
	<td class="line x" title="83:288	In this Figure, a car review is summarized in the table." ></td>
	<td class="line x" title="84:288	Mileage(urban) 7.0km/litter Mileage(highway) 9.0km/litter Plus This is a four door car, but its so cool." ></td>
	<td class="line x" title="85:288	Minus The seat is ragged and the light is dark." ></td>
	<td class="line x" title="86:288	Figure 2: Opinion sentences in table." ></td>
	<td class="line x" title="87:288	We can predict that there are opinion sentences in this table, because the left column acts as a header and there are indicators (plus and minus) in that column." ></td>
	<td class="line x" title="88:288	3.3 Linguistic pattern The third idea is based on linguistic pattern." ></td>
	<td class="line x" title="89:288	Because we treat Japanese, the pattern that is discussed in this paper depends on Japanese grammar although we think there are similar patterns in other languages including English." ></td>
	<td class="line x" title="90:288	Consider the Japanese sentences attached with English translations (Figure 3)." ></td>
	<td class="line x" title="91:288	Japanese sentences are written in italics and - denotes that the word is followed by postpositional particles." ></td>
	<td class="line x" title="92:288	For example, software-no means that software is followed by postpositional particle no." ></td>
	<td class="line x" title="93:288	Translations of each word and the entire sentence are represented below the original Japanese sentence." ></td>
	<td class="line x" title="94:288	-POST means postpositional particle." ></td>
	<td class="line x" title="95:288	In the examples, we focused on the singly underlined phrases." ></td>
	<td class="line x" title="96:288	Roughly speaking, they correspond to the advantage/weakness is to in English." ></td>
	<td class="line x" title="97:288	In these phrases, indicators (riten (advantage) and ketten (weakness)) are followed by postpositional particle -ha, which is topic marker." ></td>
	<td class="line x" title="98:288	And hence, we can recognize that something good (or bad) is the topic of the sentence." ></td>
	<td class="line x" title="99:288	Based on this observation, we crafted a linguistic pattern that can detect the singly underlined phrases." ></td>
	<td class="line x" title="100:288	And then, we extracted doubly underlined phrases as opinions." ></td>
	<td class="line x" title="101:288	They correspond to run quickly and take too much time." ></td>
	<td class="line x" title="102:288	The detail of this process is discussed in the next Section." ></td>
	<td class="line x" title="103:288	4 Automatic Corpus Construction This Section represents the detail of the corpus construction procedure." ></td>
	<td class="line x" title="104:288	As shown in the previous Section, our idea utilizes the indicator, and it is important to recognize indicators in HTML documents." ></td>
	<td class="line x" title="105:288	To do this, we manually crafted lexicon, in which positive and negative indicators are listed." ></td>
	<td class="line x" title="106:288	This lexicon consists of 303 positive and 433 negative indicators." ></td>
	<td class="line x" title="107:288	Using this lexicon, the polarity-tagged corpus is constructed from HTML documents." ></td>
	<td class="line x" title="108:288	The method consists of the following three steps: 1." ></td>
	<td class="line x" title="109:288	Preprocessing Before extracting opinion sentences, HTML documents are preprocessed." ></td>
	<td class="line x" title="110:288	This process involves separating texts form HTML tags, recognizing sentence boundary, and complementing omitted HTML tags etc. 2." ></td>
	<td class="line x" title="111:288	Opinion sentence extraction Opinion sentences are extracted from HTML documents by using the itemization, table and linguistic pattern." ></td>
	<td class="line x" title="112:288	3." ></td>
	<td class="line x" title="113:288	Filtering Since HTML documents are noisy, some of the extracted opinion sentences are not appropriate." ></td>
	<td class="line x" title="114:288	They are removed in this step." ></td>
	<td class="line x" title="115:288	For the preprocessing, we implemented simple rule-based system." ></td>
	<td class="line x" title="116:288	We cannot explain its detail for lack of space." ></td>
	<td class="line x" title="117:288	In the remainder of this Section, we describe three extraction methods respectively, and then examine filtering technique." ></td>
	<td class="line x" title="118:288	4.1 Extraction based on itemization The first method utilizes the itemization." ></td>
	<td class="line x" title="119:288	In order to extract opinion sentences, first of all, we have to find such itemization as illustrated in Figure 1." ></td>
	<td class="line x" title="120:288	They are detected by using indicator lexicon and HTML tags such as BOh1BQ and BOulBQ etc. After finding the itemizations, the sentences in the items are extracted as opinion sentences." ></td>
	<td class="line x" title="121:288	Their polarity labels are assigned according to whether the header is positive or negative indicator." ></td>
	<td class="line x" title="122:288	From the itemization in Figure 1, three positive sentences and three negative ones are extracted." ></td>
	<td class="line x" title="123:288	The problem here is how to treat such item that has more than one sentences (Figure 4)." ></td>
	<td class="line x" title="124:288	In this itemization, there are two sentences in each of the 454 (1) kono software-no riten-ha hayaku ugoku koto this software-POST advantage-POST quickly run to The advantage of this software is to run quickly." ></td>
	<td class="line x" title="125:288	(2) ketten-ha jikan-ga kakarisugiru koto-desu weakness-POST time-POST take too much to-POST The weakness is to take too much time." ></td>
	<td class="line x" title="126:288	Figure 3: Instances of the linguistic pattern." ></td>
	<td class="line x" title="127:288	third and fourth item." ></td>
	<td class="line x" title="128:288	It is hard to precisely predict the polarity of each sentence in such items, because such item sometimes includes both positive and negative sentences." ></td>
	<td class="line x" title="129:288	For example, in the third item of the Figure, there are two sentences." ></td>
	<td class="line x" title="130:288	One (Has high pixel) is positive and the other (I was not satisfied) is negative." ></td>
	<td class="line x" title="131:288	To get around this problem, we did not use such items." ></td>
	<td class="line x" title="132:288	From the itemization in Figure 4, only two positive sentences are extracted (the color is really good and this camera makes me happy while taking pictures)." ></td>
	<td class="line x" title="133:288	Pros: AF The color is really good." ></td>
	<td class="line x" title="134:288	AF This camera makes me happy while taking pictures." ></td>
	<td class="line x" title="135:288	AF Has high pixel resolution with 4 million pixels." ></td>
	<td class="line x" title="136:288	I was not satisfied with 2 million." ></td>
	<td class="line x" title="137:288	AF EVF is easy to see." ></td>
	<td class="line x" title="138:288	But, compared with SLR, its hard to see." ></td>
	<td class="line x" title="139:288	Figure 4: Itemization where more than one sentences are written in one item." ></td>
	<td class="line x" title="140:288	4.2 Extraction based on table The second method extracts opinion sentences from the table." ></td>
	<td class="line x" title="141:288	Since the combination of BOtableBQ and other tags can represent various kinds of tables, it is difficult to craft precise rules that can deal with any table." ></td>
	<td class="line x" title="142:288	Therefore, we consider only two types of tables in which opinion sentences are described (Figure 5)." ></td>
	<td class="line x" title="143:288	Type A is a table in which the leftmost column acts as a header, and there are indicators in that column." ></td>
	<td class="line x" title="144:288	Similarly, type B is a table in which the first row acts as a header." ></td>
	<td class="line x" title="145:288	The table illustrated in Figure 2 is categorized into type A. The type of the table is decided as follows." ></td>
	<td class="line x" title="146:288	The table is categorized into type A if there are both type A C1 B7 B7 B7 B7 C1 A0 A0 A0 A0 type B yC1 B7 C1 A0 y yB7 A0y yB7 A0y yB7 A0y C1 B7 :positive indicator B7:positive sentence C1 A0 :negative indicator A0:negative sentence Figure 5: Two types of tables." ></td>
	<td class="line x" title="147:288	positive and negative indicators in the leftmost column." ></td>
	<td class="line x" title="148:288	The table is categorized into type B if it is not type A and there are both positive and negative indicators in the first row." ></td>
	<td class="line x" title="149:288	After the type of the table is decided, we can extract opinion sentences from the cells that correspond to B7 and A0 in the Figure 5." ></td>
	<td class="line x" title="150:288	It is obvious which label (positive or negative) should be assigned to the extracted sentence." ></td>
	<td class="line x" title="151:288	We did not use such cell that contains more than one sentences, because it is difficult to reliably predict the polarity of each sentence." ></td>
	<td class="line x" title="152:288	This is similar to the extraction from the itemization." ></td>
	<td class="line x" title="153:288	4.3 Extraction based on linguistic pattern The third method uses linguistic pattern." ></td>
	<td class="line x" title="154:288	The characteristic of this pattern is that it takes dependency structure into consideration." ></td>
	<td class="line x" title="155:288	First of all, we explain Japanese dependency structure." ></td>
	<td class="line x" title="156:288	Figure 6 depicts the dependency representations of the sentences in the Figure 3." ></td>
	<td class="line x" title="157:288	Japanese sentence is represented by a set of dependencies between phrasal units called bunsetsuphrases." ></td>
	<td class="line x" title="158:288	Broadly speaking, bunsetsu-phrase is an unit similar to baseNP in English." ></td>
	<td class="line x" title="159:288	In the Figure, square brackets enclose bunsetsu-phrase and arrows show modifier AX head dependencies between bunsetsu-phrases." ></td>
	<td class="line x" title="160:288	In order to extract opinion sentences from these dependency representations, we crafted the following dependency pattern." ></td>
	<td class="line x" title="161:288	455 [ kono this ][ software-no software-POST ][ riten-ha advantage-POST ][ hayaku quickly ][ ugoku run ][ koto to ] [ ketten-ha weakness-POST ][ jikan-ga time-POST ][ kakari sugiru take too much ][ koto-desu to-POST ] Figure 6: Dependency representations." ></td>
	<td class="line x" title="162:288	[ INDICATOR-ha ][koto-POST*] This pattern matches the singly underlined bunsetsu-phrases in the Figure 6." ></td>
	<td class="line x" title="163:288	In the modifier part of this pattern, the indicator is followed by postpositional particle ha, which is topic marker 1." ></td>
	<td class="line x" title="164:288	In the head part, koto (to) is followed by arbitrary numbers of postpositional particles." ></td>
	<td class="line x" title="165:288	If we find the dependency that matches this pattern, a phrase between the two bunsetsu-phrases is extracted as opinion sentence." ></td>
	<td class="line x" title="166:288	In the Figure 6, the doubly underlined phrases are extracted." ></td>
	<td class="line x" title="167:288	This heuristics is based on Japanese word order constraint." ></td>
	<td class="line x" title="168:288	4.4 Filtering Sentences extracted by the above methods sometimes include noise text." ></td>
	<td class="line x" title="169:288	Such texts have to be filtered out." ></td>
	<td class="line x" title="170:288	There are two cases that need filtering process." ></td>
	<td class="line x" title="171:288	First, some of the extracted sentences do not express opinions." ></td>
	<td class="line x" title="172:288	Instead, they represent objects to which the writers opinion is directed (Table 7)." ></td>
	<td class="line x" title="173:288	From this table, the overall shape and the shape of the taillight are wrongly extracted as opinion sentences." ></td>
	<td class="line x" title="174:288	Since most of the objects are noun phrases, we removed such sentences that have the noun as the head." ></td>
	<td class="line x" title="175:288	Mileage(urban) 10.0km/litter Mileage(highway) 12.0km/litter Plus The overall shape." ></td>
	<td class="line x" title="176:288	Minus The shape of the taillight." ></td>
	<td class="line x" title="177:288	Figure 7: A table describing only objects to which the opinion is directed." ></td>
	<td class="line x" title="178:288	Secondly, we have to treat duplicate opinion sentences because there are mirror sites in the 1 To be exact, some of the indicators such as strong point consists of more than one bunsetsu-phrase, and the modifier part sometimes consists of more than one bunsetsu-phrase." ></td>
	<td class="line x" title="179:288	HTML documents." ></td>
	<td class="line x" title="180:288	When there are more than one sentences that are exactly the same, one of them is held and the others are removed." ></td>
	<td class="line x" title="181:288	5 Experimental Results and Discussion This Section examines the results of corpus construction experiment." ></td>
	<td class="line x" title="182:288	To analyze Japanese sentence we used Juman and KNP 2 . 5.1 Corpus Construction About 120 millions HTML documents were processed, and 126,610 opinion sentences were extracted." ></td>
	<td class="line x" title="183:288	Before the filtering, there were 224,002 sentences in our corpus." ></td>
	<td class="line x" title="184:288	Table2 shows the statistics of our corpus." ></td>
	<td class="line x" title="185:288	The first column represents the three extraction methods." ></td>
	<td class="line x" title="186:288	The second and third column shows the number of positive and negative sentences by extracted each method." ></td>
	<td class="line x" title="187:288	Some examples are illustrated in Table 3." ></td>
	<td class="line x" title="188:288	Table 2: # of sentences in the corpus." ></td>
	<td class="line x" title="189:288	Positive Negative Total Itemization 18,575 15,327 33,902 Table 12,103 11,016 23,119 Linguistic Pattern 34,282 35,307 69,589 Total 64,960 61,650 126,610 The result revealed that more than half of the sentences are extracted by linguistic pattern (see the fourth row)." ></td>
	<td class="line x" title="190:288	Our method turned out to be effective even in the case where only plain texts are available." ></td>
	<td class="line x" title="191:288	5.2 Quality assessment In order to check the quality of our corpus, 500 sentences were randomly picked up and two judges manually assessed whether appropriate labels are assigned to the sentences." ></td>
	<td class="line x" title="192:288	The evaluation procedure is the followings." ></td>
	<td class="line x" title="193:288	2 http://www.kc.t.u-tokyo.ac.jp/nl-resource/top.html 456 Table 3: Examples of opinion sentences." ></td>
	<td class="line x" title="194:288	label opinion sentence B7 cost keisan-ga yoininaru cost computation-POST become easy It becomes easy to compute cost." ></td>
	<td class="line x" title="195:288	B7 kantan-de jikan-ga setsuyakudekiru easy-POST time-POST can save Its easy and can save time." ></td>
	<td class="line x" title="196:288	B7 soup-ha koku-ga ari oishii soup-POST rich flavorful The soup is rich and flavorful." ></td>
	<td class="line x" title="197:288	A0 HTML keishiki-no mail-ni taioshitenai HTML format-POST mail-POST cannot use Cannot use mails in HTML format." ></td>
	<td class="line x" title="198:288	A0 jugyo-ga hijoni tsumaranai lecture-POST really boring The lecture is really boring." ></td>
	<td class="line x" title="199:288	A0 kokoro-ni nokoru ongaku-ga nai impressive music-POST there is no There is no impressive music." ></td>
	<td class="line x" title="200:288	AF Each of the 500 sentences are shown to the two judges." ></td>
	<td class="line x" title="201:288	Throughout this evaluation, We did not present the label automatically tagged by our method." ></td>
	<td class="line x" title="202:288	Similarly, we did not show HTML documents from which the opinion sentences are extracted." ></td>
	<td class="line x" title="203:288	AF The two judges individually categorized each sentence into three groups: positive, negative and neutral/ambiguous." ></td>
	<td class="line x" title="204:288	The sentence is classified into the third group, if it does not express opinion (neutral) or if its polarity depends on the context (ambiguous)." ></td>
	<td class="line x" title="205:288	Thus, two goldstandard sets were created." ></td>
	<td class="line x" title="206:288	AF The precision is estimated using the goldstandard." ></td>
	<td class="line x" title="207:288	In this evaluation, the precision refers to the ratio of sentences where correct labels are assigned by our method." ></td>
	<td class="line x" title="208:288	Since we have two goldstandard sets, we can report two different precision values." ></td>
	<td class="line x" title="209:288	A sentence that is categorized into neutral/ambiguous by the judge is interpreted as being assigned incorrect label by our method, since our corpus does not have a label that corresponds to neutral/ambiguous." ></td>
	<td class="line x" title="210:288	We investigated the two goldstandard sets, and found that the judges agree with each other in 467 out of 500 sentences (93.4%)." ></td>
	<td class="line x" title="211:288	The Kappa value was 0.901." ></td>
	<td class="line x" title="212:288	From this result, we can say that the goldstandard was reliably created by the judges." ></td>
	<td class="line x" title="213:288	Then, we estimated the precision." ></td>
	<td class="line x" title="214:288	The precision was 459/500 (91.5%) when one goldstandard was used, and 460/500 (92%) when the other was used." ></td>
	<td class="line x" title="215:288	Since these values are nearly equal to the agreement between humans (467/500), we can conclude that our method successfully constructed polaritytagged corpus." ></td>
	<td class="line x" title="216:288	After the evaluation, we analyzed errors and found that most of them were caused by the lack of context." ></td>
	<td class="line x" title="217:288	The following is a typical example." ></td>
	<td class="line x" title="218:288	You see, there is much information." ></td>
	<td class="line x" title="219:288	In our corpus this sentence is categorized into positive one." ></td>
	<td class="line x" title="220:288	The below is a part of the original document from which this sentence was extracted." ></td>
	<td class="line x" title="221:288	I recommend this guide book." ></td>
	<td class="line x" title="222:288	The Pros." ></td>
	<td class="line x" title="223:288	of this book is that, you see, there is much information." ></td>
	<td class="line x" title="224:288	On the other hand, both of the two judges categorized the above sentence into neutral/ambiguous, probably because they can easily assume context where much information is not desirable." ></td>
	<td class="line x" title="225:288	You see, there is much information." ></td>
	<td class="line x" title="226:288	But, it is not at all arranged, and makes me confused." ></td>
	<td class="line x" title="227:288	In order to precisely treat this kind of sentences, we think discourse analysis is inevitable." ></td>
	<td class="line x" title="228:288	5.3 Application to opinion classification Next, we applied our corpus to opinion sentence classification." ></td>
	<td class="line x" title="229:288	This is a task of classifying sentences into positive and negative." ></td>
	<td class="line x" title="230:288	We trained a classifier on our corpus and investigated the result." ></td>
	<td class="line x" title="231:288	Classifier and data sets As a classifier, we chose Naive Bayes with bag-of-words features, because it is one of the most popular one in this task." ></td>
	<td class="line oc" title="232:288	Negation was processed in a similar way as previous works (Pang et al. , 2002)." ></td>
	<td class="line x" title="233:288	To validate the accuracy of the classifier, three data sets were created from review pages in which the review is associated with meta-data." ></td>
	<td class="line x" title="234:288	To build data sets tagged at sentence level, we used such reviews that contain only one sentence." ></td>
	<td class="line x" title="235:288	Table 4 represents the domains and the number of sentences in each data set." ></td>
	<td class="line x" title="236:288	Note that we confirmed there is no duplicate between our corpus and the these data sets." ></td>
	<td class="line x" title="237:288	The result and discussion Naive Bayes classifier was trained on our corpus and tested on the three data sets (Table 5)." ></td>
	<td class="line x" title="238:288	In the Table, the second column represents the accuracy of the classification in each data set." ></td>
	<td class="line x" title="239:288	The third and fourth 457 Table 5: Classification result." ></td>
	<td class="line x" title="240:288	Accuracy Positive Negative Precision Recall Precision Recall Computer 0.831 0.856 0.804 0.804 0.859 Restaurant 0.849 0.905 0.859 0.759 0.832 Car 0.833 0.860 0.844 0.799 0.819 Table 4: The data sets." ></td>
	<td class="line x" title="241:288	Domain # of sentences Positive Negative Computer 933 910 Restaurant 753 409 Car 1,056 800 columns represent precision and recall of positive sentences." ></td>
	<td class="line x" title="242:288	The remaining two columns show those of negative sentences." ></td>
	<td class="line x" title="243:288	Naive Bayes achieved over 80% accuracy in all the three domains." ></td>
	<td class="line x" title="244:288	In order to compare our corpus with a small domain specific corpus, we estimated accuracy in each data set using 10 fold crossvalidation (Table 6)." ></td>
	<td class="line x" title="245:288	In two domains, the result of our corpus outperformed that of the crossvalidation." ></td>
	<td class="line x" title="246:288	In the other domain, our corpus is slightly better than the crossvalidation." ></td>
	<td class="line x" title="247:288	Table 6: Accuracy comparison." ></td>
	<td class="line x" title="248:288	Our corpus Crossvalidation Computer 0.831 0.821 Restaurant 0.849 0.848 Car 0.833 0.808 One finding is that our corpus achieved good accuracy, although it includes various domains and is not accustomed to the target domain." ></td>
	<td class="line x" title="249:288	Turney also reported good result without domain customization (Turney, 2002)." ></td>
	<td class="line x" title="250:288	We think these results can be further improved by domain adaptation technique, and it is one future work." ></td>
	<td class="line x" title="251:288	Furthermore, we examined the variance of the accuracy between different domains." ></td>
	<td class="line x" title="252:288	We trained Naive Bayes on each data set and investigate the accuracy in the other data sets (Table 7)." ></td>
	<td class="line x" title="253:288	For example, when the classifier is trained on Computer and tested on Restaurant, the accuracy was 0.757." ></td>
	<td class="line x" title="254:288	This result revealed that the accuracy is quite poor when the training and test sets are in different domains." ></td>
	<td class="line x" title="255:288	On the other hand, when Naive Bayes is trained on our corpus, there are little variance in different domains (Table 5)." ></td>
	<td class="line x" title="256:288	This experiment indicates that our corpus is relatively robust against the change of the domain compared with small domain specific corpus." ></td>
	<td class="line x" title="257:288	We think this is because our corpus is large and balanced." ></td>
	<td class="line x" title="258:288	Since we cannot always get domain specific corpus in real application, this is the strength of our corpus." ></td>
	<td class="line x" title="259:288	Table 7: Cross domain evaluation." ></td>
	<td class="line x" title="260:288	Training Computer Restaurant Car Computer  0.701 0.773 Test Restaurant 0.757  0.755 Car 0.751 0.711  6 Related Works 6.1 Learning the polarity of words There are some works that discuss learning the polarity of words instead of sentences." ></td>
	<td class="line x" title="261:288	Hatzivassiloglou and McKeown proposed a method of learning the polarity of adjectives from corpus (Hatzivassiloglou and McKeown, 1997)." ></td>
	<td class="line x" title="262:288	They hypothesized that if two adjectives are connected with conjunctions such as and/but, they have the same/opposite polarity." ></td>
	<td class="line x" title="263:288	Based on this hypothesis, their method predicts the polarity of adjectives by using a small set of adjectives labeled with the polarity." ></td>
	<td class="line x" title="264:288	Other works rely on linguistic resources such as WordNet (Kamps et al. , 2004; Hu and Liu, 2004; Esuli and Sebastiani, 2005; Takamura et al. , 2005)." ></td>
	<td class="line x" title="265:288	For example, Kamps et al. used a graph where nodes correspond to words in the WordNet, and edges connect synonymous words in the WordNet." ></td>
	<td class="line x" title="266:288	The polarity of an adjective is defined by its shortest paths from the node corresponding to good and bad." ></td>
	<td class="line x" title="267:288	Although those researches are closely related to our work, there is a striking difference." ></td>
	<td class="line x" title="268:288	In those researches, the target is limited to the polarity of words and none of them discussed sentences." ></td>
	<td class="line x" title="269:288	In addition, most of the works rely on external resources such as the WordNet, and cannot treat words that are not in the resources." ></td>
	<td class="line x" title="270:288	458 6.2 Learning subjective phrases Some researchers examined the acquisition of subjective phrases." ></td>
	<td class="line x" title="271:288	The subjective phrase is more general concept than opinion and includes both positive and negative expressions." ></td>
	<td class="line x" title="272:288	Wiebe learned subjective adjectives from a set of seed adjectives." ></td>
	<td class="line x" title="273:288	The idea is to automatically identify the synonyms of the seed and to add them to the seed adjectives (Wiebe, 2000)." ></td>
	<td class="line x" title="274:288	Riloff et al. proposed a bootstrapping approach for learning subjective nouns (Riloff et al. , 2003)." ></td>
	<td class="line x" title="275:288	Their method learns subjective nouns and extraction patterns in turn." ></td>
	<td class="line x" title="276:288	First, given seed subjective nouns, the method learns patterns that can extract subjective nouns from corpus." ></td>
	<td class="line x" title="277:288	And then, the patterns extract new subjective nouns from corpus, and they are added to the seed nouns." ></td>
	<td class="line x" title="278:288	Although this work aims at learning only nouns, in the subsequent work, they also proposed a bootstrapping method that can deal with phrases (Riloff and Wiebe, 2003)." ></td>
	<td class="line x" title="279:288	Similarly, Wiebe also proposes a bootstrapping approach to create subjective and objective classifier (Wiebe and Riloff, 2005)." ></td>
	<td class="line x" title="280:288	These works are different from ours in a sense that they did not discuss how to determine the polarity of subjective words or phrases." ></td>
	<td class="line x" title="281:288	6.3 Unsupervised sentiment classification Turney proposed the unsupervised method for sentiment classification (Turney, 2002), and similar method is utilized by many other researchers (Yu and Hatzivassiloglou, 2003)." ></td>
	<td class="line x" title="282:288	The concept behind Turneys model is that positive/negative phrases co-occur with words like excellent/poor." ></td>
	<td class="line x" title="283:288	The cooccurrence statistic is measured by the result of search engine." ></td>
	<td class="line x" title="284:288	Since his method relies on search engine, it is difficult to use rich linguistic information such as dependencies." ></td>
	<td class="line x" title="285:288	7 Conclusion This paper proposed a fully automatic method of building polarity-tagged corpus from HTML documents." ></td>
	<td class="line x" title="286:288	In the experiment, we could build a corpus consisting of 126,610 sentences." ></td>
	<td class="line x" title="287:288	As a future work, we intend to extract more opinion sentences by applying this method to larger HTML document sets and enhancing extraction rules." ></td>
	<td class="line x" title="288:288	Another important direction is to investigate more precise model that can classify or extract opinions, and learn its parameters from our corpus." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-2063
Automatic Identification Of Pro And Con Reasons In Online Reviews
Kim, Soo-Min;Hovy, Eduard H.;"></td>
	<td class="line x" title="1:265	Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 483490, Sydney, July 2006." ></td>
	<td class="line x" title="2:265	c2006 Association for Computational Linguistics Automatic Identification of Pro and Con Reasons in Online Reviews Soo-Min Kim and Eduard Hovy USC Information Sciences Institute 4676 Admiralty Way Marina del Rey, CA 90292-6695 {skim, hovy}@ISI.EDU Abstract In this paper, we present a system that automatically extracts the pros and cons from online reviews." ></td>
	<td class="line x" title="3:265	Although many approaches have been developed for extracting opinions from text, our focus here is on extracting the reasons of the opinions, which may themselves be in the form of either fact or opinion." ></td>
	<td class="line x" title="4:265	Leveraging online review sites with author-generated pros and cons, we propose a system for aligning the pros and cons to their sentences in review texts." ></td>
	<td class="line x" title="5:265	A maximum entropy model is then trained on the resulting labeled set to subsequently extract pros and cons from online review sites that do not explicitly provide them." ></td>
	<td class="line x" title="6:265	Our experimental results show that our resulting system identifies pros and cons with 66% precision and 76% recall." ></td>
	<td class="line x" title="7:265	1 Introduction Many opinions are being expressed on the Web in such settings as product reviews, personal blogs, and news group message boards." ></td>
	<td class="line x" title="8:265	People increasingly participate to express their opinions online." ></td>
	<td class="line x" title="9:265	This trend has raised many interesting and challenging research topics such as subjectivity detection, semantic orientation classification, and review classification." ></td>
	<td class="line x" title="10:265	Subjectivity detection is the task of identifying subjective words, expressions, and sentences." ></td>
	<td class="line x" title="11:265	(Wiebe et al. , 1999; Hatzivassiloglou and Wiebe, 2000; Riloff et al, 2003)." ></td>
	<td class="line x" title="12:265	Identifying subjectivity helps separate opinions from fact, which may be useful in question answering, summarization, etc. Semantic orientation classification is a task of determining positive or negative sentiment of words (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2005)." ></td>
	<td class="line x" title="13:265	Sentiment of phrases and sentences has also been studied in (Kim and Hovy, 2004; Wilson et al. , 2005)." ></td>
	<td class="line oc" title="14:265	Document level sentiment classification is mostly applied to reviews, where systems assign a positive or negative sentiment for a whole review document (Pang et al. , 2002; Turney, 2002)." ></td>
	<td class="line o" title="15:265	Building on this work, more sophisticated problems in the opinion domain have been studied by many researchers." ></td>
	<td class="line x" title="16:265	(Bethard et al. , 2004; Choi et al. , 2005; Kim and Hovy, 2006) identified the holder (source) of opinions expressed in sentences using various techniques." ></td>
	<td class="line x" title="17:265	(Wilson et al. , 2004) focused on the strength of opinion clauses, finding strong and weak opinions." ></td>
	<td class="line x" title="18:265	(Chklovski, 2006) presented a system that aggregates and quantifies degree assessment of opinions scattered throughout web pages." ></td>
	<td class="line x" title="19:265	Beyond document level sentiment classification in online product reviews, (Hu and Liu, 2004; Popescu and Etzioni, 2005) concentrated on mining and summarizing reviews by extracting opinion sentences regarding product features." ></td>
	<td class="line x" title="20:265	In this paper, we focus on another challenging yet critical problem of opinion analysis, identifying reasons for opinions, especially for opinions in online product reviews." ></td>
	<td class="line x" title="21:265	The opinion reason identification problem in online reviews seeks to answer the question What are the reasons that the author of this review likes or dislikes the product? For example, in hotel reviews, information such as found 189 positive reviews and 65 negative reviews may not fully satisfy the information needs of different users." ></td>
	<td class="line x" title="22:265	More useful information would be This hotel is great for families with young infants or Elevators are grouped according to floors, which makes the wait short." ></td>
	<td class="line x" title="23:265	This work differs in important ways from studies in (Hu and Liu, 2004) and (Popescu and Etzioni, 2005)." ></td>
	<td class="line x" title="24:265	These approaches extract features 483 of products and identify sentences that contain opinions about those features by using opinion words and phrases." ></td>
	<td class="line x" title="25:265	Here, we focus on extracting pros and cons which include not only sentences that contain opinion-bearing expressions about products and features but also sentences with reasons why an author of a review writes the review." ></td>
	<td class="line x" title="26:265	Following are examples identified by our system." ></td>
	<td class="line x" title="27:265	It creates duplicate files." ></td>
	<td class="line x" title="28:265	Video drains battery." ></td>
	<td class="line x" title="29:265	It won't play music from all music stores Even though finding reasons in opinionbearing texts is a critical part of in-depth opinion assessment, no study has been done in this particular vein partly because there is no annotated data." ></td>
	<td class="line x" title="30:265	Labeling each sentence is a timeconsuming and costly task." ></td>
	<td class="line x" title="31:265	In this paper, we propose a framework for automatically identifying reasons in online reviews and introduce a novel technique to automatically label training data for this task." ></td>
	<td class="line x" title="32:265	We assume reasons in an online review document are closely related to pros and cons represented in the text." ></td>
	<td class="line x" title="33:265	We leverage the fact that reviews on some websites such as epinions.com already contain pros and cons written by the same author as the reviews." ></td>
	<td class="line x" title="34:265	We use those pros and cons to automatically label sentences in the reviews on which we subsequently train our classification system." ></td>
	<td class="line x" title="35:265	We then apply the resulting system to extract pros and cons from reviews in other websites which do not have specified pros and cons." ></td>
	<td class="line x" title="36:265	This paper is organized as follows: Section 2 describes a definition of reasons in online reviews in terms of pros and cons." ></td>
	<td class="line x" title="37:265	Section 3 presents our approach to identify them and Section 4 explains our automatic data labeling process." ></td>
	<td class="line x" title="38:265	Section 5 describes experimental and results and finally, in Section 6, we conclude with future work." ></td>
	<td class="line x" title="39:265	2 Pros and Cons in Online Reviews This section describes how we define reasons in online reviews for our study." ></td>
	<td class="line x" title="40:265	First, we take a look at how researchers in Computational Linguistics define an opinion for their studies." ></td>
	<td class="line x" title="41:265	It is difficult to define what an opinion means in a computational model because of the difficulty of determining the unit of an opinion." ></td>
	<td class="line x" title="42:265	In general, researchers study opinion at three different levels: word level, sentence level, and document level." ></td>
	<td class="line x" title="43:265	Word level opinion analysis includes word sentiment classification, which views single lexical items (such as good or bad) as sentiment carriers, allowing one to classify words into positive and negative semantic categories." ></td>
	<td class="line x" title="44:265	Studies in sentence level opinion regard the sentence as a minimum unit of opinion." ></td>
	<td class="line x" title="45:265	Researchers try to identify opinion-bearing sentences, classify their sentiment, and identify opinion holders and topics of opinion sentences." ></td>
	<td class="line x" title="46:265	Document level opinion analysis has been mostly applied to review classification, in which a whole document written for a review is judged as carrying either positive or negative sentiment." ></td>
	<td class="line x" title="47:265	Many researchers, however, consider a whole document as the unit of an opinion to be too coarse." ></td>
	<td class="line x" title="48:265	In our study, we take the approach that a review text has a main opinion (recommendation or not) about a given product, but also includes various reasons for recommendation or nonrecommendation, which are valuable to identify." ></td>
	<td class="line x" title="49:265	Therefore, we focus on detecting those reasons in online product review." ></td>
	<td class="line x" title="50:265	We also assume that reasons in a review are closely related to pros and cons expressed in the review." ></td>
	<td class="line x" title="51:265	Pros in a product review are sentences that describe reasons why an author of the review likes the product." ></td>
	<td class="line x" title="52:265	Cons are reasons why the author doesnt like the product." ></td>
	<td class="line x" title="53:265	Based on our observation in online reviews, most reviews have both pros and cons even if sometimes one of them dominates." ></td>
	<td class="line x" title="54:265	3 Finding Pros and Cons This section describes our approach for finding pro and con sentences given a review text." ></td>
	<td class="line x" title="55:265	We first collect data from epinions.com and automatically label each sentences in the data set." ></td>
	<td class="line x" title="56:265	We then model our system using one of the machine learning techniques that have been successfully applied to various problems in Natural Language Processing." ></td>
	<td class="line x" title="57:265	This section also describes features we used for our model." ></td>
	<td class="line x" title="58:265	3.1 Automatically Labeling Pro and Con Sentences Among many web sites that have product reviews such as amazon.com and epinions.com, some of them (e.g. epinions.com) explicitly state pros and cons phrases in their respective categories by each reviews author along with the review text." ></td>
	<td class="line x" title="59:265	First, we collected a large set of <review text, pros, cons> triplets from epin484 ions.com." ></td>
	<td class="line x" title="60:265	A review document in epinions.com consists of a topic (a product model, restaurant name, travel destination, etc.), pros and cons (mostly a few keywords but sometimes complete sentences), and the review text." ></td>
	<td class="line x" title="61:265	Our automatic labeling system first collects phrases in pro and con fields and then searches the main review text in order to collect sentences corresponding to those phrases." ></td>
	<td class="line x" title="62:265	Figure 1 illustrates the automatic labeling process." ></td>
	<td class="line x" title="63:265	Figure 1." ></td>
	<td class="line x" title="64:265	The automatic labeling process of pros and cons sentences in a review." ></td>
	<td class="line x" title="65:265	The system first extracts comma-delimited phrases from each pro and con field, generating two sets of phrases: {P1, P2, , Pn} for pros and {C1, C2, , Cm} for cons." ></td>
	<td class="line x" title="66:265	In the example in Figure 1, beautiful display can be P i and not something you want to drop can be C j. Then the system compares these phrases to the sentences in the text in the Full Review." ></td>
	<td class="line x" title="67:265	For each phrase in {P1, P2, , Pn} and {C1, C2, , Cm}, the system checks each sentence to find a sentence that covers most of the words in the phrase." ></td>
	<td class="line x" title="68:265	Then the system annotates this sentence with the appropriate pro or con label." ></td>
	<td class="line x" title="69:265	All remaining sentences with neither label are marked as neither." ></td>
	<td class="line x" title="70:265	After labeling all the epinion data, we use it to train our pro and con sentence recognition system." ></td>
	<td class="line x" title="71:265	3.2 Modeling with Maximum Entropy Classification We use Maximum Entropy classification for the task of finding pro and con sentences in a given review." ></td>
	<td class="line x" title="72:265	Maximum Entropy classification has been successfully applied in many tasks in natural language processing, such as Semantic Role labeling, Question Answering, and Information Extraction." ></td>
	<td class="line x" title="73:265	Maximum Entropy models implement the intuition that the best model is the one that is consistent with the set of constraints imposed by the evidence but otherwise is as uniform as possible (Berger et al. , 1996)." ></td>
	<td class="line x" title="74:265	We modeled the conditional probability of a class c given a feature vector x as follows: )),(exp( 1 )|(  = i ii x xcf Z xcp  where x Z is a normalization factor which can be calculated by the following:   = ci iix xcfZ )),(exp(  In the first equation, ),( xcf i is a feature function which has a binary value, 0 or 1." ></td>
	<td class="line x" title="75:265	i  is a weight parameter for the feature function ),( xcf i and higher value of the weight indicates that ),( xcf i is an important feature for a class c . For our system development, we used MegaM toolkit 1 which implements the above intuition." ></td>
	<td class="line x" title="76:265	In order to build an efficient model, we separated the task of finding pro and con sentences into two phases, each being a binary classification." ></td>
	<td class="line x" title="77:265	The first is an identification phase and the second is a classification phase." ></td>
	<td class="line x" title="78:265	For this 2-phase model, we defined the 3 classes of c listed in Table 1." ></td>
	<td class="line x" title="79:265	The identification task separates pro and con candidate sentences (CR and PR in Table 1) from sentences irrelevant to either of them (NR)." ></td>
	<td class="line x" title="80:265	The classification task then classifies candidates into pros (PR) and cons (CR)." ></td>
	<td class="line x" title="81:265	Section 5 reports system results of both phases." ></td>
	<td class="line x" title="82:265	1 http://www.isi.edu/~hdaume/megam/index.html Table 1: Classes defined for the classification tasks." ></td>
	<td class="line x" title="83:265	Class symbol Description PR Sentences related to pros in a review CR Sentences related to cons in a review NR Sentences related to neither PR nor CR 485 3.3 Features The classification uses three types of features: lexical features, positional features, and opinionbearing word features." ></td>
	<td class="line x" title="84:265	For lexical features, we use unigrams, bigrams, and trigrams collected from the training set." ></td>
	<td class="line x" title="85:265	They investigate the intuition that there are certain words that are frequently used in pro and con sentences which are likely to represent reasons why an author writes a review." ></td>
	<td class="line x" title="86:265	Examples of such words and phrases are: because and thats why." ></td>
	<td class="line x" title="87:265	For positional features, we first find paragraph boundaries in review texts using html tags such as <br> and <p>." ></td>
	<td class="line x" title="88:265	After finding paragraph boundaries, we add features indicating the first, the second, the last, and the second last sentence in a paragraph." ></td>
	<td class="line x" title="89:265	These features test the intuition used in document summarization that important sentences that contain topics in a text have certain positional patterns in a paragraph (Lin and Hovy, 1997), which may apply because reasons like pros and cons in a review document are most important sentences that summarize the whole point of the review." ></td>
	<td class="line x" title="90:265	For opinion-bearing word features, we used pre-selected opinion-bearing words produced by a combination of two methods." ></td>
	<td class="line x" title="91:265	The first method derived a list of opinion-bearing words from a large news corpus by separating opinion articles such as letters or editorials from news articles which simply reported news or events." ></td>
	<td class="line x" title="92:265	The second method calculated semantic orientations of words based on WordNet 2 synonyms." ></td>
	<td class="line x" title="93:265	In our previous work (Kim and Hovy, 2005), we demonstrated that the list of words produced by a combination of those two methods performed very well in detecting opinion bearing sentences." ></td>
	<td class="line x" title="94:265	Both algorithms are described in that paper." ></td>
	<td class="line x" title="95:265	The motivation for including the list of opinion-bearing words as one of our features is that pro and con sentences are quite likely to contain opinion-bearing expressions (even though some of them are only facts), such as The waiting time was horrible and Their portion size of food was extremely generous! in restaurant reviews." ></td>
	<td class="line x" title="96:265	We presumed pro and con sentences containing only facts, such as The battery lasted 3 hours, not 5 hours like they advertised, would be captured by lexical or positional features." ></td>
	<td class="line x" title="97:265	In Section 5, we report experimental results with different combinations of these features." ></td>
	<td class="line x" title="98:265	2 http://wordnet.princeton.edu/ Table 2 summarizes the features we used for our model and the symbols we will use in the rest of this paper." ></td>
	<td class="line x" title="99:265	4 Data We collected data from two different sources: epinions.com and complaints.com 3 (see Section 3.1 for details about review data in epinion.com)." ></td>
	<td class="line x" title="100:265	Data from epinions.com is mostly used to train the system whereas data from complaints.com is to test how the trained model performs on new data." ></td>
	<td class="line x" title="101:265	Complaints.com includes a large database of publicized consumer complaints about diverse products, services, and companies collected for over 6 years." ></td>
	<td class="line x" title="102:265	Interestingly, reviews in complaint.com are somewhat different from many other web sites which are directly or indirectly linked to Internet shopping malls such as amazon.com and epinions.com." ></td>
	<td class="line x" title="103:265	The purpose of reviews in complaints.com is to share consumers mostly negative experiences and alert businesses to customers feedback." ></td>
	<td class="line x" title="104:265	However, many reviews in Internet shopping mall related reviews are positive and sometimes encourage people to buy more products or to use more services." ></td>
	<td class="line x" title="105:265	Despite its significance, however, there is no hand-annotated data that we can use to build a system to identify reasons of complaints.com." ></td>
	<td class="line x" title="106:265	In order to solve this problem, we assume that reasons in complaints reviews are similar to cons in other reviews and therefore if we are, somehow, able to build a system that can identify cons from 3 http://www.complaints.com/ Table 2: Feature summary." ></td>
	<td class="line x" title="107:265	Feature category Description Symbol Lexical Features unigrams bigrams trigrams Lex Positional Features the first, the second, the last, the second to last sentence in a paragraph Pos Opinionbearing word features pre-selected opinion-bearing words Op 486 reviews, we can apply it to identify reasons in complaints reviews." ></td>
	<td class="line x" title="108:265	Based on this assumption, we learn a system using the data from epinions.com, to which we can apply our automatic data labeling technique, and employ the resulting system to identify reasons from reviews in complaint.com." ></td>
	<td class="line x" title="109:265	The following sections describe each data set." ></td>
	<td class="line x" title="110:265	4.1 Dataset 1: Automatically Labeled Data We collected two different domains of reviews from epinions.com: product reviews and restaurant reviews." ></td>
	<td class="line x" title="111:265	As for the product reviews, we collected 3241 reviews (115029 sentences) about mp3 players made by various manufacturers such as Apple, iRiver, Creative Lab, and Samsung." ></td>
	<td class="line x" title="112:265	We also collected 7524 reviews (194393 sentences) about various types of restaurants such as family restaurants, Mexican restaurants, fast food chains, steak houses, and Asian restaurants." ></td>
	<td class="line x" title="113:265	The average numbers of sentences in a review document are 35.49 and 25.89 respectively." ></td>
	<td class="line x" title="114:265	The purpose of selecting one of electronics products and restaurants as topics of reviews for our study is to test our approach in two extremely different situations." ></td>
	<td class="line x" title="115:265	Reasons why consumers like or dislike a product in electronics reviews are mostly about specific and tangible features." ></td>
	<td class="line x" title="116:265	Also, there are somewhat a fixed set of features of a specific type of product, for example, ease of use, durability, battery life, photo quality, and shutter lag for digital cameras." ></td>
	<td class="line x" title="117:265	Consequently, we can expect that reasons in electronics reviews may share those product feature words and words that describe aspects of features such as short or long for battery life." ></td>
	<td class="line x" title="118:265	This fact might make the reason identification task easy." ></td>
	<td class="line x" title="119:265	On the other hand, restaurant reviewers talk about very diverse aspects and abstract features as reasons." ></td>
	<td class="line x" title="120:265	For example, reasons such as You feel like you are in a train station or a busy amusement park that is ill-staffed to meet demand!, preferential treatment given to large groups, and they don't offer salads of any kind are hard to predict." ></td>
	<td class="line x" title="121:265	Also, they seem rarely share common keyword features." ></td>
	<td class="line x" title="122:265	We first automatically labeled each sentence in those reviews collected from each domain with the features described in Section 3.1." ></td>
	<td class="line x" title="123:265	We divided the data for training and testing." ></td>
	<td class="line x" title="124:265	We then trained our model using the training set and tested it to see if the system can successfully label sentences in the test set." ></td>
	<td class="line x" title="125:265	4.2 Dataset 2: Complaints.com Data From the database 4 in complaints.com, we searched for the same topics of reviews as Dataset 1: 59 complaints reviews about mp3 players and 322 reviews about restaurants 5 . We tested our system on this dataset and compare the results against human judges annotation results." ></td>
	<td class="line x" title="126:265	Subsection 5.2 reports the evaluation results." ></td>
	<td class="line x" title="127:265	5 Experiments and Results We describe two goals in our experiments in this section." ></td>
	<td class="line x" title="128:265	The first is to investigate how well our pro and con detection model with different feature combinations performs on the data we collected from epinions.com." ></td>
	<td class="line x" title="129:265	The second is to see how well the trained model performs on new data from a different source, complaint.com." ></td>
	<td class="line x" title="130:265	For both datasets, we carried out two separate sets of experiments, for the domains of mp3 players and restaurant reviews." ></td>
	<td class="line x" title="131:265	We divided data into 80% for training, 10% for development, and 10% for test for our experiments." ></td>
	<td class="line x" title="132:265	5.1 Experiments on Dataset 1 Identification step: Table 3 and 4 show pros and cons sentences identification results of our system for mp3 player and restaurant reviews respectively." ></td>
	<td class="line x" title="133:265	The first column indicates which combination of features was used for our model (see Table 2 for the meaning of Op, Lex, and Pos feature categories)." ></td>
	<td class="line x" title="134:265	We measure the performance with accuracy (Acc), precision (Prec), recall (Recl), and F-score 6 . The baseline system assigned all sentences as reason and achieved 57.75% and 54.82% of accuracy." ></td>
	<td class="line x" title="135:265	The system performed well when it only used lexical features in mp3 player reviews (76.27% of accuracy in Lex), whereas it performed well with the combination of lexical and opinion features in restaurant reviews (Lex+Op row in Table 4)." ></td>
	<td class="line x" title="136:265	It was very interesting to see that the system achieved a very low score when it only used opinion word features." ></td>
	<td class="line x" title="137:265	We can interpret this phenomenon as supporting our hypothesis that pro and con sentences in reviews are often purely 4 At the time (December 2005), there were total 42593 complaint reviews available in the database." ></td>
	<td class="line x" title="138:265	5 Average numbers of sentences in a complaint is 19.57 for mp3 player reviews and 21.38 for restaurant reviews." ></td>
	<td class="line x" title="139:265	6 We calculated F-score by Recall Precision Recall Precision 2 +  487 factual." ></td>
	<td class="line x" title="140:265	However, opinion features improved both precision and recall when combined with lexical features in restaurant reviews." ></td>
	<td class="line x" title="141:265	It was also interesting that experiments on mp3 players reviews achieved mostly higher scores than restaurants." ></td>
	<td class="line x" title="142:265	Like the observation we described in Subsection 4.1, frequently mentioned keywords of product features (e.g. durability) may have helped performance, especially with lexical features." ></td>
	<td class="line x" title="143:265	Another interesting observation is that the positional features that helped in topic sentence identification did not help much for our task." ></td>
	<td class="line x" title="144:265	Classification step: Tables 5 and 6 show the system results of the pro and con classification task." ></td>
	<td class="line x" title="145:265	The baseline system marked all sentences as pros and achieved 53.87% and 50.71% accuracy for each domain." ></td>
	<td class="line x" title="146:265	All features performed better than the baseline but the results are not as good as in the identification task." ></td>
	<td class="line x" title="147:265	Unlike the identification task, opinion words by themselves achieved the best accuracy in both mp3 player and restaurant domains." ></td>
	<td class="line x" title="148:265	We think opinion words played more important roles in classifying pros and cons than identifying them." ></td>
	<td class="line x" title="149:265	Position features helped recognizing con sentences in mp3 player reviews." ></td>
	<td class="line x" title="150:265	5.2 Experiments on Dataset 2 This subsection reports the evaluation results of our system on Dataset 2." ></td>
	<td class="line x" title="151:265	Since Dataset 2 from complaints.com has no training data, we trained a system on Dataset 1 and applied it to Dataset 2." ></td>
	<td class="line x" title="152:265	Table 3: Pros and cons sentences identification results on mp3 player reviews." ></td>
	<td class="line x" title="153:265	Features used Acc (%) Prec (%) Recl (%) F-score (%) Op 60.15 65.84 57.31 61.28 Lex 76.27 66.18 76.42 70.93 Lex+Pos 63.10 71.14 60.72 65.52 Lex+Op 62.75 70.64 60.07 64.93 Lex+Pos+Op 62.23 70.58 59.35 64.48 Baseline 57.75 Table 4: Reason sentence identification results on restaurant reviews." ></td>
	<td class="line x" title="154:265	Features used Acc (%) Prec (%) Recl (%) F-score (%) Op 61.64 60.76 47.48 53.31 Lex 63.77 67.10 51.20 58.08 Lex+Pos 63.89 67.62 51.70 58.60 Lex+Op 61.66 69.13 54.30 60.83 Lex+Pos+Op 63.13 66.80 50.41 57.46 Baseline 54.82 Table 5: Pros and cons sentences classification results for mp3 player reviews." ></td>
	<td class="line x" title="155:265	Cons Pros Features used Acc (%) Prec (%) Recl (%) F-score (%) Prec (%) Recl (%) F-score (%) Op 57.18 54.43 67.10 60.10 61.18 48.00 53.80 Lex 55.88 55.49 67.45 60.89 56.52 43.88 49.40 Lex+Pos 55.62 55.26 68.12 61.02 56.24 42.62 48.49 Lex+Op 55.60 55.46 64.63 59.70 55.81 46.26 50.59 Lex+Pos+Op 56.68 56.70 62.45 59.44 56.65 50.71 53.52 baseline 53.87 (mark all as pros) Table 6: Pros and cons sentences classification results for restaurant reviews." ></td>
	<td class="line x" title="156:265	Cons Pros Features used Acc (%) Prec (%) Recl (%) F-score (%) Prec (%) Recl (%) F-score (%) Op 57.32 54.78 51.62 53.15 59.32 62.35 60.80 Lex 55.76 55.94 52.52 54.18 55.60 58.97 57.24 Lex+Pos 56.07 56.20 53.33 54.73 55.94 58.78 57.33 Lex+Op 55.88 56.10 52.39 54.18 55.68 59.34 57.45 Lex+Pos+Op 55.79 55.89 53.17 54.50 55.70 58.38 57.01 baseline 50.71 (mark all as pros) 488 A tough question, however, is how to evaluate the system results." ></td>
	<td class="line x" title="157:265	Since it seemed impossible to evaluate the system without involving a human judge, we annotated a small set of data manually for evaluation purposes." ></td>
	<td class="line x" title="158:265	Gold Standard Annotation: Four humans annotated 3 sets of test sets: Testset 1 with 5 complaints (73 sentences), Testset 2 with 7 complaints (105 sentences), and Testset 3 with 6 complaints (85 sentences)." ></td>
	<td class="line x" title="159:265	Testset 1 and 2 are from mp3 player complaints and Testset 3 is from restaurant reviews." ></td>
	<td class="line x" title="160:265	Annotators marked sentences if they describe specific reasons of the complaint." ></td>
	<td class="line x" title="161:265	Each test set was annotated by 2 humans." ></td>
	<td class="line x" title="162:265	The average pair-wise human agreement was 82.1% 7 . System Performance: Like the human annotators, our system also labeled reason sentences." ></td>
	<td class="line x" title="163:265	Since our goal is to identify reason sentences in complaints, we applied a system modeled as in the identification phase described in Subsection 3.2 instead of the classification phase 8 . Table 7 reports the accuracy, precision, and recall of the system on each test set." ></td>
	<td class="line x" title="164:265	We calculated numbers in each A and B column by assuming each annotators answers separately as a gold standard." ></td>
	<td class="line x" title="165:265	In Table 7, accuracies indicate the agreement between the system and human annotators." ></td>
	<td class="line x" title="166:265	The average accuracy 68.0% is comparable with the pair-wise human agreement 82.1% even if there is still a lot of room for improvement 9 . It was interesting to see that Testset 3, which was from restaurant complaints, achieved higher accuracy and recall than the other test sets from mp3 player complaints, suggesting that it would be interesting to further investigate the performance 7 The kappa value was 0.63." ></td>
	<td class="line x" title="167:265	8 In complaints reviews, we believe that it is more important to identify reason sentences than to classify because most reasons in complaints are likely to be cons." ></td>
	<td class="line x" title="168:265	9 The baseline system which assigned the majority class to each sentence achieved 59.9% of average accuracy." ></td>
	<td class="line x" title="169:265	of reason identification in various other review domains such as travel and beauty products in future work." ></td>
	<td class="line x" title="170:265	Also, even though we were somewhat able to measure reason sentence identification in complaint reviews, we agree that we need more data annotation for more precise evaluation." ></td>
	<td class="line x" title="171:265	Finally, the followings are examples of sentences that our system identified as reasons of complaints." ></td>
	<td class="line x" title="172:265	(1) Unfortunately, I find that I am no longer comfortable in your establishment because of the unprofessional, rude, obnoxious, and unsanitary treatment from the employees." ></td>
	<td class="line x" title="173:265	(2) They never get my order right the first time and what really disgusts me is how they handle the food." ></td>
	<td class="line x" title="174:265	(3) The kids play area at Braum's in The Colony, Texas is very dirty." ></td>
	<td class="line x" title="175:265	(4) The only complaint that I have is that the French fries are usually cold." ></td>
	<td class="line x" title="176:265	(5) The cashier there had short changed me on the payment of my bill." ></td>
	<td class="line x" title="177:265	As we can see from the examples, our system was able to detect con sentences which contained opinion-bearing expressions such as in (1), (2), and (3) as well as reason sentences that mostly described mere facts as in (4) and (5)." ></td>
	<td class="line x" title="178:265	6 Conclusions and Future work This paper proposes a framework for identifying one of the critical elements of online product reviews to answer the question, What are reasons that the author of a review likes or dislikes the product? We believe that pro and con sentences in reviews can be answers for this question." ></td>
	<td class="line x" title="179:265	We present a novel technique that automatically labels a large set of pro and con sentences in online reviews using clue phrases for pros and cons in epinions.com in order to train our system." ></td>
	<td class="line x" title="180:265	We applied it to label sentences both on epinions.com and complaints.com." ></td>
	<td class="line x" title="181:265	To investigate the reliability of our system, we tested it on two extremely different review domains, mp3 player reviews and restaurant reviews." ></td>
	<td class="line x" title="182:265	Our system with the best feature selection performs 71% F-score in the reason identification task and 61% F-score in the reason classification task." ></td>
	<td class="line x" title="183:265	Table 7: System results on Complaint.com reviews (A, B: The first and the second annotator of each set) Testset 1 Testset 2 Testset 3 A B A B A B Avg Acc(%) 65.8 63.0 67.6 61.0 77.6 72.9 68.0 Prec(%) 50.0 60.7 68.6 62.9 67.9 60.7 61.8 Recl(%) 56.0 51.5 51.1 44.0 65.5 58.6 54.5 489 The experimental results further show that pro and con sentences are a mixture of opinions and facts, making identifying them in online reviews a distinct problem from opinion sentence identification." ></td>
	<td class="line x" title="184:265	Finally, we also apply the resulting system to another review data in complaints.com in order to analyze reasons of consumers complaints." ></td>
	<td class="line x" title="185:265	In the future, we plan to extend our pro and con identification system on other sorts of opinion texts, such as debates about political and social agenda that we can find on blogs or news group discussions, to analyze why people support a specific agenda and why people are against it." ></td>
	<td class="line x" title="186:265	Reference Berger, Adam L. , Stephen Della Pietra, and Vincent Della Pietra." ></td>
	<td class="line x" title="187:265	1996." ></td>
	<td class="line x" title="188:265	A maximum entropy approach to natural language processing, Computational Linguistics, (22-1)." ></td>
	<td class="line x" title="189:265	Bethard, Steven, Hong Yu, Ashley Thornton, Vasileios Hatzivassiloglou, and Dan Jurafsky." ></td>
	<td class="line x" title="190:265	2004." ></td>
	<td class="line x" title="191:265	Automatic Extraction of Opinion Propositions and their Holders, AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications." ></td>
	<td class="line x" title="192:265	Chklovski, Timothy." ></td>
	<td class="line x" title="193:265	2006." ></td>
	<td class="line x" title="194:265	Deriving Quantitative Overviews of Free Text Assessments on the Web." ></td>
	<td class="line x" title="195:265	Proceedings of 2006 International Conference on Intelligent User Interfaces (IUI06)." ></td>
	<td class="line x" title="196:265	Sydney, Australia." ></td>
	<td class="line x" title="197:265	Choi, Y. , Cardie, C. , Riloff, E. , and Patwardhan, S. 2005." ></td>
	<td class="line x" title="198:265	Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns." ></td>
	<td class="line x" title="199:265	Proceedings of HLT/EMNLP-05." ></td>
	<td class="line x" title="200:265	Esuli, Andrea and Fabrizio Sebastiani." ></td>
	<td class="line x" title="201:265	2005." ></td>
	<td class="line x" title="202:265	Determining the semantic orientation of terms through gloss classification." ></td>
	<td class="line x" title="203:265	Proceedings of CIKM-05, 14th ACM International Conference on Information and Knowledge Management, Bremen, DE, pp." ></td>
	<td class="line x" title="204:265	617-624." ></td>
	<td class="line x" title="205:265	Hatzivassiloglou, Vasileios and Kathleen McKeown." ></td>
	<td class="line x" title="206:265	1997." ></td>
	<td class="line x" title="207:265	Predicting the Semantic Orientation of Adjectives." ></td>
	<td class="line x" title="208:265	Proceedings of 35th Annual Meeting of the Assoc." ></td>
	<td class="line x" title="209:265	for Computational Linguistics (ACL-97): 174-181 Hatzivassiloglou, Vasileios and Janyce Wiebe." ></td>
	<td class="line x" title="210:265	2000." ></td>
	<td class="line x" title="211:265	Effects of Adjective Orientation and Gradability on Sentence Subjectivity." ></td>
	<td class="line x" title="212:265	Proceedings of International Conference on Computational Linguistics (COLING-2000)." ></td>
	<td class="line x" title="213:265	Saarbrcken, Germany." ></td>
	<td class="line x" title="214:265	Hu, Minqing and Bing Liu." ></td>
	<td class="line x" title="215:265	2004." ></td>
	<td class="line x" title="216:265	Mining and summarizing customer reviews'." ></td>
	<td class="line x" title="217:265	Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD2004), Seattle, Washington, USA." ></td>
	<td class="line x" title="218:265	Kim, Soo-Min and Eduard Hovy." ></td>
	<td class="line x" title="219:265	2004." ></td>
	<td class="line x" title="220:265	Determining the Sentiment of Opinions." ></td>
	<td class="line x" title="221:265	Proceedings of COLING-04." ></td>
	<td class="line x" title="222:265	pp." ></td>
	<td class="line x" title="223:265	1367-1373." ></td>
	<td class="line x" title="224:265	Geneva, Switzerland." ></td>
	<td class="line x" title="225:265	Kim, Soo-Min and Eduard Hovy." ></td>
	<td class="line x" title="226:265	2005." ></td>
	<td class="line x" title="227:265	Automatic Detection of Opinion Bearing Words and Sentences." ></td>
	<td class="line x" title="228:265	In the Companion Volume of the Proceedings of IJCNLP-05, Jeju Island, Republic of Korea." ></td>
	<td class="line x" title="229:265	Kim, Soo-Min and Eduard Hovy." ></td>
	<td class="line x" title="230:265	2006." ></td>
	<td class="line x" title="231:265	Identifying and Analyzing Judgment Opinions." ></td>
	<td class="line x" title="232:265	Proceedings of HLT/NAACL-2006, New York City, NY." ></td>
	<td class="line x" title="233:265	Lin, Chin-Yew and Eduard Hovy." ></td>
	<td class="line x" title="234:265	1997." ></td>
	<td class="line x" title="235:265	Identifying Topics by Position." ></td>
	<td class="line x" title="236:265	Proceedings of the 5th Conference on Applied Natural Language Processing (ANLP97)." ></td>
	<td class="line x" title="237:265	Washington, D.C. Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan." ></td>
	<td class="line x" title="238:265	2002." ></td>
	<td class="line x" title="239:265	Thumbs up?" ></td>
	<td class="line x" title="240:265	Sentiment Classification using Machine Learning Techniques, Proceedings of EMNLP 2002." ></td>
	<td class="line x" title="241:265	Popescu, Ana-Maria, and Oren Etzioni." ></td>
	<td class="line x" title="242:265	2005." ></td>
	<td class="line x" title="243:265	Extracting Product Features and Opinions from Reviews, Proceedings of HLT-EMNLP 2005." ></td>
	<td class="line x" title="244:265	Riloff, Ellen, Janyce Wiebe, and Theresa Wilson." ></td>
	<td class="line x" title="245:265	2003." ></td>
	<td class="line x" title="246:265	Learning Subjective Nouns Using Extraction Pattern Bootstrapping." ></td>
	<td class="line x" title="247:265	Proceedings of Seventh Conference on Natural Language Learning (CoNLL-03)." ></td>
	<td class="line x" title="248:265	ACL SIGNLL." ></td>
	<td class="line x" title="249:265	Pages 25-32." ></td>
	<td class="line x" title="250:265	Turney, Peter D. 2002." ></td>
	<td class="line x" title="251:265	Thumbs up or thumbs down?" ></td>
	<td class="line x" title="252:265	Semantic orientation applied to unsupervised classification of reviews, Proceedings of ACL-02, Philadelphia, Pennsylvania, 417-424 Wiebe, Janyce M. , Bruce, Rebecca F. , and O'Hara, Thomas P. 1999." ></td>
	<td class="line x" title="253:265	Development and use of a gold standard data set for subjectivity classifications." ></td>
	<td class="line x" title="254:265	Proceedings of ACL-99." ></td>
	<td class="line x" title="255:265	University of Maryland, June, pp." ></td>
	<td class="line x" title="256:265	246-253." ></td>
	<td class="line x" title="257:265	Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann." ></td>
	<td class="line x" title="258:265	2005." ></td>
	<td class="line x" title="259:265	Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis." ></td>
	<td class="line x" title="260:265	Proceedings of HLT/EMNLP 2005, Vancouver, Canada Wilson, Theresa, Janyce Wiebe, and Rebecca Hwa." ></td>
	<td class="line x" title="261:265	2004." ></td>
	<td class="line x" title="262:265	Just how mad are you?" ></td>
	<td class="line x" title="263:265	Finding strong and weak opinion clauses." ></td>
	<td class="line x" title="264:265	Proceedings of 19th National Conference on Artificial Intelligence (AAAI-2004)." ></td>
	<td class="line x" title="265:265	490" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-2079
Examining The Role Of Linguistic Knowledge Sources In The Automatic Identification And Classification Of Reviews
Ng, Vincent;Dasgupta, Sajib;Arifin, S. M. Niaz;"></td>
	<td class="line x" title="1:254	Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 611618, Sydney, July 2006." ></td>
	<td class="line x" title="2:254	c2006 Association for Computational Linguistics Examining the Role of Linguistic Knowledge Sources in the Automatic Identification and Classification of Reviews Vincent Ng and Sajib Dasgupta and S. M. Niaz Arifin Human Language Technology Research Institute University of Texas at Dallas Richardson, TX 75083-0688 {vince,sajib,arif}@hlt.utdallas.edu Abstract This paper examines two problems in document-level sentiment analysis: (1) determining whether a given document is a review or not, and (2) classifying the polarity of a review as positive or negative." ></td>
	<td class="line x" title="3:254	We first demonstrate that review identification can be performed with high accuracy using only unigrams as features." ></td>
	<td class="line x" title="4:254	We then examine the role of four types of simple linguistic knowledge sources in a polarity classification system." ></td>
	<td class="line x" title="5:254	1 Introduction Sentiment analysis involves the identification of positive and negative opinions from a text segment." ></td>
	<td class="line x" title="6:254	The task has recently received a lot of attention, with applications ranging from multiperspective question-answering (e.g. , Cardie et al.(2004)) to opinion-oriented information extraction (e.g. , Riloff et al.(2005)) and summarization (e.g. , Hu and Liu (2004))." ></td>
	<td class="line x" title="9:254	Research in sentiment analysis has generally proceeded at three levels, aiming to identify and classify opinions from documents, sentences, and phrases." ></td>
	<td class="line x" title="10:254	This paper examines two problems in document-level sentiment analysis, focusing on analyzing a particular type of opinionated documents: reviews." ></td>
	<td class="line x" title="11:254	The first problem, polarity classification, has the goal of determining a reviews polarity positive ( thumbs up ) or negative ( thumbs down )." ></td>
	<td class="line x" title="12:254	Recent work has expanded the polarity classification task to additionally handle documents expressing a neutral sentiment." ></td>
	<td class="line x" title="13:254	Although studied fairly extensively, polarity classification remains a challenge to natural language processing systems." ></td>
	<td class="line x" title="14:254	We will focus on an important linguistic aspect of polarity classification: examining the role of a variety of simple, yet under-investigated, linguistic knowledge sources in a learning-based polarity classification system." ></td>
	<td class="line x" title="15:254	Specifically, we will show how to build a high-performing polarity classifier by exploiting information provided by (1) high order n-grams, (2) a lexicon composed of adjectives manually annotated with their polarity information (e.g. , happy is annotated as positive and terrible as negative), (3) dependency relations derived from dependency parses, and (4) objective terms and phrases extracted from neutral documents." ></td>
	<td class="line x" title="16:254	As mentioned above, the majority of work on document-level sentiment analysis to date has focused on polarity classification, assuming as input a set of reviews to be classified." ></td>
	<td class="line x" title="17:254	A relevant question is: what if we dont know that an input document is a review in the first place?" ></td>
	<td class="line x" title="18:254	The second task we will examine in this paper review identification attempts to address this question." ></td>
	<td class="line x" title="19:254	Specifically, review identification seeks to determine whether a given document is a review or not." ></td>
	<td class="line x" title="20:254	We view both review identification and polarity classification as a classification task." ></td>
	<td class="line x" title="21:254	For review identification, we train a classifier to distinguish movie reviews and movie-related nonreviews (e.g. , movie ads, plot summaries) using only unigrams as features, obtaining an accuracy of over 99% via 10-fold cross-validation." ></td>
	<td class="line x" title="22:254	Similar experiments using documents from the book domain also yield an accuracy as high as 97%." ></td>
	<td class="line x" title="23:254	An analysis of the results reveals that the high accuracy can be attributed to the difference in the vocabulary employed in reviews and non-reviews: while reviews can be composed of a mixture of subjective and objective language, our non-review documents rarely contain subjective expressions." ></td>
	<td class="line x" title="24:254	Next, we learn our polarity classifier using positive and negative reviews taken from two movie 611 review datasets, one assembled by Pang and Lee (2004) and the other by ourselves." ></td>
	<td class="line x" title="25:254	The resulting classifier, when trained on a feature set derived from the four types of linguistic knowledge sources mentioned above, achieves a 10-fold cross-validation accuracy of 90.5% and 86.1% on Pang et al.s dataset and ours, respectively." ></td>
	<td class="line x" title="26:254	To our knowledge, our result on Pang et al.s dataset is one of the best reported to date." ></td>
	<td class="line x" title="27:254	Perhaps more importantly, an analysis of these results show that the various types of features interact in an interesting manner, allowing us to draw conclusions that provide new insights into polarity classification." ></td>
	<td class="line x" title="28:254	2 Related Work 2.1 Review Identification As noted in the introduction, while a review can contain both subjective and objective phrases, our non-reviews are essentially factual documents in which subjective expressions can rarely be found." ></td>
	<td class="line x" title="29:254	Hence, review identification can be viewed as an instance of the broader task of classifying whether a document is mostly factual/objective or mostly opinionated/subjective." ></td>
	<td class="line x" title="30:254	There have been attempts on tackling this so-called document-level subjectivity classification task, with very encouraging results (see Yu and Hatzivassiloglou (2003) and Wiebe et al.(2004) for details)." ></td>
	<td class="line oc" title="32:254	2.2 Polarity Classification There is a large body of work on classifying the polarity of a document (e.g. , Pang et al.(2002), Turney (2002)), a sentence (e.g. , Liu et al.(2003), Yu and Hatzivassiloglou (2003), Kim and Hovy (2004), Gamon et al.(2005)), a phrase (e.g. , Wilson et al.(2005)), and a specific object (such as a product) mentioned in a document (e.g. , Morinaga et al.(2002), Yi et al.(2003), Popescu and Etzioni (2005))." ></td>
	<td class="line x" title="39:254	Below we will center our discussion of related work around the four types of features we will explore for polarity classification." ></td>
	<td class="line x" title="40:254	Higher-order n-grams." ></td>
	<td class="line x" title="41:254	While n-grams offer a simple way of capturing context, previous work has rarely explored the use of n-grams as features in a polarity classification system beyond unigrams." ></td>
	<td class="line oc" title="42:254	Two notable exceptions are the work of Dave et al.(2003) and Pang et al.(2002)." ></td>
	<td class="line p" title="45:254	Interestingly, while Dave et al. report good performance on classifying reviews using bigrams or trigrams alone, Pang et al. show that bigrams are not useful features for the task, whether they are used in isolation or in conjunction with unigrams." ></td>
	<td class="line x" title="46:254	This motivates us to take a closer look at the utility of higher-order n-grams in polarity classification." ></td>
	<td class="line x" title="47:254	Manually-tagged term polarity." ></td>
	<td class="line x" title="48:254	Much work has been performed on learning to identify and classify polarity terms (i.e. , terms expressing a positive sentiment (e.g. , happy) or a negative sentiment (e.g. , terrible)) and exploiting them to do polarity classification (e.g. , Hatzivassiloglou and McKeown (1997), Turney (2002), Kim and Hovy (2004), Whitelaw et al.(2005), Esuli and Sebastiani (2005))." ></td>
	<td class="line x" title="50:254	Though reasonably successful, these (semi-)automatic techniques often yield lexicons that have either high coverage/low precision or low coverage/high precision." ></td>
	<td class="line x" title="51:254	While manually constructed positive and negative word lists exist (e.g. , General Inquirer1), they too suffer from the problem of having low coverage." ></td>
	<td class="line x" title="52:254	This prompts us to manually construct our own polarity word lists2 and study their use in polarity classification." ></td>
	<td class="line x" title="53:254	Dependency relations." ></td>
	<td class="line x" title="54:254	There have been several attempts at extracting features for polarity classification from dependency parses, but most focus on extracting specific types of information such as adjective-noun relations (e.g. , Dave et al.(2003), Yi et al.(2003)) or nouns that enjoy a dependency relation with a polarity term (e.g. , Popescu and Etzioni (2005))." ></td>
	<td class="line x" title="57:254	Wilson et al.(2005) extract a larger variety of features from dependency parses, but unlike us, their goal is to determine the polarity of a phrase, not a document." ></td>
	<td class="line x" title="59:254	In comparison to previous work, we investigate the use of a larger set of dependency relations for classifying reviews." ></td>
	<td class="line x" title="60:254	Objective information." ></td>
	<td class="line x" title="61:254	The objective portions of a review do not contain the authors opinion; hence features extracted from objective sentences and phrases are irrelevant with respect to the polarity classification task and their presence may complicate the learning task." ></td>
	<td class="line x" title="62:254	Indeed, recent work has shown that benefits can be made by first separating facts from opinions in a document (e.g, Yu and Hatzivassiloglou (2003)) and classifying the polarity based solely on the subjective portions of the document (e.g. , Pang and Lee (2004))." ></td>
	<td class="line x" title="63:254	Motivated by the work of Koppel and Schler (2005), we identify and extract objective material from nonreviews and show how to exploit such information in polarity classification." ></td>
	<td class="line x" title="64:254	1http://www.wjh.harvard.edu/inquirer/ spreadsheet guid.htm 2Wilson et al.(2005) have also manually tagged a list of terms with their polarity, but this list is not publicly available." ></td>
	<td class="line x" title="66:254	612 Finally, previous work has also investigated features that do not fall into any of the above categories." ></td>
	<td class="line x" title="67:254	For instance, instead of representing the polarity of a term using a binary value, Mullen and Collier (2004) use Turneys (2002) method to assign a real value to represent term polarity and introduce a variety of numerical features that are aggregate measures of the polarity values of terms selected from the document under consideration." ></td>
	<td class="line x" title="68:254	3 Review Identification Recall that the goal of review identification is to determine whether a given document is a review or not." ></td>
	<td class="line x" title="69:254	Given this definition, two immediate questions come to mind." ></td>
	<td class="line x" title="70:254	First, should this problem be addressed in a domain-specific or domainindependent manner?" ></td>
	<td class="line x" title="71:254	In other words, should a review identification system take as input documents coming from the same domain or not?" ></td>
	<td class="line x" title="72:254	Apparently this is a design question with no definite answer, but our decision is to perform domain-specific review identification." ></td>
	<td class="line x" title="73:254	The reason is that the primary motivation of review identification is the need to identify reviews for further analysis by a polarity classification system." ></td>
	<td class="line x" title="74:254	Since polarity classification has almost exclusively been addressed in a domain-specific fashion, it seems natural that its immediate upstream component review identification should also assume domain specificity." ></td>
	<td class="line x" title="75:254	Note, however, that assuming domain specificity is not a self-imposed limitation." ></td>
	<td class="line x" title="76:254	In fact, we envision that the review identification system will have as its upstream component a text classification system, which will classify documents by topic and pass to the review identifier only those documents that fall within its domain." ></td>
	<td class="line x" title="77:254	Given our choice of domain specificity, the next question is: which documents are non-reviews?" ></td>
	<td class="line x" title="78:254	Here, we adopt a simple and natural definition: a non-review is any document that belongs to the given domain but is not a review." ></td>
	<td class="line x" title="79:254	Dataset." ></td>
	<td class="line x" title="80:254	Now, recall from the introduction that we cast review identification as a classification task." ></td>
	<td class="line x" title="81:254	To train and test our review identifier, we use 2000 reviews and 2000 non-reviews from the movie domain." ></td>
	<td class="line o" title="82:254	The 2000 reviews are taken from Pang et al.s polarity dataset (version 2.0)3, which consists of an equal number of positive and negative reviews." ></td>
	<td class="line x" title="83:254	We collect the non-reviews for the 3Available from http://www.cs.cornell.edu/ people/pabo/movie-review-data." ></td>
	<td class="line x" title="84:254	movie domain from the Internet Movie Database website4, randomly selecting any documents from this site that are on the movie topic but are not reviews themselves." ></td>
	<td class="line x" title="85:254	With this criterion in mind, the 2000 non-review documents we end up with are either movie ads or plot summaries." ></td>
	<td class="line x" title="86:254	Training and testing the review identifier." ></td>
	<td class="line x" title="87:254	We perform 10-fold cross-validation (CV) experiments on the above dataset, using Joachims (1999) SVMlight package5 to train an SVM classifier for distinguishing reviews and non-reviews." ></td>
	<td class="line oc" title="88:254	All learning parameters are set to their default values.6 Each document is first tokenized and downcased, and then represented as a vector of unigrams with length normalization.7 Following Pang et al.(2002), we use frequency as presence." ></td>
	<td class="line x" title="90:254	In other words, the ith element of the document vector is 1 if the corresponding unigram is present in the document and 0 otherwise." ></td>
	<td class="line x" title="91:254	The resulting classifier achieves an accuracy of 99.8%." ></td>
	<td class="line x" title="92:254	Classifying neutral reviews and non-reviews." ></td>
	<td class="line x" title="93:254	Admittedly, the high accuracy achieved using such a simple set of features is somewhat surprising, although it is consistent with previous results on document-level subjectivity classification in which accuracies of 94-97% were obtained (Yu and Hatzivassiloglou, 2003; Wiebe et al. , 2004)." ></td>
	<td class="line x" title="94:254	Before concluding that review classification is an easy task, we conduct an additional experiment: we train a review identifier on a new dataset where we keep the same 2000 non-reviews but replace the positive/negative reviews with 2000 neutral reviews (i.e. , reviews with a mediocre rating)." ></td>
	<td class="line x" title="95:254	Intuitively, a neutral review contains fewer terms with strong polarity than a positive/negative review." ></td>
	<td class="line x" title="96:254	Hence, this additional experiment would allow us to investigate whether the lack of strong polarized terms in neutral reviews would increase the difficulty of the learning task." ></td>
	<td class="line o" title="97:254	Our neutral reviews are randomly chosen from Pang et al.s pool of 27886 unprocessed movie reviews8 that have either a rating of 2 (on a 4-point scale) or 2.5 (on a 5-point scale)." ></td>
	<td class="line x" title="98:254	Each review then undergoes a semi-automatic preprocessing stage 4See http://www.imdb.com." ></td>
	<td class="line x" title="99:254	5Available from svmlight.joachims.org." ></td>
	<td class="line x" title="100:254	6We tried polynomial and RBF kernels, but none yields better performance than the default linear kernel." ></td>
	<td class="line x" title="101:254	7We observed that not performing length normalization hurts performance slightly." ></td>
	<td class="line x" title="102:254	8Also available from Pangs website." ></td>
	<td class="line x" title="103:254	See Footnote 3." ></td>
	<td class="line x" title="104:254	613 where (1) HTML tags and any header and trailer information (such as date and author identity) are removed; (2) the document is tokenized and downcased; (3) the rating information extracted by regular expressions is removed; and (4) the document is manually checked to ensure that the rating information is successfully removed." ></td>
	<td class="line x" title="105:254	When trained on this new dataset, the review identifier also achieves an accuracy of 99.8%, suggesting that this learning task isnt any harder in comparison to the previous one." ></td>
	<td class="line x" title="106:254	Discussion." ></td>
	<td class="line x" title="107:254	We hypothesized that the high accuracies are attributable to the different vocabulary used in reviews and non-reviews." ></td>
	<td class="line x" title="108:254	As part of our verification of this hypothesis, we plot the learning curve for each of the above experiments.9 We observe that a 99% accuracy was achieved in all cases even when only 200 training instances are used to acquire the review identifier." ></td>
	<td class="line x" title="109:254	The ability to separate the two classes with such a small amount of training data seems to imply that features strongly indicative of one or both classes are present." ></td>
	<td class="line x" title="110:254	To test this hypothesis, we examine the informative features for both classes." ></td>
	<td class="line x" title="111:254	To get these informative features, we rank the features by their weighted log-likelihood ratio (WLLR)10: P (wtjcj) log P (wtjcj)P (w tj:cj), where wt and cj denote the tth word in the vocabulary and the jth class, respectively." ></td>
	<td class="line x" title="112:254	Informally, a feature (in our case a unigram) w will have a high rank with respect to a class c if it appears frequently in c and infrequently in other classes." ></td>
	<td class="line x" title="113:254	This correlates reasonably well with what we think an informative feature should be." ></td>
	<td class="line x" title="114:254	A closer examination of the feature lists sorted by WLLR confirms our hypothesis that each of the two classes has its own set of distinguishing features." ></td>
	<td class="line x" title="115:254	Experiments with the book domain." ></td>
	<td class="line x" title="116:254	To understand whether these good review identification results only hold true for the movie domain, we conduct similar experiments with book reviews and non-reviews." ></td>
	<td class="line x" title="117:254	Specifically, we collect 1000 book reviews (consisting of a mixture of positive, negative, and neutral reviews) from the Barnes 9The curves are not shown due to space limitations." ></td>
	<td class="line x" title="118:254	10Nigam et al.(2000) show that this metric is effective at selecting good features for text classification." ></td>
	<td class="line x" title="120:254	Other commonly-used feature selection metrics are discussed in Yang and Pedersen (1997)." ></td>
	<td class="line x" title="121:254	and Noble website11, and 1000 non-reviews that are on the book topic (mostly book summaries) from Amazon.12 We then perform 10-fold CV experiments using these 2000 documents as before, achieving a high accuracy of 96.8%." ></td>
	<td class="line x" title="122:254	These results seem to suggest that automatic review identification can be achieved with high accuracy." ></td>
	<td class="line x" title="123:254	4 Polarity Classification Compared to review identification, polarity classification appears to be a much harder task." ></td>
	<td class="line x" title="124:254	This section examines the role of various linguistic knowledge sources in our learning-based polarity classification system." ></td>
	<td class="line x" title="125:254	4.1 Experimental Setup Like several previous work (e.g. , Mullen and Collier (2004), Pang and Lee (2004), Whitelaw et al.(2005)), we view polarity classification as a supervised learning task." ></td>
	<td class="line x" title="127:254	As in review identification, we use SVMlight with default parameter settings to train polarity classifiers13, reporting all results as 10-fold CV accuracy." ></td>
	<td class="line x" title="128:254	We evaluate our polarity classifiers on two movie review datasets, each of which consists of 1000 positive reviews and 1000 negative reviews." ></td>
	<td class="line x" title="129:254	The first one, which we will refer to as Dataset A, is the Pang et al. polarity dataset (version 2.0)." ></td>
	<td class="line x" title="130:254	The second one (Dataset B) was created by us, with the sole purpose of providing additional experimental results." ></td>
	<td class="line x" title="131:254	Reviews in Dataset B were randomly chosen from Pang et al.s pool of 27886 unprocessed movie reviews (see Section 3) that have either a positive or a negative rating." ></td>
	<td class="line x" title="132:254	We followed exactly Pang et al.s guideline when determining whether a review is positive or negative.14 Also, we took care to ensure that reviews included in Dataset B do not appear in Dataset A. We applied to these reviews the same four pre-processing steps that we did to the neutral reviews in the previous section." ></td>
	<td class="line x" title="133:254	4.2 Results The baseline classifier." ></td>
	<td class="line x" title="134:254	We can now train our baseline polarity classifier on each of the two 11www.barnesandnoble.com 12www.amazon.com 13We also experimented with polynomial and RBF kernels when training polarity classifiers, but neither yields better results than linear kernels." ></td>
	<td class="line x" title="135:254	14The guidelines come with their polarity dataset." ></td>
	<td class="line x" title="136:254	Brie y, a positive review has a rating of  3.5 (out of 5) or  3 (out of 4), whereas a negative review has a rating of 2 (out of 5) or  1.5 (out of 4)." ></td>
	<td class="line x" title="137:254	614 System Variation Dataset A Dataset B Baseline 87.1 82.7 Adding bigrams 89.2 84.7 and trigrams Adding dependency 89.0 84.5 relations Adding polarity 90.4 86.2 info of adjectives Discarding objective 90.5 86.1 materials Table 1: Polarity classification accuracies." ></td>
	<td class="line x" title="138:254	datasets." ></td>
	<td class="line x" title="139:254	Our baseline classifier employs as features the k highest-ranking unigrams according to WLLR, with k/2 features selected from each class." ></td>
	<td class="line x" title="140:254	Results with k = 10000 are shown in row 1 of Table 1.15 As we can see, the baseline achieves an accuracy of 87.1% and 82.7% on Datasets A and B, respectively." ></td>
	<td class="line x" title="141:254	Note that our result on Dataset A is as strong as that obtained by Pang and Lee (2004) via their subjectivity summarization algorithm, which retains only the subjective portions of a document." ></td>
	<td class="line oc" title="142:254	As a sanity check, we duplicated Pang et al.s (2002) baseline in which all unigrams that appear four or more times in the training documents are used as features." ></td>
	<td class="line x" title="143:254	The resulting classifier achieves an accuracy of 87.2% and 82.7% for Datasets A and B, respectively." ></td>
	<td class="line x" title="144:254	Neither of these results are significantly different from our baseline results.16 Adding higher-order n-grams." ></td>
	<td class="line oc" title="145:254	The negative results that Pang et al.(2002) obtained when using bigrams as features for their polarity classifier seem to suggest that high-order n-grams are not useful for polarity classification." ></td>
	<td class="line x" title="147:254	However, recent research in the related (but arguably simpler) task of text classification shows that a bigrambased text classifier outperforms its unigrambased counterpart (Peng et al. , 2003)." ></td>
	<td class="line x" title="148:254	This prompts us to re-examine the utility of high-order n-grams in polarity classification." ></td>
	<td class="line x" title="149:254	In our experiments we consider adding bigrams and trigrams to our baseline feature set." ></td>
	<td class="line x" title="150:254	However, since these higher-order n-grams significantly outnumber the unigrams, adding all of them to the feature set will dramatically increase the dimen15We experimented with several values of k and obtained the best result with k = 10000." ></td>
	<td class="line x" title="151:254	16We use two-tailed paired t-tests when performing significance testing, with p set to 0.05 unless otherwise stated." ></td>
	<td class="line x" title="152:254	sionality of the feature space and may undermine the impact of the unigrams in the resulting classifier." ></td>
	<td class="line x" title="153:254	To avoid this potential problem, we keep the number of unigrams and higher-order n-grams equal." ></td>
	<td class="line x" title="154:254	Specifically, we augment the baseline feature set (consisting of 10000 unigrams) with 5000 bigrams and 5000 trigrams." ></td>
	<td class="line x" title="155:254	The bigrams and trigrams are selected based on their WLLR computed over the positive reviews and negative reviews in the training set for each CV run." ></td>
	<td class="line x" title="156:254	Results using this augmented feature set are shown in row 2 of Table 1." ></td>
	<td class="line x" title="157:254	We see that accuracy rises significantly from 87.1% to 89.2% for Dataset A and from 82.7% to 84.7% for Dataset B. This provides evidence that polarity classification can indeed benefit from higher-order n-grams." ></td>
	<td class="line x" title="158:254	Adding dependency relations." ></td>
	<td class="line x" title="159:254	While bigrams and trigrams are good at capturing local dependencies, dependency relations can be used to capture non-local dependencies among the constituents of a sentence." ></td>
	<td class="line x" title="160:254	Hence, we hypothesized that our ngram-based polarity classifier would benefit from the addition of dependency-based features." ></td>
	<td class="line x" title="161:254	Unlike most previous work on polarity classification, which has largely focused on exploiting adjective-noun (AN) relations (e.g. , Dave et al.(2003), Popescu and Etzioni (2005)), we hypothesized that subject-verb (SV) and verb-object (VO) relations would also be useful for the task." ></td>
	<td class="line x" title="163:254	The following (one-sentence) review illustrates why." ></td>
	<td class="line x" title="164:254	While I really like the actors, the plot is rather uninteresting." ></td>
	<td class="line x" title="165:254	A unigram-based polarity classifier could be confused by the simultaneous presence of the positive term like and the negative term uninteresting when classifying this review." ></td>
	<td class="line x" title="166:254	However, incorporating the VO relation (like, actors) as a feature may allow the learner to learn that the author likes the actors and not necessarily the movie." ></td>
	<td class="line x" title="167:254	In our experiments, the SV, VO and AN relations are extracted from each document by the MINIPAR dependency parser (Lin, 1998)." ></td>
	<td class="line x" title="168:254	As with n-grams, instead of using all the SV, VO and AN relations as features, we select among them the best 5000 according to their WLLR and retrain the polarity classifier with our n-gram-based feature set augmented by these 5000 dependencybased features." ></td>
	<td class="line x" title="169:254	Results in row 3 of Table 1 are somewhat surprising: the addition of dependencybased features does not offer any improvements over the simple n-gram-based classifier." ></td>
	<td class="line x" title="170:254	615 Incorporating manually tagged term polarity." ></td>
	<td class="line x" title="171:254	Next, we consider incorporating a set of features that are computed based on the polarity of adjectives." ></td>
	<td class="line x" title="172:254	As noted before, we desire a high-precision, high-coverage lexicon." ></td>
	<td class="line x" title="173:254	So, instead of exploiting a learned lexicon, we manually develop one." ></td>
	<td class="line x" title="174:254	To construct the lexicon, we take Pang et al.s pool of unprocessed documents (see Section 3), remove those that appear in either Dataset A or Dataset B17, and compile a list of adjectives from the remaining documents." ></td>
	<td class="line x" title="175:254	Then, based on heuristics proposed in psycholinguistics18, we handannotate each adjective with its prior polarity (i.e. , polarity in the absence of context)." ></td>
	<td class="line x" title="176:254	Out of the 45592 adjectives we collected, 3599 were labeled as positive, 3204 as negative, and 38789 as neutral." ></td>
	<td class="line x" title="177:254	A closer look at these adjectives reveals that they are by no means domain-dependent despite the fact that they were taken from movie reviews." ></td>
	<td class="line x" title="178:254	Now let us consider a simple procedure P for deriving a feature set that incorporates information from our lexicon: (1) collect all the bigrams from the training set; (2) for each bigram that contains at least one adjective labeled as positive or negative according to our lexicon, create a new feature that is identical to the bigram except that each adjective is replaced with its polarity label19; (3) merge the list of newly generated features with the list of bigrams20 and select the top 5000 features from the merged list according to their WLLR." ></td>
	<td class="line x" title="179:254	We then repeat procedure P for the trigrams and also the dependency features, resulting in a total of 15000 features." ></td>
	<td class="line x" title="180:254	Our new feature set comprises these 15000 features as well as the 10000 unigrams we used in the previous experiments." ></td>
	<td class="line x" title="181:254	Results of the polarity classifier that incorporates term polarity information are encouraging (see row 4 of Table 1)." ></td>
	<td class="line x" title="182:254	In comparison to the classifier that uses only n-grams and dependency-based features (row 3), accuracy increases significantly (p =.1) from 89.2% to 90.4% for Dataset A, and from 84.7% to 86.2% for Dataset B. These results suggest that the classifier has benefited from the 17We treat the test documents as unseen data that should not be accessed for any purpose during system development." ></td>
	<td class="line x" title="183:254	18http://www.sci.sdsu.edu/CAL/wordlist 19Neutral adjectives are not replaced." ></td>
	<td class="line x" title="184:254	20A newly generated feature could be misleading for the learner if the contextual polarity (i.e. , polarity in the presence of context) of the adjective involved differs from its prior polarity (see Wilson et al.(2005))." ></td>
	<td class="line x" title="186:254	The motivation behind merging with the bigrams is to create a feature set that is more robust in the face of potentially misleading generalizations." ></td>
	<td class="line x" title="187:254	use of features that are less sparse than n-grams." ></td>
	<td class="line x" title="188:254	Using objective information." ></td>
	<td class="line x" title="189:254	Some of the 25000 features we generated above correspond to n-grams or dependency relations that do not contain subjective information." ></td>
	<td class="line x" title="190:254	We hypothesized that not employing these objective features in the feature set would improve system performance." ></td>
	<td class="line x" title="191:254	More specifically, our goal is to use procedure P again to generate 25000 subjective features by ensuring that the objective ones are not selected for incorporation into our feature set." ></td>
	<td class="line x" title="192:254	To achieve this goal, we first use the following rote-learning procedure to identify objective material: (1) extract all unigrams that appear in objective documents, which in our case are the 2000 non-reviews used in review identification [see Section 3]; (2) from these objective unigrams, we take the best 20000 according to their WLLR computed over the non-reviews and the reviews in the training set for each CV run; (3) repeat steps 1 and 2 separately for bigrams, trigrams and dependency relations; (4) merge these four lists to create our 80000-element list of objective material." ></td>
	<td class="line x" title="193:254	Now, we can employ procedure P to get a list of 25000 subjective features by ensuring that those that appear in our 80000-element list are not selected for incorporation into our feature set." ></td>
	<td class="line x" title="194:254	Results of our classifier trained using these subjective features are shown in row 5 of Table 1." ></td>
	<td class="line x" title="195:254	Somewhat surprisingly, in comparison to row 4, we see that our method for filtering objective features does not help improve performance on the two datasets." ></td>
	<td class="line x" title="196:254	We will examine the reasons in the following subsection." ></td>
	<td class="line x" title="197:254	4.3 Discussion and Further Analysis Using the four types of knowledge sources previously described, our polarity classifier significantly outperforms a unigram-based baseline classifier." ></td>
	<td class="line x" title="198:254	In this subsection, we analyze some of these results and conduct additional experiments in an attempt to gain further insight into the polarity classification task." ></td>
	<td class="line x" title="199:254	Due to space limitations, we will simply present results on Dataset A below, and show results on Dataset B only in cases where a different trend is observed." ></td>
	<td class="line x" title="200:254	The role of feature selection." ></td>
	<td class="line x" title="201:254	In all of our experiments we used the best k features obtained via WLLR." ></td>
	<td class="line x" title="202:254	An interesting question is: how will these results change if we do not perform feature selection?" ></td>
	<td class="line x" title="203:254	To investigate this question, we conduct two 616 experiments." ></td>
	<td class="line x" title="204:254	First, we train a polarity classifier using all unigrams from the training set." ></td>
	<td class="line x" title="205:254	Second, we train another polarity classifier using all unigrams, bigrams, and trigrams." ></td>
	<td class="line x" title="206:254	We obtain an accuracy of 87.2% and 79.5% for the first and second experiments, respectively." ></td>
	<td class="line x" title="207:254	In comparison to our baseline classifier, which achieves an accuracy of 87.1%, we can see that using all unigrams does not hurt performance, but performance drops abruptly with the addition of all bigrams and trigrams." ></td>
	<td class="line x" title="208:254	These results suggest that feature selection is critical when bigrams and trigrams are used in conjunction with unigrams for training a polarity classifier." ></td>
	<td class="line x" title="209:254	The role of bigrams and trigrams." ></td>
	<td class="line x" title="210:254	So far we have seen that training a polarity classifier using only unigrams gives us reasonably good, though not outstanding, results." ></td>
	<td class="line x" title="211:254	Our question, then, is: would bigrams alone do a better job at capturing the sentiment of a document than unigrams?" ></td>
	<td class="line x" title="212:254	To answer this question, we train a classifier using all bigrams (without feature selection) and obtain an accuracy of 83.6%, which is significantly worse than that of a unigram-only classifier." ></td>
	<td class="line oc" title="213:254	Similar results were also obtained by Pang et al.(2002)." ></td>
	<td class="line x" title="215:254	It is possible that the worse result is due to the presence of a large number of irrelevant bigrams." ></td>
	<td class="line x" title="216:254	To test this hypothesis, we repeat the above experiment except that we only use the best 10000 bigrams selected according to WLLR." ></td>
	<td class="line x" title="217:254	Interestingly, the resulting classifier gives us a lower accuracy of 82.3%, suggesting that the poor accuracy is not due to the presence of irrelevant bigrams." ></td>
	<td class="line x" title="218:254	To understand why using bigrams alone does not yield a good classification model, we examine a number of test documents and find that the feature vectors corresponding to some of these documents (particularly the short ones) have all zeroes in them." ></td>
	<td class="line x" title="219:254	In other words, none of the bigrams from the training set appears in these reviews." ></td>
	<td class="line x" title="220:254	This suggests that the main problem with the bigram model is likely to be data sparseness." ></td>
	<td class="line x" title="221:254	Additional experiments show that the trigram-only classifier yields even worse results than the bigram-only classifier, probably because of the same reason." ></td>
	<td class="line x" title="222:254	Nevertheless, these higher-order n-grams play a non-trivial role in polarity classification: we have shown that the addition of bigrams and trigrams selected via WLLR to a unigram-based classifier significantly improves its performance." ></td>
	<td class="line x" title="223:254	The role of dependency relations." ></td>
	<td class="line x" title="224:254	In the previous subsection we see that dependency relations do not contribute to overall performance on top of bigrams and trigrams." ></td>
	<td class="line x" title="225:254	There are two plausible reasons." ></td>
	<td class="line x" title="226:254	First, dependency relations are simply not useful for polarity classification." ></td>
	<td class="line x" title="227:254	Second, the higher-order n-grams and the dependency-based features capture essentially the same information and so using either of them would be sufficient." ></td>
	<td class="line x" title="228:254	To test the first hypothesis, we train a classifier using only 10000 unigrams and 10000 dependency-based features (both selected according to WLLR)." ></td>
	<td class="line x" title="229:254	For Dataset A, the classifier achieves an accuracy of 87.1%, which is statistically indistinguishable from our baseline result." ></td>
	<td class="line x" title="230:254	On the other hand, the accuracy for Dataset B is 83.5%, which is significantly better than the corresponding baseline (82.7%) at the p = .1 level." ></td>
	<td class="line x" title="231:254	These results indicate that dependency information is somewhat useful for the task when bigrams and trigrams are not used." ></td>
	<td class="line x" title="232:254	So the first hypothesis is not entirely true." ></td>
	<td class="line x" title="233:254	So, it seems to be the case that the dependency relations do not provide useful knowledge for polarity classification only in the presence of bigrams and trigrams." ></td>
	<td class="line x" title="234:254	This is somewhat surprising, since these n-grams do not capture the non-local dependencies (such as those that may be present in certain SV or VO relations) that should intuitively be useful for polarity classification." ></td>
	<td class="line x" title="235:254	To better understand this issue, we again examine a number of test documents." ></td>
	<td class="line x" title="236:254	Our initial investigation suggests that the problem might have stemmed from the fact that MINIPAR returns dependency relations in which all the verb in ections are removed." ></td>
	<td class="line x" title="237:254	For instance, given the sentence My cousin Paul really likes this long movie, MINIPAR will return the VO relation (like, movie)." ></td>
	<td class="line x" title="238:254	To see why this can be a problem, consider another sentence I like this long movie." ></td>
	<td class="line x" title="239:254	From this sentence, MINIPAR will also extract the VO relation (like, movie)." ></td>
	<td class="line x" title="240:254	Hence, this same VO relation is capturing two different situations, one in which the author himself likes the movie, and in the other, the authors cousin likes the movie." ></td>
	<td class="line x" title="241:254	The overgeneralization resulting from these stemmed relations renders dependency information not useful for polarity classification." ></td>
	<td class="line x" title="242:254	Additional experiments are needed to determine the role of dependency relations when stemming in MINIPAR is disabled." ></td>
	<td class="line x" title="243:254	617 The role of objective information." ></td>
	<td class="line x" title="244:254	Results from the previous subsection suggest that our method for extracting objective materials and removing them from the reviews is not effective in terms of improving performance." ></td>
	<td class="line x" title="245:254	To determine the reason, we examine the n-grams and the dependency relations that are extracted from the nonreviews." ></td>
	<td class="line x" title="246:254	We find that only in a few cases do these extracted objective materials appear in our set of 25000 features obtained in Section 4.2." ></td>
	<td class="line x" title="247:254	This explains why our method is not as effective as we originally thought." ></td>
	<td class="line x" title="248:254	We conjecture that more sophisticated methods would be needed in order to take advantage of objective information in polarity classification (e.g. , Koppel and Schler (2005))." ></td>
	<td class="line x" title="249:254	5 Conclusions We have examined two problems in documentlevel sentiment analysis, namely, review identification and polarity classification." ></td>
	<td class="line x" title="250:254	We first found that review identification can be achieved with very high accuracies (97-99%) simply by training an SVM classifier using unigrams as features." ></td>
	<td class="line x" title="251:254	We then examined the role of several linguistic knowledge sources in polarity classification." ></td>
	<td class="line x" title="252:254	Our results suggested that bigrams and trigrams selected according to the weighted log-likelihood ratio as well as manually tagged term polarity information are very useful features for the task." ></td>
	<td class="line x" title="253:254	On the other hand, no further performance gains are obtained by incorporating dependency-based information or filtering objective materials from the reviews using our proposed method." ></td>
	<td class="line x" title="254:254	Nevertheless, the resulting polarity classifier compares favorably to state-of-the-art sentiment classification systems." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-2081
Whose Thumb Is It Anyway? Classifying Author Personality From Weblog Text
Oberlander, Jon;Nowson, Scott;"></td>
	<td class="line x" title="1:215	Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 627634, Sydney, July 2006." ></td>
	<td class="line x" title="2:215	c2006 Association for Computational Linguistics Whose thumb is it anyway?" ></td>
	<td class="line x" title="3:215	Classifying author personality from weblog text Jon Oberlander School of Informatics University of Edinburgh 2 Buccleuch Place Edinburgh, EH8 9LW j.oberlander@ed.ac.uk Scott Nowson School of Informatics University of Edinburgh 2 Buccleuch Place Edinburgh, EH8 9LW s.nowson@ed.ac.uk Abstract We report initial results on the relatively novel task of automatic classification of author personality." ></td>
	<td class="line x" title="4:215	Using a corpus of personal weblogs, or blogs, we investigate the accuracy that can be achieved when classifying authors on four important personality traits." ></td>
	<td class="line x" title="5:215	We explore both binary and multiple classification, using differing sets of n-gram features." ></td>
	<td class="line x" title="6:215	Results are promising for all four traits examined." ></td>
	<td class="line x" title="7:215	1 Introduction There is now considerable interest in affective language processing." ></td>
	<td class="line oc" title="8:215	Work focusses on analysing subjective features of text or speech, such as sentiment, opinion, emotion or point of view (Pang et al. , 2002; Turney, 2002; Dave et al. , 2003; Liu et al. , 2003; Pang and Lee, 2005; Shanahan et al. , 2005)." ></td>
	<td class="line x" title="9:215	Discussing affective computing in general, Picard (1997) notes that phenomena vary in duration, ranging from short-lived feelings, through emotions, to moods, and ultimately to long-lived, slowly-changing personality characteristics." ></td>
	<td class="line x" title="10:215	Within computational linguistics, most work has focussed on sentiment and opinion concerning specific entities or events, and on binary classifications of these." ></td>
	<td class="line oc" title="11:215	For instance, both Pang and Lee (2002) and Turney (2002) consider the thumbs up/thumbs down decision: is a film review positive or negative?" ></td>
	<td class="line x" title="12:215	However, Pang and Lee (2005) point out that ranking items or comparing reviews will benefit from finer-grained classifications, over multiple ordered classes: is a film review twoor threeor four-star?" ></td>
	<td class="line x" title="13:215	And at the same time, some work now considers longer-term affective states." ></td>
	<td class="line x" title="14:215	For example, Mishne (2005) aims to classify the primary mood of weblog postings; the study encompasses both fine-grained (but non-ordered) multiple classification (frustrated/loved/etc)." ></td>
	<td class="line x" title="15:215	and coarse-grained binary classification (active/passive, positive/negative)." ></td>
	<td class="line x" title="16:215	This paper is about the move to finer-grained multiple classifications; and also about weblogs." ></td>
	<td class="line x" title="17:215	But it is also about even more persistent affective states; in particular, it focusses on classifying author personality." ></td>
	<td class="line x" title="18:215	We would argue that ongoing work on sentiment analysis or opinion-mining stands to benefit from progress on personalityclassification." ></td>
	<td class="line x" title="19:215	The reason is that people vary in personality, and they vary in how they appraise eventsand hence, in how strongly they phrase their praise or condemnation." ></td>
	<td class="line x" title="20:215	Reiter and Sripada (2004) suggest that lexical choice may sometimes be determined by a writers idiolecttheir personal language preferences." ></td>
	<td class="line x" title="21:215	We suggest that while idiolect can be a matter of accident or experience, it may also reflect systematic, personality-based differences." ></td>
	<td class="line x" title="22:215	This can help explain why, as Pang and Lee (2005) note, one persons four star review is anothers two-star." ></td>
	<td class="line x" title="23:215	To put it more bluntly, if youre not a very outgoing sort of person, then your thumbs up might be mistaken for someone elses thumbs down." ></td>
	<td class="line x" title="24:215	But how do we distinguish such people?" ></td>
	<td class="line x" title="25:215	Or, if we spot a thumbs-up review, how can we tell whose thumb it is, anyway?" ></td>
	<td class="line x" title="26:215	The paper is structured as follows." ></td>
	<td class="line x" title="27:215	It introduces trait theories of personality, notes work to date on personality classification, and raises some questions." ></td>
	<td class="line x" title="28:215	It then outlines the weblog corpus and the experiments, which compare classification accuracies for four personality dimensions, seven tasks, and five feature selection policies." ></td>
	<td class="line x" title="29:215	We discuss the implications of the results, and related work, and end with suggestions for next steps." ></td>
	<td class="line x" title="30:215	627 2 Background: traits and language Cattells pioneering work led to the isolation of 16 primary personality factors, and later work on secondary factors led to Costa and McCraes fivefactor model, closely related to the Big Five models emerging from lexical research (Costa and McCrae, 1992)." ></td>
	<td class="line x" title="31:215	Each factor gives a continuous dimension for personality scoring." ></td>
	<td class="line x" title="32:215	These are: Extraversion; Neuroticism; Openness; Agreeableness; and Conscientiousness (Matthews et al. , 2003)." ></td>
	<td class="line x" title="33:215	Work has also investigated whether scores on these dimensions correlate with language use (Scherer, 1979; Dewaele and Furnham, 1999)." ></td>
	<td class="line x" title="34:215	Building on the earlier work of Gottschalk and Gleser, Pennebaker and colleagues secured significant results using the Linguistic Inquiry and Word Count text analysis program (Pennebaker et al. , 2001)." ></td>
	<td class="line x" title="35:215	This primarily counts relative frequencies of word-stems in pre-defined semantic and syntactic categories." ></td>
	<td class="line x" title="36:215	It shows, for instance, that high Neuroticism scorers use: more first person singular and negative emotion words; and fewer articles and positive emotion words (Pennebaker and King, 1999)." ></td>
	<td class="line x" title="37:215	So, can a text classifier trained on such features predict the author personality?" ></td>
	<td class="line x" title="38:215	We know of only one published study: Argamon et al.(2005) focussed on Extraversion and Neuroticism, dividing Pennebaker and Kings (1999) population into the topand bottom-third scorers on a dimension, and discarding the middle third." ></td>
	<td class="line x" title="40:215	For both dimensions, using a restricted feature set, they report binary classification accuracy of around 58%: an 8% absolute improvement over their baseline." ></td>
	<td class="line x" title="41:215	Although mood is more malleable, work on it is also relevant (Mishne, 2005)." ></td>
	<td class="line x" title="42:215	Using a more typical feature set (including n-grams of words and parts-of-speech), the best mood classification accuracy was 66%, for confused." ></td>
	<td class="line x" title="43:215	At a coarser grain, moods could be classified with accuracies of 57% (active vs. passive), and 60% (positive vs. negative)." ></td>
	<td class="line x" title="44:215	So, Argamon et al. used a restricted feature set for binary classification on two dimensions: Extraversion and Neuroticism." ></td>
	<td class="line x" title="45:215	Given this, we now pursue three questions." ></td>
	<td class="line x" title="46:215	(1) Can we improve performance on a similar binary classification task?" ></td>
	<td class="line x" title="47:215	(2) How accurate can classification be on the other dimensions?" ></td>
	<td class="line x" title="48:215	(3) How accurate can multiple three-way or five-wayclassification be?" ></td>
	<td class="line x" title="49:215	3 The weblog corpus 3.1 Construction A corpus of personal weblog (blog) text has been gathered (Nowson, 2006)." ></td>
	<td class="line x" title="50:215	Participants were recruited directly via e-mail to suitable candidates, and indirectly by word-of-mouth: many participants wrote about the study in their blogs." ></td>
	<td class="line x" title="51:215	Participants were first required to answer sociobiographic and personality questionnaires." ></td>
	<td class="line x" title="52:215	The personality instrument has specifically been validated for online completion (Buchanan, 2001)." ></td>
	<td class="line x" title="53:215	It was derived from the 50-item IPIP implementation of Costa and McCraes (1992) revised NEO personality inventory; participants rate themselves on 41items using a 5-point Likert scale." ></td>
	<td class="line x" title="54:215	This provides scores for Neuroticism, Extraversion, Openness, Agreeableness and Conscientiousness." ></td>
	<td class="line x" title="55:215	After completing this stage, participants were requested to submit one months worth of prior weblog postings." ></td>
	<td class="line x" title="56:215	The month was pre-specified so as to reduce the effects of an individual choosing what they considered their best or preferred month." ></td>
	<td class="line x" title="57:215	Raw submissions were marked-up using XML so as to automate extraction of the desired text." ></td>
	<td class="line x" title="58:215	Text was also marked-up by post type, such as purely personal, commentary reporting of external matters, or direct posting of internet memes such as quizzes." ></td>
	<td class="line x" title="59:215	The corpus consisted of 71 participants (47 females, 24 males; average ages 27.8 and 29.4, respectively) and only the text marked as personal from each weblog, approximately 410,000 words." ></td>
	<td class="line x" title="60:215	To eliminate undue influence of particularly verbose individuals, the size of each weblog file was truncated at the mean word count plus 2 standard deviations." ></td>
	<td class="line x" title="61:215	3.2 Personality distribution It might be thought that bloggers are more Extravert than most (because they express themselves in public); or perhaps that they are less Extravert (because they keep diaries in the first place)." ></td>
	<td class="line x" title="62:215	In fact, plotting the Extraversion scores for the corpus authors gives an apparently normal distribution; and the same applies for three other dimensions." ></td>
	<td class="line x" title="63:215	However, scores for Openness to experience are not normally distributed." ></td>
	<td class="line x" title="64:215	Perhaps bloggers are more Open than average; or perhaps there is response bias." ></td>
	<td class="line x" title="65:215	Without a comparison sample of matched non-bloggers, one cannot say, and Openness is not discussed further in this paper." ></td>
	<td class="line x" title="66:215	628 4 Experiments We are thus confined to classifying on four personality dimensions." ></td>
	<td class="line x" title="67:215	However, a number of other variables remain: different learning algorithms can be employed; authors in the corpus can be grouped in several ways, leading to various classification tasks; and more or less restricted linguistic feature sets can be used as input to the classifier." ></td>
	<td class="line x" title="68:215	4.1 Algorithms Support Vector Machines (SVM) appear to work well for binary sentiment classification tasks, so Argamon et al.(2003) and Pang and Lee (2005) consider One-vs-All, or All-vs-All, variants on SVM, to permit multiple classifications." ></td>
	<td class="line x" title="70:215	Choice of algorithm is not our focus, but it remains to be seen whether SVM outperforms Nave Bayes (NB) for personality classification." ></td>
	<td class="line x" title="71:215	Thus, we will use both on the binary Tasks 1 to 3 (defined in section 4.2.1), for each of the personality dimensions, and each of the manually-selected feature sets (Levels I to IV, defined in section 4.3)." ></td>
	<td class="line x" title="72:215	Whichever performs better overall is then reported in full, and used for the multiple Tasks 4 to 7 (defined in section 4.2.2)." ></td>
	<td class="line x" title="73:215	Both approaches are applied as implemented in the WEKA toolkit (Witten and Frank, 1999) and use 10-fold cross validation." ></td>
	<td class="line x" title="74:215	4.2 Tasks For any blog, we have available the scores, on continuous scales, of its author on four personality dimensions." ></td>
	<td class="line x" title="75:215	But for the classifier, the task can be made more or less easy, by grouping authors on each of the dimensions." ></td>
	<td class="line x" title="76:215	The simplest tasks are, of course, binary: given the sequence of words from a blog, the classifier simply has to decide whether the author is (for instance) high or low in Agreeableness." ></td>
	<td class="line x" title="77:215	Binary tasks vary in difficulty, depending on whether authors scoring in the middle of a dimension are left out, or not; and if they are left out, what proportion of authors are left out." ></td>
	<td class="line x" title="78:215	More complex tasks will also vary in difficulty depending on who is left out." ></td>
	<td class="line x" title="79:215	But in the cases considered here, middle authors are now included." ></td>
	<td class="line x" title="80:215	For a three-way task, the classifier must decide if an author is high, medium or low; and those authors known to score between these categories may, or may not, be left out." ></td>
	<td class="line x" title="81:215	In the most challenging five-way task, no-one is left out." ></td>
	<td class="line x" title="82:215	The point of considering such tasks is to gradually approximate the most challenging task of all: continuous rating." ></td>
	<td class="line x" title="83:215	4.2.1 Binary classification tasks In these task variants, the goal is to classify authors as either high or low scorers on a dimension: 1." ></td>
	<td class="line x" title="84:215	The easiest approach is to keep the high and low groups as far apart as possible: high scorers (H) are those whose scores fall above 1 SD above the mean; low scorers (L) are those whose scores fall below 1 SD below the mean." ></td>
	<td class="line x" title="85:215	2." ></td>
	<td class="line x" title="86:215	Task-1 creates distinct groups, at the price of excluding over 50% of the corpus from the analysis." ></td>
	<td class="line x" title="87:215	To include more of the corpus, parameters are relaxed: the high group (HH) includes anyone whose score is above.5 SD above the mean; the low group (LL) is similarly placed below." ></td>
	<td class="line x" title="88:215	3." ></td>
	<td class="line x" title="89:215	The most obvious task (but not the easiest) arises from dividing the corpus in half about the mean score." ></td>
	<td class="line x" title="90:215	This creates high (HHH) and low (LLL) groups, covering the entire population." ></td>
	<td class="line x" title="91:215	Inevitably, some HHH scorers will actually have scores much closer to those of LLL scorers than to other HHH scorers." ></td>
	<td class="line x" title="92:215	These sub-groups are tabulated in Table 1, giving the size of each group within each trait." ></td>
	<td class="line x" title="93:215	Note that in Task-2, the standard-deviation-based divisions contain very nearly the top third and bottom third of the population for each dimension." ></td>
	<td class="line x" title="94:215	Hence, Task-2 is closest in proportion to the division by thirds used in Argamon et al.(2005)." ></td>
	<td class="line x" title="96:215	Lowest . . ." ></td>
	<td class="line x" title="97:215	Highest 1 L  H 2 LL  HH 3 LLL HHH N1 12  13 N2 25  22 N3 39 32 E1 11  12 E2 23  24 E3 32 39 A1 11  13 A2 22  21 A3 34 37 C1 11  14 C2 17  27 C3 30 41 Table 1: Binary task groups: division method and author numbers." ></td>
	<td class="line x" title="98:215	N = Neuroticism; E = Extraversion; A = Agreeableness; C = Conscientiousness." ></td>
	<td class="line x" title="99:215	629 4.2.2 Multiple classification tasks 4." ></td>
	<td class="line x" title="100:215	Takes the greatest distinction between high (H) and low (L) groups from Task-1, and adds a medium group, but attempts to reduce the possibility of inter-group confusion by including only the smaller medium (m) group omitted from Task-2." ></td>
	<td class="line x" title="101:215	Not all subjects are therefore included in this analysis." ></td>
	<td class="line x" title="102:215	Since the three groups to be classified are completely distinct, this should be the easiest of the four multiple-class tasks." ></td>
	<td class="line x" title="103:215	5." ></td>
	<td class="line x" title="104:215	Following Task-4, this uses the most distinct high (H) and low (L) groups, but now considers all remaining subjects medium (M)." ></td>
	<td class="line x" title="105:215	6." ></td>
	<td class="line x" title="106:215	Following Task-2, this uses the larger high (hH) and low (Ll) groups, with all those in between forming the medium (m) group." ></td>
	<td class="line x" title="107:215	7." ></td>
	<td class="line x" title="108:215	Using the distinction between the high and low groups of Task-5 and -6, this creates a 5-way split: highest (H), relatively high (h), medium (m), relatively low (l) and lowest (L)." ></td>
	<td class="line x" title="109:215	With the greatest number of classes, this task is the hardest." ></td>
	<td class="line x" title="110:215	These sub-groups are tabulated in Table 2, giving the size of each group within each trait." ></td>
	<td class="line x" title="111:215	Lowest . . ." ></td>
	<td class="line x" title="112:215	Highest 4 L  m  H 5 L M H 6 Ll m hH 7 L l m h H N4 12  24  13 N5 12 46 13 N6 25 24 22 N7 12 13 24 9 13 E4 11  24  12 E5 11 48 12 E6 23 24 24 E7 11 12 24 12 12 A4 11  28  13 A5 11 47 13 A6 22 28 21 A7 11 11 28 8 13 C4 11  27  14 C5 11 46 14 C6 17 27 27 C7 11 6 27 13 14 Table 2: 3-way/5-way task groups: division method and author numbers." ></td>
	<td class="line x" title="113:215	N = Neuroticism; E = Extraversion; A = Agreeableness; C = Conscientiousness." ></td>
	<td class="line x" title="114:215	4.3 Feature selection There are many possible features that can be used for automatic text classification." ></td>
	<td class="line x" title="115:215	These experiments use essentially word-based biand trigrams." ></td>
	<td class="line x" title="116:215	It should be noted, however, that some generalisations have been made: all proper nouns were identified via CLAWS tagging using the WMatrix tool (Rayson, 2003), and replaced with a single marker (NP1); punctuation was collapsed into a single marker (<p>); and additional tags correspond to non-linguistic features of blogs for instance, <SOP> and <EOP> were used the mark the start and end of individual blogs posts." ></td>
	<td class="line x" title="117:215	Word n-gram approaches provide a large feature space with which to work." ></td>
	<td class="line x" title="118:215	But in the general interest of computational tractability, it is useful to reduce the size of the feature set." ></td>
	<td class="line x" title="119:215	There are many automatic approaches to feature selection, exploiting, for instance, information gain (Quinlan, 1993)." ></td>
	<td class="line x" title="120:215	However, manual methods can offer principled ways of both reducing the size of the set and avoiding overfitting." ></td>
	<td class="line x" title="121:215	We therefore explore the effect of different levels of restriction on the feature sets, and compare them with automatic feature selection." ></td>
	<td class="line x" title="122:215	The levels of restriction are as follows: I The least restricted feature set consists of the n-grams most commonly occurring within the blog corpus." ></td>
	<td class="line x" title="123:215	Therefore, the feature set for each personality dimension is to be drawn from the same pool." ></td>
	<td class="line x" title="124:215	The difference lies in the number of features selected: the size of the set will match that of the next level of restriction." ></td>
	<td class="line x" title="125:215	II The next set includes only those n-grams which were distinctive for the two extremes of each personality trait." ></td>
	<td class="line x" title="126:215	Only features with a corpus frequency 5 are included." ></td>
	<td class="line x" title="127:215	This allows accurate log-likelihood G2 statistics to be computed (Rayson, 2003)." ></td>
	<td class="line x" title="128:215	Distinct collocations are identified via a three way comparison between the H and L groups in Task-1 (see section 4.2.1) and a third, neutral group." ></td>
	<td class="line x" title="129:215	This neutral group contains all those individuals who fell in the medium group (M) for all four traits in the study; the resulting group was of comparable size to the H and L groups for each trait." ></td>
	<td class="line x" title="130:215	Hence, this approach selects features using only a subset of the corpus." ></td>
	<td class="line x" title="131:215	Ngram software was used to identify and count collocations within a sub-corpus (Banerjee 630 and Pedersen, 2003)." ></td>
	<td class="line x" title="132:215	For each feature found, its frequency and relative frequency are calculated." ></td>
	<td class="line x" title="133:215	This permits relative frequency ratios and log-likelihood comparisons to be made between High-Low, High-Neutral and LowNeutral." ></td>
	<td class="line x" title="134:215	Only features that prove distinctive for the H or L groups with a significance of p < .01 are included in the feature set." ></td>
	<td class="line x" title="135:215	III The next set takes into account the possibility that, for a group used in Level-II, an ngram may be used relatively frequently, but only because a small number of authors in a group use it very frequently, while others in the same group use it not at all." ></td>
	<td class="line x" title="136:215	To enter the Level-III set, an n-gram meeting the Level-II criteria must also be used by at least 50%1 of the individuals within the subgroup for which it is reported to be distinctive." ></td>
	<td class="line x" title="137:215	IV While Level-III guards against excessive individual influence, it may abstract too far from the fine-grained variation within a personality trait." ></td>
	<td class="line x" title="138:215	The final manual set therefore includes only those n-grams that meet the Level-II criteria with p < .001, meet the Level-III criteria, and also correlate significantly (p < .05) with individual personality trait scores." ></td>
	<td class="line x" title="139:215	V Finally, it is possible to allow the n-gram feature set to be selected automatically during training." ></td>
	<td class="line x" title="140:215	The set to be selected from is the broadest of the manually filtered sets, those n-grams that meet the Level-II criteria." ></td>
	<td class="line x" title="141:215	The approach adopted is to use the defaults within the WEKA toolkit: Best First search with the CfsSubsetEval evaluator (Witten and Frank, 1999)." ></td>
	<td class="line x" title="142:215	Thus, a key question is whenif evera manual feature selection policy outperforms the automatic selection carried out under Level-V." ></td>
	<td class="line x" title="143:215	LevelsII and -III are of particular interest, since they contain features derived from a subset of the corpus." ></td>
	<td class="line x" title="144:215	Since different sub-groups are considered for each personality trait, the feature sets which meet the increasingly stringent criteria vary in size." ></td>
	<td class="line x" title="145:215	Table 3 contains the size of each of the four manuallydetermined feature sets for each of the four personality traits." ></td>
	<td class="line x" title="146:215	Note again that the number of ngrams selected from the most frequent in the cor1Conservatively rounded down in the case of an odd number of subjects." ></td>
	<td class="line x" title="147:215	I II III IV V N 747 747 169 22 19 E 701 701 167 11 20 A 823 823 237 36 34 C 704 704 197 22 25 Table 3: Number of n-grams per set." ></td>
	<td class="line x" title="148:215	Low High [was that] [this year] N [NP1 <p> NP1] [to eat] [<p> after] [slowly <p>] [is that] [and buy] [point in] [and he] E [last night <p>] [cool <p>] [it the] [<p> NP1] [is to] [to her] [thank god] [this is not] A [have any] [<p> it is] [have to] [<p> after] [turn up] [not have] [a few weeks] [by the way] C [case <p>] [<p> i hope] [okay <p>] [how i] [the game] [kind of] Table 4: Examples of significant Low and High n-grams from the Level-IV set." ></td>
	<td class="line x" title="149:215	pus for Level-I matches the size of the set for Level-II." ></td>
	<td class="line x" title="150:215	In addition, the features automatically selected are task-dependent, so the Level-V sets vary in size; here, the Table shows the number of features selected for Task-2." ></td>
	<td class="line x" title="151:215	To illustrate the types of n-grams in the feature sets, Table 4 contains four of the most significant n-grams from Level-IV for each personality class." ></td>
	<td class="line x" title="152:215	5 Results For each of the 60 binary classification tasks (1 to 3), the performance of the two approaches was compared." ></td>
	<td class="line x" title="153:215	Nave Bayes outperformed Support Vector Machines on 41/60, with 14 wins for SVM and 5 draws." ></td>
	<td class="line x" title="154:215	With limited space available, we therefore discuss only the results for NB, and use NB for Task-4 to -7." ></td>
	<td class="line x" title="155:215	The results for the binary tasks are displayed in Table 5." ></td>
	<td class="line x" title="156:215	Those for the multiple tasks are displayed in Table 6." ></td>
	<td class="line x" title="157:215	Baseline is the majority classification." ></td>
	<td class="line x" title="158:215	The most accurate performance of a feature set for each task is highlighted 631 Task Base Lv.I Lv.II Lv.III Lv.IV Lv.V N1 52.0 52.0 92.0 84.0 96.0 92.0 N2 53.2 51.1 63.8 68.1 83.6 85.1 N3 54.9 54.9 60.6 53.5 71.9 83.1 E1 52.2 56.5 91.3 95.7 87.0 100.0 E2 51.1 44.7 74.5 72.3 66.0 93.6 E3 54.9 50.7 53.5 59.2 64.8 85.9 A1 54.2 62.5 100.0 100.0 95.8 100.0 A2 51.2 60.5 81.4 79.1 72.1 97.7 A3 52.1 53.5 60.6 69.0 66.2 93.0 C1 56.0 52.0 100.0 100.0 84.0 92.0 C2 61.2 54.5 77.3 81.8 72.7 93.2 C3 57.7 54.9 63.4 71.8 70.4 84.5 Table 5: Nave Bayes performance on binary tasks." ></td>
	<td class="line x" title="159:215	Raw % accuracy for 4 personality dimensions, 3 tasks, and 5 feature selection policies." ></td>
	<td class="line x" title="160:215	in bold while the second most accurate is marked italic." ></td>
	<td class="line x" title="161:215	6 Discussion Let us consider the results as they bear in turn on the three main questions posed earlier: Can we improve on Argamon et al.s (2005) performance on binary classification for the Extraversion and Neuroticism dimensions?" ></td>
	<td class="line x" title="162:215	How accurately can we classify on the four personality dimensions?" ></td>
	<td class="line x" title="163:215	And how does performance on multiple classification compare with that on binary classification?" ></td>
	<td class="line x" title="164:215	Before addressing these questions, we note the relatively good performance of NB compared with vanilla SVM on the binary classification tasks." ></td>
	<td class="line x" title="165:215	We also note that automatic selection generally outperforms manual selection; however overfitting is very likely when examining just 71 data points." ></td>
	<td class="line x" title="166:215	Therefore, we do not discuss the Level-V results further." ></td>
	<td class="line x" title="167:215	6.1 Extraversion and Neuroticism The first main question relates to the feature sets chosen, because the main issue is whether word ngrams can give reasonable results on the Extraversion and Neuroticism classification tasks." ></td>
	<td class="line x" title="168:215	Of the current binary classification tasks, Task-2 is most closely comparable to Argamon et al.s. Here, the best performance for Extraversion was returned by the manual Level-II feature set, closely followed by Level-III." ></td>
	<td class="line x" title="169:215	The accuracy of 74.5% represents a 23.4% absolute improvement over baseline Task Base Lv.I Lv.II Lv.III Lv.IV Lv.V N4 49.0 49.0 81.6 65.3 77.6 85.7 N5 64.8 60.6 76.1 67.6 67.6 94.4 N6 35.2 31.0 47.9 46.5 66.2 70.4 N7 33.8 31.0 49.3 38.0 42.3 47.9 E4 51.1 44.7 74.5 59.6 53.2 78.7 E5 67.6 60.6 83.1 67.6 54.9 90.1 E6 33.8 23.9 53.5 46.5 46.5 56.3 E7 33.8 44.7 39.4 29.6 38.0 40.8 A4 53.8 51.9 90.4 78.8 67.3 80.8 A5 66.2 59.2 83.1 84.5 74.6 80.3 A6 39.4 31.0 67.6 60.6 56.3 85.9 A7 39.4 33.8 69.8 60.6 50.7 47.9 C4 51.9 53.8 92.3 65.4 67.3 82.7 C5 64.8 62.0 74.6 69.0 62.0 83.1 C6 38.0 39.4 59.2 59.2 50.7 78.9 C7 38.0 36.6 62.0 45.1 45.1 49.3 Table 6: Nave Bayes performance on multiple tasks." ></td>
	<td class="line x" title="170:215	Raw % accuracy for 4 personality dimensions, 4 tasks, and 5 feature selection policies." ></td>
	<td class="line x" title="171:215	(45.8% relative improvement; we report relative improvement over baseline because baseline accuracies vary between tasks)." ></td>
	<td class="line x" title="172:215	The best performance for Neuroticism was returned by Level-IV." ></td>
	<td class="line x" title="173:215	The accuracy of 83.6% represents a 30.4% absolute improvement over baseline (57.1% relative improvement)." ></td>
	<td class="line x" title="174:215	Argamon et al.s feature set combined insights from computational stylometrics (Koppel et al. , 2002; Argamon et al. , 2003) and systemicfunctional grammar." ></td>
	<td class="line x" title="175:215	Their focus on function words and appraisal-related features was intended to provide more general and informative features than the usual n-grams." ></td>
	<td class="line x" title="176:215	Now, it is unlikely that weblogs are easier to categorise than the genres studied by Argamon et al. So there are instead at least two reasons for the improvement we report." ></td>
	<td class="line x" title="177:215	First, although we did not use systemicfunctional linguistic features, we did test n-grams selected according to more or less strict policies." ></td>
	<td class="line x" title="178:215	So, considering the manual policies, it seems that the Level-IV was the best-performing set for Neuroticism." ></td>
	<td class="line x" title="179:215	This might be expected, given that Level-IV potentially overfits, allowing features to be derived from the full corpus." ></td>
	<td class="line x" title="180:215	However, in spite of this, Level-II pproved best for Extraversion." ></td>
	<td class="line x" title="181:215	Secondly, in classifying an individual as high or low on some dimension, Argamon et al. had 632 (for some of their materials) 500 words from that individual, whereas we had approximately 5000 words." ></td>
	<td class="line x" title="182:215	The availability of more words per individual is to likely to help greatly in training." ></td>
	<td class="line x" title="183:215	Additionally, a greater volume of text increases the chances that a long term property such as personality will emerge 6.2 Binary classification of all dimensions The second question concerns the relative ease of classifying the different dimensions." ></td>
	<td class="line x" title="184:215	Across each of Task-1 to -3, we find that classification accuracies for Agreeableness and Conscientiousness tend to be higher than those for Extraversion and Neuroticism." ></td>
	<td class="line x" title="185:215	In all but two cases, the automatically generated feature set (V) performs best." ></td>
	<td class="line x" title="186:215	Putting this to one side, of the manually constructed sets, the unrestricted set (I) performs worst, often below the baseline, while Level-IV is the best for classifying each task of Neuroticism." ></td>
	<td class="line x" title="187:215	Overall, II and III are better than IV, although the difference is not large." ></td>
	<td class="line x" title="188:215	As tasks increase in difficultyas high and low groups become closer together, and the left-out middle shrinksperformance drops." ></td>
	<td class="line x" title="189:215	But accuracy is still respectable." ></td>
	<td class="line x" title="190:215	6.3 Beyond binary classification The final question is about how classification accuracy suffers as the classification task becomes more subtle." ></td>
	<td class="line x" title="191:215	As expected, we find that as we add more categories, the tasks are harder: compare the results in the Tables for Task-1, -5 and -7." ></td>
	<td class="line x" title="192:215	And, as with the binary tasks, if fewer mid-scoring individuals are left out, the task is typically harder: compare results for Task-4 and 5." ></td>
	<td class="line x" title="193:215	It does seem that some personality dimensions respond to task difficulty more robustly than others." ></td>
	<td class="line x" title="194:215	For instance, on the hardest task, the best Extraversion classification accuracy is 10.9% absolute over the baseline (32.2% relative), while the best Agreeableness accuracy is 30.4% absolute over the baseline (77.2% relative)." ></td>
	<td class="line x" title="195:215	It is notable that the feature set which return the best resultsbar the automatic set V tends to be Level-II, excepting for Neuroticism on Task-6, where Level-IV considerably outperforms the other sets." ></td>
	<td class="line x" title="196:215	A supplementary question is how the best classifiers compare with human performance on this task." ></td>
	<td class="line x" title="197:215	Mishne (2005) reports that, for general mood classification on weblogs, the accuracy of his automatic classifier is comparable to human performance." ></td>
	<td class="line x" title="198:215	There are also general results on human personality classification performance in computer-mediated communication, which suggest that at least some dimensions can be accurately judged even when computer-mediated." ></td>
	<td class="line x" title="199:215	Vazire and Gosling (2004) report that for personal websites, relative accuracy of judgment was, in descending order: Openness > Extraversion > Neuroticism > Agreeableness > Conscientiousness." ></td>
	<td class="line x" title="200:215	Similarly, Gill et al.(2006) report that for personal e-mail, Extraversion is more accurately judged than Neuroticism." ></td>
	<td class="line x" title="202:215	The current study does not have a set of human judgments to report." ></td>
	<td class="line x" title="203:215	For now, it is interesting to note that the performance profile for the best classifiers, on the simplest tasks, appears to diverge from the general human profile, instead ranking on raw accuracy: Agreeableness > Conscientiousness > Neuroticism > Extraversion." ></td>
	<td class="line x" title="204:215	7 Conclusion and next steps This paper has reported the first stages of our investigations into classification of author personality from weblog text." ></td>
	<td class="line x" title="205:215	Results are quite promising, and comparable across all four personality traits." ></td>
	<td class="line x" title="206:215	It seems that even a small selection of features found to exhibit an empirical relationship with personality traits can be used to generate reasonably accurate classification results." ></td>
	<td class="line x" title="207:215	Naturally, there are still many paths to explore." ></td>
	<td class="line x" title="208:215	Simple regression analyses are reported in Nowson (2006); however, for classification, a more thorough comparison of different machine learning methodologies is required." ></td>
	<td class="line oc" title="209:215	A richer set of features besides n-grams should be checked, and we should not ignore the potential effectiveness of unigrams in this task (Pang et al. , 2002)." ></td>
	<td class="line x" title="210:215	A completely new test set can be gathered, so as to further guard against overfitting, and to explore systematically the effects of the amount of training data available for each author." ></td>
	<td class="line x" title="211:215	And as just discussed, comparison with human personality classification accuracy is potentially very interesting." ></td>
	<td class="line x" title="212:215	However, it does seem that we are making progress towards being able to deal with a realistic task: if we spot a thumbs-up review in a weblog, we should be able to check other text in that weblog, and tell whose thumb it is; or more accurately, what kind of persons thumb it is, anyway." ></td>
	<td class="line x" title="213:215	And that in turn should help tell us how high the thumb is really being held." ></td>
	<td class="line x" title="214:215	633 8 Acknowledgements We are grateful for the helpful advice of Mirella Lapata, and our three anonymous reviewers." ></td>
	<td class="line x" title="215:215	The second author was supported by a studentship from the Economic and Social Research Council." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-0301
Extracting Opinions, Opinion Holders, And Topics Expressed In Online News Media Text
Kim, Soo-Min;Hovy, Eduard H.;"></td>
	<td class="line x" title="1:178	Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 18, Sydney, July 2006." ></td>
	<td class="line x" title="2:178	c2006 Association for Computational Linguistics Extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text Soo-Min Kim and Eduard Hovy USC Information Sciences Institute 4676 Admiralty Way Marina del Rey, CA 90292-6695 {skim, hovy}@ISI.EDU Abstract This paper presents a method for identifying an opinion with its holder and topic, given a sentence from online news media texts." ></td>
	<td class="line x" title="3:178	We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective." ></td>
	<td class="line x" title="4:178	This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using data from FrameNet." ></td>
	<td class="line x" title="5:178	We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles." ></td>
	<td class="line x" title="6:178	For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet." ></td>
	<td class="line x" title="7:178	Our experimental results show that our system performs significantly better than the baseline." ></td>
	<td class="line x" title="8:178	1 Introduction The challenge of automatically identifying opinions in text automatically has been the focus of attention in recent years in many different domains such as news articles and product reviews." ></td>
	<td class="line x" title="9:178	Various approaches have been adopted in subjectivity detection, semantic orientation detection, review classification and review mining." ></td>
	<td class="line x" title="10:178	Despite the successes in identifying opinion expressions and subjective words/phrases, there has been less achievement on the factors closely related to subjectivity and polarity, such as opinion holder, topic of opinion, and inter-topic/inter-opinion relationships." ></td>
	<td class="line x" title="11:178	This paper addresses the problem of identifying not only opinions in text but also holders and topics of opinions from online news articles." ></td>
	<td class="line x" title="12:178	Identifying opinion holders is important especially in news articles." ></td>
	<td class="line x" title="13:178	Unlike product reviews in which most opinions expressed in a review are likely to be opinions of the author of the review, news articles contain different opinions of different opinion holders (e.g. people, organizations, and countries)." ></td>
	<td class="line x" title="14:178	By grouping opinion holders of different stance on diverse social and political issues, we can have a better understanding of the relationships among countries or among organizations." ></td>
	<td class="line x" title="15:178	An opinion topic can be considered as an object an opinion is about." ></td>
	<td class="line x" title="16:178	In product reviews, for example, opinion topics are often the product itself or its specific features, such as design and quality (e.g. I like the design of iPod video, The sound quality is amazing)." ></td>
	<td class="line x" title="17:178	In news articles, opinion topics can be social issues, governments acts, new events, or someones opinions." ></td>
	<td class="line x" title="18:178	(e.g., Democrats in Congress accused vice president Dick Cheneys shooting accident., Shiite leaders accused Sunnis of a mass killing of Shiites in Madaen, south of Baghdad.) As for opinion topic identification, little research has been conducted, and only in a very limited domain, product reviews." ></td>
	<td class="line x" title="19:178	In most approaches in product review mining, given a product (e.g. mp3 player), its frequently mentioned features (e.g. sound, screen, and design) are first collected and then used as anchor points." ></td>
	<td class="line x" title="20:178	In this study, we extract opinion topics from news articles." ></td>
	<td class="line x" title="21:178	Also, we do not pre-limit topics in advance." ></td>
	<td class="line x" title="22:178	We first identify an opinion and then find its holder and topic." ></td>
	<td class="line x" title="23:178	We define holder as an entity who holds an opinion, and topic, as what the opinion is about." ></td>
	<td class="line x" title="24:178	In this paper, we propose a novel method that employs Semantic Role Labeling, a task of identifying semantic roles given a sentence." ></td>
	<td class="line x" title="25:178	We de1 compose the overall task into the following steps:  Identify opinions." ></td>
	<td class="line x" title="26:178	 Label semantic roles related to the opinions." ></td>
	<td class="line x" title="27:178	 Find holders and topics of opinions among the identified semantic roles." ></td>
	<td class="line x" title="28:178	 Store <opinion, holder, topic> triples into a database." ></td>
	<td class="line x" title="29:178	In this paper, we focus on the first three subtasks." ></td>
	<td class="line x" title="30:178	The main contribution of this paper is to present a method that identifies not only opinion holders but also opinion topics." ></td>
	<td class="line x" title="31:178	To achieve this goal, we utilize FrameNet data by mapping target words to opinion-bearing words and mapping semantic roles to holders and topics, and then use them for system training." ></td>
	<td class="line x" title="32:178	We demonstrate that investigating semantic relations between an opinion and its holder and topic is crucial in opinion holder and topic identification." ></td>
	<td class="line x" title="33:178	This paper is organized as follows: Section 2 briefly introduces related work both in sentiment analysis and semantic role labeling." ></td>
	<td class="line x" title="34:178	Section 3 describes our approach for identifying opinions and labeling holders and topics by utilizing FrameNet 1 data for our task." ></td>
	<td class="line x" title="35:178	Section 4 reports our experiments and results with discussions and finally Section 5 concludes." ></td>
	<td class="line x" title="36:178	2 Related Work This section reviews previous works in both sentiment detection and semantic role labeling." ></td>
	<td class="line x" title="37:178	2.1 Subjectivity and Sentiment Detection Subjectivity detection is the task of identifying subjective words, expressions, and sentences (Wiebe et al. , 1999; Hatzivassiloglou and Wiebe, 2000; Riloff et al. , 2003)." ></td>
	<td class="line oc" title="38:178	Identifying subjectivity helps separate opinions from fact, which may be useful in question answering, summarization, etc. Sentiment detection is the task of determining positive or negative sentiment of words (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2005), phrases and sentences (Kim and Hovy, 2004; Wilson et al. , 2005), or documents (Pang et al. , 2002; Turney, 2002)." ></td>
	<td class="line o" title="39:178	Building on this work, more sophisticated problems such as opinion holder identification have also been studied." ></td>
	<td class="line x" title="40:178	(Bethard et al. , 2004) identify opinion propositions and holders." ></td>
	<td class="line x" title="41:178	Their 1 http://framenet.icsi.berkeley.edu/ work is similar to ours but different because their opinion is restricted to propositional opinion and mostly to verbs." ></td>
	<td class="line x" title="42:178	Another related works are (Choi et al. , 2005; Kim and Hovy, 2005)." ></td>
	<td class="line x" title="43:178	Both of them use the MPQA corpus 2 but they only identify opinion holders, not topics." ></td>
	<td class="line x" title="44:178	As for opinion topic identification, little research has been conducted, and only in a very limited domain, product reviews." ></td>
	<td class="line x" title="45:178	(Hu and Liu, 2004; Popescu and Etzioni, 2005) present product mining algorithms with extracting certain product features given specific product types." ></td>
	<td class="line x" title="46:178	Our paper aims at extracting topics of opinion in general news media text." ></td>
	<td class="line x" title="47:178	2.2 Semantic Role Labeling Semantic role labeling is the task of identifying semantic roles such as Agent, Patient, Speaker, or Topic, in a sentence." ></td>
	<td class="line x" title="48:178	A statistical approach for semantic role labeling was introduced by (Gildea and Jurafsky, 2002)." ></td>
	<td class="line x" title="49:178	Their system learned semantic relationship among constituents in a sentence from FrameNet, a large corpus of semantically hand-annotated data." ></td>
	<td class="line x" title="50:178	The FrameNet annotation scheme is based on Frame Semantics (Fillmore, 1976)." ></td>
	<td class="line x" title="51:178	Frames are defined as schematic representations of situations involving various frame elements such as participants, props, and other conceptual roles. For example, given a sentence Jack built a new house out of bricks, a semantic role labeling system should identify the roles for the verb built such as [ Agent Jack] built [ Created_entity a new house] [ Component out of bricks] 3." ></td>
	<td class="line x" title="52:178	In our study, we build a semantic role labeling system as an intermediate step to label opinion holders and topics by training it on opinion-bearing frames and their frame elements in FrameNet." ></td>
	<td class="line x" title="53:178	3 Finding Opinions and Their Holders and Topics For the goal of this study, extracting opinions from news media texts with their holders and topics, we utilize FrameNet data." ></td>
	<td class="line x" title="54:178	The basic idea of our approach is to explore how an opinion holder and a topic are semantically related to an opinion bearing word in a sentence." ></td>
	<td class="line x" title="55:178	Given a sentence and an opinion bearing word, our method identifies frame elements in the sentence and 2 http://www.cs.pitt.edu/~wiebe/pubs/ardasummer02/ 3 The verb build is defined under the frame Building in which Agent, Created_entity, and Components are defined as frame elements." ></td>
	<td class="line x" title="56:178	2 searches which frame element corresponds to the opinion holder and which to the topic." ></td>
	<td class="line x" title="57:178	The example in Figure 1 shows the intuition of our algorithm." ></td>
	<td class="line x" title="58:178	We decompose our task in 3 subtasks: (1) collect opinion words and opinion-related frames, (2) semantic role labeling for those frames, and (3) finally map semantic roles to holder and topic." ></td>
	<td class="line x" title="59:178	Following subsections describe each subtask." ></td>
	<td class="line x" title="60:178	3.1 Opinion Words and Related Frames We describe the subtask of collecting opinion words and related frames in 3 phases." ></td>
	<td class="line x" title="61:178	Phase 1: Collect Opinion Words In this study, we consider an opinion-bearing (positive/negative) word is a key indicator of an opinion." ></td>
	<td class="line x" title="62:178	Therefore, we first identify opinionbearing word from a given sentence and extract its holder and topic." ></td>
	<td class="line x" title="63:178	Since previous studies indicate that opinion-bearing verbs and adjectives are especially efficient for opinion identification, we focus on creating a set of opinion-bearing verbs and adjectives." ></td>
	<td class="line x" title="64:178	We annotated 1860 adjectives and 2011 verbs 4 by classifying them into positive, negative, and neutral classes." ></td>
	<td class="line x" title="65:178	Words in the positive class carry positive valence whereas 4 These were randomly selected from 8011 English verbs and 19748 English adjectives." ></td>
	<td class="line x" title="66:178	those in negative class carry negative valence." ></td>
	<td class="line x" title="67:178	Words that are not opinion-bearing are classified as neutral." ></td>
	<td class="line x" title="68:178	Note that in our study we treat word sentiment classification as a three-way classification problem instead of a two-way classification problem (i.e. positive and negative)." ></td>
	<td class="line x" title="69:178	By adding the third class, neutral, we can prevent the classifier assigning either positive or negative sentiment to weak opinion-bearing word." ></td>
	<td class="line x" title="70:178	For example, the word central that Hatzivassiloglou and McKeown (1997) marked as a positive adjective is not classified as positive by our system." ></td>
	<td class="line x" title="71:178	Instead we mark it as neutral, since it is a weak clue for an opinion." ></td>
	<td class="line x" title="72:178	For the same reason, we did not consider able classified as a positive word by General Inquirer 5, a sentiment word lexicon, as a positive opinion indicator." ></td>
	<td class="line x" title="73:178	Finally, we collected 69 positive and 151 negative verbs and 199 positive and 304 negative adjectives." ></td>
	<td class="line x" title="74:178	Phase 2: Find Opinion-related Frames We collected frames related to opinion words from the FrameNet corpus." ></td>
	<td class="line x" title="75:178	We used FrameNet II (Baker et al. , 2003) which contains 450 semantic frames and more than 3000 frame elements (FE)." ></td>
	<td class="line x" title="76:178	A frame consists of lexical items, called Lexical Unit (LU), and related frame elements." ></td>
	<td class="line x" title="77:178	For instance, LUs in ATTACK frame are verbs such as assail, assault, and attack, and nouns such as invasion, raid, and strike." ></td>
	<td class="line x" title="78:178	FrameNet II contains 5 http://www.wjh.harvard.edu/~inquirer/homecat.htm Table 1: Example of opinion related frames and lexical units Frame name Lexical units Frame elements Desiring want, wish, hope, eager, desire, interested, Event, Experiencer, Location_of_event Emotion _directed agitated, amused, anguish, ashamed, angry, annoyed, Event, Topic Experiencer, Expressor, Mental _property absurd, brilliant, careless, crazy, cunning, foolish Behavior, Protagonist, Domain, Degree Subject _stimulus delightful, amazing, annoying, amusing, aggravating, Stimulus, Degree Experiencer, Circumstances, Figure 1: An overview of our algorithm 3 approximately 7500 lexical units and over 100,000 annotated sentences." ></td>
	<td class="line x" title="79:178	For each word in our opinion word set described in Phase 1, we find a frame to which the word belongs." ></td>
	<td class="line x" title="80:178	49 frames for verbs and 43 frames for adjectives are collected." ></td>
	<td class="line x" title="81:178	Table 1 shows examples of selected frames with some of the lexical units those frames cover." ></td>
	<td class="line x" title="82:178	For example, our system found the frame Desiring from opinionbearing words want, wish, hope, etc. Finally, we collected 8256 and 11877 sentences related to selected opinion bearing frames for verbs and adjectives respectively." ></td>
	<td class="line x" title="83:178	Phase 3: FrameNet expansion Even though Phase 2 searches for a correlated frame for each verb and adjective in our opinionbearing word list, not all of them are defined in FrameNet data." ></td>
	<td class="line x" title="84:178	Some words such as criticize and harass in our list have associated frames (Case 1), whereas others such as vilify and maltreat do not have those (Case 2)." ></td>
	<td class="line x" title="85:178	For a word in Case 2, we use a clustering algorithms CBC (Clustering By Committee) to predict the closest (most reasonable) frame of undefined word from existing frames." ></td>
	<td class="line x" title="86:178	CBC (Pantel and Lin, 2002) was developed based on the distributional hypothesis (Harris, 1954) that words which occur in the same contexts tend to be similar." ></td>
	<td class="line x" title="87:178	Using CBC, for example, our clustering module computes lexical similarity between the word vilify in Case 2 and all words in Case 1." ></td>
	<td class="line x" title="88:178	Then it picks criticize as a similar word, so that we can use for vilify the frame Judgment_communication to which criticize belongs and all frame elements defined under Judgment_ communication." ></td>
	<td class="line x" title="89:178	3.2 Semantic Role Labeling To find a potential holder and topic of an opinion word in a sentence, we first label semantic roles in a sentence." ></td>
	<td class="line x" title="90:178	Modeling: We follow the statistical approaches for semantic role labeling (Gildea and Jurafsky, 2002; Fleischman et." ></td>
	<td class="line x" title="91:178	al, 2003) which separate the task into two steps: identify candidates of frame elements (Step 1) and assign semantic roles for those candidates (Step 2)." ></td>
	<td class="line x" title="92:178	Like their intuition, we treated both steps as classification problems." ></td>
	<td class="line x" title="93:178	We first collected all constituents of the given sentence by parsing it using the Charniak parser." ></td>
	<td class="line x" title="94:178	Then, in Step 1, we classified candidate constituents of frame elements from non-candidates." ></td>
	<td class="line x" title="95:178	In Step 2, each selected candidate was thus classified into one of frame element types (e.g. Stimulus, Degree, Experiencer, etc.)." ></td>
	<td class="line x" title="96:178	As a learning algorithm for our classification model, we used Maximum Entropy (Berger et al. , 1996)." ></td>
	<td class="line x" title="97:178	For system development, we used MEGA model optimization package 6, an implementation of ME models." ></td>
	<td class="line x" title="98:178	Data: We collected 8256 and 11877 sentences which were associated to opinion bearing frames for verbs and adjectives from FrameNet annotation data." ></td>
	<td class="line x" title="99:178	Each sentence in our dataset contained a frame name, a target predicate (a word whose meaning represents aspects of the frame), and frame elements labeled with element types." ></td>
	<td class="line x" title="100:178	We divided the data into 90% for training and 10% for test." ></td>
	<td class="line x" title="101:178	Features used: Table 2 describes features that we used for our classification model." ></td>
	<td class="line x" title="102:178	The target word is an opinion-bearing verb or adjective which is associated to a frame." ></td>
	<td class="line x" title="103:178	We used the Charniak parser to get a phrase type feature of a frame element and the parse tree path feature." ></td>
	<td class="line x" title="104:178	We determined a head word of a phrase by an algorithm using a tree head table 7, position feature by the order of surface words of a frame element and the target word, and the voice feature by a simple pattern." ></td>
	<td class="line x" title="105:178	Frame name for a target 6 http://www.isi.edu/~hdaume/megam/index.html 7 http://people.csail.mit.edu/mcollins/papers/heads Table 2: Features used for our semantic role labeling model." ></td>
	<td class="line x" title="106:178	Feature Description target word A predicate whose meaning represents the frame (a verb or an adjective in our task) phrase type Syntactic type of the frame element (e.g. NP, PP) head word Syntactic head of the frame element phrase parse tree path A path between the frame element and target word in the parse tree position Whether the element phrase occurs before or after the target word voice The voice of the sentence (active or passive) frame name one of our opinion-related frames 4 word was selected by methods described in Phase 2 and Phase 3 in Subsection 3.1." ></td>
	<td class="line x" title="107:178	3.3 Map Semantic Roles to Holder and Topic After identifying frame elements in a sentence, our system finally selects holder and topic from those frame elements." ></td>
	<td class="line x" title="108:178	In the example in Table 1, the frame Desiring has frame elements such as Event (The change that the Experiencer would like to see), Experiencer (the person or sentient being who wishes for the Event to occur), Location_of_event (the place involved in the desired Event), Focal_participant (entity that the Experiencer wishes to be affected by some Event)." ></td>
	<td class="line x" title="109:178	Among these FEs, we can consider that Experiencer can be a holder and Focal_participant can be a topic (if any exists in a sentence)." ></td>
	<td class="line x" title="110:178	We manually built a mapping table to map FEs to holder or topic using as support the FE definitions in each opinion related frame and the annotated sample sentences." ></td>
	<td class="line x" title="111:178	4 Experimental Results The goal of our experiment is first, to see how our holder and topic labeling system works on the FrameNet data, and second, to examine how it performs on online news media text." ></td>
	<td class="line x" title="112:178	The first data set (Testset 1) consists of 10% of data described in Subsection 3.2 and the second (Testset 2) is manually annotated by 2 humans." ></td>
	<td class="line x" title="113:178	(see Subsection 4.2)." ></td>
	<td class="line x" title="114:178	We report experimental results for both test sets." ></td>
	<td class="line x" title="115:178	4.1 Experiments on Testset 1 Gold Standard: In total, Testset 1 contains 2028 annotated sentences collected from FrameNet data set." ></td>
	<td class="line x" title="116:178	(834 from frames related to opinion verb and 1194 from opinion adjectives) We measure the system performance using precision (the percentage of correct holders/topics among systems labeling results), recall (the percentage of correct holders/topics that system retrieved), and F-score." ></td>
	<td class="line x" title="117:178	Baseline: For the baseline system, we applied two different algorithms for sentences which have opinion-bearing verbs as target words and for those that have opinion-bearing adjectives as target words." ></td>
	<td class="line x" title="118:178	For verbs, baseline system labeled a subject of a verb as a holder and an object as a topic." ></td>
	<td class="line x" title="119:178	(e.g. [ holder He] condemned [ topic the lawyer].) For adjectives, the baseline marked the subject of a predicate adjective as a holder (e.g. [ holder I] was happy)." ></td>
	<td class="line x" title="120:178	For the topics of adjectives, the baseline picks a modified word if the target adjective is a modifier (e.g. That was a stupid [ topic mistake])." ></td>
	<td class="line x" title="121:178	and a subject word if the adjective is a predicate." ></td>
	<td class="line x" title="122:178	([ topic The view] is breathtaking in January)." ></td>
	<td class="line x" title="123:178	Result: Table 3 and 4 show evaluation results of our system and the baseline system respectively." ></td>
	<td class="line x" title="124:178	Our system performed much better than the baseline system in identifying topic and holder for both sets of sentences with verb target words and those with adjectives." ></td>
	<td class="line x" title="125:178	Especially in recognizing topics of target opinion-bearing words, our system improved F-score from 30.4% to 66.5% for verb target words and from 38.2% to 70.3% for adjectives." ></td>
	<td class="line x" title="126:178	It was interesting to see that the intuition that A subject of opinionbearing verb is a holder and an object is a topic which we applied for the baseline achieved relatively good F-score (56.9%)." ></td>
	<td class="line x" title="127:178	However, our system obtained much higher F-score (78.7%)." ></td>
	<td class="line x" title="128:178	Holder identification task achieved higher Fscore than topic identification which implies that identifying topics of opinion is a harder task." ></td>
	<td class="line x" title="129:178	We believe that there are many complicated semantic relations between opinion-bearing words and their holders and topics that simple relations such as subject and object relations are not able to capture." ></td>
	<td class="line x" title="130:178	For example, in a sentence Her letter upset me, simply looking for the subjective and objective of the verb upset is not enough to recognize the holder and topic." ></td>
	<td class="line x" title="131:178	It is necessary to see a deeper level of semantic relaTable 3." ></td>
	<td class="line x" title="132:178	Precision (P), Recall (R), and Fscore (F) of Topic and Holder identification for opinion verbs (V) and adjectives (A) on Testset 1." ></td>
	<td class="line x" title="133:178	Topic Holder P (%) R (%) F (%) P (%) R (%) F (%) V 69.1 64.0 66.5 81.9 75.7 78.7 A 67.5 73.4 70.3 66.2 77.9 71.6 Table 4." ></td>
	<td class="line x" title="134:178	Baseline system on Testset 1." ></td>
	<td class="line x" title="135:178	Topic Holder P (%) R (%) F (%) P (%) R (%) F (%) V 85.5 18.5 30.4 73.7 46.4 56.9 A 68.2 26.5 38.2 12.0 49.1 19.3 5 tions: Her letter is a stimulus and me is an experiencer of the verb upset." ></td>
	<td class="line x" title="136:178	4.2 Experiments on Testset 2 Gold Standard: Two humans 8 annotated 100 sentences randomly selected from news media texts." ></td>
	<td class="line x" title="137:178	Those news data is collected from online news sources such as The New York Times, UN Office for the Coordination of Humanitarian Affairs, and BBC News 9, which contain articles about various international affaires." ></td>
	<td class="line x" title="138:178	Annotators identified opinion-bearing sentences with marking opinion word with its holder and topic if they existed." ></td>
	<td class="line x" title="139:178	The inter-annotator agreement in identifying opinion sentences was 82%." ></td>
	<td class="line x" title="140:178	Baseline: In order to identify opinion-bearing sentences for our baseline system, we used the opinion-bearing word set introduced in Phase 1 in Subsection 3.1." ></td>
	<td class="line x" title="141:178	If a sentence contains an opinion-bearing verb or adjective, the baseline system started looking for its holder and topic." ></td>
	<td class="line x" title="142:178	For holder and topic identification, we applied the 8 We refer them as Human1 and Human2 for the rest of this paper." ></td>
	<td class="line x" title="143:178	9 www.nytimes.com, www.irinnews.org, and www.bbc.co.uk same baseline algorithm as described in Subsection 4.1 to Testset 2." ></td>
	<td class="line x" title="144:178	Result: Note that Testset 1 was collected from sentences of opinion-related frames in FrameNet and therefore all sentences in the set contained either opinion-bearing verb or adjective." ></td>
	<td class="line x" title="145:178	(i.e. All sentences are opinion-bearing) However, sentences in Testset 2 were randomly collected from online news media pages and therefore not all of them are opinion-bearing." ></td>
	<td class="line x" title="146:178	We first evaluated the task of opinion-bearing sentence identification." ></td>
	<td class="line x" title="147:178	Table 5 shows the system results." ></td>
	<td class="line x" title="148:178	When we mark all sentences as opinion-bearing, it achieved 43% and 38% of accuracy for the annotation result of Human1 and Human2 respectively." ></td>
	<td class="line x" title="149:178	Our system performance (64% and 55%) is comparable with the unique assignment." ></td>
	<td class="line x" title="150:178	We measured the holder and topic identification system with precision, recall, and F-score." ></td>
	<td class="line x" title="151:178	As we can see from Table 6, our system achieved much higher precision than the baseline system for both Topic and Holder identification tasks." ></td>
	<td class="line x" title="152:178	However, we admit that there is still a lot of room for improvement." ></td>
	<td class="line x" title="153:178	The system achieved higher precision for topic identification, whereas it achieved higher recall for holder identification." ></td>
	<td class="line x" title="154:178	In overall, our system attained higher F-score in holder identification task, including the baseline system." ></td>
	<td class="line x" title="155:178	Based on Fscore, we believe that identifying topics of opinion is much more difficult than identifying holders." ></td>
	<td class="line x" title="156:178	It was interesting to see the same phenomenon that the baseline system mainly assuming that subject and object of a sentence are likely to be opinion holder and topic, achieved lower scores for both holder and topic identification tasks in Testset 2 as in Testset 1." ></td>
	<td class="line x" title="157:178	This implies that more sophisticated analysis of the relationship between opinion words (e.g. verbs and adjectives) and their topics and holders is crucial." ></td>
	<td class="line x" title="158:178	4.3 Difficulties in evaluation We observed several difficulties in evaluating holder and topic identification." ></td>
	<td class="line x" title="159:178	First, the boundary of an entity of holder or topic can be flexible." ></td>
	<td class="line x" title="160:178	For example, in sentence Senator Titus Olupitan who sponsored the bill wants the permission., not only Senator Titus Olupitan but also Senator Titus Olupitan who sponsored the bill is an eligible answer." ></td>
	<td class="line x" title="161:178	Second, some correct holders and topics which our system found were evaluated wrong even if they referred the same entities in the gold standard because human annotators marked only one of them as an answer." ></td>
	<td class="line x" title="162:178	Table 5." ></td>
	<td class="line x" title="163:178	Opinion-bearing sentence identification on Testset 2." ></td>
	<td class="line x" title="164:178	(P: precision, R: recall, F: F-score, A: Accuracy, H1: Human1, H2: Human2) P (%) R (%) F (%) A (%) H1 56.9 67.4 61.7 64.0 H2 43.1 57.9 49.4 55.0 Table 6: Results of Topic and Holder identification on Testset 2." ></td>
	<td class="line x" title="165:178	(Sys: our system, BL: baseline) Topic Holder P(%) R(%) F(%) P(%) R(%) F(%) H1 64.7 20.8 31.5 47.9 34.0 39.8 Sys H2 58.8 7.1 12.7 36.6 26.2 30.5 H1 12.5 9.4 10.7 20.0 28.3 23.4 BL H2 23.2 7.1 10.9 14.0 19.0 16.1 6 In the future, we need more annotated data for improved evaluation." ></td>
	<td class="line x" title="166:178	5 Conclusion and Future Work This paper presented a methodology to identify an opinion with its holder and topic given a sentence in online news media texts." ></td>
	<td class="line x" title="167:178	We introduced an approach of exploiting semantic structure of a sentence, anchored to an opinion bearing verb or adjective." ></td>
	<td class="line x" title="168:178	This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data." ></td>
	<td class="line x" title="169:178	Our method first identifies an opinion-bearing word, labels semantic roles related to the word in the sentence, and then finds a holder and a topic of the opinion word among labeled semantic roles." ></td>
	<td class="line x" title="170:178	There has been little previous study in identifying opinion holders and topics partly because it requires a great amount of annotated data." ></td>
	<td class="line x" title="171:178	To overcome this barrier, we utilized FrameNet data by mapping target words to opinion-bearing words and mapping semantic roles to holders and topics." ></td>
	<td class="line x" title="172:178	However, FrameNet has a limited number of words in its annotated corpus." ></td>
	<td class="line x" title="173:178	For a broader coverage, we used a clustering technique to predict a most probable frame for an unseen word." ></td>
	<td class="line x" title="174:178	Our experimental results showed that our system performs significantly better than the baseline." ></td>
	<td class="line x" title="175:178	The baseline system results imply that opinion holder and topic identification is a hard task." ></td>
	<td class="line x" title="176:178	We believe that there are many complicated semantic relations between opinion-bearing words and their holders and topics which simple relations such as subject and object relations are not able to capture." ></td>
	<td class="line x" title="177:178	In the future, we plan to extend our list of opinion-bearing verbs and adjectives so that we can discover and apply more opinion-related frames." ></td>
	<td class="line x" title="178:178	Also, it would be interesting to see how other types of part of speech such as adverbs and nouns affect the performance of the system." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-0305
Annotating Attribution In The Penn Discourse TreeBank
Prasad, Rashmi;Dinesh, Nikhil;Lee, Alan;Joshi, Aravind K.;Webber, Bonnie Lynn;"></td>
	<td class="line x" title="1:177	Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 3138, Sydney, July 2006." ></td>
	<td class="line x" title="2:177	c2006 Association for Computational Linguistics Annotating Attribution in the Penn Discourse TreeBank Rashmi Prasad and Nikhil Dinesh and Alan Lee and Aravind Joshi University of Pennsylvania Philadelphia, PA 19104 USA a0 rjprasad,nikhild,aleewk,joshi a1 @linc.cis.upenn.edu Bonnie Webber University of Edinburgh Edinburgh, EH8 9LW Scotland bonnie@inf.ed.ac.uk Abstract An emerging task in text understanding and generation is to categorize information as fact or opinion and to further attribute it to the appropriate source." ></td>
	<td class="line x" title="3:177	Corpus annotation schemes aim to encode such distinctions for NLP applications concerned with such tasks, such as information extraction, question answering, summarization, and generation." ></td>
	<td class="line x" title="4:177	We describe an annotation scheme for marking the attribution of abstract objects such as propositions, facts and eventualities associated with discourse relations and their arguments annotated in the Penn Discourse TreeBank." ></td>
	<td class="line x" title="5:177	The scheme aims to capture the source and degrees of factuality of the abstract objects." ></td>
	<td class="line x" title="6:177	Key aspects of the scheme are annotation of the text spans signalling the attribution, and annotation of features recording the source, type, scopal polarity, and determinacy of attribution." ></td>
	<td class="line x" title="7:177	1 Introduction News articles typically contain a mixture of information presented from several different perspectives, and often in complex ways." ></td>
	<td class="line x" title="8:177	Writers may present information as known to them, or from some other individuals perspective, while further distinguishing between, for example, whether that perspective involves an assertion or a belief." ></td>
	<td class="line x" title="9:177	Recent work has shown the importance of recognizing such perspectivization of information for several NLP applications, such as information extraction, summarization, question answering (Wiebe et al. , 2004; Stoyanov et al. , 2005; Riloff et al. , 2005) and generation (Prasad et al. , 2005)." ></td>
	<td class="line x" title="10:177	Part of the goal of such applications is to distinguish between factual and non-factual information, and to identify the source of the information." ></td>
	<td class="line x" title="11:177	Annotation schemes (Wiebe et al. , 2005; Wilson and Wiebe, 2005; PDTB-Group, 2006) encode such distinctions to facilitate accurate recognition and representation of such perspectivization of information." ></td>
	<td class="line x" title="12:177	This paper describes an extended annotation scheme for marking the attribution of discourse relations and their arguments annotated in the Penn Discourse TreeBank (PDTB) (Miltsakaki et al. , 2004; Prasad et al. , 2004; Webber et al. , 2005), the primary goal being to capture the source and degrees of factuality of abstract objects." ></td>
	<td class="line x" title="13:177	The scheme captures four salient properties of attribution: (a) source, distinguishing between different types of agents to whom AOs are attributed, (b) type, reflecting the degree of factuality of the AO, (c) scopal polarity of attribution, indicating polarity reversals of attributed AOs due to surface negated attributions, and (d) determinacy of attribution, indicating the presence of contexts canceling the entailment of attribution." ></td>
	<td class="line x" title="14:177	The scheme also describes annotation of the text spans signaling the attribution." ></td>
	<td class="line x" title="15:177	The proposed scheme is an extension of the core scheme used for annotating attribution in the first release of the PDTB (Dinesh et al. , 2005; PDTB-Group, 2006)." ></td>
	<td class="line x" title="16:177	Section 2 gives an overview of the PDTB, Section 3 presents the extended annotation scheme for attribution, and Section 4 presents the summary." ></td>
	<td class="line x" title="17:177	2 The Penn Discourse TreeBank (PDTB) The PDTB contains annotations of discourse relations and their arguments on the Wall Street Journal corpus (Marcus et al. , 1993)." ></td>
	<td class="line x" title="18:177	Following the approach towards discourse structure in (Webber et al. , 2003), the PDTB takes a lexicalized ap31 proacha2 towards the annotation of discourse relations, treating discourse connectives as the anchors of the relations, and thus as discourse-level predicates taking two abstract objects (AOs) as their arguments." ></td>
	<td class="line x" title="19:177	For example, in (1), the subordinating conjunction since is a discourse connective that anchors a TEMPORAL relation between the event of the earthquake hitting and a state where no music is played by a certain woman." ></td>
	<td class="line x" title="20:177	(The 4digit number in parentheses at the end of examples gives the WSJ file number of the example.)" ></td>
	<td class="line x" title="21:177	(1) She hasnt played any music since the earthquake hit." ></td>
	<td class="line x" title="22:177	(0766) There are primarily two types of connectives in the PDTB: Explicit and Implicit." ></td>
	<td class="line x" title="23:177	Explicit connectives are identified form four grammatical classes: subordinating conjunctions (e.g. , because, when, only because, particularly since), subordinators (e.g. , in order that), coordinating conjunctions (e.g. , and, or), and discourse adverbials (e.g. , however, otherwise)." ></td>
	<td class="line x" title="24:177	In the examples in this paper, Explicit connectives are underlined." ></td>
	<td class="line x" title="25:177	For sentences not related by an Explicit connective, annotators attempt to infer a discourse relation between them by inserting connectives (called Implicit connectives) that best convey the inferred relations." ></td>
	<td class="line x" title="26:177	For example, in (2), the inferred CAUSAL relation between the two sentences was annotated with because as the Implicit connective." ></td>
	<td class="line x" title="27:177	Implicit connectives together with their sense classification are shown here in small caps." ></td>
	<td class="line x" title="28:177	(2) Also unlike Mr. Ruder, Mr. Breeden appears to be in a position to get somewhere with his agenda." ></td>
	<td class="line x" title="29:177	Implicit=BECAUSE (CAUSE) As a former White House aide who worked closely with Congress, he is savvy in the ways of Washington." ></td>
	<td class="line x" title="30:177	(0955) Cases where a suitable Implicit connective could not be annotated between adjacent sentences are annotated as either (a) EntRel, where the second sentence only serves to provide some further description of an entity in the first sentence (Example 3); (b) NoRel, where no discourse relation or entity-based relation can be inferred; and (c) AltLex, where the insertion of an Implicit connective leads to redundancy, due to the relation being alternatively lexicalized by some nonconnective expression (Example 4)." ></td>
	<td class="line x" title="31:177	(3) C.B. Rogers Jr. was named chief executive officer of this business information concern." ></td>
	<td class="line x" title="32:177	Implicit=EntRel Mr. Rogers, 60 years old, succeeds J.V. White, 64, who will remain chairman and chairman of the executive committee (0929)." ></td>
	<td class="line x" title="33:177	(4) One in 1981 raised to $2,000 a year from $1,500 the amount a person could put, tax-deductible, into the tax-deferred accounts and widened coverage to people under employer retirement plans." ></td>
	<td class="line x" title="34:177	Implicit=AltLex (consequence) [This caused] an explosion of IRA promotions by brokers, banks, mutual funds and others." ></td>
	<td class="line x" title="35:177	(0933) Arguments of connectives are simply labelled Arg2, for the argument appearing in the clause syntactically bound to the connective, and Arg1, for the other argument." ></td>
	<td class="line x" title="36:177	In the examples here, Arg1 appears in italics, while Arg2 appears in bold." ></td>
	<td class="line x" title="37:177	The basic unit for the realization of an AO argument of a connective is the clause, tensed or untensed, but it can also be associated with multiple clauses, within or across sentences." ></td>
	<td class="line x" title="38:177	Nominalizations and discourse deictics (this, that), which can also be interpreted as AOs, can serve as the argument of a connective too." ></td>
	<td class="line x" title="39:177	The current version of the PDTB also contains attribution annotations on discourse relations and their arguments." ></td>
	<td class="line x" title="40:177	These annotations, however, used the earlier core scheme which is subsumed in the extended scheme described in this paper." ></td>
	<td class="line x" title="41:177	The first release of the Penn Discourse TreeBank, PDTB-1.0 (reported in PDTBGroup (2006)), is freely available from http://www.seas.upenn.edu/pdtb." ></td>
	<td class="line x" title="42:177	PDTB-1.0 contains 100 distinct types of Explicit connectives, with a total of 18505 tokens, annotated across the entire WSJ corpus (25 sections)." ></td>
	<td class="line x" title="43:177	Implicit relations have been annotated in three sections (Sections 08, 09, and 10) for the first release, totalling 2003 tokens (1496 Implicit connectives, 19 AltLex relations, 435 EntRel tokens, and 53 NoRel tokens)." ></td>
	<td class="line x" title="44:177	The corpus also includes a broadly defined sense classification for the implicit relations, and attribution annotation with the earlier core scheme." ></td>
	<td class="line x" title="45:177	Subsequent releases of the PDTB will include Implicit relations annotated across the entire corpus, attribution annotation using the extended scheme proposed here, and fine-grained sense classification for both Explicit and Implicit connectives." ></td>
	<td class="line x" title="46:177	3 Annotation of Attribution Recent work (Wiebe et al. , 2005; Prasad et al. , 2005; Riloff et al. , 2005; Stoyanov et al. , 2005), has shown the importance of recognizing and representing the source and factuality of information in certain NLP applications." ></td>
	<td class="line x" title="47:177	Information extraction systems, for example, would perform better 32 bya3 prioritizing the presentation of factual information, and multi-perspective question answering systems would benefit from presenting information from different perspectives." ></td>
	<td class="line oc" title="48:177	Most of the annotation approaches tackling these issues, however, are aimed at performing classifications at either the document level (Pang et al. , 2002; Turney, 2002), or the sentence or word level (Wiebe et al. , 2004; Yu and Hatzivassiloglou, 2003)." ></td>
	<td class="line n" title="49:177	In addition, these approaches focus primarily on sentiment classification, and use the same for getting at the classification of facts vs. opinions." ></td>
	<td class="line o" title="50:177	In contrast to these approaches, the focus here is on marking attribution on more analytic semantic units, namely the Abstract Objects (AOs) associated with predicate-argument discourse relations annotated in the PDTB, with the aim of providing a compositional classification of the factuality of AOs." ></td>
	<td class="line x" title="51:177	The scheme isolates four key properties of attribution, to be annotated as features: (1) source, which distinguishes between different types of agents (Section 3.1); (2) type, which encodes the nature of relationship between agents and AOs, reflecting the degree of factuality of the AO (Section 3.2); (3) scopal polarity, which is marked when surface negated attribution reverses the polarity of the attributed AO (Section 3.3), and (4) determinacy, which indicates the presence of contexts due to which the entailment of attribution gets cancelled (Section 3.4)." ></td>
	<td class="line x" title="52:177	In addition, to further facilitate the task of identifying attribution, the scheme also aims to annotate the text span complex signaling attribution (Section 3.5) Results from annotations using the earlier attribution scheme (PDTB-Group, 2006) show that a significant proportion (34%) of the annotated discourse relations have some non-Writer agent as the source for either the relation or one or both arguments." ></td>
	<td class="line x" title="53:177	This illustrates the simplest case of the ambiguity inherent for the factuality of AOs, and shows the potential use of the PDTB annotations towards the automatic classification of factuality." ></td>
	<td class="line x" title="54:177	The annotations also show that there are a variety of configurations in which the components of the relations are attributed to different sources, suggesting that recognition of attributions may be a complex task for which an annotated corpus may be useful." ></td>
	<td class="line x" title="55:177	For example, in some cases, a relation together with its arguments is attributed to the writer or some other agent, whereas in other cases, while the relation is attributed to the writer, one or both of its arguments is attributed to different agent(s)." ></td>
	<td class="line x" title="56:177	For Explicit connectives." ></td>
	<td class="line x" title="57:177	there were 6 unique configurations, for configurations containing more than 50 tokens, and 5 unique configurations for Implicit connectives." ></td>
	<td class="line x" title="58:177	3.1 Source The source feature distinguishes between (a) the writer of the text (Wr), (b) some specific agent introduced in the text (Ot for other), and (c) some generic source, i.e., some arbitrary (Arb) individual(s) indicated via a non-specific reference in the text." ></td>
	<td class="line x" title="59:177	The latter two capture further differences in the degree of factuality of AOs with nonwriter sources." ></td>
	<td class="line x" title="60:177	For example, an Arb source for some information conveys a higher degree of factuality than an Ot source, since it can be taken to be a generally accepted view." ></td>
	<td class="line x" title="61:177	Since arguments can get their attribution through the relation between them, they can be annotated with a fourth value Inh, to indicate that their source value is inherited from the relation." ></td>
	<td class="line x" title="62:177	Given this scheme for source, there are broadly two possibilities." ></td>
	<td class="line x" title="63:177	In the first case, a relation and both its arguments are attributed to the same source, either the writer, as in (5), or some other agent (here, Bill Biedermann), as in (6)." ></td>
	<td class="line x" title="64:177	(Attribution feature values assigned to examples are shown below each example; REL stands for the discourse relation denoted by the connective; Attribution text spans are shown boxed.)" ></td>
	<td class="line x" title="65:177	(5) Since the British auto maker became a takeover target last month, its ADRs have jumped about 78%." ></td>
	<td class="line x" title="66:177	(0048) REL Arg1 Arg2 [Source] Wr Inh Inh (6) The public is buying the market when in reality there is plenty of grain to be shipped, said Bill Biedermann a4a5a4a6a4 (0192) REL Arg1 Arg2 [Source] Ot Inh Inh As Example (5) shows, text spans for implicit Writer attributions (corresponding to implicit communicative acts such as I write, or I say), are not marked and are taken to imply Writer attribution by default (see also Section 3.5)." ></td>
	<td class="line x" title="67:177	In the second case, one or both arguments have a different source from the relation." ></td>
	<td class="line x" title="68:177	In (7), for example, the relation and Arg2 are attributed to the writer, whereas Arg1 is attributed to another agent (here, Mr. Green)." ></td>
	<td class="line x" title="69:177	On the other hand, in (8) and (9), the relation and Arg1 are attributed to the writer, whereas Arg2 is attributed to another agent." ></td>
	<td class="line x" title="70:177	33 (7) When Mr. Green won a $240,000 verdict in a land condemnation case against the state in June 1983, he says Judge OKicki unexpectedly awarded him an additional $100,000." ></td>
	<td class="line x" title="71:177	(0267) REL Arg1 Arg2 [Source] Wr Ot Inh (8) Factory orders and construction outlays were largely flat in December while purchasing agents said manufacturing shrank further in October." ></td>
	<td class="line x" title="72:177	(0178) REL Arg1 Arg2 [Source] Wr Inh Ot (9) There, on one of his first shopping trips, Mr. Paul picked up several paintings at stunning prices." ></td>
	<td class="line x" title="73:177	a4a5a4a5a4 Afterward, Mr. Paul is said by Mr. Guterman to have phoned Mr. Guterman, the New York developer selling the collection, and gloated." ></td>
	<td class="line x" title="74:177	(2113) REL Arg1 Arg2 [Source] Wr Inh Ot Example (10) shows an example of a generic source indicated by an agentless passivized attribution on Arg2 of the relation." ></td>
	<td class="line x" title="75:177	Note that passivized attributions can also be associated with a specific source when the agent is explicit, as shown in (9)." ></td>
	<td class="line x" title="76:177	Arb sources are also identified by the occurrences of adverbs like reportedly, allegedly, etc.(10) Although index arbitrage is said to add liquidity to markets, John Bachmann, a4a5a4a5a4 says too much liquidity isnt a good thing." ></td>
	<td class="line x" title="78:177	(0742) REL Arg1 Arg2 [Source] Wr Ot Arb We conclude this section by noting that Ot is used to refer to any specific individual as the source." ></td>
	<td class="line x" title="79:177	That is, no further annotation is provided to indicate who the Ot agent in the text is. Furthermore, as shown in Examples (11-12), multiple Ot sources within the same relation do not indicate whether or not they refer to the same or different agents." ></td>
	<td class="line x" title="80:177	However, we assume that the text span annotations for attribution, together with an independent mechanism for named entity recognition and anaphora resolution can be employed to identify and disambiguate the appropriate references." ></td>
	<td class="line x" title="81:177	(11) Suppression of the book, Judge Oakes observed, would operate as a prior restraint and thus involve the First Amendment." ></td>
	<td class="line x" title="82:177	Moreover, and here Judge Oakes went to the heart of the question, Responsible biographers and historians constantly use primary sources, letters, diaries, and memoranda." ></td>
	<td class="line x" title="83:177	(0944) REL Arg1 Arg2 [Source] Wr Ot Ot (12) The judge was considered imperious, abrasive and ambitious, those who practiced before him say." ></td>
	<td class="line x" title="84:177	Yet, despite the judges imperial bearing, no one ever had reason to suspect possible wrongdoing, says John Bognato, president of Cambria a4a6a4a5a4 .(0267) REL Arg1 Arg2 [Source] Wr Ot Ot 3.2 Type The type feature signifies the nature of the relation between the agent and the AO, leading to different inferences about the degree of factuality of the AO." ></td>
	<td class="line x" title="85:177	In order to capture the factuality of the AOs, we start by making a three-way distinction of AOs into propositions, facts and eventualities (Asher, 1993)." ></td>
	<td class="line x" title="86:177	This initial distinction allows for a more semantic, compositional approach to the annotation and recognition of factuality." ></td>
	<td class="line x" title="87:177	We define the attribution relations for each AO type as follows: (a) Propositions involve attribution to an agent of his/her (varying degrees of) commitment towards the truth of a proposition; (b) Facts involve attribution to an agent of an evaluation towards or knowledge of a proposition whose truth is taken for granted (i.e. , a presupposed proposition); and (c) Eventualities involve attribution to an agent of an intention/attitude towards an eventuality." ></td>
	<td class="line x" title="88:177	In the case of propositions, a further distinction is made to capture the difference in the degree of the agents commitment towards the truth of the proposition, by distinguishing between assertions and beliefs." ></td>
	<td class="line x" title="89:177	Thus, the scheme for the annotation of type ultimately uses a four-way distinction for AOs, namely between assertions, beliefs, facts, and eventualities." ></td>
	<td class="line x" title="90:177	Initial determination of the degree of factuality involves determination of the type of the AO." ></td>
	<td class="line x" title="91:177	AO types can be identified by well-defined semantic classes of verbs/phrases anchoring the attribution." ></td>
	<td class="line x" title="92:177	We consider each of these in turn." ></td>
	<td class="line x" title="93:177	Assertions are identified by assertive predicates or verbs of communication (Levin, 1993) such as say, mention, claim, argue, explain etc. They take the value Comm (for verbs of Communication)." ></td>
	<td class="line x" title="94:177	In Example (13), the Ot attribution on Arg1 takes the value Comm for type." ></td>
	<td class="line x" title="95:177	Implicit writer attributions, as in the relation of (13), also take (the default) Comm." ></td>
	<td class="line x" title="96:177	Note that when an arguments attribution source is not inherited (as in Arg1 in this example) it also takes its own independent value for type." ></td>
	<td class="line x" title="97:177	This example thus conveys that there are two different attributions expressed within the discourse relation, one for the relation and the other for one of its arguments, and that both involve assertion of propositions." ></td>
	<td class="line x" title="98:177	34 (13) When Mr. Green won a $240,000 verdict in a land condemnation case against the state in June 1983, he says Judge OKicki unexpectedly awarded him an additional $100,000." ></td>
	<td class="line x" title="99:177	(0267) REL Arg1 Arg2 [Source] Wr Ot Inh [Type] Comm Comm Null In the absence of an independent occurrence of attribution on an argument, as in Arg2 of Example (13), the Null value is used for the type on the argument, meaning that it needs to be derived by independent (here, undefined) considerations under the scope of the relation." ></td>
	<td class="line x" title="100:177	Note that unlike the Inh value of the source feature, Null does not indicate inheritance." ></td>
	<td class="line x" title="101:177	In a subordinate clause, for example, while the relation denoted by the subordinating conjunction may be asserted, the clause content itself may be presupposed, as seems to be the case for the relation and Arg2 of (13)." ></td>
	<td class="line x" title="102:177	However, we found these differences difficult to determine at times, and consequently leave this undefined in the current scheme." ></td>
	<td class="line x" title="103:177	Beliefs are identified by propositional attitude verbs (Hintikka, 1971) such as believe, think, expect, suppose, imagine, etc. They take the value PAtt (for Propostional Attitude)." ></td>
	<td class="line x" title="104:177	An example of a belief attribution is given in (14)." ></td>
	<td class="line x" title="105:177	(14) Mr. Marcus believes spot steel prices will continue to fall through early 1990 and then reverse themselves." ></td>
	<td class="line x" title="106:177	(0336) REL Arg1 Arg2 [Source] Ot Inh Inh [Type] PAtt Null Null Facts are identified by the class of factive and semi-factive verbs (Kiparsky and Kiparsky, 1971; Karttunen, 1971) such as regret, forget, remember, know, see, hear etc. They take the value Ftv (for Factive) for type (Example 15)." ></td>
	<td class="line x" title="107:177	In the current scheme, this class does not distinguish between the true factives and semi-factives, the former involving an attitute/evaluation towards a fact, and the latter involving knowledge of a fact." ></td>
	<td class="line x" title="108:177	(15) The other side, he argues knows Giuliani has always been pro-choice, even though he has personal reservations." ></td>
	<td class="line x" title="109:177	(0041) REL Arg1 Arg2 [Source] Ot Inh Inh [Type] Ftv Null Null Lastly, eventualities are identified by a class of verbs which denote three kinds of relations between agents and eventualities (Sag and Pollard, 1991)." ></td>
	<td class="line x" title="110:177	The first kind is anchored by verbs of influence like persuade, permit, order, and involve one agent influencing another agent to perform (or not perform) an action." ></td>
	<td class="line x" title="111:177	The second kind is anchored by verbs of commitment like promise, agree, try, intend, refuse, decline, and involve an agent committing to perform (or not perform) an action." ></td>
	<td class="line x" title="112:177	Finally, the third kind is anchored by verbs of orientation like want, expect, wish, yearn, and involve desire, expectation, or some similar mental orientation towards some state(s) of affairs." ></td>
	<td class="line x" title="113:177	These sub-distinctions are not encoded in the annotation, but we have used the definitions as a guide for identifying these predicates." ></td>
	<td class="line x" title="114:177	All these three types are collectively referred to and annotated as verbs of control." ></td>
	<td class="line x" title="115:177	Type for these classes takes the value Ctrl (for Control)." ></td>
	<td class="line x" title="116:177	Note that the syntactic term control is used because these verbs denote uniform structural control properties, but the primary basis for their definition is nevertheless semantic." ></td>
	<td class="line x" title="117:177	An example of the control attribution relation anchored by a verb of influence is given in (16)." ></td>
	<td class="line x" title="118:177	(16) Eward and Whittington had planned to leave the bank earlier, but Mr. Craven had persuaded them to remain until the bank was in a healthy position." ></td>
	<td class="line x" title="119:177	(1949) REL Arg1 Arg2 [Source] Ot Inh Inh [Type] Ctrl Null Null Note that while our use of the term source applies literally to agents responsible for the truth of a proposition, we continue to use the same term for the agents for facts and eventualities." ></td>
	<td class="line x" title="120:177	Thus, for facts, the source represents the bearers of attitudes/knowledge, and for considered eventualities, the source represents intentions/attitudes." ></td>
	<td class="line x" title="121:177	3.3 Scopal Polarity The scopal polarity feature is annotated on relations and their arguments to primarily identify cases when verbs of attribution are negated on the surface syntactically (e.g. , didnt say, dont think) or lexically (e.g. , denied), but when the negation in fact reverses the polarity of the attributed relation or argument content (Horn, 1978)." ></td>
	<td class="line x" title="122:177	Example (17) illustrates such a case." ></td>
	<td class="line x" title="123:177	The but clause entails an interpretation such as I think its not a main consideration, for which the negation must take narrow scope over the embedded clause rather than the higher clause." ></td>
	<td class="line x" title="124:177	In particular, the interpretation of the CONTRAST relation denoted by but requires that Arg2 should be interpreted under the scope of negation." ></td>
	<td class="line x" title="125:177	35 (17) Having the dividend increases is a supportive element in the market outlook, but I dont think its a main consideration, he says." ></td>
	<td class="line x" title="126:177	(0090) REL Arg1 Arg2 [Source] Ot Inh Inh [Type] Comm Null PAtt [Polarity] Null Null Neg To capture such entailments with surface negations on attribution verbs, an argument of a connective is marked Neg for scopal polarity when the interpretation of the connective requires the surface negation to take semantic scope over the lower argument." ></td>
	<td class="line x" title="127:177	Thus, in Example (17), scopal polarity is marked as Neg for Arg2." ></td>
	<td class="line x" title="128:177	When the neg-lowered interpretations are not present, scopal polarity is marked as the default Null (such as for the relation and Arg1 of Example 17)." ></td>
	<td class="line x" title="129:177	It is also possible for the surface negation of attribution to be interpreted as taking scope over the relation, rather than an argument." ></td>
	<td class="line x" title="130:177	We have not observed this in the corpus yet, so we describe this case with the constructed example in (18)." ></td>
	<td class="line x" title="131:177	What the example shows is that in addition to entailing (18b) in which case it would be annotated parallel to Example (17) above (18a) can also entail (18c), such that the negation is intrepreted as taking semantic scope over the relation (Lasnik, 1975), rather than one of the arguments." ></td>
	<td class="line x" title="132:177	As the scopal polarity annotations for (18c) show, lowering of the surface negation to the relation is marked as Neg for the scopal polarity of the relation." ></td>
	<td class="line x" title="133:177	(18) a. John doesnt think Mary will get cured because she took the medication." ></td>
	<td class="line x" title="134:177	b. a7a8 John thinks that because Mary took the medication, she will not get cured." ></td>
	<td class="line x" title="135:177	REL Arg1 Arg2 [Source] Ot Inh Inh [Type] PAtt Null Null [Polarity] Null Neg Null c. a7a8 John thinks that Mary will get cured not because she took the medication (but because she has started practising yoga.)" ></td>
	<td class="line x" title="136:177	REL Arg1 Arg2 [Source] Ot Inh Inh [Type] PAtt Null Null [Polarity] Neg Null Null We note that scopal polarity does not capture the appearance of (opaque) internal negation that may appear on arguments or relations themselves." ></td>
	<td class="line x" title="137:177	For example, a modified connective such as not because does not take Neg as the value for scopal polarity, but rather Null." ></td>
	<td class="line x" title="138:177	This is consistent with our goal of marking scopal polarity only for lowered negation, i.e., when surface negation from the attribution is lowered to either the relation or argument for interpretation." ></td>
	<td class="line x" title="139:177	3.4 Determinacy The determinacy feature captures the fact that the entailment of the attribution relation can be made indeterminate in context, for example when it appears syntactically embedded in negated or conditional contexts The annotation attempts to capture such indeterminacy with the value Indet." ></td>
	<td class="line x" title="140:177	Determinate contexts are simply marked as the default Null." ></td>
	<td class="line x" title="141:177	For example, the annotation in (19) conveys the idea that the belief or opinion about the effect of higher salaries on teachers performance is not really attributed to anyone, but is rather only being conjectured as a possibility." ></td>
	<td class="line x" title="142:177	(19) It is silly libel on our teachers to think they would educate our children better if only they got a few thousand dollars a year more." ></td>
	<td class="line x" title="143:177	(1286) REL Arg1 Arg2 [Source] Ot Inh Inh [Type] PAtt Null Null [Polarity] Null Null Null [Determinacy] Indet Null Null 3.5 Attribution Spans In addition to annotating the properties of attribution in terms of the features discussed above, we also propose to annotate the text span associated with the attribution." ></td>
	<td class="line x" title="144:177	The text span is annotated as a single (possibly discontinuous) complex reflecting three of the annotated features, namely source, type and scopal polarity." ></td>
	<td class="line x" title="145:177	The attribution span also includes all non-clausal modifiers of the elements contained in the span, for example, adverbs and appositive NPs." ></td>
	<td class="line x" title="146:177	Connectives, however, are excluded from the span, even though they function as modifiers." ></td>
	<td class="line x" title="147:177	Example (20) shows a discontinuous annotation of the attribution, where the parenthetical he argues is excluded from the attribution phrase the other side knows, corresponding to the factive attribution." ></td>
	<td class="line x" title="148:177	(20) The other side, he argues knows Giuliani has always been pro-choice, even though he has personal reservations." ></td>
	<td class="line x" title="149:177	(0041) REL Arg1 Arg2 [Source] Ot Inh Inh [Type] Ftv Null Null [Polarity] Null Null Null [Determinacy] Null Null Null Inclusion of the fourth feature, determinacy, is not required to be included in the current scheme because the entailment cancelling contexts 36 cana9 be very complex." ></td>
	<td class="line x" title="150:177	For example, in Example (19), the conditional interpretation leading to the indeterminacy of the relation and its arguments is due to the syntactic construction type of the entire sentence." ></td>
	<td class="line x" title="151:177	It is not clear how to annotate the indeterminacy induced by such contexts." ></td>
	<td class="line x" title="152:177	In the example, therefore, the attribution span only includes the anchor for the type of the attribution." ></td>
	<td class="line x" title="153:177	Spans for implicit writer attributions are left unmarked since there is no corresponding text that can be selected." ></td>
	<td class="line x" title="154:177	The absence of a span annotation is simply taken to reflect writer attribution, together with the Wr value on the source feature." ></td>
	<td class="line x" title="155:177	Recognizing attributions is not trivial since they are often left unexpressed in the sentence in which the AO is realized, and have to be inferred from the prior discourse." ></td>
	<td class="line x" title="156:177	For example, in (21), the relation together with its arguments in the third sentence are attributed to Larry Shapiro, but this attribution is implicit and must be inferred from the first sentence." ></td>
	<td class="line x" title="157:177	(21) There are certain cult wines that can command these higher prices, says Larry Shapiro of Martys, a4a5a4a5a4 Whats different is that it is happening with young wines just coming out." ></td>
	<td class="line x" title="158:177	Were seeing it partly because older vintages are growing more scarce. (0071) REL Arg1 Arg2 [Source] Ot Inh Inh The spans for such implicit Ot attributions mark the text that provides the inference of the implicit attribution, which is just the closest occurrence of the explicit attribution phrase in the prior text." ></td>
	<td class="line x" title="159:177	The final aspect of the span annotation is that we also annotate non-clausal phrases as the anchors attribution, such as prepositional phrases like according to X, and adverbs like reportedly, allegedly, supposedly." ></td>
	<td class="line x" title="160:177	One such example is shown in (22)." ></td>
	<td class="line x" title="161:177	(22) No foreign companies bid on the Hiroshima project, according to the bureau . But the Japanese practice of deep discounting often is cited by Americans as a classic barrier to entry in Japans market." ></td>
	<td class="line x" title="162:177	(0501) REL Arg1 Arg2 [Source] Wr Ot Inh [Type] Comm Comm Null [Polarity] Null Null Null [Determinacy] Null Null Null Note that adverbials are free to pick their own type of attribution." ></td>
	<td class="line x" title="163:177	For example, supposedly as an attribution adverb picks PAtt as the value for type." ></td>
	<td class="line x" title="164:177	3.6 Attribution of Implicit Relations Implicit connectives and their arguments in the PDTB are also marked for attribution." ></td>
	<td class="line x" title="165:177	Implicit connectives express relations that are inferred by the reader." ></td>
	<td class="line x" title="166:177	In such cases, the writer intends for the reader to infer a discourse relation." ></td>
	<td class="line x" title="167:177	As with Explicit connectives, implicit relations intended by the writer of the article are distinguished from those intended by some other agent introduced by the writer." ></td>
	<td class="line x" title="168:177	For example, while the implicit relation in Example (23) is attributed to the writer, in Example (24), both Arg1 and Arg2 have been expressed by someone else whose speech is being quoted: in this case, the implicit relation is attributed to the other agent." ></td>
	<td class="line x" title="169:177	(23) The gruff financier recently started socializing in upper-class circles." ></td>
	<td class="line x" title="170:177	Implicit = FOR EXAMPLE (ADD.INFO) Although he says he wasnt keen on going, last year he attended a New York gala where his daughter made her debut." ></td>
	<td class="line x" title="171:177	(0800) REL Arg1 Arg2 [Source] Wr Inh Inh [Type] Comm Null Null [Polarity] Null Null Null [Determinacy] Null Null Null (24) We asked police to investigate why they are allowed to distribute the flag in this way." ></td>
	<td class="line x" title="172:177	Implicit=BECAUSE (CAUSE) It should be considered against the law, said Danny Leish, a spokesman for the association . REL Arg1 Arg2 [Source] Ot Inh Inh [Type] Comm Null Null [Polarity] Null Null Null [Determinacy] Null Null Null For implicit relations, attribution is also annotated for AltLex relations but not for EntRel and NoRel, since the former but not the latter refer to the presense of discourse relations." ></td>
	<td class="line x" title="173:177	4 Summary In this paper, we have proposed and described an annotation scheme for marking the attribution of both explicit and implicit discourse connectives and their arguments in the Penn Discourse TreeBank." ></td>
	<td class="line x" title="174:177	We discussed the role of the annotations for the recognition of factuality in natural language applications, and defined the notion of attribution." ></td>
	<td class="line x" title="175:177	The scheme was presented in detail with examples, outlining the feature-based annotation in terms of the source, type, scopal polarity, and determinacy associated with attribution, and the span annotation to highlight the text reflecting the attribution features." ></td>
	<td class="line x" title="176:177	37 Acknoa10 wledgements The Penn Discourse TreeBank project is partially supported by NSF Grant: Research Resources, EIA 02-24417 to the University of Pennsylvania (PI: A. Joshi)." ></td>
	<td class="line x" title="177:177	We are grateful to Lukasz Abramowicz and the anonymous reviewers for useful comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-0306
Searching For Sentences Expressing Opinions By Using Declaratively Subjective Clues
Hiroshima, Nobuaki;Yamada, Setsuo;Furuse, Osamu;Kataoka, Ryoji;"></td>
	<td class="line x" title="1:190	Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 3946, Sydney, July 2006." ></td>
	<td class="line x" title="2:190	c2006 Association for Computational Linguistics Searching for Sentences Expressing Opinions by using Declaratively Subjective Clues Nobuaki Hiroshima, Setsuo Yamada, Osamu Furuse and Ryoji Kataoka NTT Cyber Solutions Laboratories, NTT Corporation 1-1 Hikari-no-oka Yokosuka-Shi Kanagawa, 239-0847 Japan hiroshima.nobuaki@lab.ntt.co.jp Abstract This paper presents a method for searching the web for sentences expressing opinions." ></td>
	<td class="line x" title="3:190	To retrieve an appropriate number of opinions that users may want to read, declaratively subjective clues are used to judge whether a sentence expresses an opinion." ></td>
	<td class="line x" title="4:190	We collected declaratively subjective clues in opinionexpressing sentences from Japanese web pages retrieved with opinion search queries." ></td>
	<td class="line x" title="5:190	These clues were expanded with the semantic categories of the words in the sentences and were used as feature parameters in a Support Vector Machine to classify the sentences." ></td>
	<td class="line x" title="6:190	Our experimental results using retrieved web pages on various topics showed that the opinion expressing sentences identified by the proposed method are congruent with sentences judged by humans to express opinions." ></td>
	<td class="line x" title="7:190	1 Introduction Readers have an increasing number of opportunities to read opinions (personal ideas or beliefs), feelings (mental states), and sentiments (positive or negative judgments) that have been written or posted on web pages such as review sites, personal web sites, blogs, and BBSes." ></td>
	<td class="line x" title="8:190	Such subjective information on the web can often be a useful basis for finding out what people think about a particular topic or making a decision." ></td>
	<td class="line x" title="9:190	A number of studies on automatically extracting and analyzing product reviews or reputations on the web have been conducted (Dave et al. , 2003; Morinaga et al. , 2002; Nasukawa and Yi, 2003; Tateishi et al. , 2004; Kobayashi et al. , 2004)." ></td>
	<td class="line x" title="10:190	These studies focus on using sentiment analysis to extract positive or negative information about a particular product." ></td>
	<td class="line x" title="11:190	Different kinds of subjective information, such as neutral opinions, requests, and judgments, which are not explicitly associated with positive/negative assessments, have not often been considered in previous work." ></td>
	<td class="line x" title="12:190	Although sentiments provide useful information, opinion-expressing sentences like In my opinion this product should be priced around $15, which do not express explicitly positive or negative judgments (unlike sentiments) can also be informative for a user who wants to know others opinions about a product." ></td>
	<td class="line x" title="13:190	When a user wants to collect opinions about an event, project, or social phenomenon, requests and judgments can be useful as well as sentiments." ></td>
	<td class="line x" title="14:190	With open-domain topics, sentences expressing sentiments should not be searched exclusively; other kinds of opinion expressing sentences should be searched as well." ></td>
	<td class="line x" title="15:190	The goal of our research is to achieve a web search engine that locates opinion-expressing sentences about open-domain topics on products, persons, events, projects, and social phenomena." ></td>
	<td class="line x" title="16:190	Sentence-level subjectivity/objectivity classification in some of the previous research (Riloff and Wiebe, 2003; Wiebe and Riloff, 2005) can identify subjective statements that include speculation in addition to positive/negative evaluations." ></td>
	<td class="line x" title="17:190	In these efforts, the subjectivity/objectivity of a current sentence is judged based on the existence of subjective/objective clues in both the sentence itself and the neighboring sentences." ></td>
	<td class="line x" title="18:190	The subjective clues, some adjective, some noun, and some verb phrases, as well as other collocations, are learned from corpora (Wiebe, 2000; Wiebe et al. , 2001)." ></td>
	<td class="line x" title="19:190	Some of the clues express subjective meaning unrestricted to positive/negative measurements." ></td>
	<td class="line x" title="20:190	The sentence-level subjectivity ap39 proach suggests a way of searching for opinion expressing sentences in the open domain." ></td>
	<td class="line x" title="21:190	The problem of applying sentence-level subjectivity classification to opinion-expressing sentence searches is the likelihood of collecting too many sentences for a user to read." ></td>
	<td class="line x" title="22:190	According to the work of Wiebe et al.(2001), 70% of sentences in opinion-expressing articles like editorials and 44% of sentences in non-opinion expressing articles like news reports were judged to be subjective." ></td>
	<td class="line oc" title="24:190	In analyzing opinions (Cardie et al. , 2003; Wilson et al. , 2004), judging document-level subjectivity (Pang et al. , 2002; Turney, 2002), and answering opinion questions (Cardie et al. , 2003; Yu and Hatzivassiloglou, 2003), the output of a sentence-level subjectivity classification can be used without modification." ></td>
	<td class="line x" title="25:190	However, in searching opinion-expressing sentences, it is necessary to designate criteria for opinion-expressing sentences that limit the number of retrieved sentences so that a user can survey them without difficulty." ></td>
	<td class="line x" title="26:190	While it is difficult to formally define an opinion, it is possible to practically tailor the definition of an opinion to the purpose of the application (Kim and Hovy, 2004)." ></td>
	<td class="line x" title="27:190	This study introduces the notion of declaratively subjective clues as a criterion for judging whether a sentence expresses an opinion and proposes a method for finding opinionexpressing sentences that uses these clues." ></td>
	<td class="line x" title="28:190	Declaratively subjective clues such as the subjective predicate part of the main clause and subjective sentential adverb phrases suggest that the writer is the source of the opinion." ></td>
	<td class="line x" title="29:190	We hypothesize that a user of such an opinion-expressing sentence search wants to read the writers opinions and that explicitly stated opinions are preferred over quoted or implicational opinions." ></td>
	<td class="line x" title="30:190	We suppose that writers ideas or beliefs are explicitly declared in a sentence with declaratively subjective clues whereas sentences without declaratively subjective clues mainly describe things." ></td>
	<td class="line x" title="31:190	The number of sentences with declaratively subjective clues is estimated to be less than the number of subjective sentences defined in the previous work." ></td>
	<td class="line x" title="32:190	We expect that the opinion expressing sentences identified with our method will be appropriate from the both qualitative and quantitative viewpoints." ></td>
	<td class="line x" title="33:190	Section 2 describes declaratively subjective clues and explains how we collected them from opinion-expressing sentences on Japanese web pages retrieved with opinion search queries." ></td>
	<td class="line x" title="34:190	Section 3 explains our strategy for searching opinion-expressing sentences by using declaratively subjective clues." ></td>
	<td class="line x" title="35:190	Section 4 evaluates the proposed method and shows how the opinionexpressing sentences found by the proposed method are congruent with the sentences judged by humans to be opinions." ></td>
	<td class="line x" title="36:190	2 Declaratively Subjective Clues Declaratively subjective clues are a basic criterion for judging whether a sentence expresses an opinion." ></td>
	<td class="line x" title="37:190	We extracted the declaratively subjective clues from Japanese sentences that evaluators judged to be opinions." ></td>
	<td class="line x" title="38:190	2.1 Opinion-expressing Sentence Judgment We regard a sentence to be opinion expressing if it explicitly declares the writers idea or belief at a sentence level." ></td>
	<td class="line x" title="39:190	We define as a declaratively subjective clue, the part of a sentence that contributes to explicitly conveying the writers idea or belief in the opinion-expressing sentence." ></td>
	<td class="line x" title="40:190	For example, 'I am glad' in the sentence 'I am glad to see you' can convey the writers pleasure to a reader, so we regard the sentence as an opinionexpressing sentence and I am glad as a declaratively subjective clue. Another example of a declaratively subjective clue is the exclamation mark in the sentence 'We got a contract'!" ></td>
	<td class="line x" title="41:190	It conveys the writers emotion about the event to a reader." ></td>
	<td class="line x" title="42:190	If a sentence only describes something abstract or concrete even though it has word-level or phrase-level subjective parts, we do not consider it to be opinion expressing." ></td>
	<td class="line x" title="43:190	On the other hand, some word-level or phrase-level subjective parts can be declaratively subjective clues depending on where they occur in the sentence." ></td>
	<td class="line x" title="44:190	Consider the following two sentences." ></td>
	<td class="line x" title="45:190	(1) This house is beautiful." ></td>
	<td class="line x" title="46:190	(2) We purchased a beautiful house." ></td>
	<td class="line x" title="47:190	Both (1) and (2) contain the word-level subjective part 'beautiful'." ></td>
	<td class="line x" title="48:190	Our criterion would lead us to say that sentence (1) is an opinion, because 'beautiful' is placed in the predicate part and (1) is considered to declare the writers evaluation of the house to a reader." ></td>
	<td class="line x" title="49:190	This is why beautiful in (1) is eligible as a declaratively subjective clue." ></td>
	<td class="line x" title="50:190	On the other hand, sentence (2) is not judged to contain an opinion, because 'beautiful' is placed in the noun phrase, i.e., the object of the verb purchase, and (2) is considered to report the event of the house purchase rather ob40 jectively to a reader." ></td>
	<td class="line x" title="51:190	Sentence (2) partially contains subjective information about the beauty of the house; however this information is unlikely to be what a writer wants to emphasize." ></td>
	<td class="line x" title="52:190	Thus, 'beautiful' in (2) does not work as a declaratively subjective clue." ></td>
	<td class="line x" title="53:190	These two sentences illustrate the fact that the presence of a subjective word (beautiful) does not unconditionally assure that the sentence expresses an opinion." ></td>
	<td class="line x" title="54:190	Additionally, these examples do suggest that sentences containing an opinion can be judged depending on where such wordlevel or phrase-level subjective parts as evaluative adjectives are placed in the predicate part." ></td>
	<td class="line x" title="55:190	Some word-level or phrase-level subjective parts such as subjective sentential adverbs can be declaratively subjective clues depending on where they occur in the sentence." ></td>
	<td class="line x" title="56:190	In sentence (3), amazingly expresses the writers feeling about the event." ></td>
	<td class="line x" title="57:190	Sentence (3) is judged to contain an opinion because there is a subjective sentential adverb in its main clause." ></td>
	<td class="line x" title="58:190	(3) Amazingly, few people came to my party." ></td>
	<td class="line x" title="59:190	The existence of some idiomatic collocations in the main clause also affects our judgment as to what constitutes an opinion-expressing sentence." ></td>
	<td class="line x" title="60:190	For example, sentence (4) can be judged as expressing an opinion because it includes my wish is." ></td>
	<td class="line x" title="61:190	(4) My wish is to go abroad." ></td>
	<td class="line x" title="62:190	Thus, depending on the type of declaratively subjective clue, it is necessary to consider where the expression is placed in the sentence to judge whether the sentence is an opinion." ></td>
	<td class="line x" title="63:190	2.2 Clue Expression Collection We collected declaratively subjective clues in opinion-expressing sentences from Japanese web pages." ></td>
	<td class="line x" title="64:190	Figure 1 illustrates the flow of collection of eligible expressions." ></td>
	<td class="line x" title="65:190	type querys topic Product cell phone, car, beer, cosmetic Entertainment sports, movie, game, animation Facility museum, zoo, hotel, shop Politics diplomacy, election Phenomena diction, social behavior Event firework, festival Culture artwork, book, music Organization company Food cuisine, noodle, ice cream Creature bird Table 1: Topic Examples First, we retrieved Japanese web pages from forty queries covering a wide range of topics such as products, entertainment, facilities, and phenomena, as shown in Table 1." ></td>
	<td class="line x" title="66:190	We used queries on various topics because we wanted to acquire declaratively subjective clues for opendomain opinion web searches." ></td>
	<td class="line x" title="67:190	Most of the queries contain proper nouns." ></td>
	<td class="line x" title="68:190	These queries correspond to possible situations in which a user wants to retrieve opinions from web pages about a particular topic, such as Cell phone X, Y museum, and Football coach Zs ability, where X, Y, and Z are proper nouns." ></td>
	<td class="line x" title="69:190	Next, opinion-expressing sentences were extracted from the top twenty retrieved web pages in each query, 800 pages in total." ></td>
	<td class="line x" title="70:190	There were 75,575 sentences in these pages." ></td>
	<td class="line x" title="71:190	Figure 1: Flow of Clue Expression Collection 41 Three evaluators judged whether each sentence contained an opinion or not." ></td>
	<td class="line x" title="72:190	The 13,363 sentences judged to do so by all three evaluators were very likely to be opinion expressing." ></td>
	<td class="line x" title="73:190	The number of sentences which three evaluators agreed on as non-opinion expressing was 42,346." ></td>
	<td class="line x" title="74:190	1 Out of the 13,363 opinion expressing sentences, 8,425 were then used to extract declaratively subjective clues and learn positive examples in a Support Vector Machine (SVM), and 4,938 were used to assess the performance of opinion expressing sentence search (Section 4)." ></td>
	<td class="line x" title="75:190	Out of the 42,346 non-opinion sentences, 26,340 were used to learn negative examples, and 16,006 were used to assess, keeping the number ratio of the positive and negative example sentences in learning and assessing." ></td>
	<td class="line x" title="76:190	One analyst extracted declaratively subjective clues from 8,425 of the 13,363 opinionexpressing sentences, and another analyst checked the result." ></td>
	<td class="line x" title="77:190	The number of declaratively 1 Note that not all of these opinion-expressing sentences retrieved were closely related to the query because some of the pages described miscellaneous topics." ></td>
	<td class="line x" title="78:190	subjective clues obtained was 2,936." ></td>
	<td class="line x" title="79:190	These clues were classified into fourteen types as shown in Table 2, where the underlined expressions in example sentences are extracted as declaratively subjective clues." ></td>
	<td class="line x" title="80:190	The example sentences in Table 2 are Japanese opinion-expressing sentences and their English translations." ></td>
	<td class="line x" title="81:190	Although some English counterparts of Japanese clue expressions might not be cogent because of the characteristic difference between Japanese and English, the clue types are likely to be language-independent." ></td>
	<td class="line x" title="82:190	We can see that various types of expressions compose opinion-expressing sentences." ></td>
	<td class="line x" title="83:190	As mentioned in Section 2.1, it is important to check where a declaratively subjective clue appears in the sentence in order to apply our criterion of whether the sentence is an opinion or not." ></td>
	<td class="line x" title="84:190	The clues in the types other than (b), (c) and (l) usually appear in the predicate part of a main clause." ></td>
	<td class="line x" title="85:190	The declaratively subjective clues in Japanese examples are placed in the rear parts of sentences except in types (b), (c) and (l)." ></td>
	<td class="line x" title="86:190	This reflects the heuristic rule that Japanese predicate type example sentence (English translation of Japanese sentence) (a) Thought Kono hon wa kare no dato omou." ></td>
	<td class="line x" title="87:190	(I think this book is his)." ></td>
	<td class="line x" title="88:190	(b) Declarative adverb Tabun rainen yooroppa ni iku." ></td>
	<td class="line x" title="89:190	(I will possibly go to Europe next year)." ></td>
	<td class="line x" title="90:190	(c) Interjection Waa, suteki." ></td>
	<td class="line x" title="91:190	(Oh, wonderful)." ></td>
	<td class="line x" title="92:190	(d) Intensifier Karera wa totemo jouzu ni asonda." ></td>
	<td class="line x" title="93:190	(They played extremely well) (e) Impression Kono yougo wa yayakoshii." ></td>
	<td class="line x" title="94:190	(This terminology is confusing)." ></td>
	<td class="line x" title="95:190	(f) Emotion Oai dekite ureshii desu." ></td>
	<td class="line x" title="96:190	(I am glad to see you)." ></td>
	<td class="line x" title="97:190	(g) Positive/negative judgment Anata no oodio kiki wa sugoi." ></td>
	<td class="line x" title="98:190	(Your audio system is terrific)." ></td>
	<td class="line x" title="99:190	(h) Modality about propositional attitude Sono eiga wo miru beki da." ></td>
	<td class="line x" title="100:190	(You should go to the movie)." ></td>
	<td class="line x" title="101:190	(i) Value judgment Kono bun wa imi fumei da." ></td>
	<td class="line x" title="102:190	(This sentence makes no sense)." ></td>
	<td class="line x" title="103:190	(j) Utterance-specific sentence form Towa ittemo,ima wa tada no yume dakedo." ></td>
	<td class="line x" title="104:190	(Though, it's literally just a dream now)." ></td>
	<td class="line x" title="105:190	(k) Symbol Keiyaku wo tottazo!" ></td>
	<td class="line x" title="106:190	(We got a contract!)" ></td>
	<td class="line x" title="107:190	(l) Idiomatic collocation Ii nikui." ></td>
	<td class="line x" title="108:190	(It's hard to say)." ></td>
	<td class="line x" title="109:190	(m) Uncertainty Ohiru ni nani wo tabeyou kanaa." ></td>
	<td class="line x" title="110:190	(I am wondering what I should eat for lunch)." ></td>
	<td class="line x" title="111:190	(n) Imperative Saizen wo tukushi nasai." ></td>
	<td class="line x" title="112:190	(Give it your best)." ></td>
	<td class="line x" title="113:190	Table 2: Clue Types 42 parts are in principle placed in the rear part of a sentence." ></td>
	<td class="line x" title="114:190	3 Opinion-Sentence Extraction In this section, we explain the method of classifying each sentence by using declaratively subjective clues." ></td>
	<td class="line x" title="115:190	The simplest method for automatically judging whether a sentence is an opinion is a rule-based one that extracts sentences that include declaratively subjective clues." ></td>
	<td class="line x" title="116:190	However, as mentioned in Section 2, the existence of declaratively subjective clues does not assure that the sentence expresses an opinion." ></td>
	<td class="line x" title="117:190	It is a daunting task to write rules that describe how each declaratively subjective clue should appear in an opinionexpressing sentence." ></td>
	<td class="line x" title="118:190	A more serious problem is that an insufficient collection of declaratively subjective clues will lead to poor extraction performance." ></td>
	<td class="line x" title="119:190	For that reason, we adopted a learning method that binarily classifies sentences by using declaratively subjective clues and their positions in sentences as feature parameters of an SVM." ></td>
	<td class="line x" title="120:190	With this method, a consistent framework of classification can be maintained even if we add new declaratively subjective clues, and it is possible that we can extract the opinion-expressing sentences which have unknown declaratively subjective clues." ></td>
	<td class="line x" title="121:190	3.1 Augmentation by Semantic Categories Before we can use declaratively subjective clues as feature parameters, we must address two issues:  Cost of building a corpus: It is costly to provide a sufficient amount of tagged corpus of opinion-expressing-sentence labels to ensure that learning achieves a high-performance extraction capability." ></td>
	<td class="line x" title="122:190	 Coverage of words co-occurring with declaratively subjective clues: Many of the declaratively subjective clue expressions have co-occurring words in the opinion-expressing sentence." ></td>
	<td class="line x" title="123:190	Consider the following two sentences." ></td>
	<td class="line x" title="124:190	(5) The sky is high." ></td>
	<td class="line x" title="125:190	(6) The quality of this product is high." ></td>
	<td class="line x" title="126:190	Both (5) and (6) contain the word 'high' in the predicate part." ></td>
	<td class="line x" title="127:190	Sentence (5) is considered to be less of an opinion than (6) because an evaluator might judge (5) to be the objective truth, while all evaluators are likely to judge (6) to be an opinion." ></td>
	<td class="line x" title="128:190	The adjective 'high' in the predicate part can be validated as a declaratively subjective clue depending on co-occurring words." ></td>
	<td class="line x" title="129:190	However, it is not realistic to provide all possible co-occurring words with each declaratively subjective clue expression." ></td>
	<td class="line x" title="130:190	Semantic categories can be of help in dealing with the above two issues." ></td>
	<td class="line x" title="131:190	Declaratively subjective clue expressions can be augmented by semantic categories of the words in the expressions." ></td>
	<td class="line x" title="132:190	An augmentation involving both declaratively subjective clues and co-occurrences will increase feature parameters." ></td>
	<td class="line x" title="133:190	In our implementation, we adopted the semantic categories proposed by Ikehara et al.(1997)." ></td>
	<td class="line x" title="135:190	Utilization of semantic categories has another effect: it improves the extraction performance." ></td>
	<td class="line x" title="136:190	Consider the following two sentence patterns: (7) X is beautiful." ></td>
	<td class="line x" title="137:190	(8) X is pretty." ></td>
	<td class="line x" title="138:190	The words 'beautiful' and 'pretty' are adjectives in the common semantic category, 'appearance', and the degree of declarative subjectivity of these sentences is almost the same regardless of what X is. Therefore, even if 'beautiful' is learned as a declaratively subjective clue but 'pretty' is not, the semantic category 'appearance' that the learned word 'beautiful' belongs to, enables (8) to be judged opinion expressing as well as (7)." ></td>
	<td class="line x" title="139:190	3.2 Feature Parameters to Learn We implemented our opinion-sentence extraction method by using a Support Vector Machine (SVM) because an SVM can efficiently learn the model for classifying sentences into opinionexpressing and non-opinion expressing, based on the combinations of multiple feature parameters." ></td>
	<td class="line x" title="140:190	The following are the crucial feature parameters of our method." ></td>
	<td class="line x" title="141:190	 2,936 declaratively subjective clues  2,715 semantic categories that words in a sentence can fall into If the sentence has a declaratively subjective clue of type (b), (c) or (l) in Table 2, the feature parameter about the clue is assigned a value of 1; if not, it is assigned 0." ></td>
	<td class="line x" title="142:190	If the sentence has declaratively subjective clues belonging to types 43 other than (b), (c) or (l) in the predicate part, the feature parameter about the clue is assigned 1; if not, it is assigned 0." ></td>
	<td class="line x" title="143:190	The feature parameters for the semantic category are used to compensate for the insufficient amount of declaratively subjective clues provided and to consider co-occurring words with clue expressions in the opinion-expressing sentences, as mentioned in Section 3.1." ></td>
	<td class="line x" title="144:190	The following are additional feature parameters." ></td>
	<td class="line x" title="145:190	 150 frequent words  13 parts of speech Each feature parameter is assigned a value of 1 if the sentence has any of the frequent words or parts of speech." ></td>
	<td class="line x" title="146:190	We added these feature parameters based on the hypotheses that some frequent words in Japanese have the function of changing the degree of declarative subjectivity, and that the existence of such parts of speech as adjectives and adverbs possibly influences the declarative subjectivity." ></td>
	<td class="line x" title="147:190	The effectiveness of these additional feature parameters was confirmed in our preliminary experiment." ></td>
	<td class="line x" title="148:190	4 Experiments We conducted three experiments to assess the validity of the proposed method: comparison with baseline methods, effectiveness of position information in SVM feature parameters, and effectiveness of SVM feature parameters such as declaratively subjective clues and semantic categories." ></td>
	<td class="line x" title="149:190	All experiments were performed using the Japanese sentences described in Section 2.1." ></td>
	<td class="line x" title="150:190	We used 8,425 opinion expressing sentences, which were used to collect declaratively subjective clues as a training set, and used 4,938 opinionexpressing sentences as a test set." ></td>
	<td class="line x" title="151:190	We also used 26,340 non-opinion sentences as a training set and used 16,006 non-opinion sentences as a test set." ></td>
	<td class="line x" title="152:190	The test set was divided into ten equal subsets." ></td>
	<td class="line x" title="153:190	The experiments were evaluated with the following measures following the variable scheme in Table 3: ba a P op + = ca a R op + = opop opop op RP RP F + = 2 dc d P opno + = _ db d R opno + = _ opnoopno opnoopno opno RP RP F __ __ _ 2 + = dcba da A +++ + = We evaluated ten subsets with the above measures and took the average of these results." ></td>
	<td class="line x" title="154:190	4.1 Comparison with Baseline Methods We first performed an experiment comparing two baseline methods with our proposed method." ></td>
	<td class="line x" title="155:190	We prepared a baseline method that regards a sentence as an opinion if it contains a number of declaratively subjective clues that exceeds a certain threshold." ></td>
	<td class="line x" title="156:190	The best threshold was set through trial and error at five occurrences." ></td>
	<td class="line x" title="157:190	We also prepared another baseline method that learns a model and classifies a sentence using only features about a bag of words." ></td>
	<td class="line x" title="158:190	The experimental results are shown in Table 4." ></td>
	<td class="line x" title="159:190	It can be seen that our method performs better than the two baseline methods." ></td>
	<td class="line x" title="160:190	Though the difference between our methods results and those of the bag-of-words method seems rather small, the superiority of the proposed method cannot be rejected at the significance level of 5% in t-test." ></td>
	<td class="line x" title="161:190	Answer System Opinion No opinion Opinion a b No opinion c d Opinion No opinion Method Precision Recall F-measure Precision Recall F-measure Accuracy Occurrences of DS clues (baseline 1) 66.4% 35.3% 46.0% 82.6% 94.5% 88.1% 80.5% Bag of words (baseline 2) 80.9% 64.2% 71.6% 89.6% 95.3% 92.4% 88.0% Proposed 78.6% 70.8% 74.4% 91.3% 94.0% 92.6% 88.6% Table 4: Results for comparison with baseline methods Table 3: Number of sentences in a test set 44 4.2 Feature Parameters with Position Information We inspected the effect of position information of 2,936 declaratively subjective clues based on the heuristic rule that a Japanese predicate part almost always appears in the last ten words in a sentence." ></td>
	<td class="line x" title="162:190	Instead of more precisely identifying predicate position from parsing information, we employed this heuristic rule as a feature parameter in the SVM learner for practical reasons." ></td>
	<td class="line x" title="163:190	Table 5 lists the experimental results." ></td>
	<td class="line x" title="164:190	'All words' indicates that all feature parameters are permitted at any position in the sentence." ></td>
	<td class="line x" title="165:190	'Last 10 words' indicates that all feature parameters are permitted only if they occur within the last ten words in the sentence." ></td>
	<td class="line x" title="166:190	We can see that feature parameters with position information perform better than those without position information in all evaluations." ></td>
	<td class="line x" title="167:190	This result confirms our claim that the position of the feature parameters is important for judging whether a sentence is an opinion or not." ></td>
	<td class="line x" title="168:190	However, the difference did not indicate superiority between the two results at the significance level of 5%." ></td>
	<td class="line x" title="169:190	In the last 10 word experiment, we restricted the position of 422 declaratively subjective clues like (b), (c) and (l) in Table 2, which appear in any position of a sentence, to the same conditions as with the other types of 2,514 declaratively subjective clues." ></td>
	<td class="line x" title="170:190	The fact that the equal position restriction on all declaratively subjective clues slightly improved performance suggests there will be significant improvement in performance from assigning the individual position condition to each declaratively subjective clue." ></td>
	<td class="line x" title="171:190	4.3 Effect of Feature Parameters The third experiment was designed to ascertain the effects of declaratively subjective clues and semantic categories." ></td>
	<td class="line x" title="172:190	The declaratively subjective clues and semantic categories were employed as feature parameters for the SVM learner." ></td>
	<td class="line x" title="173:190	The effect of each particular feature parameter can be seen by using it without the other feature parameter, because the feature parameters are independent of each other." ></td>
	<td class="line x" title="174:190	The experimental results are shown in Table 6." ></td>
	<td class="line x" title="175:190	The first row shows trials using only frequent words and parts of speech as feature parameters." ></td>
	<td class="line x" title="176:190	'Y' in the first and second columns indicates exclusive use of declaratively subjective clues and semantic categories as the feature parameters, respectively." ></td>
	<td class="line x" title="177:190	For instance, we can determine the effect of declaratively subjective clues by comparing the first row with the second row." ></td>
	<td class="line x" title="178:190	The results show the effects of declaratively subjective clues and semantic categories." ></td>
	<td class="line x" title="179:190	The results of the first row show that the method using only frequent words and parts of speech as the feature parameters cannot precisely classify subjective sentences." ></td>
	<td class="line x" title="180:190	Additionally, the last row of the results clearly shows that using both declaratively subjective clues and semantic categories as the feature parameters is the most effective." ></td>
	<td class="line x" title="181:190	The difference between the last row of the results and the other rows cannot be rejected even at the significance level of 5%." ></td>
	<td class="line x" title="182:190	Feature sets Opinion No opinion DS clues Semantic categories Precision Recall Fmeasure Precision Recall Fmeasure Accuracy 71.4% 53.2% 60.9% 87.7% 94.1% 90.8% 85.2% Y 79.9% 64.3% 71.2% 89.6% 95.0% 92.2% 87.8% Y 76.1% 68.9% 72.2% 90.7% 93.3% 92.0% 87.5% Y Y 78.6% 70.8% 74.4% 91.3% 94.0% 92.6% 88.6% Opinion No opinion Position Precision Recall F-measure Precision Recall F-measure Accuracy All words 76.8% 70.6% 73.5% 91.2% 93.4% 92.3% 88.0% Last 10 words 78.6% 70.8% 74.4% 91.3% 94.0% 92.6% 88.6% Table 5: Results for feature parameters with position information Table 6: Results for effect of feature parameters 45 5 Conclusion and Future Work We proposed a method of extracting sentences classified by an SVM as opinion-expressing that uses feature sets of declaratively subjective clues collected from opinion-expressing sentences in Japanese web pages and semantic categories of words obtained from a Japanese lexicon." ></td>
	<td class="line x" title="183:190	The first experiment showed that our method performed better than baseline methods." ></td>
	<td class="line x" title="184:190	The second experiment suggested that our method performed better when extraction of features was limited to the predicate part of a sentence rather than allowed anywhere in the sentence." ></td>
	<td class="line x" title="185:190	The last experiment showed that using both declaratively subjective clues and semantic categories as feature parameters yielded better results than using either clues or categories exclusively." ></td>
	<td class="line x" title="186:190	Our future work will attempt to develop an open-domain opinion web search engine." ></td>
	<td class="line x" title="187:190	To succeed, we first need to augment the proposed opinion-sentence extraction method by incorporating the query relevancy mechanism." ></td>
	<td class="line x" title="188:190	Accordingly, a user will be able to retrieve opinionexpressing sentences relevant to the query." ></td>
	<td class="line x" title="189:190	Second, we need to classify extracted sentences in terms of emotion, sentiment, requirement, and suggestion so that a user can retrieve relevant opinions on demand." ></td>
	<td class="line x" title="190:190	Finally, we need to summarize the extracted sentences so that the user can quickly learn what the writer wanted to say." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-1639
Get Out The Vote: Determining Support Or Opposition From Congressional Floor-Debate Transcripts
Thomas, Matt;Pang, Bo;Lee, Lillian;"></td>
	<td class="line x" title="1:176	Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 327335, Sydney, July 2006." ></td>
	<td class="line x" title="2:176	c2006 Association for Computational Linguistics Get out the vote: Determining support or opposition from Congressional floor-debate transcripts Matt Thomas, Bo Pang, and Lillian Lee Department of Computer Science, Cornell University Ithaca, NY 14853-7501 mattthomas84@gmail.com, pabo@cs.cornell.edu, llee@cs.cornell.edu Abstract We investigate whether one can determine from the transcripts of U.S. Congressional floor debates whether the speeches represent support of or opposition to proposed legislation." ></td>
	<td class="line x" title="3:176	To address this problem, we exploit the fact that these speeches occur as part of a discussion; this allows us to use sources of information regarding relationships between discourse segments, such as whether a given utterance indicates agreement with the opinion expressed by another." ></td>
	<td class="line x" title="4:176	We find that the incorporation of such information yields substantial improvements over classifying speeches in isolation." ></td>
	<td class="line x" title="5:176	1 Introduction One ought to recognize that the present political chaos is connected with the decay of language, and that one can probably bring about some improvement by starting at the verbal end." ></td>
	<td class="line x" title="6:176	 Orwell, Politics and the English language We have entered an era where very large amounts of politically oriented text are now available online." ></td>
	<td class="line x" title="7:176	This includes both official documents, such as the full text of laws and the proceedings of legislative bodies, and unofficial documents, such as postings on weblogs (blogs) devoted to politics." ></td>
	<td class="line x" title="8:176	In some sense, the availability of such data is simply a manifestation of a general trend of everybody putting their records on the Internet.1 The 1It is worth pointing out that the United States Library of Congress was an extremely early adopter of Web technology: the THOMAS database (http://thomas.loc.gov) of congresonline accessibility of politically oriented texts in particular, however, is a phenomenon that some have gone so far as to say will have a potentially society-changing effect." ></td>
	<td class="line x" title="9:176	In the United States, for example, governmental bodies are providing and soliciting political documents via the Internet, with lofty goals in mind: electronic rulemaking (eRulemaking) initiatives involving the electronic collection, distribution, synthesis, and analysis of public commentary in the regulatory rulemaking process, may [alter] the citizen-government relationship (Shulman and Schlosberg, 2002)." ></td>
	<td class="line x" title="10:176	Additionally, much media attention has been focused recently on the potential impact that Internet sites may have on politics2, or at least on political journalism3." ></td>
	<td class="line x" title="11:176	Regardless of whether one views such claims as clear-sighted prophecy or mere hype, it is obviously important to help people understand and analyze politically oriented text, given the importance of enabling informed participation in the political process." ></td>
	<td class="line x" title="12:176	Evaluative and persuasive documents, such as a politicians speech regarding a bill or a bloggers commentary on a legislative proposal, form a particularly interesting type of politically oriented text." ></td>
	<td class="line x" title="13:176	People are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion, given the dense nature of legislative language and the fact that (U.S)." ></td>
	<td class="line x" title="14:176	bills often reach several hundred pages in length (Smith et al. , 2005)." ></td>
	<td class="line x" title="15:176	Moreover, political opinions are exsional bills and related data was launched in January 1995, when Mosaic was not quite two years old and Altavista did not yet exist." ></td>
	<td class="line x" title="16:176	2E.g., Internet injects sweeping change into U.S. politics, Adam Nagourney, The New York Times, April 2, 2006." ></td>
	<td class="line x" title="17:176	3E.g., The End of News?, Michael Massing, The New York Review of Books, December 1, 2005." ></td>
	<td class="line x" title="18:176	327 plicitly solicited in the eRulemaking scenario." ></td>
	<td class="line x" title="19:176	In the analysis of evaluative language, it is fundamentally necessary to determine whether the author/speaker supports or disapproves of the topic of discussion." ></td>
	<td class="line x" title="20:176	In this paper, we investigate the following specific instantiation of this problem: we seek to determine from the transcripts of U.S. Congressional floor debates whether each speech (continuous single-speaker segment of text) represents support for or opposition to a proposed piece of legislation." ></td>
	<td class="line x" title="21:176	Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records." ></td>
	<td class="line x" title="22:176	Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography)." ></td>
	<td class="line oc" title="23:176	In particular, since we treat each individual speech within a debate as a single document, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al. , 2002; Turney, 2002; Dave et al. , 2003)." ></td>
	<td class="line x" title="24:176	Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently." ></td>
	<td class="line x" title="25:176	A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006)." ></td>
	<td class="line x" title="26:176	Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions." ></td>
	<td class="line x" title="27:176	For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most interested in techniques applicable across domains, we restrict consideration to NLP aspects of the problem, ignoring external problem-specific information." ></td>
	<td class="line x" title="28:176	For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via the methods we propose), we did not use party-affiliation data." ></td>
	<td class="line x" title="29:176	Indeed, in other settings (e.g. , a movie-discussion listserv) one may not be able to determine the participants political leanings, and such information may not lead to significantly improved results even if it were available." ></td>
	<td class="line x" title="30:176	tween two speakers, such as explicit assertions (I second that!) or quotation of messages in emails or postings (see Mullen and Malouf (2006) but cf.Agrawal et al.(2003))." ></td>
	<td class="line x" title="33:176	Agreement evidence can be a powerful aid in our classification task: for example, we can easily categorize a complicated (or overly terse) document if we find within it indications of agreement with a clearly positive text." ></td>
	<td class="line x" title="34:176	Obviously, incorporating agreement information provides additional benefit only when the input documents are relatively difficult to classify individually." ></td>
	<td class="line x" title="35:176	Intuition suggests that this is true of the data with which we experiment, for several reasons." ></td>
	<td class="line x" title="36:176	First, U.S. congressional debates contain very rich language and cover an extremely wide variety of topics, ranging from flag burning to international policy to the federal budget." ></td>
	<td class="line x" title="37:176	Debates are also subject to digressions, some fairly natural and others less so (e.g. , Why are we discussing this bill when the plight of my constituents regarding this other issue is being ignored?) Second, an important characteristic of persuasive language is that speakers may spend more time presenting evidence in support of their positions (or attacking the evidence presented by others) than directly stating their attitudes." ></td>
	<td class="line x" title="38:176	An extreme example will illustrate the problems involved." ></td>
	<td class="line x" title="39:176	Consider a speech that describes the U.S. flag as deeply inspirational, and thus contains only positive language." ></td>
	<td class="line x" title="40:176	If the bill under discussion is a proposed flag-burning ban, then the speech is supportive; but if the bill under discussion is aimed at rescinding an existing flag-burning ban, the speech may represent opposition to the legislation." ></td>
	<td class="line x" title="41:176	Given the current state of the art in sentiment analysis, it is doubtful that one could determine the (probably topic-specific) relationship between presented evidence and speaker opinion." ></td>
	<td class="line x" title="42:176	Qualitative summary of results The above difficulties underscore the importance of enhancing standard classification techniques with new information sources that promise to improve accuracy, such as inter-document relationships between the documents to be labeled." ></td>
	<td class="line x" title="43:176	In this paper, we demonstrate that the incorporation of agreement modeling can provide substantial improvements over the application of support vector machines (SVMs) in isolation, which represents the state of the art in the individual classification of documents." ></td>
	<td class="line x" title="44:176	The enhanced accuracies are obtained via a fairly primitive automatically-acquired agreement detector 328 total train test development speech segments 3857 2740 860 257 debates 53 38 10 5 average number of speech segments per debate 72.8 72.1 86.0 51.4 average number of speakers per debate 32.1 30.9 41.1 22.6 Table 1: Corpus statistics." ></td>
	<td class="line x" title="45:176	and a conceptually simple method for integrating isolated-document and agreement-based information." ></td>
	<td class="line x" title="46:176	We thus view our results as demonstrating the potentially large benefits of exploiting sentiment-related discourse-segment relationships in sentiment-analysis tasks." ></td>
	<td class="line x" title="47:176	2 Corpus This section outlines the main steps of the process by which we created our corpus (download site: www.cs.cornell.edu/home/llee/data/convote.html)." ></td>
	<td class="line x" title="48:176	GovTrack (http://govtrack.us) is an independent website run by Joshua Tauberer that collects publicly available data on the legislative and fundraising activities of U.S. congresspeople." ></td>
	<td class="line x" title="49:176	Due to its extensive cross-referencing and collating of information, it was nominated for a 2006 Webby award." ></td>
	<td class="line x" title="50:176	A crucial characteristic of GovTrack from our point of view is that the information is provided in a very convenient format; for instance, the floor-debate transcripts are broken into separate HTML files according to the subject of the debate, so we can trivially derive long sequences of speeches guaranteed to cover the same topic." ></td>
	<td class="line x" title="51:176	We extracted from GovTrack all available transcripts of U.S. floor debates in the House of Representatives for the year 2005 (3268 pages of transcripts in total), together with voting records for all roll-call votes during that year." ></td>
	<td class="line x" title="52:176	We concentrated on debates regarding controversial bills (ones in which the losing side generated at least 20% of the speeches) because these debates should presumably exhibit more interesting discourse structure." ></td>
	<td class="line x" title="53:176	Each debate consists of a series of speech segments, where each segment is a sequence of uninterrupted utterances by a single speaker." ></td>
	<td class="line x" title="54:176	Since speech segments represent natural discourse units, we treat them as the basic unit to be classified." ></td>
	<td class="line x" title="55:176	Each speech segment was labeled by the vote (yea or nay) cast for the proposed bill by the person who uttered the speech segment." ></td>
	<td class="line x" title="56:176	We automatically discarded those speech segments belonging to a class of formulaic, generally one-sentence utterances focused on the yielding of time on the house floor (for example, Madam Speaker, I am pleased to yield 5 minutes to the gentleman from Massachusetts), as such speech segments are clearly off-topic." ></td>
	<td class="line x" title="57:176	We also removed speech segments containing the term amendment, since we found during initial inspection that these speeches generally reflect a speakers opinion on an amendment, and this opinion may differ from the speakers opinion on the underlying bill under discussion." ></td>
	<td class="line x" title="58:176	We randomly split the data into training, test, and development (parameter-tuning) sets representing roughly 70%, 20%, and 10% of our data, respectively (see Table 1)." ></td>
	<td class="line x" title="59:176	The speech segments remained grouped by debate, with 38 debates assigned to the training set, 10 to the test set, and 5 to the development set; we require that the speech segments from an individual debate all appear in the same set because our goal is to examine classification of speech segments in the context of the surrounding discussion." ></td>
	<td class="line x" title="60:176	3 Method The support/oppose classification problem can be approached through the use of standard classifiers such as support vector machines (SVMs), which consider each text unit in isolation." ></td>
	<td class="line x" title="61:176	As discussed in Section 1, however, the conversational nature of our data implies the existence of various relationships that can be exploited to improve cumulative classification accuracy for speech segments belonging to the same debate." ></td>
	<td class="line x" title="62:176	Our classification framework, directly inspired by Blum and Chawla (2001), integrates both perspectives, optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label." ></td>
	<td class="line x" title="63:176	In this section, we discuss the specific classification framework that we adopt and the set of mechanisms that we propose for modeling specific types of relationships." ></td>
	<td class="line x" title="64:176	329 3.1 Classification framework Let s1,s2,,sn be the sequence of speech segments within a given debate, and let Y and N stand for the yea and nay class, respectively." ></td>
	<td class="line x" title="65:176	Assume we have a non-negative function ind(s,C) indicating the degree of preference that an individual-document classifier, such as an SVM, has for placing speech-segment s in class C. Also, assume that some pairs of speech segments have weighted links between them, where the non-negative strength (weight) str(lscript) for a link lscript indicates the degree to which it is preferable that the linked speech segments receive the same label." ></td>
	<td class="line x" title="66:176	Then, any class assignment c = c(s1),c(s2),,c(sn) can be assigned a cost summationdisplay s ind(s,c(s))+ summationdisplay s,sprime:c(s)negationslash=c(sprime) summationdisplay lscript betweens,sprime str(lscript), where c(s) is the opposite class from c(s)." ></td>
	<td class="line x" title="67:176	A minimum-cost assignment thus represents an optimum way to classify the speech segments so that each one tends not to be put into the class that the individual-document classifier disprefers, but at the same time, highly associated speech segments tend not to be put in different classes." ></td>
	<td class="line x" title="68:176	As has been previously observed and exploited in the NLP literature (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Barzilay and Lapata, 2005), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs." ></td>
	<td class="line x" title="69:176	In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision." ></td>
	<td class="line oc" title="70:176	3.2 Classifying speech segments in isolation In our experiments, we employed the well-known classifier SVMlight to obtain individual-document classification scores, treating Y as the positive class and using plain unigrams as features.5 Following standard practice in sentiment analysis (Pang et al. , 2002), the input to SVMlight consisted of normalized presence-of-feature (rather than frequency-of-feature) vectors." ></td>
	<td class="line x" title="71:176	The ind value 5SVMlight is available at svmlight.joachims.org." ></td>
	<td class="line x" title="72:176	Default parameters were used, although experimentation with different parameter settings is an important direction for future work (Daelemans and Hoste, 2002; Munson et al. , 2005)." ></td>
	<td class="line x" title="73:176	for each speech segment s was based on the signed distance d(s) from the vector representing s to the trained SVM decision plane: ind(s,Y) def=    1 d(s) > 2s;parenleftBig 1+ d(s)2s parenrightBig /2 |d(s)| 2s; 0 d(s) < 2s where s is the standard deviation of d(s) over all speech segments s in the debate in question, and ind(s,N) def= 1 ind(s,Y)." ></td>
	<td class="line x" title="74:176	We now turn to the more interesting problem of representing the preferences that speech segments may have for being assigned to the same class." ></td>
	<td class="line x" title="75:176	3.3 Relationships between speech segments A wide range of relationships between text segments can be modeled as positive-strength links." ></td>
	<td class="line x" title="76:176	Here we discuss two types of constraints that are considered in this work." ></td>
	<td class="line x" title="77:176	Same-speaker constraints: In Congressional debates and in general social-discourse contexts, a single speaker may make a number of comments regarding a topic." ></td>
	<td class="line x" title="78:176	It is reasonable to expect that in many settings, the participants in a discussion may be convinced to change their opinions midway through a debate." ></td>
	<td class="line x" title="79:176	Hence, in the general case we wish to be able to express soft preferences for all of an authors statements to receive the same label, where the strengths of such constraints could, for instance, vary according to the time elapsed between the statements." ></td>
	<td class="line x" title="80:176	Weighted links are an appropriate means to express such variation." ></td>
	<td class="line x" title="81:176	However, if we assume that most speakers do not change their positions in the course of a discussion, we can conclude that all comments made by the same speaker must receive the same label." ></td>
	<td class="line x" title="82:176	This assumption holds by fiat for the ground-truth labels in our dataset because these labels were derived from the single vote cast by the speaker on the bill being discussed.6 We can implement this assumption via links whose weights are essentially infinite." ></td>
	<td class="line x" title="83:176	Although one can also implement this assumption via concatenation of same-speaker speech segments (see Section 4.3), we view the fact that our graph-based framework incorporates 6We are attempting to determine whether a speech segment represents support or not." ></td>
	<td class="line x" title="84:176	This differs from the problem of determining what the speakers actual opinion is, a problem that, as an anonymous reviewer put it, is complicated by grandstanding, backroom deals, or, more innocently, plain change of mind (I voted for it before I voted against it)." ></td>
	<td class="line x" title="85:176	330 both hard and soft constraints in a principled fashion as an advantage of our approach." ></td>
	<td class="line x" title="86:176	Different-speaker agreements In House discourse, it is common for one speaker to make reference to another in the context of an agreement or disagreement over the topic of discussion." ></td>
	<td class="line x" title="87:176	The systematic identification of instances of agreement can, as we have discussed, be a powerful tool for the development of intelligently selected weights for links between speech segments." ></td>
	<td class="line x" title="88:176	The problem of agreement identification can be decomposed into two sub-problems: identifying references and their targets, and deciding whether each reference represents an instance of agreement." ></td>
	<td class="line x" title="89:176	In our case, the first task is straightforward because we focused solely on by-name references.7 Hence, we will now concentrate on the second, more interesting task." ></td>
	<td class="line x" title="90:176	We approach the problem of classifying references by representing each reference with a wordpresence vector derived from a window of text surrounding the reference.8 In the training set, we classify each reference connecting two speakers with a positive or negative label depending on whether the two voted the same way on the bill under discussion9." ></td>
	<td class="line x" title="91:176	These labels are then used to train an SVM classifier, the output of which is subsequently used to create weights on agreement links in the test set as follows." ></td>
	<td class="line x" title="92:176	Let d(r) denote the distance from the vector representing reference r to the agreement-detector SVMs decision plane, and let r be the standard deviation of d(r) over all references in the debate in question." ></td>
	<td class="line x" title="93:176	We then define the strength agr of the agreement link corresponding to the reference as: agr(r) def=    0 d(r) < agr; d(r)/4r agr  d(r)  4r;  d(r) > 4r. The free parameter  specifies the relative impor7One subtlety is that for the purposes of mining agreement cues (but not for evaluating overall support/oppose classification accuracy), we temporarily re-inserted into our dataset previously filtered speech segments containing the term yield, since the yielding of time on the House floor typically indicates agreement even though the yield statements contain little relevant text on their own." ></td>
	<td class="line x" title="94:176	8We found good development-set performance using the 30 tokens before, 20 tokens after, and the name itself." ></td>
	<td class="line x" title="95:176	9Since we are concerned with references that potentially represent relationships between speech segments, we ignore references for which the target of the reference did not speak in the debate in which the reference was made." ></td>
	<td class="line x" title="96:176	Agreement classifier (referenceagreement?) Devel." ></td>
	<td class="line x" title="97:176	set Test set majority baseline 81.51 80.26 Train: no amdmts; agr = 0 84.25 81.07 Train: with amdmts; agr = 0 86.99 80.10 Table 2: Agreement-classifier accuracy, in percent." ></td>
	<td class="line x" title="98:176	Amdmts=speech segments containing the word amendment." ></td>
	<td class="line x" title="99:176	Recall that boldface indicates results for development-set-optimal settings." ></td>
	<td class="line x" title="100:176	tance of the agr scores." ></td>
	<td class="line x" title="101:176	The threshold agr controls the precision of the agreement links, in that values of agr greater than zero mean that greater confidence is required before an agreement link can be added.10 4 Evaluation This section presents experiments testing the utility of using speech-segment relationships, evaluating against a number of baselines." ></td>
	<td class="line x" title="102:176	All reported results use values for the free parameter  derived via tuning on the development set." ></td>
	<td class="line x" title="103:176	In the tables, boldface indicates the developmentand test-set results for the development-set-optimal parameter settings, as one would make algorithmic choices based on development-set performance." ></td>
	<td class="line x" title="104:176	4.1 Preliminaries: Reference classification Recall that to gather inter-speaker agreement information, the strategy employed in this paper is to classify by-name references to other speakers as to whether they indicate agreement or not." ></td>
	<td class="line x" title="105:176	To train our agreement classifier, we experimented with undoing the deletion of amendmentrelated speech segments in the training set." ></td>
	<td class="line x" title="106:176	Note that such speech segments were never included in the development or test set, since, as discussed in Section 2, their labels are probably noisy; however, including them in the training set allows the classifier to examine more instances even though some of them are labeled incorrectly." ></td>
	<td class="line x" title="107:176	As Table 2 shows, using more, if noisy, data yields better agreement-classification results on the development set, and so we use that policy in all subsequent experiments.11 10Our implementation puts a link between just one arbitrary pair of speech segments among all those uttered by a given pair of apparently agreeing speakers." ></td>
	<td class="line x" title="108:176	The infiniteweight same-speaker links propagate the agreement information to all other such pairs." ></td>
	<td class="line x" title="109:176	11Unfortunately, this policy leads to inferior test-set agree331 Agreement classifier Precision (in percent): Devel." ></td>
	<td class="line x" title="110:176	set Test set agr = 0 86.23 82.55 agr =  89.41 88.47 Table 3: Agreement-classifier precision." ></td>
	<td class="line x" title="111:176	An important observation is that precision may be more important than accuracy in deciding which agreement links to add: false positives with respect to agreement can cause speech segments to be incorrectly assigned the same label, whereas false negatives mean only that agreement-based information about other speech segments is not employed." ></td>
	<td class="line x" title="112:176	As described above, we can raise agreement precision by increasing the threshold agr, which specifies the required confidence for the addition of an agreement link." ></td>
	<td class="line x" title="113:176	Indeed, Table 3 shows that we can improve agreement precision by setting agr to the (positive) mean agreement score  assigned by the SVM agreement-classifier over all references in the given debate12." ></td>
	<td class="line x" title="114:176	However, this comes at the cost of greatly reducing agreement accuracy (development: 64.38%; test: 66.18%) due to lowered recall levels." ></td>
	<td class="line x" title="115:176	Whether or not better speech-segment classification is ultimately achieved is discussed in the next sections." ></td>
	<td class="line x" title="116:176	4.2 Segment-based speech-segment classification Baselines The first two data rows of Table 4 depict baseline performance results." ></td>
	<td class="line x" title="117:176	The #(support)  #(oppos) baseline is meant to explore whether the speech-segment classification task can be reduced to simple lexical checks." ></td>
	<td class="line x" title="118:176	Specifically, this method uses the signed difference between the number of words containing the stem support and the number of words containing the stem oppos (returning the majority class if the difference is 0)." ></td>
	<td class="line x" title="119:176	No better than 62.67% testset accuracy is obtained by either baseline." ></td>
	<td class="line x" title="120:176	Using relationship information Applying an SVM to classify each speech segment in isolation leads to clear improvements over the two baseline methods, as demonstrated in Table 4." ></td>
	<td class="line x" title="121:176	When we impose the constraint that all speech segments uttered by the same speaker receive the same label via same-speaker links, both test-set and ment classification." ></td>
	<td class="line x" title="122:176	Section 4.5 contains further discussion." ></td>
	<td class="line x" title="123:176	12We elected not to explicitly tune the value of agr in order to minimize the number of free parameters to deal with." ></td>
	<td class="line x" title="124:176	Support/oppose classifer (speech segmentyea?) Devel." ></td>
	<td class="line x" title="125:176	set Test set majority baseline 54.09 58.37 #(support)#(oppos) 59.14 62.67 SVM [speech segment] 70.04 66.05 SVM + same-speaker links 79.77 67.21 SVM + same-speaker links + agreement links, agr = 0 89.11 70.81 + agreement links, agr =  87.94 71.16 Table 4: Segment-based speech-segment classification accuracy, in percent." ></td>
	<td class="line x" title="126:176	Support/oppose classifer (speech segmentyea?) Devel." ></td>
	<td class="line x" title="127:176	set Test set SVM [speaker] 71.60 70.00 SVM + agreement links  with agr = 0 88.72 71.28 with agr =  84.44 76.05 Table 5: Speaker-based speech-segment classification accuracy, in percent." ></td>
	<td class="line x" title="128:176	Here, the initial SVM is run on the concatenation of all of a given speakers speech segments, but the results are computed over speech segments (not speakers), so that they can be compared to those in Table 4." ></td>
	<td class="line x" title="129:176	development-set accuracy increase even more, in the latter case quite substantially so." ></td>
	<td class="line x" title="130:176	The last two lines of Table 4 show that the best results are obtained by incorporating agreement information as well." ></td>
	<td class="line x" title="131:176	The highest test-set result, 71.16%, is obtained by using a high-precision threshold to determine which agreement links to add." ></td>
	<td class="line x" title="132:176	While the development-set results would induce us to utilize the standard threshold value of 0, which is sub-optimal on the test set, the agr = 0 agreement-link policy still achieves noticeable improvement over not using agreement links (test set: 70.81% vs. 67.21%)." ></td>
	<td class="line x" title="133:176	4.3 Speaker-based speech-segment classification We use speech segments as the unit of classification because they represent natural discourse units." ></td>
	<td class="line x" title="134:176	As a consequence, we are able to exploit relationships at the speech-segment level." ></td>
	<td class="line x" title="135:176	However, it is interesting to consider whether we really need to consider relationships specifically between speech segments themselves, or whether it suffices to simply consider relationships between the speakers 332 of the speech segments." ></td>
	<td class="line x" title="136:176	In particular, as an alternative to using same-speaker links, we tried a speaker-based approach wherein the way we determine the initial individual-document classification score for each speech segment uttered by a person p in a given debate is to run an SVM on the concatenation of all of ps speech segments within that debate." ></td>
	<td class="line x" title="137:176	(We also ensure that agreement-link information is propagated from speech-segment to speaker pairs.)" ></td>
	<td class="line x" title="138:176	How does the use of same-speaker links compare to the concatenation of each speakers speech segments?" ></td>
	<td class="line x" title="139:176	Tables 4 and 5 show that, not surprisingly, the SVM individual-document classifier works better on the concatenated speech segments than on the speech segments in isolation." ></td>
	<td class="line x" title="140:176	However, the effect on overall classification accuracy is less clear: the development set favors samespeaker links over concatenation, while the test set does not." ></td>
	<td class="line x" title="141:176	But we stress that the most important observation we can make from Table 5 is that once again, the addition of agreement information leads to substantial improvements in accuracy." ></td>
	<td class="line x" title="142:176	4.4 Hard agreement constraints Recall that in in our experiments, we created finite-weight agreement links, so that speech segments appearing in pairs flagged by our (imperfect) agreement detector can potentially receive different labels." ></td>
	<td class="line x" title="143:176	We also experimented with forcing such speech segments to receive the same label, either through infinite-weight agreement links or through a speech-segment concatenation strategy similar to that described in the previous subsection." ></td>
	<td class="line x" title="144:176	Both strategies resulted in clear degradation in performance on both the development and test sets, a finding that validates our encoding of agreement information as soft preferences." ></td>
	<td class="line x" title="145:176	4.5 On the development/test set split We have seen several cases in which the method that performs best on the development set does not yield the best test-set performance." ></td>
	<td class="line x" title="146:176	However, we felt that it would be illegitimate to change the train/development/test sets in a post hoc fashion, that is, after seeing the experimental results." ></td>
	<td class="line x" title="147:176	Moreover, and crucially, it is very clear that using agreement information, encoded as preferences within our graph-based approach rather than as hard constraints, yields substantial improvements on both the development and test set; this, we believe, is our most important finding." ></td>
	<td class="line x" title="148:176	5 Related work Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al. , 2005; Cardie et al. , 2006; Kwon et al. , 2006)." ></td>
	<td class="line x" title="149:176	There has also been work focused upon determining the political leaning (e.g. , liberal vs. conservative) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the unlabeled texts) (Laver et al. , 2003; Efron, 2004; Mullen and Malouf, 2006)." ></td>
	<td class="line x" title="150:176	An exception is Grefenstette et al.(2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site." ></td>
	<td class="line x" title="152:176	Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006)." ></td>
	<td class="line x" title="153:176	Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement." ></td>
	<td class="line x" title="154:176	More sophisticated approaches have been proposed (Hillard et al. , 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al. , 2004)." ></td>
	<td class="line x" title="155:176	Also relevant is work on the general problems of dialog-act tagging (Stolcke et al. , 2000), citation analysis (Lehnert et al. , 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002)." ></td>
	<td class="line x" title="156:176	We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work." ></td>
	<td class="line x" title="157:176	Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g. , between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations." ></td>
	<td class="line x" title="158:176	Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al. , 2003)." ></td>
	<td class="line x" title="159:176	Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al.(2002), Kondor and Lafferty (2002), and Joachims (2003)." ></td>
	<td class="line x" title="161:176	Zhu (2005) maintains a survey of this area." ></td>
	<td class="line x" title="162:176	Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al. , 2001; Getoor et al. , 2002; Taskar et al. , 2002; Taskar et al. , 2003; Taskar et al. , 2004; McCallum and Wellner, 2004)." ></td>
	<td class="line x" title="163:176	It would be interesting to investigate the application of such methods to our problem." ></td>
	<td class="line x" title="164:176	However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve." ></td>
	<td class="line x" title="165:176	6 Conclusion and future work In this study, we focused on very general types of cross-document classification preferences, utilizing constraints based only on speaker identity and on direct textual references between statements." ></td>
	<td class="line x" title="166:176	We showed that the integration of even very limited information regarding inter-document relationships can significantly increase the accuracy of support/opposition classification." ></td>
	<td class="line x" title="167:176	The simple constraints modeled in our study, however, represent just a small portion of the rich network of relationships that connect statements and speakers across the political universe and in the wider realm of opinionated social discourse." ></td>
	<td class="line x" title="168:176	One intriguing possibility is to take advantage of (readily identifiable) information regarding interpersonal relationships, making use of speaker/author affiliations, positions within a social hierarchy, and so on." ></td>
	<td class="line x" title="169:176	Or, we could even attempt to model relationships between topics or concepts, in a kind of extension of collaborative filtering." ></td>
	<td class="line x" title="170:176	For example, perhaps we could infer that two speakers sharing a common opinion on evolutionary biologist Richard Dawkins (a.k.a. Darwins rottweiler) will be likely to agree in a debate centered on Intelligent Design." ></td>
	<td class="line x" title="171:176	While such functionality is well beyond the scope of our current study, we are optimistic that we can develop methods to exploit additional types of relationships in future work." ></td>
	<td class="line x" title="172:176	Acknowledgments We thank Claire Cardie, Jon Kleinberg, Michael Macy, Andrew Myers, and the six anonymous EMNLP referees for valuable discussions and comments." ></td>
	<td class="line x" title="173:176	We also thank Reviewer 1 for generously providing additional post hoc feedback, and the EMNLP chairs Eric Gaussier and Dan Jurafsky for facilitating the process (as well as for allowing authors an extra proceedings page)." ></td>
	<td class="line x" title="174:176	This paper is based upon work supported in part by the National Science Foundation under grant no." ></td>
	<td class="line x" title="175:176	IIS-0329064." ></td>
	<td class="line x" title="176:176	Any opinions, findings, and conclusions or recommendations expressed are those of the authors and do not necessarily reflect the views or official policies, either expressed or implied, of any sponsoring institutions, the U.S. government, or any other entity." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-1641
Sentiment Retrieval Using Generative Models
Eguchi, Koji;Lavrenko, Victor;"></td>
	<td class="line x" title="1:229	Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 345354, Sydney, July 2006." ></td>
	<td class="line x" title="2:229	c2006 Association for Computational Linguistics Sentiment Retrieval using Generative Models Koji Eguchi National Institute of Informatics Tokyo 101-8430, Japan eguchi@nii.ac.jp Victor Lavrenko Department of Computer Science University of Massachusetts Amherst, MA 01003, USA lavrenko@cs.umass.edu Abstract Ranking documents or sentences according to both topic and sentiment relevance should serve a critical function in helping users when topics and sentiment polarities of the targeted text are not explicitly given, as is often the case on the web." ></td>
	<td class="line x" title="3:229	In this paper, we propose several sentiment information retrieval models in the framework of probabilistic language models, assuming that a user both inputs query terms expressing a certain topic and also specifies a sentiment polarity of interest in some manner." ></td>
	<td class="line x" title="4:229	We combine sentiment relevance models and topic relevance models with model parameters estimated from training data, considering the topic dependence of the sentiment." ></td>
	<td class="line x" title="5:229	Our experiments prove that our models are effective." ></td>
	<td class="line x" title="6:229	1 Introduction The recent rapid expansion of access to information has significantly increased the demands on retrieval or classification of sentiment information from a large amount of textual data." ></td>
	<td class="line x" title="7:229	The field of sentiment classification has recently received considerable attention, where the polarities of sentiment, such as positive or negative, were identified from unstructured text (Shanahan et al. , 2005)." ></td>
	<td class="line nc" title="8:229	A number of studies have investigated sentiment classification at document level, e.g., (Pang et al. , 2002; Dave et al. , 2003), and at sentence level, e.g., (Hu and Liu, 2004; Kim and Hovy, 2004; Nigam and Hurst, 2005); however, the accuracy is still less than desirable." ></td>
	<td class="line x" title="9:229	Therefore, ranking according to the likelihood of containing sentiment information is expected to serve a crucial function in helping users." ></td>
	<td class="line x" title="10:229	We believe that our work is the first attempt at sentiment retrieval that aims at finding sentences containing information with a specific sentiment polarity on a certain topic." ></td>
	<td class="line x" title="11:229	Intuitively, the expression of sentiment in text is dependent on the topic." ></td>
	<td class="line x" title="12:229	For example, a negative view for some voting event may be expressed using flaw, while a negative view for some politician may be expressed using reckless." ></td>
	<td class="line x" title="13:229	Moreover, sentiment polarities are also dependent on topics or domains." ></td>
	<td class="line x" title="14:229	For example, the adjective unpredictable may have a negative orientation in an automotive review, in a phrase such as unpredictable steering, but it could have a positive orientation in a movie review, in a phrase such as unpredictable plot, as mentioned in (Turney, 2002) in the context of his sentiment word detection." ></td>
	<td class="line x" title="15:229	We propose sentiment retrieval models in the framework of generative language modeling, not only assuming query terms expressing a certain topic, but also assuming that the polarity of sentiment interest is specified by the user in some manner, where the topic dependence of the sentiment is considered." ></td>
	<td class="line x" title="16:229	To the best of our knowledge, there have been no other studies on a retrieval model unifying both topic and sentiment, and further, there have been no other studies on sentiment retrieval." ></td>
	<td class="line x" title="17:229	The sentiment information often appears as local in a document, and therefore focusing on finer levels, i.e., sentence or passage levels rather than document level, is crucial." ></td>
	<td class="line x" title="18:229	We thus experiment on sentiment retrieval at the sentence level in this paper." ></td>
	<td class="line x" title="19:229	The rest of this paper is structured as follows." ></td>
	<td class="line x" title="20:229	Section 2 introduces the work related to this study." ></td>
	<td class="line x" title="21:229	Section 3 describes a generative model of sentiment, which is proposed here as a theoretical framework for our work." ></td>
	<td class="line x" title="22:229	Section 4 describes the task definition and our sentiment retrieval model." ></td>
	<td class="line x" title="23:229	345 Section 5 explains the data we used for our experiments, and gives our experimental results." ></td>
	<td class="line x" title="24:229	Section 6 concludes the paper." ></td>
	<td class="line x" title="25:229	2 Related Work Some efforts for the TREC Novelty Track were related to our work." ></td>
	<td class="line x" title="26:229	Although some of the topics used in the Novelty Track in 2003 and 2004 (Soboroff and Harman, 2003; Soboroff, 2004) were related to opinions, most of the efforts were focused on topic, such as studies using term distribution within each sentence, e.g., (Allan et al. , 2003; Losada, 2005; Murdock and Croft, 2005)." ></td>
	<td class="line x" title="27:229	Amongst the participants in the TREC Novelty Track, only (Kim et al. , 2004) proposed a method specialized to opinion-bearing sentence retrieval, by making use of lists of words with positive or negative polarities." ></td>
	<td class="line x" title="28:229	They aimed to find opinions on a given topic but did not distinguish or did not care about sentiment polarities that should be represented in some sentences (hereafter, opinion retrieval)." ></td>
	<td class="line x" title="29:229	We focus on finding positive views or negative views according to a given topic and sentiment of interest (hereafter, sentiment retrieval)." ></td>
	<td class="line x" title="30:229	Our work is the first work on sentiment retrieval, to the best of our knowledge." ></td>
	<td class="line x" title="31:229	In the context of sentiment classification, some researchers have conducted studies on the topic dependence of sentiment polarities." ></td>
	<td class="line x" title="32:229	(Nasukawa and Yi, 2003) and (Yi et al. , 2003) extracted positive or negative expressions on a given product name using handmade lexicons." ></td>
	<td class="line x" title="33:229	(Engstrom, 2004) studied how the topic dependence influences the accuracy of sentiment classification and attempted to reduce the influence to improve the accuracy." ></td>
	<td class="line x" title="34:229	(Wilson et al. , 2005) investigated how context influences sentiment polarity at the phrase level in a corpus, beginning with a predefined list of words with polarities." ></td>
	<td class="line x" title="35:229	Their focus on the phenomena of topic dependence of sentiment can be shared with our work; however, their work is not directly related to ours, because we focus on a different task, sentiment retrieval, where different approaches are required." ></td>
	<td class="line x" title="36:229	3 A Generative Model of Sentiment In this section we will provide a formal underpinning for our approach to sentiment retrieval." ></td>
	<td class="line x" title="37:229	The approach is based on the generative paradigm: we describe a statistical process that could be viewed, hypothetically, as a source of every statement of interest to our system." ></td>
	<td class="line x" title="38:229	We stress that this generative process is to be treated as purely hypothetical; the process is only intended to reflect those aspects of human discourse that are pertinent to the problem of retrieving affectively appropriate and topicrelevant texts in response to a query posed by our user." ></td>
	<td class="line x" title="39:229	Before giving a formal specification of our model, we will provide a high-level overview of the main ideas." ></td>
	<td class="line x" title="40:229	We are trying to model a collection of natural-language statements, some of which are relevant to a users query." ></td>
	<td class="line x" title="41:229	In our experiments, these statements are individual sentences, but the model can be applied to textual chunks of any length." ></td>
	<td class="line x" title="42:229	We assume that the content of an individual statement can be modeled independently of all other statements in the collection." ></td>
	<td class="line x" title="43:229	Each statement consists of some topicbearing and some sentiment-bearing words." ></td>
	<td class="line x" title="44:229	We assume that the topic-bearing words represent exchangeable samples from some underlying topic language model." ></td>
	<td class="line x" title="45:229	Exchangeability means that the relative order of the words is irrelevant, but the words are not independent of each otherthe idea often stated as a bag-of-words assumption." ></td>
	<td class="line x" title="46:229	Similarly, sentiment-bearing words are viewed as an order-invariant bag, sampled from the underlying sentiment language model." ></td>
	<td class="line x" title="47:229	We will explicitly model dependency between the topic and sentiment language models, and will demonstrate that treating them independently leads to sub-optimal retrieval performance." ></td>
	<td class="line x" title="48:229	When a sentiment polarity value is observed for a given statement, we will treat it as a ternary variable influencing the topic and sentiment language models." ></td>
	<td class="line x" title="49:229	We represent a users query as just another statement, consisting of topic and sentiment parts, subject to all the independence assumptions stated above." ></td>
	<td class="line x" title="50:229	We will use the query to estimate the topic and sentiment language models that are representative of the users interests." ></td>
	<td class="line x" title="51:229	Following (Lavrenko and Croft, 2001), we will use the term relevance models to describe these models, and will use them to rank statements in order of their relevance to the query." ></td>
	<td class="line x" title="52:229	3.1 Definitions We start by providing a set of definitions that will be used in the remainder of this section." ></td>
	<td class="line x" title="53:229	The task of our model is to generate a collection of statements DB BD BMBMBMDB D2." ></td>
	<td class="line x" title="54:229	A statement DB CX is a string of 346 wordsDB CXBD BMBMBMDB CXD2 CX, drawn from a common vocabulary CE." ></td>
	<td class="line x" title="55:229	We introduce a binary variable CQ CXCY BECUCBBNCCCV as an indicator of whether the word in the CYth position of the CXth statement will be a topic word or a sentiment word." ></td>
	<td class="line x" title="56:229	For our purposes, CQ CXCY is either provided by a human annotator (manual annotation), or determined heuristically (automatic annotation)." ></td>
	<td class="line x" title="57:229	The sentiment polarity DC CX for a given statement is a discrete random variable with three outcomes: CUA0BDBNBCBNB7BDCV, representing negative, neutral and positive polarity values, respectively." ></td>
	<td class="line x" title="58:229	As a matter of convenience we will often denote a statement as a triple CUDB D7 CX BNDB D8 CX BNDC CX CV, where DB D7 CX contains the sentiment words and DB D8 CX contains the topic words." ></td>
	<td class="line x" title="59:229	As we mentioned above, the users query is treated as just another statement." ></td>
	<td class="line x" title="60:229	It will be denoted as a triple CUD5D7BND5D8BND5DCCV, corresponding to sentiment words, topic keywords, and the desired polarity value." ></td>
	<td class="line x" title="61:229	We will use D4 to denote a unigram language model, i.e., a function that assigns a number D4B4DAB5BECJBCBNBDCL to every word DA in our vocabulary CE, such that A6 DA D4B4DAB5BPBD." ></td>
	<td class="line x" title="62:229	The set of all possible unigram language models is the probability simplex C1C8." ></td>
	<td class="line x" title="63:229	Similarly, D4 DC will denote a distribution over the three possible polarity values, and C1C8 DC is the corresponding ternary probability simplex." ></td>
	<td class="line x" title="64:229	We define AP BM C1C8A2C1C8A2C1C8 DC AXCJBCBNBDCL to be a measure function that assigns a probability APB4D4 BD BND4 BE BND4 DC B5 to a pair of language models D4 BD and D4 BE together with a polarity model D4 DC . 3.2 Generative model Using the definitions presented above, and assuming that APB4B5 is given, we hypothesize that a new statement DB CX containing words DB CXBD BMBMBMDB CXD1 with sentiment polarity DC CX can be generated according to the following mechanism." ></td>
	<td class="line x" title="65:229	1." ></td>
	<td class="line x" title="66:229	Draw D4 D8 BND4 D7 and D4 DC from APB4A1BNA1BNA1B5." ></td>
	<td class="line x" title="67:229	2." ></td>
	<td class="line x" title="68:229	Sample DC CX from a polarity distribution D4 DC B4A1B5." ></td>
	<td class="line x" title="69:229	3." ></td>
	<td class="line x" title="70:229	For each position CY BP BDBMBMBMD1: (a) if CQ CXCY BPCC: draw DB CXCY from D4 D8 B4A1B5 ; (b) if CQ CXCY BPCB: draw DB CXCY from D4 D7 B4A1B5 . The probability of observing the new statement DB CXBD BMBMBMDB CXD1 under this mechanism is given by: CG D4 D8 BND4 D7 BND4 DC APB4D4 D8 BND4 D7 BND4 DC B5D4 DC B4DC CX B5 D1 CH CYBPBD B4 D4 D8 B4DB CXCY B5 if CQ CXCY BPCC D4 D7 B4DB CXCY B5 otherwise (1) The summation in equation (1) goes over all possible pairs of language models D4 D8 BND4 D7, but we can avoid integration by specifying a mass function APB4B5 that assigns nonzero probabilities to a finite subset of points in C1C8A2C1C8A2C1C8 DC . We accomplish this by using a nonparametric estimate for APB4B5, the details of which are provided below." ></td>
	<td class="line x" title="71:229	3.2.1 A nonparametric generative mass function We use a nonparametric estimate for APB4A1BNA1BNA1B5, which makes our generative model similar to kernel-based density estimators or Parzen-window classifiers (Silverman, 1986)." ></td>
	<td class="line x" title="72:229	The primary difference is that our model operates over discrete events (strings of words), and accordingly the mass function is defined over the space of distributions, rather than directly over the data points." ></td>
	<td class="line x" title="73:229	Our estimate relies on a collection of paired observations BV BP CUDBD8 CX BNDB D7 CX BNDC CX BM CXBPBDBMBMD2CV, which represent statements for which we know which words are topic words B4DB D8 CX B5, and which are sentiment words B4DB D7 CX B5." ></td>
	<td class="line x" title="74:229	Each of these observations corresponds to a unique point D4 D8CX BND4 D7CX BND4 DCCX in the space of paired distributions C1C8A2C1C8A2C1C8 DC, defined by the following coordinates: D4 D8CX B4DAB5 BP AL D8 AZB4DABNDB D8 CX B5BPAZB4DB D8 CX B5 B7 B4BDA0AL D8 B5CR D8DA D4 D7CX B4DAB5 BP AL D7 AZB4DABNDB D7 CX B5BPAZB4DB D7 CX B5 B7 B4BDA0AL D7 B5CR D7DA D4 DCCX B4DCB5 BP AL DC BD DCBPDC CX B7 B4BDA0AL DC B5BM (2) Here, AZB4DABNDB D8 CX B5 represents the number of times the word DA was observed in the topic part of statement CX, the length of which is denoted by AZB4DB D8 CX B5." ></td>
	<td class="line x" title="75:229	CR D8DA stands for the relative frequency of DA in the topic part of the collection." ></td>
	<td class="line x" title="76:229	The same definitions apply to the sentiment parameters AZB4DABNDB D7 CX B5, AZB4DB D7 CX B5 and CR D7DA . The Boolean indicator function BD DD returns one when the predicate DD is true and zero otherwise." ></td>
	<td class="line x" title="77:229	Metaparameters AL D8, AL D7 and AL DC specify the amount of Dirichlet smoothing (Zhai and Lafferty, 2001) applied to the topic, sentiment and polarity estimates respectively; values for these parameters are determined empirically." ></td>
	<td class="line x" title="78:229	We define APB4D4 D8 BND4 D7 BND4 DC B5 to have mass BD D2 when its argument D4 D8 BND4 D7 BND4 DC corresponds to some observation D4 D8CX BND4 D7CX BND4 DCCX, and zero otherwise: APB4D4 D8 BND4 D7 BND4 DC B5 BP BD D2 D2 CG CXBPBD BD D4 D8 BPD4 D8CX A2BD D4 D7 BPD4 D7CX A2BD D4 DC BPD4 DCCX BM (3) Equation (3) maintains empirical dependencies between the topic language model D4 D8 and the sentiment model D4 D7, because we assign nonzero prob347 ability mass only to pairs of models that actually co-occur in our observations." ></td>
	<td class="line x" title="79:229	3.2.2 Limitations of the model Our model represents each statement DB CX as a bag of words, or more formally an order-invariant sequence." ></td>
	<td class="line x" title="80:229	This representation is often confused with word independence, which is a much stronger assumption." ></td>
	<td class="line x" title="81:229	The generative model defined by equation (1) ignores the relative ordering of the words, but it does allow arbitrarily strong unordered dependencies among them." ></td>
	<td class="line x" title="82:229	To illustrate, consider the probability of observing the words unpredictable and plot in the same statement." ></td>
	<td class="line x" title="83:229	Suppose we set AL D8 BNAL D7 BPBD in equation (2), reducing the effects of smoothing." ></td>
	<td class="line x" title="84:229	It should be evident that C8B4unpredictable,plotB5 will be non-zero only when the two words actually co-occur in the training data." ></td>
	<td class="line x" title="85:229	By carefully selecting the smoothing parameters, the model can preserve dependencies between topic and sentiment words, and is quite capable of distinguishing the positive sentiment of unpredictable plot from the negative sentiment of unpredictable steering." ></td>
	<td class="line x" title="86:229	On the other hand, the model does ignore the ordering of the words, so it will not be able to differentiate the negative phrase gone from good to bad from its exact opposite." ></td>
	<td class="line x" title="87:229	Furthermore, our model is not well suited for modeling adjacency effects: the phrase unpredictable plot is treated in the same way as two separate words, unpredictable and plot, co-occurring in the same sentence." ></td>
	<td class="line x" title="88:229	3.3 Using the model for retrieval The generative model presented above can be applied to sentiment retrieval in the following fashion." ></td>
	<td class="line x" title="89:229	We start with a collection of statements BV and a query CUD5 D7 BND5 D8 BND5 DC CV supplied by the user." ></td>
	<td class="line x" title="90:229	We use the machinery outlined in Section 3.2 to estimate the topic and sentiment relevance models corresponding to the users information need, and then determine which statements in our collection most closely correspond to these models of relevance." ></td>
	<td class="line x" title="91:229	The topic relevance model CA D8 and sentiment relevance model CA D7 are estimated as follows." ></td>
	<td class="line x" title="92:229	We assume that our query D5 D7 BND5 D8 BND5 DC is a random sample from a distribution defined by equation (1), and then for each word DA we estimate the likelihood that DA would be observed if we sampled one more topic or sentiment word: CA D8 B4DAB5BP C8B4D5 D7 BND5 D8 DABND5 DC B5 C8B4D5 D7 BND5 D8 BND5 DC B5 BN CA D7 B4DAB5BP C8B4D5 D7 DABND5 D8 BND5 DC B5 C8B4D5 D7 BND5 D8 BND5 DC B5 BM (4) Both the numerator and denominator are computed according to equation (1), with the mass function APB4B5 given by equations (3) and (2)." ></td>
	<td class="line x" title="93:229	We use the notation D5DA to denote appending word DA to the string D5." ></td>
	<td class="line x" title="94:229	Estimation is done over the training corpus, which may or may not include numeric values of sentiment polarity.1 Once we have estimates for the topic and sentiment relevance models, we can rank testing statements DB by their similarity to CA D8 and CA D7 . We rank statements using a variation of cross-entropy, which was proposed by (Zhai, 2002): AB CG DA CA D8 B4DAB5D0D3CVD4 D8 B4DAB5B7B4BDA0ABB5 CG DA CA D7 B4DAB5D0D3CVD4 D7 B4DAB5BM (5) Here the summations extend over all words DA in the vocabulary, CA D8 and CA D7 are given by equation (4), while D4 D8 and D4 D7 are computed according to equation (2)." ></td>
	<td class="line x" title="95:229	A weighting parameter AB allows us to change the balance of topic and sentiment in the final ranking formula; its value is selected empirically." ></td>
	<td class="line x" title="96:229	4 Sentiment Retrieval Task 4.1 Task definition We define two variations of the sentiment retrieval task." ></td>
	<td class="line x" title="97:229	In one, the user supplies us with a numeric value for the desired polarity D5 DC." ></td>
	<td class="line x" title="98:229	In the other, the user supplies a set of seed words D5 D7, reflecting the desired sentiment." ></td>
	<td class="line x" title="99:229	The first task requires us to have polarity observations DC CX in our training data, while the second does not." ></td>
	<td class="line x" title="100:229	Task with training data: Input: (1) a set of topic keywordsD5 D8 and (2) a sentiment specification D5 DC BE CUA0BDBNBDCV." ></td>
	<td class="line x" title="101:229	In this case we assume D5 D7 to be the empty string." ></td>
	<td class="line x" title="102:229	Output: a ranked list of topic-relevant and sentiment-relevant sentences from the test data." ></td>
	<td class="line x" title="103:229	Task with seed words: Input: (1) a set of topic keywordsD5 D8 and (2) a set of sentiment seed words D5 D7 . In this case our model ignores D5 DC and DC CX . 1When the training corpus does not contain numeric polarity values DC CX, we assume APB4D4 D8 BND4 D7 BND4 DC B5BPAPB4D4 D8 BND4 D7 B5 and force D4 DC B4DC CX B5 to be a constant." ></td>
	<td class="line x" title="104:229	348 Output: a ranked list of topic-relevant and sentiment-relevant sentences from the test data." ></td>
	<td class="line x" title="105:229	In the first task, we split our corpus into three parts: (i) the training set, which was used for estimating the relevance models CA D7 and CA D8 ; (ii) the development set, which was used for tuning the model parameters AL D8, AL D7 and AB; and (iii) the testing set, from which we retrieved sentences in response to the query." ></td>
	<td class="line x" title="106:229	In the second task, we split the corpus into two parts: (i) the training set, which was used for tuning the model parameters; and (ii) the testing set, which was used for constructing CA D7 and CA D8 and from which we retrieved sentences in response to queries.2 The testing set was identical in both tasks." ></td>
	<td class="line x" title="107:229	Note that the sentiment relevance model CA D7 can be constructed in a topic-dependent fashion for both tasks." ></td>
	<td class="line x" title="108:229	4.2 Variations of the retrieval model slm: the retrieval model as described in Section 3.3." ></td>
	<td class="line x" title="109:229	lmt: the standard language modeling approach (Ponte and Croft, 1998; Song and Croft, 1999) on the topic keywords D5D8 for the topic part of the text DB D8." ></td>
	<td class="line x" title="110:229	lms: the standard language modeling approach on the sentiment keywords D5 D7 for the sentiment part of the text DB D7." ></td>
	<td class="line x" title="111:229	base: the weighted linear combination of lmt and lms." ></td>
	<td class="line x" title="112:229	rmt: only the topic relevance model was used for ranking using D5 D8 and for DB D8 .3 rms: only the sentiment relevance model was used for ranking using D5 D7 and for DB D7." ></td>
	<td class="line x" title="113:229	rmt-base: the slm model with AB BP BD, ignoring the sentiment relevance model." ></td>
	<td class="line x" title="114:229	rms-base: the slm model with AB BP BC, ignoring the topic relevance model." ></td>
	<td class="line x" title="115:229	2Because the training set was used for tuning the model parameters, no development set was required for this task." ></td>
	<td class="line x" title="116:229	3When we use the automatic annotation that is described in Section 5.2.2, we use the whole text instead of the topic part of the text, for the reasons given in that section." ></td>
	<td class="line x" title="117:229	This treatment is applied to the base, rmt-base, rms-base, rmt-rms, rmt-slm and slm models that are described in this section for using the automatic annotation." ></td>
	<td class="line x" title="118:229	However, we distinguish the lmt and rmt models using the topic part of the text and the lmtf and rmtf models, as baselines, using the whole text, respectively, even in the experiments using the automatic annotation." ></td>
	<td class="line x" title="119:229	rmt-rms: the rmt and rms models are treated independently." ></td>
	<td class="line x" title="120:229	rmt-slm: the rmt and rms-base models are combined." ></td>
	<td class="line x" title="121:229	lmtf: the standard language modeling approach using D5D8 for the nonsplit text, as baseline." ></td>
	<td class="line x" title="122:229	rmtf: the conventional relevance model was used for ranking using D5 D8 for the nonsplit text, as baseline." ></td>
	<td class="line x" title="123:229	lmtsf: the standard language modeling approach using both D5 D8 and D5 D7 for the nonsplit text, for reference." ></td>
	<td class="line x" title="124:229	rmtsf: the conventional relevance model was used for ranking using both D5 D8 and D5 D7 for the nonsplit text, for reference." ></td>
	<td class="line x" title="125:229	Note that the relevance models are constructed using training data for the training-based task, but are constructed using test data for the seed-based task, as mentioned in Section 4.1." ></td>
	<td class="line x" title="126:229	Therefore, the base model is only used for the training data, not for the test data, in the training-based task, while it can be performed for the test data in the case of the seed-based task." ></td>
	<td class="line x" title="127:229	Moreover, the lms, lmtsf and rmtsf models are based on the premise of using seed words to specify sentiments, and so they are only applicable to the seed-based task." ></td>
	<td class="line x" title="128:229	In the models described in this subsection, AL D8 and AL D7 in equation (2) were set to Dirichlet estimates (Zhai and Lafferty, 2001), AZB4DB D8 CX B5BPB4AZB4DB D8 CX B5B7AM D8 B5 and AZB4DB D7 CX B5BPB4AZB4DB D7 CX B5B7AM D7 B5 for the relevance models CA D8 and CA D7, respectively, in equation (4), and were fixed at 0.9 for ranking as in equation (5) for our experiments in Section 5." ></td>
	<td class="line x" title="129:229	Here, AM D8 and AM D7 were selected empirically according to the tasks described in Section 4.1." ></td>
	<td class="line x" title="130:229	The model parameter AB in equation (5) was also selected empirically in the same manner." ></td>
	<td class="line x" title="131:229	The number of ranked documents used in the relevance models CA D8 and CA D7, in equation (4), was selected empirically in the same manner as above; however, we fixed the number of terms used in the relevance models as 1000." ></td>
	<td class="line x" title="132:229	5 Experiments 5.1 Data set and evaluation measure We used the MPQA Opinion Corpus version 1.2 (Wilson et al. , 2005; Wiebe et al. , 2005) to measure the effectiveness of our sentiment re349 trieval models." ></td>
	<td class="line x" title="133:229	We summarize this data set as follows." ></td>
	<td class="line x" title="134:229	AF This corpus contains news articles collected from 187 different foreign and U.S. news sources from June 2001 to May 2002." ></td>
	<td class="line x" title="135:229	The corpus contains 535 documents, a total of 11,114 sentences." ></td>
	<td class="line x" title="136:229	AF The majority of the articles are on 10 different topics, which are labeled at document level, but, in addition to these, a number of additional articles were randomly selected from a larger corpus of 270,000 documents." ></td>
	<td class="line x" title="137:229	AF Each article was manually annotated using an annotation scheme for opinions and other private states at phrase level." ></td>
	<td class="line x" title="138:229	We only used the annotations for sentiments that included some attributes such as polarity and strength." ></td>
	<td class="line x" title="139:229	In this data set, the topic relevance for the 10 topics is known at the document level, but unknown at the sentence level." ></td>
	<td class="line x" title="140:229	We assumed that all the sentences in a relevant document could be considered relevant to the topic.4 This data set was annotated with sentiment polarities at the phrase level, but not explicitly annotated at the sentence level." ></td>
	<td class="line x" title="141:229	Therefore, we provided sentiment polarities at the sentence level to prepare training data and data for evaluation." ></td>
	<td class="line x" title="142:229	We set the sentence-level sentiment polarity equal to the polarity with the highest strength in each sentence.5 Queries were expressed using the title of one of the 10 topics and specified as positive or negative." ></td>
	<td class="line x" title="143:229	Thus, we had 20 types of queries for our experiments." ></td>
	<td class="line x" title="144:229	Because the supposed relevance judgments in this setting are imperfect at sentence level, we used bpref (Buckley and Voorhees, 2004), in both the training and testing phases, as it is known to be tolerant of imperfect judgments." ></td>
	<td class="line x" title="145:229	Bpref uses binary relevance judgments to define the preference relation (i.e. , any relevant document is preferred over any nonrelevant document for a given topic), while other measures, such as mean average precision, depend only on the ranks of the relevant documents." ></td>
	<td class="line x" title="146:229	4This is a strong assumption to make and may not be true in all cases." ></td>
	<td class="line x" title="147:229	A larger, more complete data set is required to perform a more detailed analysis, which is left as future work." ></td>
	<td class="line x" title="148:229	5We disregarded neutral and both if other polarities appeared." ></td>
	<td class="line x" title="149:229	We can also set the sentence-level sentiment polarity according to the presence of polarity in each sentence, but we did not consider this setting here." ></td>
	<td class="line x" title="150:229	5.2 Extracting sentiment expressions 5.2.1 Using manual annotation Because the MPQA corpus was annotated with phrase-level sentiments, we can use these annotations to split a sentence into a topic part DB D8 and a sentiment part DB D7." ></td>
	<td class="line x" title="151:229	The Krovetz stemmer (Krovetz, 1993) was applied to the topic part, the sentiment part and to the query terms6 and, for the retrieval experiments in Sections 5.3 and 5.4, a total of 418 stopwords from a standard stopword list were removed when they appeared." ></td>
	<td class="line x" title="152:229	5.2.2 Using automatic annotation In automatic extraction of sentiment expressions in this study, we detected sentiment-bearing words using lists of words with established polarities." ></td>
	<td class="line x" title="153:229	At this stage, topic dependence was not considered; however, at the stage of sentiment modeling, the topic dependence can be reflected, as described in Sections 3 and 4." ></td>
	<td class="line x" title="154:229	We first prepared a list of words indicating sentiments." ></td>
	<td class="line x" title="155:229	We used Hatzivassiloglou and McKeowns sentiment word list (Hatzivassiloglou and McKeown, 1997), which consists of 657 positive and 679 negative adjectives, and The General Inquirer (Stone et al. , 1966), which contains 1621 positive and 1989 negative words.7 By merging these lists, we obtained 1947 positive and 2348 negative words." ></td>
	<td class="line x" title="156:229	After stemming these words in the same manner as in Section 5.2.1, we were left with 1667 positive and 2129 negative words, which we will use hereafter in this paper." ></td>
	<td class="line x" title="157:229	The sentiment polarities are sometimes sensitive to the structural information, for instance, a negation expression reverses the following sentiment polarity." ></td>
	<td class="line x" title="158:229	To handle negation, every sentiment-bearing word was rewritten with a NEG suffix, such as good NEG, if an odd number of negation expressions was found within the five preceding words in the sentence." ></td>
	<td class="line x" title="159:229	To detect negation expressions, we used a predefined negation expression list." ></td>
	<td class="line oc" title="160:229	This negation handling is similar to that used in (Das and Chen, 2001; Pang et al. , 2002)." ></td>
	<td class="line x" title="161:229	We extracted sentiment-bearing expressions using the list of words with established po6We used the topic labels attached to the MPQA corpus as the topic query terms D5 D8 in all the experiments in Sections 5.3 and 5.4." ></td>
	<td class="line x" title="162:229	7We extracted positive and negative words from the General Inquirer basically in the same manner as in (Turney and Littman, 2003); however, we did not exclude any words, unlike (Turney and Littman, 2003), where some seed words were excluded for the evaluation of their work." ></td>
	<td class="line x" title="163:229	350 Table 1: Sample probabilities from the sentiment relevance models Reaction to President Bushs 2002 presidential election Israeli settlements in Topic-independent Topic-independent 2002 State of the Union Address in Zimbabwe Gaza and West Bank w/ manual annot." ></td>
	<td class="line x" title="164:229	w/ automatic annot." ></td>
	<td class="line x" title="165:229	w/ manual annot." ></td>
	<td class="line x" title="166:229	w/ automatic annot." ></td>
	<td class="line x" title="167:229	w/ manual annot." ></td>
	<td class="line x" title="168:229	w/ automatic annot." ></td>
	<td class="line x" title="169:229	w/ manual annot." ></td>
	<td class="line x" title="170:229	w/ automatic annot." ></td>
	<td class="line x" title="171:229	C8B4DBCYC9B5 DB C8B4DBCYC9B5 DB C8B4DBCYC9B5 DB C8B4DBCYC9B5 DB C8B4DBCYC9B5 DB C8B4DBCYC9B5 DB C8B4DBCYC9B5 DB C8B4DBCYC9B5 DB 0.047 demand 0.029 state 0.030 support 0.067 state 0.042 support 0.039 support 0.041 ask 0.097 settle 0.031 expect 0.026 support 0.016 promise 0.034 support 0.033 legitimate 0.033 legitimate 0.036 agreed 0.032 peace 0.031 defend 0.014 lead 0.014 call 0.024 call 0.031 free 0.033 lead 0.036 call 0.025 state 0.031 invite 0.013 call 0.014 excellent 0.019 meet 0.029 congratulate 0.025 free 0.033 aim 0.022 secure 0.031 humane 0.013 minister 0.013 goal 0.017 minister 0.028 fair 0.025 fair 0.028 immediate 0.015 call 0.031 safeguard 0.011 right 0.013 express 0.015 promise 0.023 please 0.018 state 0.025 aware 0.014 conflict 0.031 nutritious 0.010 foreign 0.013 best 0.014 white 0.017 confident 0.017 congratulate 0.024 key 0.013 support 0.031 helpful 0.009 hope 0.012 count 0.013 foreign 0.017 call 0.015 call 0.022 expect 0.012 right 0.016 time 0.009 meet 0.012 cooperate 0.012 success 0.012 hopeful 0.015 meet 0.018 justify 0.011 attack 0.016 say 0.008 interest 0.011 proposal 0.011 defense 0.012 express 0.013 unity 0.018 honoure 0.011 minister 0.091 evil 0.037 state 0.065 evil 0.098 state 0.029 flaw 0.028 flaw 0.018 palestinian 0.100 settle 0.080 axis 0.022 evil 0.049 axis 0.051 evil 0.018 condemn 0.026 critic 0.013 protest 0.031 state 0.045 threat 0.015 right 0.022 critic 0.028 critic 0.015 true 0.023 state 0.012 decide 0.019 peace 0.033 qualify 0.015 prison 0.011 prepare 0.017 call 0.014 critic 0.022 opposition 0.011 peace 0.014 secure NEG 0.030 wrote 0.013 critic 0.010 recognize 0.012 interest 0.012 expect 0.019 reject 0.011 fatten 0.013 critic 0.020 particular 0.010 human 0.010 reckless 0.011 move 0.011 reject 0.017 condemn 0.011 believe 0.012 force 0.020 word 0.008 support 0.010 country 0.011 reject 0.011 s 0.016 legal 0.009 plan 0.012 attack 0.018 harsh 0.008 protest 0.009 upset 0.010 slam 0.011 fair 0.015 move 0.009 fear 0.012 war 0.015 reject 0.008 war 0.009 pick 0.010 right 0.011 free 0.015 democratic 0.009 mistake 0.011 believe 0.015 dangerous 0.008 force 0.009 eyesore 0.010 attack 0.010 angry 0.014 support 0.009 continue 0.011 minister The upper and lower tables correspond to positive and negative sentiments, respectively." ></td>
	<td class="line x" title="172:229	The topic-independent sentiment relevance models (in the left two columns) correspond to rms, and the topic-dependent models (in the rest of the columns) correspond to rms-base, which is used for slm." ></td>
	<td class="line x" title="173:229	larities, considering negation, as described above." ></td>
	<td class="line x" title="174:229	Note that we used the list of words with sentiments to extract sentiment expressions, but we did not use the predefined sentiments to model sentiment relevance." ></td>
	<td class="line x" title="175:229	Some expressions are sometimes used to express a certain topic, such as settlements in Israeli settlements in Gaza and West Bank; but at other times are used to express a certain sentiment, such as the same word in All parties signed courtmediated compromise settlements." ></td>
	<td class="line x" title="176:229	Therefore, we will use whole sentences to model topic relevance, while we will use the automatically extracted sentiment expressions to model sentiment relevance, in Sections 5.3 and 5.4." ></td>
	<td class="line x" title="177:229	5.3 Experiments on training-based task We conducted experiments on the training-based task described in Section 4.1, using either manual annotation as described in Section 5.2.1 or automatic annotation as described in Section 5.2.2." ></td>
	<td class="line x" title="178:229	Table 1 contrasts sample probabilities from topicindependent sentiment relevance models and those from topic-dependent sentiment relevance models." ></td>
	<td class="line x" title="179:229	In the left two columns of this table, two sets of sample probabilities using the topic-independent model are presented." ></td>
	<td class="line x" title="180:229	One was computed from the manual annotation and the other was computed from the automatic annotation." ></td>
	<td class="line x" title="181:229	In the remaining columns, samples using the topic-dependent model are shown according to the three topics: (1) reaction to President Bushs 2002 State of the Union Address, (2) 2002 presidential election in Zimbabwe, and (3) Israeli settlements in Gaza and West Bank." ></td>
	<td class="line x" title="182:229	A number of positive expressions appeared topic dependent, such as promise (stemmed from promising or not) and support for Topic (1), legitimate and congratulate for Topic (2) and justify and secure for Topic (3); while negative expressions appeared topic-dependent, such as critic (stemmed from criticism) and eyesore for Topic (1), flaw and condemn for Topic (2) and mistake and secure NEG (i.e. , secure was negated) for Topic (3)." ></td>
	<td class="line x" title="183:229	Some expressions were unexpectedly generated regardless of the types of annotation, e.g., palestinian for Topic (3); however, we found some characteristics in the results using automatic annotation." ></td>
	<td class="line x" title="184:229	Some expressions on opinions that did not convey sentiments, such as state, frequently appeared regardless of topic." ></td>
	<td class="line x" title="185:229	This sort of expression may effectively function as degrading sentences only conveying facts, but may function harmfully by catching sentences conveying opinions without sentiments in the task of sentiment retrieval." ></td>
	<td class="line x" title="186:229	Some topic expressions, such as settle (stemmed from settlement or not) for Topic (3), were generated, because such words convey positive sentiments in some other contexts and thus they were contained in the list of sentiment-bearing words that we used for automatic annotation." ></td>
	<td class="line x" title="187:229	This will not cause a topic relevance model to drift, because we modeled the topic relevance using whole sentences, as described in Section 5.2.2; however, it may harm the sentiment relevance model to some extent." ></td>
	<td class="line x" title="188:229	351 Table 2: Experimental results of training-based task using manually annotated data 10% 25% 40% Models Bpref (AvgP) Bpref (AvgP) Bpref (AvgP) lmtf 0.1389 (0.1135) 0.1389 (0.1135) 0.1386 (0.1145) lmt 0.1499 (0.1164) 0.1499 (0.1164) 0.1444 (0.1148) rmtf 0.1811 (0.1706) 0.1887 (0.1770) 0.1841 (0.1691) rmt 0.1712 (0.1619) 0.1712 (0.1619) 0.1922 (0.1705) rmt-base 0.1922 (0.1723) 0.2005 (0.1812) 0.2100* (0.1951) rms 0.0464 (0.0384) 0.0452 (0.0394) 0.0375 (0.0320) rms-base 0.0772 (0.0640) 0.0869 (0.0704) 0.0865 (0.0724) rmt-rms 0.2025 (0.1413) 0.2210 (0.1925) 0.2117 (0.2003) rmt-slm 0.2278* (0.1715) 0.2249 (0.1676) 0.1999 (0.1819) slm 0.2006 (0.1914) 0.2247 (0.1824) 0.2441* (0.2427) * indicates statistically significant improvement over rmtf where D4 BO BCBMBCBH with the twosided Wilcoxon signed-rank test." ></td>
	<td class="line x" title="189:229	We performed retrieval experiments in the steps described in Section 4.1." ></td>
	<td class="line x" title="190:229	For this purpose, we split the data into three parts: (i) DC% as the training data, (ii) B4BHBC A0 DCB5% as the evaluation data, and (iii) BHBC% as the test data." ></td>
	<td class="line x" title="191:229	The test results of training-based task using manually annotated data and automatically annotated data are shown in Tables 2 and 3, respectively." ></td>
	<td class="line x" title="192:229	The scores were computed according to the bpref evaluation measure (Buckley and Voorhees, 2004), as mentioned in Section 5.1." ></td>
	<td class="line x" title="193:229	In addition to the bpref, mean average precision values are presented as AvgP in the tables, for reference.8 In these tables, the top row indicates the percentages of the training data DC." ></td>
	<td class="line x" title="194:229	It turned out that in all our experiments the appropriate fraction of training data was 40%." ></td>
	<td class="line x" title="195:229	In this setting, our slm model worked 76.1% better than the query likelihood model and 32.6% better than the conventional relevance model, when using manual annotation, and both improvements were statistically significant according to the Wilcoxon signed-rank test.9 When using automatic annotation, the slm model worked 67.2% better than the query likelihood model and 25.9% better than the conventional relevance model, where both improvements were statistically significant." ></td>
	<td class="line x" title="196:229	The rmt-base model also worked well with automatic annotation." ></td>
	<td class="line x" title="197:229	5.4 Experiments on seed-based task For experiments on the seed-based task that was described in Section 4.1, we used three groups of 8As mentioned in Section 5.1, the bpref is more appropriate for the evaluation of our experiments than the mean average precision." ></td>
	<td class="line x" title="198:229	9Significance tests involved only 20 queries, which makes it difficult to achieve statistical significance." ></td>
	<td class="line x" title="199:229	Table 3: Experimental results of training-based task using automatically annotated data 10% 25% 40% Models Bpref (AvgP) Bpref (AvgP) Bpref (AvgP) lmtf 0.1389 (0.1135) 0.1389 (0.1135) 0.1386 (0.1145) lmt 0.1325 (0.0972) 0.1315 (0.0976) 0.1325 (0.0972) rmtf 0.1811 (0.1706) 0.1887 (0.1770) 0.1841 (0.1691) rmt 0.1490 (0.1418) 0.1762 (0.1584) 0.1695 (0.1485) rmt-base 0.2076* (0.1936) 0.2252* (0.2139) 0.2302* (0.2196) rms 0.0347 (0.0287) 0.0501 (0.0408) 0.0501 (0.0408) rms-base 0.0943 (0.0733) 0.1196 (0.0896) 0.1241 (0.0979) rmt-rms 0.1690 (0.1182) 0.2063 (0.1938) 0.1603 (0.1591) rmt-slm 0.1980 (0.1426) 0.2013 (0.1835) 0.2148 (0.1882) slm 0.2011 (0.1537) 0.2261* (0.1716) 0.2318* (0.1802) * indicates statistically significant improvement over rmtf where D4 BO BCBMBCBH with the twosided Wilcoxon signed-rank test." ></td>
	<td class="line x" title="200:229	seed words: C3BTC5, CCCDCA and C7CABZ." ></td>
	<td class="line x" title="201:229	Each group consists of a positive word set D5 D7 B4B7B5 and a negative word set D5 D7 B4A0B5, as follows: C3BTC5: D5 D7 B4B7B5 BP CUgoodCV, and D5 D7 B4A0B5 BP CUbadCV." ></td>
	<td class="line x" title="202:229	CCCDCA: D5 D7 B4B7B5 BPCUgood, nice, excellent, positive, fortunate, correct, superiorCV, and D5 D7 B4A0B5 BPCUbad, nasty, poor, negative, unfortunate, wrong, inferiorCV." ></td>
	<td class="line x" title="203:229	C7CABZ: D5 D7 B4B7B5 BP CUsupport, demand, promise, want, hopeCV, and D5D7 B4A0B5 BP CUrefuse, accuse, criticism, fear, rejectCV." ></td>
	<td class="line x" title="204:229	C3BTC5 and CCCDCA were used in (Kamps and Marx, 2002) and (Turney and Littman, 2003), respectively." ></td>
	<td class="line x" title="205:229	We constructed C7CABZ considering sentiment-bearing words that may frequently appear in newspaper articles." ></td>
	<td class="line x" title="206:229	We experimented with the seed-based task, making use of each of these seed word groups, in the steps described in Section 4.1." ></td>
	<td class="line x" title="207:229	For this purpose, we split the data into two parts: (i) 50% as the estimation data and (ii) 50% as the test data." ></td>
	<td class="line x" title="208:229	The test results using manually annotated data and automatically annotated data are shown in Tables 4 and 5, respectively, where the scores were computed according to the bpref evaluation measure." ></td>
	<td class="line x" title="209:229	Mean average precision values are also presented as AvgP in the tables, for reference." ></td>
	<td class="line x" title="210:229	When using the manually annotated approach, our slm model worked well, especially with the seed word group C7CABZ, as shown in Table 4." ></td>
	<td class="line x" title="211:229	Using C7CABZ, the slm model worked 61.2% better than the query likelihood model and 15.2% better than the conventional relevance model, where both improvements were statistically significant according to the Wilcoxon signed-rank test." ></td>
	<td class="line x" title="212:229	Even 352 Table 4: Experimental results of seed-based task using manually annotated data ORG TUR KAM Models Bpref (AvgP) Bpref (AvgP) Bpref (AvgP) lmtf 0.1385 (0.1119) 0.1385 (0.1119) 0.1385 (0.1119) lmtsf 0.1182 (0.1035) 0.1061 (0.0884) 0.1330 (0.1062) lmt 0.1501 (0.1171) 0.1501 (0.1171) 0.1501 (0.1171) base 0.1615 (0.1319) 0.1531 (0.1217) 0.1514 (0.1180) rmtf 0.1938 (0.1776) 0.1938 (0.1776) 0.1938 (0.1776) rmtsf 0.1884 (0.1775) 0.1661 (0.1412) 0.1927 (0.1754) rmt 0.1974 (0.1826) 0.1974 (0.1826) 0.1974 (0.1826) rmt-base 0.1960 (0.1918) 0.1931 (0.1703) 0.1837 (0.1721) rms 0.0434 (0.0262) 0.0295 (0.0205) 0.0280 (0.0170) rms-base 0.1142 (0.1022) 0.1144 (0.0841) 0.1226 (0.0973) rmt-rms 0.1705 (0.1117) 0.1403 (0.1424) 0.1405 (0.0842) rmt-slm 0.2266* (0.2034) 0.2272* (0.2012) 0.2264* (0.2016) slm 0.2233* (0.2048) 0.2160 (0.1945) 0.2072 (0.1929) * indicates statistically significant improvement over rmtf where D4 BO BCBMBCBH with the twosided Wilcoxon signed-rank test." ></td>
	<td class="line x" title="213:229	using the other seed word groups, the slm model worked 4956% better than the query likelihood model and 612% better than the conventional relevance model; however, the latter improvement was not statistically significant." ></td>
	<td class="line x" title="214:229	The rmt-slm model also worked well with manual annotation." ></td>
	<td class="line x" title="215:229	When using automatic annotation, the slm model worked 4648% better than the query likelihood model and 46% better than the conventional relevance model, as shown in Table 5." ></td>
	<td class="line x" title="216:229	The improvements over the conventional relevance model were statistically significant only when using CCCDCA or C3BTC5; however, the score when using C7CABZ is almost comparable with the others." ></td>
	<td class="line x" title="217:229	6 Conclusion We propose sentiment retrieval models in the framework of probabilistic generative models, not only assuming that a user inputs query terms expressing a certain topic, but also assuming that the user specifies a sentiment polarity of interest either as a sentiment specification D5 DC BE CUA0BDBNBDCV or as a set of sentiment seed words D5 D7." ></td>
	<td class="line x" title="218:229	For this purpose, we combine sentiment relevance models and topic relevance models, considering the topic dependence of the sentiment." ></td>
	<td class="line x" title="219:229	In our experiments, our model worked significantly better than standard language modeling approaches, both when using D5DC and D5D7, and with both manual and automatic annotation of the fragments expressing sentiments in text." ></td>
	<td class="line x" title="220:229	With D5 D7 and automatic annotation, our model still worked significantly better than the standard approaches; however, the perTable 5: Experimental results of seed-based task using automatically annotated data ORG TUR KAM Models Bpref (AvgP) Bpref (AvgP) Bpref (AvgP) lmtf 0.1385 (0.1119) 0.1385 (0.1119) 0.1385 (0.1119) lmtsf 0.1182 (0.1035) 0.1061 (0.0884) 0.1330 (0.1062) lmt 0.1325 (0.0972) 0.1325 (0.0972) 0.1325 (0.0972) basef 0.1550 (0.1369) 0.1451 (0.1188) 0.1416 (0.1142) rmtf 0.1938 (0.1776) 0.1938 (0.1776) 0.1938 (0.1776) rmtsf 0.1884 (0.1775) 0.1661 (0.1412) 0.1927 (0.1754) rmt 0.1757 (0.1578) 0.1757 (0.1578) 0.1757 (0.1578) rmt-base 0.1957 (0.1862) 0.1976 (0.1882) 0.1825 (0.1704) rms 0.0421 (0.0236) 0.0364 (0.0205) 0.0217 (0.0147) rms-base 0.1268 (0.1096) 0.1301 (0.1148) 0.1326 (0.1158) rmt-rms 0.1465 (0.1514) 0.1390 (0.1393) 0.1252 (0.0757) rmt-slm 0.1977 (0.1811) 0.2008 (0.1649) 0.1959 (0.1677) slm 0.2031 (0.1714) 0.2055* (0.1668) 0.2044* (0.1698) * indicates statistically significant improvement over rmtf where D4 BO BCBMBCBH with the twosided Wilcoxon signed-rank test." ></td>
	<td class="line x" title="221:229	formance did not reach that achieved with other settings." ></td>
	<td class="line x" title="222:229	We believe the performance can be improved with larger-scale data." ></td>
	<td class="line x" title="223:229	We experimented to find sentences that were relevant to a given topic and were appropriate to a given sentiment; however, our models can also be applied to textual chunks of any length, such as at document level or passage level." ></td>
	<td class="line x" title="224:229	Our model can be easily extended to opinion retrieval, if the opinion retrieval is defined as retrieving sentences or documents that contain either positive or negative sentiments." ></td>
	<td class="line x" title="225:229	This issue is worth pursuing in future work." ></td>
	<td class="line x" title="226:229	Approaches considering polarity strength or continuous values for the polarity specification, rather than using CUA0BDBNBDCV, can also be considered in future work." ></td>
	<td class="line x" title="227:229	Acknowledgments We thank James Allan, W. Bruce Croft and the anonymous reviewers for valuable discussions and comments." ></td>
	<td class="line x" title="228:229	This work was supported in part by the Overseas Research Scholars Program and the Grant-in-Aid for Scientific Research (#17680011) from the Ministry of Education, Culture, Sports, Science and Technology, Japan, in part by the Telecommunications Advancement Foundation, Japan, in part by the Center for Intelligent Information Retrieval, and in part by the Defense Advanced Research Projects Agency (DARPA), USA under contract number HR0011-06-C-0023." ></td>
	<td class="line x" title="229:229	Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect those of the sponsor." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-1652
Feature Subsumption For Opinion Analysis
Riloff, Ellen;Patwardhan, Siddharth;Wiebe, Janyce M.;"></td>
	<td class="line x" title="1:189	Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 440448, Sydney, July 2006." ></td>
	<td class="line x" title="2:189	c2006 Association for Computational Linguistics Feature Subsumption for Opinion Analysis Ellen Riloff and Siddharth Patwardhan School of Computing University of Utah Salt Lake City, UT 84112 {riloff,sidd}@cs.utah.edu Janyce Wiebe Department of Computer Science University of Pittsburgh Pittsburgh, PA 15260 wiebe@cs.pitt.edu Abstract Lexical features are key to many approaches to sentiment analysis and opinion detection." ></td>
	<td class="line x" title="3:189	A variety of representations have been used, including single words, multi-word Ngrams, phrases, and lexicosyntactic patterns." ></td>
	<td class="line x" title="4:189	In this paper, we use a subsumption hierarchy to formally de ne different types of lexical features and their relationship to one another, both in terms of representational coverage and performance." ></td>
	<td class="line x" title="5:189	We use the subsumption hierarchy in two ways: (1) as an analytic tool to automatically identify complex features that outperform simpler features, and (2) to reduce a feature set by removing unnecessary features." ></td>
	<td class="line x" title="6:189	We show that reducing the feature set improves performance on three opinion classi cation tasks, especially when combined with traditional feature selection." ></td>
	<td class="line x" title="7:189	1 Introduction Sentiment analysis and opinion recognition are active research areas that have many potential applications, including review mining, product reputation analysis, multi-document summarization, and multi-perspective question answering." ></td>
	<td class="line x" title="8:189	Lexical features are key to many approaches, and a variety of representations have been used, including single words, multi-word Ngrams, phrases, and lexico-syntactic patterns." ></td>
	<td class="line x" title="9:189	It is common for different features to overlap representationally." ></td>
	<td class="line x" title="10:189	For example, the unigram happy will match all of the texts that the bigram very happy matches." ></td>
	<td class="line x" title="11:189	Since both features represent a positive sentiment and the bigram matches fewer contexts than the unigram, it is probably suf cient just to have the unigram." ></td>
	<td class="line x" title="12:189	However, there are many cases where a feature captures a subtlety or non-compositional meaning that a simpler feature does not." ></td>
	<td class="line x" title="13:189	For example, basket case is a highly opinionated phrase, but the words basket and case individually are not." ></td>
	<td class="line x" title="14:189	An open question in opinion analysis is how often more complex feature representations are needed, and which types of features are most valuable." ></td>
	<td class="line x" title="15:189	Our rst goal is to devise a method to automatically identify features that are representationally subsumed by a simpler feature but that are better opinion indicators." ></td>
	<td class="line x" title="16:189	These subjective expressions could then be added to a subjectivity lexicon (Esuli and Sebastiani, 2005), and used to gain understanding about which types of complex features capture meaningful expressions that are important for opinion recognition." ></td>
	<td class="line x" title="17:189	Many opinion classi ers are created by adopting a kitchen sink approach that throws together a variety of features." ></td>
	<td class="line x" title="18:189	But in many cases adding new types of features does not improve performance." ></td>
	<td class="line oc" title="19:189	For example, Pang et al.(2002) found that unigrams outperformed bigrams, and unigrams outperformed the combination of unigrams plus bigrams." ></td>
	<td class="line x" title="21:189	Our second goal is to automatically identify features that are unnecessary because similar features provide equal or better coverage and discriminatory value." ></td>
	<td class="line x" title="22:189	Our hypothesis is that a reduced feature set, which selectively combines unigrams with only the most valuable complex features, will perform better than a larger feature set that includes the entire kitchen sink of features." ></td>
	<td class="line x" title="23:189	In this paper, we explore the use of a subsumption hierarchy to formally de ne the subsumption relationships between different types of textual features." ></td>
	<td class="line x" title="24:189	We use the subsumption hierarchy in two ways." ></td>
	<td class="line x" title="25:189	First, we use subsumption as an an440 alytic tool to compare features of different complexities and automatically identify complex features that substantially outperform their simpler counterparts." ></td>
	<td class="line x" title="26:189	Second, we use the subsumption hierarchy to reduce a feature set based on representational overlap and on performance." ></td>
	<td class="line x" title="27:189	We conduct experiments with three opinion data sets and show that the reduced feature sets can improve classi cation performance." ></td>
	<td class="line x" title="28:189	2 The Subsumption Hierarchy 2.1 Text Representations We analyze two feature representations that have been used for opinion analysis: Ngrams and Extraction Patterns." ></td>
	<td class="line x" title="29:189	Information extraction (IE) patterns are lexico-syntactic patterns that represent expressions which identify role relationships." ></td>
	<td class="line x" title="30:189	For example, the pattern <subj> ActVP(recommended) extracts the subject of active-voice instances of the verb recommended as the recommender." ></td>
	<td class="line x" title="31:189	The pattern <subj> PassVP(recommended) extracts the subject of passive-voice instances of recommended as the object being recommended." ></td>
	<td class="line x" title="32:189	(Riloff and Wiebe, 2003) explored the idea of using extraction patterns to represent more complex subjective expressions that have noncompositional meanings." ></td>
	<td class="line x" title="33:189	For example, the expression drive (someone) up the wall expresses the feeling of being annoyed, but the meanings of the words drive, up, and wall have no emotional connotations individually." ></td>
	<td class="line x" title="34:189	Furthermore, this expression is not a xed word sequence that can be adequately modeled by Ngrams." ></td>
	<td class="line x" title="35:189	Any noun phrase can appear between the words drive and up, so a exible representation is needed to capture the general pattern drives <NP> up the wall." ></td>
	<td class="line x" title="36:189	This example represents a general phenomenon: many expressions allow intervening noun phrases and/or modifying terms." ></td>
	<td class="line x" title="37:189	For example: stepped on <mods> toes Ex: stepped on the boss toes dealt <np> <mods> blow Ex: dealt the company a decisive blow brought <np> to <mods> knees Ex: brought the man to his knees (Riloff and Wiebe, 2003) also showed that syntactic variations of the same verb phrase can behave very differently." ></td>
	<td class="line x" title="38:189	For example, they found that passive-voice constructions of the verb ask had a 100% correlation with opinion sentences, but active-voice constructions had only a 63% correlation with opinions." ></td>
	<td class="line x" title="39:189	Pattern Type Example Pattern <subj> PassVP <subj> is satis ed <subj> ActVP <subj> complained <subj> ActVP Dobj <subj> dealt blow <subj> ActInfVP <subj> appear to be <subj> PassInfVP <subj> is meant to be <subj> AuxVP Dobj <subj> has position <subj> AuxVP Adj <subj> is happy ActVP <dobj> endorsed <dobj> InfVP <dobj> to condemn <dobj> ActInfVP <dobj> get to know <dobj> PassInfVP <dobj> is meant to be <dobj> Subj AuxVP <dobj> fact is <dobj> NP Prep <np> opinion on <np> ActVP Prep <np> agrees with <np> PassVP Prep <np> is worried about <np> InfVP Prep <np> to resort to <np> <possessive> NP <noun>s speech Figure 1: Extraction Pattern Types Our goal is to use the subsumption hierarchy to identify Ngram and extraction pattern features that are more strongly associated with opinions than simpler features." ></td>
	<td class="line x" title="40:189	We used three types of features in our research: unigrams, bigrams, and IE patterns." ></td>
	<td class="line x" title="41:189	The Ngram features were generated using the Ngram Statistics Package (NSP) (Banerjee and Pedersen, 2003).1 The extraction patterns (EPs) were automatically generated using the Sundance/AutoSlog software package (Riloff and Phillips, 2004)." ></td>
	<td class="line x" title="42:189	AutoSlog relies on the Sundance shallow parser and can be applied exhaustively to a text corpus to generate IE patterns that can extract every noun phrase in the corpus." ></td>
	<td class="line x" title="43:189	AutoSlog has been used to learn IE patterns for the domains of terrorism, joint ventures, and microelectronics (Riloff, 1996), as well as for opinion analysis (Riloff and Wiebe, 2003)." ></td>
	<td class="line x" title="44:189	Figure 1 shows the 17 types of extraction patterns that AutoSlog generates." ></td>
	<td class="line x" title="45:189	PassVP refers to passive-voice verb phrases (VPs), ActVP refers to active-voice VPs, InfVP refers to in nitive VPs, and AuxVP refers 1NSP is freely available for use under the GPL from http://search.cpan.org/dist/Text-NSP." ></td>
	<td class="line x" title="46:189	We discarded Ngrams that consisted entirely of stopwords." ></td>
	<td class="line x" title="47:189	We used a list of 281 stopwords." ></td>
	<td class="line x" title="48:189	441 to VPs where the main verb is a form of to be or to have . Subjects (subj), direct objects (dobj), PP objects (np), and possessives can be extracted by the patterns.2 2.2 The Subsumption Hierarchy We created a subsumption hierarchy that de nes the representational scope of different types of features." ></td>
	<td class="line x" title="49:189	We will say that feature A representationally subsumes feature B if the set of text spans that match feature A is a superset of the set of text spans that match feature B. For example, the unigram happy subsumes the bigram very happy because the set of text spans that match happy includes the text spans that match very happy . First, we de ne a hierarchy of valid subsumption relationships, shown in Figure 2." ></td>
	<td class="line x" title="50:189	The 2Gram node, for example, is a child of the 1Gram node because a 1Gram can subsume a 2Gram." ></td>
	<td class="line x" title="51:189	Ngrams may subsume extraction patterns as well." ></td>
	<td class="line x" title="52:189	Every extraction pattern has at least one corresponding 1Gram that will subsume it.3." ></td>
	<td class="line x" title="53:189	For example, the 1Gram recommended subsumes the pattern <subj> ActVP(recommended) because the pattern only matches active-voice instances of recommended . An extraction pattern may also subsume another extraction pattern." ></td>
	<td class="line x" title="54:189	For example, <subj> ActVP(recommended) subsumes <subj> ActVP(recommended) Dobj(movie) . To compare speci c features we need to formally de ne the representation of each type of feature in the hierarchy." ></td>
	<td class="line x" title="55:189	For example, the hierarchy dictates that a 2Gram can subsume the pattern ActInfVP <dobj>, but this should hold only if the words in the bigram correspond to adjacent words in the pattern." ></td>
	<td class="line x" title="56:189	For example, the 2Gram to sh subsumes the pattern ActInfVP(like to sh) <dobj> . But the 2Gram like sh should not subsume it." ></td>
	<td class="line x" title="57:189	Similarly, consider the pattern InfVP(plan) <dobj>, which represents the in nitive to plan . This pattern subsumes the pattern ActInfVP(want to plan) <dobj>, but it should not subsume the pattern ActInfVP(plan to start) . To ensure that different features truly subsume each other representationally, we formally de ne each type of feature based on words, sequential 2However, the items extracted by the patterns are not actually used by our opinion classi ers; only the patterns themselves are matched against the text." ></td>
	<td class="line x" title="58:189	3Because every type of extraction pattern shown in Figure 1 contains at least one word (not including the extracted phrases, which are not used as part of our feature representation)." ></td>
	<td class="line x" title="59:189	dependencies, and syntactic dependencies." ></td>
	<td class="line x" title="60:189	A sequential dependency between words wi and wi+1 means that wi and wi+1 must be adjacent, and that wi must precede wi+1." ></td>
	<td class="line x" title="61:189	Figure 3 shows the formal de nition of a bigram (2Gram) node." ></td>
	<td class="line x" title="62:189	The bigram is de ned as two words with a sequential dependency indicating that they must be adjacent." ></td>
	<td class="line x" title="63:189	Name = 2Gram Constituent[0] = WORD1 Constituent[1] = WORD2 Dependency = Sequential(0, 1) Figure 3: 2Gram De nition A syntactic dependency between words wi and wi+1 means that wi has a speci c syntactic relationship to wi+1, and wi must precede wi+1." ></td>
	<td class="line x" title="64:189	For example, consider the extraction pattern NP Prep <np>, in which the object of the preposition attaches to the NP." ></td>
	<td class="line x" title="65:189	Figure 4 shows the de nition of this extraction pattern in the hierarchy." ></td>
	<td class="line x" title="66:189	The pattern itself contains three components: the NP, the attaching preposition, and the object of the preposition (which is the NP that the pattern extracts)." ></td>
	<td class="line x" title="67:189	The de nition also includes two syntactic dependencies: the rst dependency is between the NP and the preposition (meaning that the preposition syntactically attaches to the NP), while the second dependency is between the preposition and the extraction (meaning that the extracted NP is the syntactic object of the preposition)." ></td>
	<td class="line x" title="68:189	Name = NP Prep <np> Constituent[0] = NP Constituent[1] = PREP Constituent[2] = NP EXTRACTION Dependency = Syntactic(0, 1) Dependency = Syntactic(1, 2) Figure 4: NP Prep <np> Pattern De nition Consequently, the bigram affair with will not subsume the extraction pattern affair with <np> because the bigram requires the noun and preposition to be adjacent but the pattern does not." ></td>
	<td class="line x" title="69:189	For example, the extraction pattern matches the text an affair in his mind with Countess Olenska but the bigram does not." ></td>
	<td class="line x" title="70:189	Conversely, the extraction pattern does not subsume the bigram either because the pattern requires syntactic attachment but the bigram does not." ></td>
	<td class="line x" title="71:189	For example, the bigram matches 442 <subj> ActVP <subj> ActInfVP <subj> ActVP Dobj <subj> PassVP <subj> PassInfVP InfVP <dobj> ActInfVP <dobj> PassInfVP <dobj> 1Gram 2Gram <possessive> NP <subj> AuxVP AdjP <subj> AuxVP Dobj ActVP <dobj> ActVP Prep <np> NP Prep <np> PassVP Prep <np> Subj AuxVP <dobj> 3Gram ActVP Prep:OF <np> InfVP Prep <np> NP Prep:OF <np> PassVP Prep:OF <np> 4Gram InfVP Prep:OF <np> Figure 2: The Subsumption Hierarchy the sentence He ended the affair with a sense of relief, but the extraction pattern does not." ></td>
	<td class="line x" title="72:189	Figure 5 shows the de nition of another extraction pattern, InfVP <dobj>, which includes both syntactic and sequential dependencies." ></td>
	<td class="line x" title="73:189	This pattern would match the text to protest high taxes . The pattern de nition has three components: the in nitive to, a verb, and the direct object of the verb (which is the NP that the pattern extracts)." ></td>
	<td class="line x" title="74:189	The de nition also shows two syntactic dependencies." ></td>
	<td class="line x" title="75:189	The rst dependency indicates that the verb syntactically attaches to the in nitive to . The second dependency indicates that the extracted NP syntactically attaches to the verb (i.e. , it is the direct object of that particular verb)." ></td>
	<td class="line x" title="76:189	The pattern de nition also includes a sequential dependency, which speci es that to must be adjacent to the verb." ></td>
	<td class="line x" title="77:189	Strictly speaking, our parser does not require them to be adjacent." ></td>
	<td class="line x" title="78:189	For example, the parser allows intervening adverbs to split in nitives (e.g. , to strongly protest high taxes ), and this does happen occasionally." ></td>
	<td class="line x" title="79:189	But split innitives are relatively rare, so in the vast majority of cases the in nitive to will be adjacent to the verb." ></td>
	<td class="line x" title="80:189	Consequently, we decided that a bigram (e.g. , to protest ) should representationally subsume this extraction pattern because the syntactic exibility afforded by the pattern is negligible." ></td>
	<td class="line x" title="81:189	The sequential dependency link represents this judgment call that the in nitive to and the verb are adjacent in most cases." ></td>
	<td class="line x" title="82:189	For all of the node de nitions, we used our best judgment to make decisions of this kind." ></td>
	<td class="line x" title="83:189	We tried to represent major distinctions between features, without getting caught up in minor differences that were likely to be negligible in practice." ></td>
	<td class="line x" title="84:189	Name = InfVP <dobj> Constituent[0] = INFINITIVE TO Constituent[1] = VERB Constituent[2] = DOBJ EXTRACTION Dependency = Syntactic(0, 1) Dependency = Syntactic(1, 2) Dependency = Sequential(0, 1) Figure 5: InfVP <dobj> Pattern De nition To use the subsumption hierarchy, we assign each feature to its appropriate node in the hierarchy based on its type." ></td>
	<td class="line x" title="85:189	Then we perform a topdown breadthrst traversal." ></td>
	<td class="line x" title="86:189	Each feature is compared with the features at its ancestor nodes." ></td>
	<td class="line x" title="87:189	If a features words and dependencies are a superset of an ancestors words and dependencies, then it is subsumed by the (more general) ancestor and discarded.4 When the subsumption process is nished, a feature remains in the hierarchy only if 4The words that they have in common must also be in the same relative order." ></td>
	<td class="line x" title="88:189	443 there are no features above it that subsume it." ></td>
	<td class="line x" title="89:189	2.3 Performance-based Subsumption Representational subsumption is concerned with whether one feature is more general than another." ></td>
	<td class="line x" title="90:189	But the purpose of using the subsumption hierarchy is to identify more complex features that outperform simpler ones." ></td>
	<td class="line x" title="91:189	Applying the subsumption hierarchy to features without regard to performance would simply eliminate all features that have a more general counterpart in the feature set." ></td>
	<td class="line x" title="92:189	For example, all bigrams would be discarded if their component unigrams were also present in the hierarchy." ></td>
	<td class="line x" title="93:189	To estimate the quality of a feature, we use Information Gain (IG) because that has been shown to work well as a metric for feature selection (Forman, 2003)." ></td>
	<td class="line x" title="94:189	We will say that feature A behaviorally subsumes feature B if two criteria are met: (1) A representationally subsumes B, and (2) IG(A) IG(B) , where  is a parameter representing an acceptable margin of performance difference." ></td>
	<td class="line x" title="95:189	For example, if =0 then condition (2) means that feature A is just as valuable as feature B because its information gain is the same or higher." ></td>
	<td class="line x" title="96:189	If >0 then feature A is allowed to be a little worse than feature B, but within an acceptable margin." ></td>
	<td class="line x" title="97:189	For example, =.0001 means that As information gain may be up to .0001 lower than Bs information gain, and that is considered to be an acceptable performance difference (i.e. , A is good enough that we are comfortable discarding B in favor of the more general feature A)." ></td>
	<td class="line x" title="98:189	Note that based on the subsumption hierarchy shown in Figure 2, all 1Grams will always survive the subsumption process because they cannot be subsumed by any other types of features." ></td>
	<td class="line x" title="99:189	Our goal is to identify complex features that are worth adding to a set of unigram features." ></td>
	<td class="line x" title="100:189	3 Data Sets We used three opinion-related data sets for our analyses and experiments: the OP data set created by (Wiebe et al. , 2004), the Polarity data set5 created by (Pang and Lee, 2004), and the MPQA data set created by (Wiebe et al. , 2005).6 The OP and Polarity data sets involve document-level opinion classi cation, while the MPQA data set involves 5Version v2.0, which is available at: http://www.cs.cornell.edu/people/pabo/movie-review-data/ 6Available at http://www.cs.pitt.edu/mpqa/databaserelease/ sentence-level classi cation." ></td>
	<td class="line x" title="101:189	The OP data consists of 2,452 documents from the Penn Treebank (Marcus et al. , 1993)." ></td>
	<td class="line x" title="102:189	Metadata tags assigned by the Wall Street Journal de ne the opinion/non-opinion classes: the class of any document labeled Editorial, Letter to the Editor, Arts & Leisure Review, or Viewpoint by the Wall Street Journal is opinion, and the class of documents in all other categories (such as Business and News) is non-opinion." ></td>
	<td class="line x" title="103:189	This data set is highly skewed, with only 9% of the documents belonging to the opinion class." ></td>
	<td class="line x" title="104:189	Consequently, a trivial (but useless) opinion classi er that labels all documents as nonopinion articles would achieve 91% accuracy." ></td>
	<td class="line x" title="105:189	The Polarity data consists of 700 positive and 700 negative reviews from the Internet Movie Database (IMDb) archive." ></td>
	<td class="line x" title="106:189	The positive and negative classes were derived from author ratings expressed in stars or numerical values." ></td>
	<td class="line x" title="107:189	The MPQA data consists of English language versions of articles from the world press." ></td>
	<td class="line x" title="108:189	It contains 9,732 sentences that have been manually annotated for subjective expressions." ></td>
	<td class="line x" title="109:189	The opinion/non-opinion classes are derived from the lower-level annotations: a sentence is an opinion if it contains a subjective expression of medium or higher intensity; otherwise, it is a non-opinion sentence." ></td>
	<td class="line x" title="110:189	55% of the sentences belong to the opinion class." ></td>
	<td class="line x" title="111:189	4 Using the Subsumption Hierarchy for Analysis In this section, we illustrate how the subsumption hierarchy can be used as an analytic tool to automatically identify features that substantially outperform simpler counterparts." ></td>
	<td class="line x" title="112:189	These features represent specialized usages and expressions that would be good candidates for addition to a subjectivity lexicon." ></td>
	<td class="line x" title="113:189	Figure 6 shows pairs of features, where the rst is more general and the second is more speci c. These feature pairs were identi ed by the subsumption hierarchy as being representationally similar but behaviorally different (so the more speci c feature was retained)." ></td>
	<td class="line x" title="114:189	The IGain column shows the information gain values produced from the training set of one cross-validation fold." ></td>
	<td class="line x" title="115:189	The Class column shows the class that the more speci c feature is correlated with (the more general feature is usually not strongly correlated with either class)." ></td>
	<td class="line x" title="116:189	The top table in Figure 6 contains examples for the opinion/non-opinion classi cation task from 444 Opinion/Non-Opinion Classi cation ID Feature IGain Class Example A1 line .0016 . . ." ></td>
	<td class="line x" title="117:189	issue consists of notes backed by credit line receivables A2 the line .0075 opin lays it on the line; steps across the line B1 nation .0046 . . ." ></td>
	<td class="line x" title="118:189	has 750,000 cable-tv subscribers around the nation B2 a nation .0080 opin Its not that we are spawning a nation of ascetics . . ." ></td>
	<td class="line x" title="119:189	C1 begin .0006 Campeau buyers will begin writing orders C2 begin with .0036 opin To begin with, we should note that in contrast D1 bene ts .0040 . . ." ></td>
	<td class="line x" title="120:189	earlier period included $235,000 in tax bene ts." ></td>
	<td class="line x" title="121:189	DEP NP Prep(bene ts to) .0090 opin . . ." ></td>
	<td class="line x" title="122:189	boon to the rich with no proven bene ts to the economy E1 due .0001 . . ." ></td>
	<td class="line x" title="123:189	an estimated $ 1.23 billion in debt due next spring EEP ActVP Prep(due to) .0038 opin Its all due to the intense scrutiny Positive/Negative Sentiment Classi cation ID Feature IGain Class Example F1 short .0014 to make a long story short F2 nothing short .0039 pos nothing short of spectacular G1 ugly .0008 an ugly monster on a cruise liner G2 and ugly .0054 neg its a disappointment to see something this dumb and ugly H1 disaster .0010 rated pg-13 for disaster related elements HEP AuxVP Dobj(be disaster) .0048 neg . . ." ></td>
	<td class="line x" title="124:189	this is such a confused disaster of a lm I1 work .0002 the next day during the drive to work IEP ActVP(work) .0062 pos the lm will work just as well J1 manages .0003 he still manages to nd time for his wife JEP ActInfVP(manages to keep) .0054 pos this lm manages to keep up a rapid pace Figure 6: Sample features that behave differently, as revealed by the subsumption hierarchy." ></td>
	<td class="line x" title="125:189	(1 ) unigram; 2 ) bigram; EP ) extraction pattern) the OP data." ></td>
	<td class="line x" title="126:189	The more speci c features are more strongly correlated with opinion articles." ></td>
	<td class="line x" title="127:189	Surprisingly, simply adding a determiner can dramatically change behavior." ></td>
	<td class="line x" title="128:189	Consider A2." ></td>
	<td class="line x" title="129:189	There are many subjective idioms involving the line (two are shown in the table; others include toe the line and draw the line ), while objective language about credit lines, phone lines, etc. uses the determiner less often." ></td>
	<td class="line x" title="130:189	Similarly, consider B2." ></td>
	<td class="line x" title="131:189	Adding a to nation often corresponds to an abstract reference used when making an argument (e.g. , a nation of ascetics ), whereas other instances of nation are used more literally (e.g. , the 6th largest in the nation )." ></td>
	<td class="line x" title="132:189	21% of feature B1s instances appear in opinion articles, while 70% of feature B2s instances are in opinion articles." ></td>
	<td class="line x" title="133:189	Begin with (C2) captures an adverbial phrase used in argumentation ( To begin with ) but does not match objective usages such as will begin an action." ></td>
	<td class="line x" title="134:189	The word bene ts alone (D1) matches phrases like tax bene ts and employee bene ts that are not opinion expressions, while DEP typically matches positive senses of the word bene ts . Interestingly, the bigram bene ts to is not highly correlated with opinions because it matches in nitive phrases such as tax bene ts to provide and health bene ts to cut . In this case, the extraction pattern NP Prep(bene ts to) is more discriminating than the bigram for opinion classi cation." ></td>
	<td class="line x" title="135:189	The extraction pattern EEP is also highly correlated with opinions, while the unigram due and the bigram due to are not." ></td>
	<td class="line x" title="136:189	The bottom table in Figure 6 shows feature pairs identi ed for their behavioral differences on the Polarity data set, where the task is to distinguish positive reviews from negative reviews." ></td>
	<td class="line x" title="137:189	F2 and G2 are bigrams that behave differently from their component unigrams." ></td>
	<td class="line x" title="138:189	The expression nothing short (of) is typically used to express positive sentiments, while nothing and short by themselves are not." ></td>
	<td class="line x" title="139:189	The word ugly is often used as a descriptive modi er that is not expressing a sentiment per se, while and ugly appears in predicate adjective constructions that are expressing a negative sentiment." ></td>
	<td class="line x" title="140:189	The extraction pattern HEP is more discriminatory than H1 because it distinguishes negative sentiments ( the lm is a disaster!" ></td>
	<td class="line x" title="141:189	) from plot descriptions ( the disaster movie )." ></td>
	<td class="line x" title="142:189	IEP shows that active-voice usages of work are strong positive indicators, while the unigram work appears in a variety of both positive and negative contexts." ></td>
	<td class="line x" title="143:189	Finally, JEP shows that the expression manages to keep is a strong positive indicator, while manages by itelf is much less discriminating." ></td>
	<td class="line x" title="144:189	445 These examples illustrate that the subsumption hierarchy can be a powerful tool to better understand the behaviors of different kinds of features, and to identify speci c features that may be desirable for inclusion in specialized lexical resources." ></td>
	<td class="line x" title="145:189	5 Using the Subsumption Hierarchy to Reduce Feature Sets When creating opinion classi ers, people often throw in a variety of features and trust the machine learning algorithm to gure out how to make the best use of them." ></td>
	<td class="line x" title="146:189	However, we hypothesized that classi ers may perform better if we can proactively eliminate features that are not necesary because they are subsumed by other features." ></td>
	<td class="line x" title="147:189	In this section, we present a series of experiments to explore this hypothesis." ></td>
	<td class="line x" title="148:189	First, we present the results for an SVM classi er trained using different sets of unigram, bigram, and extraction pattern features, both before and after subsumption." ></td>
	<td class="line x" title="149:189	Next, we evaluate a standard feature selection approach as an alternative to subsumption and then show that combining subsumption with standard feature selection produces the best results of all." ></td>
	<td class="line x" title="150:189	5.1 Classi cation Experiments To see whether feature subsumption can improve classi cation performance, we trained an SVM classi er for each of the three opinion data sets." ></td>
	<td class="line x" title="151:189	We used the SVMlight (Joachims, 1998) package with a linear kernel." ></td>
	<td class="line x" title="152:189	For the Polarity and OP data we discarded all features that have frequency < 5, and for the MPQA data we discarded features that have frequency < 2 because this data set is substantially smaller." ></td>
	<td class="line x" title="153:189	All of our experimental results are averages over 3-fold cross-validation." ></td>
	<td class="line x" title="154:189	First, we created 4 baseline classi ers: a 1Gram classi er that uses only the unigram features; a 1+2Gram classi er that uses unigram and bigram features; a 1+EP classi er that uses unigram and extraction pattern features, and a 1+2+EP classier that uses all three types of features." ></td>
	<td class="line x" title="155:189	Next, we created analogous 1+2Gram, 1+EP, and 1+2+EP classi ers but applied the subsumption hierarchy rst to eliminate unnecessary features before training the classi er." ></td>
	<td class="line x" title="156:189	We experimented with three delta values for the subsumption process: =.0005, .001, and .002." ></td>
	<td class="line x" title="157:189	Figures 7, 8, and 9 show the results." ></td>
	<td class="line x" title="158:189	The subsumption process produced small but consistent improvements on all 3 data sets." ></td>
	<td class="line x" title="159:189	For example, Figure 8 shows the results on the OP data, where all of the accuracy values produced after subsumption (the rightmost 3 columns) are higher than the accuracy values produced without subsumption (the Base[line] column)." ></td>
	<td class="line x" title="160:189	For all three data sets, the best overall accuracy (shown in boldface) was always achieved after subsumption." ></td>
	<td class="line x" title="161:189	Features Base =.0005 =.001 =.002 1Gram 79.8 1+2Gram 81.2 81.0 81.3 81.0 1+EP 81.7 81.4 81.4 82.0 1+2+EP 81.7 82.3 82.3 82.7 Figure 7: Accuracies on Polarity Data Features Base =.0005 =.001 =.002 1Gram 97.5 1+2Gram 98.0 98.7 98.6 98.7 1+EP 97.2 97.8 97.9 97.9 1+2+EP 97.8 98.6 98.7 98.7 Figure 8: Accuracies on OP Data Features Base =.0005 =.001 =.002 1Gram 74.8 1+2Gram 74.3 74.9 74.6 74.8 1+EP 74.4 74.6 74.6 74.6 1+2+EP 74.4 74.9 74.7 74.6 Figure 9: Accuracies on MPQA Data We also observed that subsumption had a dramatic effect on the F-measure scores on the OP data, which are shown in Figure 10." ></td>
	<td class="line x" title="162:189	The OP data set is fundamentally different from the other data sets because it is so highly skewed, with 91% of the documents belonging to the non-opinion class." ></td>
	<td class="line x" title="163:189	Without subsumption, the classi er was conservative about assigning documents to the opinion class, achieving F-measure scores in the 82-88 range." ></td>
	<td class="line x" title="164:189	After subsumption, the overall accuracy improved but the F-measure scores increased more dramatically." ></td>
	<td class="line x" title="165:189	These numbers show that the subsumption process produced not only a more accurate classi er, but a more useful classi er that identi es more documents as being opinion articles." ></td>
	<td class="line x" title="166:189	For the MPQA data, we get a very small improvement of 0.1% (74.8% ! 74.9%) using subsumption." ></td>
	<td class="line x" title="167:189	But note that without subsumption the performance actually decreased when bigrams and 446 Features Base =.0005 =.001 =.002 1Gram 84.5 1+2Gram 88.0 92.5 92.0 92.3 1+EP 82.4 86.9 87.4 87.4 1+2+EP 86.7 91.8 92.5 92.3 Figure 10: F-measures on OP Data 97.6 97.8 98 98.2 98.4 98.6 98.8 99 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Accuracy (%) Top N Baseline Subsumption d=0.002 Feature Selection Subsumption d=0.002 + Feature Selection Figure 11: Feature Selection on OP Data extraction patterns were added!" ></td>
	<td class="line x" title="168:189	The subsumption process counteracted the negative effect of adding the more complex features." ></td>
	<td class="line x" title="169:189	5.2 Feature Selection Experiments We conducted a second series of experiments to determine whether a traditional feature selection approach would produce the same, or better, improvements as subsumption." ></td>
	<td class="line x" title="170:189	For each feature, we computed its information gain (IG) and then selected the N features with the highest scores.7 We experimented with values of N ranging from 1,000 to 10,000 in increments of 1,000." ></td>
	<td class="line x" title="171:189	We hypothesized that applying subsumption before traditional feature selection might also help to identify a more diverse set of high-performing features." ></td>
	<td class="line x" title="172:189	In a parallel set of experiments, we explored this hypothesis by rst applying subsumption to reduce the size of the feature set, and then selecting the best N features using information gain." ></td>
	<td class="line x" title="173:189	Figures 11, 12, and 13 show the results of these experiments for the 1+2+EP classi ers." ></td>
	<td class="line x" title="174:189	Each graph shows four lines." ></td>
	<td class="line x" title="175:189	One line corresponds to the baseline classi er with no subsumption, and another line corresponds to the baseline classi er with subsumption using the best  value for that data set." ></td>
	<td class="line x" title="176:189	Each of these two lines corresponds to 7In the case of ties, we included all features with the same score as the Nth-best as well." ></td>
	<td class="line x" title="177:189	78 78.5 79 79.5 80 80.5 81 81.5 82 82.5 83 83.5 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Accuracy (%) Top N Baseline Subsumption d=0.002 Feature Selection Subsumption d=0.002 + Feature Selection Figure 12: Feature Selection on Polarity Data 72 72.5 73 73.5 74 74.5 75 75.5 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Accuracy (%) Top N Baseline Subsumption d=0.0005 Feature Selection Subsumption d=0.0005 + Feature Selection Figure 13: Feature Selection on MPQA Data just a single data point (accuracy value), but we drew that value as a line across the graph for the sake of comparison." ></td>
	<td class="line x" title="178:189	The other two lines on the graph correspond to (a) feature selection for different values of N (shown on the x-axis), and (b) subsumption followed by feature selection for different values of N. On all 3 data sets, traditional feature selection performs worse than the baseline in some cases, and it virtually never outperforms the best classier trained after subsumption (but without feature selection)." ></td>
	<td class="line x" title="179:189	Furthermore, the combination of subsumption plus feature selection generally performs best of all, and nearly always outperforms feature selection alone." ></td>
	<td class="line x" title="180:189	For all 3 data sets, our best accuracy results were achieved by performing subsumption prior to feature selection." ></td>
	<td class="line x" title="181:189	The best accuracy results are 99.0% on the OP data, 83.1% on the Polarity data, and 75.4% on the MPQA data." ></td>
	<td class="line x" title="182:189	For the OP data, the improvement over baseline for both accuracy and F-measure are statistically signi cant at the p < 0.05 level (paired t-test)." ></td>
	<td class="line x" title="183:189	For the MPQA data, the improvement over baseline is 447 statistically signi cant at the p < 0.10 level." ></td>
	<td class="line x" title="184:189	6 Related Work Many features and classi cation algorithms have been explored in sentiment analysis and opinion recognition." ></td>
	<td class="line oc" title="185:189	Lexical cues of differing complexities have been used, including single words and Ngrams (e.g. , (Mullen and Collier, 2004; Pang et al. , 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003; Wiebe et al. , 2004)), as well as phrases and lexico-syntactic patterns (e.g, (Kim and Hovy, 2004; Hu and Liu, 2004; Popescu and Etzioni, 2005; Riloff and Wiebe, 2003; Whitelaw et al. , 2005))." ></td>
	<td class="line n" title="186:189	While many of these studies investigate combinations of features and feature selection, this is the rst work that uses the notion of subsumption to compare Ngrams and lexico-syntactic patterns to identify complex features that outperform simpler counterparts and to reduce a combined feature set to improve opinion classi cation." ></td>
	<td class="line x" title="187:189	7 Conclusions This paper uses a subsumption hierarchy of feature representations as (1) an analytic tool to compare features of different complexities, and (2) an automatic tool to remove unnecessary features to improve opinion classi cation performance." ></td>
	<td class="line x" title="188:189	Experiments with three opinion data sets showed that subsumption can improve classi cation accuracy, especially when combined with feature selection." ></td>
	<td class="line x" title="189:189	Acknowledgments This research was supported by NSF Grants IIS0208798 and IIS-0208985, the ARDA AQUAINT Program, and the Institute for Scienti c Computing Research and the Center for Applied Scienti c Computing within Lawrence Livermore National Laboratory." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-2915
Which Side Are You On? Identifying Perspectives At The Document And Sentence Levels
Lin, Wei-Hao;Wilson, Theresa;Wiebe, Janyce M.;Hauptmann, Alexander G.;"></td>
	<td class="line x" title="1:149	Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), pages 109116, New York City, June 2006." ></td>
	<td class="line x" title="2:149	c2006 Association for Computational Linguistics Which Side are You on?" ></td>
	<td class="line x" title="3:149	Identifying Perspectives at the Document and Sentence Levels Wei-Hao Lin Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 whlin@cs.cmu.edu Theresa Wilson, Janyce Wiebe Intelligent Systems Program University of Pittsburgh Pittsburgh, PA 15260 {twilson,wiebe}@cs.pitt.edu Alexander Hauptmann School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 alex@cs.cmu.edu Abstract In this paper we investigate a new problem of identifying the perspective from which a document is written." ></td>
	<td class="line x" title="4:149	By perspective we mean a point of view, for example, from the perspective of Democrats or Republicans." ></td>
	<td class="line x" title="5:149	Can computers learn to identify the perspective of a document?" ></td>
	<td class="line x" title="6:149	Not every sentence is written strongly from a perspective." ></td>
	<td class="line x" title="7:149	Can computers learn to identify which sentences strongly convey a particular perspective?" ></td>
	<td class="line x" title="8:149	We develop statistical models to capture how perspectives are expressed at the document and sentence levels, and evaluate the proposed models on articles about the Israeli-Palestinian conflict." ></td>
	<td class="line x" title="9:149	The results show that the proposed models successfully learn how perspectives are reflected in word usage and can identify the perspective of a document with high accuracy." ></td>
	<td class="line x" title="10:149	1 Introduction In this paper we investigate a new problem of automatically identifying the perspective from which a document is written." ></td>
	<td class="line x" title="11:149	By perspective we mean a subjective evaluation of relative significance, a point-of-view.1 For example, documents about the Palestinian-Israeli conflict may appear to be about the same topic but reveal different perspectives: 1The American Heritage Dictionary of the English Language, 4th ed." ></td>
	<td class="line x" title="12:149	(1) The inadvertent killing by Israeli forces of Palestinian civilians  usually in the course of shooting at Palestinian terrorists  is considered no different at the moral and ethical level than the deliberate targeting of Israeli civilians by Palestinian suicide bombers." ></td>
	<td class="line x" title="13:149	(2) In the first weeks of the Intifada, for example, Palestinian public protests and civilian demonstrations were answered brutally by Israel, which killed tens of unarmed protesters." ></td>
	<td class="line x" title="14:149	Example 1 is written from an Israeli perspective; Example 2 is written from a Palestinian perspective." ></td>
	<td class="line x" title="15:149	Anyone knowledgeable about the issues of the Israeli-Palestinian conflict can easily identify the perspectives from which the above examples were written." ></td>
	<td class="line x" title="16:149	However, can computers learn to identify the perspective of a document given a training corpus?" ></td>
	<td class="line x" title="17:149	When an issue is discussed from different perspectives, not every sentence strongly reflects the perspective of the author." ></td>
	<td class="line x" title="18:149	For example, the following sentences were written by a Palestinian and an Israeli." ></td>
	<td class="line x" title="19:149	(3) The Rhodes agreements of 1949 set them as the ceasefire lines between Israel and the Arab states." ></td>
	<td class="line x" title="20:149	(4) The green line was drawn up at the Rhodes Armistice talks in 1948-49." ></td>
	<td class="line x" title="21:149	Examples 3 and 4 both factually introduce the background of the issue of the green line without expressing explicit perspectives." ></td>
	<td class="line x" title="22:149	Can we develop a 109 system to automatically discriminate between sentences that strongly indicate a perspective and sentences that only reflect shared background information?" ></td>
	<td class="line x" title="23:149	A system that can automatically identify the perspective from which a document is written will be a valuable tool for people analyzing huge collections of documents from different perspectives." ></td>
	<td class="line x" title="24:149	Political analysts regularly monitor the positions that countries take on international and domestic issues." ></td>
	<td class="line x" title="25:149	Media analysts frequently survey broadcast news, newspapers, and weblogs for differing viewpoints." ></td>
	<td class="line x" title="26:149	Without the assistance of computers, analysts have no choice but to read each document in order to identify those from a perspective of interest, which is extremely time-consuming." ></td>
	<td class="line x" title="27:149	What these analysts need is to find strong statements from different perspectives and to ignore statements that reflect little or no perspective." ></td>
	<td class="line x" title="28:149	In this paper we approach the problem of learning individual perspectives in a statistical framework." ></td>
	<td class="line x" title="29:149	We develop statistical models to learn how perspectives are reflected in word usage, and we treat the problem of identifying perspectives as a classification task." ></td>
	<td class="line x" title="30:149	Although our corpus contains documentlevel perspective annotations, it lacks sentence-level annotations, creating a challenge for learning the perspective of sentences." ></td>
	<td class="line x" title="31:149	We propose a novel statistical model to overcome this problem." ></td>
	<td class="line x" title="32:149	The experimental results show that the proposed statistical models can successfully identify the perspective from which a document is written with high accuracy." ></td>
	<td class="line x" title="33:149	2 Related Work Identifying the perspective from which a document is written is a subtask in the growing area of automatic opinion recognition and extraction." ></td>
	<td class="line x" title="34:149	Subjective language is used to express opinions, emotions, and sentiments." ></td>
	<td class="line oc" title="35:149	So far, research in automatic opinion recognition has primarily addressed learning subjective language (Wiebe et al. , 2004; Riloff et al. , 2003), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Yu and Hatzivassiloglou, 2003; Riloff et al. , 2003), and discriminating between positive and negative language (Pang et al. , 2002; Morinaga et al. , 2002; Yu and Hatzivassiloglou, 2003; Turney and Littman, 2003; Dave et al. , 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Wilson et al. , 2005)." ></td>
	<td class="line x" title="36:149	While by its very nature we expect much of the language that is used when presenting a perspective or point-of-view to be subjective, labeling a document or a sentence as subjective is not enough to identify the perspective from which it is written." ></td>
	<td class="line x" title="37:149	Moreover, the ideology and beliefs authors possess are often expressed in ways other than positive or negative language toward specific targets." ></td>
	<td class="line oc" title="38:149	Research on the automatic classification of movie or product reviews as positive or negative (e.g. , (Pang et al. , 2002; Morinaga et al. , 2002; Turney and Littman, 2003; Nasukawa and Yi, 2003; Mullen and Collier, 2004; Beineke et al. , 2004; Hu and Liu, 2004)) is perhaps the most similar to our work." ></td>
	<td class="line x" title="39:149	As with review classification, we treat perspective identification as a document-level classification task, discriminating, in a sense, between different types of opinions." ></td>
	<td class="line x" title="40:149	However, there is a key difference." ></td>
	<td class="line x" title="41:149	A positive or negative opinion toward a particular movie or product is fundamentally different from an overall perspective." ></td>
	<td class="line x" title="42:149	Ones opinion will change from movie to movie, whereas ones perspective can be seen as more static, often underpinned by ones ideology or beliefs about the world." ></td>
	<td class="line x" title="43:149	There has been research in discourse analysis that examines how different perspectives are expressed in political discourse (van Dijk, 1988; Pan et al. , 1999; Geis, 1987)." ></td>
	<td class="line x" title="44:149	Although their research may have some similar goals, they do not take a computational approach to analyzing large collections of documents." ></td>
	<td class="line x" title="45:149	To the best of our knowledge, our approach to automatically identifying perspectives in discourse is unique." ></td>
	<td class="line x" title="46:149	3 Corpus Our corpus consists of articles published on the bitterlemonswebsite2." ></td>
	<td class="line x" title="47:149	The website is set up to contribute to mutual understanding [between Palestinians and Israelis] through the open exchange of ideas.3 Every week an issue about the IsraeliPalestinian conflict is selected for discussion (e.g. , 2http://www.bitterlemons.org 3http://www.bitterlemons.org/about/ about.html 110 Disengagement: unilateral or coordinated?), and a Palestinian editor and an Israeli editor each contribute one article addressing the issue." ></td>
	<td class="line x" title="48:149	In addition, the Israeli and Palestinian editors invite one Israeli and one Palestinian to express their views on the issue (sometimes in the form of an interview), resulting in a total of four articles in a weekly edition." ></td>
	<td class="line x" title="49:149	We choose the bitterlemons website for two reasons." ></td>
	<td class="line x" title="50:149	First, each article is already labeled as either Palestinian or Israeli by the editors, allowing us to exploit existing annotations." ></td>
	<td class="line x" title="51:149	Second, the bitterlemons corpus enables us to test the generalizability of the proposed models in a very realistic setting: training on articles written by a small number of writers (two editors) and testing on articles from a much larger group of writers (more than 200 different guests)." ></td>
	<td class="line x" title="52:149	We collected a total of 594 articles published on the website from late 2001 to early 2005." ></td>
	<td class="line x" title="53:149	The distribution of documents and sentences are listed in Table 1." ></td>
	<td class="line x" title="54:149	We removed metadata from all articles, inPalestinian Israeli Written by editors 148 149 Written by guests 149 148 Total number of documents 297 297 Average document length 740.4 816.1 Number of sentences 8963 9640 Table 1: The basic statistics of the corpus cluding edition numbers, publication dates, topics, titles, author names and biographic information." ></td>
	<td class="line x" title="55:149	We used OpenNLP Tools4 to automatically extract sentence boundaries, and reduced word variants using the Porter stemming algorithm." ></td>
	<td class="line x" title="56:149	We evaluated the subjectivity of each sentence using the automatic subjective sentence classifier from (Riloff and Wiebe, 2003), and find that 65.6% of Palestinian sentences and 66.2% of Israeli sentences are classified as subjective." ></td>
	<td class="line x" title="57:149	The high but almost equivalent percentages of subjective sentences in the two perspectives support our observation in Section 2 that a perspective is largely expressed using subjective language, but that the amount of subjectivity in a document is not necessarily indicative of 4http://sourceforge.net/projects/ opennlp/ its perspective." ></td>
	<td class="line x" title="58:149	4 Statistical Modeling of Perspectives We develop algorithms for learning perspectives using a statistical framework." ></td>
	<td class="line x" title="59:149	Denote a training corpus as a set of documents Wn and their perspectives labels Dn, n = 1,,N, where N is the total number of documents in the corpus." ></td>
	<td class="line x" title="60:149	Given a new document W with a unknown document perspective, the perspective D is calculated based on the following conditional probability." ></td>
	<td class="line x" title="61:149	P( D| W,{Dn,Wn}Nn=1) (5) We are also interested in how strongly each sentence in a document conveys perspective information." ></td>
	<td class="line x" title="62:149	Denote the intensity of the m-th sentence of the n-th document as a binary random variable Sm,n. To evaluate Sm,n, how strongly a sentence reflects a particular perspective, we calculate the following conditional probability." ></td>
	<td class="line x" title="63:149	P(Sm,n|{Dn,Wn}Nn=1) (6) 4.1 Nave Bayes Model We model the process of generating documents from a particular perspective as follows: pi  Beta(pi,pi)   Dirichlet() Dn  Binomial(1,pi) Wn  Multinomial(Ln,d) First, the parameters pi and  are sampled once from prior distributions for the whole corpus." ></td>
	<td class="line x" title="64:149	Beta and Dirichlet are chosen because they are conjugate priors for binomial and multinomial distributions, respectively." ></td>
	<td class="line x" title="65:149	We set the hyperparameters pi,pi, and  to one, resulting in non-informative priors." ></td>
	<td class="line x" title="66:149	A document perspective Dn is then sampled from a binomial distribution with the parameter pi." ></td>
	<td class="line x" title="67:149	The value of Dn is either d0 (Israeli) or d1 (Palestinian)." ></td>
	<td class="line x" title="68:149	Words in the document are then sampled from a multinomial distribution, where Ln is the length of the document." ></td>
	<td class="line x" title="69:149	A graphical representation of the model is shown in Figure 1." ></td>
	<td class="line x" title="70:149	111 pi  Dn Wn N Figure 1: Nave Bayes Model The model described above is commonly known as a nave Bayes (NB) model." ></td>
	<td class="line x" title="71:149	NB models have been widely used for various classification tasks, including text categorization (Lewis, 1998)." ></td>
	<td class="line x" title="72:149	The NB model is also a building block for the model described later that incorporates sentence-level perspective information." ></td>
	<td class="line x" title="73:149	To predict the perspective of an unseen document using nave Bayes, we calculate the posterior distribution of D in (5) by integrating out the parameters, integraldisplay integraldisplay P( D,pi,|{(Dn,Wn)}Nn=1, W)dpid (7) However, the above integral is difficult to compute." ></td>
	<td class="line x" title="74:149	As an alternative, we use Markov Chain Monte Carlo (MCMC) methods to obtain samples from the posterior distribution." ></td>
	<td class="line x" title="75:149	Details about MCMC methods can be found in Appendix A. 4.2 Latent Sentence Perspective Model We introduce a new binary random variable, S, to model how strongly a perspective is reflected at the sentence level." ></td>
	<td class="line x" title="76:149	The value of S is either s1 or s0, where s1 indicates a sentence is written strongly from a perspective while s0 indicates it is not." ></td>
	<td class="line x" title="77:149	The whole generative process is modeled as follows: pi  Beta(pi,pi)   Beta(,)   Dirichlet() Dn  Binomial(1,pi) Sm,n  Binomial(1,) Wm,n  Multinomial(Lm,n,) The parameters pi and  have the same semantics as in the nave Bayes model." ></td>
	<td class="line x" title="78:149	S is naturally modeled as a binomial variable, where  is the parameter of S. S represents how likely it is that a sentence strongly conveys a perspective." ></td>
	<td class="line x" title="79:149	We call this model the Latent Sentence Perspective Model (LSPM) because S is not directly observed." ></td>
	<td class="line x" title="80:149	The graphical model representation of LSPM is shown in Figure 2." ></td>
	<td class="line x" title="81:149	pi   Dn Sm,n Wm,n N Mn Figure 2: Latent Sentence Perspective Model To use LSPM to identify the perspective of a new document D with unknown sentence perspectives S, we calculate posterior probabilities by summing out possible combinations of sentence perspective in the document and parameters." ></td>
	<td class="line x" title="82:149	integraldisplay integraldisplay integraldisplay summationdisplay Sm,n summationdisplay S P( D,Sm,n, S,pi,,| (8) {(Dn,Wn)}Nn=1, W)dpidd As before, we resort to MCMC methods to sample from the posterior distributions, given in Equations (5) and (6)." ></td>
	<td class="line x" title="83:149	As is often encountered in mixture models, there is an identifiability issue in LSPM." ></td>
	<td class="line x" title="84:149	Because the values of S can be permuted without changing the likelihood function, the meanings of s0 and s1 are ambiguous." ></td>
	<td class="line x" title="85:149	In Figure 3a, four  values are used to represent the four possible combinations of document perspective d and sentence perspective intensity s. If we do not impose any constraints, s1 and s0 are exchangeable, and we can no longer strictly interpret s1 as indicating a strong sentence-level perspective and s0 as indicating that a sentence carries little or no perspective information." ></td>
	<td class="line x" title="86:149	The other problem of this parameterization is that any improvement from LSPM over the nave Bayes model is not necessarily 112 d0 d0,s0 s0 d0,s1 s1 d1 d1,s0 s0 d0,s0 s1 (a) s0 and s1 are not identifiable s1 d0,s1 d0 d1,s1 d1 s0 s0 (b) sharing d1,s0 and d0,s0 Figure 3: Two different parameterization of  due to the explicit modeling of sentence-level perspective." ></td>
	<td class="line x" title="87:149	S may capture aspects of the document collection that we never intended to model." ></td>
	<td class="line x" title="88:149	For example, s0 may capture the editors writing styles and s1 the guests writing styles in the bitterlemons corpus." ></td>
	<td class="line x" title="89:149	We solve the identifiability problem by forcing d1,s0 and d0,s0 to be identical and reducing the number of  parameters to three." ></td>
	<td class="line x" title="90:149	As shown in Figure 3b, there are separate  parameters conditioned on the document perspective (left branch of the tree, d0 is Israeli and d1 is Palestinian), but there is single  parameter when S = s0 shared by both documentlevel perspectives (right branch of the tree)." ></td>
	<td class="line x" title="91:149	We assume that the sentences with little or no perspective information, i.e., S = s0, are generated independently of the perspective of a document." ></td>
	<td class="line x" title="92:149	In other words, sentences that are presenting common background information or introducing an issue and that do not strongly convey any perspective should look similar whether they are in Palestinian or Israeli documents." ></td>
	<td class="line x" title="93:149	By forcing this constraint, we become more confident that s0 represents sentences of little perspectives and s1 represents sentences of strong perspectives from d1 and d0 documents." ></td>
	<td class="line x" title="94:149	5 Experiments 5.1 Identifying Perspective at the Document Level We evaluate three different models for the task of identifying perspective at the document level: two nave Bayes models (NB) with different inference methods and Support Vector Machines (SVM) (Cristianini and Shawe-Taylor, 2000)." ></td>
	<td class="line x" title="95:149	NB-B uses full Bayesian inference and NB-M uses Maximum a posteriori (MAP)." ></td>
	<td class="line x" title="96:149	We compare NB with SVM not only because SVM has been very effective for classifying topical documents (Joachims, 1998), but also to contrast generative models like NB with discriminative models like SVM." ></td>
	<td class="line x" title="97:149	For training SVM, we represent each document as a V -dimensional feature vector, where V is the vocabulary size and each coordinate is the normalized term frequency within the document." ></td>
	<td class="line x" title="98:149	We use a linear kernel for SVM and search for the best parameters using grid methods." ></td>
	<td class="line x" title="99:149	To evaluate the statistical models, we train them on the documents in the bitterlemons corpus and calculate how accurately each model predicts document perspective in ten-fold cross-validation experiments." ></td>
	<td class="line x" title="100:149	Table 2 reports the average classification accuracy across the the 10 folds for each model." ></td>
	<td class="line x" title="101:149	The accuracy of a baseline classifier, which randomly assigns the perspective of a document as Palestinian or Israeli, is 0.5, because there are equivalent numbers of documents from the two perspectives." ></td>
	<td class="line x" title="102:149	Model Data Set Accuracy Reduction Baseline 0.5 SVM Editors 0.9724 NB-M Editors 0.9895 61% NB-B Editors 0.9909 67% SVM Guests 0.8621 NB-M Guests 0.8789 12% NB-B Guests 0.8859 17% Table 2: Results for Identifying Perspectives at the Document Level The last column of Table 2 is error reduction relative to SVM." ></td>
	<td class="line x" title="103:149	The results show that the nave Bayes models and SVM perform surprisingly well on both the Editors and Guests subsets of the bitterlemons corpus." ></td>
	<td class="line x" title="104:149	The nave Bayes models perform slightly better than SVM, possibly because generative models (i.e. , nave Bayes models) achieve optimal performance with a smaller number of training examples than discriminative models (i.e. , SVM) (Ng and Jordan, 2002), and the size of the bitterlemonscorpus is indeed small." ></td>
	<td class="line x" title="105:149	NB-B, which performs full Bayesian inference, improves 113 on NB-M, which only performs point estimation." ></td>
	<td class="line x" title="106:149	The results suggest that the choice of words made by the authors, either consciously or subconsciously, reflects much of their political perspectives." ></td>
	<td class="line x" title="107:149	Statistical models can capture word usage well and can identify the perspective of documents with high accuracy." ></td>
	<td class="line x" title="108:149	Given the performance gap between Editors and Guests, one may argue that there exist distinct editing artifacts or writing styles of the editors and guests, and that the statistical models are capturing these things rather than perspectives. To test if the statistical models truly are learning perspectives, we conduct experiments in which the training and testing data are mismatched, i.e., from different subsets of the corpus." ></td>
	<td class="line x" title="109:149	If what the SVM and nave Bayes models learn are writing styles or editing artifacts, the classification performance under the mismatched conditions will be considerably degraded." ></td>
	<td class="line x" title="110:149	Model Training Testing Accuracy Baseline 0.5 SVM Guests Editors 0.8822 NB-M Guests Editors 0.9327 43% NB-B Guests Editors 0.9346 44% SVM Editors Guests 0.8148 NB-M Editors Guests 0.8485 18% NB-B Editors Guests 0.8585 24% Table 3: Identifying Document-Level Perspectives with Different Training and Testing Sets The results on the mismatched training and testing experiments are shown in Table 3." ></td>
	<td class="line x" title="111:149	Both SVM and the two variants of nave Bayes perform well on the different combinations of training and testing data." ></td>
	<td class="line x" title="112:149	As in Table 2, the nave Bayes models perform better than SVM with larger error reductions, and NB-B slightly outperforms NB-M." ></td>
	<td class="line x" title="113:149	The high accuracy on the mismatched experiments suggests that statistical models are not learning writing styles or editing artifacts." ></td>
	<td class="line x" title="114:149	This reaffirms that document perspective is reflected in the words that are chosen by the writers." ></td>
	<td class="line x" title="115:149	We list the most frequent words (excluding stopwords) learned by the the NB-M model in Table 4." ></td>
	<td class="line x" title="116:149	The frequent words overlap greatly between the Palestinian and Israeli perspectives, including state, peace, process, secure (security), and govern (government)." ></td>
	<td class="line x" title="117:149	This is in contrast to what we expect from topical text classification (e.g. , Sports vs. Politics), in which frequent words seldom overlap." ></td>
	<td class="line x" title="118:149	Authors from different perspectives often choose words from a similar vocabulary but emphasize them differently." ></td>
	<td class="line x" title="119:149	For example, in documents that are written from the Palestinian perspective, the word palestinian is mentioned more frequently than the word israel. It is, however, the reverse for documents that are written from the Israeli perspective." ></td>
	<td class="line x" title="120:149	Perspectives are also expressed in how frequently certain people (sharon v.s. arafat), countries (international v.s. america), and actions (occupation v.s. settle) are mentioned." ></td>
	<td class="line x" title="121:149	While one might solicit these contrasting word pairs from domain experts, our results show that statistical models such as SVM and nave Bayes can automatically acquire them." ></td>
	<td class="line x" title="122:149	5.2 Identifying Perspectives at the Sentence Level In addition to identifying the perspective of a document, we are interested in knowing which sentences of the document strongly conveys perspective information." ></td>
	<td class="line x" title="123:149	Sentence-level perspective annotations do not exist in the bitterlemons corpus, which makes estimating parameters for the proposed Latent Sentence Perspective Model (LSPM) difficult." ></td>
	<td class="line x" title="124:149	The posterior probability that a sentence strongly covey a perspective (Example (6)) is of the most interest, but we can not directly evaluate this model without gold standard annotations." ></td>
	<td class="line x" title="125:149	As an alternative, we evaluate how accurately LSPM predicts the perspective of a document, again using 10-fold cross validation." ></td>
	<td class="line x" title="126:149	Although LSPM predicts the perspective of both documents and sentences, we will doubt the quality of the sentence-level predictions if the document-level predictions are incorrect." ></td>
	<td class="line x" title="127:149	The experimental results are shown in Table 5." ></td>
	<td class="line x" title="128:149	We include the results for the nave Bayes models from Table 3 for easy comparison." ></td>
	<td class="line x" title="129:149	The accuracy of LSPM is comparable or even slightly better than that of the nave Bayes models." ></td>
	<td class="line x" title="130:149	This is very encouraging and suggests that the proposed LSPM closely captures how perspectives are reflected at both the document and sentence levels." ></td>
	<td class="line x" title="131:149	Examples 1 and 2 from the introduction were predicted by LSPM as likely to 114 Palestinian palestinian, israel, state, politics, peace, international, people, settle, occupation, sharon, right, govern, two, secure, end, conflict, process, side, negotiate Israeli israel, palestinian, state, settle, sharon, peace, arafat, arab, politics, two, process, secure, conflict, lead, america, agree, right, gaza, govern Table 4: The top twenty most frequent stems learned by the NB-M model, sorted by P(w|d) Model Training Testing Accuracy Baseline 0.5 NB-M Guests Editors 0.9327 NB-B Guests Editors 0.9346 LSPM Guests Editors 0.9493 NB-M Editors Guests 0.8485 NB-B Editors Guests 0.8585 LSPM Editors Guests 0.8699 Table 5: Results for Perspective Identification at the Document and Sentence Levels contain strong perspectives, i.e., large Pr(S = s1)." ></td>
	<td class="line x" title="132:149	Examples 3 and 4 from the introduction were predicted by LSPM as likely to contain little or no perspective information, i.e., high Pr(S = s0)." ></td>
	<td class="line x" title="133:149	The comparable performance between the nave Bayes models and LSPM is in fact surprising." ></td>
	<td class="line x" title="134:149	We can train a nave Bayes model directly on the sentences and attempt to classify a sentence as reflecting either a Palestinian or Israeli perspective." ></td>
	<td class="line x" title="135:149	A sentence is correctly classified if the predicted perspective for the sentence is the same as the perspective of the document from which it was extracted." ></td>
	<td class="line x" title="136:149	Using this model, we obtain a classification accuracy of only 0.7529, which is much lower than the accuracy previously achieved at the document level." ></td>
	<td class="line x" title="137:149	Identifying perspectives at the sentence level is thus more difficult than identifying perspectives at the document level." ></td>
	<td class="line x" title="138:149	The high accuracy at the document level shows that LSPM is very effective in pooling evidence from sentences that individually contain little perspective information." ></td>
	<td class="line x" title="139:149	6 Conclusions In this paper we study a new problem of learning to identify the perspective from which a text is written at the document and sentence levels." ></td>
	<td class="line x" title="140:149	We show that much of a documents perspective is expressed in word usage, and statistical learning algorithms such as SVM and nave Bayes models can successfully uncover the word patterns that reflect author perspective with high accuracy." ></td>
	<td class="line x" title="141:149	In addition, we develop a novel statistical model to estimate how strongly a sentence conveys perspective, in the absence of sentence-level annotations." ></td>
	<td class="line x" title="142:149	By introducing latent variables and sharing parameters, the Latent Sentence Perspective Model is shown to capture well how perspectives are reflected at the document and sentence levels." ></td>
	<td class="line x" title="143:149	The small but positive improvement due to sentence-level modeling in LSPM is encouraging." ></td>
	<td class="line x" title="144:149	In the future, we plan to investigate how consistently LSPM sentence-level predictions are with human annotations." ></td>
	<td class="line x" title="145:149	Acknowledgment This material is based on work supported by the Advanced Research and Development Activity (ARDA) under contract number NBCHC040037." ></td>
	<td class="line x" title="146:149	A Gibbs Samplers Based the model specification described in Section 4.2 we derive the Gibbs samplers (Chen et al. , 2000) for the Latent Sentence Perspective Model as follows, pi(t+1)  Beta(pi + Nsummationdisplay n=1 dn + d(t+1), pi +N  Nsummationdisplay n=1 dn + 1 d(t+1)) (t+1)  Beta( + Nsummationdisplay n=1 Mnsummationdisplay m=1 sm,n + Msummationdisplay m=1 sm,  + Nsummationdisplay n=1 Mn  Nsummationdisplay n=1 Mnsummationdisplay m=1 sm,n + M  Msummationdisplay m=1 sm) 115 (t+1)  Dirichlet( + Nsummationdisplay n=1 Mnsummationdisplay m=1 wm,n) Pr(S(t+1)n,m = s1)  P(Wm,n|Sm,n = 1,(t)) Pr(S(t+1)m,n = 1|,Dn) Pr( D(t+1) = d1)  Mproductdisplay m=1 dbinom((t+1)d ) Mproductdisplay m=1 dmultinom(d,m(t))dbinom(pi(t)) where dbinom and dmultinom are the density functions of binomial and multinomial distributions, respectively." ></td>
	<td class="line x" title="147:149	The superscript t indicates that a sample is from the t-th iteration." ></td>
	<td class="line x" title="148:149	We run three chains and collect 5000 samples." ></td>
	<td class="line x" title="149:149	The first half of burn-in samples are discarded." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D07-1035
Low-Quality Product Review Detection in Opinion Summarization
Liu, Jingjing;Cao, Yunbo;Lin, Chin Yew;Huang, Yalou;Zhou, Ming;"></td>
	<td class="line x" title="1:276	Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp." ></td>
	<td class="line x" title="2:276	334342, Prague, June 2007." ></td>
	<td class="line x" title="3:276	c2007 Association for Computational Linguistics Low-Quality Product Review Detection in Opinion Summarization Jingjing Liu Nankai University Tianjin, China v-jingil@microsoft.com Yunbo Cao Microsoft Research Asia Beijing, China yucao@microsoft.com Chin-Yew Lin Microsoft Research Asia Beijing, China cyl@microsoft.com Yalou Huang Nankai University Tianjin, China huangyl@nankai.edu.cn Ming Zhou Microsoft Research Asia Beijing, China mingzhou@microsoft.com Abstract Product reviews posted at online shopping sites vary greatly in quality." ></td>
	<td class="line x" title="4:276	This paper addresses the problem of detecting lowquality product reviews." ></td>
	<td class="line x" title="5:276	Three types of biases in the existing evaluation standard of product reviews are discovered." ></td>
	<td class="line x" title="6:276	To assess the quality of product reviews, a set of specifications for judging the quality of reviews is first defined." ></td>
	<td class="line x" title="7:276	A classificationbased approach is proposed to detect the low-quality reviews." ></td>
	<td class="line x" title="8:276	We apply the proposed approach to enhance opinion summarization in a two-stage framework." ></td>
	<td class="line x" title="9:276	Experimental results show that the proposed approach effectively (1) discriminates lowquality reviews from high-quality ones and (2) enhances the task of opinion summarization by detecting and filtering lowquality reviews." ></td>
	<td class="line oc" title="10:276	1 Introduction In the past few years, there has been an increasing interest in mining opinions from product reviews (Pang, et al, 2002; Liu, et al, 2004; Popescu and Etzioni, 2005)." ></td>
	<td class="line n" title="11:276	However, due to the lack of editorial and quality control, reviews on products vary greatly in quality." ></td>
	<td class="line x" title="12:276	Thus, it is crucial to have a mechanism capable of assessing the quality of reviews and detecting low-quality/noisy reviews." ></td>
	<td class="line x" title="13:276	Some shopping sites already provide a function of assessing the quality of reviews." ></td>
	<td class="line x" title="14:276	For example, Amazon1 allows users to vote for the helpfulness of each review and then ranks the reviews based on the accumulated votes." ></td>
	<td class="line x" title="15:276	However, according to our survey in Section 3, users votes at Amazon have three kinds of biases as follows: (1) imbalance vote bias, (2) winner circle bias, and (3) early bird bias." ></td>
	<td class="line x" title="16:276	Existing studies (Kim et al, 2006; Zhang and Varadarajan, 2006) used these users votes for training ranking models to assess the quality of reviews, which therefore are subject to these biases." ></td>
	<td class="line x" title="17:276	In this paper, we demonstrate the aforementioned biases and define a standard specification to measure the quality of product reviews." ></td>
	<td class="line x" title="18:276	We then manually annotate a set of ground-truth with real world product review data conforming to the specification." ></td>
	<td class="line x" title="19:276	To automatically detect low-quality product reviews, we propose a classification-based approach learned from the annotated ground-truth." ></td>
	<td class="line x" title="20:276	The proposed approach explores three aspects of product reviews, namely informativeness, readability, and subjectiveness." ></td>
	<td class="line x" title="21:276	We apply the proposed approach to opinion summarization, a typical opinion mining task." ></td>
	<td class="line x" title="22:276	The proposed approach enhances the existing work in a two-stage framework, where the low-quality review detection is applied right before the summarization stage." ></td>
	<td class="line x" title="23:276	Experimental results show that the proposed approach can discriminate low-quality reviews from high-quality ones effectively." ></td>
	<td class="line x" title="24:276	In addition, the task of opinion summarization can be enhanced by detecting and filtering low-quality reviews." ></td>
	<td class="line x" title="25:276	1 http://www.amazon.com 334 The rest of the paper is organized as follows: Section 2 introduces the related work." ></td>
	<td class="line x" title="26:276	In Section 3, we define the quality of product reviews." ></td>
	<td class="line x" title="27:276	In Section 4, we present our approach to detecting lowquality reviews." ></td>
	<td class="line x" title="28:276	In Section 5, we empirically verify the effectiveness of the proposed approach and its use for opinion summarization." ></td>
	<td class="line x" title="29:276	Section 6 summarizes our work in this paper and points out the future work." ></td>
	<td class="line x" title="30:276	2 Related Work 2.1 Evaluating Helpfulness of Reviews The problem of evaluating helpfulness of reviews (Kim et al, 2006), also known as learning utility of reviews (Zhang and Varadarajan, 2006), is quite similar to our problem of assessing the quality of reviews." ></td>
	<td class="line x" title="31:276	In practice, researchers in this area considered the problem as a ranking problem and solved it with regression models." ></td>
	<td class="line x" title="32:276	In the process of model training and testing, they used the ground-truth derived from users votes of helpfulness provided by Amazon." ></td>
	<td class="line x" title="33:276	As we will show later in Section 3, these models all suffered from three types of voting bias." ></td>
	<td class="line x" title="34:276	In our work, we avoid using users votes by developing a specification on the quality of reviews and building a ground-truth according to the specification." ></td>
	<td class="line x" title="35:276	2.2 Mining Opinions from Reviews One area of research on opinion mining from product reviews is to judge whether a review expresses a positive or a negative opinion." ></td>
	<td class="line x" title="36:276	For example, Turney (2006) presented a simple unsupervised learning algorithm in judging reviews as thumbs up (recommended) or thumbs down (not recommended)." ></td>
	<td class="line oc" title="37:276	Pang et al (2002) considered the same problem and presented a set of supervised machine learning approaches to it." ></td>
	<td class="line x" title="38:276	For other work see also Dave et al.(2003), Pang and Lee (2004, 2005)." ></td>
	<td class="line x" title="40:276	Another area of research on opinion mining is to extract and summarize users opinions from product reviews (Hu and Liu, 2004; Liu et al. , 2005; Popescu and Etzioni, 2005)." ></td>
	<td class="line x" title="41:276	Typically, a sentence or a text segment in the reviews is treated as the basic unit." ></td>
	<td class="line x" title="42:276	The polarity of users sentiments on a product feature in each unit is extracted." ></td>
	<td class="line x" title="43:276	Then the aggregation of the polarities of individual sentiments is presented to users so that they can have an at-a-glance view on how other experienced users rated on a certain product." ></td>
	<td class="line x" title="44:276	The major weakness in the existing studies is that all the reviews, including low-quality ones, are taken into consideration and treated equally for generating the summary." ></td>
	<td class="line x" title="45:276	In this paper, we enhance the application by detecting and filtering low-quality reviews." ></td>
	<td class="line x" title="46:276	In order to achieve that, we first define what the quality of reviews is. 3 Quality of Product Reviews In this section, we will first show three biases of users votes observed on Amazon, and then present our specification on the quality of product reviews." ></td>
	<td class="line x" title="47:276	3.1 Amazon Ground-truth In our study, we use the product reviews on digital cameras crawled from Amazon as our data set." ></td>
	<td class="line x" title="48:276	The data set consists of 23,141 reviews on 946 digital cameras." ></td>
	<td class="line x" title="49:276	At the Amazon site, users could vote for a review with a helpful or unhelpful label." ></td>
	<td class="line x" title="50:276	Thus, for each review there are two numbers indicating the statistics of these two labels, namely the number of helpful votes and that of unhelpful ones." ></td>
	<td class="line x" title="51:276	Kim et al (2006) used the percentage of helpful votes as the measure of evaluating the quality of reviews in their experiments." ></td>
	<td class="line x" title="52:276	We call the ground-truth based on this measure as Amazon ground-truth." ></td>
	<td class="line x" title="53:276	Certainly, the ground-truth has the advantage of convenience." ></td>
	<td class="line x" title="54:276	However, we identify three types of biases that make the Amazon ground-truth not always suitable for determining the quality of reviews." ></td>
	<td class="line x" title="55:276	We describe these biases in details in the rest of this section." ></td>
	<td class="line x" title="56:276	3.1.1 Imbalance Vote Bias Figure 1." ></td>
	<td class="line x" title="57:276	Reviews percentage scores At the Amazon site, users tend to value others opinions positively rather than negatively." ></td>
	<td class="line x" title="58:276	From Figure 1, we can see that a half of the 23,141 0 2000 4000 6000 8000 10000 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 # R ev iews Percentage of 'helpful' votes 335 reviews (corresponding to the two bars on the right of the figure) have more than 90% helpful votes, including 9,100 reviews with 100% helpful votes." ></td>
	<td class="line x" title="59:276	From an in-depth investigation on these highly-voted reviews, we observed that some did not really have as good quality as the votes hint." ></td>
	<td class="line x" title="60:276	For example, in Figure 2, the review about Canon PowerShot S500 receives 40 helpful votes out of 40 votes although it only gives very brief description on the product features in its second paragraph." ></td>
	<td class="line x" title="61:276	We call this type of bias imbalance vote bias." ></td>
	<td class="line x" title="62:276	This is my second Canon digital elph camera." ></td>
	<td class="line x" title="63:276	Both were great cameras." ></td>
	<td class="line x" title="64:276	Recently upgraded to the S500." ></td>
	<td class="line x" title="65:276	About 6 months later I get the dreaded E18 error." ></td>
	<td class="line x" title="66:276	I searched the Internet and found numerous people having problems." ></td>
	<td class="line x" title="67:276	When I determined the problem to be the lens not fully extending I decided to give it a tug." ></td>
	<td class="line x" title="68:276	It clicked and the camera came on, ready to take pictures." ></td>
	<td class="line x" title="69:276	Turning it off and on produced the E18 again." ></td>
	<td class="line x" title="70:276	While turning it on I gave it a nice little bump on the side (where the USB connector is) and the lens popped out on its own." ></td>
	<td class="line x" title="71:276	No problems since." ></td>
	<td class="line x" title="72:276	Its a nice compact and light camera and takes great photos and videos." ></td>
	<td class="line x" title="73:276	Only complaint (other than E18) is the limit of 30-second videos on 640x480 mode." ></td>
	<td class="line x" title="74:276	I've got a 512MB compact flash card, I should be able to take as much footage as I have memory in one take." ></td>
	<td class="line x" title="75:276	Figure 2." ></td>
	<td class="line x" title="76:276	An example review 3.1.2 Winner Circle Bias Figure 3." ></td>
	<td class="line x" title="77:276	Votes of the top-50 ranked reviews There also exists a bootstrapping effect of hot reviews at the Amazon site." ></td>
	<td class="line x" title="78:276	Figure 3 shows the helpful votes for the top 50 ranked reviews." ></td>
	<td class="line x" title="79:276	The numbers are averaged over 127 digital cameras which have no less than 50 reviews." ></td>
	<td class="line x" title="80:276	As shown in this figure, the top two reviews hold more than 250 and 140 votes respectively on average; while the numbers of votes held by lower-ranked reviews decrease exponentially." ></td>
	<td class="line x" title="81:276	This is so-called the winner circle bias: the more votes a review gains, the more default authority it would appear to the readers, which in turn will influence the objectivity of the readers votes." ></td>
	<td class="line x" title="82:276	Also, the higher ranked reviews would attract more eyeballs and therefore gain more peoples votes." ></td>
	<td class="line x" title="83:276	This mutual influence among labelers should be avoided when the votes are used as the evaluation standard." ></td>
	<td class="line x" title="84:276	3.1.3 Early Bird Bias Figure 4." ></td>
	<td class="line x" title="85:276	Dependency on publication date Publication date can influence the accumulation of users votes." ></td>
	<td class="line x" title="86:276	In Figure 4, the nth publication date represents the nth month after the product is released." ></td>
	<td class="line x" title="87:276	The number in the figure is averaged over all the digital cameras in the data set." ></td>
	<td class="line x" title="88:276	We can observe a clear trend that the earlier a review is posted, the more votes it will get." ></td>
	<td class="line x" title="89:276	This is simply because reviews posted earlier are exposed to users for a longer time." ></td>
	<td class="line x" title="90:276	Therefore, some high quality reviews may get fewer users vote because of later publication." ></td>
	<td class="line x" title="91:276	We call this early bird bias." ></td>
	<td class="line x" title="92:276	3.2 Specification of Quality Besides these aforementioned biases, using the raw rating from readers directly also fails to provide a clear guideline for what a good review consists of." ></td>
	<td class="line x" title="93:276	In this section, we provide such a guideline, which we name as the specification (SPEC)." ></td>
	<td class="line x" title="94:276	In the SPEC, we define four categories of review quality which represent different values of the reviews to users purchase decision: best review, good review, fair review, and bad review." ></td>
	<td class="line x" title="95:276	A generic description of the SPEC is as follows: A best review must be a rather complete and detailed comment on a product." ></td>
	<td class="line x" title="96:276	It presents several aspects of a product and provides convincing opinions with enough evidence." ></td>
	<td class="line x" title="97:276	Usually a best review could be taken as the main reference that users only need to read before making their purchase decision on a certain product." ></td>
	<td class="line x" title="98:276	The first review in Figure 5 is a best review." ></td>
	<td class="line x" title="99:276	It presents several product features and provides convincing opinions with sufficient evidence." ></td>
	<td class="line x" title="100:276	It is also in a good format for readers to easily understand." ></td>
	<td class="line x" title="101:276	Note that we omit some words in the example to save the space." ></td>
	<td class="line x" title="102:276	0 50 100 150 200 250 300 1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 #V ote s he ld by re vie ws Ranking positions of reviews 0 10 20 30 40 50 60 1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49# Vot es he ld by re vie ws Publication Date 336 A good review is a relatively complete comment on a product, but not with as much supporting evidence as necessary." ></td>
	<td class="line x" title="103:276	It could be used as a strong and influential reference, but not as the only recommendation." ></td>
	<td class="line x" title="104:276	The second review in Figure 5 is such an example." ></td>
	<td class="line x" title="105:276	A fair review contains a very brief description on a product." ></td>
	<td class="line x" title="106:276	It does not supply detailed evaluation on the product, but only comments on some aspects of the product." ></td>
	<td class="line x" title="107:276	For example, the third review in Figure 5 mainly talks about the delay between pictures, but less about other aspects of the camera." ></td>
	<td class="line x" title="108:276	A bad review is usually an incorrect description of a product with misleading information." ></td>
	<td class="line x" title="109:276	It talks little about a specific product but much about some general topics (e.g. photography)." ></td>
	<td class="line x" title="110:276	For example, the last review in Figure 5 talks about the topic of generic battery, but does not specify any digital camera." ></td>
	<td class="line x" title="111:276	A bad review is an unhelpful review that can be ignored." ></td>
	<td class="line x" title="112:276	Best Review: I purchased this camera about six months ago after my Kodak Easyshare camera completely died on me. I did a little research and read only good things about this Canon camera so I decided to go with it because it was very reasonably priced (about $200)." ></td>
	<td class="line x" title="113:276	Not only did the camera live up to my expectations, it surpassed them by leaps and bounds!" ></td>
	<td class="line x" title="114:276	Here are the things I have loved about this camera: BATTERY this camera has the best battery of any digital camera I have ever owned or used." ></td>
	<td class="line x" title="115:276	 EASY TO USE I was able to  PICTURE QUALITY all of the pictures I've taken and printed out have been great." ></td>
	<td class="line x" title="116:276	 FEATURES I love the ability to quickly and easily  LCD SCREEN I was hoping  SD MEMORY CARD I was also looking for a camera that used SD memory cards." ></td>
	<td class="line x" title="117:276	Mostly because I cannot stress how highly I recommend this camera." ></td>
	<td class="line x" title="118:276	I will never buy another digital camera besides Canon again." ></td>
	<td class="line x" title="119:276	And the A610 (as well as the A620 the 7.0MP version) is the best digital camera I've ever used." ></td>
	<td class="line x" title="120:276	Good Review: The Sony DSC 'P10' Digital Camera is the top pick for CSC." ></td>
	<td class="line x" title="121:276	Running against cameras like Olympus stylus, Canon Powereshot, Sony V1, Nikon, Fuji, and More." ></td>
	<td class="line x" title="122:276	The new release of 5.0 mega pixels has shot prices for digital cameras up to $1000+." ></td>
	<td class="line x" title="123:276	This camera I purchased through a Private Dealer cost me $400.86." ></td>
	<td class="line x" title="124:276	The Retail Price is Running $499.00 to $599.00." ></td>
	<td class="line x" title="125:276	Purchase this camera from a wholesale dealer for the best price $377.00." ></td>
	<td class="line x" title="126:276	Great Photo Even in dim light w/o a flash." ></td>
	<td class="line x" title="127:276	The p10 is very compact." ></td>
	<td class="line x" title="128:276	Can easily fit into any pocket." ></td>
	<td class="line x" title="129:276	The camera can record 90 minutes of mpeg like a home movie." ></td>
	<td class="line x" title="130:276	There are a lot of great digital cameras on the market that shoot good pictures and video." ></td>
	<td class="line x" title="131:276	What makes the p10 the top pick is it comes with a rechargeable lithium battery." ></td>
	<td class="line x" title="132:276	Many use AA batteries, the digital camera consumes theses AA batteries in about two hours time while the unit is on." ></td>
	<td class="line x" title="133:276	That can add continuous expense to the camera." ></td>
	<td class="line x" title="134:276	It's also the best resolution on the market." ></td>
	<td class="line x" title="135:276	6.0 megapix is out, though only a few." ></td>
	<td class="line x" title="136:276	And the smallest that we found." ></td>
	<td class="line x" title="137:276	Also the best price for a major brand." ></td>
	<td class="line x" title="138:276	Fair Review: There is nothing wrong with the 2100 except for the very noticeable delay between pics." ></td>
	<td class="line x" title="139:276	The camera's digital processor takes about 5 seconds after a photo is snapped to ready itself for the next one." ></td>
	<td class="line x" title="140:276	Otherwise, the optics, the 3X optical zoom and the 2 megapixel resolution are fine for anything from Internet apps to 8' x 10' print enlarging." ></td>
	<td class="line x" title="141:276	It is competent, not spectacular, but it gets the job done at an agreeable price point." ></td>
	<td class="line x" title="142:276	Bad Review: I want to point out that you should never buy a generic battery, like the person from San Diego who reviewed the S410 on May 15, 2004, was recommending." ></td>
	<td class="line x" title="143:276	Yes you'd save money, but there have been many reports of generic batteries exploding when charged for too long." ></td>
	<td class="line x" title="144:276	And don't think if your generic battery explodes you can sue somebody and win millions." ></td>
	<td class="line x" title="145:276	These batteries are made in sweatshops in China, India and Korea, and I doubt you can find anybody to sue." ></td>
	<td class="line x" title="146:276	So play it safe, both for your own sake and the camera's sake." ></td>
	<td class="line x" title="147:276	If you want a spare, get a real Canon one." ></td>
	<td class="line x" title="148:276	Figure 5." ></td>
	<td class="line x" title="149:276	Example reviews 3.3 Annotation of Quality According to the SPEC defined above, we built a ground-truth from the Amazon data set." ></td>
	<td class="line x" title="150:276	We randomly selected 100 digital cameras and 50 reviews for each camera." ></td>
	<td class="line x" title="151:276	Totally we have 4,909 reviews since some digital cameras have fewer than 50 unique reviews." ></td>
	<td class="line x" title="152:276	Then we hired two annotators to label the reviews with the SPEC as their guideline." ></td>
	<td class="line x" title="153:276	As the result, we have two independent copies of annotations on 4,909 reviews, with the labels of best, good, fair, and bad." ></td>
	<td class="line x" title="154:276	Table 1 shows the confusion matrix between the two copies of annotation." ></td>
	<td class="line x" title="155:276	The value of the kappa statistic (Cohen, 1960) calculated from the matrix is 0.8142." ></td>
	<td class="line x" title="156:276	This shows that the two annotators achieved highly consistent results by following the SPEC, although they worked independently." ></td>
	<td class="line x" title="157:276	Annotation 1 Annotation 2 best good fair bad total best 294 44 2 0 340 good 66 639 113 0 818 fair 0 200 1,472 113 1,785 bad 1 2 78 1,885 1,966 total 361 885 1,665 1,998 4,909 Table 1." ></td>
	<td class="line x" title="158:276	Confusion matrix bet." ></td>
	<td class="line x" title="159:276	the annotations In order to examine the difference between our annotations and Amazon ground-truth, we evaluate the Amazon ground-truth against the annotations, 337 with the measure of error rate of preference pairs (Herbrich et al, 1999)." ></td>
	<td class="line x" title="160:276	         = |                        ||                  | (1) where the preference pair is defined as a pair of reviews with a order." ></td>
	<td class="line x" title="161:276	For example, a best review and a good review correspond to a preference pair with the order of best review preferring to good review." ></td>
	<td class="line x" title="162:276	The all preference pairs are collected from one of the annotations (the annotation 1 or the annotation 2) by ignoring the pairs from the same category." ></td>
	<td class="line x" title="163:276	The incorrect preference pairs are the preference pairs collected from the Amazon ground-truth but not with the same order as that in the all preference pairs." ></td>
	<td class="line x" title="164:276	The order of the preference pair collected from the Amazon ground-truth is evaluated on the basis of the percentage score as described in Section 3.1." ></td>
	<td class="line x" title="165:276	The error rate of preference pairs based on the annotation 1 and that based on the annotation 2 are 0.448 and 0.446, respectively, averaged over 100 digital cameras." ></td>
	<td class="line x" title="166:276	The high error rate of preference pairs demonstrates that the Amazon ground-truth diverges from the annotations (our ground-truth) significantly." ></td>
	<td class="line x" title="167:276	To discover which kind of ground-truth is more reasonable, we ask an additional annotator (the third annotator) to compare these two kinds of ground-truth." ></td>
	<td class="line x" title="168:276	More specifically, we randomly selected 100 preference pairs whose orders the two kinds of ground-truth dont agree on (called incorrect preference pairs in the evaluation above)." ></td>
	<td class="line x" title="169:276	As for our ground-truth, we choose the Annotation 1 in the new test." ></td>
	<td class="line x" title="170:276	Then, the third annotator is asked to assign a preference order for each selected pair." ></td>
	<td class="line x" title="171:276	Note that the third annotator is blind to both our specification and the existing preference order." ></td>
	<td class="line x" title="172:276	Last, we evaluate the two kinds of ground-truth with the new annotation." ></td>
	<td class="line x" title="173:276	Among 100 pairs, our ground-truth agrees to the new annotation on 85 pairs while the Amazon ground-truth agrees to the new annotation on 15 pairs." ></td>
	<td class="line x" title="174:276	To confirm the result, yet another annotator (the fourth annotator) is called to repeat the same annotation independently as the third one." ></td>
	<td class="line x" title="175:276	And we obtain the same statistical result (85 vs. 15) although the fourth annotator does not agree with the third annotator on some pairs." ></td>
	<td class="line x" title="176:276	In practice, we treat the reviews in the first three categories (best, good and fair) as highquality reviews and those in the bad category as low-quality reviews, since our goal is to identify low quality reviews that should not be considered when creating product review summaries." ></td>
	<td class="line x" title="177:276	4 Classification of Product Reviews We employ a statistical machine learning approach to address the problem of detecting low-quality products reviews." ></td>
	<td class="line x" title="178:276	Given a training data set  =  ,  1, we construct a model that can minimize the error in prediction of y given x (generalization error)." ></td>
	<td class="line x" title="179:276	Here     and   = {          ,           } represents a product review and a label, respectively." ></td>
	<td class="line x" title="180:276	When applied to a new instance x, the model predicts the corresponding y and outputs the score of the prediction." ></td>
	<td class="line x" title="181:276	4.1 The Learning Model In our study, we focus on differentiating lowquality product reviews from high-quality ones." ></td>
	<td class="line x" title="182:276	Thus, we treat the task as a binary classification problem." ></td>
	<td class="line x" title="183:276	We employ SVM (Support Vector Machines) (Vapnik, 1995) as the model of classification." ></td>
	<td class="line x" title="184:276	Given an instance x (product review), SVM assigns a score to it based on   =    +  (2) where w denotes a vector of weights and b denotes an intercept." ></td>
	<td class="line x" title="185:276	The higher the value of f(x) is, the higher the quality of the instance x is. In classification, the sign of f(x) is used." ></td>
	<td class="line x" title="186:276	If it is positive, then x is classified into the positive category (high-quality reviews), otherwise into the negative category (low-quality reviews)." ></td>
	<td class="line x" title="187:276	The construction of SVM needs labeled training data (in our case, the categories are high-quality reviews and low-quality reviews)." ></td>
	<td class="line x" title="188:276	Briefly, the learning algorithm creates the hyper plane in (2), such that the hyper plane separates the positive and negative instances in the training data with the largest margin." ></td>
	<td class="line x" title="189:276	4.2 Product Feature Resolution Product features (e.g. , image quality for digital camera) in a review are good indicators of review quality." ></td>
	<td class="line x" title="190:276	However, different product features may refer to the same meaning (e.g. , battery life and power), which will bring redundancy in the study." ></td>
	<td class="line x" title="191:276	In this paper, we formulize the problem as the resolution of product features." ></td>
	<td class="line x" title="192:276	Thus, the 338 problem is reduced to how to determine the equivalence of a product feature in different forms." ></td>
	<td class="line x" title="193:276	In (Hu and Liu, 2004), the matching of different product features is mentioned briefly and addressed by fuzzy matching." ></td>
	<td class="line x" title="194:276	However, there exist many cases where the method fails to match the multiple mentions, e.g., battery life and power, because it only considers string similarity." ></td>
	<td class="line x" title="195:276	In this paper we propose to resolve the problem by leveraging two kinds of evidence: one is surface string evidence, the other is contextual evidence." ></td>
	<td class="line x" title="196:276	We use edit distance (Ukkonen, 1985) to compare the similarity between the surface strings of two mentions, and use contextual similarity to reflect the semantic similarity between two mentions." ></td>
	<td class="line x" title="197:276	When using contextual similarity, we split all the reviews into sentences." ></td>
	<td class="line x" title="198:276	For each mention of a product feature, we take it as a query and search for all the relevant sentences." ></td>
	<td class="line x" title="199:276	Then we construct a vector for the mention, by taking each unique term in the relevant sentences as a dimension of the vector." ></td>
	<td class="line x" title="200:276	The cosine similarity between two vectors of mentions is then present to measure the contextual similarity between two mentions." ></td>
	<td class="line x" title="201:276	4.3 Feature Development for Learning To detect low-quality reviews, our proposed approach explores three aspects of product reviews, namely informativeness, subjectiveness, and readability." ></td>
	<td class="line x" title="202:276	We denote the features employed for learning as learning features, discriminative from the product features we discussed above." ></td>
	<td class="line x" title="203:276	4.3.1 Features on Informativeness As for informativeness, the resolution of product features is employed when we generate the learning features as listed below." ></td>
	<td class="line x" title="204:276	Pairs mapping to the same product feature will be treated as the same product feature, when we calculate the frequency and the number of product features." ></td>
	<td class="line x" title="205:276	We apply the approach proposed in (Hu and Liu, 2004) to extract product features." ></td>
	<td class="line x" title="206:276	We also use a list of product names and a list of brand names to generate the learning features." ></td>
	<td class="line x" title="207:276	Both lists can be collected from the Amazon site because they are relatively stable within a time interval." ></td>
	<td class="line x" title="208:276	The learning features on the informativeness of a review are as follows." ></td>
	<td class="line x" title="209:276	 Sentence level (SL)  The number of sentences in the review  The average length of sentences  The number of sentences with product features  Word level (WL)  The number of words in the review  The number of products (e.g. , DMC-FZ50, EX-Z1000) in the review  The number of products in the title of a review  The number of brand names (e.g. , Canon, Sony) in the review  The number of brand names in the title of a review  Product feature level (PFL)  The number of product features in the review  The total frequency of product features in the review  The average frequency of product features in the review  The number of product features in the title of a review  The total frequency of product features in the title of a review 4.3.2 Features on Readability We make use of several features at paragraph level which indicate the underlying structure of the reviews." ></td>
	<td class="line x" title="210:276	These features include,  The number of paragraphs in the review  The average length of paragraphs in the review  The number of paragraph separators in the review Here, we refer to the keywords, such as Pros vs. Cons as paragraph separators." ></td>
	<td class="line x" title="211:276	The keywords usually appear at the beginning of paragraphs for categorizing two contrasting aspects of a product." ></td>
	<td class="line x" title="212:276	We extract the nouns and noun phrases at the beginning of each paragraph from the 4,909 reviews and use the most frequent 30 pairs of keywords as paragraph separators." ></td>
	<td class="line x" title="213:276	Table 2 provides some examples of the extracted separators." ></td>
	<td class="line x" title="214:276	Separators Separators Positive Negative Positive Negative Pros Cons The Good The Bad Strength Weakness Thumb up Bummer PLUSES MINUSES Positive Negative Advantages Drawbacks Likes Dislikes The upsides Downsides GOOD THINGS BAD THINGS Table 2." ></td>
	<td class="line x" title="215:276	Examples of paragraph separators 339 4.3.3 Features on Subjectiveness We also take the subjectiveness of reviews into consideration." ></td>
	<td class="line x" title="216:276	Unlike previous work (Kim et al, 2006; Zhang and Varadarajan, 2006) using shallow syntactic information directly, we use a sentiment analysis tool (Hu and Liu, 2004) which aggregates a set of shallow syntactic information." ></td>
	<td class="line x" title="217:276	The tool is a classifier capable of determining the sentiment polarity of each sentence." ></td>
	<td class="line x" title="218:276	We create three learning features regarding the subjectiveness of reviews." ></td>
	<td class="line x" title="219:276	 The percentage of positive sentences in the review  The percentage of negative sentences in the review  The percentage of subjective sentences (regardless of positive or negative) in the review 5 Experiments In this section, we describe our experiments with the proposed classification-based approach to lowquality review detection, and its effectiveness on the task of opinion summarization." ></td>
	<td class="line x" title="220:276	5.1 Detecting Low-quality Reviews In our proposed approach, the problem of assessing quality of reviews is formalized as a binary classification problem." ></td>
	<td class="line x" title="221:276	We conduct experiments by taking reviews in the categories of best, good, and fair as high-quality reviews and those in the bad category as low-quality reviews." ></td>
	<td class="line x" title="222:276	As for classification model, we utilize the SVMLight toolkit (Joachims, 2004)." ></td>
	<td class="line x" title="223:276	We randomly divide the 100 queries of digital cameras into two sets, namely a training set of 50 queries and a test set of 50 queries." ></td>
	<td class="line x" title="224:276	For the two copies of annotations, we use the same division." ></td>
	<td class="line x" title="225:276	We use the training set from annotation 1 to train the model and apply the model to the test sets from both annotation 1 and annotation 2, respectively." ></td>
	<td class="line x" title="226:276	Table 3 reports the accuracies of our approach to review classification." ></td>
	<td class="line x" title="227:276	The accuracy is defined as the percentage of correctly classified reviews." ></td>
	<td class="line x" title="228:276	We take the approach that utilizes only the category of features on sentence level (SL) as the baseline, and incrementally add other categories of features on informativeness, readability and subjectiveness." ></td>
	<td class="line x" title="229:276	We can see that both the features on word level (WL) and those on product feature level (PFL) can improve the performance of classification much." ></td>
	<td class="line x" title="230:276	The features on readability can still increase the accuracy although the contribution is much less." ></td>
	<td class="line x" title="231:276	The features on subjectiveness, however, make no contribution." ></td>
	<td class="line x" title="232:276	Feature Category Annotation1 Annotation2 Informativeness SL 73.59% 72.81% WL 80.41% 79.15% PFL 83.30% 82.37% Readability 83.93% 82.91% Subjectiveness 83.84% 82.96% Table 3." ></td>
	<td class="line x" title="233:276	Low-quality reviews detection We also conduct a more detailed analysis on each individual feature." ></td>
	<td class="line x" title="234:276	Two categories of features on title and brand name have poor performance, which is due to the lack of information in the title and the low coverage of brand names in a review, respectively." ></td>
	<td class="line x" title="235:276	5.2 Summarizing Sentiments of Reviews One potential application of low-quality review detection is the opinion summarization of reviews." ></td>
	<td class="line x" title="236:276	The process of opinion summarization of reviews with regards to a query of a product consists of the following steps (Liu et al, 2005): 1." ></td>
	<td class="line x" title="237:276	From each of the reviews, identify every text segment with opinion in the review, and determine the polarities of the opinion segments." ></td>
	<td class="line x" title="238:276	2." ></td>
	<td class="line x" title="239:276	For each product feature, generate a positive opinion set and a negative opinion set of opinion segments, denoted as POS( ) and NOS( )." ></td>
	<td class="line x" title="240:276	3." ></td>
	<td class="line x" title="241:276	For each product feature, aggregate the numbers of segments in POS( ) andNOS( ), as opinion summarization on the product feature." ></td>
	<td class="line x" title="242:276	In this process, all the reviews contribute the same." ></td>
	<td class="line x" title="243:276	However, different reviews do hold different authorities." ></td>
	<td class="line x" title="244:276	A positive/negative opinion from a high-quality review should not have the same weight as that from a low-quality review." ></td>
	<td class="line x" title="245:276	We use a two-stage approach to enhance the reliability of summarization." ></td>
	<td class="line x" title="246:276	That is, we add a process of low-quality review detection before the summarization process, so that the summarization result is obtained based on the high-quality reviews only." ></td>
	<td class="line x" title="247:276	We are to demonstrate how much difference the proposed two-stage approach can bring into the opinion summarization." ></td>
	<td class="line x" title="248:276	We use the best classification model trained as described in Section 5.1 to filter low-quality reviews, and do summarization on the high-quality 340 reviews associated to the 50 test queries." ></td>
	<td class="line x" title="249:276	We denote the proposed approach and the old approach as two-stage and one-stage, respectively." ></td>
	<td class="line x" title="250:276	Due to the limited space, we only give a visual comparison of the two approaches on image quality in Figure 6." ></td>
	<td class="line x" title="251:276	The upper figure shows the summarization of positive opinions and the lower figure shows that of negative opinions." ></td>
	<td class="line x" title="252:276	From the figures we can see that the two-stage approach preserves fewer text segments as the result of filtering out many low-quality product reviews." ></td>
	<td class="line x" title="253:276	Figure 6." ></td>
	<td class="line x" title="254:276	Summarization on image quality To show the comparison on more features in a compressed space, we give the statistic ratio of change between two approaches instead." ></td>
	<td class="line x" title="255:276	As for the evaluation measure, we define RatioOfChange (ROC) on a feature f as, ROC  = Rateone  stage   Ratetwo  stage ( )Rate one  stage ( ) (3) where Rate *(f) is defined as, Rate ( ) = |POS( )||POS( )| + |NOS( )| (4) Table 4 shows some statistic results on ROC on five product features, namely image quality(IQ), battery, LCD screen (LCD), flash and movie mode (MM)." ></td>
	<td class="line x" title="256:276	The values in the cells are the percentage of queries whose ROC is larger/smaller than the respective thresholds." ></td>
	<td class="line x" title="257:276	We can see that a large portion of queries have big changes on the values of ROC." ></td>
	<td class="line x" title="258:276	This means that the result achieved by the two-stage approach is substantially different from that achieved by the one-stage approach." ></td>
	<td class="line x" title="259:276	%Query RatioOfChange (+) >0.30 >0.25 >0.20 >0.15 >0.10 >0.05 IQ 2% 4% 4% 10% 14% 22% Battery 10% 14% 18% 30% 38% 50% LCD 12% 18% 20% 22% 24% 28% Flash 6% 10% 16% 20% 26% 42% MM 6% 8% 8% 12% 18% 26% %Query RatioOfChange (-) <-0.30 <-0.25 <-0.20 <-0.15 <-0.10 <-0.05 IQ 4% 6% 10% 14% 18% 44% Battery 2% 4% 4% 10% 14% 22% LCD 4% 4% 8% 12% 22% 28% Flash 4% 6% 8% 16% 18% 28% MM 8% 10% 16% 18% 34% 42% Table 4." ></td>
	<td class="line x" title="260:276	RatioOfChange on five features There is no standard way to evaluate the quality of opinion summarization as it is rather a subjective problem." ></td>
	<td class="line x" title="261:276	In order to demonstrate the impact of the two-stage approach, we turn to external authoritative sources other than Amazon.com as the objective evaluation reference." ></td>
	<td class="line x" title="262:276	We observe that CNET2 provides a professional editors review for many products, which gives a rating in the range of 1~10 on product features." ></td>
	<td class="line x" title="263:276	9 digital cameras out of the 50 test queries are found to have the editors rating on image quality at CNET." ></td>
	<td class="line x" title="264:276	We use this rating to compare with the results of our opinion summarization." ></td>
	<td class="line x" title="265:276	We rescale the Rate scores obtained by both the one-stage approach and the two-stage approach into the range of 1-10 in order to perform the comparison." ></td>
	<td class="line x" title="266:276	Figure 7 provides the visual comparison." ></td>
	<td class="line x" title="267:276	We can see that the result achieved by the two-stage approach has a much better (closer) resemblance to CNET rating than one-stage approach does." ></td>
	<td class="line x" title="268:276	This indicates that our two-stage approach can achieve a more consistent summarization result to the professional evaluations by the editors." ></td>
	<td class="line x" title="269:276	Although the CNET rating is not the absolute standard for product evaluation, it provides a professional yet objective evaluation of the products." ></td>
	<td class="line x" title="270:276	Therefore, the experimental results demonstrate that our proposed approach could achieve more reliable opinion summarization which is closer to the generic evaluation from authoritative sources." ></td>
	<td class="line x" title="271:276	2 http://www.cnet.com 0 30 60 90 120 1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 Num be r of suppor ting se nte nce s (P os itiv e) QueryID One-stage Two-stage 0 20 40 60 80 1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 Num be r of suppor ting se nte nce s (Ne gat ive ) QueryID One-stage Two-stage 341 Figure 7." ></td>
	<td class="line x" title="272:276	Comparison with CNET rating 6 Conclusion In this paper, we studied the problem of detecting low-quality product reviews." ></td>
	<td class="line x" title="273:276	Our contribution can be summarized in two-fold: (1) we discovered three types of biases in the ground-truth used extensively in the existing work, and proposed a specification on the quality of product reviews." ></td>
	<td class="line x" title="274:276	The three biases that we discovered are imbalance vote bias, winner circle bias, and early bird bias." ></td>
	<td class="line x" title="275:276	(2) Rooting on the new ground-truth (conforming to the proposed specification), we proposed a classification-based approach to low-quality product review detection, which yields better performance of opinion summarization." ></td>
	<td class="line x" title="276:276	We hope to explore our future work in several areas, such as further consolidating the new ground-truth from different points of view and verifying the effectiveness of low-quality review detection with other applications." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D07-1114
Extracting Aspect-Evaluation and Aspect-Of Relations in Opinion Mining
Kobayashi, Nozomi;Inui, Kentaro;Matsumoto, Yuji;"></td>
	<td class="line x" title="1:239	Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp." ></td>
	<td class="line x" title="2:239	10651074, Prague, June 2007." ></td>
	<td class="line x" title="3:239	c2007 Association for Computational Linguistics Extracting Aspect-Evaluation and Aspect-of Relations in Opinion Mining Nozomi Kobayashi  Kentaro Inui, and Yuji Matsumoto Graduate School of Information Science, Nara Institute of Science and Technology 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan {nozomi-k,inui,matsu}@is.naist.jp Abstract The technology of opinion extraction allows users to retrieve and analyze peoples opinions scattered over Web documents." ></td>
	<td class="line x" title="4:239	We define an opinion unit as a quadruple consisting of the opinion holder, the subject being evaluated, the part or the attribute in which the subject is evaluated, and the value of the evaluation that expresses a positive or negative assessment." ></td>
	<td class="line x" title="5:239	We use this definition as the basis for our opinion extraction task." ></td>
	<td class="line x" title="6:239	We focus on two important subtasks of opinion extraction: (a) extracting aspect-evaluation relations, and (b) extracting aspect-of relations, and we approach each task using methods which combine contextual and statistical clues." ></td>
	<td class="line x" title="7:239	Our experiments on Japanese weblog posts show that the use of contextual clues improve the performance for both tasks." ></td>
	<td class="line x" title="8:239	1 Introduction The explosive increase in Web communication has attracted increasing interest in technologies for automatically mining personal opinions from Web documents such as product reviews and weblogs." ></td>
	<td class="line x" title="9:239	Such technologies would benefit users who seek reviews on certain consumer products of interest." ></td>
	<td class="line x" title="10:239	Previous approaches to the task of mining a largescale document collection of customer opinions (or  Currently, NTT Cyber Space Laboratories, 1-1, Hikarinooka, Yokosuka, Kanagawa, 239-0847 Japan reviews) can be classified into two approaches: Document classification and information extraction." ></td>
	<td class="line x" title="11:239	The former is the task of classifying documents or passages according to their semantic orientation such as positive vs. negative." ></td>
	<td class="line oc" title="12:239	This direction has been forming the mainstream of research on opinion-sensitive text processing (Pang et al. , 2002; Turney, 2002, etc.)." ></td>
	<td class="line x" title="13:239	The latter, on the other hand, focuses on the task of extracting opinions consisting of information about, for example, who feels how about which aspect of what product from unstructured text data." ></td>
	<td class="line x" title="14:239	In this paper, we refer to this information extractionoriented task as opinion extraction." ></td>
	<td class="line x" title="15:239	In contrast to sentiment classification, opinion extraction aims at producing richer information and requires an indepth analysis of opinions, which has only recently been attempted by a growing but still relatively small research community (Yi et al. , 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005, etc.)." ></td>
	<td class="line x" title="16:239	Most previous work on customer opinion extraction assumes the source of information to be customer reviews collected from customer review sites (Popescu and Etzioni, 2005; Hu and Liu, 2004; Liu et al. , 2005)." ></td>
	<td class="line x" title="17:239	In contrast, in this paper, we consider the task of extracting customer opinions from unstructured weblog posts." ></td>
	<td class="line x" title="18:239	Compared with extraction from review articles, extraction from weblogs is more challenging because weblog posts tend to exhibit greater diversity in topics, goals, vocabulary, style, etc. and are much more likely to include descriptions irrelevant to the subject in question." ></td>
	<td class="line x" title="19:239	In this paper, we first describe our task setting of opinion extraction." ></td>
	<td class="line x" title="20:239	We conducted a corpus study and investigated the feasibility of the task def1065 inition by showing the statistics and inter-annotator agreement of our corpus annotation." ></td>
	<td class="line x" title="21:239	Next, we show that the crucial body of the above opinion extraction task can be decomposed into two kinds of relation extraction, i.e. aspect-evaluation relation extraction and aspect-of relation extraction." ></td>
	<td class="line x" title="22:239	For example, the passage I went out for lunch at the Deli and ordered a curry with chicken." ></td>
	<td class="line x" title="23:239	It was pretty good has an aspect-evaluation relation curry with chicken, was good and an aspect-of relation The Deli, curry with the chicken." ></td>
	<td class="line x" title="24:239	The former task can be regarded as a special type of predicate-argument structure analysis or semantic role labeling." ></td>
	<td class="line x" title="25:239	The latter, on the other hand, can be regarded as bridging reference resolution (Clark, 1977), which is the task of identifying relations between definite noun phrases and discourse-new entities implicitly related to some previously mentioned entities." ></td>
	<td class="line x" title="26:239	Most of the previous work on customer opinion extraction, however, does not adopt the state-of-theart techniques in those fields, relying only on simple proximity-based or pattern-based methods." ></td>
	<td class="line x" title="27:239	In this context, this paper empirically shows that incorporating machine learning-based techniques devised for predicate-argument structure analysis and bridging reference resolution improve the performance of both aspect-evaluation and aspect-of relation extraction." ></td>
	<td class="line x" title="28:239	Furthermore, we also show that combining contextual clues with a common co-occurrence statistics-based technique for bridging reference resolution makes a significant improvement on aspectof relation extraction." ></td>
	<td class="line x" title="29:239	2 Opinion extraction: Task design Our present goal is to build a computational model to extract opinions from Web documents in such a form as: Who feels how on which aspects of which subjects." ></td>
	<td class="line x" title="30:239	Given the passage presented in Figure 1, for example, the opinion we want to extract is: the writer feels that the colors of pictures taken with Powershot (product) are beautiful. As suggested by this example, we consider it reasonable to start with an assumption that most evaluative opinions can be structured as a frame composed of the following constituents: Opinion holder The person who is making an evaluation." ></td>
	<td class="line x" title="31:239	An opinion holder is typically the first a0a2a1a4a3a5a0 a6a8a7a10a9a12a11a14a13a16a15a4a17a18a9a4a19a21a20a12a13a23a22a25a24a26a17a4a27a29a28a31a30a32a11a33a20a34a17a21a13a29a22a16a35a36a28a37a27a39a38a4a22a37a40a12a11 a22a12a19a18a17a18a41a31a6a42a13a2a17a31a17a21a43a44a11a12a17a42a45a16a28a47a46a21a48a49a12a13a36a9a42a30a36a28a12a11a44a9a42a11a18a48a50a37a19a51a13a10a20a12a28 a49a34a22a21a45a16a28a31a30a2a22a14a41a42a52a53a17a55a54a17a42a30a32a11a51a22a21a30a32a28a47a11a33a17a56a15a14a28a34a22a18a9a12a13a57a48a35a36a9a18a54a34a28a37a58a31a28a12a50 a27a29a20a18a28a31a50a44a35a59a54a22a14a11a18a20a60a48a11a44a9a31a11a14a28a31a38a18a41a14a61a16a54a11a4a17a60a28a34a22a37a11a33a40a60a13a2a17a62a19a34a30a63a48a46 a11a21a48a50a12a49a18a28a64a13a36a20a42a28a47a15a5a17a55a38a33a40a60a20a34a22a21a11a65a22a51a19a34a30a63a48a46a60a20a34a22a14a50a42a38a18a54a28a31a41 a17a42a46a18a48a50a55a48a17a21a50a47a20a4a17a66a54a38a33a28a42a30a68a67a4a27a29a30a59a48a13a36a28a42a30a63a69 a11a12a9a31a15a32a7a36a28a31a49a12a13a67a34a24a55a17a34a27a68a28a42a30a36a11a33a20a31a17a18a13a57a69 a22a18a11a12a46a18a28a12a49a12a13a67a31a69 a28a18a58a21a22a34a54a9a34a22a37a13a8a48a17a42a50a62a67a37a28a31a22a14a11a14a40a64a13a70a17a64a19a21a30a59a48a46a37a69 a71a33a72a37a73a74a14a73a71a14a74a29a75a14a74a14a73a76a78a77 a17a42a46a21a48a50a42a48a17a18a50a47a20a12a17a55a54a38a14a28a31a30a68a67a4a27a29a30a59a48a13a59a28a31a30a79a69 a11a12a9a42a15a80a7a59a28a31a49a12a13a67a34a24a26a17a4a27a29a28a12a30a36a11a33a20a31a17a21a13a59a69 a22a18a11a12a46a18a28a18a49a12a13a67a21a46a21a48a49a12a13a36a9a31a30a63a28a31a81a18a49a34a17a21a54a17a55a30a32a11a21a69 a28a18a58a34a22a21a54a9a34a22a14a13a8a48a17a55a50a25a67a21a15a14a28a34a22a37a9a12a13a8a48a35a32a9a55a54a69 a71a33a72a37a73a74a14a73a71a37a74a16a75a33a74a14a73a76a83a82 Figure 1: Extraction of opinion units person (the author)." ></td>
	<td class="line x" title="32:239	We say the opinion holder is unspecified if the opinion is mentioned as a rumor." ></td>
	<td class="line x" title="33:239	Subject A named entity (product or company) of a given particular class of interest (e.g. a car model name in the automobile domain)." ></td>
	<td class="line x" title="34:239	Aspect A part, member or related object, or an attribute (of a part) of the subject on which the evaluation is made (engine, size, etc.) Evaluation An evaluative or subjective phrase used to express an evaluation or the opinion holders mental/emotional attitude (good, poor, powerful, stylish, (I) like, (I) am satisfied, etc.) According to this typology, the example in Figure 1 has six constituents, the writer (opinion holder), Powershot (subject), pictures (aspect), colors (aspect), beautiful (evaluation), easy to grip (evaluation), and constitute two units of opinions as presented in the right half of the figure." ></td>
	<td class="line x" title="35:239	We call such a unit an opinion unit." ></td>
	<td class="line x" title="36:239	In this paper, we only consider explicitly mentioned evaluative opinions as our targets of extraction, excluding opinions indirectly expressed through, for example, style or language choice from our scope." ></td>
	<td class="line x" title="37:239	Under this assumption, opinion extraction can be defined as a task of filling a fixed number of slots as above for each of the evaluations expressed in a given text collection." ></td>
	<td class="line x" title="38:239	Two issues then immediately arise." ></td>
	<td class="line x" title="39:239	First, it is necessary to make sure that the definition of the opinion units is clear enough for human annotators to be able to carry out the task with sufficient accuracy." ></td>
	<td class="line x" title="40:239	Second, all the slots might not consist of simple expressions in that the filler of an aspect slot may have a hierarchical structure in itself." ></td>
	<td class="line x" title="41:239	For example, the leather cover of the seats (of a car) refers to a part of a part of a car." ></td>
	<td class="line x" title="42:239	In theory, such a hierarchical chain can be of any length, which 1066 may affect the feasibility of the task." ></td>
	<td class="line x" title="43:239	For tackling these issues, we built a corpus annotated with the above sort of information and investigated the feasibility of the task." ></td>
	<td class="line x" title="44:239	2.1 Corpus study We first collected 116 Japanese weblog posts in the restaurant domain by randomly sampling from a collection of posts classified under the gourmet category on a major blog site: http://blog.livedoor.com/." ></td>
	<td class="line x" title="45:239	We asked two annotators to annotate them independently of each other following the above specification." ></td>
	<td class="line x" title="46:239	The annotators first identified evaluative phrases, and then for each evaluative phrase judged whether it was concerning a particular subject (i.e. a restaurant) in the given domain." ></td>
	<td class="line x" title="47:239	If judged yes, the annotators filled the opinion holder and subject slots obligatorily." ></td>
	<td class="line x" title="48:239	The annotators filled the aspect slot only when its filler appeared in the document and identified the hierarchical relations between aspects if any (e.g. noodle and its volume)." ></td>
	<td class="line x" title="49:239	Note that, if a sentence has two or more evaluations, they have to make one opinion unit for each." ></td>
	<td class="line x" title="50:239	2.1.1 Inter-annotator agreement We investigated the degree of inter-annotator agreement." ></td>
	<td class="line x" title="51:239	In the task of identifying evaluations, one annotator A1 identified 450 evaluations while the other A2 identified 392, and 329 cases of them coincided." ></td>
	<td class="line x" title="52:239	The two annotators did not identify the same number of evaluations, so instead of using kappa statistics, we use the following metric for measuring agreement as Wiebe et al.(2005) do: agr(A1||A2) = # of tags agreed by A1 and A2# of tags annotated by A 1 agr(A1||A2) was 0.73 and agr(A2||A1) was 0.83." ></td>
	<td class="line x" title="54:239	The F1 measure of the agreement between the two was therefore 0.79, which indicate that humans can identify evaluation at a reasonable level." ></td>
	<td class="line x" title="55:239	Next, we investigated the inter-annotator agreement of the aspect-evaluation and subject-evaluation relations." ></td>
	<td class="line x" title="56:239	Annotator A1 identified 328 relations, and A2 identified 346 relations." ></td>
	<td class="line x" title="57:239	295 cases coincided, and agr(A2||A1) was 0.90 and agr(A1||A2) was 0.86 (F1 measure was 0.88)." ></td>
	<td class="line x" title="58:239	This shows that we obtained high consistency." ></td>
	<td class="line x" title="59:239	Finally, for the subject-aspect and aspect-aspect relations, annotator A1 identified 296 relations, while A2 identified 293, 233 cases of which got agreement." ></td>
	<td class="line x" title="60:239	agr(A2||A1) was 0.79 Table 1: Statistics of opinion-annotated corpus (Restaurant, Automobile, cellular phone and video game) Rest Auto Phone Game articles 1,356 564 481 361 sentences 21,666 14,005 11,638 6,448 # of opinion units 4,267 1,519 1,518 775 Asp-Eval 3,692 943 965 521 I Asp-Asp 1,426 280 296 221 Subj-Asp 2,632 877 850 451 II Subj-Eval 575 576 553 243 Subj-Asp-Eval 2,314 736 768 351 Subj-Asp-Asp-Eval 1,065 175 172 127 other 313 32 25 54 Non-writer op." ></td>
	<td class="line x" title="61:239	holder 95 17 22 2 and agr(A1||A2) was 0.80 (F1 measure was 0.79), which show that the human annotators can carry out the task at a reasonable accuracy." ></td>
	<td class="line x" title="62:239	Based on this corpus study, we believe that our definitions of two relations are clear enough for constructing annotated corpus." ></td>
	<td class="line x" title="63:239	2.1.2 Opinion-annotated corpus Based on these results, we collected a larger set of weblog posts in four domains: restaurant, automobile, cellular phone and video game." ></td>
	<td class="line x" title="64:239	We then asked annotator A1 to annotate them in the same annotation scheme as above." ></td>
	<td class="line x" title="65:239	The results are summarized in Table 1." ></td>
	<td class="line x" title="66:239	I in the table shows the number of the identified opinion units and relations, and II shows the number of hierarchical chains of aspects." ></td>
	<td class="line x" title="67:239	For example, Nokia 6800 has a nice color screen is counted as Subj-Asp-Eval since this example includes a subject Nokia 6800, an aspect color screen and an evaluation nice." ></td>
	<td class="line x" title="68:239	Other indicates the number of the case where the length of hierarchical chains of aspects is three or more." ></td>
	<td class="line x" title="69:239	One observation is that, for all the domains, 90 % of all the opinion units have a hierarchical chain of aspects whose length is two or less." ></td>
	<td class="line x" title="70:239	From this, we can conclude that hierarchical chains longer than two are rare, and the problem is not so complicated, though they can be of any length in theory." ></td>
	<td class="line x" title="71:239	The row of Non-writer op(inion) holder at the bottom of Table 1 shows the number of opinion units whose opinion holder is not the writer of the weblog." ></td>
	<td class="line x" title="72:239	This result indicates that when an evaluative expression is found, its opinion holder is highly likely to be the writer of the blogs." ></td>
	<td class="line x" title="73:239	Therefore, we put aside the task of filling the opinion holder slot in this paper." ></td>
	<td class="line x" title="74:239	1067 2.2 Related work on task settings of opinion extraction There are several researches on customer opinion extraction." ></td>
	<td class="line x" title="75:239	Hu and Liu (2004) considered the task of extracting Aspect, Sentence, Semantic-orientation triples in our terminology, where Sentence is the one that includes the Aspect, and Semantic-orientation is either positive or negative." ></td>
	<td class="line x" title="76:239	The notion of Evaluation in our term has also been introduced by previous work (Popescu and Etzioni, 2005; Tateishi et al. , 2004; Suzuki et al. , 2006; Kobayashi et al. , 2005, etc.)." ></td>
	<td class="line x" title="77:239	For example, our previous paper (Kobayashi et al. , 2005) addresses the task of extracting Subject,Aspect,Evaluation." ></td>
	<td class="line x" title="78:239	However, none of those papers reports on such an extensive corpus study as what we report in this paper." ></td>
	<td class="line x" title="79:239	In addition, in this paper, we consider not only aspect-evaluation relations but also hierarchical chains of subject-aspect and aspect-aspect relations, which has never been addressed in previous work." ></td>
	<td class="line x" title="80:239	Open-domain opinion extraction is another trend of research on opinion extraction, which aims to extract a wider range of opinions from such texts as newspaper articles (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Wiebe et al. , 2005; Choi et al. , 2006)." ></td>
	<td class="line x" title="81:239	To the best of our knowledge, one of the most extensive corpus studies in this field has been conducted in the MPQA project (Wiebe et al. , 2005); while their concerns include the types of opinions we consider, they annotate newspaper articles, which presumably exhibit considerably different characteristics from customer-generated texts." ></td>
	<td class="line x" title="82:239	Though we do not discuss the problem of determining semantic orientation, we assume availability of state-of-the-art methods that perform this task (Suzuki et al. , 2006; Takamura et al. , 2006, etc.)." ></td>
	<td class="line x" title="83:239	The problem of determining semantic orientation will be solved by using these techniques, so we focus on the main issue: Extracting opinion units from given texts." ></td>
	<td class="line x" title="84:239	3 Method for opinion extraction Before designing a model for our opinion extraction task, it is important to note that aspect phrases are open-class expressions and tend to be heavily domain-dependent." ></td>
	<td class="line x" title="85:239	In fact, according to our investigation on our opinion-annotated corpus, the number a0 a1a2a0a2a0 a3 a0a2a0a2a0 a3 a1a2a0a2a0 a4a2a0a2a0a2a0 a4a2a1a2a0a2a0 a5a2a0a2a0a2a0 a6 a7 a8 a9a11a10a12 a9a11a13a6a11a14a15a11a6a12 a16a17a19a18 a20a22a21a24a23 a25 a20a24a23 a3 a26a28a27a28a29a31a30a2a32a33 a4a2a26a28a27a28a29a31a30a2a32a33a2a34 a5a2a26a2a27a35a29a31a30a2a32a33a19a34 a36a2a26a2a27a35a29a31a30a2a32a33a19a34 a0 a3 a3 a6 a7 a8 a12 a12 a16 a25 a3 a33a3 a33 Figure 2: The distributions of evaluation and aspect expressions in the four domains of aspect types is nearly 3,200, and only 3% of them appear in two or more domains as shown in Figure 2." ></td>
	<td class="line x" title="86:239	For evaluation expressions, on the other hand, the number of types is much smaller than that of aspect expressions, and 27% of them appear in multiple domains." ></td>
	<td class="line x" title="87:239	This indicates that evaluation expressions are more likely to be used commonly across different domains compared with aspects." ></td>
	<td class="line x" title="88:239	To prove this assumption, we created a dictionary of evaluation expressions from customer reviews of automobiles (230,000 sentences in total) using the semi-automatic method proposed by Kobayashi et al.(2004)." ></td>
	<td class="line x" title="90:239	We expanded the dictionary by hand with external resources including publicly available ordinal thesauri." ></td>
	<td class="line x" title="91:239	As a result, we collected 5,550 entries." ></td>
	<td class="line x" title="92:239	According to our investigation of the coverage by the dictionary, 0.84 (restaurant), 0.88 (cellular phone), 0.91 (automobile), and 0.93 (video game) of the evaluations annotated in our corpus are covered by the dictionary." ></td>
	<td class="line x" title="93:239	From this observation, we consider that it is reasonable to start opinion extraction with the identification of evaluation expressions." ></td>
	<td class="line x" title="94:239	We therefore design the process of extracting Subject, Aspect, Evaluation as follows: 1." ></td>
	<td class="line x" title="95:239	Aspect-evaluation relation extraction: For each of the candidate evaluations that are selected from a given document by dictionary look-up, identify the target of the evaluation." ></td>
	<td class="line x" title="96:239	Here the identified target may be a subject (e.g. IXY (is well-designed)) or an aspect of a subject (e.g. the quality (is amazing))." ></td>
	<td class="line x" title="97:239	Hereafter, we use the term aspect to refer to both an aspect and a subject itself, since the subject can be regarded as the top element in the hierarchical chain of aspects." ></td>
	<td class="line x" title="98:239	2." ></td>
	<td class="line x" title="99:239	Opinion-hood determination: Judge whether or not the obtained pair aspect, evaluation is an expression of an opinion by considering the given context." ></td>
	<td class="line x" title="100:239	If it is judged yes, go to step3; otherwise, return to step 1 with a new candidate 1068 evaluation expression." ></td>
	<td class="line x" title="101:239	3." ></td>
	<td class="line x" title="102:239	Aspect-of relation extraction: If the identified aspect is not a subject, search for its antecedent, i.e. an expression that is a higher aspect or a subject of the current aspect." ></td>
	<td class="line x" title="103:239	Repeat step 3 until reaching a subject or no parent is found." ></td>
	<td class="line x" title="104:239	3.1 Related work on opinion extraction A common approach to the customer opinion extraction task mainly uses simple proximityor patternbased techniques." ></td>
	<td class="line x" title="105:239	For example, Tateishi et al.(2004) implement five syntactic patterns and Popescu et al.(2005) use ten syntactic patterns." ></td>
	<td class="line x" title="108:239	Such an approach is limited in two respects." ></td>
	<td class="line x" title="109:239	First, it assumes the availability of a list of potential aspect expressions as well as evaluation expressions; however creating such a list of aspects for a variety of domains can be prohibitively expensive because of the domain dependency of aspect expressions." ></td>
	<td class="line x" title="110:239	In contrast, our method does not require any aspect lexicon." ></td>
	<td class="line x" title="111:239	Second, their approach lacks the perspective of viewing aspect-evaluation extraction as a specific type of predicate-argument structure analysis, i.e. the task of identifying the arguments of a given predicate in a given text, and fails to benefit from the state-of-the-art techniques of this rapidly growing field." ></td>
	<td class="line x" title="112:239	The syntactic patterns used in their research are analyzed by a dependency parser, however, aspect-evaluation relations appear in diverse syntactic patterns, which cannot be easily captured by a handful of manually devised rules." ></td>
	<td class="line x" title="113:239	An exception is the model reported by Kanayama et al.(2004), which uses a component of an existing MT system to identify the aspect argument of a given evaluation predicate." ></td>
	<td class="line x" title="114:239	However, the MT component they use is not publicly available, and even if it were, it would be difficult to apply it to tasks in hand due of the opaqueness of its mechanism." ></td>
	<td class="line x" title="115:239	Our approach aims to develop a more generally applicable model of aspect-evaluation extraction." ></td>
	<td class="line x" title="116:239	In open-domain opinion extraction, some approaches use syntactic features obtained from parsed input sentences (Choi et al. , 2006; Kim and Hovy, 2006), as is commonly done in semantic role labeling." ></td>
	<td class="line x" title="117:239	Choi et al.(2006) address the task of extracting opinion entities and their relations, and incorporate syntactic features to their relation extraction model." ></td>
	<td class="line x" title="119:239	Kim and Hovy (2006) proposed a method for extracting opinion holders, topics and opinion words, in which they use semantic role labeling as an intermediate step to label opinion holders and topics." ></td>
	<td class="line x" title="120:239	However, these approaches do not address the task of extracting aspect-of relations and make use of syntactic features only for labeling opinion holders and topics." ></td>
	<td class="line x" title="121:239	In contrast, as we describe below, we find the significant overlap between aspectevaluation relation extraction and aspect-of relation extraction and apply the same approach to both tasks, gaining the generality of the model." ></td>
	<td class="line x" title="122:239	Aspect-of relations can be regarded as a sub-type of bridging reference (Clark, 1977), which is a common linguistic phenomenon where the referent of a definite noun phrase refers to a discourse-new entity implicitly related to some previously mentioned entity." ></td>
	<td class="line x" title="123:239	For example, we can see a relation of bridging reference between the door and the room in She entered the room." ></td>
	<td class="line x" title="124:239	The door closed automatically. A common approach is to use cooccurrence statistics between the referring expression (e.g. the door in the above example) and the related entity (the room) (Bunescu, 2003; Poesio et al. , 2004)." ></td>
	<td class="line x" title="125:239	Our approach newly incorporates automatically induced syntactic patterns as contextual clues into such a co-occurrence model, producing significant improvements of accuracy." ></td>
	<td class="line x" title="126:239	3.2 Our approach Now we describe our approach to aspect-evaluation and aspect-of relation extraction." ></td>
	<td class="line x" title="127:239	The key idea is to combine the following two kinds of information using a machine learning technique for both tasks." ></td>
	<td class="line x" title="128:239	Contextual clues: Syntactic patterns such as Aspect-ga VP-te, Evaluation Aspect-NOM VP-CONJ Evaluation which matches such a sentence as sekkyaku-ga kunrens-aretei-te kimochiyoi service-NOM be trained-CONJ feel comfortable (The waiters were well-trained, so I felt comfortable.)" ></td>
	<td class="line x" title="129:239	are considered to be useful for extracting relations between slot fillers when they appear in a single sentence (Here,  indicates a slot filler)." ></td>
	<td class="line x" title="130:239	We employ a supervised learning technique to search for such useful syntactic patterns." ></td>
	<td class="line x" title="131:239	Context-independent statistical clues: Statistics such as aspect-aspect and aspect-evaluation 1069 a0a2a1a3a0a5a4a6 a7a9a8 a10a12a11a2a13a15a14a17a16a18a5a19a21a20 a22a24a23a26a25a27a12a28a15a29a30 a8 a10a26a31a26a14a12a14a17a32a33a14a12a16a34a3a19a21a35 a36a2a37a38a26a39a40a37a37 a41 a14a2a42a43a10a2a43a44a46a45a12a47 a23 a8a17a48a49 a49 a50 a29 a49 a50 a10a12a44a46a51a17a52a11a2a43a51a26a16a53a54a19a21a34a56a55 a57a59a58a9a60a3a61a63a62a3a64a65a62a54a66a5a61a63a62a54a66a65a67a2a68a70a69a72a71a62a56a62 a73a17a74a17a75a48a76 a29 a76a26a77 a41 a11a2a78a59a42a43a51a2a79a40a16a44a2a80 a11a17a47a2a81a2a14a12a10a15a52 a51a2a44 a41 a14 a14a33a82a2a11a2a42a45a26a11a15a52a59a43a44a46a51a51a26a44a41 a14 a83a65a84a56a85a87a86a56a88a90a89 a11 a91 a92a26a93a12a91 a94a54a74a15a75a48a76 a29 a76a26a77a96a95a5a97a54a95a63a98 a29 a7a9a8 a22a24a23a2a25a27a12a28a15a29a30 a8 a23 a8a15a48a49 a49 a50 a29 a49 a50 a36a26a37a38a26a39a40a37a37 a99a5a100a102a101a9a95a46a103a63a104a77a12a105a94 a8 a75 a106a48a76 a30a46a107a108a21a22 a77a17a76a12a49a8a15a48a76 a22a90a23 a50a12a50 a108 a50a109a8a17a76a73 a8 a75 a50a109a110a5a111a15a112 a37 a113a33a37a36a40a114a26a38a115a116 a57a72a117a3a60a118a71a62a3a64a3a71a62a63a119a40a62a54a66a5a69a120a58a46a69a72a121a122a3a66a123a122a65a124a125a121a66a65a64a3a126a40a69a56a69a72a71a62a63a62 a127 a128a129 a127 a130 a130a130 a83a65a84a56a85a87a86a56a88 a52a120a14a83a65a84a63a85a131a86a63a88a51a26a44 Figure 3: Representation of input data co-occurrences are expected to be useful." ></td>
	<td class="line x" title="132:239	We obtain such statistical clues automatically from a large collection of raw documents." ></td>
	<td class="line x" title="133:239	In what follows, we describe our method for aspect-evaluation." ></td>
	<td class="line x" title="134:239	The aspect-of relation extraction is done in an an analogous way." ></td>
	<td class="line x" title="135:239	3.2.1 Supervised learning of contextual clues Let us consider the problem of searching for the aspect of a given evaluation expression t. This problem can be decomposed into binary classification problems of deciding whether each pair of candidate aspect c and target t is in an aspect-evaluation relation or not." ></td>
	<td class="line x" title="136:239	Our goal is to learn a discrimination function for this classification problem." ></td>
	<td class="line x" title="137:239	If such a function is obtained, we can identify the most likely candidate aspect simply by selecting the best scored c-t pair and, if its score is negative for all possible candidates, we conclude that t has no corresponding aspect in the candidate set." ></td>
	<td class="line x" title="138:239	For finding syntactic patterns that extract an aspect c starting with an evaluation t, we first represent all the sentences in the annotated corpus that has both an aspect and its evaluation, as shown in Figure 3." ></td>
	<td class="line x" title="139:239	A sentence is analyzed by a dependency parser, then the dependency tree is converted so as to represent the relation between content words clearly and to attach other information (such as POS labels and other morphological features of content words and the functional words attached to the content words) as shown in the lower part of Figure 3." ></td>
	<td class="line x" title="140:239	Among various classifier induction algorithms for tree-structured data, in our experiments, we have so far examined Kudo and Matsumoto (2004)s algorithm, packaged as a free software named BACT." ></td>
	<td class="line x" title="141:239	Given a set of training examples represented as ordered trees labeled either positive or negative class, this algorithm learns a list of weighted decision stumps as a discrimination function with the Boosting algorithm." ></td>
	<td class="line x" title="142:239	Each decision stump is associated with tuple s,l,w, where s is a subtree appearing in the training set, l a label, and w a weight of this pattern." ></td>
	<td class="line x" title="143:239	The strength of this algorithm is that it automatically acquires structured features and allows us to analyze the utility of features." ></td>
	<td class="line x" title="144:239	Given a c-t pair in an annotated sentence, tree encoding of this sentence is done as follows: First, we use a dependency parser to obtain a dependency tree as in Figure 3 (a)." ></td>
	<td class="line x" title="145:239	We assume keki (cake) as the candidate aspect c and oishii (delicious) as the target evaluation t. We then find the path between t and c together with their daughter nodes." ></td>
	<td class="line x" title="146:239	For example, the node Darling-no (Darlings) is kept since it is a daughter of c. Then, all the content words are abstracted to either of the class types, evaluation, aspect or node, that is, c is renamed as aspect, t as evaluation and all other content words as node." ></td>
	<td class="line x" title="147:239	Other information of a content word and the information of functional words attaching to the content word are represented as the leaf nodes as shown in Figure 3 (b)." ></td>
	<td class="line x" title="148:239	The features used in our experiments are summarized in Table 2." ></td>
	<td class="line x" title="149:239	We apply the same method to the aspect-of relation extraction by replacing the evaluation label as the second aspect label." ></td>
	<td class="line x" title="150:239	3.3 Context-independent statistical clues We also introduce the following two kinds of statistical clues." ></td>
	<td class="line x" title="151:239	i. Co-occurrences of aspect-evaluation/aspectaspect: Among various ways to estimate the strength of association (e.g. the number of hits returned from a search engine), in our experiments, we extracted aspect-aspect and aspect-evaluation co-occurrences in 1.7 million weblog posts using the patterns aspect ga/wa/mo evaluation (aspect is (subject-marker) evaluation) and aspect A no aspect B ga/wa (aspect B of aspect A is)." ></td>
	<td class="line x" title="152:239	To avoid the data sparseness problem, we use Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999) to estimate conditional probabilities P(Aspect|Evaluation) and P(Aspect A|Aspect B)." ></td>
	<td class="line x" title="153:239	We then incorporate the 1070 information of these probability scores into the learning model described in 3.2 by encoding them as a feature that indicates the relative score rank of each candidate in a given candidate set (see Table 2)." ></td>
	<td class="line x" title="154:239	ii." ></td>
	<td class="line x" title="155:239	Aspect-hood of candidate aspects: Aspect-hood is an index of the degree that measures how plausible a term is used as an aspect within a given domain." ></td>
	<td class="line x" title="156:239	We consider that a phrase directly co-occurred with a subject often is likely to be an aspect of the subject, and extract the expression X which appears in the form Subject no X (X of Subject) and the expression Y which appears in the form X no Y." ></td>
	<td class="line x" title="157:239	We calculate the aspect-hood of the expressions X and Y by the pointwise mutual information." ></td>
	<td class="line x" title="158:239	This score is also used as a features (see Table 2)." ></td>
	<td class="line x" title="159:239	3.4 Intra-/inter-sentential relation extraction Syntactic pattern induction as described in 3.2.1 can apply only when an aspect-evaluation (or aspect-of) relation appears in a single sentence." ></td>
	<td class="line x" title="160:239	We therefore build a separate model for inter-sentential relation extraction, which is carried out after intra-sentential relation extraction." ></td>
	<td class="line x" title="161:239	1) Intra-sentential relation identification: Given a target evaluation (or aspect), select the most likely candidate aspect c within the target sentence with the intra-sentential model described in 3.2.1." ></td>
	<td class="line x" title="162:239	If the score of c is positive, return c; otherwise, go to the inter-sentential relation extraction phase." ></td>
	<td class="line x" title="163:239	2) Inter-sentential relation identification: Search for the most likely candidate aspect in the sentences preceding the target evaluation (or aspect)." ></td>
	<td class="line x" title="164:239	This task can be regarded as a zero-anaphora resolution problem." ></td>
	<td class="line x" title="165:239	For this purpose, we employ the supervised learning model for zero-anaphora resolution proposed by (Iida et al. , 2003)." ></td>
	<td class="line x" title="166:239	3.5 Opinion-hood determination Evaluation phrases do not always extract correct opinion units in a given domain." ></td>
	<td class="line x" title="167:239	Consider an example from the digital camera domain, The weather was good." ></td>
	<td class="line x" title="168:239	so I went to the park to take some pictures." ></td>
	<td class="line x" title="169:239	good expresses the evaluation for the weather, but the weather is not an aspect of digital cameras." ></td>
	<td class="line x" title="170:239	Therefore, the weather, good is not an opinion in the digital camera domain." ></td>
	<td class="line x" title="171:239	We can consider a binary classification task of judging whether the obtained opinion unit is a real opinion or not in a given domain." ></td>
	<td class="line x" title="172:239	In this paper, we conduct a preliminary experiment which uses the opinion-hood determination model learned by Support Vector Machines." ></td>
	<td class="line x" title="173:239	We conduct the model using our opinionannotated corpus." ></td>
	<td class="line x" title="174:239	The positive examples are aspectevaluation pairs annotated in the corpus." ></td>
	<td class="line x" title="175:239	The negative examples are artificially generated as follows: We first identify the expression in the evaluation dictionary that appear in our annotated corpus." ></td>
	<td class="line x" title="176:239	We then apply the above aspect-evaluation extraction method and get the most plausible candidate aspect." ></td>
	<td class="line x" title="177:239	The result is regarded as a negative example if the extracted aspect is not the true aspect." ></td>
	<td class="line x" title="178:239	The features we used in our experiments are summarized in Table 2." ></td>
	<td class="line x" title="179:239	4 Experiments We conducted experiments with our Japanese opinion-annotated corpus to empirically evaluate the performance of our approach." ></td>
	<td class="line x" title="180:239	In these experiments, we separately evaluated the models of aspectevaluation relation extraction, aspect-of relation extraction, and opinion-hood determination." ></td>
	<td class="line x" title="181:239	4.1 Common settings We chose 395 weblog posts in the restaurant domain from our opinion-annotated corpus described in 2.1, and conducted 5-fold cross validation on that dataset." ></td>
	<td class="line x" title="182:239	As preprocessing, we analyzed this corpus using the Japanese morphological analyzer ChaSen1 and the Japanese dependency structure analyzer CaboCha2." ></td>
	<td class="line x" title="183:239	4.2 Models The results are summarized in Tables 3 and 4." ></td>
	<td class="line x" title="184:239	We evaluated the results by recall R and precision P defined as follows R = correctly extracted relationstotal number of relations, P = correctly extracted relationstotal number of relations found by the system." ></td>
	<td class="line x" title="185:239	Note that, in aspect-of relations, we permit A,C to be correct when the data includes the chain of aspect-of relations A,B and B,C." ></td>
	<td class="line x" title="186:239	Therefore, we merged the intraand inter-sentential results as shown in Table 4." ></td>
	<td class="line x" title="187:239	1http://chasen.naist.jp/ 2http://chasen.org/taku/software/cabocha/ 1071 Table 2: Feature list: t denotes a given target (evaluation or aspect) and c a candidate Features for contextual clues  Position of c / t in the sentence (beginning, end, other)  Base phrase distance between c and t (1, 2, 3, 4, other)  Whether c and t has a immediate dependency relation  Whether c precedes t  Whether c appears in a quoted sentence  Part-of-speech of c / t  Suffix of c (-sei, -sa (-ty), etc.)  Character type of c (English, Chinese, Katakana, etc.)  Semantic class of c derived from Nihongo Goi Taikei (Ikehara et al. , 1997)." ></td>
	<td class="line x" title="188:239	Features for statistical clues  Co-occurrence score rank of c (1st, 2nd, 3rd, 4th, other)  Aspect-hood score rank of c (1st, 2nd, 3rd, 4th, other) The Contextual and Contextual+statistics models are our proposed models where the former uses only contextual clues (3.2.1) and the latter uses both contextual and statistical clues." ></td>
	<td class="line x" title="189:239	We prepared two baseline models, one for each of the above tasks." ></td>
	<td class="line x" title="190:239	The Pattern model (in Table 3) simulates the patternbased method proposed by Tateishi at al." ></td>
	<td class="line x" title="191:239	(2004), which uses the following patterns: Aspect caseparticle Evaluation and Evaluation syntactically depends on Aspect." ></td>
	<td class="line x" title="192:239	The Co-occurrence model (in Table 4) simulates the co-occurrence statistics-based model used in bridging reference resolution (Bunescu, 2003): For an aspect expression, we select the nearest candidate that has the highest positive score of the pointwise mutual information regardless of its occurrence (i.e. interor intra-sentential)." ></td>
	<td class="line x" title="193:239	Comparing the Pattern (Cooccurrence) model with the Contextual model shows the effects of the supervised learning with contextual clues, while comparison of the Contextual and Contextual+statistics models shows the joint effect of combining contextual and statistical clues." ></td>
	<td class="line x" title="194:239	4.3 Results and discussions As for the aspect-evaluation relation extraction, concerning the intra-sentential cases, we can see that the models using the contextual clues show nearly 10% improvement in both precision and recall." ></td>
	<td class="line x" title="195:239	This indicates that the machine learning-based method has a great advantage over the pattern-based approach." ></td>
	<td class="line x" title="196:239	Similar results are seen in aspect-of relation extraction." ></td>
	<td class="line x" title="197:239	The models using the contextual clues achieved more than 10% improvement in preTable 3: The results of aspect-evaluation relation intra-sent." ></td>
	<td class="line x" title="198:239	inter-sent." ></td>
	<td class="line x" title="199:239	Patterns P 0.56 (432/774) R 0.53 (432/809) Contextual P 0.70 (504/723) 0.13 (46/360) R 0.62 (504/809) 0.17 (46/274) Contextual P 0.72 (502/694) 0.14 (53/389) +statistics R 0.62 (502/809) 0.19 (53/274) Table 4: The results of aspect-of relation precision recall Co-occurrence 0.27 (175/ 682) 0.17 (175/1048) Contextual 0.44 (458/1047) 0.44 (458/1048) Contextual+statistics 0.45 (474/1047) 0.45 (474/1048) cision and 20% improvement in recall over the cooccurrence statistics-based model." ></td>
	<td class="line x" title="200:239	We can say that contextual clues are also useful in aspect-of relation extraction." ></td>
	<td class="line x" title="201:239	In comparing the Contextual and Contextual+statistics models, on the other hand, we could get only a slight improvement, which indicates that we need to estimate the statistical clues more precisely." ></td>
	<td class="line x" title="202:239	We found that the unsophisticated estimation of the statistical clues was a major source of errors in aspect-of relation extraction, however, this estimation is not so easy since the correct expressions are appeared only once in large data." ></td>
	<td class="line x" title="203:239	We are seeking efficient ways to avoid data sparseness problem (e.g. categorize the aspects)." ></td>
	<td class="line x" title="204:239	In the aspect-evaluation relation extraction, we evaluated the results against the human annotated gold-standard in a strict manner." ></td>
	<td class="line x" title="205:239	However, according to our error analysis, some of the errors can be regarded as correct for some real applications." ></td>
	<td class="line x" title="206:239	In the following example, a relation annotated by the human is aji (taste), koi-me (strong)." ></td>
	<td class="line x" title="207:239	misoshiru-wa aji-ga koi-me miso soup-TOP taste-NOM strong (The taste of the miso soup is strong.)" ></td>
	<td class="line x" title="208:239	However, there is no harm to consider that misoshiru (miso soup), koi-me (strong) is also correct." ></td>
	<td class="line x" title="209:239	If we judge these cases as correct, the Proposed models achieve nearly 0.8 precision and 0.7 recall, and the baseline model also get 7 % improvement (precision 0.63 and recall 0.6)." ></td>
	<td class="line x" title="210:239	Based on this result, we consider that we achieved reasonable performance in intra-sentential aspect-evaluation relation extraction." ></td>
	<td class="line x" title="211:239	As Table 3 shows, inter-sentential relation extraction achieved very poorly." ></td>
	<td class="line x" title="212:239	In the case of inter1072 sentential relations, our model tends to rely heavily on the statistical clues, because syntactic pattern features cannot be used." ></td>
	<td class="line x" title="213:239	However, our current method for estimating co-occurrence distributions is not sophisticated as we discussed above." ></td>
	<td class="line x" title="214:239	We need to seek for more effective use of large scale domain dependent data to obtain better statistics." ></td>
	<td class="line x" title="215:239	We also conducted a preliminary test of the opinion-hood determination model using the features used in aspect-evaluation relation extraction." ></td>
	<td class="line x" title="216:239	As a result, we got 0.5 precision and 0.45 recall." ></td>
	<td class="line x" title="217:239	Opinion-hood determination problem includes two decisions: whether the evaluation candidate is an opinion or not, and whether the opinion is related to the given domain if the evaluation candidate is an opinion." ></td>
	<td class="line x" title="218:239	We plan to use various features known to be effective in the sentence subjectivity recognition task." ></td>
	<td class="line x" title="219:239	This task involves challenging problems." ></td>
	<td class="line x" title="220:239	For example, sentence (1) includes the writers evaluation on shrimps served at a particular restaurant." ></td>
	<td class="line x" title="221:239	In contrast, very similar sentence (2) does not express evaluation since it is a generic description of the writers taste." ></td>
	<td class="line x" title="222:239	(1) watashi-wa konomise-no ebi-ga suki-desu I the restaurant shrimp like (I like shrimps of the restaurant.)" ></td>
	<td class="line x" title="223:239	(2) watashi-wa ebi-ga suki-desu I shrimps like (I like shrimps.)" ></td>
	<td class="line x" title="224:239	Thus we need to conduct further investigation in order to resolve this kind of problems." ></td>
	<td class="line x" title="225:239	4.4 Portability of intra-sentential model We next evaluated effectiveness of the contextual clues learned in the domains to other domains by testing a model trained on the certain domains to other domain." ></td>
	<td class="line x" title="226:239	We selected two new domains, cellular phone and automobile, and annotated 290 weblog posts in each domain." ></td>
	<td class="line x" title="227:239	For the restaurant domain, we randomly selected 290 posts from the previously mentioned our annotated corpus." ></td>
	<td class="line x" title="228:239	We then divide each data set to a training set and a test set so that we had the same amount of training data for each domain." ></td>
	<td class="line x" title="229:239	Then we trained a model on the data for each domain, and applied it to each of the three set of data." ></td>
	<td class="line x" title="230:239	Table 5 shows the results of the experiment." ></td>
	<td class="line x" title="231:239	Compared with the model trained on the same domain, the models trained on different domains exhibited almost comparable performance." ></td>
	<td class="line x" title="232:239	This inTable 5: Comparing intra-sentential models among three domains (upper: aspect-eval, lower: aspect-of) test restaurant cellular phone automobile same P 0.72 (502/694) 0.75 (522/693) 0.76 (562/738) dom." ></td>
	<td class="line x" title="233:239	R 0.62 (502/809) 0.63 (522/833) 0.65 (562/870) other P 0.73 (468/638) 0.72 (517/710) 0.74 (565/768) dom R 0.58 (468/809) 0.62 (517/833) 0.65 (565/870) same P 0.43 (139/321) 0.62 (139/224) 0.66 (185/280) dom." ></td>
	<td class="line x" title="234:239	R 0.59 (139/234) 0.60 (139/230) 0.66 (185/279) other P 0.42 (124/293) 0.53 (138/260) 0.59 (195/329) dom R 0.52 (124/234) 0.60 (138/230) 0.70 (195/279) dicates that the contextual clues learned in other domains are effective in another domain, showing the cross-domain portability of our intra-sentential model." ></td>
	<td class="line x" title="235:239	5 Conclusion In this paper, we described our opinion extraction task, which extract opinion units consisting of four constituents." ></td>
	<td class="line x" title="236:239	We showed the feasibility of the task definition based on our corpus study." ></td>
	<td class="line x" title="237:239	We consider the task as two kinds of relation extraction tasks, aspect-evaluation relation extraction and aspect-of relation extraction, and proposed a machine learning-based method which combines contextual clues and statistical clues." ></td>
	<td class="line x" title="238:239	Our experimental results show that the model using contextual clues improved the performance for both tasks." ></td>
	<td class="line x" title="239:239	We also showed domain portability of the contextual clues." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N07-1033
Using ``Annotator Rationales'' to Improve Machine Learning for Text Categorization
Zaidan, Omar F.;Eisner, Jason M.;Piatko, Christine;"></td>
	<td class="line x" title="1:231	Proceedings of NAACL HLT 2007, pages 260267, Rochester, NY, April 2007." ></td>
	<td class="line x" title="2:231	c2007 Association for Computational Linguistics Using Annotator Rationales to Improve Machine Learning for Text Categorization Omar F. Zaidan and Jason Eisner Department of Computer Science Johns Hopkins University Baltimore, MD 21218, USA {ozaidan,jason}@cs.jhu.edu Christine D. Piatko JHU Applied Physics Laboratory 11100 Johns Hopkins Road Laurel, MD 20723 USA christine.piatko@jhuapl.edu Abstract We propose a new framework for supervised machine learning." ></td>
	<td class="line x" title="3:231	Our goal is to learn from smaller amounts of supervised training data, by collecting a richer kind of training data: annotations with rationales. When annotating an example, the human teacher will also highlight evidence supporting this annotationthereby teaching the machine learner why the example belongs to the category." ></td>
	<td class="line x" title="4:231	We provide some rationale-annotated data and present a learning method that exploits the rationales during trainingtoboostperformancesignificantlyonasample task, namely sentiment classification of movie reviews." ></td>
	<td class="line x" title="5:231	We hypothesize that in some situations, providing rationales is a more fruitful use of an annotators time than annotating more examples." ></td>
	<td class="line x" title="6:231	1 Introduction Annotation cost is a bottleneck for many natural language processing applications." ></td>
	<td class="line x" title="7:231	While supervised machine learning systems are effective, it is laborintensive and expensive to construct the many training examples needed." ></td>
	<td class="line x" title="8:231	Previous research has exploredactiveorsemi-supervisedlearningaspossible ways to lessen this burden." ></td>
	<td class="line x" title="9:231	Weproposeanewwayofbreakingthisannotation bottleneck." ></td>
	<td class="line x" title="10:231	Annotators currently indicate what the correct answers are on training data." ></td>
	<td class="line x" title="11:231	We propose that they should also indicate why, at least by coarse hints." ></td>
	<td class="line x" title="12:231	We suggest new machine learning approaches that can benefit from this why information." ></td>
	<td class="line x" title="13:231	For example, an annotator who is categorizing phrases or documents might also be asked to highlight a few substrings that significantly influenced her judgment." ></td>
	<td class="line x" title="14:231	We call such clues rationales. They need not correspond to machine learning features." ></td>
	<td class="line x" title="15:231	This work was supported by the JHU WSE/APL Partnership Fund; National Science Foundation grant No. 0347822 to the second author; and an APL Hafstad Fellowship to the third." ></td>
	<td class="line x" title="16:231	In some circumstances, rationales should not be too expensive or time-consuming to collect." ></td>
	<td class="line x" title="17:231	As long as the annotator is spending the time to study example xi and classify it, it may not require much extra effort for her to mark reasons for her classification." ></td>
	<td class="line x" title="18:231	2 Using Rationales to Aid Learning We will not rely exclusively on the rationales, but use them only as an added source of information." ></td>
	<td class="line x" title="19:231	The idea is to help direct the learning algorithms attentionhelping it tease apart signal from noise." ></td>
	<td class="line x" title="20:231	Machine learning algorithms face a well-known credit assignment problem." ></td>
	<td class="line x" title="21:231	Given a complex datum xi and the desired response yi, many features of xi could be responsible for the choice of yi." ></td>
	<td class="line x" title="22:231	The learning algorithm must tease out which features were actually responsible." ></td>
	<td class="line x" title="23:231	This requires a lot of training data, and often a lot of computation as well." ></td>
	<td class="line x" title="24:231	Our rationales offer a shortcut to solving this credit assignment problem, by providing the learning algorithm with hints as to which features of xi were relevant." ></td>
	<td class="line x" title="25:231	Rationales should help guide the learning algorithm toward the correct classification function, by pushing it toward a function that correctly pays attention to each examples relevant features." ></td>
	<td class="line x" title="26:231	This should help the algorithm learn from lessdataandavoidgettingtrappedinlocalmaxima.1 In this paper, we demonstrate the annotator rationales technique on a text categorization problem previously studied by others." ></td>
	<td class="line x" title="27:231	1To understand the local maximum issue, consider the hard problem of training a standard 3-layer feed-forward neural network." ></td>
	<td class="line x" title="28:231	If the activations of the hidden layers features (nodes) were observed at training time, then the network would decompose into a pair of independent 2-layer perceptrons." ></td>
	<td class="line x" title="29:231	This turns an NP-hard problem with local maxima (Blum and Rivest, 1992) to a polytime-solvable convex problem." ></td>
	<td class="line x" title="30:231	Although rationales might only provide indirect evidence of the hidden layer, this would still modify the objective function (see section 8) in a way that tended to make the correct weights easier to discover." ></td>
	<td class="line x" title="31:231	260 3 Discriminative Approach One popular approach for text categorization is to use a discriminative model such as a Support Vector Machine (SVM) (e.g.(Joachims, 1998; Dumais, 1998))." ></td>
	<td class="line x" title="33:231	We propose that SVM training can in general incorporate annotator rationales as follows." ></td>
	<td class="line x" title="34:231	From the rationale annotations on a positive example xi, we will construct one or more not-quiteas-positive contrast examples vij." ></td>
	<td class="line x" title="35:231	In our text categorization experiments below, each contrast document vij was obtained by starting with the original and masking out one or all of the several rationale substrings that the annotator had highlighted (rij)." ></td>
	<td class="line x" title="36:231	The intuition is that the correct model should be less sureofapositiveclassificationonthecontrastexample vij than on the original example vectorxi, because vij lacks evidence that the annotator found significant." ></td>
	<td class="line x" title="37:231	We can translate this intuition into additional constraints on the correct model, i.e., on the weight vector vectorw." ></td>
	<td class="line x" title="38:231	In addition to the usual SVM constraint on positive examples that vectorwxi  1, we also want (for each j) that vectorw vectorxi  vectorw vij  , where   0 controls the size of the desired margin between original and contrast examples." ></td>
	<td class="line x" title="39:231	An ordinary soft-margin SVM chooses vectorw and vector to minimize 1 2bardblvectorwbardbl 2 + C(summationdisplay i i) (1) subject to the constraints (i) vectorw xi yi  1i (2) (i) i  0 (3) where xi is a training example, yi  {1,+1} is its desired classification, and i is a slack variable that allows training example xi to miss satisfying the margin constraint if necessary." ></td>
	<td class="line x" title="40:231	The parameter C > 0 controls the cost of taking such slack, and should generally be lower for noisier or less linearly separable datasets." ></td>
	<td class="line x" title="41:231	We add the contrast constraints (i,j) vectorw (xi vij)yi  (1ij), (4) where vij is one of the contrast examples constructed from example xi, and ij  0 is an associated slack variable." ></td>
	<td class="line x" title="42:231	Just as these extra constraints have their own margin , their slack variables have theirowncost, sotheobjectivefunction(1)becomes 1 2bardblvectorwbardbl 2 + C(summationdisplay i i) + Ccontrast( summationdisplay i,j ij) (5) The parameter Ccontrast  0 determines the importance of satisfying the contrast constraints." ></td>
	<td class="line x" title="43:231	It should generally be less than C if the contrasts are noisier than the training examples.2 Inpractice, itispossibletosolvethisoptimization using a standard soft-margin SVM learner." ></td>
	<td class="line x" title="44:231	Dividing equation (4) through by , it becomes (i,j) vectorw xij yi  1ij, (6) where xij def= xivij." ></td>
	<td class="line x" title="45:231	Since equation (6) takes the same form as equation (2), we simply add the pairs (xij,yi) to the training set as pseudoexamples, weighted by Ccontrast rather than C so that the learner will use the objective function (5)." ></td>
	<td class="line x" title="46:231	There is one subtlety." ></td>
	<td class="line x" title="47:231	To allow a biased hyperplane, we use the usual trick of prepending a 1 element to each training example." ></td>
	<td class="line x" title="48:231	Thus we require vectorw  (1,xi)  1  i (which makes w0 play the role of a bias term)." ></td>
	<td class="line x" title="49:231	This means, however, that we must prepend a 0 element to each pseudoexample: vectorw  (1,vectorxi)(1,vij) = vectorw (0,xij)  1ij." ></td>
	<td class="line x" title="50:231	In our experiments, we optimize , C, and Ccontrast on held-out data (see section 5.2)." ></td>
	<td class="line x" title="51:231	4 Rationale Annotation for Movie Reviews In order to demonstrate that annotator rationales help machine learning, we needed annotated data that included rationales for the annotations." ></td>
	<td class="line pc" title="52:231	We chose a dataset that would be enjoyable to reannotate: the movie review dataset of (Pang et al. , 2002; Pang and Lee, 2004).3 The dataset consists of 1000 positive and 1000 negative movie reviews obtained from the Internet Movie Database (IMDb) review archive, all written before 2002 by a total of 312 authors, with a cap of 20 reviews per author per 2Taking Ccontrast to be constant means that all rationales are equally valuable." ></td>
	<td class="line x" title="53:231	One might instead choose, for example, to reduce Ccontrast for examples xi that have many rationales, to prevent xis contrast examples vij from together dominating the optimization." ></td>
	<td class="line x" title="54:231	However, in this paper we assume that an xi with more rationales really does provide more evidence about the true classifier vectorw." ></td>
	<td class="line x" title="55:231	3Polarity dataset version 2.0." ></td>
	<td class="line x" title="56:231	261 category." ></td>
	<td class="line x" title="57:231	Pang and Lee have divided the 2000 documents into 10 folds, each consisting of 100 positive reviews and 100 negative reviews." ></td>
	<td class="line x" title="58:231	The dataset is arguably artificial in that it keeps only reviews where the reviewer provided a rather high or rather low numerical rating, allowing Pang and Lee to designate the review as positive or negative." ></td>
	<td class="line x" title="59:231	Nonetheless, most reviews contain a difficult mix of praise, criticism, and factual description." ></td>
	<td class="line x" title="60:231	In fact, it is possible for a mostly critical review to give a positive overall recommendation, or vice versa." ></td>
	<td class="line x" title="61:231	4.1 Annotation procedure Rationale annotators were given guidelines4 that read, in part: Each review was intended to give either a positive or a negative overall recommendation." ></td>
	<td class="line x" title="62:231	You will be asked to justify why a review is positive or negative." ></td>
	<td class="line x" title="63:231	To justify why a review is positive, highlight the most important words and phrases that would tell someone to see the movie." ></td>
	<td class="line x" title="64:231	To justify why a review is negative, highlight words and phrases that would tell someone not to see the movie." ></td>
	<td class="line x" title="65:231	These words and phrases are called rationales." ></td>
	<td class="line x" title="66:231	You can highlight the rationales as you notice them, which should result in several rationales per review." ></td>
	<td class="line x" title="67:231	Do your best to mark enough rationales to provide convincing support for the class of interest." ></td>
	<td class="line x" title="68:231	You do not need to go out of your way to mark everything." ></td>
	<td class="line x" title="69:231	You are probably doing too much work if you find yourself going back to a paragraph to look for even more rationales in it." ></td>
	<td class="line x" title="70:231	Furthermore, it is perfectly acceptable to skim through sections that you feel would not contain many rationales, such as a reviewers plot summary, even if that might cause you to miss a rationale here and there." ></td>
	<td class="line x" title="71:231	The last two paragraphs were intended to provide some guidance on how many rationales to annotate." ></td>
	<td class="line x" title="72:231	Even so, as section 4.2 shows, some annotators were considerably more thorough (and slower)." ></td>
	<td class="line x" title="73:231	Annotators were also shown the following examples5 of positive rationales:  you will enjoy the hell out of American Pie." ></td>
	<td class="line x" title="74:231	 fortunately, they managed to do it in an interesting and funny way." ></td>
	<td class="line x" title="75:231	 he is one of the most exciting martial artists on the big screen, continuing to perform his own stunts and dazzling audiences with his flashy kicks and punches." ></td>
	<td class="line x" title="76:231	 the romance was enchanting." ></td>
	<td class="line x" title="77:231	and the following examples5 of negative rationales: 4Available at http://cs.jhu.edu/ozaidan/rationales." ></td>
	<td class="line x" title="78:231	5For our controlled study of annotation time (section 4.2), different examples were given with full document context." ></td>
	<td class="line x" title="79:231	Figure 1: Histograms of rationale counts per document (A0s annotations)." ></td>
	<td class="line x" title="80:231	The overall mean of 8.55 is close to that of the four annotators in Table 1." ></td>
	<td class="line x" title="81:231	The median and mode are 8 and 7." ></td>
	<td class="line x" title="82:231	 A woman in peril." ></td>
	<td class="line x" title="83:231	A confrontation." ></td>
	<td class="line x" title="84:231	An explosion." ></td>
	<td class="line x" title="85:231	The end." ></td>
	<td class="line x" title="86:231	Yawn." ></td>
	<td class="line x" title="87:231	Yawn." ></td>
	<td class="line x" title="88:231	Yawn." ></td>
	<td class="line x" title="89:231	 when a film makes watching Eddie Murphy a tedious experience, you know something is terribly wrong." ></td>
	<td class="line x" title="90:231	 the movie is so badly put together that even the most casualviewermaynoticethemiserablepacingandstray plot threads." ></td>
	<td class="line x" title="91:231	 dont go see this movie The annotation involves boldfacing the rationale phrases using an HTML editor." ></td>
	<td class="line x" title="92:231	Note that a fancier annotation tool would be necessary for a task like namedentitytagging, whereanannotatormustmark many named entities in a single document." ></td>
	<td class="line x" title="93:231	At any given moment, such a tool should allow the annotator to highlight, view, and edit only the several rationales for the current annotated entity (the one most recently annotated or re-selected)." ></td>
	<td class="line x" title="94:231	One of the authors (A0) annotated folds 08 of the movie review set (1,800 documents) with rationales that supported the gold-standard classifications." ></td>
	<td class="line x" title="95:231	This training/development set was used for all of the learning experiments in sections 56." ></td>
	<td class="line x" title="96:231	A histogram of rationale counts is shown in Figure 1." ></td>
	<td class="line x" title="97:231	As mentioned in section 3, the rationale annotations were just textual substrings." ></td>
	<td class="line x" title="98:231	The annotator did not require knowledge of the classifier features." ></td>
	<td class="line x" title="99:231	Thus, our rationale dataset is a new resource4 that could also be used to study exploitation of rationales under feature sets or learning methods other than those considered here (see section 8)." ></td>
	<td class="line x" title="100:231	4.2 Inter-annotator agreement To study the annotation process, we randomly selected 150 documents from the dataset." ></td>
	<td class="line x" title="101:231	The doc262 Rationales % rationales also % rationales also % rationales also % rationales also % rationales also per document annotated by A1 annotated by A2 annotated by AX annotated by AY ann." ></td>
	<td class="line x" title="102:231	by anyone else A1 5.02 (100) 69.6 63.0 80.1 91.4 A2 10.14 42.3 (100) 50.2 67.8 80.9 AX 6.52 49.0 68.0 (100) 79.9 90.9 AY 11.36 39.7 56.2 49.3 (100) 75.5 Table 1: Average number of rationales and inter-annotator agreement for Tasks 2 and 3." ></td>
	<td class="line x" title="103:231	A rationale by Ai (I think this is a great movie!) is considered to have been annotated also by Aj if at least one of Ajs rationales overlaps it (I think this is a great movie!)." ></td>
	<td class="line x" title="104:231	In computing pairwise agreement on rationales, we ignored documents where Ai and Aj disagreed on the class." ></td>
	<td class="line x" title="105:231	Notice that the most thorough annotatorAYcaught most rationales marked by the others (exhibiting high recall), and that most rationales enjoyed some degree of consensus, especially those marked by the least thorough annotator A1 (exhibiting high precision)." ></td>
	<td class="line x" title="106:231	uments were split into three groups, each consisting of 50 documents (25 positive and 25 negative)." ></td>
	<td class="line x" title="107:231	Each subset was used for one of three tasks:6  Task 1: Given the document, annotate only the class (positive/negative)." ></td>
	<td class="line x" title="108:231	 Task 2: Given the document and its class, annotate some rationales for that class." ></td>
	<td class="line x" title="109:231	 Task 3: Given the document, annotate both the class and some rationales for it." ></td>
	<td class="line x" title="110:231	We carried out a pilot study (annotators AX and AY: two of the authors) and a later, more controlled study (annotators A1 and A2: paid students)." ></td>
	<td class="line x" title="111:231	The latter was conducted in a more controlled environment where both annotators used the same annotation tool and annotation setup as each other." ></td>
	<td class="line x" title="112:231	Their guidelines were also more detailed (see section 4.1)." ></td>
	<td class="line x" title="113:231	In addition, the documents for the different tasks were interleaved to avoid any practice effect." ></td>
	<td class="line x" title="114:231	The annotators classification accuracies in Tasks 1 and 3 (against Pang & Lees labels) ranged from 92%97%, with 4-way agreement on the class for 89% of the documents, and pairwise agreement also ranging from 92%97%." ></td>
	<td class="line x" title="115:231	Table 1 shows how many rationales the annotators provided and how well their rationales agreed." ></td>
	<td class="line x" title="116:231	Interestingly, in Task 3, four of AXs rationales for a positive class were also partially highlighted by AY as support for AYs (incorrect) negative classifications, such as: 6Each task also had a warmup set of 10 documents to be annotated before that taskss 50 documents." ></td>
	<td class="line x" title="117:231	Documents for Tasks 2 and 3 would automatically open in an HTML editor while Task 1 documents opened in an HTML viewer with no editing option." ></td>
	<td class="line x" title="118:231	The annotators recorded their classifications for Tasks 1 and 3 on a spreadsheet." ></td>
	<td class="line x" title="119:231	min./KB A1 time A2 time AX time AY time Task 1 0.252 0.112 0.150 0.422 Task 2 0.396 0.537 0.242 0.626 Task 3 0.399 0.505 0.288 1.01 min./doc." ></td>
	<td class="line x" title="120:231	A1 time A2 time AX time AY time Task 1 1.04 0.460 0.612 1.73 min./rat." ></td>
	<td class="line x" title="121:231	A1 time A2 time AX time AY time Task 2 0.340 0.239 0.179 0.298 Task 3 0.333 0.198 0.166 0.302 Table 2: Average annotation rates on each task." ></td>
	<td class="line x" title="122:231	 Even with its numerous flaws, the movie all comes together, if only for those who   Beloved acts like an incredibly difficult chamber drama paired with a ghost story." ></td>
	<td class="line x" title="123:231	4.3 Annotation time Average annotation times are in Table 2." ></td>
	<td class="line x" title="124:231	As hoped, rationales did not take too much extra time for most annotators to provide." ></td>
	<td class="line x" title="125:231	For each annotator except A2, providing rationales only took roughly twice the time (Task 3 vs. Task 1), even though it meant marking an average of 511 rationales in addition to the class." ></td>
	<td class="line x" title="126:231	Why this low overhead?" ></td>
	<td class="line x" title="127:231	Because marking the class already required the Task 1 annotator to read the document and find some rationales, even if s/he did not mark them." ></td>
	<td class="line x" title="128:231	The only extra work in Task 3 is in making them explicit." ></td>
	<td class="line x" title="129:231	This synergy between class annotation and rationale annotation is demonstrated by the fact that doing both at once (Task 3) was faster than doing them separately (Tasks 1+2)." ></td>
	<td class="line x" title="130:231	We remark that this taskbinary classification on full documentsseems to be almost a worst-case scenario for the annotation of rationales." ></td>
	<td class="line x" title="131:231	At a purely mechanical level, it was rather heroic of A0 to attach 89 new rationale phrases rij to every bit yi of ordinary annotation." ></td>
	<td class="line x" title="132:231	Imagine by contrast a more local task of identifying entities or relations." ></td>
	<td class="line x" title="133:231	Each 263 lower-level annotation yi will tend to have fewer rationalesrij, whileyi itselfwillbemorecomplexand hence more difficult to mark." ></td>
	<td class="line x" title="134:231	Thus, we expect that the overhead of collecting rationales will be less in many scenarios than the factor of 2 we measured." ></td>
	<td class="line x" title="135:231	Annotation overhead could be further reduced." ></td>
	<td class="line x" title="136:231	Foramulti-classproblemlikerelationdetection,one couldasktheannotatortoproviderationales only for the rarer classes." ></td>
	<td class="line x" title="137:231	This small amount of extra time where the data is sparsest would provide extra guidance where it was most needed." ></td>
	<td class="line x" title="138:231	Another possibility is passive collection of rationales via eye tracking." ></td>
	<td class="line oc" title="139:231	5 Experimental Procedures 5.1 Feature extraction Although this dataset seems to demand discourselevel features that contextualize bits of praise and criticism, we exactly follow Pang et al.(2002) and Pang and Lee (2004) in merely using binary unigram features, corresponding to the 17,744 unstemmed word or punctuation types with count  4 in the full 2000-document corpus." ></td>
	<td class="line x" title="141:231	Thus, each document is reduced to a 0-1 vector with 17,744 dimensions, which is then normalized to unit length.7 We used the method of section 3 to place additionalconstraintsonalinearclassifier." ></td>
	<td class="line x" title="142:231	Givenatrainingdocument, wecreateseveralcontrastdocuments, each by deleting exactly one rationale substring from the training document." ></td>
	<td class="line x" title="143:231	Converting documents to feature vectors, we obtained an original example xi and several contrast examples vi1,vi2,8 Again, our training method required each original document to be classified more confidently (by a margin ) than its contrast documents." ></td>
	<td class="line x" title="144:231	Ifwewereusingmorethanunigramfeatures,then simply deleting a rationale substring would not always be the best way to create a contrast document, as the resulting ungrammatical sentences might cause deep feature extraction to behave strangely (e.g. , parseerrorsduringpreprocessing)." ></td>
	<td class="line x" title="145:231	Thegoalin creating the contrast document is merely to suppress 7The vectors are normalized before prepending the 1 corresponding to the bias term feature (mentioned in section 3)." ></td>
	<td class="line x" title="146:231	8The contrast examples were not normalized to precisely unitlength, butinsteadwerenormalizedbythesamefactorused to normalize xi." ></td>
	<td class="line x" title="147:231	This conveniently ensured that the pseudoexamples xij def= vectorxivij were sparse vectors, with 0 coordinates for all words not in the jth rationale." ></td>
	<td class="line x" title="148:231	features (n-grams, parts of speech, syntactic dependencies ) that depend in part on material in one or more rationales." ></td>
	<td class="line x" title="149:231	This could be done directly by modifying the feature extractors, or if one prefers to use existing feature extractors, by masking rather thandeletingtherationalesubstringe.g. , replacing each of its word tokens with a special MASK token that is treated as an out-of-vocabulary word." ></td>
	<td class="line x" title="150:231	5.2 Training and testing procedures We transformed this problem to an SVM problem (seesection3)andappliedSVMlight fortrainingand testing, using the default linear kernel." ></td>
	<td class="line x" title="151:231	We used only A0s rationales and the true classifications." ></td>
	<td class="line x" title="152:231	Fold 9 was reserved as a test set." ></td>
	<td class="line x" title="153:231	All accuracy results reported in the paper are the result of testing on fold 9, after training on subsets of folds 08." ></td>
	<td class="line x" title="154:231	Our learning curves show accuracy after training on T < 9 folds (i.e. , 200T documents), for various T. To reduce the noise in these results, the accuracy we report for training on T folds is actually the average of 9 different experiments with different (albeit overlapping) training sets that cover folds 08: 1 9 8summationdisplay i=0 acc(F9 | ,Fi+1 Fi+T) (7) where Fj denotes the fold numbered j mod 9, and acc(Z | ,Y ) means classification accuracy on the set Z after training on Y with hyperparameters ." ></td>
	<td class="line x" title="155:231	To evaluate whether two different training methods A and B gave significantly different averageaccuracy values, we used a paired permutation test (generalizing a sign test)." ></td>
	<td class="line x" title="156:231	The test assumes independence among the 200 test examples but not among the 9 overlapping training sets." ></td>
	<td class="line x" title="157:231	For each of the 200 test examples in fold 9, we measured (ai,bi), where ai (respectively bi) is the number of the 9 training sets under which A (respectively B) classified the example correctly." ></td>
	<td class="line x" title="158:231	The p value is the probability that the absolute difference between the average-accuracy values would reach or exceed the observed absolute difference, namely | 1200summationtext200i=1 aibi9 |, ifeach(ai,bi)hadanindependent 1/2 chance of being replaced with (bi,ai), as per the null hypothesis that A and B are indistinguishable." ></td>
	<td class="line x" title="159:231	For any given value of T and any given training method, we chose hyperparameters  = 264 Figure 2: Classification accuracy under five different experimental setups (S1S5)." ></td>
	<td class="line x" title="160:231	At each training size, the 5 accuraciesarepairwisesignificantlydifferent(pairedpermutationtest, p < 0.02; see section 5.2), except for {S3,S4} or {S4,S5} at some sizes." ></td>
	<td class="line x" title="161:231	(C,,Ccontrast) to maximize the following crossvalidation performance:9  = argmax  8summationdisplay i=0 acc(Fi | ,Fi+1 Fi+T) (8) Weusedasimplealternatingoptimizationprocedure that begins at 0 = (1.0,1.0,1.0) and cycles repeatedly through the three dimensions, optimizing along each dimension by a local grid search with resolution 0.1.10 Of course, when training without rationales, we did not have to optimize  or Ccontrast." ></td>
	<td class="line x" title="162:231	6 Experimental Results 6.1 The value of rationales The top curve (S1) in Figure 2 shows that performance does increase when we introduce rationales for the training examples as contrast examples (section 3)." ></td>
	<td class="line x" title="163:231	S1 is significantly higher than the baseline curve (S2) immediately below it, which trains an ordinary SVM classifier without using rationales." ></td>
	<td class="line x" title="164:231	At the largest training set size, rationales raise the accuracy from 88.5% to 92.2%, a 32% error reduction." ></td>
	<td class="line x" title="165:231	9One might obtain better performance (across all methods being compared) by choosing a separate  for each of the 9 training sets." ></td>
	<td class="line x" title="166:231	However, to simulate real limited-data training conditions, one should then find the  for each {i,,j} using a separate cross-validation withini,,j}only; this would slow down the experiments considerably." ></td>
	<td class="line x" title="167:231	10For optimizing along the C dimension, one could use the efficient method of Beineke et al.(2004), but not in SVMlight." ></td>
	<td class="line x" title="169:231	The lower three curves (S3S5) show that learning is separately helped by the rationale and the non-rationale portions of the documents." ></td>
	<td class="line x" title="170:231	S3S5 are degraded versions of the baseline S2: they are ordinary SVM classifiers that perform significantly worse than S2 (p < 0.001)." ></td>
	<td class="line x" title="171:231	Removing the rationale phrases from the training documents (S3) made the test documents much harder to discriminate (compared to S2)." ></td>
	<td class="line x" title="172:231	This suggests that annotator A0s rationales often covered most of the usable evidence for the true class." ></td>
	<td class="line x" title="173:231	However, the pieces to solving the classification puzzle cannot be found solely in the short rationale phrases." ></td>
	<td class="line x" title="174:231	Removing all non-rationale text from the training documents (S5) was even worse than removing the rationales (S3)." ></td>
	<td class="line x" title="175:231	In other words, we cannot hope to do well simply by training on just the rationales (S5), although that approach is improved somewhat in S4 by treating each rationale (similarly to S1) as a separate SVM training example." ></td>
	<td class="line x" title="176:231	This presents some insight into why our method gives the best performance." ></td>
	<td class="line x" title="177:231	The classifier in S1 is able to extract subtle patterns from the corpus, like S2, S3, or any other standard machine learning method, but it is also able to learn from a human annotators decision-making strategy." ></td>
	<td class="line x" title="178:231	6.2 Using fewer rationales In practice, one might annotate rationales for only some training documentseither when annotating a new corpus or when adding rationales post hoc to an existing corpus." ></td>
	<td class="line x" title="179:231	Thus, a range of options can be found between curves S2 and S1 of Figure 2." ></td>
	<td class="line x" title="180:231	Figure 3 explores this space, showing how far the learning curve S2 moves upward if one has time to annotate rationales for a fixed number of documents R. The key useful discovery is that much of the benefit can actually be obtained with relatively few rationales." ></td>
	<td class="line x" title="181:231	For example, with 800 training documents, annotating(0%,50%,100%)ofthemwithrationales gives accuracies of (86.9%, 89.2%, 89.3%)." ></td>
	<td class="line x" title="182:231	With the maximum of 1600 training documents, annotating (0%, 50%, 100%) with rationales gives (88.5%, 91.7%, 92.2%)." ></td>
	<td class="line x" title="183:231	To make this point more broadly, we find that the R = 200 curve is significantly above the R = 0 curve (p < 0.05) at all T  1200." ></td>
	<td class="line x" title="184:231	By contrast, the R = 800,R = 1000,R = 1600 points at each T 265 Figure 3: Classification accuracy for T  {200,400,,1600} training documents (x-axis) when only R  {0,200,,T} of them are annotated with rationales (different curves)." ></td>
	<td class="line x" title="185:231	The R = 0 curve above corresponds to the baseline S2 from Figure 2." ></td>
	<td class="line x" title="186:231	S1s points are found above as the leftmost points on the other curves, where R = T. value are all-pairs statistically indistinguishable." ></td>
	<td class="line x" title="187:231	The figure also suggests that rationales and documents may be somewhat orthogonal in their benefit." ></td>
	<td class="line x" title="188:231	When one has many documents and few rationales, there is no longer much benefit in adding more documents (the curve is flattening out), but adding more rationales seems to provide a fresh benefit: rationales have not yet reached their point of diminishing returns." ></td>
	<td class="line x" title="189:231	(While this fresh benefit was often statistically significant, and greater than the benefit from more documents, our experiments did not establish that it was significantly greater.)" ></td>
	<td class="line x" title="190:231	Theaboveexperimentskeep all ofA0srationales on a fraction of training documents." ></td>
	<td class="line x" title="191:231	We also experimented with keeping a fraction of A0s rationales (chosen randomly with randomized rounding) on all training documents." ></td>
	<td class="line x" title="192:231	This yielded no noteworthy or statistically significant differences from Figure 3." ></td>
	<td class="line x" title="193:231	These latter experiments simulate a lazy annotator who is less assiduous than A0." ></td>
	<td class="line x" title="194:231	Such annotators may be common in the real world." ></td>
	<td class="line x" title="195:231	We also suspect that they will be more desirable." ></td>
	<td class="line x" title="196:231	First, they should be able to add more rationales per hour than the A0style annotator from Figure 3: some rationales are simplymorenoticeablethanothers, andalazyannotatorwillquicklyfindthemostnoticeableoneswithout wasting time tracking down the rest." ></td>
	<td class="line x" title="197:231	Second, the most noticeable rationales that they mark may be the most effective ones for learning, although our random simulation of laziness could not test that." ></td>
	<td class="line x" title="198:231	7 Related Work Our rationales resemble side information in machine learningsupplementary information about the target function that is available at training time." ></td>
	<td class="line x" title="199:231	Side information is sometimes encoded as virtual examples like our contrast examples or pseudoexamples." ></td>
	<td class="line x" title="200:231	However, past work generates these by automatically transforming the training examples in ways that are expected to preserve or alter the classification (Abu-Mostafa, 1995)." ></td>
	<td class="line x" title="201:231	In another formulation, virtual examples are automatically generated but must be manually annotated (Kuusela and Ocone, 2004)." ></td>
	<td class="line x" title="202:231	Our approach differs because a human helps to generate the virtual examples." ></td>
	<td class="line x" title="203:231	Enforcing a margin between ordinary examples and contrast examples also appears new." ></td>
	<td class="line x" title="204:231	Other researchers have considered how to reduce annotation effort." ></td>
	<td class="line x" title="205:231	In active learning, the annotator classifies only documents where the system so far is less confident (Lewis and Gale, 1994), or in an information extraction setting, incrementally corrects details of the systems less confident entity segmentationsandlabelings(CulottaandMcCallum, 2005)." ></td>
	<td class="line x" title="206:231	Raghavan et al.(2005) asked annotators to identify globally relevant features." ></td>
	<td class="line x" title="208:231	In contrast, our approach does not force the annotator to evaluate the importance of features individually, nor in a global context outside any specific document, nor even to know the learners feature space." ></td>
	<td class="line x" title="209:231	Annotators only mark text that supports their classification decision." ></td>
	<td class="line x" title="210:231	Our methods then consider the combined effect of this text on the feature vector, which may include complex features not known to the annotator." ></td>
	<td class="line x" title="211:231	8 Future Work: Generative models Our SVM contrast method (section 3) is not the only possible way to use rationales." ></td>
	<td class="line x" title="212:231	We would like to explicitly model rationale annotation as a noisy process that reflects, imperfectly and incompletely, the annotators internal decision procedure." ></td>
	<td class="line x" title="213:231	A natural approach would start with log-linear models in place of SVMs." ></td>
	<td class="line x" title="214:231	We can define a probabilistic classifier p(y | x) def= 1Z(x) exp ksummationdisplay h=1 hfh(x,y) (9) 266 where vectorf() extracts a feature vector from a classified document." ></td>
	<td class="line x" title="215:231	A standard training method would be to choose  to maximize the conditional likelihood of the training classifications: argmax vector nproductdisplay i=1 p(yi | xi) (10) When a rationale ri is also available for each (xi,yi), we propose to maximize a likelihood that tries to predict these rationale data as well: argmax vector nproductdisplay i=1 p(yi | xi)pprime(ri | xi,yi,) (11) Notice that a given guess of  might make equation (10) large, yet accord badly with the annotators rationales." ></td>
	<td class="line x" title="216:231	In that case, the second term of equation (11) will exert pressure on  to change to something that conforms more closely to the rationales." ></td>
	<td class="line x" title="217:231	If the annotator is correct, such a  will generalize better beyond the training data." ></td>
	<td class="line x" title="218:231	Inequation(11),pprime modelsthestochasticprocess of rationale annotation." ></td>
	<td class="line x" title="219:231	What is an annotator actually doing when she annotates rationales?" ></td>
	<td class="line x" title="220:231	In particular, how do her rationales derive from the true value of  and thereby tell us about ?" ></td>
	<td class="line x" title="221:231	Building a good model pprime of rationale annotation will require some exploratory data analysis." ></td>
	<td class="line x" title="222:231	Roughly, we expect that if hfh(xi,y) is much higher for y = yi than for other values of y, then the annotators ri is correspondingly more likely to indicate in some way that feature fh strongly influenced annotation yi." ></td>
	<td class="line x" title="223:231	However, we must also model the annotators limited patience (she may not annotate all important features), sloppiness (she may indicate only indirectly that fh is important), and bias (tendency to annotate some kinds of features at the expense of others)." ></td>
	<td class="line x" title="224:231	One advantage of this generative approach is that it eliminates the need for contrast examples." ></td>
	<td class="line x" title="225:231	Consider a non-textual example in which an annotator highlights the line crossing in a digital image of the digit 8 to mark the rationale that distinguishes it from 0. In this case it is not clear how to mask out that highlighted rationale to create a contrast example in which relevant features would not fire.11 11One cannot simply flip those highlighted pixels to white 9 Conclusions We have proposed a quite simple approach to improving machine learning by exploiting the cleverness of annotators, asking them to provide enriched annotations for training." ></td>
	<td class="line x" title="226:231	We developed and tested a particular discriminative method that can use annotator rationaleseven on a fraction of the training setto significantly improve sentiment classification of movie reviews." ></td>
	<td class="line x" title="227:231	We found fairly good annotator agreement on the rationales themselves." ></td>
	<td class="line x" title="228:231	Most annotators provided several rationales per classification without taking too much extra time, even in our text classification scenario, where the rationales greatly outweigh the classifications in number and complexity." ></td>
	<td class="line x" title="229:231	Greater speed might be possible through an improved user interface or passive feedback (e.g. , eye tracking)." ></td>
	<td class="line x" title="230:231	In principle, many machine learning methods might be modified to exploit rationale data." ></td>
	<td class="line x" title="231:231	While our experiments in this paper used a discriminative SVM, we plan to explore generative approaches." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N07-1038
Multiple Aspect Ranking Using the Good Grief Algorithm
Snyder, Benjamin;Barzilay, Regina;"></td>
	<td class="line x" title="1:212	Proceedings of NAACL HLT 2007, pages 300307, Rochester, NY, April 2007." ></td>
	<td class="line x" title="2:212	c2007 Association for Computational Linguistics Multiple Aspect Ranking using the Good Grief Algorithm Benjamin Snyder and Regina Barzilay Computer Science and Arti cial Intelligence Laboratory Massachusetts Institute of Technology {bsnyder,regina}@csail.mit.edu Abstract We address the problem of analyzing multiple related opinions in a text." ></td>
	<td class="line x" title="3:212	For instance, in a restaurant review such opinions may include food, ambience and service." ></td>
	<td class="line x" title="4:212	We formulate this task as a multiple aspect ranking problem, where the goal is to produce a set of numerical scores, one for each aspect." ></td>
	<td class="line x" title="5:212	We present an algorithm that jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks." ></td>
	<td class="line x" title="6:212	This algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions, such as agreement and contrast." ></td>
	<td class="line x" title="7:212	We prove that our agreementbased joint model is more expressive than individual ranking models." ></td>
	<td class="line x" title="8:212	Our empirical results further con rm the strength of the model: the algorithm provides signi cant improvement over both individual rankers and a state-of-the-art joint ranking model." ></td>
	<td class="line oc" title="9:212	1 Introduction Previous work on sentiment categorization makes an implicit assumption that a single score can express the polarity of an opinion text (Pang et al. , 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003)." ></td>
	<td class="line x" title="10:212	However, multiple opinions on related matters are often intertwined throughout a text." ></td>
	<td class="line x" title="11:212	For example, a restaurant review may express judgment on food quality as well as the service and ambience of the restaurant." ></td>
	<td class="line x" title="12:212	Rather than lumping these aspects into a single score, we would like to capture each aspect of the writers opinion separately, thereby providing a more ne-grained view of opinions in the review." ></td>
	<td class="line x" title="13:212	To this end, we aim to predict a set of numeric ranks that re ects the users satisfaction for each aspect." ></td>
	<td class="line x" title="14:212	In the example above, we would assign a numeric rank from 1-5 for each of: food quality, service, and ambience." ></td>
	<td class="line x" title="15:212	A straightforward approach to this task would be to rank1 the text independently for each aspect, using standard ranking techniques such as regression or classi cation." ></td>
	<td class="line x" title="16:212	However, this approach fails to exploit meaningful dependencies between users judgments across different aspects." ></td>
	<td class="line x" title="17:212	Knowledge of these dependencies can be crucial in predicting accurate ranks, as a users opinions on one aspect can in uence his or her opinions on others." ></td>
	<td class="line x" title="18:212	The algorithm presented in this paper models the dependencies between different labels via the agreement relation." ></td>
	<td class="line x" title="19:212	The agreement relation captures whether the user equally likes all aspects of the item or whether he or she expresses different degrees of satisfaction." ></td>
	<td class="line x" title="20:212	Since this relation can often be determined automatically for a given text (Marcu and Echihabi, 2002), we can readily use it to improve rank prediction." ></td>
	<td class="line x" title="21:212	The Good Grief model consists of a ranking model for each aspect as well as an agreement model which predicts whether or not all rank aspects are 1In this paper, ranking refers to the task of assigning an integer from 1 tok to each instance." ></td>
	<td class="line x" title="22:212	This task is sometimes referred to as ordinal regression (Crammer and Singer, 2001) and rating prediction (Pang and Lee, 2005)." ></td>
	<td class="line x" title="23:212	300 equal." ></td>
	<td class="line x" title="24:212	The Good Grief decoding algorithm predicts a set of ranks one for each aspect which maximally satisfy the preferences of the individual rankers and the agreement model." ></td>
	<td class="line x" title="25:212	For example, if the agreement model predicts consensus but the individual rankers select ranks h5,5,4i, then the decoder decides whether to trust the the third ranker, or alter its prediction and outputh5,5,5ito be consistent with the agreement prediction." ></td>
	<td class="line x" title="26:212	To obtain a model well-suited for this decoding, we also develop a joint training method that conjoins the training of multiple aspect models." ></td>
	<td class="line x" title="27:212	We demonstrate that the agreement-based joint model is more expressive than individual ranking models." ></td>
	<td class="line x" title="28:212	That is, every training set that can be perfectly ranked by individual ranking models for each aspect can also be perfectly ranked with our joint model." ></td>
	<td class="line x" title="29:212	In addition, we give a simple example of a training set which cannot be perfectly ranked without agreement-based joint inference." ></td>
	<td class="line x" title="30:212	Our experimental results further con rm the strength of the Good Grief model." ></td>
	<td class="line x" title="31:212	Our model signi cantly outperforms individual ranking models as well as a stateof-the-art joint ranking model." ></td>
	<td class="line oc" title="32:212	2 Related Work Sentiment Classi cation Traditionally, categorization of opinion texts has been cast as a binary classication task (Pang et al. , 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003; Dave et al. , 2003)." ></td>
	<td class="line x" title="33:212	More recent work (Pang and Lee, 2005; Goldberg and Zhu, 2006) has expanded this analysis to the ranking framework where the goal is to assess review polarity on a multi-point scale." ></td>
	<td class="line x" title="34:212	While this approach provides a richer representation of a single opinion, it still operates on the assumption of one opinion per text." ></td>
	<td class="line x" title="35:212	Our work generalizes this setting to the problem of analyzing multiple opinions or multiple aspects of an opinion." ></td>
	<td class="line x" title="36:212	Since multiple opinions in a single text are related, it is insuf cient to treat them as separate single-aspect ranking tasks." ></td>
	<td class="line x" title="37:212	This motivates our exploration of a new method for joint multiple aspect ranking." ></td>
	<td class="line x" title="38:212	Ranking The ranking, or ordinal regression, problem has been extensivly studied in the Machine Learning and Information Retrieval communities." ></td>
	<td class="line x" title="39:212	In this section we focus on two online ranking methods which form the basis of our approach." ></td>
	<td class="line x" title="40:212	The rst is a model proposed by Crammer and Singer (2001)." ></td>
	<td class="line x" title="41:212	The task is to predict a rank y 2f1,,kg for every input x 2 Rn." ></td>
	<td class="line x" title="42:212	Their model stores a weight vector w 2 Rn and a vector of increasing boundaries b0 = 1 b1 bk1 bk = 1 which divide the real line into k segments, one for each possible rank." ></td>
	<td class="line x" title="43:212	The model rst scores each input with the weight vector: score(x) = w x. Finally, the model locates score(x) on the real line and returns the appropriate rank as indicated by the boundaries." ></td>
	<td class="line x" title="44:212	Formally, the model returns the rank r such that br1 score(x) < br." ></td>
	<td class="line x" title="45:212	The model is trained with the Perceptron Ranking algorithm (or PRank algorithm ), which reacts to incorrect predictions on the training set by updating the weight and boundary vectors." ></td>
	<td class="line x" title="46:212	The PRanking model and algorithm were tested on the EachMovie dataset with a separate ranking model learned for each user in the database." ></td>
	<td class="line x" title="47:212	An extension of this model is provided by Basilico and Hofmann (2004) in the context of collaborative ltering." ></td>
	<td class="line x" title="48:212	Instead of training a separate model for each user, Basilico and Hofmann train a joint ranking model which shares a set of boundaries across all users." ></td>
	<td class="line x" title="49:212	In addition to these shared boundaries, userspeci c weight vectors are stored." ></td>
	<td class="line x" title="50:212	To compute the score for input x and user i, the weight vectors for all users are employed: scorei(x) = w[i] x + summationdisplay j sim(i,j)(w[j] x) (1) where 0 sim(i,j) 1 is the cosine similarity between users i and j, computed on the entire training set." ></td>
	<td class="line x" title="51:212	Once the score has been computed, the prediction rule follows that of the PRanking model." ></td>
	<td class="line x" title="52:212	The model is trained using the PRank algorithm, with the exception of the new de nition for the scoring function.2 While this model shares information between the different ranking problems, it fails to explicitly model relations between the rank predictions." ></td>
	<td class="line x" title="53:212	In contrast, our algorithm uses an agreement model to learn such relations and inform joint predictions." ></td>
	<td class="line x" title="54:212	2In the notation of Basilico and Hofmann (2004), this definition of scorei(x) corresponds to the kernel K = (KidU + KcoU )KatX . 301 3 The Algorithm The goal of our algorithm is to nd a rank assignment that is consistent with predictions of individual rankers and the agreement model." ></td>
	<td class="line x" title="55:212	To this end, we develop the Good Grief decoding procedure that minimizes the dissatisfaction (grief ) of individual components with a joint prediction." ></td>
	<td class="line x" title="56:212	In this section, we formally de ne the grief of each component, and a mechanism for its minimization." ></td>
	<td class="line x" title="57:212	We then describe our method for joint training of individual rankers that takes into account the Good Grief decoding procedure." ></td>
	<td class="line x" title="58:212	3.1 Problem Formulation In an m-aspect ranking problem, we are given a training sequence of instance-label pairs (x1,y1),,(xt,yt), Each instance xt is a feature vector in Rn and the label yt is a vector of m ranks in Ym, where Y = f1, ,kg is the set of possible ranks." ></td>
	<td class="line x" title="59:212	The ith component of yt is the rank for the ith aspect, and will be denoted by y[i]t. The goal is to learn a mapping from instances to rank sets, H : X !Ym, which minimizes the distance between predicted ranks and true ranks." ></td>
	<td class="line x" title="60:212	3.2 The Model Our m-aspect ranking model containsm+1 components: (hw[1],b[1]i,,hw[m],b[m]i,a)." ></td>
	<td class="line x" title="61:212	The rst m components are individual ranking models, one for each aspect, and the nal component is the agreement model." ></td>
	<td class="line x" title="62:212	For each aspect i2 1m, w[i] 2Rn is a vector of weights on the input features, and b[i] 2Rk1 is a vector of boundaries which divide the real line into k intervals, corresponding to the k possible ranks." ></td>
	<td class="line x" title="63:212	The default prediction of the aspect ranking model simply uses the ranking rule of the PRank algorithm." ></td>
	<td class="line x" title="64:212	This rule predicts the rank r such that b[i]r1 scorei(x) < b[i]r.3 The value scorei(x) can be de ned simply as the dot product w[i] x, or it can take into account the weight vectors for other aspects weighted by a measure of interaspect similarity." ></td>
	<td class="line x" title="65:212	We adopt the de nition given in equation 1, replacing the user-speci c weight vectors with our aspect-speci c weight vectors." ></td>
	<td class="line x" title="66:212	3More precisely (taking into account the possibility of ties): y[i] = minr{1, ,k}{r : scorei(x)b[i]r < 0} The agreement model is a vector of weights a 2 Rn." ></td>
	<td class="line x" title="67:212	A value of a x > 0 predicts that the ranks of all m aspects are equal, and a value of a x 0 indicates disagreement." ></td>
	<td class="line x" title="68:212	The absolute value ja xj indicates the con dence in the agreement prediction." ></td>
	<td class="line x" title="69:212	The goal of the decoding procedure is to predict a joint rank for the m aspects which satis es the individual ranking models as well as the agreement model." ></td>
	<td class="line x" title="70:212	For a given input x, the individual model for aspect i predicts a default rank y[i] based on its feature weight and boundary vectorshw[i],b[i]i. In addition, the agreement model makes a prediction regarding rank consensus based on a x. However, the default aspect predictions y[1] y[m] may not accord with the agreement model." ></td>
	<td class="line x" title="71:212	For example, if a x > 0, but y[i]6= y[j] for some i,j21m, then the agreement model predicts complete consensus, whereas the individual aspect models do not." ></td>
	<td class="line x" title="72:212	We therefore adopt a joint prediction criterion which simultaneously takes into account all model components individual aspect models as well as the agreement model." ></td>
	<td class="line x" title="73:212	For each possible prediction r = (r[1],,r[m]) this criterion assesses the level of grief associated with the ith-aspect ranking model, gi(x,r[i])." ></td>
	<td class="line x" title="74:212	Similarly, we compute the grief of the agreement model with the joint prediction, ga(x,r) (bothgi andga are de ned formally below)." ></td>
	<td class="line x" title="75:212	The decoder then predicts the m ranks which minimize the overall grief: H(x) = arg min rYm bracketleftBigg ga(x,r) + msummationdisplay i=1 gi(x,r[i]) bracketrightBigg (2) If the default rank predictions for the aspect models, y = (y[1],, y[m]), are in accord with the agreement model (both indicating consensus or both indicating contrast), then the grief of all model components will be zero, and we simply output y. On the other hand, if y indicates disagreement but the agreement model predicts consensus, then we have the option of predicting y and bearing the grief of the agreement model." ></td>
	<td class="line x" title="76:212	Alternatively, we can predict some consensus yprime (i.e. with yprime[i] = yprime[j],8i,j) and bear the grief of the component ranking models." ></td>
	<td class="line x" title="77:212	The decoder H chooses the option with lowest overall grief.4 4This decoding criterion assumes that the griefs of the com302 Now we formally de ne the measures of grief used in this criterion." ></td>
	<td class="line x" title="78:212	Aspect Model Grief We de ne the grief of theithaspect ranking model with respect to a rank r to be the smallest magnitude correction term which places the inputs score into therth segment of the real line: gi(x,r) = minjcj s.t. b[i]r1 scorei(x) +c<b[i]r Agreement Model Grief Similarly, we de ne the grief of the agreement model with respect to a joint rank r = (r[1],,r[m]) as the smallest correction needed to bring the agreement score into accord with the agreement relation between the individual ranks r[1],,r[m]: ga(x,r) = minjcj s.t. a x +c> 0^8i,j21m : r[i] = r[j] _ a x +c 0^9i,j21m : r[i]6= r[j] 3.3 Training Ranking models Pseudo-code for Good Grief training is shown in Figure 1." ></td>
	<td class="line x" title="79:212	This training algorithm is based on PRanking (Crammer and Singer, 2001), an online perceptron algorithm." ></td>
	<td class="line x" title="80:212	The training is performed by iteratively ranking each training input x and updating the model." ></td>
	<td class="line x" title="81:212	If the predicted rank y is equal to the true rank y, the weight and boundaries vectors remain unchanged." ></td>
	<td class="line x" title="82:212	On the other hand, if y6= y, then the weights and boundaries are updated to improve the prediction for x (step 4.c in Figure 1)." ></td>
	<td class="line x" title="83:212	See (Crammer and Singer, 2001) for explanation and analysis of this update rule." ></td>
	<td class="line x" title="84:212	Our algorithm departs from PRanking by conjoining the updates for the m ranking models." ></td>
	<td class="line x" title="85:212	We achieve this by using Good Grief decoding at each step throughout training." ></td>
	<td class="line x" title="86:212	Our decoder H(x) (from equation 2) uses all the aspect component models ponent models are comparable." ></td>
	<td class="line x" title="87:212	In practice, we take an uncalibrated agreement model aprime and reweight it with a tuning parameter: a = aprime." ></td>
	<td class="line x" title="88:212	The value of  is estimated using the development set." ></td>
	<td class="line x" title="89:212	We assume that the griefs of the ranking models are comparable since they are jointly trained." ></td>
	<td class="line x" title="90:212	as well as the (previously trained) agreement model to determine the predicted rank for each aspect." ></td>
	<td class="line x" title="91:212	In concrete terms, for every training instance x, we predict the ranks of all aspects simultaneously (step 2 in Figure 1)." ></td>
	<td class="line x" title="92:212	Then, for each aspect we make a separate update based on this joint prediction (step 4 in Figure 1), instead of using the individual models predictions." ></td>
	<td class="line x" title="93:212	Agreement model The agreement model a is assumed to have been previously trained on the same training data." ></td>
	<td class="line x" title="94:212	An instance is labeled with a positive label if all the ranks associated with this instance are equal." ></td>
	<td class="line x" title="95:212	The rest of the instances are labeled as negative." ></td>
	<td class="line x" title="96:212	This model can use any standard training algorithm for binary classi cation such as Perceptron or SVM optimization." ></td>
	<td class="line oc" title="97:212	3.4 Feature Representation Ranking Models Following previous work on sentiment classi cation (Pang et al. , 2002), we represent each review as a vector of lexical features." ></td>
	<td class="line x" title="98:212	More speci cally, we extract all unigrams and bigrams, discarding those that appear fewer than three times." ></td>
	<td class="line x" title="99:212	This process yields about 30,000 features." ></td>
	<td class="line x" title="100:212	Agreement Model The agreement model also operates over lexicalized features." ></td>
	<td class="line x" title="101:212	The effectiveness of these features for recognition of discourse relations has been previously shown by Marcu and Echihabi (2002)." ></td>
	<td class="line x" title="102:212	In addition to unigrams and bigrams, we also introduce a feature that measures the maximum contrastive distance between pairs of words in a review." ></td>
	<td class="line x" title="103:212	For example, the presence of delicious and dirty indicate high contrast, whereas the pair expensive and slow indicate low contrast." ></td>
	<td class="line x" title="104:212	The contrastive distance for a pair of words is computed by considering the difference in relative weight assigned to the words in individually trained PRanking models." ></td>
	<td class="line x" title="105:212	4 Analysis In this section, we prove that our model is able to perfectly rank a strict superset of the training corpora perfectly rankable by m ranking models individually." ></td>
	<td class="line x" title="106:212	We rst show that if the independent ranking models can individually rank a training set perfectly, then our model can do so as well." ></td>
	<td class="line x" title="107:212	Next, we show that our model is more expressive by providing 303 Input : (x1,y1),,(xT,yT), Agreement model a, Decoder de ntion H(x) (from equation 2)." ></td>
	<td class="line x" title="108:212	Initialize : Set w[i]1 = 0, b[i]11,,b[i]1k1 = 0, b[i]1k =1,8i21m." ></td>
	<td class="line x" title="109:212	Loop : For t = 1,2,,T : 1." ></td>
	<td class="line x" title="110:212	Get a new instance xt2Rn." ></td>
	<td class="line x" title="111:212	2." ></td>
	<td class="line x" title="112:212	Predict yt = H(x; wt,bt,a) (Equation 2)." ></td>
	<td class="line x" title="113:212	3." ></td>
	<td class="line x" title="114:212	Get a new label yt." ></td>
	<td class="line x" title="115:212	4." ></td>
	<td class="line x" title="116:212	For aspect i = 1,,m: If y[i]t6= y[i]t update model (otherwise set w[i]t+1 = w[i]t, b[i]t+1r = b[i]tr,8r): 4.a For r = 1,,k 1 : If y[i]t r then y[i]tr = 1 else y[i]tr = 1." ></td>
	<td class="line x" title="117:212	4.b For r = 1,,k 1 : If (y[i]t r)y[i]tr 0 then [i]tr = y[i]tr else [i]tr = 0." ></td>
	<td class="line x" title="118:212	4.c Update w[i]t+1 w[i]t + (summationtextr[i]tr)xt." ></td>
	<td class="line x" title="119:212	For r = 1,,k 1 update : b[i]t+1r b[i]tr [i]tr." ></td>
	<td class="line x" title="120:212	Output : H(x; wT+1,bT+1,a)." ></td>
	<td class="line x" title="121:212	Figure 1: Good Grief Training." ></td>
	<td class="line x" title="122:212	The algorithm is based on PRanking training algorithm." ></td>
	<td class="line x" title="123:212	Our algorithm differs in the joint computation of all aspect predictions yt based on the Good Grief Criterion (step 2) and the calculation of updates for each aspect based on the joint prediction (step 4)." ></td>
	<td class="line x" title="124:212	a simple illustrative example of a training set which can only be perfectly ranked with the inclusion of an agreement model." ></td>
	<td class="line x" title="125:212	First we introduce some notation." ></td>
	<td class="line x" title="126:212	For each training instance (xt,yt), each aspect i 2 1m, and each rank r 2 1k, de ne an auxiliary variable y[i]tr with y[i]tr = 1 if y[i]t r and y[i]tr = 1 if y[i]t > r. In words, y[i]tr indicates whether the true rank y[i]t is to the right or left of a potential rank r. Now suppose that a training set (x1,y1),,(xT,yT) is perfectly rankable for each aspect independently." ></td>
	<td class="line x" title="127:212	That is, for each aspect i 2 1m, there exists some ideal model v[i] = (w[i],b[i]) such that the signed distance from the prediction to the rth boundary: w[i] xt b[i]r has the same sign as the auxiliary variable y[i]tr." ></td>
	<td class="line x" title="128:212	In other words, the minimum margin over all training instances and ranks,  = minr,tf(w[i] xt b[i]r)y[i]trg, is no less than zero." ></td>
	<td class="line x" title="129:212	Now for the tth training instance, de ne an agreement auxiliary variable at, where at = 1 when all aspects agree in rank and at = 1 when at least two aspects disagree in rank." ></td>
	<td class="line x" title="130:212	First consider the case where the agreement model a perfectly classi es all training instances: (a xt)at > 0,8t." ></td>
	<td class="line x" title="131:212	It is clear that Good Grief decoding with the ideal joint model (hw[1],b[1]i,,hw[m],b[m]i,a) will produce the same output as the component ranking models run separately (since the grief will always be zero for the default rank predictions)." ></td>
	<td class="line x" title="132:212	Now consider the case where the training data is not linearly separable with regard to agreement classi cation." ></td>
	<td class="line x" title="133:212	De ne the margin of the worst case error to be = maxtfj(a xt)j: (a xt)at < 0g." ></td>
	<td class="line x" title="134:212	If <, then again Good Grief decoding will always produce the default results (since the grief of the agreement model will be at most in cases of error, whereas the grief of the ranking models for any deviation from their default predictions will be at least )." ></td>
	<td class="line x" title="135:212	On the other hand, if  , then the agreement model errors could potentially disrupt the perfect ranking." ></td>
	<td class="line x" title="136:212	However, we need only rescale w := w( +epsilon1) and b := b( +epsilon1) to ensure that the grief of the ranking models will always exceed the grief of the agreement model in cases where the latter is in error." ></td>
	<td class="line x" title="137:212	Thus whenever independent ranking models can perfectly rank a training set, a joint ranking model with Good Grief decoding can do so as well." ></td>
	<td class="line x" title="138:212	Now we give a simple example of a training set which can only be perfectly ranked with the addition of an agreement model." ></td>
	<td class="line x" title="139:212	Consider a training set of four instances with two rank aspects: 304 hx1,y1i=h(1,0,1), (2,1)i hx2,y2i=h(1,0,0), (2,2)i hx3,y3i=h(0,1,1), (1,2)i hx4,y4i=h(0,1,0), (1,1)i We can interpret these inputs as feature vectors corresponding to the presence of good, bad, and but not in the following four sentences: The food was good, but not the ambience." ></td>
	<td class="line x" title="140:212	The food was good, and so was the ambience." ></td>
	<td class="line x" title="141:212	The food was bad, but not the ambience." ></td>
	<td class="line x" title="142:212	The food was bad, and so was the ambience." ></td>
	<td class="line x" title="143:212	We can further interpret the rst rank aspect as the quality of food, and the second as the quality of the ambience, both on a scale of 1-2." ></td>
	<td class="line x" title="144:212	A simple ranking model which only considers the words good and bad perfectly ranks the food aspect." ></td>
	<td class="line x" title="145:212	However, it is easy to see that no single model perfectly ranks the ambience aspect." ></td>
	<td class="line x" title="146:212	Consider any model hw,b = (b)i. Note that w x1 < b and w x2 b together imply that w3 < 0, whereas w x3 b and w x4 < b together imply that w3 > 0." ></td>
	<td class="line x" title="147:212	Thus independent ranking models cannot perfectly rank this corpus." ></td>
	<td class="line x" title="148:212	The addition of an agreement model, however, can easily yield a perfect ranking." ></td>
	<td class="line x" title="149:212	With a = (0,0, 5) (which predicts contrast with the presence of the words but not ) and a ranking model for the ambience aspect such as w = (1, 1,0),b = (0), the Good Grief decoder will produce a perfect rank." ></td>
	<td class="line x" title="150:212	5 Experimental Set-Up We evaluate our multi-aspect ranking algorithm on a corpus5 of restaurant reviews available on the website http://www.we8there.com." ></td>
	<td class="line x" title="151:212	Reviews from this website have been previously used in other sentiment analysis tasks (Higashinaka et al. , 2006)." ></td>
	<td class="line x" title="152:212	Each review is accompanied by a set of ve ranks, each on a scale of 1-5, covering food, ambience, service, value, and overall experience." ></td>
	<td class="line x" title="153:212	These ranks are provided by consumers who wrote original reviews." ></td>
	<td class="line x" title="154:212	Our corpus does not contain incomplete data points since all the reviews available on this website contain both a review text and the values for all the ve aspects." ></td>
	<td class="line x" title="155:212	Training and Testing Division Our corpus con5Data and code used in this paper are available at http://people.csail.mit.edu/bsnyder/naacl07 tains 4,488 reviews, averaging 115 words." ></td>
	<td class="line x" title="156:212	We randomly select 3,488 reviews for training, 500 for development and 500 for testing." ></td>
	<td class="line x" title="157:212	Parameter Tuning We used the development set to determine optimal numbers of training iterations for our model and for the baseline models." ></td>
	<td class="line x" title="158:212	Also, given an initial uncalibrated agreement model aprime, we de ne our agreement model to be a = aprime for an appropriate scaling factor ." ></td>
	<td class="line x" title="159:212	We tune the value of  on the development set." ></td>
	<td class="line x" title="160:212	Corpus Statistics Our training corpus contains 528 among 55 = 3025 possible rank sets." ></td>
	<td class="line x" title="161:212	The most frequent rank set h5,5,5,5,5i accounts for 30.5% of the training set." ></td>
	<td class="line x" title="162:212	However, no other rank set comprises more than 5% of the data." ></td>
	<td class="line x" title="163:212	To cover 90% of occurrences in the training set, 227 rank sets are required." ></td>
	<td class="line x" title="164:212	Therefore, treating a rank tuple as a single label is not a viable option for this task." ></td>
	<td class="line x" title="165:212	We also nd that reviews with full agreement across rank aspects are quite common in our corpus, accounting for 38% of the training data." ></td>
	<td class="line x" title="166:212	Thus an agreementbased approach is natural and relevant." ></td>
	<td class="line x" title="167:212	A rank of 5 is the most common rank for all aspects and thus a prediction of all 5s gives a MAJORITY baseline and a natural indication of task dif culty." ></td>
	<td class="line x" title="168:212	Evaluation Measures We evaluate our algorithm and the baseline using ranking loss (Crammer and Singer, 2001; Basilico and Hofmann, 2004)." ></td>
	<td class="line x" title="169:212	Ranking loss measures the average distance between the true rank and the predicted rank." ></td>
	<td class="line x" title="170:212	Formally, given N test instances (x1,y1),,(xN,yN) of an m-aspect ranking problem and the corresponding predictions y1,,yN, ranking loss is de ned assummationtext t,i |y[i]ty[i]t| mN . Lower values of this measure cor-respond to a better performance of the algorithm." ></td>
	<td class="line x" title="171:212	6 Results Comparison with Baselines Table 1 shows the performance of the Good Grief training algorithm GG TRAIN+DECODE along with various baselines, including the simple MAJORITY baseline mentioned in section 5." ></td>
	<td class="line x" title="172:212	The rst competitive baseline, PRANK, learns a separate ranker for each aspect using the PRank algorithm." ></td>
	<td class="line x" title="173:212	The second competitive baseline, SIM, shares the weight vectors across aspects using a similarity measure (Basilico and Hofmann, 2004)." ></td>
	<td class="line x" title="174:212	305 Food Service Value Atmosphere Experience Total MAJORITY 0.848 1.056 1.030 1.044 1.028 1.001 PRANK 0.606 0.676 0.700 0.776 0.618 0.675 SIM 0.562 0.648 0.706 0.798 0.600 0.663 GG DECODE 0.544 0.648 0.704 0.798 0.584 0.656 GG TRAIN+DECODE 0.534 0.622 0.644 0.774 0.584 0.632 GG ORACLE 0.510 0.578 0.674 0.694 0.518 0.595 Table 1: Ranking loss on the test set for variants of Good Grief and various baselines." ></td>
	<td class="line x" title="175:212	Figure 2: Rank loss for our algorithm and baselines as a function of training round." ></td>
	<td class="line x" title="176:212	Both of these methods are described in detail in Section 2." ></td>
	<td class="line x" title="177:212	In addition, we consider two variants of our algorithm: GG DECODE employs the PRank training algorithm to independently train all component ranking models and only applies Good Grief decoding at test time." ></td>
	<td class="line x" title="178:212	GG ORACLE uses Good Grief training and decoding but in both cases is given perfect knowledge of whether or not the true ranks all agree (instead of using the trained agreement model)." ></td>
	<td class="line x" title="179:212	Our model achieves a rank error of 0.632, compared to 0.675 for PRANK and 0.663 for SIM." ></td>
	<td class="line x" title="180:212	Both of these differences are statistically signi cant at p< 0.002 by a Fisher Sign Test." ></td>
	<td class="line x" title="181:212	The gain in performance is observed across all ve aspects." ></td>
	<td class="line x" title="182:212	Our model also yields signi cant improvement (p< 0.05) over the decoding-only variant GG DECODE, con rming the importance of joint training." ></td>
	<td class="line x" title="183:212	As shown in Figure 2, our model demonstrates consistent improvement over the baselines across all the training rounds." ></td>
	<td class="line x" title="184:212	Model Analysis We separately analyze our perConsensus Non-consensus PRANK 0.414 0.864 GG TRAIN+DECODE 0.324 0.854 GG ORACLE 0.281 0.830 Table 2: Ranking loss for our model and PRANK computed separately on cases of actual consensus and actual disagreement." ></td>
	<td class="line x" title="185:212	formance on the 210 test instances where all the target ranks agree and the remaining 290 instances where there is some contrast." ></td>
	<td class="line x" title="186:212	As Table 2 shows, we outperform the PRANK baseline in both cases." ></td>
	<td class="line x" title="187:212	However on the consensus instances we achieve a relative reduction in error of 21.8% compared to only a 1.1% reduction for the other set." ></td>
	<td class="line x" title="188:212	In cases of consensus, the agreement model can guide the ranking models by reducing the decision space to ve rank sets." ></td>
	<td class="line x" title="189:212	In cases of disagreement, however, our model does not provide suf cient constraints as the vast majority of ranking sets remain viable." ></td>
	<td class="line x" title="190:212	This explains the performance of GG ORACLE, the variant of our algorithm with perfect knowledge of agreement/disagreement facts." ></td>
	<td class="line x" title="191:212	As shown in Table 1, GG ORACLE yields substantial improvement over our algorithm, but most of this gain comes from consensus instances (see Table 2)." ></td>
	<td class="line x" title="192:212	We also examine the impact of the agreement model accuracy on our algorithm." ></td>
	<td class="line x" title="193:212	The agreement model, when considered on its own, achieves classi cation accuracy of 67% on the test set, compared to a majority baseline of 58%." ></td>
	<td class="line x" title="194:212	However, those instances with high con denceja xjexhibit substantially higher classi cation accuracy." ></td>
	<td class="line x" title="195:212	Figure 3 shows the performance of the agreement model as a function of the con dence value." ></td>
	<td class="line x" title="196:212	The 10% of the data with highest con dence values can be classi ed by 306 Figure 3: Accuracy of the agreement model on subsets of test instances with highest con denceja xj." ></td>
	<td class="line x" title="197:212	the agreement model with 90% accuracy, and the third of the data with highest con dence can be classi ed at 80% accuracy." ></td>
	<td class="line x" title="198:212	This property explains why the agreement model helps in joint ranking even though its overall accuracy may seem low." ></td>
	<td class="line x" title="199:212	Under the Good Grief criterion, the agreement models prediction will only be enforced when its grief outweighs that of the ranking models." ></td>
	<td class="line x" title="200:212	Thus in cases where the prediction con dence (ja xj) is relatively low,6 the agreement model will essentially be ignored." ></td>
	<td class="line x" title="201:212	7 Conclusion and Future Work We considered the problem of analyzing multiple related aspects of user reviews." ></td>
	<td class="line x" title="202:212	The algorithm presented jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks." ></td>
	<td class="line x" title="203:212	The strength of our algorithm lies in its ability to guide the prediction of individual rankers using rhetorical relations between aspects such as agreement and contrast." ></td>
	<td class="line x" title="204:212	Our method yields signi cant empirical improvements over individual rankers as well as a state-of-the-art joint ranking model." ></td>
	<td class="line x" title="205:212	Our current model employs a single rhetorical relation agreement vs. contrast to model dependencies between different opinions." ></td>
	<td class="line x" title="206:212	As our analy6What counts as relatively low will depend on both the value of the tuning parameter  and the con dence of the component ranking models for a particular input x. sis shows, this relation does not provide suf cient constraints for non-consensus instances." ></td>
	<td class="line x" title="207:212	An avenue for future research is to consider the impact of additional rhetorical relations between aspects." ></td>
	<td class="line x" title="208:212	We also plan to theoretically analyze the convergence properties of this and other joint perceptron algorithms." ></td>
	<td class="line x" title="209:212	Acknowledgments The authors acknowledge the support of the National Science Foundation (CAREER grant IIS-0448168 and grant IIS0415865) and the Microsoft Research Faculty Fellowship." ></td>
	<td class="line x" title="210:212	Thanks to Michael Collins, Pawan Deshpande, Jacob Eisenstein, Igor Malioutov, Luke Zettlemoyer, and the anonymous reviewers for helpful comments and suggestions." ></td>
	<td class="line x" title="211:212	Thanks also to Vasumathi Raman for programming assistance." ></td>
	<td class="line x" title="212:212	Any opinions, ndings, and conclusions or recommendations expressed above are those of the authors and do not necessarily re ect the views of the NSF." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N07-1039
Extracting Appraisal Expressions
Bloom, Kenneth;Garg, Navendu;Argamon, Shlomo;"></td>
	<td class="line x" title="1:46	Proceedings of NAACL HLT 2007, pages 308315, Rochester, NY, April 2007." ></td>
	<td class="line x" title="2:46	c2007 Association for Computational Linguistics Extracting Appraisal Expressions Kenneth Bloom and Navendu Garg and Shlomo Argamon Computer Science Department Illinois Institute of Technology 10 W. 31st St. Chicago, IL 60616 {kbloom1,gargnav,argamon}@iit.edu Abstract Sentiment analysis seeks to characterize opinionated or evaluative aspects of natural language text." ></td>
	<td class="line x" title="3:46	We suggest here that appraisal expression extraction should be viewed as a fundamental task in sentiment analysis." ></td>
	<td class="line x" title="4:46	An appraisal expression is a textual unit expressing an evaluative stance towards some target." ></td>
	<td class="line x" title="5:46	The task is to find and characterize the evaluative attributes of such elements." ></td>
	<td class="line x" title="6:46	This paper describes a system for effectively extracting and disambiguating adjectival appraisal expressions in English outputting a generic representation in terms of their evaluative function in the text." ></td>
	<td class="line x" title="7:46	Data mining on appraisal expressions gives meaningful and non-obvious insights." ></td>
	<td class="line x" title="8:46	1 Introduction Sentiment analysis, which seeks to analyze opinion in natural language text, has grown in interest in recent years." ></td>
	<td class="line oc" title="9:46	Sentiment analysis includes a variety of different problems, including: sentiment classification techniques to classify reviews as positive or negative, based on bag of words (Pang et al. , 2002) or positive and negative words (Turney, 2002; Mullen and Collier, 2004); classifying sentences in a document as either subjective or objective (Riloff and Wiebe, 2003; Pang and Lee, 2004); identifying or classifying appraisal targets (Nigam and Hurst, 2004); identifying the source of an opinion in a text (Choi et al. , 2005), whether the author is expressing the opinion, or whether he is attributing the opinion to someone else; and developing interactive and visual opinion mining methods (Gamon et al. , 2005; Popescu and Etzioni, 2005)." ></td>
	<td class="line n" title="10:46	Much of this work has utilized the fundamental concept of semantic orientation, (Turney, 2002); however, sentiment analysis still lacks a unified field theory." ></td>
	<td class="line x" title="11:46	We propose in this paper that a fundamental task underlying many of these formulations is the extraction and analysis of appraisal expressions, defined as those structured textual units which express an evaluation of some object." ></td>
	<td class="line x" title="12:46	An appraisal expression hasthreemaincomponents: an attitude (whichtakes an evaluative stance about an object), a target (the object of the stance), and a source (the person taking the stance) which may be implied." ></td>
	<td class="line x" title="13:46	The idea of appraisal extraction is a generalization of problem formulations developed in earlier works." ></td>
	<td class="line x" title="14:46	Mullen and Colliers (2004) notion of classifying appraisal terms using a multidimensional set of attributes is closely tied to the definition of an appraisal expression, which is classified along several dimensions." ></td>
	<td class="line x" title="15:46	In previous work (Whitelaw et al. , 2005), we presented a related technique of finding opinion phrases, using a multidimensional set of attributes and modeling the semantics of modifiers in these phrases." ></td>
	<td class="line x" title="16:46	The use of multiple text classifiers by Wiebe and colleagues (Wilson et al. , 2005; Wiebe et al. , 2004) for various kinds of sentiment classification can also be viewed as a sentencelevel technique for analyzing appraisal expressions." ></td>
	<td class="line x" title="17:46	Nigam and Hursts (2004) work on detecting opinions about a certain topic presages our notion of connecting attitudes to targets, while Popescu and Etzionis (2005) opinion mining technique also fits well into our framework." ></td>
	<td class="line x" title="18:46	In this paper we describe a system for extracting adjectival appraisal expressions, based on a handbuilt lexicon, a combination of heuristic shallow parsing and dependency parsing, and expectationmaximization word sense disambiguation." ></td>
	<td class="line x" title="19:46	Each ex308 tracted appraisal expression is represented as a set of feature values in terms of its evaluative function in thetext." ></td>
	<td class="line x" title="20:46	Wehaveappliedthissystemtotwodomains of texts: product reviews, and movie reviews." ></td>
	<td class="line x" title="21:46	Manual evaluation of the extraction shows our system to work well, as well as giving some directions for improvement." ></td>
	<td class="line x" title="22:46	We also show how straightforward data mining can give users very useful information about public opinion." ></td>
	<td class="line x" title="23:46	2 Appraisal Expressions We define an appraisal expression to be an elementary linguistic unit that conveys an attitude of some kind towards some target." ></td>
	<td class="line x" title="24:46	An appraisal expression is defined to comprise a source, an attitude, and a target, each represented by various attributes." ></td>
	<td class="line x" title="25:46	For example, in I found the movie quite monotonous, the speaker (the Source) expresses a negative Attitude (quite monotonous) towards the movie (the Target)." ></td>
	<td class="line x" title="26:46	Note that attitudes come in different types; for example, monotonous describes an inherent quality of the Target, while loathed would describe the emotional reaction of the Source." ></td>
	<td class="line x" title="27:46	Attitude may be expressed through nouns, verbs, adjectives and metaphors." ></td>
	<td class="line x" title="28:46	Extracting all of this information accurately for all of these types of appraisal expressions is a very difficult problem." ></td>
	<td class="line x" title="29:46	We therefore restrict ourselves for now to adjectival appraisal expressions that are each contained in a single sentence." ></td>
	<td class="line x" title="30:46	Additionally, we focus here only on extracting and analyzing the attitude and the target, but not the source." ></td>
	<td class="line x" title="31:46	Even with these restrictions, we obtain interesting results (Sec." ></td>
	<td class="line x" title="32:46	7)." ></td>
	<td class="line x" title="33:46	2.1 Appraisal attributes Our method is grounded in Appraisal Theory, developed by Martin and White (2005), which analyzes the way opinion is expressed." ></td>
	<td class="line x" title="34:46	Following Martin and White, we define: Attitude type is type of appraisal being expressedone of affect, appreciation, or judgment (Figure 1)." ></td>
	<td class="line x" title="35:46	Affect refers to an emotional state (e.g. , happy, angry), and is the most explicitly subjective type of appraisal." ></td>
	<td class="line x" title="36:46	The other two types express evaluation of external entities, differentiating between intrinsic appreciation of object properties (e.g. , slender, ugly) and social judgment (e.g. , heroic, idiotic)." ></td>
	<td class="line x" title="37:46	Orientation is whether the attitude is positive Attitude Type Appreciation Composition Balance: consistent, discordant, Complexity: elaborate, convoluted,  Reaction Impact: amazing, compelling, dull,  Quality: beautiful, elegant, hideous,  Valuation: innovative, profound, inferior,  Affect: happy, joyful, furious,  Judgment Social Esteem Capacity: clever, competent, immature,  Tenacity: brave, hard-working, foolhardy,  Normality: famous, lucky, obscure,  Social Sanction Propriety: generous, virtuous, corrupt,  Veracity: honest, sincere, sneaky,  Figure 1: The Attitude Type taxonomy, with examples of adjectives from the lexicon." ></td>
	<td class="line x" title="38:46	(good) or negative (bad)." ></td>
	<td class="line x" title="39:46	Force describes the intensity of the appraisal." ></td>
	<td class="line x" title="40:46	Force is largely expressed via modifiers such as very (increased force), or slightly (decreased force), but may also be expressed lexically, for example greatest vs. great vs. good." ></td>
	<td class="line x" title="41:46	Polarity of an appraisal is marked if it is scoped in a polarity marker (such as not), or unmarked otherwise." ></td>
	<td class="line x" title="42:46	Other attributes of appraisal are affected by negation; e.g., not good also has the opposite orientation from good." ></td>
	<td class="line x" title="43:46	Target type is a domain-dependent semantic type for the target." ></td>
	<td class="line x" title="44:46	This attribute takes on values fromadomain-dependenttaxonomy,representing important (and easily extractable) distinctions between targets in the domain." ></td>
	<td class="line x" title="45:46	2.2 Target taxonomies Two domain-dependent target type taxonomies are shown in Figure 2." ></td>
	<td class="line x" title="46:46	In both, the primary distinction is between a direct naming of a kind of Thing or a deictic/pronominal reference (e.g. , those or it), since the system does not currently rely on coreference resolution." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1053
Opinion Mining using Econometrics: A Case Study on Reputation Systems
Ghose, Anindya;Ipeirotis, Panagiotis;Sundararajan, Arun;"></td>
	<td class="line x" title="1:201	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 416423, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:201	c2007 Association for Computational Linguistics Opinion Mining Using Econometrics: A Case Study on Reputation Systems Anindya Ghose Panagiotis G. Ipeirotis Department of Information, Operations, and Management Sciences Leonard N. Stern School of Business, New York University {aghose,panos,arun}@stern.nyu.edu Arun Sundararajan Abstract Deriving the polarity and strength of opinions is an important research topic, attracting significant attention over the last few years." ></td>
	<td class="line x" title="3:201	In this work, to measure the strength and polarity of an opinion, we consider the economic context in which the opinion is evaluated, instead of using human annotators or linguistic resources." ></td>
	<td class="line x" title="4:201	We rely on the fact that text in on-line systems influences the behavior of humans and this effect can be observed using some easy-to-measure economic variables, such as revenues or product prices." ></td>
	<td class="line x" title="5:201	By reversing the logic, we infer the semantic orientation and strength of an opinion by tracing the changes in the associated economic variable." ></td>
	<td class="line x" title="6:201	In effect, we use econometrics to identify the economic value of text and assign a dollar value to each opinion phrase, measuring sentiment effectively and without the need for manual labeling." ></td>
	<td class="line x" title="7:201	We argue that by interpreting opinions using econometrics, we have the first objective, quantifiable, and contextsensitive evaluation of opinions." ></td>
	<td class="line x" title="8:201	We make the discussion concrete by presenting results on the reputation system of Amazon.com." ></td>
	<td class="line x" title="9:201	We show that user feedback affects the pricing power of merchants and by measuring their pricing power we can infer the polarity and strength of the underlying feedback postings." ></td>
	<td class="line x" title="10:201	1 Introduction A significant number of websites today allow users to post articles where they express opinions about products, firms, people, and so on." ></td>
	<td class="line x" title="11:201	For example, users on Amazom.com post reviews about products they bought and users on eBay.com post feedback describing their experiences with sellers." ></td>
	<td class="line x" title="12:201	The goal of opinion mining systems is to identify such pieces of the text that express opinions (Breck et al. , 2007; Konig and Brill, 2006) and then measure the polarity and strength of the expressed opinions." ></td>
	<td class="line x" title="13:201	While intuitively the task seems straightforward, there are multiple challenges involved." ></td>
	<td class="line x" title="14:201	 What makes an opinion positive or negative?" ></td>
	<td class="line x" title="15:201	Is there an objective measure for this task?" ></td>
	<td class="line x" title="16:201	 How can we rank opinions according to their strength?" ></td>
	<td class="line x" title="17:201	Can we define an objective measure for ranking opinions?" ></td>
	<td class="line x" title="18:201	 How does the context change the polarity and strength of an opinion and how can we take the context into consideration?" ></td>
	<td class="line x" title="19:201	To evaluate the polarity and strength of opinions, most of the existing approaches rely either on training from human-annotated data (Hatzivassiloglou and McKeown, 1997), or use linguistic resources (Hu and Liu, 2004; Kim and Hovy, 2004) like WordNet, or rely on co-occurrence statistics (Turney, 2002) between words that are unambiguously positive (e.g. , excellent) and unambiguously negative (e.g. , horrible)." ></td>
	<td class="line oc" title="20:201	Finally, other approaches rely on reviews with numeric ratings from websites (Pang and Lee, 2002; Dave et al. , 2003; Pang and Lee, 2004; Cui et al. , 2006) and train (semi-)supervised learning algorithms to classify reviews as positive or negative, or in more fine-grained scales (Pang and Lee, 2005; Wilson et al. , 2006)." ></td>
	<td class="line o" title="21:201	Implicitly, the supervised learning techniques assume that numeric ratings fully encapsulate the sentiment of the review.416 In this paper, we take a different approach and instead consider the economic context in which an opinion is evaluated." ></td>
	<td class="line x" title="22:201	We observe that the text in on-line systems influence the behavior of the readers." ></td>
	<td class="line x" title="23:201	This effect can be measured by observing some easy-tomeasure economic variable, such as product prices." ></td>
	<td class="line x" title="24:201	For instance, online merchants on eBay with positive feedback can sell products for higher prices than competitors with negative evaluations." ></td>
	<td class="line x" title="25:201	Therefore, each of these (positive or negative) evaluations has a (positive or negative) effect on the prices that the merchant can charge." ></td>
	<td class="line x" title="26:201	For example, everything else being equal, a seller with speedy delivery may be able to charge $10 more than a seller with slow delivery." ></td>
	<td class="line x" title="27:201	Using this information, we can conclude that speedy is better than slow when applied to delivery and their difference is $10." ></td>
	<td class="line x" title="28:201	Thus, we can infer the semantic orientation and the strength of an evaluation from the changes in the observed economic variable." ></td>
	<td class="line x" title="29:201	Following this idea, we use techniques from econometrics to identify the economic value of text and assign a dollar value to each text snippet, measuring sentiment strength and polarity effectively and without the need for labeling or any other resource." ></td>
	<td class="line x" title="30:201	We argue that by interpreting opinions within an econometric framework, we have the first objective and context-sensitive evaluation of opinions." ></td>
	<td class="line x" title="31:201	For example, consider the comment good packaging, posted by a buyer to evaluate a merchant." ></td>
	<td class="line x" title="32:201	This comment would have been considered unambiguously positive by the existing opinion mining systems." ></td>
	<td class="line x" title="33:201	We observed, though, that within electronic markets, such as eBay, a posting that contains the words good packaging has actually negative effect on the power of a merchant to charge higher prices." ></td>
	<td class="line x" title="34:201	This surprising effect reflects the nature of the comments in online marketplaces: buyers tend to use superlatives and highly enthusiastic language to praise a good merchant, and a lukewarm good packaging is interpreted as negative." ></td>
	<td class="line x" title="35:201	By introducing the econometric interpretation of opinions we can effortlessly capture such challenging scenarios, something that is impossible to achieve with the existing approaches." ></td>
	<td class="line x" title="36:201	We focus our paper on reputation systems in electronic markets and we examine the effect of opinions on the pricing power of merchants in the marketplace of Amazon.com." ></td>
	<td class="line x" title="37:201	(We discuss more applications in Section 7)." ></td>
	<td class="line x" title="38:201	We demonstrate the value of our technique using a dataset with 9,500 transactions that took place over 180 days." ></td>
	<td class="line x" title="39:201	We show that textual feedback affects the power of merchants to charge higher prices than the competition, for the same product, and still make a sale." ></td>
	<td class="line x" title="40:201	We then reverse the logic and determine the contribution of each comment in the pricing power of a merchant." ></td>
	<td class="line x" title="41:201	Thus, we discover the polarity and strength of each evaluation without the need for human annotation or any other form of linguistic resource." ></td>
	<td class="line x" title="42:201	The structure of the rest of the paper is as follows." ></td>
	<td class="line x" title="43:201	Section 2 gives the basic background on reputation systems." ></td>
	<td class="line x" title="44:201	Section 3 describes our methodology for constructing the data set that we use in our experiments." ></td>
	<td class="line x" title="45:201	Section 4 shows how we combine established techniques from econometrics with text mining techniques to identify the strength and polarity of the posted feedback evaluations." ></td>
	<td class="line x" title="46:201	Section 5 presents the experimental evaluations of our techniques." ></td>
	<td class="line x" title="47:201	Finally, Section 6 discusses related work and Section 7 discusses further applications and concludes the paper." ></td>
	<td class="line x" title="48:201	2 Reputation Systems and Price Premiums When buyers purchase products in an electronic market, they assess and pay not only for the product they wish to purchase but for a set of fulfillment characteristics as well, e.g., packaging, delivery, and the extent to which the product description matches the actual product." ></td>
	<td class="line x" title="49:201	Electronic markets rely on reputation systems to ensure the quality of these characteristics for each merchant, and the importance of such systems is widely recognized in the literature (Resnick et al. , 2000; Dellarocas, 2003)." ></td>
	<td class="line x" title="50:201	Typically, merchants reputation in electronic markets is encoded by a reputation profile that includes: (a) the number of past transactions for the merchant, (b) a summary of numeric ratings from buyers who have completed transactions with the seller, and (c) a chronological list of textual feedback provided by these buyers." ></td>
	<td class="line x" title="51:201	Studies of online reputation, thus far, base a merchants reputation on the numeric rating that characterizes the seller (e.g. , average number of stars and number of completed transactions) (Melnik and Alm, 2002)." ></td>
	<td class="line x" title="52:201	The general conclusion of these studies show that merchants with higher (numeric) reputation can charge higher prices than the competition, for the same products, and still manage to make a sale." ></td>
	<td class="line x" title="53:201	This price premium that the merchants can command over the competition is a measure of their reputation." ></td>
	<td class="line x" title="54:201	Definition 2.1 Consider a set of merchants s1,,sn selling a product for prices p1,,pn." ></td>
	<td class="line x" title="55:201	If si makes417 Figure 1: A set of merchants on Amazon.com selling an identical product for different prices the sale for price pi, then si commands a price premium equal to pi  pj over sj and a relative price premium equal to pipjpi." ></td>
	<td class="line x" title="56:201	Hence, a transaction that involves n competing merchants generates n 1 price premiums.1 The average price premium for the transaction is summationtext jnegationslash=i(pipj) n1 and the average relative price premium is summationtext jnegationslash=i(pipj) pi(n1) . a50 Example 2.1 Consider the case in Figure 1 where three merchants sell the same product for $631.95, $632.26, and $637.05, respectively." ></td>
	<td class="line x" title="57:201	If GameHog sells the product, then the price premium against XP Passport is $4.79 (= $637.05$632.26) and against the merchant BuyPCsoft is $5.10." ></td>
	<td class="line x" title="58:201	The relative price premium is 0.75% and 0.8%, respectively." ></td>
	<td class="line x" title="59:201	Similarly, the average price premium for this transaction is $4.95 and the average relative price premium 0.78%." ></td>
	<td class="line x" title="60:201	a50 Different sellers in these markets derive their reputation from different characteristics: some sellers have a reputation for fast delivery, while some others have a reputation of having the lowest price among their peers." ></td>
	<td class="line x" title="61:201	Similarly, while some sellers are praised for their packaging in the feedback, others get good comments for selling high-quality goods but are criticized for being rather slow with shipping." ></td>
	<td class="line x" title="62:201	Even though previous studies have established the positive correlation between higher (numeric) reputation and higher price premiums, they ignored completely the role of the textual feedback and, in turn, the multi-dimensional nature of reputation in electronic markets." ></td>
	<td class="line x" title="63:201	We show that the textual feedback adds significant additional value to the numerical scores, and affects the pricing power of the merchants." ></td>
	<td class="line x" title="64:201	1As an alternative definition we can ignore the negative price premiums." ></td>
	<td class="line x" title="65:201	The experimental results are similar for both versions." ></td>
	<td class="line x" title="66:201	3 Data We compiled a data set using software resellers from publicly available information on software product listings at Amazon.com." ></td>
	<td class="line x" title="67:201	Our data set includes 280 individual software titles." ></td>
	<td class="line x" title="68:201	The sellers reputation matters when selling identical goods, and the price variation observed can be attributed primarily to variation in the merchants reputation." ></td>
	<td class="line x" title="69:201	We collected the data using Amazon Web Services over a period of 180 days, between October 2004 and March 2005." ></td>
	<td class="line x" title="70:201	We describe below the two categories of data that we collected." ></td>
	<td class="line x" title="71:201	Transaction Data: The first part of our data set contains details of the transactions that took place on the marketplace of Amazon.com for each of the software titles." ></td>
	<td class="line x" title="72:201	The Amazon Web Services associates a unique transaction ID for each unique product listed by a seller." ></td>
	<td class="line x" title="73:201	This transaction ID enables us to distinguish between multiple or successive listings of identical products sold by the same merchant." ></td>
	<td class="line x" title="74:201	Keeping with the methodology in prior research (Ghose et al. , 2006), we crawl the Amazons XML listings every 8 hours and when a transaction ID associated with a particular listing is removed, we infer that the listed product was successfully sold in the prior 8 hour window.2 For each transaction that takes place, we keep the price at which the product was sold and the merchants reputation at the time of the transaction (more on this later)." ></td>
	<td class="line x" title="75:201	Additionally, for each of the competing listings for identical products, we keep the listed price along with the competitors reputation." ></td>
	<td class="line x" title="76:201	Using the collected data, we compute the price premium variables for each transaction3 using Definition 2.1." ></td>
	<td class="line x" title="77:201	Overall, our data set contains 1,078 merchants, 9,484 unique transactions and 107,922 price premiums (recall that each transaction generates multiple price premiums)." ></td>
	<td class="line x" title="78:201	Reputation Data: The second part of our data set contains the reputation history of each merchant that had a (monitored) product for sale during our 180-day window." ></td>
	<td class="line x" title="79:201	Each of these merchants has a feedback profile, which consists of numerical scores and text-based feedback, posted by buyers." ></td>
	<td class="line x" title="80:201	We had an average of 4,932 postings per merchant." ></td>
	<td class="line x" title="81:201	The numerical ratings 2Amazon indicates that their seller listings remain on the site indefinitely until they are sold and sellers can change the price of the product without altering the transaction ID. 3Ideally, we would also include the tax and shipping cost charged by each merchant in the computation of the price premiums." ></td>
	<td class="line x" title="82:201	Unfortunately, we could not capture these costs using our methodology." ></td>
	<td class="line x" title="83:201	Assuming that the fees for shipping and tax are independent of the merchants reputation, our analysis is not affected.418 are provided on a scale of one to five stars." ></td>
	<td class="line x" title="84:201	These ratings are averaged to provide an overall score to the seller." ></td>
	<td class="line x" title="85:201	Note that we collect all feedback (both numerical and textual) associated with a seller over the entire lifetime of the seller and we reconstruct each sellers exact feedback profile at the time of each transaction." ></td>
	<td class="line x" title="86:201	4 Econometrics-based Opinion Mining In this section, we describe how we combine econometric techniques with NLP techniques to derive the semantic orientation and strength of the feedback evaluations." ></td>
	<td class="line x" title="87:201	Section 4.1 describes how we structure the textual feedback and Section 4.2 shows how we use econometrics to estimate the polarity and strength of the evaluations." ></td>
	<td class="line x" title="88:201	4.1 Retrieving the Dimensions of Reputation We characterize a merchant using a vector of reputation dimensions X = (X1,X2,,Xn), representing its ability on each of n dimensions." ></td>
	<td class="line x" title="89:201	We assume that each of these n dimensions is expressed by a noun, noun phrase, verb, or a verb phrase chosen from the set of all feedback postings, and that a merchant is evaluated on these n dimensions." ></td>
	<td class="line x" title="90:201	For example, dimension 1 might be shipping, dimension 2 might be packaging and so on." ></td>
	<td class="line x" title="91:201	In our model, each of these dimensions is assigned a numerical score." ></td>
	<td class="line x" title="92:201	Of course, when posting textual feedback, buyers do not assign explicit numeric scores to any dimension." ></td>
	<td class="line x" title="93:201	Rather, they use modifiers (typically adjectives or adverbs) to evaluate the seller along each of these dimensions (we describe how we assign numeric scores to each modifier in Section 4.2)." ></td>
	<td class="line x" title="94:201	Once we have identified the set of all dimensions, we can then parse each of the feedback postings, associate a modifier with each dimension, and represent a feedback posting as an n-dimensional vector  of modifiers." ></td>
	<td class="line x" title="95:201	Example 4.1 Suppose dimension 1 is delivery, dimension 2 is packaging, and dimension 3 is service. The feedback posting I was impressed by the speedy delivery!" ></td>
	<td class="line x" title="96:201	Great service! is then encoded as 1 = [speedy,NULL,great], while the posting The item arrived in awful packaging, and the delivery was slow is encoded as 2 = [slow,awful,NULL]." ></td>
	<td class="line x" title="97:201	a50 Let M = {NULL,1,,M} be the set of modifiers and consider a seller si with p postings in its reputation profile." ></td>
	<td class="line x" title="98:201	We denote with ijk M the modifier that appears in the j-th posting and is used to assess the k-th reputation dimension." ></td>
	<td class="line x" title="99:201	We then structure the merchants feedback as an np matrix M(si) whose rows are the p encoded vectors of modifiers associated with the seller." ></td>
	<td class="line x" title="100:201	We construct M(si) as follows: 1." ></td>
	<td class="line x" title="101:201	Retrieve the postings associated with a merchant." ></td>
	<td class="line x" title="102:201	2." ></td>
	<td class="line x" title="103:201	Parse the postings to identify the dimensions across which the buyer evaluates a seller, keeping4 the nouns, noun phrases, verbs, and verbal phrases as reputation characteristics.5." ></td>
	<td class="line x" title="104:201	3. Retrieve adjectives and adverbs that refer to6 dimensions (Step 2) and construct the  vectors." ></td>
	<td class="line x" title="105:201	We have implemented this algorithm on the feedback postings of each of our sellers." ></td>
	<td class="line x" title="106:201	Our analysis yields 151 unique dimensions, and a total of 142 modifiers (note that the same modifier can be used to evaluate multiple dimensions)." ></td>
	<td class="line x" title="107:201	4.2 Scoring the Dimensions of Reputation As discussed above, the textual feedback profile of merchant si is encoded as a np matrix M(si); the elements of this matrix belong to the set of modifiers M. In our case, we are interested in computing the score a(,d,j) that a modifier   M assigns to the dimension d, when it appears in the j-th posting." ></td>
	<td class="line x" title="108:201	Since buyers tend to read only the first few pages of text-based feedback, we weight higher the influence of recent text postings." ></td>
	<td class="line x" title="109:201	We model this by assuming that K is the number of postings that appear on each page (K = 25 on Amazon.com), and that c is the probability of clicking on the Next link and moving the next page of evaluations.7 This assigns a posting-specific weight rj = cfloorleft jKfloorright/summationtextpq=1 cfloorleft qKfloorright for the jth posting, where j is the rank of the posting, K is the number of postings per page, and p is the total number of postings for the given seller." ></td>
	<td class="line x" title="110:201	Then, we set a(,d,j) = rj a(,d) where a(,d) is the global score that modifier  assigns to dimension d. Finally, since each reputation dimension has potentially a different weight, we use a weight vector w to 4We eliminate all dimensions appearing in the profiles of less than 50 (out of 1078) merchants, since we cannot extract statistically meaningful results for such sparse dimensions 5The technique as described in this paper, considers words like shipping and  delivery as separate dimensions, although they refer to the same real-life dimension." ></td>
	<td class="line x" title="111:201	We can use Latent Dirichlet Allocation (Blei et al. , 2003) to reduce the number of dimensions, but this is outside the scope of this paper." ></td>
	<td class="line x" title="112:201	6To associate the adjectives and adverbs with the correct dimensions, we use the Collins HeadFinder capability of the Stanford NLP Parser." ></td>
	<td class="line x" title="113:201	7We report only results for c = 0.5." ></td>
	<td class="line x" title="114:201	We conducted experiments other values of c as well and the results are similar.419 weight the contribution of each reputation dimension to the overall reputation score (si) of seller si: (si) = rT A(M(si))w (1) where rT = [r1,r2,rp] is the vector of the postingspecific weights and A(M(i)) is a matrix that contains as element the score a(j,dk) where M(si) contains the modifier j in the column of the dimension dk." ></td>
	<td class="line x" title="115:201	If we model the buyers preferences as independently distributed along each dimension and each modifier score a(,dk) also as an independent random variable, then the random variable (si) is a sum of random variables." ></td>
	<td class="line x" title="116:201	Specifically, we have: (si) = Msummationdisplay j=1 nsummationdisplay k=1 (wk a(j,dk))R(j,dk) (2) where R(j,dk) is equal to the sum of the ri weights across all postings in which the modifier j modifies dimension dk." ></td>
	<td class="line x" title="117:201	We can easily compute the R(j,dk) values by simply counting appearances and weighting each appearance using the definition of ri." ></td>
	<td class="line x" title="118:201	The question is, of course, how to estimate the values of wk  a(j,dk), which determine the polarity and intensity of the modifier j modifying the dimension dk." ></td>
	<td class="line x" title="119:201	For this, we observe that the appearance of such modifier-dimension opinion phrases has an effect on the price premiums that a merchant can charge." ></td>
	<td class="line x" title="120:201	Hence, there is a correlation between the reputation scores () of the merchants and the price premiums observed for each transaction." ></td>
	<td class="line x" title="121:201	To discover the level of association, we use regression." ></td>
	<td class="line x" title="122:201	Since we are dealing with panel data, we estimate ordinary-leastsquares (OLS) regression with fixed effects (Greene, 2002), where the dependent variable is the price premium variable, and the independent variables are the reputation scores () of the merchants, together with a few other control variables." ></td>
	<td class="line x" title="123:201	Generally, we estimate models of the form: PricePremiumij = summationdisplay c Xcij +fij +epsilon1ij+ t1 (merchant)ij +t2 (competitor)ij (3) where PricePremiumij is one of the variations of price premium as given in Definition 2.1 for a seller si and product j, c, t1, and t2 are the regressor coefficients, Xc are the control variables, () are the text reputation scores (see Equation 1), fij denotes the fixed effects and epsilon1 is the error term." ></td>
	<td class="line x" title="124:201	In Section 5, we give the details about the control variables and the regression settings." ></td>
	<td class="line x" title="125:201	Interestingly, if we expand the () variables according to Equation 2, we can run the regression using the modifier-dimension pairs as independent variables, whose values are equal to the R(j,dk) values." ></td>
	<td class="line x" title="126:201	After running the regression, the coefficients assigned to each modifier-dimension pair correspond to the value wk  a(j,dk) for each modifier-dimension pair." ></td>
	<td class="line x" title="127:201	Therefore, we can easily estimate in economic terms the value of a particular modifier when used to evaluate a particular dimension." ></td>
	<td class="line x" title="128:201	5 Experimental Evaluation In this section, we first present the experimental settings (Section 5.1), and then we describe the results of our experimental evaluation (Section 5.2)." ></td>
	<td class="line x" title="129:201	5.1 Regression Settings In Equation 3 we presented the general form of the regression for estimating the scores a(j,dk)." ></td>
	<td class="line x" title="130:201	Since we want to eliminate the effect of any other factors that may influence the price premiums, we also use a set of control variables." ></td>
	<td class="line x" title="131:201	After all the control factors are taken into consideration, the modifier scores reflect the additional value of the text opinions." ></td>
	<td class="line x" title="132:201	Specifically, we used as control variables the products price on Amazon, the average star rating of the merchant, the number of merchants past transactions, and the number of sellers for the product." ></td>
	<td class="line x" title="133:201	First, we ran OLS regressions with product-seller fixed effects controlling for unobserved heterogeneity across sellers and products." ></td>
	<td class="line x" title="134:201	These fixed effects control for average product quality and differences in seller characteristics." ></td>
	<td class="line x" title="135:201	We run multiple variations of our model, using different versions of the price premium variable as listed in Definition 2.1." ></td>
	<td class="line x" title="136:201	We also tested variations where we include as independent variable not the individual reputation scores but the difference (merchant)(competitor)." ></td>
	<td class="line x" title="137:201	All regressions yielded qualitatively similar results, so due to space restrictions we only report results for the regressions that include all the control variables and all the text variables; we report results using the price premium as the dependent variable." ></td>
	<td class="line x" title="138:201	Our regressions in this setting contain 107,922 observations, and a total of 547 independent variables." ></td>
	<td class="line x" title="139:201	5.2 Experimental Results Recall of Extraction: The first step of our experimental evaluation is to examine whether the opinion extraction technique of Section 4.1 indeed captures all the reputation characteristics expressed in the feed-420 Dimension Human Recall Computer Recall Product Condition 0.76 0.76 Price 0.91 0.61 Package 0.96 0.66 Overall Experience 0.65 0.55 Delivery Speed 0.96 0.92 Item Description 0.22 0.43 Product Satisfaction 0.68 0.58 Problem Response 0.30 0.37 Customer Service 0.57 0.50 Average 0.66 0.60 Table 1: The recall of our technique compared to the recall of the human annotators back (recall) and whether the dimensions that we capture are accurate (precision)." ></td>
	<td class="line x" title="140:201	To examine the recall question, we used two human annotators." ></td>
	<td class="line x" title="141:201	The annotators read a random sample of 1,000 feedback postings, and identified the reputation dimensions mentioned in the text." ></td>
	<td class="line x" title="142:201	Then, they examined the extracted modifierdimension pairs for each posting and marked whether the modifier-dimension pairs captured the identified real reputation dimensions mentioned in the posting and which pairs were spurious, non-opinion phrases." ></td>
	<td class="line x" title="143:201	Both annotators identified nine reputation dimensions (see Table 1)." ></td>
	<td class="line x" title="144:201	Since the annotators did not agree in all annotations, we computed the average human recall hRecd = agreeddalld for each dimension d, where agreedd is the number of postings for which both annotators identified the reputation dimension d, and alld is the number of postings in which at least one annotator identified the dimension d. Based on the annotations, we computed the recall of our algorithm against each annotator." ></td>
	<td class="line x" title="145:201	We report the average recall for each dimension, together with the human recall in Table 1." ></td>
	<td class="line x" title="146:201	The recall of our technique is only slightly inferior to the performance of humans, indicating that the technique of Section 4.1 extracts the majority of the posted evaluations.8 Interestingly, precision is not an issue in our setting." ></td>
	<td class="line x" title="147:201	In our framework, if an particular modifier-dimension pair is just noise, then it is almost impossible to have a statistically significant correlation with the price premiums." ></td>
	<td class="line x" title="148:201	The noisy opinion phrases are statistically guaranteed to be filtered out by the regression." ></td>
	<td class="line x" title="149:201	Estimating Polarity and Strength: In Table 2, 8In the case of Item Description, where the computer recall was higher than the human recall, our technique identified almost all the phrases of one annotator, but the other annotator had a more liberal interpretation of Item Description dimension and annotated significantly more postings with the dimension Item Description than the other annotator, thus decreasing the human recall." ></td>
	<td class="line x" title="150:201	we present the modifier-dimension pairs (positive and negative) that had the strongest dollar value and were statistically significant across all regressions." ></td>
	<td class="line x" title="151:201	(Due to space issues, we cannot list the values for all pairs)." ></td>
	<td class="line x" title="152:201	These values reflect changes in the merchantss pricing power after taking their average numerical score and level of experience into account, and also highlight the additional the value contained in textbased reputation." ></td>
	<td class="line x" title="153:201	The examples that we list here illustrate that our technique generates a natural ranking of the opinion phrases, inferring the strength of each modifier within the context in which this opinion is evaluated." ></td>
	<td class="line x" title="154:201	This holds true even for misspelled evaluations that would break existing techniques based on annotation or on resources like WordNet." ></td>
	<td class="line x" title="155:201	Furthermore, these values reflect the context in which the opinion is evaluated." ></td>
	<td class="line x" title="156:201	For example, the pair good packaging has a dollar value of -$0.58." ></td>
	<td class="line x" title="157:201	Even though this seems counterintuitive, it actually reflects the nature of an online marketplace where most of the positive evaluations contain superlatives, and a mere good is actually interpreted by the buyers as a lukewarm, slightly negative evaluation." ></td>
	<td class="line x" title="158:201	Existing techniques cannot capture such phenomena." ></td>
	<td class="line x" title="159:201	Price Premiums vs. Ratings: One of the natural comparisons is to examine whether we could reach similar results by just using the average star rating associated with each feedback posting to infer the score of each opinion phrase." ></td>
	<td class="line x" title="160:201	The underlying assumption behind using the ratings is that the review is perfectly summarized by the star rating, and hence the text plays mainly an explanatory role and carries no extra information, given the star rating." ></td>
	<td class="line x" title="161:201	For this, we examined the R2 fit of the regression, with and without the use of the text variables." ></td>
	<td class="line x" title="162:201	Without the use of text variables, the R2 was 0.35, while when using only the text-based regressors, the R2 fit increased to 0.63." ></td>
	<td class="line x" title="163:201	This result clearly indicates that the actual text contains significantly more information than the ratings." ></td>
	<td class="line x" title="164:201	We also experimented with predicting which merchant will make a sale, if they simultaneously sell the same product, based on their listed prices and on their numeric and text reputation." ></td>
	<td class="line x" title="165:201	Our C4.5 classifier (Quinlan, 1992) takes a pair of merchants and decides which of the two will make a sale." ></td>
	<td class="line x" title="166:201	We used as training set the transactions that took place in the first four months and as test set the transactions in the last two months of our data set." ></td>
	<td class="line x" title="167:201	Table 3 summarizes the results for different sets of features used." ></td>
	<td class="line x" title="168:201	The 55%421 Modifier Dimension Dollar Value [wonderful experience] $5.86 [outstanding seller] $5.76 [excellant service] $5.27 [lightning delivery] $4.84 [highly recommended] $4.15 [best seller] $3.80 [perfectly packaged] $3.74 [excellent condition] $3.53 [excellent purchase] $3.22 [excellent seller] $2.70 [excellent communication] $2.38 [perfect item] $1.92 [terrific condition] $1.87 [top quality] $1.67 [awesome service] $1.05 [A+++ seller] $1.03 [great merchant] $0.93 [friendly service] $0.81 [easy service] $0.78 [never received] -$7.56 [defective product] -$6.82 [horible experience] -$6.79 [never sent] -$6.69 [never recieved] -$5.29 [bad experience] -$5.26 [cancelled order] -$5.01 [never responded] -$4.87 [wrong product] -$4.39 [not as advertised] -$3.93 [poor packaging] -$2.92 [late shipping] -$2.89 [wrong item] -$2.50 [not yet received] -$2.35 [still waiting] -$2.25 [wrong address] -$1.54 [never buy] -$1.48 Table 2: The highest scoring opinion phrases, as determined by the product wk a(j,dk)." ></td>
	<td class="line x" title="169:201	accuracy when using only prices as features indicates that customers rarely choose a product based solely on price." ></td>
	<td class="line x" title="170:201	Rather, as indicated by the 74% accuracy, they also consider the reputation of the merchants." ></td>
	<td class="line x" title="171:201	However, the real value of the postings relies on the text and not on the numeric ratings: the accuracy is 87%89% when using the textual reputation variables." ></td>
	<td class="line x" title="172:201	In fact, text subsumes the numeric variables but not vice versa, as indicated by the results in Table 3." ></td>
	<td class="line x" title="173:201	6 Related Work To the best of our knowledge, our work is the first to use economics for measuring the effect of opinions and deriving their polarity and strength in an econometric manner." ></td>
	<td class="line x" title="174:201	A few papers in the past tried to combine text analysis with economics (Das and Chen, 2006; Lewitt and Syverson, 2005), but the text analysis was limited to token counting and did not use Features Accuracy on Test Set Price 55% Price + Numeric Reputation 74% Price + Numeric Reputation 89% + Text Reputation Price + Text Reputation 87% Table 3: Predicting the merchant who makes the sale." ></td>
	<td class="line x" title="175:201	any NLP techniques." ></td>
	<td class="line x" title="176:201	The technique of Section 4.1 is based on existing research in sentiment analysis." ></td>
	<td class="line x" title="177:201	For instance, (Hatzivassiloglou and McKeown, 1997; Nigam and Hurst, 2004) use annotated data to create a supervised learning technique to identify the semantic orientation of adjectives." ></td>
	<td class="line x" title="178:201	We follow the approach by Turney (2002), who note that the semantic orientation of an adjective depends on the noun that it modifies and suggest using adjective-noun or adverb-verb pairs to extract semantic orientation." ></td>
	<td class="line x" title="179:201	However, we do not rely on linguistic resources (Kamps and Marx, 2002) or on search engines (Turney and Littman, 2003) to determine the semantic orientation, but rather rely on econometrics for this task." ></td>
	<td class="line x" title="180:201	Hu and Liu (2004), whose study is the closest to our work, use WordNet to compute the semantic orientation of product evaluations and try to summarize user reviews by extracting the positive and negative evaluations of the different product features." ></td>
	<td class="line x" title="181:201	Similarly, Snyder and Barzilay (2007) decompose an opinion across several dimensions and capture the sentiment across each dimension." ></td>
	<td class="line x" title="182:201	Other work in this area includes (Lee, 2004; Popescu and Etzioni, 2005) which uses text mining in the context product reviews, but none uses the economic context to evaluate the opinions." ></td>
	<td class="line x" title="183:201	7 Conclusion and Further Applications We demonstrated the value of using econometrics for extracting a quantitative interpretation of opinions." ></td>
	<td class="line x" title="184:201	Our technique, additionally, takes into consideration the context within which these opinions are evaluated." ></td>
	<td class="line x" title="185:201	Our experimental results show that our techniques can capture the pragmatic meaning of the expressed opinions using simple economic variables as a form of training data." ></td>
	<td class="line x" title="186:201	The source code with our implementation together with the data set used in this paper are available from http://economining.stern.nyu.edu." ></td>
	<td class="line x" title="187:201	There are many other applications beyond reputation systems." ></td>
	<td class="line x" title="188:201	For example, using sales rank data from Amazon.com, we can examine the effect of product reviews on product sales and detect the weight that422 customers put on different product features; furthermore, we can discover how customer evaluations on individual product features affect product sales and extract the pragmatic meaning of these evaluations." ></td>
	<td class="line x" title="189:201	Another application is the analysis of the effect of news stories on stock prices: we can examine what news topics are important for the stock market and see how the views of different opinion holders and the wording that they use can cause the market to move up or down." ></td>
	<td class="line x" title="190:201	In a slightly different twist, we can analyze news stories and blogs in conjunction with results from prediction markets and extract the pragmatic effect of news and blogs on elections or other political events." ></td>
	<td class="line x" title="191:201	Another research direction is to examine the effect of summarizing product descriptions on product sales: short descriptions reduce the cognitive load of consumers but increase their uncertainty about the underlying product characteristics; a longer description has the opposite effect." ></td>
	<td class="line x" title="192:201	The optimum description length is the one that balances both effects and maximizes product sales." ></td>
	<td class="line x" title="193:201	Similar approaches can improve the state of art in both economics and computational linguistics." ></td>
	<td class="line x" title="194:201	In economics and in social sciences in general, most researchers handle textual data manually or with simplistic token counting techniques; in the worst case they ignore text data altogether." ></td>
	<td class="line x" title="195:201	In computational linguistics, researchers often rely on human annotators to generate training data, a laborious and errorprone task." ></td>
	<td class="line x" title="196:201	We believe that cross-fertilization of ideas between the fields of computational linguistics and econometrics can be beneficial for both fields." ></td>
	<td class="line x" title="197:201	Acknowledgments The authors would like to thank Elena Filatova for the useful discussions and the pointers to related literature." ></td>
	<td class="line x" title="198:201	We also thank Sanjeev Dewan, Alok Gupta, Bin Gu, and seminar participants at Carnegie Mellon University, Columbia University, Microsoft Research, New York University, Polytechnic University, and University of Florida for their comments and feedback." ></td>
	<td class="line x" title="199:201	We thank Rhong Zheng for assistance in data collection." ></td>
	<td class="line x" title="200:201	This work was partially supported by a Microsoft Live Labs Search Award, a Microsoft Virtual Earth Award, and by NSF grants IIS-0643847 and IIS-0643846." ></td>
	<td class="line x" title="201:201	Any opinions, findings, and conclusions expressed in this material are those of the authors and do not necessarily reflect the views of the Microsoft Corporation or of the National Science Foundation." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1055
Structured Models for Fine-to-Coarse Sentiment Analysis
McDonald, Ryan;Hannan, Kerry;Neylon, Tyler;Wells, Mike;Reynar, Jeffrey C.;"></td>
	<td class="line x" title="1:215	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 432439, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:215	c2007 Association for Computational Linguistics Structured Models for Fine-to-Coarse Sentiment Analysis Ryan McDonald Kerry Hannan Tyler Neylon Mike Wells Jeff Reynar Google, Inc. 76 Ninth Avenue New York, NY 10011 Contact email: ryanmcd@google.com Abstract In this paper we investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity." ></td>
	<td class="line x" title="3:215	Inference in the model is based on standard sequence classification techniques using constrained Viterbi to ensure consistent solutions." ></td>
	<td class="line x" title="4:215	The primary advantage of such a model is that it allows classification decisions from one level in the text to influence decisions at another." ></td>
	<td class="line x" title="5:215	Experiments show that this method can significantly reduce classification error relative to models trained in isolation." ></td>
	<td class="line x" title="6:215	1 Introduction Extractingsentimentfromtextisachallengingproblem with applications throughout Natural Language Processing and Information Retrieval." ></td>
	<td class="line oc" title="7:215	Previous workonsentimentanalysishascoveredawiderange of tasks, including polarity classification (Pang et al. , 2002; Turney, 2002), opinion extraction (Pang and Lee, 2004), and opinion source assignment (Choi et al. , 2005; Choi et al. , 2006)." ></td>
	<td class="line oc" title="8:215	Furthermore, these systems have tackled the problem at different levels of granularity, from the document level (Pang et al. , 2002), sentence level (Pang and Lee, 2004; Mao and Lebanon, 2006), phrase level (Turney, 2002; Choi et al. , 2005), as well as the speaker level in debates (Thomas et al. , 2006)." ></td>
	<td class="line x" title="9:215	The ability to classify sentiment on multiple levels is importantsincedifferentapplicationshavedifferentneeds." ></td>
	<td class="line x" title="10:215	For example, a summarization system for product reviews might require polarity classification at the sentence or phrase level; a question answering system would most likely require the sentiment of paragraphs; and a system that determines which articles from an online news source are editorial in nature would require a document level analysis." ></td>
	<td class="line x" title="11:215	This work focuses on models that jointly classify sentimentonmultiplelevelsofgranularity." ></td>
	<td class="line x" title="12:215	Consider the following example, This is the first Mp3 player that I have used I thought it sounded great  After only a few weeks, it started having trouble with the earphone connection  I wont be buying another." ></td>
	<td class="line x" title="13:215	Mp3 player review from Amazon.com Thisexcerptexpressesanoverallnegativeopinionof the product being reviewed." ></td>
	<td class="line x" title="14:215	However, not all parts of the review are negative." ></td>
	<td class="line x" title="15:215	The first sentence merely provides some context on the reviewers experience with such devices and the second sentence indicates that, at least in one regard, the product performed well." ></td>
	<td class="line x" title="16:215	We call the problem of identifying the sentiment of the document and of all its subcomponents, whether at the paragraph, sentence, phrase or word level, fine-to-coarse sentiment analysis." ></td>
	<td class="line x" title="17:215	The simplest approach to fine-to-coarse sentiment analysis would be to create a separate system for each level of granularity." ></td>
	<td class="line x" title="18:215	There are, however, obvious advantages to building a single model that classifies each level in tandem." ></td>
	<td class="line x" title="19:215	Consider the sentence, My 11 year old daughter has also been using it and it is a lot harder than it looks." ></td>
	<td class="line x" title="20:215	Inisolation, thissentenceappearstoconveynegative sentiment." ></td>
	<td class="line x" title="21:215	However, it is part of a favorable review 432 for a piece of fitness equipment, where hard essentially means good workout." ></td>
	<td class="line x" title="22:215	In this domain, hards sentiment can only be determined in context (i.e. , hard toassembleversusa hard workout)." ></td>
	<td class="line x" title="23:215	Iftheclassifierknewtheoverallsentimentofadocument, then disambiguating such cases would be easier." ></td>
	<td class="line x" title="24:215	Conversely, document level analysis can benefit from finer level classification by taking advantage of common discourse cues, such as the last sentence being a reliable indicator for overall sentiment in reviews." ></td>
	<td class="line x" title="25:215	Furthermore, during training, the model will not need to modify its parameters to explain phenomena like the typically positive word great appearing in a negative text (as is the case above)." ></td>
	<td class="line x" title="26:215	The model can also avoid overfitting to features derived from neutral or objective sentences." ></td>
	<td class="line x" title="27:215	In fact, it has already been established that sentence level classification can improve document level analysis (Pang and Lee, 2004)." ></td>
	<td class="line x" title="28:215	This line of reasoning suggests that a cascaded approach would also be insufficient." ></td>
	<td class="line x" title="29:215	Valuable information is passed in both directions, which means any model of fine-to-coarse analysis should account for this." ></td>
	<td class="line x" title="30:215	In Section 2 we describe a simple structured model that jointly learns and infers sentiment on different levels of granularity." ></td>
	<td class="line x" title="31:215	In particular, we reduce the problem of joint sentence and document level analysis to a sequential classification problem using constrained Viterbi inference." ></td>
	<td class="line x" title="32:215	Extensions to the model that move beyond just two-levels of analysis are also presented." ></td>
	<td class="line x" title="33:215	In Section 3 an empirical evaluation of the model is given that shows significant gains in accuracy over both single level classifiers and cascaded systems." ></td>
	<td class="line x" title="34:215	1.1 Related Work The models in this work fall into the broad class of globalstructuredmodels, whicharetypicallytrained with structured learning algorithms." ></td>
	<td class="line x" title="35:215	Hidden Markov models (Rabiner, 1989) are one of the earliest structured learning algorithms, which have recently been followedbydiscriminativelearningapproachessuch as conditional random fields (CRFs) (Lafferty et al. , 2001; Sutton and McCallum, 2006), the structured perceptron (Collins, 2002) and its large-margin variants (Taskar et al. , 2003; Tsochantaridis et al. , 2004; McDonald et al. , 2005; Daume III et al. , 2006)." ></td>
	<td class="line x" title="36:215	These algorithms are usually applied to sequential labeling or chunking, but have also been applied to parsing (Taskar et al. , 2004; McDonald et al. , 2005), machine translation (Liang et al. , 2006) and summarization (Daume III et al. , 2006)." ></td>
	<td class="line x" title="37:215	Structured models have previously been used for sentiment analysis." ></td>
	<td class="line x" title="38:215	Choi et al.(2005, 2006) use CRFs to learn a global sequence model to classify and assign sources to opinions." ></td>
	<td class="line x" title="40:215	Mao and Lebanon (2006) used a sequential CRF regression model to measure polarity on the sentence level in order to determine the sentiment flow of authors in reviews." ></td>
	<td class="line x" title="41:215	Here we show that fine-to-coarse models of sentiment can often be reduced to the sequential case." ></td>
	<td class="line x" title="42:215	Cascaded models for fine-to-coarse sentiment analysis were studied by Pang and Lee (2004)." ></td>
	<td class="line x" title="43:215	In that work an initial model classified each sentence as being subjective or objective using a global mincut inference algorithm that considered local labeling consistencies." ></td>
	<td class="line x" title="44:215	The top subjective sentences are then input into a standard document level polarity classifier with improved results." ></td>
	<td class="line x" title="45:215	The current work differs from that in Pang and Lee through the use of a single joint structured model for both sentence and document level analysis." ></td>
	<td class="line x" title="46:215	Many problems in natural language processing can be improved by learning and/or predicting multiple outputs jointly." ></td>
	<td class="line x" title="47:215	This includes parsing and relation extraction (Miller et al. , 2000), entity labeling and relation extraction (Roth and Yih, 2004), and part-of-speech tagging and chunking (Sutton et al. , 2004)." ></td>
	<td class="line x" title="48:215	One interesting work on sentiment analysis isthatofPopescuandEtzioni(2005)whichattempts to classify the sentiment of phrases with respect to possible product features." ></td>
	<td class="line x" title="49:215	To do this an iterative algorithm is used that attempts to globally maximize the classification of all phrases while satisfying local consistency constraints." ></td>
	<td class="line x" title="50:215	2 Structured Model In this section we present a structured model for fine-to-coarse sentiment analysis." ></td>
	<td class="line x" title="51:215	We start by examining the simple case with two-levels of granularity  the sentence and document  and show that the problem can be reduced to sequential classification with constrained inference." ></td>
	<td class="line x" title="52:215	We then discuss the feature space and give an algorithm for learning the parameters based on large-margin structured learning." ></td>
	<td class="line x" title="53:215	433 Extensions to the model are also examined." ></td>
	<td class="line x" title="54:215	2.1 A Sentence-Document Model Let Y(d) be a discrete set of sentiment labels at the document level and Y(s) be a discrete set of sentiment labels at the sentence level." ></td>
	<td class="line x" title="55:215	As input a system is given a document containing sentences s = s1,,sn and must produce sentiment labels for the document, yd  Y(d), and each individual sentence, ys = ys1,,ysn, where ysi  Y(s)  1  i  n. Define y = (yd,ys) = (yd,ys1,,ysn) as the joint labeling of the document and sentences." ></td>
	<td class="line x" title="56:215	For instance, in Pang and Lee (2004), yd would be the polarity of the document and ysi would indicate whether sentence si is subjective or objective." ></td>
	<td class="line x" title="57:215	The models presented here are compatible with arbitrary sets of discrete output labels." ></td>
	<td class="line x" title="58:215	Figure 1 presents a model for jointly classifying the sentiment of both the sentences and the document." ></td>
	<td class="line x" title="59:215	In this undirected graphical model, the label of each sentence is dependent on the labels of its neighbouring sentences plus the label of the document." ></td>
	<td class="line x" title="60:215	The label of the document is dependent on the label of every sentence." ></td>
	<td class="line x" title="61:215	Note that the edges between the input (each sentence) and the output labels are not solid, indicating that they are given as input and are not being modeled." ></td>
	<td class="line x" title="62:215	The fact that the sentiment of sentences is dependent not only on the local sentiment of other sentences, but also the global document sentiment  and vice versa  allows the model to directly capture the importance of classification decisions across levels in fine-tocoarse sentiment analysis." ></td>
	<td class="line x" title="63:215	The local dependencies between sentiment labels on sentences is similar to the work of Pang and Lee (2004) where soft local consistency constraints were created between every sentence in adocument and inference wassolved using a min-cut algorithm." ></td>
	<td class="line x" title="64:215	However, jointly modeling the document label and allowing for non-binary labelscomplicatesmin-cut stylesolutionsasinference becomes intractable." ></td>
	<td class="line x" title="65:215	Learning and inference in undirected graphical models is a well studied problem in machine learning and NLP." ></td>
	<td class="line x" title="66:215	For example, CRFs define the probability over the labels conditioned on the input using the property that the joint probability distribution over the labels factors over clique potentials in undirected graphical models (Lafferty et al. , 2001)." ></td>
	<td class="line x" title="67:215	Figure 1: Sentence and document level model." ></td>
	<td class="line x" title="68:215	In this work we will use structured linear classifiers (Collins, 2002)." ></td>
	<td class="line x" title="69:215	We denote the score of a labeling y for an input s as score(y,s) and define this score as the sum of scores over each clique, score(y,s) = score((yd,ys),s) = score((yd,ys1,,ysn),s) = nsummationdisplay i=2 score(yd,ysi1,ysi,s) where each clique score is a linear combination of features and their weights, score(yd,ysi1,ysi,s) = wf(yd,ysi1,ysi,s) (1) and f is a high dimensional feature representation of the clique and w a corresponding weight vector." ></td>
	<td class="line x" title="70:215	Note that s is included in each score since it is given as input and can always be conditioned on." ></td>
	<td class="line x" title="71:215	Ingeneral, inferenceinundirectedgraphicalmodels is intractable." ></td>
	<td class="line x" title="72:215	However, for the common case of sequences(a.k.a.linear-chain models)theViterbialgorithm can be used (Rabiner, 1989; Lafferty et al. , 2001)." ></td>
	<td class="line x" title="73:215	Fortunately there is a simple technique that reduces inference in the above model to sequence classification with a constrained version of Viterbi." ></td>
	<td class="line x" title="74:215	2.1.1 Inference as Sequential Labeling The inference problem is to find the highest scoring labeling y for an input s, i.e., argmax y score(y,s) If the document label yd is fixed, then inference in the model from Figure 1 reduces to the sequential case." ></td>
	<td class="line x" title="75:215	This is because the search space is only over the sentence labels ysi, whose graphical structure forms a chain." ></td>
	<td class="line x" title="76:215	Thus the problem of finding the 434 Input: s = s1,,sn 1." ></td>
	<td class="line x" title="77:215	y = null 2." ></td>
	<td class="line x" title="78:215	for each yd  Y(d) 3." ></td>
	<td class="line x" title="79:215	ys = argmaxys score((yd,ys),s) 4." ></td>
	<td class="line x" title="80:215	yprime = (yd,ys) 5." ></td>
	<td class="line x" title="81:215	if score(yprime,s) > score(y,s) or y = null 6." ></td>
	<td class="line x" title="82:215	y = yprime 7." ></td>
	<td class="line x" title="83:215	return y Figure 2: Inference algorithm for model in Figure 1." ></td>
	<td class="line x" title="84:215	The argmax in line 3 can be solved using Viterbis algorithm since yd is fixed." ></td>
	<td class="line x" title="85:215	highest scoring sentiment labels for all sentences, given a particular document label yd, can be solved efficiently using Viterbis algorithm." ></td>
	<td class="line x" title="86:215	The general inference problem can then be solved by iterating over each possible yd, finding ys maximizing score((yd,ys),s) and keeping the single best y = (yd,ys)." ></td>
	<td class="line x" title="87:215	This algorithm is outlined in Figure 2 and has a runtime of O(|Y(d)||Y(s)|2n), due to running Viterbi |Y(d)| times over a label space of size |Y(s)|." ></td>
	<td class="line x" title="88:215	The algorithm can be extended to produce exact k-best lists." ></td>
	<td class="line x" title="89:215	This is achieved by using k-best Viterbi techniques to return the k-best global labelings for each document label in line 3." ></td>
	<td class="line x" title="90:215	Merging these sets will produce the final k-best list." ></td>
	<td class="line x" title="91:215	It is possible to view the inference algorithm in Figure 2 as a constrained Viterbi search since it is equivalent to flattening the model in Figure 1 to a sequential model with sentence labels from the set Y(s)  Y(d)." ></td>
	<td class="line x" title="92:215	The resulting Viterbi search would then need to be constrained to ensure consistent solutions, i.e., the label assignments agree on the document label over all sentences." ></td>
	<td class="line x" title="93:215	If viewed this way, it is also possible to run a constrained forwardbackward algorithm and learn the parameters for CRFs as well." ></td>
	<td class="line x" title="94:215	2.1.2 Feature Space In this section we define the feature representation for each clique, f(yd,ysi1,ysi,s)." ></td>
	<td class="line x" title="95:215	Assume that each sentence si is represented by a set of binary predicates P(si)." ></td>
	<td class="line x" title="96:215	This set can contain any predicate over the input s, but for the present purposes it will include all the unigram, bigram and trigrams in the sentence si conjoined with their part-of-speech (obtained from an automatic classifier)." ></td>
	<td class="line x" title="97:215	Back-offs of each predicate are also included where one or more word is discarded." ></td>
	<td class="line x" title="98:215	For instance, if P(si) contains the predicate a:DT great:JJ product:NN, then it would also have the predicates a:DT great:JJ *:NN, a:DT *:JJ product:NN, *:DT great:JJ product:NN, a:DT *:JJ *:NN, etc. Each predicate, p, is then conjoined with the label information to construct a binary feature." ></td>
	<td class="line x" title="99:215	For example, if the sentence label set is Y(s) = {subj,obj} and the document set is Y(d) = {pos,neg}, then the system might contain the following feature, f(j)(yd,ysi1,ysi,s) =     1 if p  P(si) and ysi1 = obj and ysi = subj and yd = neg 0 otherwise Where f(j) is the jth dimension of the feature space." ></td>
	<td class="line x" title="100:215	For each feature, a set of back-off features are included that only consider the document label yd, the current sentence label ysi, the current sentence and document label ysi and yd, and the current and previous sentence labels ysi and ysi1." ></td>
	<td class="line x" title="101:215	Note that through these back-off features the joint models feature set will subsume the feature set of any individual level model." ></td>
	<td class="line x" title="102:215	Only features observed in the training data were considered." ></td>
	<td class="line x" title="103:215	Depending on the data set, the dimension of the feature vector f ranged from 350K to 500K." ></td>
	<td class="line x" title="104:215	Though the feature vectors can be sparse, the feature weights will be learned using large-margin techniques that are well known to be robust to large and sparse feature representations." ></td>
	<td class="line x" title="105:215	2.1.3 Training the Model Let Y = Y(d)  Y(s)n be the set of all valid sentence-document labelings for an input s. The weights, w, are set using the MIRA learning algorithm, which is an inference based online largemargin learning technique (Crammer and Singer, 2003; McDonald et al. , 2005)." ></td>
	<td class="line x" title="106:215	An advantage of this algorithm is that it relies only on inference to learn the weight vector (see Section 2.1.1)." ></td>
	<td class="line x" title="107:215	MIRA has been shown to provide state-of-the-art accuracy for many language processing tasks including parsing, chunking and entity extraction (McDonald, 2006)." ></td>
	<td class="line x" title="108:215	The basic algorithm is outlined in Figure 3." ></td>
	<td class="line x" title="109:215	The algorithm works by considering a single training instance during each iteration." ></td>
	<td class="line x" title="110:215	The weight vector w is updated in line 4 through a quadratic programming problem." ></td>
	<td class="line x" title="111:215	This update modifies the weight vector so 435 Training data: T = {(yt,st)}Tt=1 1." ></td>
	<td class="line x" title="112:215	w(0) = 0; i = 0 2." ></td>
	<td class="line x" title="113:215	for n : 1N 3." ></td>
	<td class="line x" title="114:215	for t : 1T 4." ></td>
	<td class="line x" title="115:215	w(i+1) = argminw*  w*w(i)   s.t. score(yt,st) score(yprime,s)  L(yt,yprime) relative to w* yprime  C  Y, where |C| = k 5." ></td>
	<td class="line x" title="116:215	i = i + 1 6." ></td>
	<td class="line x" title="117:215	return w(NT) Figure 3: MIRA learning algorithm." ></td>
	<td class="line x" title="118:215	that the score of the correct labeling is larger than the score of every labeling in a constraint set C with a margin proportional to the loss." ></td>
	<td class="line x" title="119:215	The constraint set C can be chosen arbitrarily, but it is usually taken to be the k labelings that have the highest score under the old weight vector w(i) (McDonald et al. , 2005)." ></td>
	<td class="line x" title="120:215	In this manner, the learning algorithm can update its parameters relative to those labelings closest to the decisionboundary." ></td>
	<td class="line x" title="121:215	Ofalltheweightvectorsthatsatisfy these constraints, MIRA chooses the one that is as close as possible to the previous weight vector in order to retain information about previous updates." ></td>
	<td class="line x" title="122:215	The loss function L(y,yprime) is a positive real valued function and is equal to zero when y = yprime." ></td>
	<td class="line x" title="123:215	This function is task specific and is usually the hamming loss for sequence classification problems (Taskar et al. , 2003)." ></td>
	<td class="line x" title="124:215	Experiments with different loss functions forthe jointsentence-document modelon adevelopment data set indicated that the hamming loss over sentence labels multiplied by the 0-1 loss over document labels worked best." ></td>
	<td class="line x" title="125:215	An important modification that was made to the learning algorithm deals with how the k constraints arechosenfortheoptimization." ></td>
	<td class="line x" title="126:215	Typicallytheseconstraints are the k highest scoring labelings under the current weight vector." ></td>
	<td class="line x" title="127:215	However, early experiments showed that the model quickly learned to discard any labeling with an incorrect document label for the instances in the training set." ></td>
	<td class="line x" title="128:215	As a result, the constraints were dominated by labelings that only differed over sentence labels." ></td>
	<td class="line x" title="129:215	This did not allow the algorithm adequate opportunity to set parameters relative to incorrect document labeling decisions." ></td>
	<td class="line x" title="130:215	To combat this, k was divided by the number of document labels, to get a new value kprime." ></td>
	<td class="line x" title="131:215	For each document label, the kprime highest scoring labelings were Figure 4: An extension to the model from Figure 1 incorporating paragraph level analysis." ></td>
	<td class="line x" title="132:215	extracted." ></td>
	<td class="line x" title="133:215	Each of these sets were then combined to produce the final constraint set." ></td>
	<td class="line x" title="134:215	This allowed constraints to be equally distributed amongst different document labels." ></td>
	<td class="line x" title="135:215	Based on performance on the development data set the number of training iterations was set to N = 5 and the number of constraints to k = 10." ></td>
	<td class="line x" title="136:215	Weight averaging was also employed (Collins, 2002), which helped improve performance." ></td>
	<td class="line x" title="137:215	2.2 Beyond Two-Level Models To this point, we have focused solely on a model for two-level fine-to-coarse sentiment analysis not only for simplicity, but because the experiments in Section 3 deal exclusively with this scenario." ></td>
	<td class="line x" title="138:215	In this section, we briefly discuss possible extensions for more complex situations." ></td>
	<td class="line x" title="139:215	For example, longer documents might benefit from an analysis on the paragraph level as well as the sentence and document levels." ></td>
	<td class="line x" title="140:215	One possible model for this case is given in Figure 4, which essentially inserts an additional layer between the sentence and document level from the original model." ></td>
	<td class="line x" title="141:215	Sentence level analysis is dependent on neighbouring sentences as well as the paragraph level analysis, and the paragraph analysis is dependent on each of the sentences within it, the neighbouring paragraphs, and the document level analysis." ></td>
	<td class="line x" title="142:215	This can be extended to an arbitrary level of fine-to-coarse sentiment analysis by simply inserting new layers in this fashion to create more complex hierarchical models." ></td>
	<td class="line x" title="143:215	The advantage of using hierarchical models of this form is that they are nested, which keeps inference tractable." ></td>
	<td class="line x" title="144:215	Observe that each pair of adjacent levels in the model is equivalent to the original model from Figure 1." ></td>
	<td class="line x" title="145:215	As a result, the scores of the every label at each node in the graph can be calculated with a straight-forward bottom-up dynamic programming algorithm." ></td>
	<td class="line x" title="146:215	Details are omitted 436 Sentence Stats Document Stats Pos Neg Neu Tot Pos Neg Tot Car 472 443 264 1179 98 80 178 Fit 568 635 371 1574 92 97 189 Mp3 485 464 214 1163 98 89 187 Tot 1525 1542 849 3916 288 266 554 Table 1: Data statistics for corpus." ></td>
	<td class="line x" title="147:215	Pos = positive polarity, Neg = negative polarity, Neu = no polarity." ></td>
	<td class="line x" title="148:215	for space reasons." ></td>
	<td class="line x" title="149:215	Other models are possible where dependencies occur across non-neighbouring levels, e.g., by inserting edges between the sentence level nodes and the document level node." ></td>
	<td class="line x" title="150:215	In the general case, inference is exponential in the size of each clique." ></td>
	<td class="line x" title="151:215	Both the models in Figure 1 and Figure 4 have maximum clique sizes of three." ></td>
	<td class="line x" title="152:215	3 Experiments 3.1 Data To test the model we compiled a corpus of 600 online product reviews from three domains: car seats forchildren, fitnessequipment, andMp3players." ></td>
	<td class="line x" title="153:215	Of the original 600 reviews that were gathered, we discarded duplicate reviews, reviews with insufficient text, and spam." ></td>
	<td class="line x" title="154:215	All reviews were labeled by onlinecustomersashavingapositiveornegativepolarity on the document level, i.e., Y(d) = {pos,neg}." ></td>
	<td class="line x" title="155:215	Each review was then split into sentences and every sentence annotated by a single annotator as either being positive, negative or neutral, i.e., Y(s) = {pos,neg,neu}." ></td>
	<td class="line x" title="156:215	Data statistics for the corpus are given in Table 1." ></td>
	<td class="line x" title="157:215	All sentences were annotated based on their context within the document." ></td>
	<td class="line x" title="158:215	Sentences were annotated as neutral if they conveyed no sentiment or had indeterminate sentiment from their context." ></td>
	<td class="line x" title="159:215	Many neutral sentences pertain to the circumstances under which the product was purchased." ></td>
	<td class="line x" title="160:215	A common class of sentences were those containing product features." ></td>
	<td class="line x" title="161:215	These sentences were annotated as having positive or negative polarity if the context supported it." ></td>
	<td class="line x" title="162:215	This could include punctuation such as exclamation points, smiley/frowny faces, question marks, etc. The supporting evidence could also come from another sentence, e.g., I love it." ></td>
	<td class="line x" title="163:215	It has 64Mb of memory and comes with a set of earphones." ></td>
	<td class="line x" title="164:215	3.2 Results Three baseline systems were created,  Document-Classifier is a classifier that learns to predict the document label only." ></td>
	<td class="line x" title="165:215	 Sentence-Classifier is a classifier that learns to predict sentence labels in isolation of one another, i.e., without consideration for either the document or neighbouring sentences sentiment." ></td>
	<td class="line x" title="166:215	 Sentence-Structured is another sentence classifier, but this classifier uses a sequential chain model to learn and classify sentences." ></td>
	<td class="line x" title="167:215	The thirdbaselineisessentiallythemodelfromFigure1withoutthetopleveldocumentnode." ></td>
	<td class="line x" title="168:215	This baselinewillhelptogagetheempiricalgainsof the different components of the joint structured model on sentence level classification." ></td>
	<td class="line x" title="169:215	The model described in Section 2 will be called Joint-Structured." ></td>
	<td class="line x" title="170:215	All models use the same basic predicate space: unigram, bigram, trigram conjoined with part-of-speech, plus back-offs of these (see Section 2.1.2 for more)." ></td>
	<td class="line x" title="171:215	However, due to the structure of the model and its label space, the feature space of each might be different, e.g., the document classifier will only conjoin predicates with the document label to create the feature set." ></td>
	<td class="line x" title="172:215	All models are trained using the MIRA learning algorithm." ></td>
	<td class="line x" title="173:215	Results for each model are given in the first four rows of Table 2." ></td>
	<td class="line x" title="174:215	These results were gathered using 10-fold cross validation with one fold for development and the other nine folds for evaluation." ></td>
	<td class="line x" title="175:215	This table shows that classifying sentences in isolation from one another is inferior to accounting for a more global context." ></td>
	<td class="line x" title="176:215	A significant increase in performance can be obtained when labeling decisions between sentences are modeled (Sentence-Structured)." ></td>
	<td class="line x" title="177:215	More interestingly, even further gains can be had when document level decisions are modeled (JointStructured)." ></td>
	<td class="line x" title="178:215	In many cases, these improvements are highly statistically significant." ></td>
	<td class="line x" title="179:215	On the document level, performance can also be improved by incorporating sentence level decisions  though these improvements are not consistent." ></td>
	<td class="line x" title="180:215	This inconsistency may be a result of the model overfitting on the small set of training data." ></td>
	<td class="line x" title="181:215	We 437 suspect this because the document level error rate on the Mp3 training set converges to zero much more rapidly for the Joint-Structured model than the Document-Classifier." ></td>
	<td class="line x" title="182:215	This suggests that the JointStructured model might be relying too much on the sentence level sentiment features  in order to minimize its error rate  instead of distributing the weights across all features more evenly." ></td>
	<td class="line x" title="183:215	One interesting application of sentence level sentiment analysis is summarizing product reviews on retail websites like Amazon.com or review aggregators like Yelp.com." ></td>
	<td class="line x" title="184:215	In this setting the correct polarity of a document is often known, but we wish to label sentiment on the sentence or phrase level to aid in generating a cohesive and informative summary." ></td>
	<td class="line x" title="185:215	The joint model can be used to classify sentences in this setting by constraining inference to the known fixed document label for a review." ></td>
	<td class="line x" title="186:215	If this is done, then sentiment accuracy on the sentence level increases substantially from 62.6% to 70.3%." ></td>
	<td class="line x" title="187:215	Finally we should note that experiments using CRFs to train the structured models and logistic regression to train the local models yielded similar results to those in Table 2." ></td>
	<td class="line x" title="188:215	3.2.1 Cascaded Models Another approach to fine-to-coarse sentiment analysis is to use a cascaded system." ></td>
	<td class="line x" title="189:215	In such a system, a sentence level classifier might first be run on the data, and then the results input into a document level classifier  or vice-versa.1 Two cascaded systems were built." ></td>
	<td class="line x" title="190:215	The first uses the SentenceStructured classifier to classify all the sentences from a review, then passes this information to the document classifier as input." ></td>
	<td class="line x" title="191:215	In particular, for every predicate in the original document classifier, an additional predicate that specifies the polarity of the sentence in which this predicate occurred was created." ></td>
	<td class="line x" title="192:215	The second cascaded system uses the document classifier to determine the global polarity, then passes this information as input into the SentenceStructured model, constructing predicates in a similar manner." ></td>
	<td class="line x" title="193:215	The results for these two systems can be seen in the last two rows of Table 2." ></td>
	<td class="line x" title="194:215	In both cases there 1Alternatively, decisions from the sentence classifier can guide which input is seen by the document level classifier (Pang and Lee, 2004)." ></td>
	<td class="line x" title="195:215	is a slight improvement in performance suggesting that an iterative approach might be beneficial." ></td>
	<td class="line x" title="196:215	That is, a system could start by classifying documents, use the document information to classify sentences, use the sentence information to classify documents, and repeat until convergence." ></td>
	<td class="line x" title="197:215	However, experiments showedthatthisdidnotimproveaccuracyoverasingle iteration and often hurt performance." ></td>
	<td class="line x" title="198:215	Improvements from the cascaded models are far less consistent than those given from the joint structure model." ></td>
	<td class="line x" title="199:215	This is because decisions in the cascaded system are passed to the next layer as the gold standard at test time, which results in errors from the first classifier propagating to errors in the second." ></td>
	<td class="line x" title="200:215	This could be improved by passing a lattice of possibilities from the first classifier to the second withcorrespondingconfidences." ></td>
	<td class="line x" title="201:215	However, solutions such as these are really just approximations of the joint structured model that was presented here." ></td>
	<td class="line x" title="202:215	4 Future Work One important extension to this work is to augment the models for partially labeled data." ></td>
	<td class="line x" title="203:215	It is realistic to imagine a training set where many examples do not have every level of sentiment annotated." ></td>
	<td class="line x" title="204:215	For example, there are thousands of online product reviews with labeled document sentiment, but a much smaller amount where sentences are also labeled." ></td>
	<td class="line x" title="205:215	Work on learning with hidden variables can be used for both CRFs (Quattoni et al. , 2004) and for inference based learning algorithms like those used in this work (Liang et al. , 2006)." ></td>
	<td class="line x" title="206:215	Another area of future work is to empirically investigate the use of these models on longer documents that require more levels of sentiment analysis than product reviews." ></td>
	<td class="line x" title="207:215	In particular, the relative position of a phrase to a contrastive discourse connective or a cue phrase like in conclusion or to summarize may lead to improved performance since higher level classifications can learn to weigh information passed from these lower level components more heavily." ></td>
	<td class="line x" title="208:215	5 Discussion In this paper we have investigated the use of a global structured model that learns to predict sentiment on different levels of granularity for a text." ></td>
	<td class="line x" title="209:215	We de438 Sentence Accuracy Document Accuracy Car Fit Mp3 Total Car Fit Mp3 Total Document-Classifier 72.8 80.1 87.2 80.3 Sentence-Classifier 54.8 56.8 49.4 53.1 Sentence-Structured 60.5 61.4 55.7 58.8 Joint-Structured 63.5 65.2 60.1 62.6 81.5 81.9 85.0 82.8 Cascaded Sentence  Document 60.5 61.4 55.7 58.8 75.9 80.7 86.1 81.1 Cascaded Document  Sentence 59.7 61.0 58.3 59.5 72.8 80.1 87.2 80.3 Table 2: Fine-to-coarse sentiment accuracy." ></td>
	<td class="line x" title="210:215	Significance calculated using McNemars test between top two performing systems." ></td>
	<td class="line x" title="211:215	Statistically significant p < 0.05." ></td>
	<td class="line x" title="212:215	Statistically significant p < 0.005." ></td>
	<td class="line x" title="213:215	scribed a simple model for sentence-document analysis and showed that inference in it is tractable." ></td>
	<td class="line x" title="214:215	Experiments show that this model obtains higher accuracy than classifiers trained in isolation as well as cascaded systems that pass information from one leveltoanotherattesttime." ></td>
	<td class="line x" title="215:215	Furthermore, extensions to the sentence-document model were discussed and it was argued that a nested hierarchical structure would be beneficial since it would allow for efficient inference algorithms." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1056
Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification
Blitzer, John;Dredze, Mark;Pereira, Fernando C. N.;"></td>
	<td class="line x" title="1:215	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440447, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:215	c2007 Association for Computational Linguistics Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification John Blitzer Mark Dredze Department of Computer and Information Science University of Pennsylvania {blitzer|mdredze|pereria@cis.upenn.edu} Fernando Pereira Abstract Automatic sentiment classification has been extensively studied and applied in recent years." ></td>
	<td class="line x" title="3:215	However, sentiment is expressed differently in different domains, and annotating corporaforeverypossibledomainofinterest is impractical." ></td>
	<td class="line x" title="4:215	We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products." ></td>
	<td class="line x" title="5:215	First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline." ></td>
	<td class="line x" title="6:215	Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another." ></td>
	<td class="line x" title="7:215	This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains." ></td>
	<td class="line oc" title="8:215	1 Introduction Sentiment detection and classification has received considerable attention recently (Pang et al. , 2002; Turney, 2002; Goldberg and Zhu, 2004)." ></td>
	<td class="line x" title="9:215	While movie reviews have been the most studied domain, sentiment analysis has extended to a number of new domains, ranging from stock message boards to congressional floor debates (Das and Chen, 2001; Thomas et al. , 2006)." ></td>
	<td class="line x" title="10:215	Research results have been deployed industrially in systems that gauge market reaction and summarize opinion from Web pages, discussion boards, and blogs." ></td>
	<td class="line x" title="11:215	With such widely-varying domains, researchers and engineers who build sentiment classification systems need to collect and curate data for each new domain they encounter." ></td>
	<td class="line x" title="12:215	Even in the case of market analysis, if automatic sentiment classification were to be used across a wide range of domains, the effort to annotate corpora for each domain may become prohibitive, especially since product features change over time." ></td>
	<td class="line x" title="13:215	We envision a scenario in which developers annotate corpora for a small number of domains, train classifiers on those corpora, and then apply them to other similar corpora." ></td>
	<td class="line x" title="14:215	However, this approach raises two important questions." ></td>
	<td class="line x" title="15:215	First, it is well known that trained classifiers lose accuracy when the test data distribution is significantly differentfromthetrainingdatadistribution 1." ></td>
	<td class="line x" title="16:215	Second, itis not clear which notion of domain similarity should be used to select domains to annotate that would be good proxies for many other domains." ></td>
	<td class="line x" title="17:215	We propose solutions to these two questions and evaluate them on a corpus of reviews for four different types of products from Amazon: books, DVDs, electronics, and kitchen appliances2." ></td>
	<td class="line x" title="18:215	First, we show how to extend the recently proposed structural cor1For surveys of recent research on domain adaptation, see the ICML 2006 Workshop on Structural Knowledge Transfer for Machine Learning (http://gameairesearch.uta." ></td>
	<td class="line x" title="19:215	edu/) and the NIPS 2006 Workshop on Learning when test and training inputs have different distribution (http://ida. first.fraunhofer.de/projects/different06/) 2The dataset will be made available by the authors at publication time." ></td>
	<td class="line x" title="20:215	440 respondence learning (SCL) domain adaptation algorithm (Blitzer et al. , 2006) for use in sentiment classification." ></td>
	<td class="line x" title="21:215	A key step in SCL is the selection of pivot features thatare usedtolink thesourceandtarget domains." ></td>
	<td class="line x" title="22:215	We suggest selecting pivots based not only on their common frequency but also according to their mutual information with the source labels." ></td>
	<td class="line x" title="23:215	For data as diverse as product reviews, SCL can sometimes misalign features, resulting in degradation when we adapt between domains." ></td>
	<td class="line x" title="24:215	In our second extensionweshowhowtocorrectmisalignmentsusing a very small number of labeled instances." ></td>
	<td class="line x" title="25:215	Second, we evaluate the A-distance (Ben-David et al. , 2006) between domains as measure of the loss due to adaptation from one to the other." ></td>
	<td class="line x" title="26:215	The Adistancecanbemeasuredfromunlabeleddata, andit was designed to take into account only divergences which affect classification accuracy." ></td>
	<td class="line x" title="27:215	We show that it correlates well with adaptation loss, indicating that we can use the A-distance to select a subset of domains to label as sources." ></td>
	<td class="line x" title="28:215	In the next section we briefly review SCL and introduce our new pivot selection method." ></td>
	<td class="line x" title="29:215	Section 3 describes datasets and experimental method." ></td>
	<td class="line x" title="30:215	Section 4 gives results for SCL and the mutual information method for selecting pivot features." ></td>
	<td class="line x" title="31:215	Section 5 shows how to correct feature misalignments using a small amount of labeled target domain data." ></td>
	<td class="line x" title="32:215	Section 6 motivates the A-distance and shows that it correlates well with adaptability." ></td>
	<td class="line x" title="33:215	We discuss related work in Section 7 and conclude in Section 8." ></td>
	<td class="line x" title="34:215	2 Structural Correspondence Learning Before reviewing SCL, we give a brief illustrative example." ></td>
	<td class="line x" title="35:215	Suppose that we are adapting from reviews of computers to reviews of cell phones." ></td>
	<td class="line x" title="36:215	While many of the features of a good cell phone review are the same as a computer review  the words excellent and awful for example  many words are totally new, like reception." ></td>
	<td class="line x" title="37:215	At the same time, many features which were useful for computers, such as dual-core are no longer useful for cell phones." ></td>
	<td class="line x" title="38:215	Our key intuition is that even when good-quality reception and fast dual-core are completely distinct for each domain, if they both have high correlation with excellent and low correlation with awful on unlabeled data, then we can tentatively align them." ></td>
	<td class="line x" title="39:215	After learning a classifier for computer reviews, when we see a cell-phone feature like goodquality reception, we know it should behave in a roughly similar manner to fast dual-core." ></td>
	<td class="line x" title="40:215	2.1 Algorithm Overview Given labeled data from a source domain and unlabeled data from both source and target domains, SCLfirstchoosesasetofmpivotfeatureswhichoccur frequently in both domains." ></td>
	<td class="line x" title="41:215	Then, it models the correlations between the pivot features and all other features by training linear pivot predictors to predict occurrences of each pivot in the unlabeled data from both domains (Ando and Zhang, 2005; Blitzer et al. , 2006)." ></td>
	<td class="line x" title="42:215	The lscriptth pivot predictor is characterized by its weight vector wlscript; positive entries in that weight vector mean that a non-pivot feature (like fast dualcore) is highly correlated with the corresponding pivot (like excellent)." ></td>
	<td class="line x" title="43:215	The pivot predictor column weight vectors can be arranged into a matrix W = [wlscript]nlscript=1." ></td>
	<td class="line x" title="44:215	Let   Rkd be the top k left singular vectors of W (here d indicatesthetotalnumberoffeatures)." ></td>
	<td class="line x" title="45:215	Thesevectorsare the principal predictors for our weight space." ></td>
	<td class="line x" title="46:215	If we chose our pivot features well, then we expect these principal predictors to discriminate among positive and negative words in both domains." ></td>
	<td class="line x" title="47:215	At training and test time, suppose we observe a feature vector x. We apply the projection x to obtain k new real-valued features." ></td>
	<td class="line x" title="48:215	Now we learn a predictor for the augmented instance x,x." ></td>
	<td class="line x" title="49:215	If  contains meaningful correspondences, then the predictor which uses  will perform well in both source and target domains." ></td>
	<td class="line x" title="50:215	2.2 Selecting Pivots with Mutual Information The efficacy of SCL depends on the choice of pivot features." ></td>
	<td class="line x" title="51:215	For the part of speech tagging problem studied by Blitzer et al.(2006), frequently-occurring words in both domains were good choices, since they often correspond to function words such as prepositions and determiners, which are good indicators of parts of speech." ></td>
	<td class="line x" title="53:215	This is not the case for sentiment classification, however." ></td>
	<td class="line x" title="54:215	Therefore, we require that pivot features also be good predictors of the source label." ></td>
	<td class="line x" title="55:215	Among those features, we then choose the ones with highest mutual information to the source label." ></td>
	<td class="line x" title="56:215	Table 1 shows the set-symmetric 441 SCL, not SCL-MI SCL-MI, not SCL book one <num> so all a must a wonderful loved it very about they like weak dont waste awful good when highly recommended and easy Table 1: Top pivots selected by SCL, but not SCLMI (left) and vice-versa (right) differencesbetweenthetwomethodsforpivotselectionwhenadaptingaclassifierfrombookstokitchen appliances." ></td>
	<td class="line x" title="57:215	Wereferthroughouttherest ofthiswork to our method for selecting pivots as SCL-MI." ></td>
	<td class="line x" title="58:215	3 Dataset and Baseline We constructed a new dataset for sentiment domain adaptation by selecting Amazon product reviews for fourdifferentproducttypes: books, DVDs, electronics and kitchen appliances." ></td>
	<td class="line x" title="59:215	Each review consists of a rating (0-5 stars), a reviewer name and location, a product name, a review title and date, and the review text." ></td>
	<td class="line x" title="60:215	Reviews with rating > 3 were labeled positive, those with rating < 3 were labeled negative, and the rest discarded because their polarity was ambiguous." ></td>
	<td class="line oc" title="61:215	After this conversion, we had 1000 positive and 1000 negative examples for each domain, the same balanced composition as the polarity dataset (Pang et al. , 2002)." ></td>
	<td class="line x" title="62:215	In addition to the labeled data, we included between 3685 (DVDs) and 5945 (kitchen)instancesofunlabeleddata." ></td>
	<td class="line x" title="63:215	Thesizeofthe unlabeled data was limited primarily by the number of reviews we could crawl and download from the Amazon website." ></td>
	<td class="line x" title="64:215	Since we were able to obtain labels for all of the reviews, we also ensured that they were balanced between positive and negative examples, as well." ></td>
	<td class="line x" title="65:215	While the polarity dataset is a popular choice in the literature, we were unable to use it for our task." ></td>
	<td class="line x" title="66:215	Our method requires many unlabeled reviews and despite a large number of IMDB reviews available online, the extensive curation requirements made preparing a large amount of data difficult 3." ></td>
	<td class="line x" title="67:215	For classification, we use linear predictors on unigram and bigram features, trained to minimize the Huber loss with stochastic gradient descent (Zhang, 3For a description of the construction of the polarity dataset, see http://www.cs.cornell.edu/people/ pabo/movie-review-data/." ></td>
	<td class="line x" title="68:215	2004)." ></td>
	<td class="line oc" title="69:215	On the polarity dataset, this model matches the results reported by Pang et al.(2002)." ></td>
	<td class="line x" title="71:215	When we reportresultswithSCLandSCL-MI,werequirethat pivots occur in more than five documents in each domain." ></td>
	<td class="line x" title="72:215	Wesetk,thenumberofsingularvectorsofthe weight matrix, to 50." ></td>
	<td class="line x" title="73:215	4 Experiments with SCL and SCL-MI Each labeled dataset was split into a training set of 1600 instances and a test set of 400 instances." ></td>
	<td class="line x" title="74:215	All the experiments use a classifier trained on the training set of one domain and tested on the test set of a possibly different domain." ></td>
	<td class="line x" title="75:215	The baseline is a linear classifier trained without adaptation, while the gold standard is an in-domain classifier trained on the same domain as it is tested." ></td>
	<td class="line x" title="76:215	Figure 1 gives accuracies for all pairs of domain adaptation." ></td>
	<td class="line x" title="77:215	The domains are ordered clockwise from the top left: books, DVDs, electronics, and kitchen." ></td>
	<td class="line x" title="78:215	For each set of bars, the first letter is the source domain and the second letter is the target domain." ></td>
	<td class="line x" title="79:215	The thick horizontal bars are the accuracies of the in-domain classifiers for these domains." ></td>
	<td class="line x" title="80:215	Thus the first set of bars shows that the baseline achieves 72.8% accuracy adapting from DVDs to books." ></td>
	<td class="line x" title="81:215	SCL-MI achieves 79.7% and the in-domain gold standard is 80.4%." ></td>
	<td class="line x" title="82:215	We say that the adaptation loss for the baseline model is 7.6% and the adaptationlossfortheSCL-MImodelis0.7%." ></td>
	<td class="line x" title="83:215	Therelative reduction in error due to adaptation of SCL-MI for this test is 90.8%." ></td>
	<td class="line x" title="84:215	We can observe from these results that there is a rough grouping of our domains." ></td>
	<td class="line x" title="85:215	Books and DVDs are similar, as are kitchen appliances and electronics, but the two groups are different from one another." ></td>
	<td class="line x" title="86:215	Adapting classifiers from books to DVDs, for instance, is easier than adapting them from books to kitchen appliances." ></td>
	<td class="line x" title="87:215	We note that when transferring from kitchen to electronics, SCL-MI actually outperforms the in-domain classifier." ></td>
	<td class="line x" title="88:215	This is possiblesincetheunlabeleddatamaycontaininformation that the in-domain classifier does not have access to." ></td>
	<td class="line x" title="89:215	At the beginning of Section 2 we gave examples of how features can change behavior across domains." ></td>
	<td class="line x" title="90:215	The first type of behavior is when predictive features from the source domain are not predictive or do not appear in the target domain." ></td>
	<td class="line x" title="91:215	The second is 442 65 70 75 80 85 90 D->BE->BK->BB->DE->DK->D baselineSCLSCL-MIbooks 72.8 76.8 79.7 70.7 75.475.4 70.9 66.1 68.6 80.4 82.4 77.2 74.0 75.8 70.6 74.3 76.2 72.7 75.4 76.9 dvd 65 70 75 80 85 90 B->ED->EK->EB->KD->KE->K electronics kitchen 70.8 77.5 75.9 73.0 74.1 74.1 82.7 83.7 86.8 84.4 87.7 74.5 78.778.9 74.0 79.4 81.484.0 84.4 85.9 Figure 1: Accuracy results for domain adaptation between all pairs using SCL and SCL-MI." ></td>
	<td class="line x" title="92:215	Thick black lines are the accuracies of in-domain classifiers." ></td>
	<td class="line x" title="93:215	domain\polarity negative positive books plot <num> pages predictable reader grisham engaging reading this page <num> must read fascinating kitchen the plastic poorly designed excellent product espresso leaking awkward to defective are perfect years now a breeze Table 2: Correspondences discovered by SCL for books and kitchen appliances." ></td>
	<td class="line x" title="94:215	The top row shows features that only appear in books and the bottom features that only appear in kitchen appliances." ></td>
	<td class="line x" title="95:215	The left and right columns show negative and positive features in correspondence, respectively." ></td>
	<td class="line x" title="96:215	when predictive features from the target domain do not appear in the source domain." ></td>
	<td class="line x" title="97:215	To show how SCL deals with those domain mismatches, we look at the adaptation from book reviews to reviews of kitchen appliances." ></td>
	<td class="line x" title="98:215	We selected the top 1000 most informative features in both domains." ></td>
	<td class="line x" title="99:215	In both cases, between 85 and 90% of the informative features from one domain were not among the most informative of the other domain4." ></td>
	<td class="line x" title="100:215	SCL addresses both of these issues simultaneously by aligning features from the two domains." ></td>
	<td class="line x" title="101:215	4There is a third type, features which are positive in one domain but negative in another, but they appear very infrequently in our datasets." ></td>
	<td class="line x" title="102:215	Table 2 illustrates one row of the projection matrix for adapting from books to kitchen appliances; the features on each row appear only in the corresponding domain." ></td>
	<td class="line x" title="103:215	A supervised classifier trained on book reviews cannot assign weight to the kitchen features in the second row of table 2." ></td>
	<td class="line x" title="104:215	In contrast, SCL assigns weight to these features indirectly through the projection matrix." ></td>
	<td class="line x" title="105:215	When we observe the feature predictable with a negative book review, we update parameters corresponding to the entire projection, including the kitchen-specific features poorly designed and awkward to." ></td>
	<td class="line x" title="106:215	While some rows of the projection matrix  are 443 useful for classification, SCL can also misalign features." ></td>
	<td class="line x" title="107:215	This causes problems when a projection is discriminative in the source domain but not in the target." ></td>
	<td class="line x" title="108:215	This is the case for adapting from kitchen appliances to books." ></td>
	<td class="line x" title="109:215	Since the book domain is quite broad, many projections in books model topic distinctions such as between religious and political books." ></td>
	<td class="line x" title="110:215	These projections, which are uninformative as to the target label, are put into correspondence with the fewer discriminating projections in the much narrower kitchen domain." ></td>
	<td class="line x" title="111:215	When we adapt from kitchen to books, we assign weight to these uninformative projections, degrading target classification accuracy." ></td>
	<td class="line x" title="112:215	5 Correcting Misalignments We now show how to use a small amount of target domain labeled data to learn to ignore misaligned projections from SCL-MI." ></td>
	<td class="line x" title="113:215	Using the notation of AndoandZhang(2005),wecanwritethesupervised training objective of SCL on the source domain as minw,v summationdisplay i Lparenleftbigwprimexi +vprimexi,yiparenrightbig+ ||w||2 + ||v||2, where y is the label." ></td>
	<td class="line x" title="114:215	The weight vector w  Rd weighs the original features, while v  Rk weighs the projected features." ></td>
	<td class="line x" title="115:215	Ando and Zhang (2005) and Blitzeretal.(2006)suggest = 104, = 0,which we have used in our results so far." ></td>
	<td class="line x" title="116:215	Suppose now that we have trained source model weight vectors ws and vs. A small amount of target domain data is probably insufficient to significantly change w, but we can correct v, which is much smaller." ></td>
	<td class="line x" title="117:215	We augment each labeled target instance xj with the label assigned by the source domain classifier (Florian et al. , 2004; Blitzer et al. , 2006)." ></td>
	<td class="line x" title="118:215	Then we solve minw,vsummationtextj L(wprimexj +vprimexj,yj) + ||w||2 +||vvs||2." ></td>
	<td class="line x" title="119:215	Sincewedontwanttodeviatesignificantlyfromthe source parameters, we set  =  = 101." ></td>
	<td class="line x" title="120:215	Figure 2 shows the corrected SCL-MI model using 50 target domain labeled instances." ></td>
	<td class="line x" title="121:215	We chose this number since we believe it to be a reasonable amount for a single engineer to label with minimal effort." ></td>
	<td class="line x" title="122:215	For reasons of space, for each target domain dom \ model base base scl scl-mi scl-mi +targ +targ books 8.9 9.0 7.4 5.8 4.4 dvd 8.9 8.9 7.8 6.1 5.3 electron 8.3 8.5 6.0 5.5 4.8 kitchen 10.2 9.9 7.0 5.6 5.1 average 9.1 9.1 7.1 5.8 4.9 Table 3: For each domain, we show the loss due to transfer for each method, averaged over all domains." ></td>
	<td class="line x" title="123:215	The bottom row shows the average loss over all runs." ></td>
	<td class="line x" title="124:215	we show adaptation from only the two domains on which SCL-MI performed the worst relative to the supervised baseline." ></td>
	<td class="line x" title="125:215	For example, the book domain shows only results from electronics and kitchen, but not DVDs." ></td>
	<td class="line x" title="126:215	As a baseline, we used the label of the sourcedomainclassifierasafeatureinthetarget, but did not use any SCL features." ></td>
	<td class="line x" title="127:215	We note that the baseline is very close to just using the source domain classifier, because with only 50 target domain instances we do not have enough data to relearn all of the parameters inw." ></td>
	<td class="line x" title="128:215	As we can see, though, relearning the 50 parameters in v is quite helpful." ></td>
	<td class="line x" title="129:215	The corrected model always improves over the baseline for every possible transfer, including those not shown in the figure." ></td>
	<td class="line x" title="130:215	The idea of using the regularizer of a linear model to encourage the target parameters to be close to the source parameters has been used previously in domain adaptation." ></td>
	<td class="line x" title="131:215	In particular, Chelba and Acero (2004) showed how this technique can be effective for capitalization adaptation." ></td>
	<td class="line x" title="132:215	The major difference between our approach and theirs is that we only penalize deviation from the source parameters for the weights v of projected features, while they work with the weights of the original features only." ></td>
	<td class="line x" title="133:215	For our small amount of labeled target data, attempting to penalize w using ws performed no better than our baseline." ></td>
	<td class="line x" title="134:215	Because we only need to learn to ignore projections that misalign features, we can make much better use of our labeled data by adapting only 50 parameters, rather than 200,000." ></td>
	<td class="line x" title="135:215	Table 3 summarizes the results of sections 4 and 5." ></td>
	<td class="line x" title="136:215	Structural correspondence learning reduces the error due to transfer by 21%." ></td>
	<td class="line x" title="137:215	Choosing pivots by mutual information allows us to further reduce the error to 36%." ></td>
	<td class="line x" title="138:215	Finally, by adding 50 instances of target domain data and using this to correct the misaligned projections, we achieve an average relative 444 65 70 75 80 85 90 E->BK->BB->DK->DB->ED->EB->KE->K base+50-targSCL-MI+50-targ books kitchen 70.9 76.0 70.7 76.8 78.5 72.7 80.4 87.7 76.6 70.8 76.6 73.0 77.9 74.3 80.7 84.3 dvd electronics 82.4 84.4 73.2 85.9 Figure 2: Accuracy results for domain adaptation with 50 labeled target domain instances." ></td>
	<td class="line x" title="139:215	reduction in error of 46%." ></td>
	<td class="line x" title="140:215	6 Measuring Adaptability Sections 2-5 focused on how to adapt to a target domain when you had a labeled source dataset." ></td>
	<td class="line x" title="141:215	We now take a step back to look at the problem of selecting source domain data to label." ></td>
	<td class="line x" title="142:215	We study a setting where an engineer knows roughly her domains of interest but does not have any labeled data yet." ></td>
	<td class="line x" title="143:215	In that case, she can ask the question Which sources should I label to obtain the best performance over all my domains? On our product domains, for example, if we are interested in classifying reviews of kitchen appliances, we know from sections 4-5 that it would be foolish to label reviews of books or DVDs rather than electronics." ></td>
	<td class="line x" title="144:215	Here we show how to select source domains using only unlabeled data and the SCL representation." ></td>
	<td class="line x" title="145:215	6.1 The A-distance We propose to measure domain adaptability by using the divergence of two domains after the SCL projection." ></td>
	<td class="line x" title="146:215	We can characterize domains by their induced distributions on instance space: the more different the domains, the more divergent the distributions." ></td>
	<td class="line x" title="147:215	Here we make use of the A-distance (BenDavid et al. , 2006)." ></td>
	<td class="line x" title="148:215	The key intuition behind the A-distance is that while two domains can differ in arbitrary ways, we are only interested in the differences that affect classification accuracy." ></td>
	<td class="line x" title="149:215	Let A be the family of subsets of Rk corresponding to characteristic functions of linear classifiers (sets on which a linear classifier returns positive value)." ></td>
	<td class="line x" title="150:215	ThentheAdistancebetweentwoprobability distributions is dA(D,Dprime) = 2 sup AA |PrD [A]  PrDprime [A]| . That is, we find the subset in A on which the distributions differ the most in the L1 sense." ></td>
	<td class="line x" title="151:215	Ben-David et al.(2006) show that computing the A-distance for a finite sample is exactly the problem of minimizing the empirical risk of a classifier that discriminatesbetweeninstancesdrawnfromD andinstances drawn from Dprime." ></td>
	<td class="line x" title="153:215	This is convenient for us, since it allows us to use classification machinery to compute the A-distance." ></td>
	<td class="line x" title="154:215	6.2 Unlabeled Adaptability Measurements We follow Ben-David et al.(2006) and use the Huber loss as a proxy for the A-distance." ></td>
	<td class="line x" title="156:215	Our procedure is as follows: Given two domains, we compute the SCL representation." ></td>
	<td class="line x" title="157:215	Then we create a data set where each instance x is labeled with the identity of the domain from which it came and train a linear classifier." ></td>
	<td class="line x" title="158:215	For each pair of domains we compute the empirical average per-instance Huber loss, subtract it from 1, and multiply the result by 100." ></td>
	<td class="line x" title="159:215	We refer to this quantity as the proxy A-distance." ></td>
	<td class="line x" title="160:215	When it is 100, the two domains are completely distinct." ></td>
	<td class="line x" title="161:215	When it is 0, the two domains are indistinguishable using a linear classifier." ></td>
	<td class="line x" title="162:215	Figure 3 is a correlation plot between the proxy A-distance and the adaptation error." ></td>
	<td class="line x" title="163:215	Suppose we wantedtolabeltwodomainsoutofthefourinsucha 445 0 2 4 6 8 10 12 14 6065707580859095100 Proxy A-distance Adaptation Loss EK BD DE DK BE, BK Figure 3: The proxy A-distance between each domain pair plotted against the average adaptation loss of as measured by our baseline system." ></td>
	<td class="line x" title="164:215	Each pair of domains is labeled by their first letters: EK indicates the pair electronics and kitchen." ></td>
	<td class="line x" title="165:215	way asto minimizeour erroron all thedomains." ></td>
	<td class="line x" title="166:215	Using the proxy A-distance as a criterion, we observe that we would choose one domain from either books or DVDs, but not both, since then we would not be abletoadequatelycoverelectronicsorkitchenappliances." ></td>
	<td class="line x" title="167:215	Similarly we would also choose one domain fromeitherelectronicsorkitchenappliances, butnot both." ></td>
	<td class="line oc" title="168:215	7 Related Work Sentiment classification has advanced considerably since the work of Pang et al.(2002), which we use as our baseline." ></td>
	<td class="line x" title="170:215	Thomas et al.(2006) use discourse structurepresentincongressionalrecordstoperform more accurate sentiment classification." ></td>
	<td class="line x" title="172:215	Pang and Lee (2005) treat sentiment analysis as an ordinal ranking problem." ></td>
	<td class="line x" title="173:215	In our work we only show improvement for the basic model, but all of these new techniques also make use of lexical features." ></td>
	<td class="line x" title="174:215	Thus webelievethatouradaptationmethodscouldbealso applied to those more refined models." ></td>
	<td class="line x" title="175:215	While work on domain adaptation for sentiment classifiers is sparse, it is worth noting that other researchers have investigated unsupervised and semisupervised methods for domain adaptation." ></td>
	<td class="line x" title="176:215	The work most similar in spirit to ours that of Turney (2002)." ></td>
	<td class="line x" title="177:215	He used the difference in mutual information with two human-selected features (the words excellent and poor) to score features in a completely unsupervised manner." ></td>
	<td class="line x" title="178:215	Then he classified documents according to various functions of these mutual information scores." ></td>
	<td class="line x" title="179:215	We stress that our method improves a supervised baseline." ></td>
	<td class="line x" title="180:215	While we do not have a direct comparison, we note that Turney (2002) performs worse on movie reviews than on his other datasets, the same type of data as the polarity dataset." ></td>
	<td class="line x" title="181:215	We also note the work of Aue and Gamon (2005), who performed a number of empirical tests on domain adaptation of sentiment classifiers." ></td>
	<td class="line x" title="182:215	Most of these tests were unsuccessful." ></td>
	<td class="line x" title="183:215	We briefly note their results on combining a number of source domains." ></td>
	<td class="line x" title="184:215	They observed that source domains closer to the target helped more." ></td>
	<td class="line x" title="185:215	In preliminary experiments we confirmed these results." ></td>
	<td class="line x" title="186:215	Adding more labeled data always helps, but diversifying training data does not." ></td>
	<td class="line x" title="187:215	When classifying kitchen appliances, for any fixed amount of labeled data, it is always better to draw from electronics as a source than use some combination of all three other domains." ></td>
	<td class="line x" title="188:215	Domain adaptation alone is a generally wellstudied area, and we cannot possibly hope to cover all of it here." ></td>
	<td class="line x" title="189:215	As we noted in Section 5, we are able to significantly outperform basic structural correspondence learning (Blitzer et al. , 2006)." ></td>
	<td class="line x" title="190:215	We also note that while Florian et al.(2004) and Blitzer et al.(2006) observe that including the label of a source classifier asa featureon smallamounts of target data tends to improve over using either the source alone or the target alone, we did not observe that for our data." ></td>
	<td class="line x" title="193:215	We believe the most important reason for this is that they explore structured prediction problems, where labels of surrounding words from the source classifier may be very informative, even if the current label is not." ></td>
	<td class="line x" title="194:215	In contrast our simple binary predictionproblemdoesnotexhibitsuchbehavior." ></td>
	<td class="line x" title="195:215	This may also be the reason that the model of Chelba and Acero (2004) did not aid in adaptation." ></td>
	<td class="line x" title="196:215	Finally we note that while Blitzer et al.(2006) did combine SCL with labeled target domain data, they only compared using the label of SCL or non-SCL source classifiers as features, following the work of Florian et al.(2004)." ></td>
	<td class="line x" title="199:215	By only adapting the SCLrelated part of the weight vector v, we are able to make better use of our small amount of unlabeled data than these previous techniques." ></td>
	<td class="line x" title="200:215	446 8 Conclusion Sentiment classification has seen a great deal of attention." ></td>
	<td class="line x" title="201:215	Its application to many different domains of discourse makes it an ideal candidate for domain adaptation." ></td>
	<td class="line x" title="202:215	This work addressed two important questions of domain adaptation." ></td>
	<td class="line x" title="203:215	First, we showed that for a given source and target domain, we can significantly improve for sentiment classification the structural correspondence learning model of Blitzer et al.(2006)." ></td>
	<td class="line x" title="205:215	We chose pivot features using not only common frequency among domains but also mutual information with the source labels." ></td>
	<td class="line x" title="206:215	We also showed how to correct structural correspondence misalignments by using a small amount of labeled target domain data." ></td>
	<td class="line x" title="207:215	Second, we provided a method for selecting those source domains most likely to adapt well to given target domains." ></td>
	<td class="line x" title="208:215	The unsupervised A-distance measure of divergence between domains correlates well with loss due to adaptation." ></td>
	<td class="line x" title="209:215	Thus we can use the Adistance to select source domains to label which will give low target domain error." ></td>
	<td class="line x" title="210:215	In the future, we wish to include some of the more recent advances in sentiment classification, as well as addressing the more realistic problem of ranking." ></td>
	<td class="line x" title="211:215	We are also actively searching for a larger and morevariedsetofdomainsonwhichtotestourtechniques." ></td>
	<td class="line x" title="212:215	Acknowledgements We thank Nikhil Dinesh for helpful advice throughout the course of this work." ></td>
	<td class="line x" title="213:215	This material is based upon work partially supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No." ></td>
	<td class="line x" title="214:215	NBCHD03001." ></td>
	<td class="line x" title="215:215	Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA or the Department of Interior-National BusinessCenter (DOI-NBC)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1123
Learning Multilingual Subjective Language via Cross-Lingual Projections
Mihalcea, Rada;Banea, Carmen;Wiebe, Janyce M.;"></td>
	<td class="line x" title="1:193	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 976983, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:193	c2007 Association for Computational Linguistics Learning Multilingual Subjective Language via Cross-Lingual Projections Rada Mihalcea and Carmen Banea Department of Computer Science University of North Texas rada@cs.unt.edu, carmenb@unt.edu Janyce Wiebe Department of Computer Science University of Pittsburgh wiebe@cs.pitt.edu Abstract This paper explores methods for generating subjectivity analysis resources in a new language by leveraging on the tools and resources available in English." ></td>
	<td class="line x" title="3:193	Given a bridge between English and the selected target language (e.g. , a bilingual dictionary or a parallel corpus), the methods can be used to rapidly create tools for subjectivity analysis in the new language." ></td>
	<td class="line x" title="4:193	1 Introduction There is growing interest in the automatic extraction of opinions, emotions, and sentiments in text (subjectivity), to provide tools and support for various natural language processing applications." ></td>
	<td class="line x" title="5:193	Most of the research to date has focused on English, which is mainly explained by the availability of resources for subjectivity analysis, such as lexicons and manually labeled corpora." ></td>
	<td class="line x" title="6:193	In this paper, we investigate methods to automatically generate resources for subjectivity analysis for a new target language by leveraging on the resources and tools available for English, which in many cases took years of work to complete." ></td>
	<td class="line x" title="7:193	Specifically, through experiments with cross-lingual projection of subjectivity, we seek answers to the following questions." ></td>
	<td class="line x" title="8:193	First, can we derive a subjectivity lexicon for a new language using an existing English subjectivity lexicon and a bilingual dictionary?" ></td>
	<td class="line x" title="9:193	Second, can we derive subjectivity-annotated corpora in a new language using existing subjectivity analysis tools for English and a parallel corpus?" ></td>
	<td class="line x" title="10:193	Finally, third, can we build tools for subjectivity analysis for a new target language by relying on these automatically generated resources?" ></td>
	<td class="line x" title="11:193	We focus our experiments on Romanian, selected as a representative of the large number of languages that have only limited text processing resources developed to date." ></td>
	<td class="line x" title="12:193	Note that, although we work with Romanian, the methods described are applicable to any other language, as in these experiments we (purposely) do not use any language-specific knowledge of the target language." ></td>
	<td class="line x" title="13:193	Given a bridge between English and the selected target language (e.g. , a bilingual dictionary or a parallel corpus), the methods can be applied to other languages as well." ></td>
	<td class="line x" title="14:193	After providing motivations, we present two approaches to developing sentence-level subjectivity classifiers for a new target language." ></td>
	<td class="line x" title="15:193	The first uses a subjectivity lexicon translated from an English one." ></td>
	<td class="line x" title="16:193	The second uses an English subjectivity classifier and a parallel corpus to create target-language training data for developing a statistical classifier." ></td>
	<td class="line oc" title="17:193	2 Motivation Automatic subjectivity analysis methods have been used in a wide variety of text processing applications, such as tracking sentiment timelines in online forums and news (Lloyd et al. , 2005; Balog et al. , 2006), review classification (Turney, 2002; Pang et al. , 2002), mining opinions from product reviews (Hu and Liu, 2004), automatic expressive text-to-speech synthesis (Alm et al. , 2005), text semantic analysis (Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2006), and question answering (Yu and Hatzivassiloglou, 2003)." ></td>
	<td class="line x" title="18:193	976 While much recent work in subjectivity analysis focuses on sentiment (a type of subjectivity, namely positive and negative emotions, evaluations, and judgments), we opt to focus on recognizing subjectivity in general, for two reasons." ></td>
	<td class="line x" title="19:193	First, even when sentiment is the desired focus, researchers in sentiment analysis have shown that a two-stage approach is often beneficial, in which subjective instances are distinguished from objective ones, and then the subjective instances are further classified according to polarity (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al. , 2005; Kim and Hovy, 2006)." ></td>
	<td class="line x" title="20:193	In fact, the problem of distinguishing subjective versus objective instances has often proved to be more difficult than subsequent polarity classification, so improvements in subjectivity classification promise to positively impact sentiment classification." ></td>
	<td class="line x" title="21:193	This is reported in studies of manual annotation of phrases (Takamura et al. , 2006), recognizing contextual polarity of expressions (Wilson et al. , 2005), and sentiment tagging of words and word senses (Andreevskaia and Bergler, 2006; Esuli and Sebastiani, 2006)." ></td>
	<td class="line x" title="22:193	Second, an NLP application may seek a wide range of types of subjectivity attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments." ></td>
	<td class="line x" title="23:193	For instance, the opinion tracking system Lydia (Lloyd et al. , 2005) gives separate ratings for subjectivity and sentiment." ></td>
	<td class="line x" title="24:193	These can be detected with subjectivity analysis but not by a method focused only on sentiment." ></td>
	<td class="line x" title="25:193	There is world-wide interest in text analysis applications." ></td>
	<td class="line x" title="26:193	While work on subjectivity analysis in other languages is growing (e.g. , Japanese data are used in (Takamura et al. , 2006; Kanayama and Nasukawa, 2006), Chinese data are used in (Hu et al. , 2005), and German data are used in (Kim and Hovy, 2006)), much of the work in subjectivity analysis has been applied to English data." ></td>
	<td class="line x" title="27:193	Creating corpora and lexical resources for a new language is very time consuming." ></td>
	<td class="line x" title="28:193	In general, we would like to leverage resources already developed for one language to more rapidly create subjectivity analysis tools for a new one." ></td>
	<td class="line x" title="29:193	This motivates our exploration and use of cross-lingual lexicon translations and annotation projections." ></td>
	<td class="line x" title="30:193	Most if not all work on subjectivity analysis has been carried out in a monolingual framework." ></td>
	<td class="line x" title="31:193	We are not aware of multi-lingual work in subjectivity analysis such as that proposed here, in which subjectivity analysis resources developed for one language are used to support developing resources in another." ></td>
	<td class="line x" title="32:193	3 A Lexicon-Based Approach Many subjectivity and sentiment analysis tools rely on manually or semi-automatically constructed lexicons (Yu and Hatzivassiloglou, 2003; Riloff and Wiebe, 2003; Kim and Hovy, 2006)." ></td>
	<td class="line x" title="33:193	Given the success of such techniques, the first approach we take to generating a target-language subjectivity classifier is to create a subjectivity lexicon by translating an existing source language lexicon, and then build a classifier that relies on the resulting lexicon." ></td>
	<td class="line x" title="34:193	Below, we describe the translation process and discuss the results of an annotation study to assess the quality of the translated lexicon." ></td>
	<td class="line x" title="35:193	We then describe and evaluate a lexicon-based target-language classifier." ></td>
	<td class="line x" title="36:193	3.1 Translating a Subjectivity Lexicon The subjectivity lexicon we use is from OpinionFinder (Wiebe and Riloff, 2005), an English subjectivity analysis system which, among other things, classifies sentences as subjective or objective." ></td>
	<td class="line x" title="37:193	The lexicon was compiled from manually developed resources augmented with entries learned from corpora." ></td>
	<td class="line x" title="38:193	It contains 6,856 unique entries, out of which 990 are multi-word expressions." ></td>
	<td class="line x" title="39:193	The entries in the lexicon have been labeled for part of speech, and for reliability  those that appear most often in subjective contexts are strong clues of subjectivity, while those that appear less often, but still more often than expected by chance, are labeled weak." ></td>
	<td class="line x" title="40:193	To perform the translation, we use two bilingual dictionaries." ></td>
	<td class="line x" title="41:193	The first is an authoritative EnglishRomanian dictionary, consisting of 41,500 entries,1 which we use as the main translation resource for the lexicon translation." ></td>
	<td class="line x" title="42:193	The second dictionary, drawn from the Universal Dictionary download site (UDP, 2007) consists of 4,500 entries written largely by Web volunteer contributors, and thus is not error free." ></td>
	<td class="line x" title="43:193	We use this dictionary only for those entries that do not appear in the main dictionary." ></td>
	<td class="line x" title="44:193	1Unique English entries, each with multiple Romanian translations." ></td>
	<td class="line x" title="45:193	977 There were several challenges encountered in the translation process." ></td>
	<td class="line x" title="46:193	First, although the English subjectivity lexicon contains inflected words, we must use the lemmatized form in order to be able to translate the entries using the bilingual dictionary." ></td>
	<td class="line x" title="47:193	However, words may lose their subjective meaning once lemmatized." ></td>
	<td class="line x" title="48:193	For instance, the inflected form of memories becomes memory." ></td>
	<td class="line x" title="49:193	Once translated into Romanian (as memorie), its main meaning is objective, referring to the power of retaining information as in Iron supplements may improve a womans memory." ></td>
	<td class="line x" title="50:193	Second, neither the lexicon nor the bilingual dictionary provides information on the sense of the individual entries, and therefore the translation has to rely on the most probable sense in the target language." ></td>
	<td class="line x" title="51:193	Fortunately, the bilingual dictionary lists the translations in reverse order of their usage frequencies." ></td>
	<td class="line x" title="52:193	Nonetheless, the ambiguity of the words and the translations still seems to represent an important source of error." ></td>
	<td class="line x" title="53:193	Moreover, the lexicon sometimes includes identical entries expressed through different parts of speech, e.g., grudge has two separate entries, for its noun and verb roles, respectively." ></td>
	<td class="line x" title="54:193	On the other hand, the bilingual dictionary does not make this distinction, and therefore we have again to rely on the most frequent heuristic captured by the translation order in the bilingual dictionary." ></td>
	<td class="line x" title="55:193	Finally, the lexicon includes a significant number (990) of multi-word expressions that pose translation difficulties, sometimes because their meaning is idiomatic, and sometimes because the multi-word expression is not listed in the bilingual dictionary and the translation of the entire phrase is difficult to reconstruct from the translations of the individual words." ></td>
	<td class="line x" title="56:193	To address this problem, when a translation is not found in the dictionary, we create one using a word-by-word approach." ></td>
	<td class="line x" title="57:193	These translations are then validated by enforcing that they occur at least three times on the Web, using counts collected from the AltaVista search engine." ></td>
	<td class="line x" title="58:193	The multi-word expressions that are not validated in this process are discarded, reducing the number of expressions from an initial set of 990 to a final set of 264." ></td>
	<td class="line x" title="59:193	The final subjectivity lexicon in Romanian contains 4,983 entries." ></td>
	<td class="line x" title="60:193	Table 1 shows examples of entries in the Romanian lexicon, together with their corresponding original English form." ></td>
	<td class="line x" title="61:193	The table Romanian English attributes nfrumuseta beautifying strong, verb notabil notable weak, adj plin de regret full of regrets strong, adj sclav slaves weak, noun Table 1: Examples of entries in the Romanian subjectivity lexicon also shows the reliability of the expression (weak or strong) and the part of speech  attributes that are provided in the English subjectivity lexicon." ></td>
	<td class="line x" title="62:193	Manual Evaluation." ></td>
	<td class="line x" title="63:193	We want to assess the quality of the translated lexicon, and compare it to the quality of the original English lexicon." ></td>
	<td class="line x" title="64:193	The English subjectivity lexicon was evaluated in (Wiebe and Riloff, 2005) against a corpus of English-language news articles manually annotated for subjectivity (the MPQA corpus (Wiebe et al. , 2005))." ></td>
	<td class="line x" title="65:193	According to this evaluation, 85% of the instances of the clues marked as strong and 71.5% of the clues marked as weak are in subjective sentences in the MPQA corpus." ></td>
	<td class="line x" title="66:193	Since there is no comparable Romanian corpus, an alternate way to judge the subjectivity of a Romanian lexicon entry is needed." ></td>
	<td class="line x" title="67:193	Two native speakers of Romanian annotated the subjectivity of 150 randomly selected entries." ></td>
	<td class="line x" title="68:193	Each annotator independently read approximately 100 examples of each drawn from the Web, including a large number from news sources." ></td>
	<td class="line x" title="69:193	The subjectivity of a word was consequently judged in the contexts where it most frequently appears, accounting for its most frequent meanings on the Web." ></td>
	<td class="line x" title="70:193	The tagset used for the annotations consists of S(ubjective), O(bjective), and B(oth)." ></td>
	<td class="line x" title="71:193	A W(rong) label is also used to indicate a wrong translation." ></td>
	<td class="line x" title="72:193	Table 2 shows the contingency table for the two annotators judgments on this data." ></td>
	<td class="line x" title="73:193	S O B W Total S 53 6 9 0 68 O 1 27 1 0 29 B 5 3 18 0 26 W 0 0 0 27 27 Total 59 36 28 27 150 Table 2: Agreement on 150 entries in the Romanian lexicon Without counting the wrong translations, the agreement is measured at 0.80, with a Kappa  = 978 0.70, which indicates consistent agreement." ></td>
	<td class="line x" title="74:193	After the disagreements were reconciled through discussions, the final set of 123 correctly translated entries does include 49.6% (61) subjective entries, but fully 23.6% (29) were found in the study to have primarily objective uses (the other 26.8% are mixed)." ></td>
	<td class="line x" title="75:193	Thus, this study suggests that the Romanian subjectivity clues derived through translation are less reliable than the original set of English clues." ></td>
	<td class="line x" title="76:193	In several cases, the subjectivity is lost in the translation, mainly due to word ambiguity in either the source or target language, or both." ></td>
	<td class="line x" title="77:193	For instance, the word fragile correctly translates into Romanian as fragil, yet this word is frequently used to refer to breakable objects, and it loses its subjective meaning of delicate." ></td>
	<td class="line x" title="78:193	Other words, such as one-sided, completely lose subjectivity once translated, as it becomes in Romanian cu o singura latura, meaning with only one side (as of objects)." ></td>
	<td class="line x" title="79:193	Interestingly, the reliability of clues in the English lexicon seems to help preserve subjectivity." ></td>
	<td class="line x" title="80:193	Out of the 77 entries marked as strong, 11 were judged to be objective in Romanian (14.3%), compared to 14 objective Romanian entries obtained from the 36 weak English clues (39.0%)." ></td>
	<td class="line x" title="81:193	3.2 Rule-based Subjectivity Classifier Using a Subjectivity Lexicon Starting with the Romanian lexicon, we developed a lexical classifier similar to the one introduced by (Riloff and Wiebe, 2003)." ></td>
	<td class="line x" title="82:193	At the core of this method is a high-precision subjectivity and objectivity classifier that can label large amounts of raw text using only a subjectivity lexicon." ></td>
	<td class="line x" title="83:193	Their method is further improved with a bootstrapping process that learns extraction patterns." ></td>
	<td class="line x" title="84:193	In our experiments, however, we apply only the rule-based classification step, since the extraction step cannot be implemented without tools for syntactic parsing and information extraction not available in Romanian." ></td>
	<td class="line x" title="85:193	The classifier relies on three main heuristics to label subjective and objective sentences: (1) if two or more strong subjective expressions occur in the same sentence, the sentence is labeled Subjective; (2) if no strong subjective expressions occur in a sentence, and at most two weak subjective expressions occur in the previous, current, and next sentence combined, then the sentence is labeled Objective; (3) otherwise, if none of the previous rules apply, the sentence is labeled Unknown." ></td>
	<td class="line x" title="86:193	The quality of the classifier was evaluated on a Romanian gold-standard corpus annotated for subjectivity." ></td>
	<td class="line x" title="87:193	Two native Romanian speakers (Ro1 and Ro2) manually annotated the subjectivity of the sentences of five randomly selected documents (504 sentences) from the Romanian side of an EnglishRomanian parallel corpus, according to the annotation scheme in (Wiebe et al. , 2005)." ></td>
	<td class="line x" title="88:193	Agreement between annotators was measured, and then their differences were adjudicated." ></td>
	<td class="line x" title="89:193	The baseline on this data set is 54.16%, which can be obtained by assigning a default Subjective label to all sentences." ></td>
	<td class="line x" title="90:193	(More information about the corpus and annotations are given in Section 4 below, where agreement between English and Romanian aligned sentences is also assessed.)" ></td>
	<td class="line x" title="91:193	As mentioned earlier, due to the lexicon projection process that is performed via a bilingual dictionary, the entries in our Romanian subjectivity lexicon are in a lemmatized form." ></td>
	<td class="line x" title="92:193	Consequently, we also lemmatize the gold-standard corpus, to allow for the identification of matches with the lexicon." ></td>
	<td class="line x" title="93:193	For this purpose, we use the Romanian lemmatizer developed by Ion and Tufis (Ion, 2007), which has an estimated accuracy of 98%.2 Table 3 shows the results of the rule-based classifier." ></td>
	<td class="line x" title="94:193	We show the precision, recall, and F-measure independently measured for the subjective, objective, and all sentences." ></td>
	<td class="line x" title="95:193	We also evaluated a variation of the rule-based classifier that labels a sentence as objective if there are at most three weak expressions in the previous, current, and next sentence combined, which raises the recall of the objective classifier." ></td>
	<td class="line x" title="96:193	Our attempts to increase the recall of the subjective classifier all resulted in significant loss in precision, and thus we kept the original heuristic." ></td>
	<td class="line x" title="97:193	In its original English implementation, this system was proposed as being high-precision but low coverage." ></td>
	<td class="line x" title="98:193	Evaluated on the MPQA corpus, it has subjective precision of 90.4, subjective recall of 34.2, objective precision of 82.4, and objective recall of 30.7; overall, precision is 86.7 and recall is 32.6 (Wiebe and Riloff, 2005)." ></td>
	<td class="line x" title="99:193	We see a similar behavior on Romanian for subjective sentences." ></td>
	<td class="line x" title="100:193	The subjective precision is good, albeit at the cost of low 2Dan Tufis, personal communication." ></td>
	<td class="line x" title="101:193	979 Measure Subjective Objective All subj = at least two strong; obj = at most two weak Precision 80.00 56.50 62.59 Recall 20.51 48.91 33.53 F-measure 32.64 52.52 43.66 subj = at least two strong; obj = at most three weak Precision 80.00 56.85 61.94 Recall 20.51 61.03 39.08 F-measure 32.64 58.86 47.93 Table 3: Evaluation of the rule-based classifier recall, and thus the classifier could be used to harvest subjective sentences from unlabeled Romanian data (e.g. , for a subsequent bootstrapping process)." ></td>
	<td class="line x" title="102:193	The system is not very effective for objective classification, however." ></td>
	<td class="line x" title="103:193	Recall that the objective classifier relies on the weak subjectivity clues, for which the transfer of subjectivity in the translation process was particularly low." ></td>
	<td class="line x" title="104:193	4 A Corpus-Based Approach Given the low number of subjective entries found in the automatically generated lexicon and the subsequent low recall of the lexical classifier, we decided to also explore a second, corpus-based approach." ></td>
	<td class="line oc" title="105:193	This approach builds a subjectivity-annotated corpus for the target language through projection, and then trains a statistical classifier on the resulting corpus (numerous statistical classifiers have been trained for subjectivity or sentiment classification, e.g., (Pang et al. , 2002; Yu and Hatzivassiloglou, 2003))." ></td>
	<td class="line x" title="106:193	The hypothesis is that we can eliminate some of the ambiguities (and consequent loss of subjectivity) observed during the lexicon translation by accounting for the context of the ambiguous words, which is possible in a corpus-based approach." ></td>
	<td class="line x" title="107:193	Additionally, we also hope to improve the recall of the classifier, by addressing those cases not covered by the lexicon-based approach." ></td>
	<td class="line x" title="108:193	In the experiments reported in this section, we use a parallel corpus consisting of 107 documents from the SemCor corpus (Miller et al. , 1993) and their manual translations into Romanian.3 The corpus consists of roughly 11,000 sentences, with approximately 250,000 tokens on each side." ></td>
	<td class="line x" title="109:193	It is a balanced corpus covering a number of topics in sports, politics, fashion, education, and others." ></td>
	<td class="line x" title="110:193	3The translation was carried out by a Romanian native speaker, student in a department of Foreign Languages and Translations in Romania." ></td>
	<td class="line x" title="111:193	Below, we begin with a manual annotation study to assess the quality of annotation and preservation of subjectivity in translation." ></td>
	<td class="line x" title="112:193	We then describe the automatic construction of a target-language training set, and evaluate a classifier trained on that data." ></td>
	<td class="line x" title="113:193	Annotation Study." ></td>
	<td class="line x" title="114:193	We start by performing an agreement study meant to determine the extent to which subjectivity is preserved by the cross-lingual projections." ></td>
	<td class="line x" title="115:193	In the study, three annotators  one native English speaker (En) and two native Romanian speakers (Ro1 and Ro2)  first trained on 3 randomly selected documents (331 sentences)." ></td>
	<td class="line x" title="116:193	They then independently annotated the subjectivity of the sentences of two randomly selected documents from the parallel corpus, accounting for 173 aligned sentence pairs." ></td>
	<td class="line x" title="117:193	The annotators had access exclusively to the version of the sentences in their language, to avoid any bias that could be introduced by seeing the translation in the other language." ></td>
	<td class="line x" title="118:193	Note that the Romanian annotations (after all differences between the Romanian annotators were adjudicated) of all 331 + 173 sentences make up the gold standard corpus used in the experiments reported in Sections 3.2 and 4.1." ></td>
	<td class="line x" title="119:193	Before presenting the results of the annotation study, we give some examples." ></td>
	<td class="line x" title="120:193	The following are English subjective sentences and their Romanian translations (the subjective elements are shown in bold)." ></td>
	<td class="line x" title="121:193	[en] The desire to give Broglio as many starts as possible." ></td>
	<td class="line x" title="122:193	[ro] Dorinta de a-i da lui Broglio cat mai multe starturi posibile." ></td>
	<td class="line x" title="123:193	[en] Suppose he did lie beside Lenin, would it be permanent ? [ro] Sa presupunem ca ar fi asezat alaturi de Lenin, oare va fi pentru totdeauna?" ></td>
	<td class="line x" title="124:193	The following are examples of objective parallel sentences." ></td>
	<td class="line x" title="125:193	[en]The Pirates have a 9-6 record this year and the Redbirds are 7-9." ></td>
	<td class="line x" title="126:193	[ro] Piratii au un palmares de 9 la 6 anul acesta si Pasarile Rosii au 7 la 9." ></td>
	<td class="line x" title="127:193	[en] One of the obstacles to the easy control of a 2-year old child is a lack of verbal communication." ></td>
	<td class="line x" title="128:193	[ro] Unul dintre obstacolele n controlarea unui copil de 2 ani este lipsa comunicarii verbale." ></td>
	<td class="line x" title="129:193	980 The annotators were trained using the MPQA annotation guidelines (Wiebe et al. , 2005)." ></td>
	<td class="line x" title="130:193	The tagset consists of S(ubjective), O(bjective) and U(ncertain)." ></td>
	<td class="line x" title="131:193	For the U tags, a class was also given; OU means, for instance, that the annotator is uncertain but she is leaning toward O. Table 4 shows the pairwise agreement figures and the Kappa () calculated for the three annotators." ></td>
	<td class="line x" title="132:193	The table also shows the agreement when the borderline uncertain cases are removed." ></td>
	<td class="line x" title="133:193	all sentences Uncertain removed pair agree  agree  (%) removed Ro1 & Ro2 0.83 0.67 0.89 0.77 23 En & Ro1 0.77 0.54 0.86 0.73 26 En & Ro2 0.78 0.55 0.91 0.82 20 Table 4: Agreement on the data set of 173 sentences." ></td>
	<td class="line x" title="134:193	Annotations performed by three annotators: one native English speaker (En) and two native Romanian speakers (Ro1 and Ro2) When all the sentences are included, the agreement between the two Romanian annotators is measured at 0.83 ( = 0.67)." ></td>
	<td class="line x" title="135:193	If we remove the borderline cases where at least one annotators tag is Uncertain, the agreement rises to 0.89 with  = 0.77." ></td>
	<td class="line x" title="136:193	These figures are somewhat lower than the agreement observed during previous subjectivity annotation studies conducted on English (Wiebe et al. , 2005) (the annotators were more extensively trained in those studies), but they nonetheless indicate consistent agreement." ></td>
	<td class="line x" title="137:193	Interestingly, when the agreement is conducted cross-lingually between an English and a Romanian annotator, the agreement figures, although somewhat lower, are comparable." ></td>
	<td class="line x" title="138:193	In fact, once the Uncertain tags are removed, the monolingual and cross-lingual agreement and  values become almost equal, which suggests that in most cases the sentence-level subjectivity is preserved." ></td>
	<td class="line x" title="139:193	The disagreements were reconciled first between the labels assigned by the two Romanian annotators, followed by a reconciliation between the resulting Romanian gold-standard labels and the labels assigned by the English annotator." ></td>
	<td class="line x" title="140:193	In most cases, the disagreement across the two languages was found to be due to a difference of opinion about the sentence subjectivity, similar to the differences encountered in monolingual annotations." ></td>
	<td class="line x" title="141:193	However, there are cases where the differences are due to the subjectivity being lost in the translation." ></td>
	<td class="line x" title="142:193	Sometimes, this is due to several possible interpretations for the translated sentence." ></td>
	<td class="line x" title="143:193	For instance, the following sentence: [en] They honored the battling Billikens last night." ></td>
	<td class="line x" title="144:193	[ro] Ei i-au celebrat pe Billikens seara trecuta. is marked as Subjective in English (in context, the English annotator interpreted honored as referring to praises of the Billikens)." ></td>
	<td class="line x" title="145:193	However, the Romanian translation of honored is celebrat which, while correct as a translation, has the more frequent interpretation of having a party." ></td>
	<td class="line x" title="146:193	The two Romanian annotators chose this interpretation, which correspondingly lead them to mark the sentence as Objective." ></td>
	<td class="line x" title="147:193	In other cases, in particular when the subjectivity is due to figures of speech such as irony, the translation sometimes misses the ironic aspects." ></td>
	<td class="line x" title="148:193	For instance, the translation of egghead was not perceived as ironic by the Romanian annotators, and consequently the following sentence labeled Subjective in English is annotated as Objective in Romanian." ></td>
	<td class="line x" title="149:193	[en] I have lived for many years in a Connecticut commuting town with a high percentage of [] business executives of egghead tastes." ></td>
	<td class="line x" title="150:193	[ro] Am trait multi ani ntr-un oras din apropiere de Connecticut ce avea o mare proportie de [] oameni de afaceri cu gusturi intelectuale." ></td>
	<td class="line x" title="151:193	4.1 Translating a Subjectivity-Annotated Corpus and Creating a Machine Learning Subjectivity Classifier To further validate the corpus-based projection of subjectivity, we developed a subjectivity classifier trained on Romanian subjectivity-annotated corpora obtained via cross-lingual projections." ></td>
	<td class="line x" title="152:193	Ideally, one would generate an annotated Romanian corpus by translating English documents manually annotated for subjectivity such as the MPQA corpus." ></td>
	<td class="line x" title="153:193	Unfortunately, the manual translation of this corpus would be prohibitively expensive, both timewise and financially." ></td>
	<td class="line x" title="154:193	The other alternative  automatic machine translation  has not yet reached a level that would enable the generation of a highquality translated corpus." ></td>
	<td class="line x" title="155:193	We therefore decided to use a different approach where we automatically annotate the English side of an existing EnglishRomanian corpus, and subsequently project the annotations onto the Romanian side of the parallel cor981 Precision Recall F-measure high-precision 86.7 32.6 47.4 high-coverage 79.4 70.6 74.7 Table 5: Precision, recall, and F-measure for the two OpinionFinder classifiers, as measured on the MPQA corpus." ></td>
	<td class="line x" title="156:193	pus across the sentence-level alignments available in the corpus." ></td>
	<td class="line x" title="157:193	For the automatic subjectivity annotations, we generated two sets of the English-side annotations, one using the high-precision classifier and one using the high-coverage classifier available in the OpinionFinder tool." ></td>
	<td class="line x" title="158:193	The high-precision classifier in OpinionFinder uses the clues of the subjectivity lexicon to harvest subjective and objective sentences from a large amount of unannotated text; this data is then used to automatically identify a set of extraction patterns, which are then used iteratively to identify a larger set of subjective and objective sentences." ></td>
	<td class="line x" title="159:193	In addition, in OpinionFinder, the high-precision classifier is used to produce an English labeled data set for training, which is used to generate its Naive Bayes high-coverage subjectivity classifier." ></td>
	<td class="line x" title="160:193	Table 5 shows the performance of the two classifiers on the MPQA corpus as reported in (Wiebe and Riloff, 2005)." ></td>
	<td class="line x" title="161:193	Note that 55% of the sentences in the MPQA corpus are subjective  which represents the baseline for this data set." ></td>
	<td class="line x" title="162:193	The two OpinionFinder classifiers are used to label the training corpus." ></td>
	<td class="line x" title="163:193	After removing the 504 test sentences, we are left with 10,628 sentences that are automatically annotated for subjectivity." ></td>
	<td class="line x" title="164:193	Table 6 shows the number of subjective and objective sentences obtained with each classifier." ></td>
	<td class="line x" title="165:193	Classifier Subjective Objective All high-precision 1,629 2,334 3,963 high-coverage 5,050 5,578 10,628 Table 6: Subjective and objective training sentences automatically annotated with OpinionFinder." ></td>
	<td class="line x" title="166:193	Next, the OpinionFinder annotations are projected onto the Romanian training sentences, which are then used to develop a probabilistic classifier for the automatic labeling of subjectivity in Romanian sentences." ></td>
	<td class="line oc" title="167:193	Similar to, e.g., (Pang et al. , 2002), we use a Naive Bayes algorithm trained on word features cooccurring with the subjective and the objective classifications." ></td>
	<td class="line x" title="168:193	We assume word independence, and we use a 0.3 cut-off for feature selection." ></td>
	<td class="line x" title="169:193	While recent work has also considered more complex syntactic features, we are not able to generate such features for Romanian as they require tools currently not available for this language." ></td>
	<td class="line x" title="170:193	We create two classifiers, one trained on each data set." ></td>
	<td class="line x" title="171:193	The quality of the classifiers is evaluated on the 504-sentence Romanian gold-standard corpus described above." ></td>
	<td class="line x" title="172:193	Recall that the baseline on this data set is 54.16%, the percentage of sentences in the corpus that are subjective." ></td>
	<td class="line x" title="173:193	Table 7 shows the results." ></td>
	<td class="line x" title="174:193	Subjective Objective All projection source: OF high-precision classifier Precision 65.02 69.62 64.48 Recall 82.41 47.61 64.48 F-measure 72.68 56.54 64.68 projection source: OF high-coverage classifier Precision 66.66 70.17 67.85 Recall 81.31 52.17 67.85 F-measure 72.68 56.54 67.85 Table 7: Evaluation of the machine learning classifier using training data obtained via projections from data automatically labeled by OpinionFinder (OF)." ></td>
	<td class="line x" title="175:193	Our best classifier has an F-measure of 67.85, and is obtained by training on projections from the high-coverage OpinionFinder annotations." ></td>
	<td class="line x" title="176:193	Although smaller than the 74.70 F-measure obtained by the English high-coverage classifier (see Table 5), the result appears remarkable given that no language-specific Romanian information was used." ></td>
	<td class="line x" title="177:193	The overall results obtained with the machine learning approach are considerably higher than those obtained from the rule-based classifier (except for the precision of the subjective sentences)." ></td>
	<td class="line x" title="178:193	This is most likely due to the lexicon translation process, which as mentioned in the agreement study in Section 3.1, leads to ambiguity and loss of subjectivity." ></td>
	<td class="line x" title="179:193	Instead, the corpus-based translations seem to better account for the ambiguity of the words, and the subjectivity is generally preserved in the sentence translations." ></td>
	<td class="line x" title="180:193	5 Conclusions In this paper, we described two approaches to generating resources for subjectivity annotations for a new 982 language, by leveraging on resources and tools available for English." ></td>
	<td class="line x" title="181:193	The first approach builds a target language subjectivity lexicon by translating an existing English lexicon using a bilingual dictionary." ></td>
	<td class="line x" title="182:193	The second generates a subjectivity-annotated corpus in a target language by projecting annotations from an automatically annotated English corpus." ></td>
	<td class="line x" title="183:193	These resources were validated in two ways." ></td>
	<td class="line x" title="184:193	First, we carried out annotation studies measuring the extent to which subjectivity is preserved across languages in each of the two resources." ></td>
	<td class="line x" title="185:193	These studies show that only a relatively small fraction of the entries in the lexicon preserve their subjectivity in the translation, mainly due to the ambiguity in both the source and the target languages." ></td>
	<td class="line x" title="186:193	This is consistent with observations made in previous work that subjectivity is a property associated not with words, but with word meanings (Wiebe and Mihalcea, 2006)." ></td>
	<td class="line x" title="187:193	In contrast, the sentence-level subjectivity was found to be more reliably preserved across languages, with cross-lingual inter-annotator agreements comparable to the monolingual ones." ></td>
	<td class="line x" title="188:193	Second, we validated the two automatically generated subjectivity resources by using them to build a tool for subjectivity analysis in the target language." ></td>
	<td class="line x" title="189:193	Specifically, we developed two classifiers: a rulebased classifier that relies on the subjectivity lexicon described in Section 3.1, and a machine learning classifier trained on the subjectivity-annotated corpus described in Section 4.1." ></td>
	<td class="line x" title="190:193	While the highest precision for the subjective classification is obtained with the rule-based classifier, the overall best result of 67.85 F-measure is due to the machine learning approach." ></td>
	<td class="line x" title="191:193	This result is consistent with the annotation studies, showing that the corpus projections preserve subjectivity more reliably than the lexicon translations." ></td>
	<td class="line x" title="192:193	Finally, neither one of the classifiers relies on language-specific information, but rather on knowledge obtained through projections from English." ></td>
	<td class="line x" title="193:193	A similar method can therefore be used to derive tools for subjectivity analysis in other languages." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-2023
Test Collection Selection and Gold Standard Generation for a Multiply-Annotated Opinion Corpus
Ku, Lun-Wei;Lo, Yong-Sheng;Chen, Hsin-Hsi;"></td>
	<td class="line x" title="1:111	Proceedings of the ACL 2007 Demo and Poster Sessions, pages 8992, Prague, June 2007." ></td>
	<td class="line x" title="2:111	c2007 Association for Computational Linguistics Test Collection Selection and Gold Standard Generation for a Multiply-Annotated Opinion Corpus Lun-Wei Ku, Yong-Shen Lo and Hsin-Hsi Chen Department of Computer Science and Information Engineering National Taiwan University {lwku, yslo}@nlg.csie.ntu.edu.tw; hhchen@csie.ntu.edu.tw Abstract Opinion analysis is an important research topic in recent years." ></td>
	<td class="line x" title="3:111	However, there are no common methods to create evaluation corpora." ></td>
	<td class="line x" title="4:111	This paper introduces a method for developing opinion corpora involving multiple annotators." ></td>
	<td class="line x" title="5:111	The characteristics of the created corpus are discussed, and the methodologies to select more consistent testing collections and their corresponding gold standards are proposed." ></td>
	<td class="line x" title="6:111	Under the gold standards, an opinion extraction system is evaluated." ></td>
	<td class="line x" title="7:111	The experiment results show some interesting phenomena." ></td>
	<td class="line x" title="8:111	1 Introduction Opinion information processing has been studied for several years." ></td>
	<td class="line oc" title="9:111	Researchers extracted opinions from words, sentences, and documents, and both rule-based and statistical models are investigated (Wiebe et al. , 2002; Pang et al. , 2002)." ></td>
	<td class="line x" title="10:111	The evaluation metrics precision, recall and f-measure are usually adopted." ></td>
	<td class="line x" title="11:111	A reliable corpus is very important for the opinion information processing because the annotations of opinions concern human perspectives." ></td>
	<td class="line x" title="12:111	Though the corpora created by researchers were analyzed (Wiebe et al. , 2002), the methods to increase the reliability of them were seldom touched." ></td>
	<td class="line x" title="13:111	The strict and lenient metrics for opinions were mentioned, but not discussed in details together with the corpora and their annotations." ></td>
	<td class="line x" title="14:111	This paper discusses the selection of testing collections and the generation of the corresponding gold standards under multiple annotations." ></td>
	<td class="line x" title="15:111	These testing collections are further used in an opinion extraction system and the system is evaluated with the corresponding gold standards." ></td>
	<td class="line x" title="16:111	The analysis of human annotations makes the improvements of opinion analysis systems feasible." ></td>
	<td class="line x" title="17:111	2 Corpus Annotation Opinion corpora are constructed for the research of opinion tasks, such as opinion extraction, opinion polarity judgment, opinion holder extraction, opinion summarization, opinion question answering, etc The materials of our opinion corpus are news documents from NTCIR CIRB020 and CIRB040 test collections." ></td>
	<td class="line x" title="18:111	A total of 32 topics concerning opinions are selected, and each document is annotated by three annotators." ></td>
	<td class="line x" title="19:111	Because different people often feel differently about an opinion due to their own perspectives, multiple annotators are necessary to build a reliable corpus." ></td>
	<td class="line x" title="20:111	For each sentence, whether it is relevant to a given topic, whether it is an opinion, and if it is, its polarity, are assigned." ></td>
	<td class="line x" title="21:111	The holders of opinions are also annotated." ></td>
	<td class="line x" title="22:111	The details of this corpus are shown in Table 1." ></td>
	<td class="line x" title="23:111	Topics Documents Sentences Quantity 32 843 11,907 Table 1." ></td>
	<td class="line x" title="24:111	Corpus size 3 Analysis of Annotated Corpus As mentioned, each sentence in our opinion corpus is annotated by three annotators." ></td>
	<td class="line x" title="25:111	Although this is a must for building reliable annotations, the inconsistency is unavoidable." ></td>
	<td class="line x" title="26:111	In this section, all the possible combinations of annotations are listed and two methods are introduced to evaluate the quality of the human-tagged opinion corpora." ></td>
	<td class="line x" title="27:111	3.1 Combinations of annotations Three major properties are annotated for sentences in this corpus, i.e., the relevancy, the opinionated issue, and the holder of the opinion." ></td>
	<td class="line x" title="28:111	The combinations of relevancy annotations are simple, and annotators usually have no argument over the opinion holders." ></td>
	<td class="line x" title="29:111	However, for the annotation of the opinionated issue, the situation is more com89 plex." ></td>
	<td class="line x" title="30:111	Annotations may have an argument about whether a sentence contains opinions, and their annotations may not be consistent on the polarities of an opinion." ></td>
	<td class="line x" title="31:111	Here we focus on the annotations of the opinionated issue." ></td>
	<td class="line x" title="32:111	Sentences may be considered as opinions only when more than two annotators mark them opinionated." ></td>
	<td class="line x" title="33:111	Therefore, they are targets for analysis." ></td>
	<td class="line x" title="34:111	The possible combinations of opinionated sentences and their polarity are shown in Figure 1." ></td>
	<td class="line x" title="35:111	A B C E D Positive/Neutral/Negative Figure 1." ></td>
	<td class="line x" title="36:111	Possible combinations of annotations In Figure 1, Cases A, B, C are those sentences which are annotated as opinionated by all three annotators, while cases D, E are those sentences which are annotated as opinionated only by two annotators." ></td>
	<td class="line x" title="37:111	In case A and case D, the polarities annotated by annotators are identical." ></td>
	<td class="line x" title="38:111	In case B, the polarities annotated by two of three annotators are agreed." ></td>
	<td class="line x" title="39:111	However, in cases C and E, the polarities annotated disagree with each other." ></td>
	<td class="line x" title="40:111	The statistics of these five cases are shown in Table 2." ></td>
	<td class="line x" title="41:111	Case A B C D E All Number 1,660 1,076 124 2,413 1,826 7,099 Table 2." ></td>
	<td class="line x" title="42:111	Statistics of cases A-E 3.2 Inconsistency 3 P P P N N N X X X 3 Multiple annotators bring the inconsistency." ></td>
	<td class="line x" title="43:111	There are several kinds of inconsistency in annotations, for example, relevant/non-relevant, opinionated/non-opinionated, and the inconsistency of polarities." ></td>
	<td class="line x" title="44:111	The relevant/non-relevant inconsistency is more like an information retrieval issue." ></td>
	<td class="line x" title="45:111	For opinions, because their strength varies, sometimes it is hard for annotators to tell if a sentence is opinionated." ></td>
	<td class="line x" title="46:111	However, for the opinion polarities, the inconsistency between positive and negative annotations is obviously stronger than that between positive and neutral, or neutral and negative ones." ></td>
	<td class="line x" title="47:111	Here we define a sentence strongly inconsistent if both positive and negative polarities are assigned to a sentence by different annotators." ></td>
	<td class="line x" title="48:111	The strong inconsistency may occur in case B (171), C (124), and E (270)." ></td>
	<td class="line x" title="49:111	In the corpus, only about 8% sentences are strongly inconsistent, which shows the annotations are reliable." ></td>
	<td class="line x" title="50:111	P P N N X X 2 3 P X N P N X N P X N X P X P N X N P P N 3.3 Kappa value for agreement We further assess the usability of the annotated corpus by Kappa values." ></td>
	<td class="line x" title="51:111	Kappa value gives a quantitative measure of the magnitude of interannotator agreement." ></td>
	<td class="line x" title="52:111	Table 3 shows a commonly used scale of the Kappa values." ></td>
	<td class="line x" title="53:111	Kappa value Meaning <0 less than change agreement 0.01-0.20 slight agreement 0.21-0.40 fair agreement 0.41-0.60 moderate agreement 0.61-0.80 substantial agreement 0.81-0.99 almost perfect agreement Table 3." ></td>
	<td class="line x" title="54:111	Interpretation of Kappa value The inconsistency of annotations brings difficulties in generating the gold standard." ></td>
	<td class="line x" title="55:111	Sentences should first be selected as the testing collection, N P P X N X X P NX 2 P P N N X X P NX 90 and then the corresponding gold standard can be generated." ></td>
	<td class="line x" title="56:111	Our aim is to generate testing collections and their gold standards which agree mostly to annotators." ></td>
	<td class="line x" title="57:111	Therefore, we analyze the kappa value not between annotators, but between the annotator and the gold standard." ></td>
	<td class="line x" title="58:111	The methodologies are introduced in the next section." ></td>
	<td class="line x" title="59:111	4 Testing Collections and Gold Standards The gold standard of relevance, the opinionated issue, and the opinion holder must be generated according to all the annotations." ></td>
	<td class="line x" title="60:111	Answers are chosen based on the agreement of annotations." ></td>
	<td class="line x" title="61:111	Considering the agreement among annotations themselves, the strict and the lenient testing collections and their corresponding gold standard are generated." ></td>
	<td class="line x" title="62:111	Considering the Kappa values of each annotator and the gold standard, topics with high agreement are selected as the testing collection." ></td>
	<td class="line x" title="63:111	Moreover, considering the consistency of polarities, the substantial consistent testing collection is generated." ></td>
	<td class="line x" title="64:111	In summary, two metrics for generating gold standards and four testing collections are adopted." ></td>
	<td class="line x" title="65:111	4.1 Strict and lenient Namely, the strict metric is different from the lenient metric in the agreement of annotations." ></td>
	<td class="line x" title="66:111	For the strict metric, sentences with annotations agreed by all three annotators are selected as the testing collection and the annotations are treated as the strict gold standard; for the lenient metric, sentences with annotations agreed by at least two annotators are selected as the testing collection and the majority of annotations are treated as the lenient gold standard." ></td>
	<td class="line x" title="67:111	For example, for the experiments of extracting opinion sentences, sentences in cases A, B, and C in Figure 1 are selected in both strict and lenient testing collections, while sentences in cases D and E are selected only in the lenient testing collection because three annotations are not totally agreed with one another." ></td>
	<td class="line x" title="68:111	For the experiments of opinion polarity judgment, sentences in case A in Figure 1 are selected in both strict and lenient testing collections, while sentences in cases B, C, D and E are selected only in the lenient testing collection." ></td>
	<td class="line x" title="69:111	Because every opinion sentence should be given a polarity, the polarities of sentences in cases B and D are the majority of annotations, while the polarity of sentences in cases C are given the polarity neutral in the lenient gold standard." ></td>
	<td class="line x" title="70:111	The polarities of sentences in case E are decided by rules P+X=P, N+X=N, and P+N=X. As for opinion holders, holders are found in opinion sentences of each testing collection." ></td>
	<td class="line x" title="71:111	The strict and lenient metrics are also applied in annotations of relevance." ></td>
	<td class="line x" title="72:111	4.2 High agreement To see how the generated gold standards agree with the annotations of all annotators, we analyze the kappa value from the agreements of each annotator and the gold standard for all 32 topics." ></td>
	<td class="line x" title="73:111	Each topic has two groups of documents from NTCIR: very relevant and relevant to topic." ></td>
	<td class="line x" title="74:111	However, one topic has only the relevant type document, it results in a total of 63 (2*31+1) groups of documents." ></td>
	<td class="line x" title="75:111	Note that the lenient metric is applied for generating the gold standard of this testing collection because the strict metric needs perfect agreement with each annotators annotations." ></td>
	<td class="line x" title="76:111	The distribution of kappa values of 63 groups is shown in Table 4 and Table 5." ></td>
	<td class="line x" title="77:111	The cumulative frequency bar graphs of Table 4 and Table 5 are shown in Figure 2 and Figure 3." ></td>
	<td class="line x" title="78:111	Kappa <=00-0.2 0.21-0.4 0.41-0.6 0.61-0.8 0.81-0.99 Number 1 2 12 14 33 1 Table 4." ></td>
	<td class="line x" title="79:111	Kappa values for opinion extraction Kappa <=00-0.2 0.21-0.4 0.41-0.6 0.61-0.8 0.81-0.99 Number 9 0 7 21 17 9 Table 5." ></td>
	<td class="line x" title="80:111	Kappa values for polarity judgment Figure 2." ></td>
	<td class="line x" title="81:111	Cumulative frequency of Table 4 1 3 15 29 62 63 0 10 20 30 40 50 60 70 <=0 0-0.2 0.21-0.4 0.41-0.6 0.61-0.8 0.81-0.99 99 16 37 54 63 0 10 20 30 40 50 60 70 <=0 0-0.2 0.21-0.4 0.41-0.6 0.61-0.8 0.81-0.99 Figure 3." ></td>
	<td class="line x" title="82:111	Cumulative frequency of Table 5 According to Figure 2 and Figure 3, document groups with kappa values above 0.4 are selected as 91 the high agreement testing collection, that is, document groups with moderate agreement in Table 3." ></td>
	<td class="line x" title="83:111	A total of 48 document groups are collected for opinion extraction and 47 document groups are collected for opinion polarity judgment." ></td>
	<td class="line x" title="84:111	4.3 Substantial Consistency In Section 3.2, sentences which are strongly inconsistent are defined." ></td>
	<td class="line x" title="85:111	The substantial consistency test collection expels strongly inconsistent sentences to achieve a higher consistency." ></td>
	<td class="line x" title="86:111	Notice that this test collection is still less consistent than the strict test collection, which is perfectly consistent with annotators." ></td>
	<td class="line x" title="87:111	The lenient metric is applied for generating the gold standard for this collection." ></td>
	<td class="line x" title="88:111	5 An Opinion System -CopeOpi A Chinese opinion extraction system for opinionated information, CopeOpi, is introduced here." ></td>
	<td class="line x" title="89:111	(Ku et al. , 2007) When judging the opinion polarity of a sentence in this system, three factors are considered: sentiment words, negation operators and opinion holders." ></td>
	<td class="line x" title="90:111	Every sentiment word has its own sentiment score." ></td>
	<td class="line x" title="91:111	If a sentence consists of more positive sentiments than negative sentiments, it must reveal something good, and vice versa." ></td>
	<td class="line x" title="92:111	However, a negation operator, such as not and never, may totally change the sentiment polarity of a sentiment word." ></td>
	<td class="line x" title="93:111	Therefore, when a negation operator appears together with a sentiment word, the opinion score of the sentiment word S will be changed to -S to keep the strength but reverse the polarity." ></td>
	<td class="line x" title="94:111	Opinion holders are also considered for opinion sentences, but how they influence opinions has not been investigated yet." ></td>
	<td class="line x" title="95:111	As a result, they are weighted equally at first." ></td>
	<td class="line x" title="96:111	A word is considered an opinion holder of an opinion sentence if either one of the following two criteria is met: 1." ></td>
	<td class="line x" title="97:111	The part of speech is a person name, organization name or personal." ></td>
	<td class="line x" title="98:111	2." ></td>
	<td class="line x" title="99:111	The word is in class A (human), type Ae (job) of the Cilin Dictionary (Mei et al. , 1982)." ></td>
	<td class="line x" title="100:111	6 Evaluation Results and Discussions Experiment results of CopeOpi using four designed testing collections are shown in Table 6." ></td>
	<td class="line x" title="101:111	Under the lenient metric with the lenient test collection, fmeasure scores 0.761 and 0.383 are achieved by CopeOpi." ></td>
	<td class="line x" title="102:111	The strict metric is the most severe, and the performance drops a lot under it." ></td>
	<td class="line x" title="103:111	Moreover, when using high agreement (H-A) and substantial consistency (S-C) test collections, the performance of the system does not increase in portion to the increase of agreement." ></td>
	<td class="line x" title="104:111	According to the agreement of annotators, people should perform best in the strict collection, and both high agreement and substantial consistency testing collections are easier than the lenient one." ></td>
	<td class="line x" title="105:111	This phenomenon shows that though this systems performance is satisfactory, its behavior is not like human beings." ></td>
	<td class="line x" title="106:111	For a computer system, the lenient testing collection is fuzzier and contains more information for judgment." ></td>
	<td class="line x" title="107:111	However, this also shows that the system may only take advantage of the surface information." ></td>
	<td class="line x" title="108:111	If we want our systems really judge like human beings, we should enhance the performance on strict, high agreement, and substantial consistency testing collections." ></td>
	<td class="line x" title="109:111	This analysis gives us, or other researchers who use this corpus for experiments, a direction to improve their own systems." ></td>
	<td class="line x" title="110:111	Opinion Extraction Opinion + Polarity Measure P R F P R F Lenient 0.664 0.890 0.761 0.335 0.448 0.383 Strict 0.258 0.921 0.404 0.104 0.662 0.180 H-A 0.677 0.885 0.767 0.339 0.455 0.388 S-C 0.308 0.452 0.367 Table 6." ></td>
	<td class="line x" title="111:111	Evaluation results Acknowledgments Research of this paper was partially supported by Excellent Research Projects of National Taiwan University, under the contract 95R0062-AE00-02." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-3012
Towards a Computational Treatment of Superlatives
Scheible, Silke;"></td>
	<td class="line x" title="1:155	Proceedings of the ACL 2007 Student Research Workshop, pages 6772, Prague, June 2007." ></td>
	<td class="line x" title="2:155	c2007 Association for Computational Linguistics Towards a Computational Treatment of Superlatives Silke Scheible Institute for Communicating and Collaborative Systems (ICCS) School of Informatics University of Edinburgh S.Scheible@sms.ed.ac.uk Abstract I propose a computational treatment of superlatives, starting with superlative constructions and the main challenges in automatically recognising and extracting their components." ></td>
	<td class="line x" title="3:155	Initial experimental evidence is provided for the value of the proposed work for Question Answering." ></td>
	<td class="line x" title="4:155	I also briefly discuss its potential value for Sentiment Detection and Opinion Extraction." ></td>
	<td class="line x" title="5:155	1 Introduction Although superlatives are frequently found in natural language, with the exception of recent work by Bos and Nissim (2006) and Jindal and Liu (2006), they have not yet been investigated within a computational framework." ></td>
	<td class="line x" title="6:155	And within the framework of theoretical linguistics, studies of superlatives have mainly focused on particular semantic properties that may only rarely occur in natural language (Szabolcsi, 1986; Heim, 1999)." ></td>
	<td class="line x" title="7:155	My goal is a comprehensive computational treatment of superlatives." ></td>
	<td class="line x" title="8:155	The initial question I address is how useful information can be automatically extracted from superlative constructions." ></td>
	<td class="line x" title="9:155	Due to the great semantic complexity and the variety of syntactic structures in which superlatives occur, this is a major challenge." ></td>
	<td class="line x" title="10:155	However, meeting it will benefit NLP applications such as Question Answering, Sentiment Detection and Opinion Extraction, and Ontology Learning." ></td>
	<td class="line x" title="11:155	2 What are Superlatives?" ></td>
	<td class="line x" title="12:155	In linguistics, the term superlative describes a well-defined class of word forms which (in English) are derived from adjectives or adverbs in two different ways: Inflectionally, where the suffix -est is appended to the base form of the adjective or adverb (e.g. lowest, nicest, smartest), or analytically, where the base adjective/adverb is preceded by the markers most/least (e.g. most interesting, least beautiful)." ></td>
	<td class="line x" title="13:155	Certain adjectives and adverbs have irregular superlative forms: good (best), bad (worst), far (furthest/farthest), well (best), badly (worst), much (most), and little (least)." ></td>
	<td class="line x" title="14:155	In order to be able to form superlatives, adjectives and adverbs must be gradable, which means that it must be possible to place them on a scale of comparison, at a position higher or lower than the one indicated by the adjective/adverb alone." ></td>
	<td class="line x" title="15:155	In English, this can be done by using the comparative and superlative forms of the adjective or adverb: [1] (a) Maths is more difficult than Physics." ></td>
	<td class="line x" title="16:155	(b) Chemistry is less difficult than Physics." ></td>
	<td class="line x" title="17:155	[2] (a) Maths is the most difficult subject at school." ></td>
	<td class="line x" title="18:155	(b) History is the least difficult subject at school." ></td>
	<td class="line x" title="19:155	The comparative form of an adjective or adverb is commonly used to compare two entities to one another with respect to a certain quality." ></td>
	<td class="line x" title="20:155	For example, in [1], Maths is located at a higher point on the difficulty scale than Physics, and Chemistry at a lower point." ></td>
	<td class="line x" title="21:155	The superlative form of an adjective is usually used to compare one entity to a set of other entities, and expresses the end spectrum of the scale: In [2], Maths and History are located at the highest and lowest points of the difficulty scale, respectively, while all the other subjects at school range somewhere in between." ></td>
	<td class="line x" title="22:155	3 Why are Superlatives Interesting?" ></td>
	<td class="line x" title="23:155	From a computational perspective, superlatives are of interest because they express a comparison 67 between a target entity (indicated in bold) and its comparison set (underlined), as in: [3] The blue whale is the largest mammal." ></td>
	<td class="line x" title="24:155	Here, the target blue whale is compared to the comparison set of mammals." ></td>
	<td class="line x" title="25:155	Milosavljevic (1999) has investigated the discourse purpose of different types of comparisons." ></td>
	<td class="line x" title="26:155	She classifies superlatives as a type of set complement comparison, whose purpose is to highlight the uniqueness of the target entity compared to its contrast set." ></td>
	<td class="line x" title="27:155	My initial investigation of superlative forms showed that there are two types of relation that hold between a target and its comparison set: Relation 1: Superlative relation Relation 2: IS-A relation The superlative relation specifies a property which all members of the set share, but which the target has the highest (or lowest) degree or value of." ></td>
	<td class="line x" title="28:155	The IS-A (or hypernymy) relation expresses the membership of the target in the comparison class (e.g. its parent class in a generalisation hierarchy)." ></td>
	<td class="line x" title="29:155	Both of these relations are of great interest from a relation extraction point of view, and in Section 6, I discuss their use in applications such as Question Answering (QA) and Sentiment Detection and Opinion Extraction." ></td>
	<td class="line x" title="30:155	That a computational treatment of superlatives is a worthwhile undertaking is also supported by the frequency of superlative forms in ordinary text: In a 250,000 word subcorpus of the WSJ corpus 1 I found 602 instances (which amounts to roughly one superlative form in every 17 sentences), while in the corpus of animal encyclopaedia entries used by Milosavljevic (1999), there were 1059 superlative forms in 250,000 words (about one superlative form in every 11 sentences).2 These results show significant variation in the distribution of superlatives across different text genres." ></td>
	<td class="line x" title="31:155	4 Elements of a Computational Treatment of Superlatives For an interpretation of comparisons, two things are generally of interest: What is being compared, and with respect to what this comparison is made." ></td>
	<td class="line x" title="32:155	Given that superlatives express set comparisons, a 1 www.ldc.upenn.edu/Catalog/LDC2000T43.html 2 In the following, these 250,000 word subcorpora will be referred to as SubWSJ and SubAC." ></td>
	<td class="line x" title="33:155	computational treatment should therefore help to identify: a) The target and comparison set b) The type of superlative relation that holds between them (cf.Relation 1 in Section 3) However, this task is far from straightforward, firstly because superlatives occur in a variety of different constructions." ></td>
	<td class="line x" title="35:155	Consider for example: [4] The pipe organ is the largest instrument." ></td>
	<td class="line x" title="36:155	[5] Of all the musicians in the brass band, Peter plays the largest instrument." ></td>
	<td class="line x" title="37:155	[6] The human foot is narrowest at the heel." ></td>
	<td class="line x" title="38:155	[7] First Class mail usually arrives the fastest." ></td>
	<td class="line x" title="39:155	[8] This year, Jodie Foster was voted best actress." ></td>
	<td class="line x" title="40:155	[9] I will get there at 8 at the earliest." ></td>
	<td class="line x" title="41:155	[10] I am most tired of your constant moaning." ></td>
	<td class="line x" title="42:155	[11] Most successful bands are from the U.S. All these examples contain a superlative form (bold italics)." ></td>
	<td class="line x" title="43:155	However, they differ not only in their syntactic structure, but also in the way in which they express a comparison." ></td>
	<td class="line x" title="44:155	Example [4] contains a clear-cut comparison between a target item and its comparison set: The pipe organ is compared to all other instruments with respect to its size." ></td>
	<td class="line x" title="45:155	However, although the superlative form in [4] occurs in the same noun phrase as in [5], the comparisons differ: What is being compared in [5] is not just the instruments, but the musicians in the brass band with respect to the size of the instrument that they play." ></td>
	<td class="line x" title="46:155	In example [6], the target and comparison set are even less easy to identify." ></td>
	<td class="line x" title="47:155	What is being compared here is not the human foot and a set of other entities, but rather different parts of the human foot." ></td>
	<td class="line x" title="48:155	In contrast to the first two examples, this superlative form is not incorporated in a noun phrase, but occurs freely in the sentence." ></td>
	<td class="line x" title="49:155	The same applies to fastest in example [7], which is an adverbial superlative." ></td>
	<td class="line x" title="50:155	The comparison here is between First Class mail and other mail delivery services." ></td>
	<td class="line x" title="51:155	Finally, examples [8] to [11] are not proper comparisons: best actress in [8] is an idiomatic expression, earliest in [9] is part of a so-called PP superlative construction (Corver and Matushansky, 2006), and [10] and [11] describe two non-comparative uses of most, as an intensifier and a proportional quantifier, respectively (Huddleston and Pullum, 2002)." ></td>
	<td class="line x" title="52:155	Initially, I will focus on cases like [4], which I call IS-A superlatives because they make explicit the IS-A relation that holds between target and comparison set (cf.Relation 2 in Section 3)." ></td>
	<td class="line x" title="54:155	They 68 are a good initial focus for a computational approach because both their target and comparison set are explicitly realised in the text (usually, though not necessarily, in the same sentence)." ></td>
	<td class="line x" title="55:155	Common surface forms of IS-A superlatives involve the verb to be ([12]-[14]), appositive position [15], and other copula verbs or expressions ([16] and [17]): [12] The blue whale is the largest mammal." ></td>
	<td class="line x" title="56:155	[13] The blue whale is the largest of all mammals." ></td>
	<td class="line x" title="57:155	[14] Of all mammals, the blue whale is the largest." ></td>
	<td class="line x" title="58:155	[15] The largest mammal, the blue whale, weighs [16] The ostrich is considered the largest bird." ></td>
	<td class="line x" title="59:155	[17] Mexico claimed to be the most peaceful country in the Americas." ></td>
	<td class="line x" title="60:155	IS-A superlatives are also the most frequent type of superlative comparison, with 176 instances in SubWSJ (ca." ></td>
	<td class="line x" title="61:155	30% of all superlative forms), and 350 instances in SubAC (ca." ></td>
	<td class="line x" title="62:155	33% of all superlative forms)." ></td>
	<td class="line x" title="63:155	The second major problem in a computational treatment of superlatives is to correctly identify and interpret the comparison set." ></td>
	<td class="line x" title="64:155	The challenge lies in the fact that it can be restricted in a variety of ways, for example by preceding possessives and premodifiers, or by postmodifiers such as PPs and various kinds of clauses." ></td>
	<td class="line x" title="65:155	Consider for example: [18] VW is [Europes largest maker of cars]." ></td>
	<td class="line x" title="66:155	[19] VW is [the largest European car maker with this product range]." ></td>
	<td class="line x" title="67:155	[20] VW is [the largest car maker in Europe] with an impressive product range." ></td>
	<td class="line x" title="68:155	[21] In China, VW is by far [the largest car maker]." ></td>
	<td class="line x" title="69:155	The phrases of cars and car in [18] and [19] both have the role of specifying the type of maker that constitutes the comparison set." ></td>
	<td class="line x" title="70:155	The phrases Europes, European and in Europe occur in determinative, premodifying, and postmodifying position, respectively, but all have the role of restricting the set of car makers to the ones in Europe." ></td>
	<td class="line x" title="71:155	And finally, the with PP phrases in [19] and [20] both occur in postmodifying position, but differ in that the one in [19] is involved in the comparison, while the one in [20] is non-restrictive." ></td>
	<td class="line x" title="72:155	In addition, restrictors of the comparison can also occur elsewhere in the sentence, as shown by the PP phrase and adverbial in [21]." ></td>
	<td class="line x" title="73:155	It is evident that in order to extract useful and reliable information, a thorough syntactic and semantic analysis of superlative constructions is required." ></td>
	<td class="line x" title="74:155	5 Previous Approaches 5.1 Jindal and Liu (2006) Jindal and Liu (2006) propose the study of comparative sentence mining, by which they mean the study of sentences that express an ordering relation between two sets of entities with respect to some common features (2006)." ></td>
	<td class="line x" title="75:155	They consider three kinds of relations: non-equal gradable (e.g. better), equative (e.g. as good as) and superlative (e.g. best)." ></td>
	<td class="line x" title="76:155	Having identified comparative sentences in a given text, the task is to extract comparative relations from them, in form of a vector like (relationWord, features, entityS1, entityS2), where relationWord represents the keyword used to express a comparative relation, features are a set of features being compared, and entityS1 and entityS2 are the sets of entities being compared, where entityS1 appears to the left of the relation word and entityS2 to the right." ></td>
	<td class="line x" title="77:155	Thus, for a sentence like Canons optics is better than those of Sony and Nikon, the system is expected to extract the vector (better, {optics}, {Canon}, {Sony, Nikon})." ></td>
	<td class="line x" title="78:155	For extracting the comparative relations, Jindal and Liu use what they call label sequential rules (LSR), mainly based on POS tags." ></td>
	<td class="line x" title="79:155	Their overall Fscore for this extraction task is 72%, a big improvement to the 58% achieved by their baseline system." ></td>
	<td class="line x" title="80:155	Although this result suggests that their system represents a powerful way of dealing with superlatives computationally, a closer inspection of their approach, and in particular of the gold standard data set, reveals some serious problems." ></td>
	<td class="line x" title="81:155	Jindal and Liu claim that for superlatives, the entityS2 slot is normally empty (2006)." ></td>
	<td class="line x" title="82:155	Assuming that the members of entityS2 usually represent the comparison set, this is somewhat counterintuitive." ></td>
	<td class="line x" title="83:155	A look at the data shows that even in cases where the comparison set is explicitly mentioned in the sentence, the entityS2 slot remains empty." ></td>
	<td class="line x" title="84:155	For example, although the comparison set in [22] is represented by the string these 2nd generation jukeboxes ( ipod, archos, dell, samsung ), it is not annotated as entityS2 in the gold standard: [22] all reviews i 've seen seem to indicate that the creative mp3 jukeboxes have the best sound quality of these 2nd generation jukeboxes ( ipod, archos, dell, samsung )." ></td>
	<td class="line x" title="85:155	(best, {sound quality}, {creative mp3 jukeboxes}, {--}) Jindal and Liu (2006) 69 Furthermore, Jindal and Liu do not distinguish between different types of superlatives." ></td>
	<td class="line x" title="86:155	In constructions where the superlative form is incorporated into an NP, Jindal and Liu consistently interpret the string following the superlative form as a feature, which is appropriate for cases like [22], but does not apply to superlative sentences involving the copula verb to be (as e.g. in [4]), where the NP head denotes the comparison set rather than a feature." ></td>
	<td class="line x" title="87:155	A further major problem is that restrictions on the comparison set as the ones discussed in Section 4 and negation are not considered at all." ></td>
	<td class="line x" title="88:155	Therefore, the reliability of the output produced by the system is questionable." ></td>
	<td class="line x" title="89:155	5.2 Bos and Nissim (2006) In contrast to Jindal and Liu (2006), Bos and Nissims (2006) approach to superlatives is explicitly semantic." ></td>
	<td class="line x" title="90:155	They describe an implementation of a system that can automatically detect superlatives, and determine the correct comparison set for attributive cases, where the superlative form is incorporated into an NP." ></td>
	<td class="line x" title="91:155	For example in [23], the comparison set of the superlative oldest spans from word 3 to word 7: [23] wsj00 1690 [] Scope: 3-7 The oldest bell-ringing group in the country, the Ancient Society of College Youths, founded in 1637, remains male-only, [] ." ></td>
	<td class="line x" title="92:155	(Bos and Nissim 2006) Bos and Nissims system, called DLA (Deep Linguistic Analysis), uses a wide-coverage parser to produce semantic representations of superlative sentences, which are then exploited to select the comparison set among attributive cases." ></td>
	<td class="line x" title="93:155	Compared with a baseline result, the results for this are very good, with an accuracy of 69%-83%." ></td>
	<td class="line x" title="94:155	The results are clearly very promising and show that comparison sets can be identified with high accuracy." ></td>
	<td class="line x" title="95:155	However, this only represents a first step towards the goal of the present work." ></td>
	<td class="line x" title="96:155	Apart from the superlative keyword oldest, the only information example [23] provides is that the comparison set spans from word 3 to word 7." ></td>
	<td class="line x" title="97:155	However, what would be interesting to know is that the target of the comparison appears in the same sentence and spans from word 9 to word 14 (the Ancient Society of College Youths)." ></td>
	<td class="line x" title="98:155	Furthermore, no analysis of the semantic roles of the constituents of the resulting string is carried out: We lose the information that the Ancient Society of College Youths IS-A kind of bell-ringing group, and that the set of bell-ringing groups is restricted in location (in the country)." ></td>
	<td class="line x" title="99:155	6 Applications The proposed work will be beneficial for a variety of areas in NLP, for example Question Answering (QA), Sentiment Detection/Opinion Extraction, Ontology Learning, or Natural Language Generation." ></td>
	<td class="line x" title="100:155	In this section I will discuss applications in the first two areas." ></td>
	<td class="line x" title="101:155	6.1 Question Answering In open-domain QA, the proposed work will be useful for answering two question types." ></td>
	<td class="line x" title="102:155	A superlative sentence like [24], found in a corpus, can be used to answer both a factoid question [25] and a definition question [26]: [24] A: The Nile is the longest river in the world." ></td>
	<td class="line x" title="103:155	[25] Q: What is the worlds longest river?" ></td>
	<td class="line x" title="104:155	[26] Q: What is the Nile?" ></td>
	<td class="line x" title="105:155	Here I will focus on the latter." ></td>
	<td class="line x" title="106:155	The common assumption that superlatives are useful with respect to answering definition questions is based on the observation that superlatives like the one in [24] both place an entity in a generalisation hierarchy, and distinguish it from its contrast set." ></td>
	<td class="line x" title="107:155	To investigate this assumption, I carried out a study involving the TREC QA other question nuggets3, which are snippets of text that contain relevant information for the definition of a specific topic." ></td>
	<td class="line x" title="108:155	In a recent study of judgement consistency (Lin and Demner-Fushman, 2006), relevant nuggets were judged as either 'vital' or 'okay' by 10 different judges rather than the single assessor standardly used in TREC." ></td>
	<td class="line x" title="109:155	For example, the first three nuggets for the topic Merck & Co. are: [27] Qid 75.8: 'other' question for target Merck & Co. 75.8 1 vital World's largest drug company." ></td>
	<td class="line x" title="110:155	75.8 2 okay Spent $1.68 billion on RandD in 1997." ></td>
	<td class="line x" title="111:155	75.8 3 okay Has experience finding new uses for established drugs." ></td>
	<td class="line x" title="112:155	(taken from TREC 2005; 'vital' and 'okay' reflect the opinion of the TREC evaluator)." ></td>
	<td class="line x" title="113:155	My investigation of the nugget judgements in Lin and Demner-Fushman's study yielded two in3 http://trec.nist.gov/data/qa.html 70 teresting results: First of all, a relatively high proportion of relevant nuggets contains superlatives: On average, there is one superlative nugget for at least half of the TREC topics." ></td>
	<td class="line x" title="114:155	Secondly, of 69 superlative nuggets altogether, 32 (i.e. almost half) are judged vital by more than 9 assessors." ></td>
	<td class="line x" title="115:155	Furthermore, I found that the nuggets can be distinguished by how the question target (i.e. the TREC topic, referred to as T1) relates to the superlative target (T2): In the first case, T1 and T2 coincide (referred to as class S1)." ></td>
	<td class="line x" title="116:155	In the second one, T2 is part of or closely related to T1, or T2 is part of the comparison set (class S2)." ></td>
	<td class="line x" title="117:155	In the third case, T1 is unrelated or only distantly related to T2 (S3)." ></td>
	<td class="line x" title="118:155	Table 1 shows examples of each class: T1 nugget (T2 in bold) S1 Merck & Co. World's largest drug company S2 Florence Nightingale Nightingale Medal highest international nurses award S3 Kurds Irbil largest city controlled by Kurds Table 1." ></td>
	<td class="line x" title="119:155	Examples of superlative nuggets." ></td>
	<td class="line x" title="120:155	Of the 69 nuggets containing superlatives, 46 fall into subclass S1, 15 into subclass S2 and 8 into subclass S3." ></td>
	<td class="line x" title="121:155	While I noted earlier that 32/69 (46%) of superlative-containing nuggets were judged vital by more than 9 assessors, these judgements are not equally distributed over the subclasses: Table 2 shows that 87% of S1 judgements are 'vital', while only 38% of S3 judgements are." ></td>
	<td class="line x" title="122:155	number of instances % of vital judgements % of okay judgements S1 46 87% 13% S2 15 59% 40% S3 8 38% 60% Table 2." ></td>
	<td class="line x" title="123:155	Ratings of the classes S1, S2, and S3." ></td>
	<td class="line x" title="124:155	These results strongly suggest that the presence of superlatives, and in particular S1 membership, is a good indicator of the importance of nuggets, and thus for answering definition questions." ></td>
	<td class="line x" title="125:155	Some experiments carried out in the framework of TREC 2006 (Kaisser et al. , 2006), however, showed that superlatives alone are not a winning indicator of nugget importance, but S1 membership may be." ></td>
	<td class="line x" title="126:155	A similar simple technique was used by Ahn et al.(2005) and by Razmara and Kosseim (2007)." ></td>
	<td class="line x" title="128:155	All just looked for the presence of a superlative and raised the score without further analysing the type of superlative or its role in the sentence." ></td>
	<td class="line x" title="129:155	This calls for a more sophisticated approach, where class S1 superlatives can be distinguished." ></td>
	<td class="line x" title="130:155	6.2 Sentiment Detection/Opinion Extraction Like adjectives and adverbs, superlatives can be objective or subjective." ></td>
	<td class="line x" title="131:155	Compare for example: [28] The Black Forest is the largest forest in Germany." ></td>
	<td class="line x" title="132:155	[objective] [29] The Black Forest is the most beautiful area in Germany." ></td>
	<td class="line nc" title="133:155	[subjective] So far, none of the studies in sentiment detection (e.g. Wilson et al. , 2005; Pang et al. , 2002) or opinion extraction (e.g. Hu and Liu, 2004; Popescu and Etzioni, 2005) have specifically looked at the role of superlatives in these areas." ></td>
	<td class="line x" title="134:155	Like subjective adjectives, subjective superlatives can either express positive or negative opinions." ></td>
	<td class="line x" title="135:155	This polarity depends strongly on the adjective or adverb that the superlative is derived from.4 As superlatives place the adjective or adverb at the highest or lowest point of the comparison scale (cf.Section 2), the question of interest is how this affects the polarity of the adjective/adverb." ></td>
	<td class="line x" title="137:155	If the intensity of the polarity increases in a likewise manner, then subjective superlatives are bound to express the strongest or weakest opinions possible." ></td>
	<td class="line x" title="138:155	If this hypothesis holds true, an extreme opinion extraction system could be created by combining the proposed superlative extraction system with a subjectivity recognition system that can identify subjective superlatives." ></td>
	<td class="line x" title="139:155	This would clearly be of interest to many companies and market researchers." ></td>
	<td class="line x" title="140:155	Initial searches in Hu and Lius annotated corpus of customer reviews (2004) look promising." ></td>
	<td class="line x" title="141:155	Sentences in this corpus are annotated with information about positive and negative opinions, which are located on a six-point scale, where [+/-3] stand for the strongest positive/negative opinions, and [+/-1] stand for the weakest positive/negative opinions." ></td>
	<td class="line x" title="142:155	A search for annotated sentences containing superlatives shows that an overwhelming majority are marked with strongest opinion labels." ></td>
	<td class="line x" title="143:155	7 Summary and Future Work This paper proposed the task of automatically extracting useful information from superlatives oc4 It may, however, also depend on whether the superlative expresses the highest ('most') or the lowest ('least') point in the scale." ></td>
	<td class="line x" title="144:155	71 curring in free text." ></td>
	<td class="line x" title="145:155	It provided an overview of superlative constructions and the main challenges that have to be faced, described previous computational approaches and their limitations, and discussed applications in two areas in NLP: QA and Sentiment Detection/Opinion Extraction." ></td>
	<td class="line x" title="146:155	The proposed task can be seen as consisting of three subtasks: TASK 1: Decide whether a given sentence contains a superlative form TASK 2: Given a sentence containing a superlative form, identify what type of superlative it is (initially: IS-A superlative or not)?" ></td>
	<td class="line x" title="147:155	TASK 3: For set comparisons, identify the target and the comparison set, as well as the superlative relation Task 1 can be tackled by a simple approach relying on POS tags (e.g. JJS and RBS in the Penn Treebank tagset)." ></td>
	<td class="line x" title="148:155	For Task 2, I have carried out a thorough analysis of the different types of superlative forms and postulated a new classification for them." ></td>
	<td class="line x" title="149:155	My present efforts are on the creation of a gold standard data set for the extraction task." ></td>
	<td class="line x" title="150:155	As superlatives are particularly frequent in encyclopaedic language (cf.Section 3), I am considering using the Wikipedia5 as a knowledge base." ></td>
	<td class="line x" title="152:155	The main challenge is to devise a suitable annotation scheme which can account for all syntactic structures in which IS-A superlatives occur and which incorporates their semantic properties in an adequate way (semantic role labelling)." ></td>
	<td class="line x" title="153:155	Finally, for Task 3, I plan to use both manually created rules and machine learning techniques." ></td>
	<td class="line x" title="154:155	Acknowledgements I would like to thank Bonnie Webber and Maria Milosavljevic for their helpful comments and suggestions on this paper." ></td>
	<td class="line x" title="155:155	Many thanks also go to Nitin Jindal and Bing Liu, Johan Bos and Malvina Nissim, and Jimmy Lin and Dina DemnerFushman for making their data available." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-1515
Annotating Expressions of Appraisal in English
Read, Jonathon;Hope, David;Carroll, John A.;"></td>
	<td class="line x" title="1:159	Proceedings of the Linguistic Annotation Workshop, pages 93100, Prague, June 2007." ></td>
	<td class="line x" title="2:159	c2007 Association for Computational Linguistics Annotating Expressions of Appraisal in English Jonathon Read, David Hope and John Carroll Department of Informatics University of Sussex United Kingdom {j.l.read,drh21,j.a.carroll}@sussex.ac.uk Abstract The Appraisal framework is a theory of the languageofevaluation, developedwithinthe tradition of systemic functional linguistics." ></td>
	<td class="line x" title="3:159	The framework describes a taxonomy of the types of language used to convey evaluation and position oneself with respect to the evaluations of other people." ></td>
	<td class="line x" title="4:159	Accurate automatic recognition of these types of language can inform an analysis of document sentiment." ></td>
	<td class="line x" title="5:159	This paper describes the preparation of test data for algorithms for automatic Appraisal analysis." ></td>
	<td class="line x" title="6:159	The difficulty of the task is assessed by way of an inter-annotator agreement study, based on measures analogous to those used in the MUC-7 evaluation." ></td>
	<td class="line x" title="7:159	1 Introduction The Appraisal framework (Martin and White, 2005) describes a taxonomy of the language employed in communicating evaluation, explaining how users of Englishconveyattitude(emotion, judgementofpeople and appreciation of objects), engagement (assessment of the evaluations of other people) and how writers may modify the strength of their attitude/engagement." ></td>
	<td class="line oc" title="8:159	Accurate automatic analysis of these aspects of language will augment existing research in the fields of sentiment (Pang et al. , 2002) andsubjectivityanalysis(Wiebeetal., 2004), butassessing the usefulness of analysis algorithms leveragingtheAppraisalframeworkwillrequiretestdata." ></td>
	<td class="line x" title="10:159	At present there are no machine-readable Appraisal-annotated texts publicly available." ></td>
	<td class="line x" title="11:159	Realworld instances of Appraisal in use are limited to example extracts that demonstrate the theory, coming from a wide variety of genres as disparate as news reporting (White, 2002; Martin, 2004) and poetry (Martin and White, 2005)." ></td>
	<td class="line x" title="12:159	These examples, while useful in demonstrating the various aspects of Appraisal, can only be employed in a qualitative analysis and would bring about inconsistencies if analysed collectively  one can expect the writing style to depend upon the genre, resulting in significantly different syntactic constructions and lexical choices." ></td>
	<td class="line x" title="13:159	We therefore need to examine Appraisal across documents in the same genre and investigate patterns within that particular register." ></td>
	<td class="line x" title="14:159	This paper discusses the methodology of an Appraisal annotation study and an analysis of the inter-annotator agreement exhibited by two human judges." ></td>
	<td class="line x" title="15:159	The output of this study has the additional benefit of bringing a set of machine-readable annotations of Appraisal into the public domain for further research." ></td>
	<td class="line x" title="16:159	This paper is structured as follows." ></td>
	<td class="line x" title="17:159	The next section offers an overview of the Appraisal framework." ></td>
	<td class="line x" title="18:159	Section 3 discusses the methodology adopted for the annotation study." ></td>
	<td class="line x" title="19:159	Section 4 discusses the measures employed to assess inter-annotator agreement and reports the results of these measures." ></td>
	<td class="line x" title="20:159	Section 5 offers an analysis of cases of systematic disagreement." ></td>
	<td class="line x" title="21:159	Other computational work utilising the Appraisal framework is reviewed in Section 6." ></td>
	<td class="line x" title="22:159	Section 7 summarises the paper and outlines future work." ></td>
	<td class="line x" title="23:159	2 The linguistic framework of Appraisal The Appraisal framework (Martin and White, 2005) is a development of work in Systemic Functional 93 appraisal attitude engagement graduation affect judgement appreciation inclination happiness security satisfaction esteem sanction normality capacity tenacity veracity propriety reaction composition valuation impact quality balance complexity contract expand disclaim proclaim deny counter pronounce endorse concur affirm concede entertain attribute acknowledge distance force focus quantification intensification number mass extent proximity (space) proximity (time) distribution (space) distribution (time) degree vigour Figure 1: The Appraisal framework." ></td>
	<td class="line x" title="24:159	Linguistics (Halliday, 1994) and is concerned with interpersonal meaning in textthe negotiation of social relationships by communicating emotion, judgement and appreciation." ></td>
	<td class="line x" title="25:159	The taxonomy described by the Appraisal framework is depicted in Figure 1." ></td>
	<td class="line x" title="26:159	Appraisal consists of three subsystems that operate in parallel: attitude looks at how one expresses private state (Quirk et al. , 1985) (ones emotion and opinions); engagement considers the positioning of oneself with respect to the opinions of others and graduation investigates how the use of language functions to amplify or diminish the attitude and engagement conveyed by a text." ></td>
	<td class="line x" title="27:159	2.1 Attitude: emotion, ethics and aesthetics TheAttitudesub-systemdescribesthreeareasofprivate state: emotion, ethics and aesthetics." ></td>
	<td class="line x" title="28:159	An attitude is further qualified by its polarity (positive or negative)." ></td>
	<td class="line x" title="29:159	Affect identifies feelingsauthors emotions as represented by their text." ></td>
	<td class="line x" title="30:159	Judgement deals with authors attitude towards the behaviour of people; how authors applaud or reproach the actions of others." ></td>
	<td class="line x" title="31:159	Appreciation considers the evaluation of thingsboth man-made and natural phenomena." ></td>
	<td class="line x" title="32:159	2.2 Engagement: appraisals of appraisals Through engagement, Martin and White (2005) deal with the linguistic constructions by which authors construe their point of view and the resources used to adopt stances towards the opinions of other people." ></td>
	<td class="line x" title="33:159	The theory of engagement follows Stubbs (1996) in that it assumes that all utterances convey point of view and Bakhtin (1981) in supposing that all utterances occur in a miscellany of other utterances on the same motif, and that they carry both implicit and explicit responses to one another." ></td>
	<td class="line x" title="34:159	In other words, all text is inherently dialogistic as it encodesauthorsreactionstotheirexperiences(includingpreviousinteractionwithotherwriters)." ></td>
	<td class="line x" title="35:159	Engagementcanbebothretrospective(thatis,anauthorwill acknowledge and agree or disagree with the stances of others who have previously appraised a subject), andprospective(onemayanticipatetheresponsesof an intended audience and include counter-responses in the original text)." ></td>
	<td class="line x" title="36:159	2.3 Graduation: strength of evaluations Martin and White (2005) consider the resources by which writers alter the strength of their evaluation as a system of graduation." ></td>
	<td class="line x" title="37:159	Graduation is a general property of both attitude and engagement." ></td>
	<td class="line x" title="38:159	In attitude it enables authors to convey greater or lesser degrees of positivity or negativity, while graduation of engagements scales authors conviction in their utterance." ></td>
	<td class="line x" title="39:159	Graduation is divided into two subsystems." ></td>
	<td class="line x" title="40:159	Force alters appraisal propositions in terms of its inten94 sity, quantity or temporality, or by means of spatial metaphor." ></td>
	<td class="line x" title="41:159	Focus considers the resolution of semantic categories, for example: They play real jazz." ></td>
	<td class="line x" title="42:159	They play jazz, sort of." ></td>
	<td class="line x" title="43:159	In real terms a musician either plays jazz or they do not, but these examples demonstrate how authors blur the lines of semantic sets and how binary relationships can be turned into scalar ones." ></td>
	<td class="line x" title="44:159	3 Annotation methodology The corpus used in this study consists of unedited book reviews." ></td>
	<td class="line x" title="45:159	Book reviews are good candidates for this study as, while they are likely to contain similar language by virtue of being from the same genre of writing, we can also expect examples of Appraisals many classes (for example, the emotion attributed to the characters in reviews of novels, judgements of authors competence and character, appreciation of the qualities of books and engagement with the propositions put forth by the authors under review)." ></td>
	<td class="line x" title="46:159	The articles were taken from the web sites of four British newspapers (The Guardian, The Independent, The Telegraph and The Times) on two different dates31 July 2006 and 11 September 2006." ></td>
	<td class="line x" title="47:159	Each review is attributed to a unique author." ></td>
	<td class="line x" title="48:159	The corpus is comprised of 38 documents, containing a total of 36,997 tokens in 1,245 sentences." ></td>
	<td class="line x" title="49:159	Two human annotators, d and j, participated in this study, assigning tags independently." ></td>
	<td class="line x" title="50:159	The annotators were well-versed in the Appraisal framework, having studied the latest literature." ></td>
	<td class="line x" title="51:159	The judges were asked to annotate appraisal-bearing terms with the appraisal type presumed to be intended by the author of the text." ></td>
	<td class="line x" title="52:159	They were asked to highlight each example of appraisal and specify the type of attitude, engagement or graduation present." ></td>
	<td class="line x" title="53:159	They also assigned a polarity (positive or negative) to attitudinal items and a scaling (up or down) to graduating items, employing a custom-developed software tool to annotate the documents." ></td>
	<td class="line x" title="54:159	Four alternative annotation strategies were considered." ></td>
	<td class="line x" title="55:159	Oneapproachistoallowonlyasingletoken per annotation." ></td>
	<td class="line x" title="56:159	However, this is too simplistic for an Appraisal annotation studya unit of Appraisal is frequently larger than a single token." ></td>
	<td class="line x" title="57:159	Consider the following examples: (1) The design was deceptivelyVERACITY simple COMPLEXITY." ></td>
	<td class="line x" title="58:159	() (2) The design was deceptively simpleCOMPLEXITY." ></td>
	<td class="line x" title="59:159	Example 1 demonstrates that a single-token approach is inappropriate as it ascribes a judgement of someones honesty, whereas Example 2 indicates the correct analysisthe sentence is an appreciation of the simplicity of the design." ></td>
	<td class="line x" title="60:159	This example shows how it is necessary to annotate larger units of appraisal-bearing language." ></td>
	<td class="line x" title="61:159	Including more tokens, however, increases the complexity of the annotation task, and reduces the likelihood of agreement between the judges, as the annotated tokens of one judge may be a subset of, or overlap with, those of another." ></td>
	<td class="line x" title="62:159	We therefore experimented with tagging entire sentences in order to constrain the annotators range of choices." ></td>
	<td class="line x" title="63:159	This resultedinitsownproblemsasthereisoftenmorethan one appraisal in a sentence, for example: (3) The design was deceptively simpleCOMPLEXITY and belied his ingenuityCAPACITY." ></td>
	<td class="line x" title="64:159	An alternative approach is to permit annotators to tag an arbitrary number of contiguous tokens." ></td>
	<td class="line x" title="65:159	Arbitrary-length tagging is disadvantageous as the judges will frequently tag units of differing length, but this can be compensated for by relaxing the rules for agreementfor example, by allowing intersecting annotations to match successfully (Wiebe et al. , 2005)." ></td>
	<td class="line x" title="66:159	Bruce and Wiebe (1999) employ another approach, creating units from every non-compound sentence and each conjunct of every compound sentence." ></td>
	<td class="line x" title="67:159	This side-steps the problem of ambiguity in appraisalunitlength,butwillstillfailtocaptureboth appraisals demonstrated in the second conjunct of Example 4." ></td>
	<td class="line x" title="68:159	(4) The design was deceptively simpleCOMPLEXITY and belied his remarkableNORMALITY ingenuityCAPACITY." ></td>
	<td class="line x" title="69:159	Ultimately in this study, we permitted judges to annotate any number of tokens in order to allow for multiple Appraisal units of differing sizes within sentences." ></td>
	<td class="line x" title="70:159	Annotation was carried out over two rounds, punctuated by an intermediary analysis of 95 d j d j d j Inclination 1.26 3.50 Balance 2.64 1.84 Distance 0.69 0.59 Happiness 2.80 2.32 Complexity 2.52 2.74 Number 0.82 2.63 Security 4.31 2.22 Valuation 6.08 9.29 Mass 0.22 1.63 Satisfaction 1.67 2.32 Deny 3.05 3.67 Proximity (Space) 0.09 0.14 Normality 8.00 4.44 Counter 4.79 3.78 Proximity (Time) 0.03 0.55 Capacity 11.46 9.63 Pronounce 3.84 1.21 Distribution (Space) 0.41 1.39 Tenacity 3.72 4.44 Endorse 2.05 1.49 Distribution (Time) 0.82 2.56 Veracity 3.15 2.01 Affirm 0.54 1.14 Degree 4.38 5.72 Propriety 13.32 12.61 Concede 0.38 0.03 Vigour 0.60 0.45 Impact 6.11 4.23 Entertain 2.27 2.43 Focus 3.02 2.29 Quality 2.55 3.40 Acknowledge 2.42 3.33 Table 1: The distribution of the Appraisal types selected by each annotator (%)." ></td>
	<td class="line x" title="71:159	d j Documents 115.74 77.21 Sentences 3.65 2.43 Words 0.12 0.08 Table 2: The density of annotations relative to the number of documents, sentences and words." ></td>
	<td class="line x" title="72:159	agreement and disagreement between the two annotators." ></td>
	<td class="line x" title="73:159	The judges discussed examples of the most common types of disagreement in an attempt to acquireacommonunderstandingforthesecondround, but annotations from the first round were left unaltered." ></td>
	<td class="line x" title="74:159	Following the methodology described above, d made 3,176 annotations whilst j made 2,886 annotations." ></td>
	<td class="line x" title="75:159	The distribution of the Appraisal types ascribed is shown in Table 1, while Table 2 details the density of annotations in documents, sentences and words." ></td>
	<td class="line x" title="76:159	4 Measuring inter-annotator agreement The study of inter-annotator agreement begins by considering the level of agreement exhibited by the annotators in deciding which tokens are representative of Appraisal, irrespective of the type." ></td>
	<td class="line x" title="77:159	As discussed, this is problematic as judges are liable to choose different length token spans when marking up what is essentially the same appraisal, as demonstrated by Example 5." ></td>
	<td class="line x" title="78:159	(5) [d] It is tempting to point to the bombs in London and elsewhere, to the hideous messQUALITY in Iraq, to recent victories of the Islamists, to theviolent and polarised rhetoricPROPRIETY and answer yes." ></td>
	<td class="line x" title="79:159	[j] It is tempting to point to the bombs in London and elsewhere, to the hideousQUALITY messBALANCE in Iraq, to recent victories of Islamists, to the violentPROPRIETY and polarised PROPRIETY rhetoric and answer yes." ></td>
	<td class="line x" title="80:159	Wiebe et al.(2005), who faced this problem when annotating expressions of opinion under their own framework, acceptthatitisnecessarytoconsiderthe validity of all judges interpretations and therefore consider intersecting annotations (such as hideous and hideous mess) to be matches." ></td>
	<td class="line x" title="82:159	The same relaxation of constraints is employed in this study." ></td>
	<td class="line x" title="83:159	Tasks with a known number of annotative units can be analysed with measures of agreement such as Cohens  Coefficient (1960), but the judges freedom in this task prohibits meaningful application of thismeasure." ></td>
	<td class="line x" title="84:159	Forexample,considerhowwordsense annotatorsareobligedtochoosefromalimitedfixed set of senses for each token, whereas judges annotating Appraisal are free to select one of thirty-two classes for any contiguous substring of any length within each document; there are 16parenleftbign2  nparenrightbig possible choices in a document of n tokens (approximately 6.5  108 possibilities in this corpus)." ></td>
	<td class="line x" title="85:159	A wide range of evaluation metrics have been employed by the Message Understanding Conferences (MUCs)." ></td>
	<td class="line x" title="86:159	The MUC-7 tasks included extraction of named entities, equivalence classes, attributes, facts and events (Chinchor, 1998)." ></td>
	<td class="line x" title="87:159	The participating systems were evaluated using a variety of related measures, defined in Table 3." ></td>
	<td class="line x" title="88:159	These tasks are similar to Appraisal annotation in that the units are formed of an arbitrary number of contiguous tokens." ></td>
	<td class="line x" title="89:159	In this study the agreement exhibited by an annotator a is evaluated as a pair-wise comparison against the other annotator b. Annotator b provides 96 COR Number correct INC Number incorrect MIS Number missing SPU Number spurious POS Number possible = COR + INC + MIS ACT Number actual = COR + INC + SPU FSC F-score = (2  REC  PRE) /(REC + PRE) REC Precision = COR/POS PRE Recall = COR/ACT SUB Substitution = INC/(COR + INC) ERR Error per response = (INC + SPU + MIS) /(COR + INC + SPU + MIS) UND Under-generation = MIS/POS OVG Over-generation = SPU/ACT Table 3: MUC-7 score definitions (Chinchor 1998)." ></td>
	<td class="line x" title="90:159	FSC REC PRE ERR UND OVG d 0.682 0.706 0.660 0.482 0.294 0.340 j 0.715 0.667 0.770 0.444 0.333 0.230 x 0.698 0.686 0.711 0.462 0.312 0.274 Table 4: MUC-7 test scores, evaluating the agreement in text anchors selected by the annotators." ></td>
	<td class="line x" title="91:159	x denotes the average value, calculated using the harmonic mean." ></td>
	<td class="line x" title="92:159	a presumed gold standard for the purposes of evaluating agreement." ></td>
	<td class="line x" title="93:159	Note, however, that in this case it does not necessarily follow that REC(a w.r.t. b) = PRE(b w.r.t. a)." ></td>
	<td class="line x" title="94:159	Consider that a may tend to make one-word annotations whilst b prefers to annotate phrases; the set of as annotations will contain multiple matches for some of the phrases annotated by b (refer to Example 5, for instance)." ></td>
	<td class="line x" title="95:159	The number correct will differ for each annotator in the pair under evaluation." ></td>
	<td class="line x" title="96:159	Table 4 lists the values for the MUC-7 measures applied to the text spans selected by the annotators." ></td>
	<td class="line x" title="97:159	Annotator d is inclined to identify text as Appraisal more frequently than annotator j. This results in higher recall for d, but with lower precision." ></td>
	<td class="line x" title="98:159	Naturally, the opposite observation can be made about annotator j. Both annotators exhibit a high error rate at 48.2% and 44.4% for d and j respectively." ></td>
	<td class="line x" title="99:159	The substitution rate is not listed as there are no classes to substitute when considering only text anchor agreement." ></td>
	<td class="line x" title="100:159	The second round of annotation achieved slightly higher agreement (the mean F-score increased by 0.033)." ></td>
	<td class="line x" title="101:159	FSC REC PRE SUB ERR 0 0.698 0.686 0.711 0.000 0.462 1 0.635 0.624 0.647 0.090 0.511 2 0.528 0.518 0.538 0.244 0.594 3 0.448 0.441 0.457 0.357 0.655 4 0.396 0.388 0.403 0.433 0.696 5 0.395 0.388 0.403 0.433 0.696 Table 5: Harmonic means of the MUC-7 test scores evaluating the agreement in text anchors and Appraisal classes selected by the annotators, at each level of hierarchical abstraction." ></td>
	<td class="line x" title="102:159	Having considered the annotators agreement with respect to text anchors, we go on to analyse the agreement exhibited by the annotators with respect to the types of Appraisal assigned to the text anchors." ></td>
	<td class="line x" title="103:159	The Appraisal framework is a hierarchical systema tree with leaves corresponding to the annotation types chosen by the judges." ></td>
	<td class="line x" title="104:159	When investigating agreement in Appraisal type, the following measures include not just the leaf nodes but also their parent types, collapsing the nodes into increasingly abstract representations." ></td>
	<td class="line x" title="105:159	For example happiness is a kind of affect, which is a kind of attitude, which is a kind of appraisal." ></td>
	<td class="line x" title="106:159	These relationships are depicted in full in Figure 2." ></td>
	<td class="line x" title="107:159	Note that in the following measurements of inter-annotator agreement leaf nodes are included in subsequent levels (for example, focus is a leaf node at level 2, but is also considered to be a member of levels 3, 4 and 5)." ></td>
	<td class="line x" title="108:159	Table 5 shows the harmonic means of the MUC7 measures of the annotators agreement at each of the levels depicted in Figure 2." ></td>
	<td class="line x" title="109:159	As one might expect, the agreement steadily drops as the classes become more concreteclasses become more specific and more numerous so the complexity of the task increases." ></td>
	<td class="line x" title="110:159	Table 5 also lists the average rate of substitutions as the annotation tasks complexity increases, showing that the annotators were able to fairly easily distinguish between instances of the three subsystems of Appraisal (Attitude, Engagement and Graduation) as the substitution rate at level 1 is low (only 9%)." ></td>
	<td class="line x" title="111:159	Asthenumberofpossibleclassesincreasesannotators are more likely to confuse appraisal types, with disagreement occurring on approximately 44% of annotations at level 5." ></td>
	<td class="line x" title="112:159	The second round of annotations resulted in slightly improved agreement at 97 Level 0:.698 Level 1: .635 Level 2: .528 Level 3: .448 Level 4: .396 Level 5: .395 appraisal attitude: .701 engagement: .507 graduation: .479 affect: .519 judgement: .586 appreciation: .567 contract: .502 expand: .445 force: .420 focus: .287 inclination: .249 happiness: .448 security: .335 satisfaction: .374 esteem: .489 sanction: .575 reaction: .510 composition: .432 valuation: .299 disclaim: .555 proclaim: .336 entertain: .459 attribute: .427 quantification: .233 intensification: .513 normality: .289 capacity: .431 tenacity: .395 veracity: .519 propriety: .540 impact: .462 quality: .336 balance: .300 complexity: .314 deny: .451 counter: .603 pronounce: .195 endorse: .331 concur: .297 acknowledge: .390 distance: .415 number: .191 mass: .104 extent: .242 degree: .510 vigour: .117 affirm: .325 concede: .000 proximity (space): .000 proximity (time): .000 distribution (space): .110 distribution (time): .352 Figure 2: The Appraisal framework with hierarchical levels highlighted." ></td>
	<td class="line x" title="113:159	Appraisal classes and levels are accompanied by the harmonic mean of the F-scores of the annotators for that class/level." ></td>
	<td class="line x" title="114:159	eachlevelofabstraction(themeanF-scoreincreased by 0.051 at the most abstract level)." ></td>
	<td class="line x" title="115:159	Of course, some Appraisal classes are easier to identify than others." ></td>
	<td class="line x" title="116:159	Figure 2 summarises the agreement for each node in the Appraisal hierarchy with the harmonic mean of the F-scores of the annotators for each class." ></td>
	<td class="line x" title="117:159	Typically, the attitude annotations are easiest to identify, whereas the other subsystems of engagementandgraduationtendtobemoredifficult." ></td>
	<td class="line x" title="118:159	The Proximity children of Extent exhibited no agreement whatsoever." ></td>
	<td class="line x" title="119:159	This seems to have arisen from the differences in the judges interpretations of proximity." ></td>
	<td class="line x" title="120:159	In the case of Proximity (Space), for example, one judge annotated words that function to modify the spatial distance of other concepts (e.g. near), whereastheotherselectedwordsplacingconcepts at a specific location (e.g. homegrown, local)." ></td>
	<td class="line x" title="121:159	This confusion between modifying words and spe98 cific locations also accounts for the low agreement in the Distribution (Space) type." ></td>
	<td class="line x" title="122:159	The measures show that it is also difficult to achieve a consensus on what qualifies as engagements of the Pronounce type." ></td>
	<td class="line x" title="123:159	Both annotators select expressions that assert the irrefutability of a proposition (e.g. certainly or in fact or it has to be said)." ></td>
	<td class="line x" title="124:159	Judge d, however, tends to perceive pronouncement as occurring wherever the author makes an assertion (e.g. this is or there will be)." ></td>
	<td class="line x" title="125:159	Judge j seems to require that the assertion carry a degree of emphasis to include a term in the Pronounce class." ></td>
	<td class="line x" title="126:159	The low agreement of the Mass graduations can also be explained in this way, as both d and j select strong expressions relating to size (e.g. massive or scant)." ></td>
	<td class="line x" title="127:159	Annotator j found additional but weaker terms like largely or slightly." ></td>
	<td class="line x" title="128:159	The Pronounce and Mass classes provide typical examples of the disagreement exhibited by the annotators." ></td>
	<td class="line x" title="129:159	It is not that the judges have wildly differentunderstandingsofthesystem, butrathertheydisagree in the bounds of a classone annotator may require a greater degree of strength of a term to warrant its inclusion in a class." ></td>
	<td class="line x" title="130:159	Contingency tables (not depicted due to space constraints) reveal some interesting tendencies for confusion between the two annotators." ></td>
	<td class="line x" title="131:159	Approximately 33% of ds annotations of Proximity (Space) were ascribed as Capacity by j. The high percentage is due to the rarity of annotations of Proximity (Space), but the confusion comes from differing units of Appraisal, as shown in Example 6." ></td>
	<td class="line x" title="132:159	(6) [d] But at key points in this story, one gets the feeling that the essential factors are operating just outsidePROXIMITY (SPACE) Jamess field of visionCAPACITY." ></td>
	<td class="line x" title="133:159	[j] But at key points in this story, one gets the feeling that the essential factors are operating just outside Jamess field of visionCAPACITY." ></td>
	<td class="line x" title="134:159	Another interesting case of frequent confusion is the pair of Satisfaction and Propriety." ></td>
	<td class="line x" title="135:159	Though not closely related in the Attitude subsystem, j chooses Propriety for 21% of ds annotations of Satisfaction." ></td>
	<td class="line x" title="136:159	The confusion is typified by Example 7, where it is apparent that there is disagreement in terms of who is being appraised." ></td>
	<td class="line x" title="137:159	(7) [d] Like him, Vermeer  or so he chose to believe  was an artist neglectedSATISFACTION and wrongedSATISFACTION by critics and who had died an almost unknown." ></td>
	<td class="line x" title="138:159	[j] Like him, Vermeer  or so he chose to believe wasanartistneglected and wrongedPROPRIETY by critics and who had died an almost unknown." ></td>
	<td class="line x" title="139:159	Annotator d believes that the author is communicating the artists dissatisfaction with the way he is treated by critics, whereas j believes that the critics are being reproached for their treatment of the artist." ></td>
	<td class="line x" title="140:159	This highlights a problem with the coding scheme, which simplifies the task by assuming only one type of Appraisal is conveyed by each unit." ></td>
	<td class="line x" title="141:159	5 Related work Taboada and Grieve (2004) initiated computational experimentation with the Appraisal framework, assigning adjectives into one of the three broad attitudeclasses." ></td>
	<td class="line x" title="142:159	TheauthorsapplySO-PMI-IR(Turney, 2002) to extract and determine the polarity of adjectives." ></td>
	<td class="line x" title="143:159	They then use a variant of SO-PMI-IR to determine a potential value for affect, judgement and appreciation, calculating the mutual information between the adjective and three pronoun-copular pairs: I was (affect); he was (judgement) and it was (appreciation)." ></td>
	<td class="line x" title="144:159	While the pairs seem compelling markers of the respective attitude types, they incorrectly assume that appraisals of affect are limited to the first person whilst judgements are made only of the third person." ></td>
	<td class="line x" title="145:159	We can expect a high degree of overlap between the sets of documents retrieved by queries formed using these pairs (e.g. I was a happy X; he was a happy X; It was a happy X)." ></td>
	<td class="line x" title="146:159	Whitelaw et al.(2005) use the Appraisal framework to specify frames of sentiment." ></td>
	<td class="line x" title="148:159	These Appraisal Groups are derived from aspects of Attitude and Graduation: Attitude: affect | judgement | appreciation Orientation positive | negative Force: low | neutral | high Focus: low | neutral | high Polarity: marked | unmarked Their process begins with a semi-automatically constructed lexicon of these Appraisal groups, built using example terms from Martin and White (2005) as seeds into WordNet synsets." ></td>
	<td class="line x" title="149:159	The frames supplement bag of words-based machine learning techniques for 99 sentiment analysis and they achieve minor improvements over unigram features." ></td>
	<td class="line x" title="150:159	6 Summary This paper has discussed the methodology of an exercise annotating book reviews according to the Appraisal framework, a functional linguistic theory of evaluation in English." ></td>
	<td class="line x" title="151:159	The agreement exhibited by two human judges was measured by analogy with theevaluationemployedfortheMUC-7sharedtasks (Chinchor, 1998)." ></td>
	<td class="line x" title="152:159	The agreement varied greatly depending on the level of abstraction in the Appraisal hierarchy (a mean F-score of 0.698 at the most abstract level through to 0.395 at the most concrete level)." ></td>
	<td class="line x" title="153:159	The agreement also depended on the type being annotatedthere was more agreement evident for types of attitude compared to types of engagement or graduation." ></td>
	<td class="line x" title="154:159	The exercise is the first step in an ongoing study of approaches for the automatic analysis of expressions of Appraisal." ></td>
	<td class="line x" title="155:159	The primary output of this work is a corpus of book reviews independently annotated with Appraisal types by two coders." ></td>
	<td class="line x" title="156:159	Agreement was in general low, but if one assumes that the intersection of both sets of annotations contains reliable examples, this leaves 2,223 usable annotations." ></td>
	<td class="line x" title="157:159	Future work will employ these annotations to evaluate algorithms for the analysis of Appraisal, and investigate the usefulness of the Appraisal framework when in the computational analysis of document sentiment and subjectivity." ></td>
	<td class="line x" title="158:159	Acknowledgments We would like to thank Bill Keller for advice when designing the annotation methodology." ></td>
	<td class="line x" title="159:159	The work of the first author is supported by a UK EPSRC studentship." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1031
Mining Opinions in Comparative Sentences
Ganapathibhotla, Murthy;Liu, Bing;"></td>
	<td class="line x" title="1:290	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 241248 Manchester, August 2008 Mining Opinions in Comparative Sentences Murthy Ganapathibhotla Department of Computer Science University of Illinois at Chicago 851 South Morgan Street Chicago, IL 60607-7053 sganapat@cs.uic.edu Bing Liu Department of Computer Science University of Illinois at Chicago 851 South Morgan Street Chicago, IL 60607-7053 liub@cs.uic.edu   Abstract This paper studies sentiment analysis from the user-generated content on the Web." ></td>
	<td class="line x" title="2:290	In particular, it focuses on mining opinions from comparative sentences, i.e., to determine which entities in a comparison are preferred by its author." ></td>
	<td class="line x" title="3:290	A typical comparative sentence compares two or more entities." ></td>
	<td class="line x" title="4:290	For example, the sentence, the picture quality of Camera X is better than that of Camera Y, compares two entities Camera X and Camera Y with regard to their picture quality." ></td>
	<td class="line x" title="5:290	Clearly, Camera X is the preferred entity." ></td>
	<td class="line x" title="6:290	Existing research has studied the problem of extracting some key elements in a comparative sentence." ></td>
	<td class="line x" title="7:290	However, there is still no study of mining opinions from comparative sentences, i.e., identifying preferred entities of the author." ></td>
	<td class="line x" title="8:290	This paper studies this problem, and proposes a technique to solve the problem." ></td>
	<td class="line x" title="9:290	Our experiments using comparative sentences from product reviews and forum posts show that the approach is effective." ></td>
	<td class="line x" title="10:290	1 Introduction In the past few years, there was a growing interest in mining opinions in the user-generated content (UGC) on the Web, e.g., customer reviews, forum posts, and blogs." ></td>
	<td class="line oc" title="11:290	One major focus is sentiment classification and opinion mining (e.g., Pang et al 2002; Turney 2002; Hu and Liu 2004; Wilson et al 2004; Kim and Hovy 2004; Popescu and Etzioni 2005)   2008." ></td>
	<td class="line x" title="12:290	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/)." ></td>
	<td class="line x" title="13:290	Some rights reserved." ></td>
	<td class="line n" title="14:290	However, these studies mainly center on direct opinions or sentiments expressed on entities." ></td>
	<td class="line x" title="15:290	Little study has been done on comparisons, which represent another type of opinion-bearing text." ></td>
	<td class="line x" title="16:290	Comparisons are related to but are also quite different from direct opinions." ></td>
	<td class="line x" title="17:290	For example, a typical direct opinion sentence is the picture quality of Camera X is great, while a typical comparative sentence is the picture quality of Camera X is better than that of Camera Y. We can see that comparisons use different language constructs from direct opinions." ></td>
	<td class="line x" title="18:290	A comparison typically expresses a comparative opinion on two or more entities with regard to their shared features or attributes, e.g., picture quality." ></td>
	<td class="line x" title="19:290	Although direct opinions are most common in UGC, comparisons are also widely used (about 10% of the sentences), especially in forum discussions where users often ask questions such as X vs. Y (X and Y are competing products)." ></td>
	<td class="line x" title="20:290	Discussions are then centered on comparisons." ></td>
	<td class="line x" title="21:290	Jindal and Liu (2006) proposed a technique to identify comparative sentences from reviews and forum posts, and to extract entities, comparative words, and entity features that are being compared." ></td>
	<td class="line x" title="22:290	For example, in the sentence, Camera X has longer battery life than Camera Y, the technique extracts Camera X and Camera Y as entities, and longer as the comparative word and battery life as the attribute of the cameras being compared." ></td>
	<td class="line x" title="23:290	However, the technique does not find which entity is preferred by the author." ></td>
	<td class="line x" title="24:290	For this example, clearly Camera Y is the preferred camera with respect to the battery life of the cameras." ></td>
	<td class="line x" title="25:290	This paper aims to solve this problem, which is useful in many applications because the preferred entity is the key piece of information in a comparative opinion." ></td>
	<td class="line x" title="26:290	For example, a potential customer clearly wants to buy the product that is better or preferred." ></td>
	<td class="line x" title="27:290	In this work, we treat a sentence as the basic 241 information unit." ></td>
	<td class="line x" title="28:290	Our objective is thus to identify the preferred entity in each comparative sentence." ></td>
	<td class="line x" title="29:290	A useful observation about comparative sentences is that in each such sentence there is usually a comparative word (e.g., better, worse and er word) or a superlative word (e.g., best, worst and est word)." ></td>
	<td class="line x" title="30:290	The entities being compared often appear on the two sides of the comparative word." ></td>
	<td class="line x" title="31:290	A superlative sentence may only have one entity, e.g., Camera X is the best." ></td>
	<td class="line x" title="32:290	For simplicity, we use comparative words (sentences) to mean both comparative words (sentences) and superlative words (sentences)." ></td>
	<td class="line x" title="33:290	Clearly, the preferred entity in a comparative sentence is mainly determined by the comparative word in the sentence." ></td>
	<td class="line x" title="34:290	Some comparative words explicitly indicate user preferences, e.g., better, worse, and best." ></td>
	<td class="line x" title="35:290	We call such words opinionated comparative words." ></td>
	<td class="line x" title="36:290	For example, in the sentence, the picture quality of Camera X is better than that of Camera Y, Camera X is preferred due to the opinionated comparative word better." ></td>
	<td class="line x" title="37:290	However, many comparative words are not opinionated, or their opinion orientations (i.e., positive or negative) depend on the context and/or the application domain." ></td>
	<td class="line x" title="38:290	For instance, the word longer is not opinionated as it is normally used to express that the length of some feature of an entity is greater than the length of the same feature of another entity." ></td>
	<td class="line x" title="39:290	However, in a particular context, it can express a desired (or positive) or undesired (or negative) state." ></td>
	<td class="line x" title="40:290	For example, in the sentence, the battery life of Camera X is longer than Camera Y, longer clearly expresses a desired state for battery life (although this is an objective sentence with no explicit opinion)." ></td>
	<td class="line x" title="41:290	Camera X is thus preferred with regard to battery life of the cameras." ></td>
	<td class="line x" title="42:290	The opinion in this sentence is called an implicit opinion." ></td>
	<td class="line x" title="43:290	We also say that longer is positive in this context." ></td>
	<td class="line x" title="44:290	We know this because of our existing domain knowledge." ></td>
	<td class="line x" title="45:290	However, longer may also be used to express an undesirable state in a different context, e.g., Program Xs execution time is longer than Program Y." ></td>
	<td class="line x" title="46:290	longer is clearly negative here." ></td>
	<td class="line x" title="47:290	Program Y is thus preferred." ></td>
	<td class="line x" title="48:290	We call comparative words such as longer and smaller context-dependent opinion comparatives." ></td>
	<td class="line x" title="49:290	Sentences with opinionated words (e.g., better, and worse) are usually easy to handle." ></td>
	<td class="line x" title="50:290	Then the key to solve our problem is to identify the opinion orientations (positive or negative) of context-dependent comparative words." ></td>
	<td class="line x" title="51:290	To this end, two questions need to be answered: (1) what is a context and (2) how to use the context to help determine the opinion orientation of a comparative word?" ></td>
	<td class="line x" title="52:290	The simple answer to question (1) is the whole sentence." ></td>
	<td class="line x" title="53:290	However, a whole sentence as context is too complex because it may contain too much irrelevant information, which can confuse the system." ></td>
	<td class="line x" title="54:290	Intuitively, we want to use the smallest context that can determine the orientation of the comparative word." ></td>
	<td class="line x" title="55:290	Obviously, the comparative word itself must be involved." ></td>
	<td class="line x" title="56:290	We thus conjecture that the context should consist of the entity feature being compared and the comparative word." ></td>
	<td class="line x" title="57:290	Our experimental results show that this context definition works quite well." ></td>
	<td class="line x" title="58:290	To answer the second question, we need external information or knowledge because there is no way that a computer program can solve the problem by analyzing the sentence itself." ></td>
	<td class="line x" title="59:290	In this paper, we propose to use the external information in customer reviews on the Web to help solve the problem." ></td>
	<td class="line x" title="60:290	There are a large number of such reviews on almost any product or service." ></td>
	<td class="line x" title="61:290	These reviews can be readily downloaded from many sites." ></td>
	<td class="line x" title="62:290	In our work, we use reviews from epinions.com." ></td>
	<td class="line x" title="63:290	Each review in epinions.com has separate Pros and Cons (which is also the case in most other review sites)." ></td>
	<td class="line x" title="64:290	Thus, positive and negative opinions are known as they are separated by reviewers." ></td>
	<td class="line x" title="65:290	However, they cannot be used directly because Pros and Cons seldom contain comparative words." ></td>
	<td class="line x" title="66:290	We need to deal with this problem." ></td>
	<td class="line x" title="67:290	Essentially, the proposed method computes whether the comparative word and the feature are more associated in Pros or in Cons." ></td>
	<td class="line x" title="68:290	If they are more associated in Pros (or Cons) than Cons (or Pros), then the comparative word is likely to be positive (or negative) for the feature." ></td>
	<td class="line x" title="69:290	A new association measure is also proposed to suit our purpose." ></td>
	<td class="line x" title="70:290	Our experiment results show that it can achieve high precision and recall." ></td>
	<td class="line x" title="71:290	2 Related Work Sentiment analysis has been studied by many researchers recently." ></td>
	<td class="line x" title="72:290	Two main directions are sentiment classification at the document and sentence levels, and feature-based opinion mining." ></td>
	<td class="line oc" title="73:290	Sentiment classification at the document level investigates ways to classify each evaluative document (e.g., product review) as positive or negative (Pang et al 2002; Turney 2002)." ></td>
	<td class="line x" title="74:290	Sentiment classification at the sentence-level has also been studied (e.g., Riloff and Wiebe 2003; Kim and Hovy 2004; Wilson et al 2004; Gamon et al 242 2005; Stoyanov and Cardie 2006)." ></td>
	<td class="line x" title="75:290	These works are different from ours as we study comparatives." ></td>
	<td class="line x" title="76:290	The works in (Hu and Liu 2004; Liu et al 2005; Popescu and Etzioni 2005; Mei et al 2007) perform opinion mining at the feature level." ></td>
	<td class="line x" title="77:290	The task involves (1) extracting entity features (e.g., picture quality and battery life in a camera review) and (2) finding orientations (positive, negative or neutral) of opinions expressed on the features by reviewers." ></td>
	<td class="line x" title="78:290	Again, our work is different because we deal with comparisons." ></td>
	<td class="line x" title="79:290	Discovering orientations of context dependent opinion comparative words is related to identifying domain opinion words (Hatzivassiloglou and McKeown 1997; Kanayama and Nasukawa 2006)." ></td>
	<td class="line x" title="80:290	Both works use conjunction rules to find such words from large domain corpora." ></td>
	<td class="line x" title="81:290	One conjunction rule states that when two opinion words are linked by and, their opinions are the same." ></td>
	<td class="line x" title="82:290	Our method is different in three aspects." ></td>
	<td class="line x" title="83:290	First, we argue that finding domain opinion words is problematic because in the same domain the same word may indicate different opinions depending on what features it is applied to." ></td>
	<td class="line x" title="84:290	For example, in the camera domain, long is positive in the battery life is very long but negative in it takes a long time to focus." ></td>
	<td class="line x" title="85:290	Thus, we should consider both the feature and the opinion word rather than only the opinion word." ></td>
	<td class="line x" title="86:290	Second, we focus on studying opinionated comparative words." ></td>
	<td class="line x" title="87:290	Third, our technique is quite different as we utilize readily available external opinion sources." ></td>
	<td class="line x" title="88:290	As discussed in the introduction, a closely related work to ours is (Jindal and Liu 2006)." ></td>
	<td class="line x" title="89:290	However, it does not find which entities are preferred by authors." ></td>
	<td class="line x" title="90:290	Bos and Nissim (2006) proposes a method to extract some useful items from superlative sentences." ></td>
	<td class="line x" title="91:290	Fiszman et al (2007) studied the problem of identifying which entity has more of certain features in comparative sentences." ></td>
	<td class="line x" title="92:290	It does not find which entity is preferred." ></td>
	<td class="line x" title="93:290	3 Problem Statement Definition (entity and feature): An entity is the name of a person, a product, a company, a location, etc, under comparison in a comparative sentence." ></td>
	<td class="line x" title="94:290	A feature is a part or attribute of the entity that is being compared." ></td>
	<td class="line x" title="95:290	For example, in the sentence, Camera Xs battery life is longer than that of Camera Y, Camera X and Camera Y are entities and battery life is the camera feature." ></td>
	<td class="line x" title="96:290	Types of Comparatives 1)  Non-equal gradable: Relations of the type greater or less than that express a total ordering of some entities with regard to their shared features." ></td>
	<td class="line x" title="97:290	For example, the sentence, Camera Xs battery life is longer than that of Camera Y, orders Camera X and Camera Y based on their shared feature battery life." ></td>
	<td class="line x" title="98:290	2)  Equative: Relations of the type equal to that state two objects as equal with respect to some features, e.g., Camera X and Camera Y are about the same size." ></td>
	<td class="line x" title="99:290	3)  Superlative: Relations of the type greater or less than all others that rank one object over all others, Camera Xs battery life is the longest." ></td>
	<td class="line x" title="100:290	4)  Non-gradable: Sentences which compare features of two or more entities, but do not explicitly grade them, e.g., Camera X and Camera Y have different features The first three types are called gradable comparatives." ></td>
	<td class="line x" title="101:290	This paper focuses on the first and the third types as they express ordering relationships of entities." ></td>
	<td class="line x" title="102:290	Equative and non-gradable sentences usually do not express preferences." ></td>
	<td class="line x" title="103:290	Definition (comparative relation): A comparative relation is the following: <ComparativeWord, Features, EntityS1, EntityS2, Type> ComparativeWord is the keyword used to express a comparative relation in the sentence." ></td>
	<td class="line x" title="104:290	Features is a set of features being compared." ></td>
	<td class="line x" title="105:290	EntityS1 and EntityS2 are sets of entities being compared." ></td>
	<td class="line x" title="106:290	Entities in EntityS1 appear on the left of the comparative word and entities in EntityS2 appear on the right." ></td>
	<td class="line x" title="107:290	Type is non-equal gradable, equative or superlative." ></td>
	<td class="line x" title="108:290	Let us see an example." ></td>
	<td class="line x" title="109:290	For the sentence Camera X has longer battery life than Camera Y, the extracted relation is: <longer, {battery life}, {Camera X}, {Camera Y}, non-equal gradable>." ></td>
	<td class="line x" title="110:290	We assume that the work in (Jindal and Liu 2006) has extracted the above relation from a comparative sentence." ></td>
	<td class="line x" title="111:290	In this work, we aim to identify the preferred entity of the author, which is not studied in (Jindal and Liu 2006)." ></td>
	<td class="line x" title="112:290	Our objective: Given the extracted comparative relation from a comparative sentence, we want to identify whether the entities in EntityS1 or in EntityS2 are preferred by the author." ></td>
	<td class="line x" title="113:290	4 Proposed Technique We now present the proposed technique." ></td>
	<td class="line x" title="114:290	As discussed above, the primary determining factors of the preferred entity in a comparative sentence are 243 the feature being compared and the comparative word, which we conjecture, form the context for opinions (or preferred entities)." ></td>
	<td class="line x" title="115:290	We develop our ideas from here." ></td>
	<td class="line x" title="116:290	4.1 Comparatives and superlatives In English, comparatives and superlatives are special forms of adjectives and adverbs." ></td>
	<td class="line x" title="117:290	In general, comparatives are formed by adding the suffix -er and superlatives are formed by adding the suffix est to the base adjectives and adverbs." ></td>
	<td class="line x" title="118:290	We call this type of comparatives and superlatives Type 1 comparatives and superlatives." ></td>
	<td class="line x" title="119:290	For simplicity, we will use Type 1 comparatives to represent both from now on." ></td>
	<td class="line x" title="120:290	Adjectives and adverbs with two syllables or more and not ending in y do not form comparatives or superlatives by adding er or est." ></td>
	<td class="line x" title="121:290	Instead, more, most, less and least are used before such words, e.g., more beautiful." ></td>
	<td class="line x" title="122:290	We call this type of comparatives and superlatives Type 2 comparatives and Type 2 superlatives." ></td>
	<td class="line x" title="123:290	These two types are called regular comparatives and superlatives respectively." ></td>
	<td class="line x" title="124:290	In English, there are also some irregular comparatives and superlatives, which do not follow the above rules, i.e., more, most, less, least, better, best, worse, worst, further/farther and furthest/farthest." ></td>
	<td class="line x" title="125:290	They behave similarly to Type 1 comparatives and superlatives and thus are grouped under Type 1." ></td>
	<td class="line x" title="126:290	Apart from these comparatives and superlatives, there are non-standard words that express gradable comparisons, e.g., prefer, and superior." ></td>
	<td class="line x" title="127:290	For example, the sentence, in term of battery life, Camera X is superior to Camera Y, says that Camera X is preferred." ></td>
	<td class="line x" title="128:290	We obtained a list of 27 such words from (Jindal and Liu 2006) (which used more words, but most of them are not used to express gradable comparisons)." ></td>
	<td class="line x" title="129:290	Since these words behave similarly to Type 1 comparatives, they are thus grouped under Type 1." ></td>
	<td class="line x" title="130:290	Further analysis also shows that we can group comparatives into two categories according to whether they express increased or decreased values: Increasing comparatives: Such a comparative expresses an increased value of a quantity, e.g., more, and longer." ></td>
	<td class="line x" title="131:290	Decreasing comparatives: Such a comparative expresses a decreased value of a quantity, e.g., less, and fewer." ></td>
	<td class="line x" title="132:290	As we will see later, this categorization is very useful in identifying the preferred entity." ></td>
	<td class="line x" title="133:290	Since comparatives originate from adjectives and adverbs, they may carry positive or negative sentiments/opinions." ></td>
	<td class="line x" title="134:290	Along this dimension, we can divide them into two categories." ></td>
	<td class="line x" title="135:290	1." ></td>
	<td class="line x" title="136:290	Opinionated comparatives: For Type 1 comparatives, this category contains words such as 'better', 'worse', etc, which has explicit opinions." ></td>
	<td class="line x" title="137:290	In sentences involving such words, it is normally easy to determine which entity is the preferred one of the sentence author." ></td>
	<td class="line x" title="138:290	In the case of Type 2 comparatives, formed by adding more, less, most, and least before adjectives or adverbs, the opinion (or preferred entity) is determined by both words." ></td>
	<td class="line x" title="139:290	The following rules apply: increasing comparative Negative     Negative Opinion increasing comparative Positive      Positive Opinion decreasing comparative Negative    Positive Opinion decreasing comparative Positive     Negative Opinion  The first rule says that the combination of an increasing comparative word (e.g., more) and a negative opinion adjective/adverb (e.g., awful) implies a negative Type 2 comparative." ></td>
	<td class="line x" title="140:290	The other rules are similar." ></td>
	<td class="line x" title="141:290	These rules are intuitive and will not be discussed further." ></td>
	<td class="line x" title="142:290	2." ></td>
	<td class="line x" title="143:290	Comparatives with context-dependent opinions: These comparatives are used to compare gradable quantities of entities." ></td>
	<td class="line x" title="144:290	In the case of Type 1 comparatives, such words include higher, lower, etc. Although they do not explicitly describe the opinion of the author, they often carry implicit sentiments or preferences based on contexts." ></td>
	<td class="line x" title="145:290	For example, in Car X has higher mileage per gallon than Car Y, it is hard to know whether higher is positive or negative without domain knowledge." ></td>
	<td class="line x" title="146:290	It is only when the two words, higher and mileage, are combined we know that higher is desirable for mileage from our domain knowledge." ></td>
	<td class="line x" title="147:290	In the case of Type 2 comparatives, the situation is similar." ></td>
	<td class="line x" title="148:290	However, the comparative word (more, most, less or least), the adjective/adverb and the feature are all important in determining the opinion or the preference." ></td>
	<td class="line x" title="149:290	If we know whether the comparative word is increasing or decreasing (which is easy since there are only four such words), then the opinion can be determined by applying the four rules above in (1)." ></td>
	<td class="line x" title="150:290	For this work, we used the opinion word list from (Hu and Liu 2004), which was compiled using a bootstrapping approach based on WordNet." ></td>
	<td class="line x" title="151:290	For opinionated comparatives, due to the observation below we simply convert the opinion 244 adjectives/adverbs to their comparative forms, which is done automatically based on grammar (comparative formation) rules described above and WordNet." ></td>
	<td class="line x" title="152:290	Observation: If a word is positive (or negative), then its comparative or superlative form is also positive (or negative), e.g., good, better and best." ></td>
	<td class="line x" title="153:290	After the conversion, these words are manually categorized into increasing and decreasing comparatives." ></td>
	<td class="line x" title="154:290	Although this consumes some time, it is only a one-time effort." ></td>
	<td class="line x" title="155:290	4.2 Contexts To deal with comparatives with context dependent opinions, we need contexts." ></td>
	<td class="line x" title="156:290	It is conjectured that the comparative and the feature in the sentence form the context." ></td>
	<td class="line x" title="157:290	This works very well." ></td>
	<td class="line x" title="158:290	For a Type 2 comparative, we only need the feature and the adjective/adverb to form a context." ></td>
	<td class="line x" title="159:290	For example, in the sentence, Program X runs more quickly than Program Y, the context is the pair, (run, quickly), where run is a verb feature." ></td>
	<td class="line x" title="160:290	If we find out that (run, quickly) is positive based on some external information, we can conclude that Program X is preferred using one of the four rules above since more is an increasing comparative." ></td>
	<td class="line x" title="161:290	We will use such contexts to find opinion orientations of comparatives with regard to some features from the external information, i.e., Pros and Cons in online reviews." ></td>
	<td class="line x" title="162:290	4.3 Pros and Cons in Reviews Figure 1 shows a popular review format." ></td>
	<td class="line x" title="163:290	The reviewer first describes Pros and Cons briefly, and then writes a full review." ></td>
	<td class="line x" title="164:290	Pros and Cons are used in our work for two main reasons." ></td>
	<td class="line x" title="165:290	First, the brief information in Pros and Cons contains the essential information related to opinions." ></td>
	<td class="line x" title="166:290	Each phrase or sentence segment usually contains an entity feature and an opinion word." ></td>
	<td class="line x" title="167:290	Second, depending on whether it is in Pros or in Cons, the user opinion on the product feature is clear." ></td>
	<td class="line x" title="168:290	To use the Pros and Cons phrases, we separate them use punctuations and words, i.e., ,, ., and, and but." ></td>
	<td class="line x" title="169:290	Pros in Figure 1 can be separated into 5 phrases or segments, great photos  <photo> easy to use    <use> good manual  <manual> many options <option> takes videos <video> We can see that each segment describes an entity feature on which the reviewer has expressed an opinion." ></td>
	<td class="line x" title="170:290	The entity feature for each segment is listed within <>." ></td>
	<td class="line x" title="171:290	4.4 Identifying Preferred Entities: The Algorithm Since we use Pros and Cons as the external information source to help determine whether the combination of a comparative and an entity feature is positive or negative, we need to find comparative and entity features words in Pros and Cons." ></td>
	<td class="line x" title="172:290	However, in Pros and Cons, comparatives are seldom used (entity features are always there)." ></td>
	<td class="line x" title="173:290	Thus we need to first convert comparatives to their base forms." ></td>
	<td class="line x" title="174:290	This can be done automatically using WordNet and grammar rules described in Section 4.1." ></td>
	<td class="line x" title="175:290	We will not discuss the process here as it is fairly straightforward." ></td>
	<td class="line x" title="176:290	We now put everything together to identify the preferred entity in a comparative sentence." ></td>
	<td class="line x" title="177:290	For easy reference, we denote the comparative word as C and the feature being compared as F. After obtaining the base forms of C, we work on two main cases for the two types of comparatives: Case 1." ></td>
	<td class="line x" title="178:290	Type 1 Comparative or Superlative: There are four sub-cases." ></td>
	<td class="line x" title="179:290	1.A. C is opinionated: If the comparative or superlative C has a positive orientation (e.g., better), EntityS1 (which appears before C in the sentence) is temporarily assigned as the preferred entity." ></td>
	<td class="line x" title="180:290	Otherwise, EntityS2 is assigned as the preferred entity." ></td>
	<td class="line x" title="181:290	The reason for the temporary assignment is that the sentence may contain negations, e.g., not, which is discussed below." ></td>
	<td class="line x" title="182:290	1.B. C is not opinionated but F is opinionated: An example is, Car X generates more noise than Car Y, which has the feature F noise, a negative noun." ></td>
	<td class="line x" title="183:290	If the orientation of F is positive and C is an increasing comparative word, we assign EntityS1 as the preferred entity." ></td>
	<td class="line x" title="184:290	Otherwise, we assign EntityS2 as the preferred entity." ></td>
	<td class="line x" title="185:290	The possibilities are listed as four rules below, which are derived from the 4 rules earlier: increasing C + Positive   EntityS1 preferred decreasing C + Positive   EntityS2 preferred  Figure 1: An example review 245 increasing C + Negative   EntityS2 preferred decreasing C + Negative   EntityS1 preferred Positive and Negative stand for the orientation of feature F being positive and negative respectively." ></td>
	<td class="line x" title="186:290	1.C. Both C and F are not opinionated: In this case, we need external information to identify the preferred entity." ></td>
	<td class="line x" title="187:290	We use phrases in Pros and Cons from reviews." ></td>
	<td class="line x" title="188:290	In this case, we look for the feature F and comparative word C, (i.e., the context) in the list of phrases in Pros and Cons." ></td>
	<td class="line x" title="189:290	In order to find whether the combination of C and F indicates a positive or negative opinion, we compute their associations in Pros and in Cons." ></td>
	<td class="line x" title="190:290	If they are more associated in Pros than in Cons, we conclude that the combination indicates a positive sentiment, and otherwise a negative sentiment." ></td>
	<td class="line x" title="191:290	The result decides the preferred entity." ></td>
	<td class="line x" title="192:290	Point-wise mutual information (PMI) is commonly used for computing the association of two terms (e.g., Turney 2002), which is defined as: nullnullnull null null,null null nullnullnull nullnullnullnull,nullnull nullnull null null null nullnullnullnullnull . However, we argue that PMI is not a suitable measure for our purpose." ></td>
	<td class="line x" title="193:290	The reason is that PMI is symmetric in the sense that PMI(F, C) is the same as PMI(C, F)." ></td>
	<td class="line x" title="194:290	However, in our case, the feature F and comparative word C association is not symmetric because although a feature is usually modified by a particular adjective word, the adjective word can modify many other features." ></td>
	<td class="line x" title="195:290	For example, long can be used in long lag, but it can also be used in long battery life, long execution time and many others." ></td>
	<td class="line x" title="196:290	Thus, this association is asymmetric." ></td>
	<td class="line x" title="197:290	We are more interested in the conditional probability of C (including its synonyms) given F, which is essentially the confidence measure in traditional data mining." ></td>
	<td class="line x" title="198:290	However, confidence does not handle well the situation where C occurs frequently but F appears rarely." ></td>
	<td class="line x" title="199:290	In such cases a high conditional probability Pr(C|F) may just represent some pure chance, and consequently the resulting association may be spurious." ></td>
	<td class="line x" title="200:290	We propose the following measure, which we call one-side association (OSA), and it works quite well: nullnullnull null null,null null nullnullnull nullnullnullnull,nullnullnullnullnullnull|nullnull nullnull null null null nullnullnullnullnull  The difference between OSA and PMI is the conditional probability Pr(C|F) used in OSA, which biases the mutual association of F and C to one side." ></td>
	<td class="line x" title="201:290	Given the comparative word C and the feature F, we first compute an OSA value for positive, denoted by OSA P (F, C), and then compute an OSA value for negative, denoted by OSA N (F, C)." ></td>
	<td class="line x" title="202:290	The decision rule is simply the following: If OSA P (F, C)  OSA N (F, C)  0 then EntityS1 is preferred Otherwise,  EntityS2 is preferred Computing OSA P (F, C): We need to compute Pr P (F, C), for which we need to count the number of times that comparative word C and the feature F co-occur." ></td>
	<td class="line x" title="203:290	Instead of using C alone, we also use its base forms and synonyms and antonyms." ></td>
	<td class="line x" title="204:290	Similarly, for F, we also use its synonyms." ></td>
	<td class="line x" title="205:290	If C (or a synonym of C) and F (or a synonym) co-occur in a Pros phrase, we count 1." ></td>
	<td class="line x" title="206:290	If an antonym of C and F (or a synonym) co-occur in a Cons phrase, we also count 1." ></td>
	<td class="line x" title="207:290	Thus, although we only evaluate for positive, we actually use both Pros and Cons." ></td>
	<td class="line x" title="208:290	This is important because it allows us to find more occurrences to produce more reliable results." ></td>
	<td class="line x" title="209:290	Synonyms and antonyms are obtained from WordNet." ></td>
	<td class="line x" title="210:290	Currently, synonyms and antonyms are only found for single word features." ></td>
	<td class="line x" title="211:290	We then count the number of occurrences of the comparative word C and the feature F separately in both Pros and Cons to compute Pr P (F) and Pr P (C)." ></td>
	<td class="line x" title="212:290	In counting the number of occurrences of C, we consider both its synonyms in Pros and antonyms in Cons." ></td>
	<td class="line x" title="213:290	In counting the number of occurrences of F, we consider its synonyms in both Pros and Cons." ></td>
	<td class="line x" title="214:290	Computing OSA N (F, C): To compute Pr N (F, C), we use a similar strategy as for computing Pr P (F, C)." ></td>
	<td class="line x" title="215:290	In this case, we start with Cons." ></td>
	<td class="line x" title="216:290	1.D. C is a feature indicator: An example sentence is Camera X is smaller than Camera Y, where smaller is the feature indicator for feature size." ></td>
	<td class="line x" title="217:290	In this case, we simply count the number of times (denoted by n+) that C appears in Pros and the number of times (denoted by n-) that C appears in Cons." ></td>
	<td class="line x" title="218:290	If n+  n-, we temporarily assign EntityS1 as the preferred entity." ></td>
	<td class="line x" title="219:290	Otherwise, we assign EntityS2 as the preferred entity." ></td>
	<td class="line x" title="220:290	Note that in some sentences, the entity features do not appear explicitly in the sentences but are implied." ></td>
	<td class="line x" title="221:290	The words that imply the features are called feature indicators." ></td>
	<td class="line x" title="222:290	246 Case 2: Type 2 Comparative or Superlative: There are two sub-cases: 2.A. Adjective/adverb in the comparison is opinionated: In this case, the feature F is not important." ></td>
	<td class="line x" title="223:290	An example sentence is: Car X has more beautiful interior than Car Y, more is an increasing comparative, and beautiful is the adjective with a positive orientation (the feature F is interior)." ></td>
	<td class="line x" title="224:290	Car X is clearly preferred in this case." ></td>
	<td class="line x" title="225:290	Another example is: Car X is more beautiful than Car Y." ></td>
	<td class="line x" title="226:290	In this case, beautiful is a feature indicator for the feature appearance." ></td>
	<td class="line x" title="227:290	Again, Car X is preferred." ></td>
	<td class="line x" title="228:290	This sub-case can be handled similarly as case 1.B. 2.B. adjective/adverb in the comparison is not opinionated: If the adjective/adverb in comparison is a feature indicator, we can use the method in 1.D. Otherwise, we form a context using the feature and adjective/adverb, and apply the method in 1.C. We then combine the result with the comparative word before the adjective/adverb to decide based on the rules in 1.B. Negations: The steps above temporarily determine which entity is the preferred entity." ></td>
	<td class="line x" title="229:290	However, a comparative sentence may contain a negation word or phrase (we have compiled 26 of them), e.g., Camera Xs battery life is not longer than that of Camera Y. Without considering not, Camera X is preferred." ></td>
	<td class="line x" title="230:290	After considering not, we assign the preferred entity to Camera Y." ></td>
	<td class="line x" title="231:290	This decision may be problematic because not longer does not mean shorter (thus it can also be seen to have no preference)." ></td>
	<td class="line x" title="232:290	5 Evaluation A system, called PCS (Preferred entities in Comparative Sentences), has been implemented based the proposed method." ></td>
	<td class="line x" title="233:290	Since there is no existing system that can perform the task, we could not compare with an existing approach." ></td>
	<td class="line x" title="234:290	Below, we first describe the evaluation datasets and then present the results." ></td>
	<td class="line x" title="235:290	5.1 Evaluation Datasets Our comparative sentence dataset consists of two subsets." ></td>
	<td class="line x" title="236:290	The first subset is from (Jindal and Liu 2006), which are product review and forum discussion sentences on digital cameras, DVD players, MP3 players, Intel vs AMD, Coke vs Pepsi, and Microsoft vs Google." ></td>
	<td class="line x" title="237:290	The original dataset used in (Jindal and Liu 2006) also contains many non-gradable comparative sentences, which are not used here as most such sentences do not express any preferences." ></td>
	<td class="line x" title="238:290	To make the data more diverse, we collected more forum discussion data about mobile phones from http://www.howardforums.com/, and reviews from amazon.com and cnet.com on products such as laptops, cameras and mobile phones." ></td>
	<td class="line x" title="239:290	Table 1 gives the number of sentences from these two sources." ></td>
	<td class="line x" title="240:290	Although we only have 837 comparative sentences, they were collected from thousands of sentences in reviews and forums." ></td>
	<td class="line x" title="241:290	About 10% of the sentences from them are comparative sentences." ></td>
	<td class="line x" title="242:290	Skewed Distribution: An interesting observation about comparative sentences is that a large proportion (based on our data) of them (84%) has EntityS1 as the preferred entity." ></td>
	<td class="line x" title="243:290	This means that when people make comparisons, they tend to put the preferred entities first." ></td>
	<td class="line x" title="244:290	Pros and Cons corpus: The Pros and Cons corpus was crawled from reviews of epinions.com." ></td>
	<td class="line x" title="245:290	It has 15162 Pros and 15162 Cons extracted from 15162 reviews of three types of products, i.e., digital cameras (8479), and printers (5778), and Strollers (905)." ></td>
	<td class="line x" title="246:290	Table 1." ></td>
	<td class="line x" title="247:290	Sentences from different sources Data Sources No." ></td>
	<td class="line x" title="248:290	of Comparative Sentences (Jindal and Liu 2006) 418 Reviews and forum posts 419 Total 837 5.2 Results The results on the whole dataset are given in Table 2." ></td>
	<td class="line x" title="249:290	Note that 84% of the sentences have EntityS1 as the preferred entity." ></td>
	<td class="line x" title="250:290	If a system does nothing but simply announces that EntityS1 is preferred, we will have the accuracy of 84%." ></td>
	<td class="line x" title="251:290	However, PCS using the OSA measure achieves the accuracy of 94.4%, which is much better than the baseline of taking the majority." ></td>
	<td class="line x" title="252:290	Since in skewed datasets accuracy does not reflect the prediction well, we will mainly use precision (Prec.), recall (Rec.)" ></td>
	<td class="line x" title="253:290	and F-score (F) in evaluation." ></td>
	<td class="line x" title="254:290	For the case that EntityS1 is preferred, the algorithm does extremely well." ></td>
	<td class="line x" title="255:290	For the case that EntityS2 is preferred, the algorithm also does well although not as well as for the EntityS1 case." ></td>
	<td class="line x" title="256:290	Based on our observation, we found that in such cases, the sentences are usually more complex." ></td>
	<td class="line x" title="257:290	Next, we compare with the case that the system does not use Pros and Cons (then OSA or PMI is not needed) (row 2)." ></td>
	<td class="line x" title="258:290	When a sentence requires context dependency handling, the system simply takes the majority as the default, i.e., 247 assigning EntityS1 as the preferred entity." ></td>
	<td class="line x" title="259:290	From the results in Table 2, we can see that F-scores are all worse." ></td>
	<td class="line x" title="260:290	In the case that EntityS1 is the preferred entity, taking defaults is not so bad, which is not surprising because of the skewed data distribution." ></td>
	<td class="line x" title="261:290	Even in this case, the precision improvement of PCS(OSA) is statistically significant at the 95% confidence level." ></td>
	<td class="line x" title="262:290	The recall is slight less but their difference is not statistically significant." ></td>
	<td class="line x" title="263:290	When EntityS2 is the preferred entity, its F-score (row 2) is much worse, which shows that our technique is effective." ></td>
	<td class="line x" title="264:290	The recall improvement of PCS (OSA) is dramatic (statistically significant at the 95% confidence level)." ></td>
	<td class="line x" title="265:290	The two precisions are not statistically different." ></td>
	<td class="line x" title="266:290	For OSA vs. PMI, see below." ></td>
	<td class="line x" title="267:290	Table 2: Preferred entity identification: whole data   EntityS1 Preferred EntityS2 Preferred Prec." ></td>
	<td class="line x" title="268:290	Rec." ></td>
	<td class="line x" title="269:290	F Prec." ></td>
	<td class="line x" title="270:290	Rec." ></td>
	<td class="line x" title="271:290	F PCS (OSA) 0.967 0.966 0.966 0.822 0.828 0.825 PCS: No Pros & Cons 0.925 0.980 0.952 0.848 0.582 0.690 PCS (PMI) 0.967 0.961 0.964 0.804 0.828 0.816 Now let us look at only the 187 sentences that need context dependency handling." ></td>
	<td class="line x" title="272:290	The data is still skewed." ></td>
	<td class="line x" title="273:290	72.2% of the sentences have EntityS1 as the preferred entities." ></td>
	<td class="line x" title="274:290	Table 3 shows the results of PCS with and without using Pros and Cons." ></td>
	<td class="line x" title="275:290	The results of PCS without Pros and Cons (OSA or PMI is not needed) are based on assigning EntityS1 as preferred for every sentence (taking the majority)." ></td>
	<td class="line x" title="276:290	Again, we can see that using external Pros and Cons (PCS(OSA)) helps dramatically." ></td>
	<td class="line x" title="277:290	Not surprisingly, the improvements are statistically significant except the recall when EntityS1 is preferred." ></td>
	<td class="line x" title="278:290	Table 3: Preferred entity identification with 187 sentences that need context dependency handling   EntityS1 Preferred EntityS2 Preferred Prec." ></td>
	<td class="line x" title="279:290	Rec." ></td>
	<td class="line x" title="280:290	F Prec." ></td>
	<td class="line x" title="281:290	Rec." ></td>
	<td class="line x" title="282:290	F PCS (OSA) 0.896 0.877 0.886 0.696 0.736 0.716 PCS: No Pros & Cons 0.722 1.000 0.839 0.000 0.000 0.000 PCS (PMI) 0.894 0.855 0.874 0.661 0.736 0.696 OSA vs. PMI: Comparing PCS(OSA) with PCS (PMI) (Table 3), OSA is better in F-score when EntityS1 is preferred by 1.2%, and better in Fscore when EntityS2 is preferred by 2%." ></td>
	<td class="line x" title="283:290	Although OSAs improvements over PMI are not large, we believe that in principle OSA is a more suitable measure." ></td>
	<td class="line x" title="284:290	Comparing with PMI when the whole dataset is used (Table 2), OSAs gains are less because the number of sentences requiring context dependency handling is small (22%)." ></td>
	<td class="line x" title="285:290	6 Conclusions This paper studied sentiments expressed in comparative sentences." ></td>
	<td class="line x" title="286:290	To our knowledge, no work has been reported on this topic." ></td>
	<td class="line x" title="287:290	This paper proposed an effective method to solve the problem, which also deals with context based sentiments by exploiting external information available on the Web." ></td>
	<td class="line x" title="288:290	To use the external information, we needed a measure of association of the comparative word and the entity feature." ></td>
	<td class="line x" title="289:290	A new measure, called one-side association (OSA), was then proposed." ></td>
	<td class="line x" title="290:290	Experimental results show that the technique produces accurate results." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1104
From Words to Senses: A Case Study of Subjectivity Recognition
Su, Fangzhong;Markert, Katja;"></td>
	<td class="line x" title="1:203	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 825832 Manchester, August 2008 From Words to Senses: A Case Study of Subjectivity Recognition Fangzhong Su School of Computing University of Leeds, UK fzsu@comp.leeds.ac.uk Katja Markert School of Computing University of Leeds, UK markert@comp.leeds.ac.uk Abstract We determine the subjectivity of word senses." ></td>
	<td class="line x" title="2:203	To avoid costly annotation, we evaluate how useful existing resources established in opinion mining are for this task." ></td>
	<td class="line x" title="3:203	We show that results achieved with existing resources that are not tailored towards word sense subjectivity classification can rival results achieved with supervision on a manually annotated training set." ></td>
	<td class="line x" title="4:203	However, results with different resources vary substantially and are dependent on the different definitions of subjectivity used in the establishment of the resources." ></td>
	<td class="line x" title="5:203	1 Introduction In recent years, subjectivity analysis and opinion mining have attracted considerable attention in the NLP community." ></td>
	<td class="line x" title="6:203	Unlike traditional information extraction and document classification tasks which usually focus on extracting facts or categorizing documents into topics (e.g., sports, politics, medicine), subjectivity analysis focuses on determiningwhetheralanguageunit(suchasaword, sentence or document) expresses a private state, opinion or attitude and, if so, what polarity is expressed, i.e. a positive or negative attitude." ></td>
	<td class="line x" title="7:203	Inspired by Esuli and Sebastiani (2006) and Wiebe and Mihalcea (2006), we explore the automatic detection of the subjectivity of word senses, in contrast to the more frequently explored task of determining the subjectivity of words (see Section 2)." ></td>
	<td class="line x" title="8:203	This is motivated by many words being c2008." ></td>
	<td class="line x" title="9:203	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="10:203	Some rights reserved." ></td>
	<td class="line x" title="11:203	subjectivity-ambiguous, i.e. having both subjective and objective senses, such as the word positive with its two example senses given below.1 (1) positive, electropositivehaving a positive electric charge;protons are positive (objective) (2) plus, positiveinvolving advantage or good; a plus (or positive) factor (subjective) Subjectivity labels for senses add an additional layerofannotationtoelectroniclexicaandallowto group many fine-grained senses into higher-level classes based on subjectivity/objectivity." ></td>
	<td class="line x" title="12:203	This can increase the lexicas usability." ></td>
	<td class="line x" title="13:203	As an example, Wiebe and Mihalcea (2006) prove that subjectivity information for WordNet senses can improve word sense disambiguation tasks for subjectivityambiguous words (such as positive)." ></td>
	<td class="line x" title="14:203	In addition, Andreevskaia and Bergler (2006) show that the performance of automatic annotation of subjectivity at the word level can be hurt by the presence of subjectivity-ambiguous words in the training sets they use." ></td>
	<td class="line x" title="15:203	Moreover, the prevalence of different word senses in different domains also means that a subjective or an objective sense of a word might be dominant in different domains; thus, in a science text positive is likely not to have a subjective reading." ></td>
	<td class="line x" title="16:203	Theannotationofwordsassubjectiveand objective or positive and negative independent of senseordomaindoesnotcapturesuchdistinctions." ></td>
	<td class="line x" title="17:203	In this paper, we validate whether word sense subjectivity labeling can be achieved with existing resources for subjectivity analysis at the word and sentence level without creating a dedicated, manually annotated training set of WordNet senses labeled for subjectivity.2 We show that such an ap1All examples in this paper are from WordNet 2.0." ></td>
	<td class="line x" title="18:203	2We use a subset of WordNet senses that are manually annotated for subjectivity as test set (see Section 3)." ></td>
	<td class="line x" title="19:203	825 proach  even using a simple rule-based unsupervised algorithm  can compete with a standard supervised approach and also compares well to prior research on word sense subjectivity labeling." ></td>
	<td class="line x" title="20:203	However, success depends to a large degree on the definition of subjectivity used in the establishment of the prior resources." ></td>
	<td class="line x" title="21:203	The remainder of this paper is organized as follows." ></td>
	<td class="line x" title="22:203	Section 2 discusses previous work." ></td>
	<td class="line x" title="23:203	Section 3 introduces our human annotation scheme for word sense subjectivity and also shows that subjectivity-ambiguous words are frequent." ></td>
	<td class="line x" title="24:203	Section 4 describes our proposed classification algorithms in detail." ></td>
	<td class="line x" title="25:203	Section 5 presents the experimental results and evaluation, followed by conclusions and future work in Section 6." ></td>
	<td class="line oc" title="26:203	2 Related Work There has been extensive research in opinion mining at the document level, for example on product and movie reviews (Pang et al., 2002; Pang and Lee, 2004; Dave et al., 2003; Popescu and Etzioni, 2005)." ></td>
	<td class="line x" title="27:203	Several other approaches focus on the subjectivity classification of sentences (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff and Wiebe, 2003)." ></td>
	<td class="line x" title="28:203	They often build on the presence of subjective words in the sentence to be classified." ></td>
	<td class="line x" title="29:203	Closer to our work is the large body of work on the automatic, context-independent classification of words according to their polarity, i.e as positive or negative (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005)." ></td>
	<td class="line x" title="30:203	They use either co-occurrence patterns in corpora or dictionarybased methods." ></td>
	<td class="line x" title="31:203	Many papers assume that subjectivity recognition, i.e. separating subjective from objective words, has already been achieved prior to polarity recognition and test against word lists containing subjective words only (Hatzivassiloglou and McKeown, 1997; Takamura et al., 2005)." ></td>
	<td class="line x" title="32:203	However, Kim and Hovy (2004) and Andreevskaia and Bergler (2006) also address the classification into subjective/objective words and show this to be a potentially harder task than polarity classification with lower human agreement and automatic performance." ></td>
	<td class="line x" title="33:203	There are only two prior approaches addressing word sense subjectivity or polarity classification." ></td>
	<td class="line x" title="34:203	Esuli and Sebastiani (2006) determine the polarity of word senses in WordNet, distinguishing among positive, negative and objective." ></td>
	<td class="line x" title="35:203	They expand a small, manually determined seed set of strongly positive/negative WordNet senses by following WordNet relations and use the resulting larger training set for supervised classification." ></td>
	<td class="line x" title="36:203	The resulting labeled WordNet gives three scores for each sense, representing the positive, negative and objective score respectively." ></td>
	<td class="line x" title="37:203	However, there is no evaluation as to the accuracy of their approach." ></td>
	<td class="line x" title="38:203	They then extend their work (Esuli and Sebastiani, 2007) by applying the Page Rank algorithm to ranking the WordNet senses in terms of how strongly a sense possesses a given semantic property (e.g., positive or negative)." ></td>
	<td class="line x" title="39:203	Wiebe and Mihalcea (2006) label word senses in WordNet as subjective or objective." ></td>
	<td class="line x" title="40:203	They use a method relying on distributional similarity as well as an independent, large manually annotated opinion corpus (MPQA) (Wiebe et al., 2005) for determining subjectivity." ></td>
	<td class="line x" title="41:203	One of the disadvantages of their algorithm is that it is restricted to senses that have distributionally similar words in the MPQA corpus, excluding 23.2% of their test data from automatic classification." ></td>
	<td class="line x" title="42:203	3 Human Annotation of Word Sense Subjectivity and Polarity In contrast to other researchers (Hatzivassiloglou and McKeown, 1997; Takamura et al., 2005), we do not see polarity as a category that is dependent on prior subjectivity assignment and therefore applicable to subjective senses only." ></td>
	<td class="line x" title="43:203	We follow Wiebe and Mihalcea (2006) in that we see subjective expressions as private states that are not open to objective observation or verification." ></td>
	<td class="line x" title="44:203	This includes direct references to emotions, beliefs and judgements (such as anger, criticise) as well as expressions that let a private state be inferred, for example by referring to a doctor as a quack." ></td>
	<td class="line x" title="45:203	In contrast, polarity refers to positive or negative associations of a word or sense." ></td>
	<td class="line x" title="46:203	Whereas there is a dependency in that most subjective senses have a relatively clear polarity, polarity can be attached to objective words/senses as well." ></td>
	<td class="line x" title="47:203	For example, tuberculosis is not subjective  it does not describe a private state, is objectively verifiable and would not cause a sentence containing it to carry an opinion, but it does carry negative associations for the vast majority of people." ></td>
	<td class="line x" title="48:203	We therefore annotate subjectivity of word senses similarly to Wiebe and Mihalcea (2006), 826 distinguishing between subjective (S), objective (O) or both (B)." ></td>
	<td class="line x" title="49:203	Both is used if a literal and metaphoric sense of a word are collapsed into one WordNet synset or if a WordNet synset contains both opinionated and objective expressions (such as bastard and illegitimate child in Ex." ></td>
	<td class="line x" title="50:203	3 below)." ></td>
	<td class="line x" title="51:203	We expand their annotation scheme by also annotatingpolarity, usingthelabelspositive(P),negative (N) and varied (V)." ></td>
	<td class="line x" title="52:203	The latter is used when a senses polarity varies strongly with the context, such as Example 8 below, where we would expect uncompromising to be a judgement but this judgement will be positive or negative depending on what a person is uncompromising about." ></td>
	<td class="line x" title="53:203	To avoid prevalence of personalised associations, annotators were told to only annotate polarity for subjective senses, as well as objective senses that carry a strong association likely to be shared by most people at least in Western culture (such as the negative polarity for words referring to diseases and crime)." ></td>
	<td class="line x" title="54:203	Other objective senses would receive the label O:NoPol." ></td>
	<td class="line x" title="55:203	Therefore, we have 7 sub categories in total: O:NoPol, O:P, O:N, S:P, S:N, S:V, and B. The notation before and after the colon represents the subjectivity and polarity label respectively." ></td>
	<td class="line x" title="56:203	We list some annotated examples below." ></td>
	<td class="line x" title="57:203	(3) bastard, by-blow, love child, illegitimate child, illegitimate, whoreson the illegitimate offspring of unmarried parents (B) (4) atrophyundergo atrophy; Muscles that are not used will atrophy (O:N) (5) guard, safety, safety devicea device designed to prevent injury (O:P) (6) nasty, awfuloffensive or even (of persons) malicious;in a nasty mood;a nasty accident; a nasty shock (S:N) (7) happyenjoying or showing or marked by joy or pleasure or good fortune; a happy smile;spent many happy days on the beach; a happy marriage (S:P) (8) uncompromising, inflexiblenot making concessions; took an uncompromising stance in the peace talks (S:V) As far as we are aware, this is the first annotation scheme for both subjectivity and polarity of word senses." ></td>
	<td class="line x" title="58:203	We believe both are relevant for opinion extraction: subjectivity for finding and analysing directly expressed opinions, and polarity for either classifying these further or extracting objective words that, however, serve to colour a text or present bias rather than explicitly stated opinions." ></td>
	<td class="line x" title="59:203	Su and Markert (2008) describe the annotation scheme and agreement study in full." ></td>
	<td class="line x" title="60:203	3.1 Agreement Study WeusedtheMicro-WNOpcorpuscontaining1105 WordNet synsets to test our annotation scheme.3 The Micro-WNOp corpus is representative of the part-of-speech distribution in WordNet." ></td>
	<td class="line x" title="61:203	Twoannotators(bothnear-nativeEnglishspeakers) independently annotated 606 synsets of the Micro-WNOp corpus for subjectivity and polarity." ></td>
	<td class="line x" title="62:203	One annotator is the second author of this paper whereas the other is not a linguist." ></td>
	<td class="line x" title="63:203	The overall agreement using all 7 categories is 84.6%, with a kappa of 0.77, showing high reliability for a difficult pragmatic task." ></td>
	<td class="line x" title="64:203	This at first seems at odds with the notion of sentiment as a fuzzy category as expressed in (Andreevskaia and Bergler, 2006) but we believe is due to three factors:  The annotation of senses instead of words splits most subjectivity-ambiguous words into several senses, removing one source of annotation difficulty." ></td>
	<td class="line x" title="65:203	 The annotation of senses in a dictionary provided the annotators with sense descriptions in form of Wordnet glosses as well as relatedsenses, providingmoreinformationthan a pure word annotation task." ></td>
	<td class="line x" title="66:203	 The split of subjectivity and polarity annotation made the task clearer and the annotation of only very strong connotations for objective word senses de-individualized the task." ></td>
	<td class="line x" title="67:203	As in this paper we are only interested in subjectivity recognition, we collapse S:V, S:P, and S:N into a single label S and O:NoPol, O:P, and O:N into a single label O. Label B remains unchanged." ></td>
	<td class="line x" title="68:203	For this three-way annotation overall percentage agreement is 90.1%, with a kappa of 0.79." ></td>
	<td class="line x" title="69:203	3.2 Gold Standard After cases with disagreement were negotiated between the two annotators, a gold standard annotation was agreed upon." ></td>
	<td class="line x" title="70:203	Our test set consists of this agreed set as well as the remainder of the MicroWNOp corpus annotated by one of the annotators alone after agreement was established." ></td>
	<td class="line x" title="71:203	This set is available for research purposes at http://www." ></td>
	<td class="line x" title="72:203	comp.leeds.ac.uk/markert/data." ></td>
	<td class="line x" title="73:203	3The corpus has originally been annotated by the providers (Esuli and Sebastiani, 2007) with scores for positive, negative and objective/no polarity, thus a mixture of subjectivity and polarity annotation." ></td>
	<td class="line x" title="74:203	We re-annotated the corpus with our annotation scheme." ></td>
	<td class="line x" title="75:203	827 How many words are subjectivity-ambiguous?" ></td>
	<td class="line x" title="76:203	As the number of senses increases with word frequency, we expect rare words to be less likely to be subjectivity-ambiguous than frequent words." ></td>
	<td class="line x" title="77:203	The Micro-WNOp corpus contains relatively frequent words so we will get an overestimation of subjective-ambiguous word types from this corpus, though not necessarily of word tokens." ></td>
	<td class="line x" title="78:203	It includes 298 different words with all their synsets in WordNet 2.0." ></td>
	<td class="line x" title="79:203	Of all words, 97 (32.5%) are subjectivity-ambiguous, a substantial number." ></td>
	<td class="line x" title="80:203	4 Algorithms In this section, we present experiments using five different resources as training sets or clue sets for thistask." ></td>
	<td class="line x" title="81:203	ThefirstistheMicro-WNOpcorpuswith our own dedicated word sense subjectivity annotation which is used in a standard supervised approach as training and test set via 10-fold crossvalidation." ></td>
	<td class="line x" title="82:203	This technique presupposes a manual annotation effort tailored directly to our task to provide training data." ></td>
	<td class="line x" title="83:203	As it is costly to create such training sets, we investigate whether existing resources such as two different subjective sentence lists (Section 4.2) and two different subjective word lists (Section 4.3) can be adapted to provide training data or clue sets although they do not provide any information about word senses." ></td>
	<td class="line x" title="84:203	All resources are used to create training data for supervised approaches; the subjective word lists are also used in a simple rule-based unsupervised approach." ></td>
	<td class="line x" title="85:203	All algorithms were tested on the Micro-WNOp corpus by comparing to the human gold standard annotation." ></td>
	<td class="line x" title="86:203	However, we excluded all senses with the label both from Micro-WNOp for testing the automatic algorithms, resulting in a final 1061 senses, with 703 objective and 358 subjective senses." ></td>
	<td class="line x" title="87:203	We also compare all algorithms to a baseline of always assigning the most frequent category (objective) to each sense, which results in an overall accuracy of 66.3%." ></td>
	<td class="line x" title="88:203	4.1 Standard Supervised Approach: 10-fold Cross-validation (CV) on Micro-WNOp We use 10-fold cross validation for training and testing on the annotated synsets in the MicroWNOp corpus." ></td>
	<td class="line x" title="89:203	We applied a Naive Bayes classifier, 4 using the following three types of features: 4We also experimented with KNN, Maximum Entropy, Rocchio and SVM algorithms and overall Naive Bayes perLexical Features: These are unigrams in the glosses." ></td>
	<td class="line x" title="90:203	Weuseabag-of-wordsapproachandfilter out stop words." ></td>
	<td class="line x" title="91:203	As glosses are usually quite short, using a bagof-word feature representation will result in highdimensional and sparse feature vectors, which often deteriorate classification performance." ></td>
	<td class="line x" title="92:203	In order toaddressthisproblemtosomedegree,wealsoexplored other features which are available as training and test instances are WordNet synsets." ></td>
	<td class="line x" title="93:203	Part-of-Speech (POS) Features: each sense gets its POS as a feature (adjective, noun, verb or adverb)." ></td>
	<td class="line x" title="94:203	Relation Features: WordNet relations are good indicators for determining subjectivity as many of them are subjectivity-preserving." ></td>
	<td class="line x" title="95:203	For example, if sense A is subjective, then its antonym sense B is likely to be subjective." ></td>
	<td class="line x" title="96:203	We employ 8 relations hereantonym, similar-to, derivedfrom, attribute, also-see, direct-hyponym, directhypernym, and extended-antonym." ></td>
	<td class="line x" title="97:203	Each relation R leads to 2 features that describe for a sense A how many links of that type it has to synsets in the subjectiveortheobjectivetrainingsetrespectively." ></td>
	<td class="line x" title="98:203	Finally, we represent the feature weights through a TF*IDF measure." ></td>
	<td class="line x" title="99:203	Considering the size of WordNet (115,424 synsetsinWordNet2.0), thelabeledMicro-WNOp corpus is small." ></td>
	<td class="line x" title="100:203	Therefore, the question arises whether it is possible to adapt other data sources that provide subjectivity information to our task." ></td>
	<td class="line x" title="101:203	4.2 Sentence Collections: Movie and MPQA It is reasonable to cast word sense subjectivity classification as a sentence classification task, with the glosses that WordNet provides for each sense as the sentences to be classified." ></td>
	<td class="line x" title="102:203	Then we can in theory feed any collection of annotated subjective and objective sentences as training data into our classifier while the annotated Micro-WNOp corpus is used as test data." ></td>
	<td class="line x" title="103:203	We experimented with two different available data sets to test this assumption." ></td>
	<td class="line x" title="104:203	Movie-domainSubjectivityDataSet(Movie): Pang and Lee (2004) used a collection of labeled subjective and objective sentences in their work on review classification.5 The data set contains 5000 subjective sentences, extracted from movie reviews collected from the Rotten Tomatoes web formed best." ></td>
	<td class="line x" title="105:203	5Available at http://www.cs.cornell.edu/ People/pabo/movie-review-data/ 828 site.6 The 5000 objective sentences were collected from movie plot summaries from the Internet Movie Database (IMDB)." ></td>
	<td class="line x" title="106:203	The assumption is that all the snippets from the Rotten Tomatoes pages are subjective (as they come from a review site), while all the sentences from IMDB are objective (as they focus on movie plot descriptions)." ></td>
	<td class="line x" title="107:203	The MPQA Corpus contains news articles manually annotated at the phrase level for opinions, their polarity and their strength." ></td>
	<td class="line x" title="108:203	The corpus (Version 1.2) contains 11,112 sentences." ></td>
	<td class="line x" title="109:203	We convert it into a corpus of subjective and objective sentencesfollowingexactlytheapproachin(Riloff et al., 2003; Riloff and Wiebe, 2003) and obtain 6127 subjective and 4985 objective sentences respectively." ></td>
	<td class="line x" title="110:203	Basically any sentence that contains at least one strong subjective annotation at the phrase level is seen as a subjective sentence." ></td>
	<td class="line x" title="111:203	We again use a Naive Bayes algorithm with lexical unigram features." ></td>
	<td class="line x" title="112:203	Note that part-of-speech and relation features are not applicable here as the trainingsetconsistsofcorpussentences,notWordNet synsets." ></td>
	<td class="line x" title="113:203	4.3 Word Lists: General Inquirer and Subjectivity List Several word lists annotated for subjectivity or polaritysuchastheGeneralInquirer(GI) 7orthesubjectivity clues list (SL) collated by Janyce Wiebe and her colleagues8 are available." ></td>
	<td class="line x" title="114:203	The General Inquirer (GI) was developed by Philip Stone and colleagues in the 1960s." ></td>
	<td class="line x" title="115:203	It concentrates on word polarity." ></td>
	<td class="line x" title="116:203	Here we make the simple assumption that both positive and negative words in the GI list are subjective clues whereas all other words are objective." ></td>
	<td class="line x" title="117:203	The Subjectivity Lexicon (SL) centers on subjectivity so that it is ideally suited for our task." ></td>
	<td class="line x" title="118:203	It provides fine-grained information for each clue, such as part-of-speech, subjectivity strength (strong/weak), and prior polarity (positive, negative, or neutral)." ></td>
	<td class="line x" title="119:203	For example, object(verb) is a subjective clue whereas object(noun) is objective." ></td>
	<td class="line x" title="120:203	Regarding strength, the adjective evil is marked as strong subjective whereas the adjective exposed is marked as a weak subjective clue." ></td>
	<td class="line x" title="121:203	Both lexica do not include any information about word senses and therefore cannot be used directly for subjectivity assignment at the sense 6http://www.rottentomatoes.com/ 7http://www.wjh.harvard.edu/inquirer/ 8http://www.cs.pitt.edu/mpqa/ level." ></td>
	<td class="line x" title="122:203	For example, at least one sense of any subjectivity-ambiguouswordwillbelabeledincorrectly if we just adopt a word-based label." ></td>
	<td class="line x" title="123:203	In addition, theselistsarefarfromcomplete: comparedto the over 100,000 synsets in WordNet, GI contains 11,788 words marked for polarity (1915 positive, 2291 negative and 7582 no-polarity words) and the SL list contains about 8,000 subjective words." ></td>
	<td class="line x" title="124:203	Still, it is a reasonable assumption that any gloss that contains several subjective words indicates a subjective sense overall." ></td>
	<td class="line x" title="125:203	This intuition is strengthened by the characteristics of glosses." ></td>
	<td class="line x" title="126:203	They normallyareshortandconcisewithoutacomplexsyntactic structure, thus the occurrence of subjective words in such a short string is likely to indicate a subjective sense overall." ></td>
	<td class="line x" title="127:203	This contrasts, for example, with sentences in newspapers where one clause might express an opinion, whereas other parts of the sentence are objective." ></td>
	<td class="line x" title="128:203	Therefore, for the rule-based unsupervised algorithm we lemmatized and POS-tagged the glosses in the Micro-WNOp test set." ></td>
	<td class="line x" title="129:203	Then we compute a subjectivity score S for each synset by summing up the weight values of all subjectivity clues in its gloss." ></td>
	<td class="line x" title="130:203	If S is equal or higher than an agreed threshold T, then the synset is classified as subjective, otherwise as objective." ></td>
	<td class="line x" title="131:203	For the GI lexicon, all subjectivity clues have the same weight 1, whereas for the SL list we assign a weight value 2 to strongly subjective clues and 1 to weakly subjective clues." ></td>
	<td class="line x" title="132:203	We experimented with several thresholds T and report here the results for the best thresholds, which were 2 for SL and 4 for the GI word list." ></td>
	<td class="line x" title="133:203	The corresponding methods are called Rule-SL and Rule-GI." ></td>
	<td class="line x" title="134:203	This approach does not allow us to easily integrate relational WordNet features." ></td>
	<td class="line x" title="135:203	It might also suffer from the incompleteness of the lexica and the fact that it has to make decisions for borderline cases (at the value of the threshold set)." ></td>
	<td class="line x" title="136:203	We therefore explored instead to generate larger, more reliable training data consisting of WordNet synsets from the word lists." ></td>
	<td class="line x" title="137:203	To achieve this, we assign a subjectivity score S as above to all WordNetsynsets(excludingsynsetsinthetestset)." ></td>
	<td class="line x" title="138:203	If S is higher or equal to a threshold T1 it is added to the subjective training set, if it is lower or equal to T2 it is added to the objective training set." ></td>
	<td class="line x" title="139:203	This allows us to choose quite clear thresholds so that borderline cases with a score between T1 and T2 arenotinthetrainingset." ></td>
	<td class="line x" title="140:203	Italsoallowstousepart829 of-speech and relational features as the training set then consists of WordNet synsets." ></td>
	<td class="line x" title="141:203	In this way, we can automatically generate (potentially noisy) training data of WordNet senses marked for subjectivity without annotating any WordNet senses manually for subjectivity." ></td>
	<td class="line x" title="142:203	We experimented with several different thresholdsetsbutwefoundthattheyactuallyhaveaminimal impact on the final results." ></td>
	<td class="line x" title="143:203	We report here the best results for a threshold T1 of 4 and T2 of 2 for the SL lexicon and of 3 and 1 respectively for the GI word list." ></td>
	<td class="line x" title="144:203	5 Experiments and Evaluation We measure the classification performance with overall accuracy as well as precision, recall and balancedF-scoreforbothcategories(objectiveand subjective)." ></td>
	<td class="line x" title="145:203	All results are summarised in Table 1." ></td>
	<td class="line x" title="146:203	Results are compared to the baseline of majority classification using a McNemar test at the significance level of 5%." ></td>
	<td class="line x" title="147:203	5.1 Experimental Results Table 1 shows that SL performs best among all the methodologies." ></td>
	<td class="line x" title="148:203	All CV, Rule-SL and SL methods significantly beat the baseline." ></td>
	<td class="line x" title="149:203	In addition, if we compare the results of methods with and without additional parts-of-speech and WordNet relation features, we see a small but consistent improvement when we use additional features." ></td>
	<td class="line x" title="150:203	It is also worthwhile to expand the rule-based unsupervised method into a method for generating training data and use additional features as SL significantly outperforms Rule-SL." ></td>
	<td class="line x" title="151:203	5.2 Discussion WordLists." ></td>
	<td class="line x" title="152:203	Surprisingly,usingSLgreatlyoutperforms GI, regardless of whether we use the supervised or unsupervised method or whether we use lexical features only or the other features as well.9 There are several reasons for this." ></td>
	<td class="line x" title="153:203	First, the GI lexicon is annotated for polarity, not subjectivity." ></td>
	<td class="line x" title="154:203	More specifically, words that we see as objective but with a strong positive or negative association (such as words for crimes) and words that we see as subjective are annotated with the same polarity label in the GI lexicon." ></td>
	<td class="line x" title="155:203	Therefore, the GI definition of subjectivity does not match ours." ></td>
	<td class="line x" title="156:203	Also, 9This pattern is repeated for all threshold combinations, which are not reported here." ></td>
	<td class="line x" title="157:203	the GI lexicon does not operate with a clearly expressed polarity definition, leading to conflicting annotations and casting doubt on its widespread use in the opinion mining community as a gold standard (Turney and Littman, 2003; Takamura et al., 2005; Andreevskaia and Bergler, 2006)." ></td>
	<td class="line x" title="158:203	For example, amelioration is seen as non-polar in GI but improvement is annotated with positive polarity." ></td>
	<td class="line x" title="159:203	Second, in contrast to SL, GI does not consider different parts-of-speech of a word and subjectivity strength (strong/weak subjectivity)." ></td>
	<td class="line x" title="160:203	Third, GI contains many fewer subjective clues than SL." ></td>
	<td class="line x" title="161:203	Sentence Data." ></td>
	<td class="line x" title="162:203	When using the Movie dataset and MPQA corpus as training data, the results are not satisfactory." ></td>
	<td class="line x" title="163:203	We first checked the purity of these two datasets to see whether they are too noisy." ></td>
	<td class="line x" title="164:203	For this purpose, we used a naive Bayes algorithm with unigram features and conducted a 10-foldcrossvalidationexperimentonrecognizing subjective/objective sentences within the Movie dataset and MPQA independently." ></td>
	<td class="line x" title="165:203	Interestingly, the accuracy for the Movie dataset and MPQA corpus achieved 91% and 76% respectively." ></td>
	<td class="line x" title="166:203	Considering that they are balanced datasets with a most frequent category baseline of about 50%, this accuracy is high, especially for the Movie dataset." ></td>
	<td class="line x" title="167:203	However, again the subjectivity definition in the Movie corpus does not seem to match ours." ></td>
	<td class="line x" title="168:203	Recall that we see a word sense or a sentence as subjective if it expresses a private state (i.e., emotion, opinion, sentiment, etc.), and objective otherwise." ></td>
	<td class="line x" title="169:203	Inspecting the movie data set, we found that indeed the sentences included in its subjective set would mostly be seen as subjective in our sense as well as they contain opinions about the movie such as it desperately wants to be a wacky , screwball comedy , but the most screwy thing here is how so many talented people were convinced to waste their time." ></td>
	<td class="line x" title="170:203	It is also true that the sentences (plot descriptions) in its objective data set relatively rarely contain opinions about the movie." ></td>
	<td class="line x" title="171:203	However, they still contain other opinionated content like opinions and emotions of the characters in the movie such as the obsession of a character with John Lennon in the beatles fan is a drama about Albert, a psychotic prisoner who is a devoted fan of John Lennon and the beatles." ></td>
	<td class="line x" title="172:203	Since the data sets definition of subjective sentences is closer to ours than the one for objective sentences, we conducted a one-class learning approach (Li and Liu, 2003) using Movies subjective sentences as 830 Table 1: Results Method Subjective Objective Accuracy Precision Recall F-score Precision Recall F-score Baseline N/A 0 N/A 66.3% 100% 79.7% 66.3% CV 65.2% 52.8% 58.3% 78.1% 85.6% 81.7% 74.6% CV 69.5% 55.3% 61.6% 79.4% 87.6% 83.3% 76.7% Movie 43.8% 60.1% 50.6% 74.9% 60.7% 67.1% 60.5% MPQA 44.5% 78.5% 56.8% 82.1% 50.1% 62.2% 59.7% GI 50.4% 39.4% 44.2% 72.2% 80.2% 76.0% 66.4% GI 54.5% 33.5% 41.5% 71.7% 85.8% 78.1% 68.1% SL 64.3% 62.8% 63.6% 81.3% 82.2% 81.8% 75.7% SL 66.2% 64.5% 65.3% 82.2% 83.2% 82.7% 76.9% Rule-GI 38.5% 5.6% 9.8% 66.5% 95.4% 78.4% 65.1% Rule-SL 59.7% 70.4% 64.6% 83.4% 75.8% 79.4% 74.0% 1 CV, GI and SL correspond to methods using lexical features only." ></td>
	<td class="line x" title="173:203	2 CV, GI and SL correspond to methods using a feature combination of lexical, part-of-speech, and WordNet relations." ></td>
	<td class="line x" title="174:203	3  indicates results significantly better than the baseline." ></td>
	<td class="line x" title="175:203	the only training data." ></td>
	<td class="line x" title="176:203	The algorithm 10 combines Expectation Maximization and Naive Bayes algorithms, and we used randomly extracted 50,000 unlabeled synsets in WordNet as the necessary unlabeled data." ></td>
	<td class="line x" title="177:203	This approach achieves an accuracy of 69.4% on Micro-WNOp, which is significantly better than the baseline." ></td>
	<td class="line x" title="178:203	The subjectivity definition in the MPQA corpus is quite close to ours." ></td>
	<td class="line x" title="179:203	However, our mapping from its phrase annotation to sentence annotation might betoocoarse-grainedasmanysentencesinthecorpus span several clauses containing both opinions andfactualdescription." ></td>
	<td class="line x" title="180:203	Weassumethatthisispossibly also the reason why its purity is lower than in the Movie dataset." ></td>
	<td class="line x" title="181:203	We therefore experimented again with a one-class learning approach using just the subjective phrases in MPQA as training data." ></td>
	<td class="line x" title="182:203	The accuracy does improve to 67.6% but is still not significantly higher than the baseline." ></td>
	<td class="line x" title="183:203	5.3 Comparison to Prior Approaches Esuli and Sebastiani (2006) make their labeled WordNet SentiWordNet 1.0 publically available.11 Recall that they actually use polarity classification: however, as there is a dependency between polarity and subjectivity classification for subjective senses, we map their polarity scores to our subjectivity labels as follows." ></td>
	<td class="line x" title="184:203	If the sum of positive and 10Available at http://www.cs.uic.edu/liub/LPU/." ></td>
	<td class="line x" title="185:203	11Available at http://sentiwordnet.isti.cnr." ></td>
	<td class="line x" title="186:203	it/ negativescoresofasenseinSentiWordNetismore than or equal to 0.5, then it is subjective and otherwise objective.12 Using this mapping, it achieves an accuracy of 75.3% on the Micro-WNOp corpus, compared to our gold standard." ></td>
	<td class="line x" title="187:203	Therefore our methods CV and SL perform slightly better than theirs, although the improvement is not significant." ></td>
	<td class="line x" title="188:203	The task definition in Wiebe and Mihalcea (2006) is much more similar to ours but they use different annotated test data, which is not publicallyavailable,soanexactcomparisonisnotpossible." ></td>
	<td class="line x" title="189:203	Both data sets, however, seem to include relatively frequent words." ></td>
	<td class="line x" title="190:203	One disadvantage of their method is that it is not applicable to all WordNet senses as it is dependent on distributionally similar words being available in the MPQA." ></td>
	<td class="line x" title="191:203	Thus, 23% of their test data is excluded from evaluation, whereas our methods can be used on any WordNet sense." ></td>
	<td class="line x" title="192:203	They measure precision and recall for subjective senses in a precision/recall curve: Precision is about 48/9% at a recall of 60% for subjective senses whereas our best SL method has a precision of 66% at about the same recall." ></td>
	<td class="line x" title="193:203	Although this suggests better performance of our method, it is not possible to draw final conclusions from this comparison due to the data set differences." ></td>
	<td class="line x" title="194:203	12We experimented with slightly different mappings but this mapping gave SentiWordNet the best possible result." ></td>
	<td class="line x" title="195:203	There is a relatively large number of cases with a 0.5/0.5 split in SentiWordNet, making it hard to decide between subjective and objective senses." ></td>
	<td class="line x" title="196:203	831 6 Conclusion and Future Work We proposed different ways of extracting training dataandcluesetsforwordsensesubjectivitylabeling from existing opinion mining resources." ></td>
	<td class="line x" title="197:203	The effectiveness of the resulting algorithms depends greatlyonthegeneratedtrainingdata, morespecifically on the different definitions of subjectivity used in resource creation." ></td>
	<td class="line x" title="198:203	However, we were able to show that at least one of these methods (based on the SL word list) resulted in a classifier that performed on a par with a supervised classifier that useddedicatedtrainingdatadevelopedforthistask (CV)." ></td>
	<td class="line x" title="199:203	Thus, it is possible to avoid any manual annotation for the subjectivity classification of word senses." ></td>
	<td class="line x" title="200:203	Our future work will explore new methodologies in feature representation by importing more background information (e.g., syntactic information)." ></td>
	<td class="line x" title="201:203	Furthermore, our current method of integrating the rich relation information in WordNet (using them as standard features) does not use joint classification of several senses." ></td>
	<td class="line x" title="202:203	Instead, we think it will be more promising to use the relations to construct graphs for semi-supervised graph-based learningofwordsensesubjectivity." ></td>
	<td class="line x" title="203:203	Inaddition,we will also explore whether the derived sense labels improve applications such as sentence classification and clustering WordNet senses." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1111
Emotion Classification Using Massive Examples Extracted from the Web
Tokhisa, Ryoko;Inui, Kentaro;Matsumoto, Yuji;"></td>
	<td class="line x" title="1:185	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 881888 Manchester, August 2008 Emotion Classification Using Massive Examples Extracted from the Web Ryoko TOKUHISA   Toyota Central R&D Labs., INC. Nagakute Aichi JAPAN tokuhisa@mosk.tytlabs.co.jp Kentaro INUI   Nara Institute of Science and Technology Ikoma Nara JAPAN {ryoko-t,inui,matsu}@is.naist.jp Yuji MATSUMOTO  Abstract In this paper, we propose a data-oriented method for inferring the emotion of a speaker conversing with a dialog system from the semantic content of an utterance." ></td>
	<td class="line x" title="2:185	We first fully automatically obtain a huge collection of emotion-provoking event instances from the Web." ></td>
	<td class="line x" title="3:185	With Japanese chosen as a target language, about 1.3 million emotion provoking event instances are extracted using an emotion lexicon and lexical patterns." ></td>
	<td class="line x" title="4:185	We then decompose the emotion classification task into two sub-steps: sentiment polarity classification (coarsegrained emotion classification), and emotion classification (fine-grained emotion classification)." ></td>
	<td class="line x" title="5:185	For each subtask, the collection of emotion-proviking event instances is used as labelled examples to train a classifier." ></td>
	<td class="line x" title="6:185	The results of our experiments indicate that our method significantly outperforms the baseline method." ></td>
	<td class="line x" title="7:185	We also find that compared with the singlestep model, which applies the emotion classifier directly to inputs, our two-step model significantly reduces sentiment polarity errors, which are considered fatal errors in real dialog applications." ></td>
	<td class="line x" title="8:185	1 Introduction Previous research into human-computer interaction has mostly focused on task-oriented dialogs, where the goal is considered to be to achieve a c2008." ></td>
	<td class="line x" title="9:185	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="10:185	Some rights reserved." ></td>
	<td class="line x" title="11:185	given task as precisely and efficiently as possible by exchanging information required for the task through dialog (Allen et al., 1994, etc.)." ></td>
	<td class="line x" title="12:185	More recent research (Foster, 2007; Tokuhisa and Terashima, 2006, etc.), on the other hand, has been providing evidence for the importance of the affective or emotional aspect in a wider range of dialogic contexts, which has been largely neglected in the context of task-oriented dialogs." ></td>
	<td class="line x" title="13:185	A dialog system may be expected to serve, for example, as an active listening 1 partner of an elderly user living alone who sometimes wishes to have a chat." ></td>
	<td class="line x" title="14:185	In such a context, the dialog system is expected to understand the users emotions and sympathize with the user." ></td>
	<td class="line x" title="15:185	For example, given an utterence I traveled far to get to the shop, but it was closed from the user, if the system could infer the users emotion behind it, it would know that it would be appropriate to say Thats too bad or Thats really disappointing." ></td>
	<td class="line x" title="16:185	It can be easily imagined that such affective behaviors of a dialog system would be beneficial not only for active listening but also for a wide variety of dialog purposes including even task-oriented dialogs." ></td>
	<td class="line x" title="17:185	To be capable of generating sympathetic responses, a dialog system needs a computational model that can infer the users emotion behind his/her utterence." ></td>
	<td class="line x" title="18:185	There have been a range of studies for building a model for classifying a users emotions based on acoustic-prosodic features and facial expressions (Pantic and Rothkrantz, 2004, etc.)." ></td>
	<td class="line x" title="19:185	Such methods are, however, severely limited in that they tend to work well only when the user expresses his/her emotions by exaggerated 1 Active listening is a specific communication skill, based on the work of psychologist Carl Rogers, which involves giving free and undivided attention to the speaker (Robertson, 2005)." ></td>
	<td class="line x" title="20:185	881 X54X68X65X20X72X65X73X74X61X75X72X61X6EX74X20X77X61X73X20 X20X20X20X76X65X72X79X20X66X61X72X20X62X75X74X20X69X74X20X77X61X73X20 X20X20X20X63X6CX6FX73X65X64 X49X6EX70X75X74 X45X6DX6FX74X69X6FX6EX20 X63X6CX61X73X73X69X66X69X63X61X74X69X6FX6EX20 X75X73X69X6EX67X20X45X50X20X43X6FX72X70X75X73X20 X28X73X65X63X74X69X6FX6EX20X33X2EX34X29 X3CX64X69X73X61X70X70X6FX69X6EX74X6DX65X6EX74X3E X4FX75X74X70X75X74 X49X20X77X61X73X20X64X69X73X61X70X70X6FX69X6EX74X65X64X20X62X65X63X61X75X73X65X20X74X68X65X20X73X68X6FX70X20X77X61X73X20 X63X6CX6FX73X65X64X20X61X6EX64X20X49 X64X20X74X72X61X76X65X6CX65X64X20X61X20X6CX6FX6EX67X20X77X61X79X20X74X6FX20X67X65X74X20X74X68X65X72X65X20 X49X20X77X61X73X20X64X69X73X61X70X70X6FX69X6EX74X65X64X20X74X68X61X74X20X69X74X20X73X74X61X72X74X65X64X20X72X61X69X6EX69X6EX67X20 X49X20X61X6DX20X68X61X70X70X79X20X73X69X6EX63X65X20X74X68X65X20X62X6FX6FX6BX20X73X74X6FX72X65X20X77X61X73X20X6FX70X65X6EX20 X77X68X65X6EX20X49X20X67X6FX74X20X62X61X63X6BX20X68X6FX6DX65X20 X57X65X62X20X54X65X78X74 X53X65X6EX74X69X6DX65X6EX74X20 X70X6FX6CX61X72X69X74X79X20 X63X6CX61X73X73X69X66X69X63X61X74X69X6FX6EX20 X28X73X65X63X74X69X6FX6EX20X33X2EX33X29 X45X6DX6FX74X69X6FX6EX2DX70X72X6FX76X6FX6BX69X6EX67X20X65X76X65X6EX74X20X63X6FX72X70X75X73X20X28X45X50X20X63X6FX72X70X75X73X29 X74X68X65X20X73X68X6FX70X20X77X61X73X20X63X6CX6FX73X65X64X20X61X6EX64X20X49 X64X20 X74X72X61X76X65X6CX65X64X20X61X20X6CX6FX6EX67X20X77X61X79X20X74X6FX20X67X65X74X20X74X68X65X72X65X20 X69X74X20X73X74X61X72X74X65X64X20X72X61X69X6EX69X6EX67X20 X20 X49X20X61X6DX20X61X6CX6FX6EX65X20X6FX6EX20X43X68X72X69X73X74X6DX61X73X20 X74X68X65X20X62X6FX6FX6BX20X73X74X6FX72X65X20X77X61X73X20X6FX70X65X6EX20X77X68X65X6EX20X49X20 X67X6FX74X20X62X61X63X6BX20X68X6FX6DX65 X45X6DX6FX74X69X6FX6EX2DX70X72X6FX76X6FX6BX69X6EX67X20X65X76X65X6EX74X20X20X20X20X20X20X20X20X20X20X20X20X45X6DX6FX74X69X6FX6EX28X70X6FX6CX61X72X69X74X79X29 X64X69X73X61X70X70X6FX69X6EX74X6DX65X6EX74X20 X28X6EX65X67X61X74X69X76X65X29 X68X61X70X70X69X6EX65X73X73X20X28X70X6FX73X69X74X69X76X65X29X20 X45X6DX6FX74X69X6FX6EX20X63X6CX61X73X73X69X66X69X63X61X74X69X6FX6E X42X75X69X6CX64X69X6EX67X20 X65X6DX6FX74X69X6FX6EX2DX70X72X6FX76X6FX6BX69X6EX67X20 X65X76X65X6EX74X20X63X6FX72X70X75X73X20 X28X73X65X63X74X69X6FX6EX20X33X2EX32X29 X4FX46X46X4CX49X4EX45 X4FX4EX4CX49X4EX45 X6CX6FX6EX65X6CX69X6EX65X73X73X20X28X6EX65X67X61X74X69X76X65X29X20 X64X69X73X61X70X70X6FX69X6EX74X6DX65X6EX74X20 X28X6EX65X67X61X74X69X76X65X29 Figure 1: Overview of our approach to emotion classification prosodic or facial expressions." ></td>
	<td class="line x" title="21:185	Furthermore, what is required in generating sympathetic responses is the identification of the users emotion in a finer grain-size." ></td>
	<td class="line x" title="22:185	For example, in contrast to the above example of disappointing, one may expect the response to My pet parrot died yesterday should be Thats really sad, wheras the response to I may have forgotten to lock my house should be Youre worried about that." ></td>
	<td class="line x" title="23:185	In this paper, we address the above issue of emotion classification in the context of humancomputer dialog, and demonstrate that massive examples of emotion-provoking events can be extracted from the Web with a reasonable accuracy and those examples can be used to build a semantic content-based model for fine-grained emotion classification." ></td>
	<td class="line x" title="24:185	2 Related Work Recently, several studies have reported about dialog systems that are capable of classifying emotions in a human-computer dialog (Batliner et al., 2004; Ang et al., 2002; Litman and Forbes-Riley, 2004; Rotaru et al., 2005)." ></td>
	<td class="line x" title="25:185	ITSPOKE is a tutoring dialog system, that can recognize the users emotion using acoustic-prosodic features and lexical features." ></td>
	<td class="line x" title="26:185	However, the emotion classes are limited to Uncertain and Non-Uncertain because the purpose of ITSPOKE is to recognize the users problem or discomfort in a tutoring dialog." ></td>
	<td class="line x" title="27:185	Our goal, on the other hand, is to classify the users emotions into more fine-grained emotion classes." ></td>
	<td class="line x" title="28:185	In a more general research context, while quite a few studies have been presented about opinion mining and sentiment analysis (Liu, 2006), research into fine-grained emotion classification has emerged only recently." ></td>
	<td class="line x" title="29:185	There are two approaches commonly used in emotion classification: a rulebased approach and a statistical approach." ></td>
	<td class="line x" title="30:185	Masum et al.(2007) and Chaumartin (2007) propose a rule-based approach to emotion classification." ></td>
	<td class="line x" title="32:185	Chaumartin has developed a linguistic rulebased system, which classifies the emotions engendered by news headlines using the WordNet, SentiWordNet, and WordNet-Affect lexical resources." ></td>
	<td class="line x" title="33:185	The system detects the sentiment polarity for each word in a news headline based on linguistic resources, and then attempts emotion classification by using rules based on its knowledge of sentence structures." ></td>
	<td class="line x" title="34:185	The recall of this system is low, however, because of the limited coverage of the lexical resources." ></td>
	<td class="line x" title="35:185	Regarding the statistical approach, Kozareva et al.(2007) apply the theory of (Hatzivassiloglou and McKeown, 1997) and (Turney, 2002) to emotion classification and propose a method based on the co-occurrence distribution over content words and six emotion words (e.g. joy, fear)." ></td>
	<td class="line x" title="37:185	For example, birthday appears more often with joy, while war appears more often with fear." ></td>
	<td class="line x" title="38:185	However, the accuracy achieved by their method is not practical in applications assumed in this paper." ></td>
	<td class="line x" title="39:185	As we demonstrate in Section 4, our method significantly outperforms Kozarevas method." ></td>
	<td class="line x" title="40:185	3 Emotion Classification 3.1 The basic idea We consider the task of emotion classification as a classification problem where a given input sentence (a users utterance) is to be classified either into such 10 emotion classes as given later in Table1orasneutral if no emotion is involved in the input." ></td>
	<td class="line x" title="41:185	Since it is a classification problem, the task should be approached straightforwardly in a vari882 Table 1: Distribution of the emotion expressions and examples Sentiment 10 Emotion Emotion lexicon (349 Japanese emotion words) Polarity Classes Total Examples happiness 90.`M(happy)|*(joyful)|*(glad)|(glad) Positive pleasantness 7`M(pleasant)|`(enjoy)|`(can enjoy) relief 5	(relief)|lq(relief) fear 22M(fear)| M(fear)|`M(frightening) sadness 21 u`M(sad)|i`M(sad)| u`(feel sad) disappointment 15UlT(lose heart)|UlX(drop ones head) Negative unpleasantness 109O(disgust)|OU(dislike)|OM(dislike) loneliness 15	`M(lonely)|`M(lonely)||`M(lonely) anxiety 17 (anxiety)|	 (anxiety)|>UT(worry) anger 48qh`M(angry)| qm(get angry)|q (angry) ety of machine learning-based methods if a sufficient number of labelled examples were available." ></td>
	<td class="line x" title="42:185	Our basic idea is to learn what emotion is typically provoked in what situation, from massive examples that can be collected from the Web." ></td>
	<td class="line x" title="43:185	The development of this approach and its subsequent implementation has forced us to consider the following two issues." ></td>
	<td class="line x" title="44:185	First, we have to consider the quantity and accuracy of emotion-provoking examples to be collected." ></td>
	<td class="line x" title="45:185	The process we use to collect emotionprovoking examples is illustrated in the upper half of Figure 1." ></td>
	<td class="line x" title="46:185	For example, from the sentence I was disappointed because the shop was closed and Id I traveled a long way to get there, pulled from the Web, we learn that the clause the shop was closed and Id traveled a long way to get there is an example of an event that provokes disappointment." ></td>
	<td class="line x" title="47:185	In this paper, we refer to such an example as an emotion-provoking event and a collection of eventprovoking events as an emotion-provoking event corpus (an EP corpus)." ></td>
	<td class="line x" title="48:185	Details are described in Section 3.2." ></td>
	<td class="line x" title="49:185	Second, assuming that an EP corpus can be obtained, the next issue is how to use it for our emotion classification task." ></td>
	<td class="line x" title="50:185	We propose a method whereby an input utterance (sentence) is classified in two steps, sentiment polarity classification followed by fine-grained emotion classification as shown in the lower half of Figure 1." ></td>
	<td class="line x" title="51:185	Details are given in Sections 3.3 and 3.4." ></td>
	<td class="line x" title="52:185	3.2 Building an EP corpus We used ten emotions happiness, pleasantness, relief, fear, sadness, disappointment, unpleasantness, loneliness, anxiety, anger in our emotion classification experiment." ></td>
	<td class="line x" title="53:185	First, we built a handcrafted lexicon of emotion words classified into the ten emotions." ></td>
	<td class="line x" title="54:185	From the Japanese Evaluation Expression Dictionary created by Kobayashi et al.(2005), we identified 349 emotion words based X73X75X62X6FX72X64X69X6EX61X74X65X20X63X6CX61X75X73X65 X65X6DX6FX74X69X6FX6EX20X70X72X6FX76X6FX6BX69X6EX67X20X65X76X65X6EX74 X63X6FX6EX6EX65X63X74X69X76X65 X65X6DX6FX74X69X6FX6EX20X6CX65X78X69X63X6FX6E Figure 2: An example of a lexico-syntactic pattern Table 2: Number of emotion-provoking events 10 Emotions EP event 10 Emotions EP event happiness 387,275 disappoint106,284 ment pleasantness 209,682 unpleasantness 396,002 relief 46,228 loneliness 26,493 fear 49,516 anxiety 45,018 sadness 31,369 anger 8,478 on the definition of emotion words proposed by Teramura (1982)." ></td>
	<td class="line x" title="56:185	The distribution is shown in Table 1 with major examples." ></td>
	<td class="line x" title="57:185	We then went on to find sentences in the Web corpus that possibly contain emotion-provoking events." ></td>
	<td class="line x" title="58:185	A subordinate clause was extracted as an emotion-provoking event instance if (a) it was subordinated to a matrix clause headed by an emotion word and (b) the relation between the subordinate and matrix clauses is marked by one of the following eight connectives:wp,T,h, o,wx,wU,\qx,\qU. An example is given in Figure 2." ></td>
	<td class="line x" title="59:185	In the sentence  U	Z`h wxUlTi(I was disappointed that it suddenly started raining), the subordinate clause   U	Z`h(it suddenly started raining) modifies UlTi(I was disappointed) with the connective wx(that)." ></td>
	<td class="line x" title="60:185	In this case, therefore, the event mention  U	Z`h(it suddenly started raining) is learned as an event instance that provokesdisappointment." ></td>
	<td class="line x" title="61:185	Applying the emotion lexicon and the lexical patterns to the Japanese Web corpus (Kawahara and Kurohashi, 2006), which contains 500 million sentences, we were able to collect about 1.3 million events as causes of emotion." ></td>
	<td class="line x" title="62:185	The distribution is shown in Table 2." ></td>
	<td class="line x" title="63:185	Tables 3 and 4 show the results of our evalua883 Table 4: Examples from in the EP corpus EP-Corpus Result of evaluation Emotion-provoking Event Emotion word 10 Emotions (P/N) Polarity Emotion VjUqM(A flower died quickly)i(diappointed) disappointment(N) Correct Correct 'UM(There are a lot of enemies)^V(lose interest) unpleasantness(N) Correct Context-dep." ></td>
	<td class="line x" title="64:185	j[JUM(There is a lot of Chinese cabbage) .`M(happy) happiness(P) Context-dep." ></td>
	<td class="line x" title="65:185	Context-dep." ></td>
	<td class="line x" title="66:185	UhM(I would like to drink orange juice) G!i(terrible) unpleasantness(N) Error Error Table 3: Correctness of samples from the EP corpus Polarity Emotion Correct 1140 (57.0%) 988 (49.4%) Context-dep." ></td>
	<td class="line x" title="67:185	678 (33.9%) 489 (24.5%) Error 182 (9.1%) 523 (26.2%) tion for the resultant EP corpus." ></td>
	<td class="line x" title="68:185	One annotator, who was not the developer of the EP corpus, evaluated 2000 randomly chosen events." ></td>
	<td class="line x" title="69:185	The Polarity column in Table 3 shows the results of evaluating whether the sentiment polarity of each event is correctly labelled, whereas theEmotion column shows the correctness at the level of the 10 emotion classes." ></td>
	<td class="line x" title="70:185	The correctness of each example was evaluated as exemplified in Table 4." ></td>
	<td class="line x" title="71:185	Correct indicates a correct example, Contex-dep." ></td>
	<td class="line x" title="72:185	indicates a context-dependent example, and Error is an error example." ></td>
	<td class="line x" title="73:185	For example, in the case of There are a lot of enemies in Table 4, the Polarity is Correct because it represents a negative emotion." ></td>
	<td class="line x" title="74:185	However, its emotion classunpleasantness is judged Context-dep." ></td>
	<td class="line x" title="75:185	As Table 3 shows, the Sentiment Polarity is correct in 57.0% of cases and partially correct (Correct + Context-dep.)" ></td>
	<td class="line x" title="76:185	in 90.9% of cases." ></td>
	<td class="line x" title="77:185	On the other hand, the Emotion is correct in only 49.4% of cases and partially correct in 73.9% of cases." ></td>
	<td class="line x" title="78:185	These figures may not seem very impressive." ></td>
	<td class="line x" title="79:185	As far as its impact on the emotion classification accuracy is concerned, however, the use of our EP corpus, which requires no supervision, makes remarkable improvements upon Kozareva et al.(2007)s unsupervised method as we show later." ></td>
	<td class="line x" title="81:185	3.3 Sentiment Polarity Classification Given the large collection of emotion-labelled examples, it may seem straightforward to develop a trainable model for emotion classification." ></td>
	<td class="line x" title="82:185	Before moving on to emotion classification, however, it should be noted that a users input utterance may not involve any emotion." ></td>
	<td class="line x" title="83:185	For example, if the user gives an utterance I have a lunch at the school cafeteria every day, it is not appropriate for the system to make any sympathetic response." ></td>
	<td class="line x" title="84:185	In such a case, the users input should be classified as neutral." ></td>
	<td class="line x" title="85:185	The classification between emotion-involved and neutral is not necessarily a simple problem, however, because we have not found yet any practical method for collecting training examples of the classneutral." ></td>
	<td class="line x" title="86:185	We cannot rely on the analogy to the pattern-based method we have adopted to collect emotion-provoking events  there seems no reliable lexico-syntactic pattern for extracting neutral examples." ></td>
	<td class="line x" title="87:185	Alternatively, if the majority of the sentences on the Web were neutral, one would simply use a set of randomly sampled sentences as labelled data for neutral." ></td>
	<td class="line x" title="88:185	This strategy, however, does not work because neutral sentences are not the majority in real Web texts." ></td>
	<td class="line x" title="89:185	As an attempt, we collected 1000 sentences randomly from the Web and investigated their distribution of sentiment polarity." ></td>
	<td class="line x" title="90:185	The results, shown in Table 5, revealed that the ratio of neutral events was unexpectedly low." ></td>
	<td class="line x" title="91:185	These results indicate the difficulty of collecting neutral events from Web documents." ></td>
	<td class="line x" title="92:185	Taking this problem into account, we adopt a two-step approach, where we first classify a given input into three sentiment polarity classes, either positive, negative or neutral, and then classify only those judged positive or negative into our 10 finegrained emotion classes." ></td>
	<td class="line x" title="93:185	In the first step, i.e. sentiment polarity classification, we use only the positive and negative examples stored in the EP corpus and assume sentence to be neutral if the output of the classification model is near the decision boundary." ></td>
	<td class="line x" title="94:185	There are additional advantages in this approach." ></td>
	<td class="line x" title="95:185	First, it is generally known that performing fine-grained classification after coarse classification often provides good results particularly when the number of the classes is large." ></td>
	<td class="line x" title="96:185	Second, in the context of dialog, a misunderstanding the users emotion at the sentiment polarity level would be a disaster." ></td>
	<td class="line x" title="97:185	Imagine that the system says You must be happy when the user in fact feels sad." ></td>
	<td class="line x" title="98:185	As we show in Section 4.2, such fatal errors can be reduced by taking the two-step approach." ></td>
	<td class="line x" title="99:185	884 Table 5: Distribution of the Sentiment polarity of sentences randomly sampled from the Web Sentiment Polarity Number Ratio positive 650 65.0% negative 153 15.3% neutral 117 11.7% Context-dep." ></td>
	<td class="line oc" title="100:185	80 8.0% Positive child education Positive cost Negative SUBJECT increase Figure 3: An example of a word-polarity lattice Various methods have already been proposed for sentiment polarity classification, ranging from the use of co-occurrence with typical positive and negative words (Turney, 2002) to bag of words (Pang et al., 2002) and dependency structure (Kudo and Matsumoto, 2004)." ></td>
	<td class="line x" title="101:185	Our sentiment polarity classification model is trained with SVMs (Vapnik, 1995), and the features are {1-gram, 2-gram, 3gram} of words and the sentiment polarity of the words themselves." ></td>
	<td class="line x" title="102:185	Figure 3 illustrates how the sentence ww rU Q(The cost of educating my child increases) is encoded to a feature vector." ></td>
	<td class="line x" title="103:185	Here we assume the sentiment polarity of the (child) and (education) are positive, while the r(cost) is negative." ></td>
	<td class="line x" title="104:185	These polarity values are represented in parallel with the corresponding words, as shown in Figure 3." ></td>
	<td class="line x" title="105:185	By expanding {1-gram, 2-gram, 3-gram} in this lattice representation, the following list of features are extracted:(child), Positive,(child)w(of), Positive-w(of),(child)-w(of)-(education), etc The polarity value of each word is defined in our sentiment polarity dictionary, which includes 1880 positive words and 2490 negative words." ></td>
	<td class="line x" title="106:185	To create this dictionary, one annotator identified positive and negative words from the 50 thousand most frequent words sampled from the Web." ></td>
	<td class="line x" title="107:185	Table 6 shows some examples." ></td>
	<td class="line x" title="108:185	3.4 Emotion Classification For fine-grained emotion classification, we propose a k-nearest-neighbor approach (kNN) using the EP corpus." ></td>
	<td class="line x" title="109:185	Given an input utterance, the kNN model retrieves k-most similar labelled examples from the EP corpus." ></td>
	<td class="line x" title="110:185	Given the input The restaurant was very far but it was closed as Figure 1, for example, the kNN model finds similar labelled examples, say, labelled example {the shop was closed and Id traveled far to get there} in the EP corpus." ></td>
	<td class="line x" title="111:185	Table 6: Examples of positive and negative words P(child)|Fs(summer vacation)| qm(useful)| Rb(succeed) N r(cost)|`M(difficult)| `M(difficult)|b(failure) Ranking of similar events rank event emotion similarity 1." ></td>
	<td class="line x" title="112:185	2. 2." ></td>
	<td class="line x" title="113:185	4. 5." ></td>
	<td class="line x" title="114:185	{event1} <disappointment> 1." ></td>
	<td class="line x" title="115:185	2. 3." ></td>
	<td class="line x" title="116:185	{event2} <unpleasantness> {event3} <loneliness>          0.70 {event4} <loneliness>          0.67 0.75 0.70 {event5} <loneliness>          0.63 Ranking of emotion rank emotion score <loneliness> <unpleasantness> <disappointment> 2.0 0.75 0.70 voting Figure 4: Emotion Classification by kNN (k=5) For the similarity measure, we use cosine similarity between bag-of-words vectors; sim(I,EP)= IEP |I||EP| for input sentence I and an emotionprovoking event EPin the EP corpus." ></td>
	<td class="line x" title="117:185	The score of each class is given by the sum of its similarity scores." ></td>
	<td class="line x" title="118:185	An example is presented in Figure 4." ></td>
	<td class="line x" title="119:185	The emotion of the most similar event is disappointment, that of the second-most similar event is unpleasantness tied with loneliness." ></td>
	<td class="line x" title="120:185	After calculating the sum for each emotion, the system outputs loneliness as the emotion for the input I because the score for loneliness is the highest." ></td>
	<td class="line x" title="121:185	4 Experiments 4.1 Sentiment polarity classification We conducted experiments on sentiment polarity classification using the following two test sets: TestSet1: The first test set was a set of utterances which 6 subject speakers produced interacting with our prototype dialog system." ></td>
	<td class="line x" title="122:185	This data include 31 positive utterances, 34 negative utterances, and 25 neutral utterances." ></td>
	<td class="line x" title="123:185	TestSet2: For the second test set, we used the 1140 samples that were judged Correct with respect to sentiment polarity in Table 3." ></td>
	<td class="line x" title="124:185	491 samples (43.1%) were positive and 649 (56.9%) were negative." ></td>
	<td class="line x" title="125:185	We then added 501 neutral sentences newly sampled from the Web." ></td>
	<td class="line x" title="126:185	These samples are disjoint from the EP corpus used for training classifiers." ></td>
	<td class="line x" title="127:185	For each test set, we tested our sentiment polarity classifier in both the two-class (positive/negative) setting, where only positive or negative test samples were used, and the three-class (positive/negative/neutral) setting." ></td>
	<td class="line x" title="128:185	The performance was evaluated in F-measure." ></td>
	<td class="line x" title="129:185	885 Table 7: F-values of sentiment polarity classification (positive/negative) TestSet1 TestSet2 Pos Neg Pos Neg Word 0.839 0.853 0.794 0.842 Word + Polarity 0.833 0.857 0.793 0.841 Table 8: F-values of sentiment polarity classification (positive/negative/neutral) TestSet1 TestSet2 Pos Neg Pos Neg Word 0.743 0.758 0.610 0.742 Word + Polarity 0.758 0.769 0.610 0.742 Table 7 shows the results for the two-class setting, whereas Table 8 shows the results for the three-class." ></td>
	<td class="line x" title="130:185	Word denotes the model trained with only word n-gram features, whereas Word+Polarity denotes the model trained with n-gram features extracted from a word-polarity lattice (see Figure 3)." ></td>
	<td class="line x" title="131:185	The results shown in Table 7 indicate that both the Word and Word+Polarity models are capable of separating positive samples from negative ones at a high level of accuracy." ></td>
	<td class="line x" title="132:185	This is an important finding, given the degree of the correctness of our EP corpus." ></td>
	<td class="line x" title="133:185	As we have shown in Table 3, only 57% of samples in our EP corpus are exactly correct in terms of sentiment polarity." ></td>
	<td class="line x" title="134:185	The figures in Table 7 indicate that context-dependent samples are also useful for training a classifier." ></td>
	<td class="line x" title="135:185	Table 7 also indicates that no significant difference is found between the Word and Word+Polarity models." ></td>
	<td class="line x" title="136:185	In fact, we also examined another model which used dependency-structure information as well; however, no significant gain was achieved." ></td>
	<td class="line x" title="137:185	From these results, we speculate that, as far as the two-class sentiment polarity problem is concerned, word n-gram features might be sufficient if a very large set of labelled data are available." ></td>
	<td class="line x" title="138:185	On the other hand, Table 8 indicates that the three-class problem is much harder than the twoclass problem." ></td>
	<td class="line x" title="139:185	Specifically, positive sentences tend to be classified as neutral." ></td>
	<td class="line x" title="140:185	This method has to be improved in future models." ></td>
	<td class="line x" title="141:185	4.2 Emotion classification For fine-grained emotion classification, we used the following three test sets: TestSet1 (2p, best): Two annotators were asked to annotate each positive or negative sentence in TestSet1 with one of the 10 emotion classess." ></td>
	<td class="line x" title="142:185	The annotators chose only one emotion class even if they thought several emotions would fit a sentence." ></td>
	<td class="line x" title="143:185	Some examples are shown in Table 9." ></td>
	<td class="line x" title="144:185	The inter-annotator agreement is =0.76 in the kappa statistic (Cohen, 1960)." ></td>
	<td class="line x" title="145:185	For sentences annotated with two different labels (i.e. in the cases where the two annotators disagreed with), both labels were considered correct in the experiments  a models answer was considered correct if it was identical with either of the two labels." ></td>
	<td class="line x" title="146:185	TestSet1 (1p, acceptable): One of the above two annotators was asked to annotate each positive or negative sentence in TestSet1 with all the emotions involved in it." ></td>
	<td class="line x" title="147:185	The number of emotions for a positive sentence was 1.48 on average, and 2.47 for negative sentences." ></td>
	<td class="line x" title="148:185	Table 10 lists some examples." ></td>
	<td class="line x" title="149:185	In the experiments, a models answer was considered correct if it was identical with one of the labelled classes." ></td>
	<td class="line x" title="150:185	TestSet2: For TestSet2, we used the results of our judgments on the correctness for estimating the quality of the EP corpus described in Section 3.2." ></td>
	<td class="line x" title="151:185	In the experiments, the following two models were compared: Baseline: The baseline model simulates the method proposed by (Kozareva et al., 2007)." ></td>
	<td class="line x" title="152:185	Given an input sentence, their model first estimates the pointwise mutual information (PMI) between each content word cw j included in the sentence and emotion expression e  {anger, disgust, fear, joy, sudness, surprise} by PMI(e,cw)=log hits(e,cw) hits(e)hits(cw) , where hits(x) is a hit count of word(s) x on a Web search engine." ></td>
	<td class="line x" title="153:185	The model then calculates the score of each emotion class E i by summing the PMI scores between each content word cw j in the input and emotion expression e i corresponding to that emotion class: score(E i )= summationtext j PMI(e i ,cw j )." ></td>
	<td class="line x" title="154:185	Finally, the model chooses the best scored emotion class as an output." ></td>
	<td class="line x" title="155:185	For our experiments, we selected the following 10 emotion expressions: .`M(happy),`M(pleased),	(relieved),M(affraid),u`M(sad), (disappointed),O(hate),	`M(lonely), (anxious),qh`M(angry) For hit counts, we used the Google search engine." ></td>
	<td class="line x" title="156:185	886 Table 9: Examples of TestSet1 (2p, best) Annotator A Annotator B tlh(I got a Christmas present) happiness happiness awHt!|tX(Im going to go to my friends house ) pleasantness pleasantness V_tlh U	Z`h(It rained suddenly when I went to see the cherry blossoms) sadness sadness 	L:pqrTsM(My car cant move because of the traffic jam) unpleasantness anger Table 10: Examples of TestSet1 (1p, acceptable) Annotator A tlh(I got a Christmas present) happiness awHt!|tX(Im going to go to my friends house ) pleasantness, happiness V_tlh U	Z`h(It rained suddenly when I went to see the cherry blossoms) anger, sad, unpleasantness, disappointment 	L:pqrTsM(My car cant move because of the traffic jam) unpleasantness, anger :::::| :::::| :::::| :::::| :::::| :::::| :::::| :::::| :::::| :::::| :::::| :::::| :::::| :::::| ::::| :::::| :::::| :::::| :::::| :::::| :::::| : :| : : :| : : :| : : :| : : :| : : :| : : :| : : :| : : :| : : :| : : : :| : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :w : : : : : : : : : : : : : : : : : : : : : : : :w : : : : : : : : :w : :w : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : Figure 5: Results of emotion classification k-NN: We tested the 1-NN, 3-NN and 10-NN models." ></td>
	<td class="line x" title="157:185	In each model, we examined a single-step emotion classification and twostep emotion classification." ></td>
	<td class="line x" title="158:185	In the former method, the kNN model retrieves k-most similar examples from the all of the EP corpus." ></td>
	<td class="line x" title="159:185	In the latter method, when the sentiment polarity of the input utterance has obtained by the sentiment polarity classifier, the kNN model retrieves similar examples from only the examples whose sentiment polarity are the same as the input utterance in the EP corpus." ></td>
	<td class="line x" title="160:185	The results are shown in Figure 5." ></td>
	<td class="line x" title="161:185	Emotion Classification denotes the single-step models, whereas Sentiment Polarity + Emotion Classification denotes the two-step models." ></td>
	<td class="line x" title="162:185	An important observation from Figure 5 is that our models remarkably outperformed the baseline." ></td>
	<td class="line x" title="163:185	Apparently, an important motivation behind Kozareva et al.(2007)s method is that it does not require any manual supervion." ></td>
	<td class="line x" title="165:185	However, our models, which rely on emotion-provoking event instances, are also totally unsupervised  no supervision is required to collect emotion-provoking event instances." ></td>
	<td class="line x" title="166:185	Given this commonality between the two methods, the superiority of our method in accuracy can be considered as a crucial advantage." ></td>
	<td class="line x" title="167:185	Regarding the issue of single-step vs. two-step, Figure 5 indicates that the two-step models tended to outperform the single-step models for all the test set." ></td>
	<td class="line x" title="168:185	A paired t-test for TestSet2, however, did not reach significance 2 . So we next examined this issue in further detail." ></td>
	<td class="line x" title="169:185	As argued in Section 3.3, in the context of human-computer dialog, a misunderstanding of the users emotion at the level of sentiment polarity would lead to a serious problem, which we call a fatal error." ></td>
	<td class="line x" title="170:185	On the other hand, misclassifying a case ofhappiness as, for example,pleasantness may well be tolerable." ></td>
	<td class="line x" title="171:185	Table 11 shows the ratio of fatal errors for each model." ></td>
	<td class="line x" title="172:185	For TestSet2, the single-step 10-NN model made fatal errors in 30% of cases, while the two-step 10-NN model in only 17%." ></td>
	<td class="line x" title="173:185	This improvement is statistically significant (p<0.01)." ></td>
	<td class="line x" title="174:185	5 Conclusion In this paper, we have addressed the issue of emotion classification assuming its potential applications to be human-computer dialog system including active-listening dialog." ></td>
	<td class="line x" title="175:185	We first automatically collected a huge collection, as many as 1.3M, of emotion-provoking event instances from the Web." ></td>
	<td class="line x" title="176:185	We then decomposed the emotion classification task into two sub-steps: sentiment polarity classification and emotion classification." ></td>
	<td class="line x" title="177:185	In sentiment polarity classification, we used the EP-corpus as training data." ></td>
	<td class="line x" title="178:185	The results of the polarity classification experiment showed that word n-gram features alone are more or less sufficient to classify positive and negative sentences when a very large amount of training data is available." ></td>
	<td class="line x" title="179:185	In the emotion classification experiments, on the other hand, 2 The data size of TestSet1 was not sufficient for statistical significance test 887 Table 11: Fatal error rate in emotion classification experiments Baseline Emotion Classification Sentiment Polarity 1-NN 3-NN 10-NN + Emotion Classification TestSet1 49.2% 29.2% 26.2% 24.6% 15.4% TestSet2 41.5% 37.6% 32.8% 30.0% 17.0% we adopted the k-nearest-neighbor (kNN) method." ></td>
	<td class="line x" title="180:185	The results of the experiments showed that our method significantly outperformed the baseline method." ></td>
	<td class="line x" title="181:185	The results also showed that our twostep emotion classification was effective for finegrained emotion classification." ></td>
	<td class="line x" title="182:185	Specifically, fatal errors were significantly reduced with sentiment polarity classification before fine-grained emotion classification." ></td>
	<td class="line x" title="183:185	For future work, we first need to examine other machine learning methods to see their advantages and disadvantages in our task." ></td>
	<td class="line x" title="184:185	We also need an extensive improvement in identifying neutral sentences." ></td>
	<td class="line x" title="185:185	Finally, we are planning to apply our model to the active-listening dialog system that our group has been developing and investigate its effects on the users behavior." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D08-1004
Modeling Annotators: A Generative Approach to Learning from Annotator Rationales
Zaidan, Omar F.;Eisner, Jason M.;"></td>
	<td class="line x" title="1:277	Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 3140, Honolulu, October 2008." ></td>
	<td class="line x" title="2:277	c2008 Association for Computational Linguistics Modeling Annotators: A Generative Approach to Learning from Annotator Rationales Omar F. Zaidan and Jason Eisner Dept. of Computer Science, Johns Hopkins University Baltimore, MD 21218, USA {ozaidan,jason}@cs.jhu.edu Abstract A human annotator can provide hints to a machine learner by highlighting contextual rationales for each of his or her annotations (Zaidan et al., 2007)." ></td>
	<td class="line x" title="3:277	How can one exploit this side information to better learn the desired parameters ?" ></td>
	<td class="line x" title="4:277	We present a generative model of how a given annotator, knowing the true , stochastically chooses rationales." ></td>
	<td class="line x" title="5:277	Thus, observing the rationales helps us infer the true ." ></td>
	<td class="line x" title="6:277	We collect substring rationales for a sentiment classification task (Pang and Lee, 2004) and use them to obtain significant accuracy improvements for each annotator." ></td>
	<td class="line x" title="7:277	Our new generative approach exploits the rationales more effectively than our previous masking SVM approach." ></td>
	<td class="line x" title="8:277	It is also more principled, and could be adapted to help learn other kinds of probabilistic classifiers for quite different tasks." ></td>
	<td class="line x" title="9:277	1 Background Many recent papers aim to reduce the amount of annotated data needed to train the parameters of a statistical model." ></td>
	<td class="line x" title="10:277	Well-known paradigms include active learning, semi-supervised learning, and either domain adaptation or cross-lingual transfer from existing annotated data." ></td>
	<td class="line x" title="11:277	A rather different paradigm is to change the actual task that is given to annotators, giving them a greater hand in shaping the learned classifier." ></td>
	<td class="line x" title="12:277	After all, human annotators themselves are more than just black-box classifiers to be run on training data." ></td>
	<td class="line x" title="13:277	They possess some introspective knowledge about their own classification procedure." ></td>
	<td class="line x" title="14:277	The hope is to mine this knowledge rapidly via appropriate questions and use it to help train a machine classifier." ></td>
	<td class="line x" title="15:277	How to do this, however, is still being explored." ></td>
	<td class="line x" title="16:277	1.1 Hand-crafted rules An obvious option is to have the annotators directly express their knowledge by hand-crafting rules." ></td>
	<td class="line x" title="17:277	This This work was supported by National Science Foundation grant No. 0347822 and the JHU WSE/APL Partnership Fund." ></td>
	<td class="line x" title="18:277	Special thanks to Christine Piatko for many useful discussions." ></td>
	<td class="line x" title="19:277	approach remains data-driven if the annotators repeatedly refine their system against a corpus of labeled or unlabeled examples." ></td>
	<td class="line x" title="20:277	This achieves high performance in some domains, such as NP chunking (Brill and Ngai, 1999), but requires more analytical skill from the annotators." ></td>
	<td class="line x" title="21:277	One empirical study (Ngai and Yarowsky, 2000) found that it also required more annotation time than active learning." ></td>
	<td class="line x" title="22:277	1.2 Feature selection by humans More recent work has focused on statistical classifiers." ></td>
	<td class="line x" title="23:277	Training such classifiers faces the credit assignment problem. Given a training examplexwith many features, which features are responsible for its annotated class y?" ></td>
	<td class="line x" title="24:277	It may take many training examples to distinguish useful vs. irrelevant features.1 To reduce the number of training examples needed, one can ask annotators to examine or propose some candidate features." ></td>
	<td class="line x" title="25:277	This is possible even for the very large feature sets that are typically used in NLP." ></td>
	<td class="line x" title="26:277	In document classification, Raghavan et al.(2006) show that feature selection by an oracle could be helpful, and that humans are both rapid and reasonably good at distinguishing highly usefuln-gram features from randomly chosen ones, even when viewing these n-grams out of context." ></td>
	<td class="line x" title="28:277	Druck et al.(2008) show annotators some features f from a fixed feature set, and ask them to choose a class labely such thatp(y|f) is as high as possible." ></td>
	<td class="line x" title="30:277	Haghighi and Klein (2006) do the reverse: for each class label y, they ask the annotators to propose a few prototypical featuresf such thatp(y|f) is as high as possible." ></td>
	<td class="line x" title="31:277	1.3 Feature selection in context The above methods consider features out of context." ></td>
	<td class="line x" title="32:277	An annotator might have an easier time examining 1Most NLP systems use thousands or millions of features, because it is helpful to include lexical features over a large vocabulary, often conjoined with lexical or non-lexical context." ></td>
	<td class="line x" title="33:277	31 features in context to recognize whether they appear relevant." ></td>
	<td class="line x" title="34:277	This is particularly true for features that are only modestly or only sometimes helpful, which may be abundant in NLP tasks." ></td>
	<td class="line x" title="35:277	Thus, Raghavan et al.(2006) propose an active learning method in which, while classifying a training document, the annotator also identifies some features of that document as particularly relevant." ></td>
	<td class="line x" title="37:277	E.g., the annotator might highlight particular unigrams as he or she reads the document." ></td>
	<td class="line x" title="38:277	In their proposal, a feature that is highlighted in any document is assumed to be globally more relevant." ></td>
	<td class="line x" title="39:277	Its dimension in feature space is scaled by a factor of 10 so that this feature has more influence on distances or inner products, and hence on the learned classifier." ></td>
	<td class="line x" title="40:277	1.4 Concerns about marking features Despite the success of the above work, we have several concerns about asking annotators to identify globally relevant features." ></td>
	<td class="line x" title="41:277	First, a feature in isolation really does not have a well-defined worth." ></td>
	<td class="line x" title="42:277	A feature may be useful only in conjunction with other features,2 or be useful only to the extent that other correlated features are not selected to do the same work." ></td>
	<td class="line x" title="43:277	Second, it is not clear how an annotator would easily view and highlight features in context, except for the simplest feature sets." ></td>
	<td class="line x" title="44:277	In the phrase Apple shares up 3%, there may be several features that fire on the substring Appleresponding to the string Apple, its case-invariant form apple, its lemma apple(which would also respond to apples), its context-dependent sense Apple2, its part of speech noun, etc. How does the annotator indicate which of these features are relevant?" ></td>
	<td class="line x" title="45:277	Third, annotating features is only appropriate when the feature set can be easily understood by a human." ></td>
	<td class="line x" title="46:277	This is not always the case." ></td>
	<td class="line x" title="47:277	It would be hard for annotators to read, write, or evaluate a description of a complex syntactic configuration in NLP or a convolution filter in machine vision." ></td>
	<td class="line x" title="48:277	Fourth, traditional annotation efforts usually try to remain agnostic about the machine learning methods 2For example, a linear classifier can learn that most training examples satisfyAB by settingA =5 andAB = +5, but this solution requires selecting bothAandAB as features." ></td>
	<td class="line x" title="49:277	More simply, a polynomial kernel can consider the conjunction AB only if both A and B are selected as features." ></td>
	<td class="line x" title="50:277	and features to be used." ></td>
	<td class="line x" title="51:277	The projects cost is justified by saying that the annotations will be reused by many researchers (perhaps in a shared task), who are free to compete on how they tackle the learning problem." ></td>
	<td class="line x" title="52:277	Unfortunately, feature annotation commits to a particular feature set at annotation time." ></td>
	<td class="line x" title="53:277	Subsequent research cannot easily adjust the definition of the features, or obtain annotation of new features." ></td>
	<td class="line x" title="54:277	2 Annotating Rationales To solve these problems, we propose that annotators should not select features but rather mark relevant portions of the example." ></td>
	<td class="line x" title="55:277	In earlier work (Zaidan et al., 2007), we called these markings rationales. For example, when classifying a movie review as positive or negative, the annotator would also highlight phrases that supported that judgment." ></td>
	<td class="line x" title="56:277	Figure 1 shows two such rationales." ></td>
	<td class="line x" title="57:277	A multi-annotator timing study (Zaidan et al., 2007) found that highlighting rationale phrases while reading movie reviews only doubled annotation time, although annotators marked 511 rationale substrings in addition to the simple binary class." ></td>
	<td class="line x" title="58:277	The benefit justified the extra time." ></td>
	<td class="line x" title="59:277	Furthermore, much of the benefit could have been obtained by giving rationales for only a fraction of the reviews." ></td>
	<td class="line x" title="60:277	In the visual domain, when classifying an image as containing a zoo, the annotator might circle some animals or cages and the sign reading Zoo. The Peekaboom game (von Ahn et al., 2006) was in fact built to elicit such approximate yet relevant regions of images." ></td>
	<td class="line x" title="61:277	Further scenarios were discussed in (Zaidan et al., 2007): rationale annotation for named entities, linguistic relations, or handwritten digits." ></td>
	<td class="line x" title="62:277	Annotating rationales does not require the annotator to think about the feature space, nor even to know anything about it." ></td>
	<td class="line x" title="63:277	Arguably this makes annotation easier and more flexible." ></td>
	<td class="line x" title="64:277	It also preserves the reusability of the annotated data." ></td>
	<td class="line x" title="65:277	Anyone is free to reuse our collected rationales (section 4) to aid in learning a classifier with richer features, or a different kind of classifier altogether, using either our procedures or novel procedures." ></td>
	<td class="line x" title="66:277	3 Modeling Rationale Annotations As rationales are more indirect than explicit features, they present a trickier machine learning problem." ></td>
	<td class="line x" title="67:277	32 We wish to learn the parameters  of some classifier." ></td>
	<td class="line x" title="68:277	How can the annotators rationales help us to do this without many training examples?" ></td>
	<td class="line x" title="69:277	We will have to exploit a presumed relationship between the rationales and the optimal value of  (i.e., the value that we would learn on an infinite training set)." ></td>
	<td class="line x" title="70:277	This paper exploits an explicit, parametric model of that relationship." ></td>
	<td class="line x" title="71:277	The models parameters  are intended to capture what that annotator is doing when he or she marks rationales." ></td>
	<td class="line x" title="72:277	Most importantly, they capture how he or she is influenced by the true ." ></td>
	<td class="line x" title="73:277	Given this, our learning method will prefer values of  that would adequately explain the rationales (as well as the training classifications)." ></td>
	<td class="line x" title="74:277	3.1 A generative approach For concreteness, we will assume that the task is document classification." ></td>
	<td class="line x" title="75:277	Our training data consists ofntriples{(x1,y1,r1),,(xn,yn,rn)}), wherexi is a document, yi is its annotated class, and ri is its rationale markup." ></td>
	<td class="line x" title="76:277	At test time we will have to predict yn+1 from xn+1, without any rn+1." ></td>
	<td class="line x" title="77:277	We propose to jointly choose parameter vectors  and  to maximize the following regularized conditional likelihood:3 nproductdisplay i=1 p(yi,ri|xi,,)pprior(,) (1) def= nproductdisplay i=1 p(yi|xi)p(ri|xi,yi,)pprior(,) Here we are trying to model all the annotations, both yi and ri." ></td>
	<td class="line x" title="78:277	The first factor predicts yi using an ordinary probabilistic classifier p, while the novel second factor predicts ri using a model p of how annotators generate the rationale annotations." ></td>
	<td class="line x" title="79:277	The crucial point is that the second factor depends on  (since ri is supposed to reflect the relation between xi and yi that is modeled by )." ></td>
	<td class="line x" title="80:277	As a result, the learner has an incentive to modify  in a way that increases the second factor, even if this somewhat decreases the first factor on training data.4 3It would be preferable to integrate out  (and even ), but more difficult." ></td>
	<td class="line x" title="81:277	4Interestingly, even examples where the annotation yi is wrong or unhelpful can provide useful information about  via the pair (yi,ri)." ></td>
	<td class="line x" title="82:277	Two annotators marking the same movie review might disagree on whether it is overall a positive or negaAfter training, one should simply use the first factor p(y|x) to classify test documents x. The second factor is irrelevant for test documents, since they have not been annotated with rationales r. The second factor may likewise be omitted for any training documents i that have not been annotated with rationales, as there is no ri to predict in those cases." ></td>
	<td class="line x" title="83:277	In the extreme case where no documents are annotated with rationales, equation (1) reduces to the standard training procedure." ></td>
	<td class="line x" title="84:277	3.2 Noisy channel design of rationale models Like ordinary class annotations, rationale annotations present us with a credit assignment problem, albeit a smaller one that is limited to features that fire in the vicinity of the rationale r. Some of these -features were likely responsible for the classification y and hence triggered the rationale." ></td>
	<td class="line x" title="85:277	Other such -features were just innocent bystanders." ></td>
	<td class="line x" title="86:277	Thus, the interesting part of our model is p(r | x,y,), which models the rationale annotation process." ></td>
	<td class="line x" title="87:277	The rationales r reflect , but in noisy ways." ></td>
	<td class="line x" title="88:277	Taking this noisy channel idea seriously, p(r | x,y,) should consider two questions when assessing whether r is a plausible set of rationales given ." ></td>
	<td class="line x" title="89:277	First, it needs a language model of rationales: does r consist of rationales that are well-formed a priori, i.e., before  is considered?" ></td>
	<td class="line x" title="90:277	Second, it needs a channel model: does r faithfully signal the features of  that strongly support classifying x as y?" ></td>
	<td class="line x" title="91:277	If a feature contributes heavily to the classification of document x as class y, then the channel model should tell us which parts of document x tend to be highlighted as a result." ></td>
	<td class="line x" title="92:277	The channel model must know about the particular kinds of features that are extracted by f and scored by ." ></td>
	<td class="line x" title="93:277	Suppose the feature not . . ." ></td>
	<td class="line x" title="94:277	gripping,5 with weighth, is predictive of the annotated classy." ></td>
	<td class="line x" title="95:277	This raises the probabilities of the annotators highlighting each of various words, or combinations of words, in a phrase like not the most gripping banquet on film." ></td>
	<td class="line x" title="96:277	The channel model parameters in  tive reviewbut the second factor still allows learning positive features from the first annotators positive rationales, and negative features from the second annotators negative rationales." ></td>
	<td class="line x" title="97:277	5Our current experiments use only unigram features, to match past work, but we use this example to outline how our approach generalizes to complex linguistic (or visual) features." ></td>
	<td class="line x" title="98:277	33 should specify how much each of these probabilities is raised, based on the magnitude of h  R, the class y, and the fact that the feature is an instance of the template <Neg> . . .<Adjective>." ></td>
	<td class="line x" title="99:277	(Thus,  has no parameters specific to the word gripping; it is a low-dimensional vector that only describes the annotators general style in translating  into r.)" ></td>
	<td class="line x" title="100:277	The language model, however, is independent of the feature set ." ></td>
	<td class="line x" title="101:277	It models what rationales tend to look like in the input domaine.g., documents or images." ></td>
	<td class="line x" title="102:277	In the document case,  should describe: How frequent and how long are typical rationales?" ></td>
	<td class="line x" title="103:277	Do their edges tend to align with punctuation or major syntactic boundaries in x?" ></td>
	<td class="line x" title="104:277	Are they rarer in the middle of a document, or in certain documents?6 Thanks to the language model, we do not need to posit high features to explain every word in a rationale." ></td>
	<td class="line x" title="105:277	The language model can explain away some words as having been highlighted only because this annotator prefers not to end a rationale in midphrase, or prefers to sweep up close-together features with a single long rationale rather than many short ones." ></td>
	<td class="line x" title="106:277	Similarly, the language model can help explain why some words, though important, might not have been included in any rationale of r. If there are multiple annotators, one can learn different  parameters for each annotator, reflecting their different annotation styles.7 We found this to be useful (section 8.2)." ></td>
	<td class="line x" title="107:277	We remark that our generative modeling approach (equation (1)) would also apply if r were not rationale markup, but some other kind of so-called side information, such as the feature annotations discussed in section 1." ></td>
	<td class="line x" title="108:277	For example, Raghavan et al.(2006) assume that if feature h is relevanta bi6Our current experiments do not model this last point." ></td>
	<td class="line x" title="110:277	However, we imagine that if the document only has a few -features that support the classification, the annotator will probably mark most of them, whereas if such features are abundant, the annotator may lazily mark only a few of the strongest ones." ></td>
	<td class="line x" title="111:277	A simple approach would equip  with a different bias or threshold parameter x for each rationale training document x, to modulate the a priori probability of marking a rationale in x. By fitting this bias parameter, we deduce how lazy the annotator was (for whatever reason) on document x. If desired, a prior on x could consider whether x has many strong -features, whether the annotator has recently had a coffee break, etc. 7Given insufficient rationale data to recover some annotatorswell, one could smooth using data from other annotators." ></td>
	<td class="line x" title="112:277	But in our situation,  had relatively few parameters to learn." ></td>
	<td class="line x" title="113:277	nary distinctioniff it was selected in at least one document." ></td>
	<td class="line x" title="114:277	But it might be more informative to observe that h was selected in 3 of the 10 documents where it appeared, and to predict this via a model p(3 of 10|h), wheredescribes (e.g.) how to derive a binomial parameter nonlinearly from h. This approach would not how oftenhwas marked and infer how relevant is feature h (i.e., infer h)." ></td>
	<td class="line x" title="115:277	In this case, p is a simple channel that transforms relevant features into direct indicators of the feature." ></td>
	<td class="line x" title="116:277	Our side information merely requires a more complex transformationfrom relevant features into wellformed rationales, modulated by documents." ></td>
	<td class="line x" title="117:277	4 Experimental Data: Movie Reviews In Zaidan et al.(2007), we introduced the Movie Review Polarity Dataset Enriched with Annotator Rationales.8 It is based on the dataset of Pang and Lee (2004),9 which consists of 1000 positive and 1000 negative movie reviews, tokenized and divided into 10 folds (F0F9)." ></td>
	<td class="line x" title="119:277	All our experiments use F9 as their final blind test set." ></td>
	<td class="line x" title="120:277	The enriched dataset adds rationale annotations produced by an annotator A0, who annotated folds F0F8 of the movie review set with rationales (in the form of textual substrings) that supported the goldstandard classifications." ></td>
	<td class="line x" title="121:277	We will use A0s data to determine the improvement of our method over a (log-linear) baseline model without rationales." ></td>
	<td class="line x" title="122:277	We also use A0 to compare against the masking SVM method and SVM baseline of Zaidan et al.(2007)." ></td>
	<td class="line x" title="124:277	Since  can be tuned to a particular annotator, we would also like to know how well this works with data from annotators other than A0." ></td>
	<td class="line x" title="125:277	We randomly selected 100 reviews (50 positive and 50 negative) and collected both class and rationale annotation data from each of six new annotators A3A8,10 following the same procedures as (Zaidan et al., 2007)." ></td>
	<td class="line x" title="126:277	We report results using only data from A3A5, since we used the data from A6A8 as development data in the early stages of our work." ></td>
	<td class="line x" title="127:277	We use this new rationale-enriched dataset8 to determine if our method works well across annotators." ></td>
	<td class="line x" title="128:277	We will only be able to carry out that comparison 8Available at http://cs.jhu.edu/ozaidan/rationales." ></td>
	<td class="line x" title="129:277	9Polarity dataset version 2.0." ></td>
	<td class="line x" title="130:277	10We avoid annotator names A1A2, which were already used in (Zaidan et al., 2007)." ></td>
	<td class="line x" title="131:277	34 Figure 1: Rationales as sequence annotation: the annotator highlighted two textual segments as rationales for a positive class." ></td>
	<td class="line x" title="132:277	Highlighted words in vectorx are tagged I in vectorr, and other words are tagged O. The figure also shows some -features." ></td>
	<td class="line x" title="133:277	For instance, gO(,)-I is a count of O-I transitions that occur with a comma as the left word." ></td>
	<td class="line x" title="134:277	Notice also that grel is the sum of the underlined values." ></td>
	<td class="line x" title="135:277	at small training set sizes, due to limited data from A3A8." ></td>
	<td class="line x" title="136:277	The larger A0 dataset will still allow us to evaluate our method on a range of training set sizes." ></td>
	<td class="line x" title="137:277	5 Detailed Models 5.1 Modeling class annotations with p We define the basic classifierp in equation (1) to be a standard conditional log-linear model: p(y|x) def= exp( vectorvectorf(x,y)) Z(x) def= u(x,y) Z(x) (2) where vectorf() extracts a feature vector from a classified document, vector are the corresponding weights of those features, and Z(x) def=summationtextyu(x,y) is a normalizer." ></td>
	<td class="line oc" title="138:277	We use the same set of binary features as in previous work on this dataset (Pang et al., 2002; Pang and Lee, 2004; Zaidan et al., 2007)." ></td>
	<td class="line x" title="139:277	Specifically, let V = {v1,,v17744} be the set of word types with count4 in the full 2000-document corpus." ></td>
	<td class="line x" title="140:277	Define fh(x,y) to be y if vh appears at least once in x, and 0 otherwise." ></td>
	<td class="line x" title="141:277	Thus R17744, and positive weights in favor class labely = +1 and equally discourage y =1, while negative weights do the opposite." ></td>
	<td class="line x" title="142:277	This standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales." ></td>
	<td class="line x" title="143:277	Future work should consider more complex features and how they are signaled by rationales, as discussed in section 3.2." ></td>
	<td class="line x" title="144:277	5.2 Modeling rationale annotations with p The rationales collected in this task are textual segments of a document to be classified." ></td>
	<td class="line x" title="145:277	The document itself is a word token sequencevectorx = x1,,xM." ></td>
	<td class="line x" title="146:277	We encode its rationales as a corresponding tag sequence vectorr = r1,,rM, as illustrated in Figure 1." ></td>
	<td class="line x" title="147:277	Here rm {I,O} according to whether the token xm is in a rationale (i.e.,xm was at least partly highlighted) or outside all rationales." ></td>
	<td class="line x" title="148:277	x1 and xM are special boundary symbols, tagged with O. We predict the full tag sequence vectorr at once using a conditional random field (Lafferty et al., 2001)." ></td>
	<td class="line x" title="149:277	A CRF is just another conditional log-linear model: p(r|x,y,vector) def= exp( vectorvectorg(r,x,y,vector)) Z(x,y,vector) def=u(r,x,y,vector) Z(x,y,vector) where vectorg() extracts a feature vector, vector are the corresponding weights of those features, and Z(x,y,vector) def=summationtextru(r,x,y,vector) is a normalizer." ></td>
	<td class="line x" title="150:277	As usual for linear-chain CRFs, vectorg() extracts two kinds of features: first-order emission features that relate rm to (xm,y,), and second-order transition features that relate rm to rm1 (although some of these also look at x)." ></td>
	<td class="line x" title="151:277	These two kinds of features respectively capture the channel model and language model of section 3.2." ></td>
	<td class="line x" title="152:277	The former says rm is I because xm is associated with a relevant -feature." ></td>
	<td class="line x" title="153:277	The latter says rm is I simply because it is next to another I. 5.3 Emission -features (channel model) Recall that our -features (at present) correspond to unigrams." ></td>
	<td class="line x" title="154:277	Given (vectorx,y,vector), let us say that a unigram w  vectorx is relevant, irrelevant, or anti-relevant if yw is respectivelygreatermuch0,0, orlessmuch0." ></td>
	<td class="line x" title="155:277	That is, w is relevant if its presence in x strongly supports the annotated class y, and anti-relevant if its presence strongly supports the opposite classy. 35 Figure 2: The function family Bs in equation (3), shown for s  {10,2,2,10}." ></td>
	<td class="line x" title="156:277	We would like to learn the extent rel to which annotators try to include relevant unigrams in their rationales, and the (usually lesser) extent antirel to which they try to exclude anti-relevant unigrams." ></td>
	<td class="line x" title="157:277	This will help us infer vector from the rationales." ></td>
	<td class="line x" title="158:277	The details are as follows." ></td>
	<td class="line x" title="159:277	rel and antirel are the weights of two emission features extracted byvectorg: grel(vectorx,y,vectorr,vector) def= Msummationdisplay m=1 I(rm = I)B10(yxm) gantirel(vectorx,y,vectorr,vector) def= Msummationdisplay m=1 I(rm = I)B10(yxm) Here I() denotes the indicator function, returning 1 or 0 according to whether its argument is true or false." ></td>
	<td class="line x" title="160:277	Relevance and negated anti-relevance are respectively measured by the differentiable nonlinear functions B10 and B10, which are defined by Bs(a) = (log(1 + exp(as))log(2))/s (3) and graphed in Figure 2." ></td>
	<td class="line x" title="161:277	Sample values of B10 and grel are shown in Figure 1." ></td>
	<td class="line x" title="162:277	How does this work?" ></td>
	<td class="line x" title="163:277	The grel feature is a sum over all unigrams in the document vectorx." ></td>
	<td class="line x" title="164:277	It does not fire strongly on the irrelevant or anti-relevant unigrams, since B10 is close to zero there.11 But it fires positively on relevant unigramsw if they are tagged with I, and the strength of such firing increases approximately linearly withw. Since the weightrel > 0 in practice, this means that raising a relevant unigrams w (if y = +1) will proportionately raise its logodds of being tagged with I. Symmetrically, since antirel > 0 in practice, lowering an anti-relevant unigrams w (if y = +1) will proportionately lower 11B10 sets the threshold for relevance to be about 0." ></td>
	<td class="line x" title="165:277	One could also include versions of the grel feature that set a higher threshold, using B10(yxmthreshold)." ></td>
	<td class="line x" title="166:277	its log-odds of being tagged with I, though not necessarily at the same rate as for relevant unigrams.12 Should  also include traditional CRF emission features, which would recognize that particular words like great tend to be tagged as I?" ></td>
	<td class="line x" title="167:277	No! Such features would undoubtedly do a better job predicting the rationales and hence increasing equation (1)." ></td>
	<td class="line x" title="168:277	However, crucially, our true goal is not to predict the rationales but to recover the classifier parameters ." ></td>
	<td class="line x" title="169:277	Thus, if great tends to be highlighted, then the model should not be permitted to explain this directly by increasing some feature great, but only indirectly by increasing great." ></td>
	<td class="line x" title="170:277	We therefore permit our rationale prediction model to consider only the two emission features grel and gantirel, which see the words in vectorx only through their -values." ></td>
	<td class="line x" title="171:277	5.4 Transition -features (language model) Annotators highlight more than just the relevant unigrams." ></td>
	<td class="line x" title="172:277	(After all, they arent told that our current -features are unigrams.)" ></td>
	<td class="line x" title="173:277	They tend to mark full phrases, though perhaps taking care to exclude antirelevant portions." ></td>
	<td class="line x" title="174:277	models these phrases shape, via weights for several language model features." ></td>
	<td class="line x" title="175:277	Most important are the 4 traditional CRF tag transition features gO-O,gO-I,gI-I,gI-O." ></td>
	<td class="line x" title="176:277	For example, gO-I counts the number of O-to-I transitions in vectorr (see Figure 1)." ></td>
	<td class="line x" title="177:277	Other things equal, an annotator with high O-I is predicted to have many rationales per 1000 words." ></td>
	<td class="line x" title="178:277	And if I-I is high, rationales are predicted to be long phrases (including more irrelevant unigrams around or between the relevant ones)." ></td>
	<td class="line x" title="179:277	We also learn more refined versions of these features, which consider how the transition probabilities are influenced by the punctuation and syntax of the document vectorx (independent of vector)." ></td>
	<td class="line x" title="180:277	These refined features are more specific and hence more sparsely trained." ></td>
	<td class="line x" title="181:277	Their weights reflect deviations from the simpler, backed-off transition features such as gO-I." ></td>
	<td class="line x" title="182:277	(Again, see Figure 1 for examples.)" ></td>
	<td class="line x" title="183:277	Conditioning on left word." ></td>
	<td class="line x" title="184:277	A feature of the form gt1(v)-t2 is specified by a pair of tag types t1,t2  {I,O}and a vocabulary word type v. It counts the 12If the two rates are equal (rel = antirel), we get a simpler model in which the log-odds change exactly linearly withw for each w, regardless of ws relevance/irrelevance/anti-relevance." ></td>
	<td class="line x" title="185:277	This follows from the fact thatBs(a)+Bs(a) simplifies toa." ></td>
	<td class="line x" title="186:277	36 number of times an t1t2 transition occurs invectorr conditioned on v appearing as the first of the two word tokens where the transition occurs." ></td>
	<td class="line x" title="187:277	Our experiments include gt1(v)-t2 features that tie I-O and O-I transitions to the 4 most frequent punctuation marks v (comma, period, ?, !)." ></td>
	<td class="line x" title="188:277	Conditioning on right word." ></td>
	<td class="line x" title="189:277	A feature gt1-t2(v) is similar, but v must appear as the second of the two word tokens where the transition occurs." ></td>
	<td class="line x" title="190:277	Again here, we use gt1-t2(v) features that tie I-O and O-I transitions to the four punctuation marks mentioned above." ></td>
	<td class="line x" title="191:277	We also include five features that tie O-I transitions to the words no, not, so, very, and quite, since in our development data, those words were more likely than others to start rationales.13 Conditioning on syntactic boundary." ></td>
	<td class="line x" title="192:277	We parsed each rationale-annotated training document (no parsing is needed at test time).14 We then marked each word bigram x1-x2 with three nonterminals: NEnd is the nonterminal of the largest constituent that contains x1 and not x2, NStart is the nonterminal of the largest constituent that contains x2 and not x1, and NCross is the nonterminal of the smallest constituent that contains both x1 and x2." ></td>
	<td class="line x" title="193:277	For a nonterminalN and pair of tag types (t1,t2), we define three features, gt1-t2/E=N, gt1-t2/S=N, and gt1-t2/C=N, which count the number of times a t1-t2 transition occurs in vectorr with N matching the NEnd, NStart, or NCross nonterminal, respectively." ></td>
	<td class="line x" title="194:277	Our experiments include these features for 11 common nonterminal types N (DOC, TOP, S, SBAR, FRAG, PRN, NP, VP, PP, ADJP, QP)." ></td>
	<td class="line x" title="195:277	6 Training: Joint Optimization of  and  To train our model, we use L-BFGS to locally maximize the log of the objective function (1):15 13These are the function words with count40 in a random sample of 100 documents, and which were associated with the O-I tag transition at more than twice the average rate." ></td>
	<td class="line x" title="196:277	We do not use any other lexical-features that referencevectorx, for fear that they would enable the learner to explain the rationales without changing  as desired (see the end of section 5.3)." ></td>
	<td class="line x" title="197:277	14We parse each sentence with the Collins parser (Collins, 1999)." ></td>
	<td class="line x" title="198:277	Then the document has one big parse tree, whose root is DOC, with each sentence being a child of DOC." ></td>
	<td class="line x" title="199:277	15One might expect this function to be convex becausep and p are both log-linear models with no hidden variables." ></td>
	<td class="line x" title="200:277	However, logp(ri|xi,yi,) is not necessarily convex in ." ></td>
	<td class="line x" title="201:277	nsummationdisplay i=1 logp(yi|xi) 122  bardblbardbl2 +C( nsummationdisplay i=1 logp(ri|xi,yi,)) 122  bardblbardbl2 (4) This definespprior from (1) to be a standard diagonal Gaussian prior, with variances 2 and 2 for the two sets of parameters." ></td>
	<td class="line x" title="202:277	We optimize 2 in our experiments." ></td>
	<td class="line x" title="203:277	As for 2, different values did not affect the results, since we have a large number of {I,O} rationale tags to train relatively few  weights; so we simply use 2 = 1 in all of our experiments." ></td>
	<td class="line x" title="204:277	Note the new C factor in equation (4)." ></td>
	<td class="line x" title="205:277	Our initial experiments showed that optimizing equation (4) without C led to an increase in the likelihood of the rationale data at the expense of classification accuracy, which degraded noticeably." ></td>
	<td class="line x" title="206:277	This is because the second sum in (4) has a much larger magnitude than the first: in a set of 100 documents, it predicts around 74,000 binary {I,O} tags, versus the one hundred binary class labels." ></td>
	<td class="line x" title="207:277	While we are willing to reduce the log-likelihood of the training classifications (the first sum) to a certain extent, focusing too much on modeling rationales (the second sum) is clearly not our ultimate goal, and so we optimize C on development data to achieve some balance between the two terms of equation (4)." ></td>
	<td class="line x" title="208:277	Typical values of C range from 1300 to 150.16 We perform alternating optimization on  and : 1." ></td>
	<td class="line x" title="209:277	Initialize  to maximize equation (4) but with C = 0 (i.e. based only on class data)." ></td>
	<td class="line x" title="210:277	2." ></td>
	<td class="line x" title="211:277	Fix , and find  that maximizes equation (4)." ></td>
	<td class="line x" title="212:277	3." ></td>
	<td class="line x" title="213:277	Fix , and find  that maximizes equation (4)." ></td>
	<td class="line x" title="214:277	4." ></td>
	<td class="line x" title="215:277	Repeat 2 and 3 until convergence." ></td>
	<td class="line x" title="216:277	The L-BFGS method requires calculating the gradient of the objective function (4)." ></td>
	<td class="line x" title="217:277	The partial derivatives with respect to components of  and  involve calculating expectations of the feature functions, which can be computed in linear time (with respect to the size of the training set) using the forward-backward algorithm for CRFs." ></td>
	<td class="line x" title="218:277	The partial derivatives also involve the derivative of (3), to determine how changing  will affect the firing strength of the emission features grel and gantirel." ></td>
	<td class="line x" title="219:277	16C also balances our confidence in the classifications y against our confidence in the rationales r; either may be noisy." ></td>
	<td class="line x" title="220:277	37 7 Experimental Procedures We report on two sets of experiments." ></td>
	<td class="line x" title="221:277	In the first set, we use the annotation data that A3A5 provided for the small set of 100 documents (as well as the data from A0 on those same 100 documents)." ></td>
	<td class="line x" title="222:277	In the second set, we used A0s abundant annotation data to evaluate our method with training set sizes up to 1600 documents, and compare it with three other methods: log-linear baseline, SVM baseline, and the SVM masking method of (Zaidan et al., 2007)." ></td>
	<td class="line x" title="223:277	7.1 Learning curves The learning curves reported in section 8.1 are generated exactly as in (Zaidan et al., 2007)." ></td>
	<td class="line x" title="224:277	Each curve shows classification accuracy at training set sizes T = 1,2,,9 folds (i.e. 200,400,,1600 training documents)." ></td>
	<td class="line x" title="225:277	For a given size T, the reported accuracy is an average of 9 experiments with different subsets of the entire training set, each of size T: 1 9 8summationdisplay i=0 acc(F9|Fi+1Fi+T) (5) where Fj denotes the fold numbered j mod 9, and acc(F9 | Y) means classification accuracy on the held-out test set F9 after training on set Y. We use an appropriate paired permutation test, detailed in (Zaidan et al., 2007), to test differences in (5)." ></td>
	<td class="line x" title="226:277	We call a difference significant at p< 0.05." ></td>
	<td class="line x" title="227:277	7.2 Comparison to masking SVM method We compare our method to the masking SVM method of (Zaidan et al., 2007)." ></td>
	<td class="line x" title="228:277	Briefly, that method used rationales to construct several so-called contrast examples from every training example." ></td>
	<td class="line x" title="229:277	A contrast example is obtained by masking out one of the rationales highlighted to support the training examples class." ></td>
	<td class="line x" title="230:277	A good classifier should have more trouble on this modified example." ></td>
	<td class="line x" title="231:277	Hence, Zaidan et al.(2007) required the learned SVM to classify each contrast example with a smaller margin than the corresponding original example (and did not require it to be classified correctly)." ></td>
	<td class="line x" title="233:277	The masking SVM learner relies on a simple geometric principle; is trivial to implement on top of an existing SVM learner; and works well." ></td>
	<td class="line x" title="234:277	However, we believe that the generative method we present here is more interesting and should apply more broadly." ></td>
	<td class="line x" title="235:277	Figure 3: Classification accuracy curves for the 4 methods: the two baseline learners that only utilize class data, and the two learners that also utilize rationale annotations." ></td>
	<td class="line x" title="236:277	The SVM curves are from (Zaidan et al., 2007)." ></td>
	<td class="line x" title="237:277	First, the masking method is specific to improving an SVM learner, whereas our method can be used to improve any classifier by adding a rationale-based regularizer (the second half of equation (4)) to its objective function during training." ></td>
	<td class="line x" title="238:277	More important, there are tasks where it is unclear how to generate contrast examples." ></td>
	<td class="line x" title="239:277	For the movie review task, it was natural to mask out a rationale by pretending its words never occurred in the document." ></td>
	<td class="line x" title="240:277	After all, most word types do not appear in most documents, so it is natural to consider the nonpresence of a word as a default state to which we can revert." ></td>
	<td class="line x" title="241:277	But in an image classification task, how should one modify the images features to ignore some spatial region marked as a rationale?" ></td>
	<td class="line x" title="242:277	There is usually no natural default value to which we could set the pixels." ></td>
	<td class="line x" title="243:277	Our method, on the other hand, eliminates contrast examples altogether." ></td>
	<td class="line x" title="244:277	8 Experimental Results and Analysis 8.1 The added benefit of rationales Fig." ></td>
	<td class="line x" title="245:277	3 shows learning curves for four methods." ></td>
	<td class="line x" title="246:277	A log-linear model shows large and significant improvements, at all training sizes, when we incorporate rationales into its training via equation (4)." ></td>
	<td class="line x" title="247:277	Moreover, the resulting classifier consistently outperforms17 prior work, the masking SVM, which starts with a slightly better baseline classifier (an SVM) but incorporates the rationales more crudely." ></td>
	<td class="line x" title="248:277	17Differences are not significant at sizes 200, 1000, and 1600." ></td>
	<td class="line x" title="249:277	38 size A0 A3 A4 A5 SVM baseline 100 72.0 72.0 72.0 70.0 SVM+contrasts 100 75.0 73.0 74.0 72.0 Log-linear baseline 100 71.0 73.0 71.0 70.0 Log-linear+rats 100 76.0 76.0 77.0 74.0 SVM baseline 20 63.4 62.2 60.4 62.6 SVM+contrasts 20 65.4 63.4 62.4 64.8 Log-linear baseline 20 63.0 62.2 60.2 62.4 Log-linear+rats 20 65.8 63.6 63.4 64.8 Table 1: Accuracy rates using each annotators data." ></td>
	<td class="line x" title="250:277	In a given column, a value in italics is not significantly different from the highest value in that column, which is boldfaced." ></td>
	<td class="line x" title="251:277	The size=20 results average over 5 experiments." ></td>
	<td class="line x" title="252:277	To confirm that we could successfully model annotators other than A0, we performed the same comparison for annotators A3A5; each had provided class and rationale annotations on a small 100document training set." ></td>
	<td class="line x" title="253:277	We trained a separate  for each annotator." ></td>
	<td class="line x" title="254:277	Table 1 shows improvements over baseline, usually significant, at 2 training set sizes." ></td>
	<td class="line x" title="255:277	8.2 Analysis Examining the learned weights vector gives insight into annotator behavior." ></td>
	<td class="line x" title="256:277	High weights include I-O and O-I transitions conditioned on punctuation, e.g., I(.)-O = 3.55,18 as well as rationales ending at the end of a major phrase, e.g., I-O/E=VP = 1.88." ></td>
	<td class="line x" title="257:277	The large emission feature weights, e.g., rel = 14.68 and antirel = 15.30, tie rationales closely to  values, as hoped." ></td>
	<td class="line x" title="258:277	For example, in Figure 1, the word w = succeeds, with w = 0.13, drives up p(I)/p(O) by a factor of 7 (in a positive document) relative to a word with w = 0." ></td>
	<td class="line x" title="259:277	In fact, feature ablation experiments showed that almost all the classification benefit from rationales can be obtained by using only these 2 emission -features and the 4 unconditioned transition features." ></td>
	<td class="line x" title="260:277	Our full  (115 features) merely improves our ability to predict the rationales (whose likelihood does increase significantly with more features)." ></td>
	<td class="line x" title="261:277	We also checked that annotators styles differ enough that it helps to tune  to the target annotatorAwho gave the rationales." ></td>
	<td class="line x" title="262:277	Table 3 shows that a model trained onAs own rationales does best at predicting new rationales fromA." ></td>
	<td class="line x" title="263:277	Table 2 shows that as 18When trained on folds F4F8 with A0s rationales." ></td>
	<td class="line x" title="264:277	A0 A3 A4 A5 Baseline A0 76.0 73.0 74.0 73.0 71.0 A3 73.0 76.0 74.0 73.0 73.0 A4 75.0 73.0 77.0 74.0 71.0 A5 74.0 71.0 72.0 74.0 70.0 Table 2: Accuracy rate for an annotators  (rows) obtained when using some other annotators  (columns)." ></td>
	<td class="line x" title="265:277	Notice that the diagonal entries and the baseline column are taken from rows of Table 1 (size=100)." ></td>
	<td class="line x" title="266:277	Trivial A0 A3 A4 A5 model L(rA0) 0.073 0.086 0.077 0.088 0.135 L(rA3) 0.084 0.068 0.071 0.068 0.130 L(rA4) 0.088 0.084 0.075 0.085 0.153 L(rA5) 0.058 0.044 0.047 0.044 0.111 Table 3: Cross-entropy per tag of rationale annotations vectorr for each annotator (rows), when predicted from that annotators vectorx and vector via a possibly different annotators  (columns)." ></td>
	<td class="line x" title="267:277	For comparison, the trivial model is a bigram model ofvectorr, which is trained on the target annotator but ignores vectorx and vector." ></td>
	<td class="line x" title="268:277	5-fold cross-validation on the 100document set was used to prevent testing on training data." ></td>
	<td class="line x" title="269:277	a result, classification performance on the test set is usually best if it wasAs ownthat was used to help learn  from As rationales." ></td>
	<td class="line x" title="270:277	In both cases, however, a different annotators  is better than nothing." ></td>
	<td class="line x" title="271:277	9 Conclusions We have demonstrated a effective method for eliciting extra knowledge from naive annotators, in the form of lightweight rationales for their annotations." ></td>
	<td class="line x" title="272:277	By explicitly modeling the annotators rationale-marking process, we are able to infer a better model of the original annotations." ></td>
	<td class="line x" title="273:277	We showed that our method performs significantly better than two strong baseline classifiers, and also outperforms our previous discriminative method for exploiting rationales (Zaidan et al., 2007)." ></td>
	<td class="line x" title="274:277	We also saw that it worked across four annotators who have different rationale-marking styles." ></td>
	<td class="line x" title="275:277	In future, we are interested in new domains that can adaptively solicit rationales for some or all training examples." ></td>
	<td class="line x" title="276:277	Our new method, being essentially Bayesian inference, is potentially extensible to many other situationsother tasks, classifier architectures, and more complex features." ></td>
	<td class="line x" title="277:277	39" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D08-1058
Using Bilingual Knowledge and Ensemble Techniques for Unsupervised Chinese Sentiment Analysis
Wan, Xiaojun;"></td>
	<td class="line x" title="1:180	Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 553561, Honolulu, October 2008." ></td>
	<td class="line x" title="2:180	c2008 Association for Computational Linguistics Using Bilingual Knowledge and Ensemble Techniques for Unsupervised Chinese Sentiment Analysis  Xiaojun Wan Institute of Compute Science and Technology Peking University Beijing 100871, China wanxiaojun@icst.pku.edu.cn  Abstract It is a challenging task to identify sentiment polarity of Chinese reviews because the resources for Chinese sentiment analysis are limited." ></td>
	<td class="line x" title="3:180	Instead of leveraging only monolingual Chinese knowledge, this study proposes a novel approach to leverage reliable English resources to improve Chinese sentiment analysis." ></td>
	<td class="line x" title="4:180	Rather than simply projecting English resources onto Chinese resources, our approach first translates Chinese reviews into English reviews by machine translation services, and then identifies the sentiment polarity of English reviews by directly leveraging English resources." ></td>
	<td class="line x" title="5:180	Furthermore, our approach performs sentiment analysis for both Chinese reviews and English reviews, and then uses ensemble methods to combine the individual analysis results." ></td>
	<td class="line x" title="6:180	Experimental results on a dataset of 886 Chinese product reviews demonstrate the effectiveness of the proposed approach." ></td>
	<td class="line x" title="7:180	The individual analysis of the translated English reviews outperforms the individual analysis of the original Chinese reviews, and the combination of the individual analysis results further improves the performance." ></td>
	<td class="line x" title="8:180	1 Introduction In recent years, sentiment analysis (including subjective/objective analysis, polarity identification, opinion extraction, etc.) has drawn much attention in the NLP field." ></td>
	<td class="line x" title="9:180	In this study, the objective of sentiment analysis is to annotate a given text for polarity orientation (positive/negative)." ></td>
	<td class="line x" title="10:180	Polarity orientation identification has many useful applications, including opinion summarization (Ku et al., 2006) and sentiment retrieval (Eguchi and Lavrenko, 2006)." ></td>
	<td class="line x" title="11:180	To date, most of the research focuses on English and a variety of reliable English resources for sentiment analysis are available, including polarity lexicon, contextual valence shifters, etc. However, the resources for other languages are limited." ></td>
	<td class="line x" title="12:180	In particular, few reliable resources are available for Chinese sentiment analysis 1  and it is not a trivial task to manually label reliable Chinese sentiment resources." ></td>
	<td class="line x" title="13:180	Instead of using only the limited Chinese knowledge, this study aims to improve Chinese sentiment analysis by making full use of bilingual knowledge in an unsupervised way, including both Chinese resources and English resources." ></td>
	<td class="line x" title="14:180	Generally speaking, there are two unsupervised scenarios for borrowing English resources for sentiment analysis in other languages: one is to generate resources in a new language by leveraging on the resources available in English via cross-lingual projections, and then perform sentiment analysis in the English language based on the generated resources, which has been investigated by Mihalcea et al.(2007); the other is to translate the texts in a new language into English texts, and then perform sentiment analysis in the English language, which has not yet been investigated." ></td>
	<td class="line x" title="16:180	In this study, we first translate Chinese reviews into English reviews by using machine translation services, and then identify the sentiment polarity of English reviews by directly leveraging English resources." ></td>
	<td class="line x" title="17:180	Furthermore, ensemble methods are employed to combine the individual analysis results in each language (i.e. Chinese and English) in order to obtain improved results." ></td>
	<td class="line x" title="18:180	Given machine translation services between the selected target language and English, the proposed approach can be applied to any other languages as well." ></td>
	<td class="line x" title="19:180	Experiments have been performed on a dataset of 886 Chinese product reviews." ></td>
	<td class="line x" title="20:180	Two commercial  1  This study focuses on Simplified Chinese." ></td>
	<td class="line x" title="21:180	553 machine translation services (i.e. Google Translate and Yahoo Babel Fish) and a baseline dictionarybased system are used for translating Chinese reviews into English reviews." ></td>
	<td class="line x" title="22:180	Experimental results show that the analysis of English reviews translated by the commercial translation services outperforms the analysis of original Chinese reviews." ></td>
	<td class="line x" title="23:180	Moreover, the analysis performance can be further improved by combining the individual analysis results in different languages." ></td>
	<td class="line x" title="24:180	The results also demonstrate that our proposed approach is more effective than the approach that leverages generated Chinese resources." ></td>
	<td class="line x" title="25:180	The rest of this paper is organized as follows: Section 2 introduces related work." ></td>
	<td class="line x" title="26:180	The proposed approach is described in detail in Section 3." ></td>
	<td class="line x" title="27:180	Section 4 shows the experimental results." ></td>
	<td class="line x" title="28:180	Lastly we conclude this paper in Section 5." ></td>
	<td class="line x" title="29:180	2 Related Work Polarity identification can be performed on word level, sentence level or document level." ></td>
	<td class="line x" title="30:180	Related work for word-level polarity identification includes (Hatzivassiloglou and McKeown, 1997; Kim and Hovy." ></td>
	<td class="line x" title="31:180	2004; Takamura et al., 2005; Yao et al. 2006; Kaji and Kitsuregawa, 2007), and related work for sentence-level polarity identification includes (Yu and Hatzivassiloglou, 2003; Kim and Hovy." ></td>
	<td class="line x" title="32:180	2004) Word-level or sentence-level sentiment analysis is not the focus of this paper." ></td>
	<td class="line x" title="33:180	Generally speaking, document-level polarity identification methods can be categorized into unsupervised and supervised." ></td>
	<td class="line x" title="34:180	Unsupervised methods involve deriving a sentiment metric for text without training corpus." ></td>
	<td class="line x" title="35:180	Turney (2002) predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs, which is denoted as the semantic oriented method." ></td>
	<td class="line x" title="36:180	Kim and Hovy (2004) build three models to assign a sentiment category to a given sentence by combining the individual sentiments of sentiment-bearing words." ></td>
	<td class="line x" title="37:180	Hiroshi et al.(2004) use the technique of deep language analysis for machine translation to extract sentiment units in text documents." ></td>
	<td class="line x" title="39:180	Kennedy and Inkpen (2006) determine the sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers." ></td>
	<td class="line x" title="40:180	Devitt and Ahmad (2007) explore a computable metric of positive or negative polarity in financial news text." ></td>
	<td class="line x" title="41:180	Supervised methods consider the sentiment analysis task as a classification task and use labeled corpus to train the classifier." ></td>
	<td class="line oc" title="42:180	Since the work of Pang et al.(2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005a; Read, 2005)." ></td>
	<td class="line x" title="44:180	Most recently, McDonald et al.(2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity." ></td>
	<td class="line x" title="46:180	Blitzer et al.(2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products." ></td>
	<td class="line x" title="48:180	Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpusbased classifier and a lexicon-based classifier with precision-based vote weighting." ></td>
	<td class="line x" title="49:180	Research work focusing on Chinese sentiment analysis includes (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 2007; Wang et al., 2007)." ></td>
	<td class="line x" title="50:180	Such work represents heuristic extensions of the unsupervised or supervised methods for English sentiment analysis." ></td>
	<td class="line x" title="51:180	To date, the most closely related work is Mihalcea et al.(2007), which explores cross-lingual projections to generate subjectivity analysis resources in Romanian by leveraging on the tools and resources available in English." ></td>
	<td class="line x" title="53:180	They have investigated two approaches: a lexicon-based approach based on Romanian subjectivity lexicon translated from English lexicon, and a corpus-based approach based on Romanian subjectivity-annotated corpora obtained via cross-lingual projections." ></td>
	<td class="line x" title="54:180	In this study, we focus on unsupervised sentiment polarity identification and we only investigate the lexicon-based approach in the experiments." ></td>
	<td class="line x" title="55:180	Other related work includes subjective/objective analysis (Hatzivassiloglon and Wiebe, 2000; Riloff and Wiebe, 2003) and opinion mining and summarization (Liu et al., 2005; Popescu and Etzioni." ></td>
	<td class="line x" title="56:180	2005; Choi et al., 2006; Ku et al., 2006; Titov and McDonald, 2008)." ></td>
	<td class="line x" title="57:180	3 The Proposed Approach 3.1 Overview The motivation of our approach is to make full use of bilingual knowledge to improve sentiment analysis in a target language, where the resources 554 for sentiment analysis are limited or unreliable." ></td>
	<td class="line x" title="58:180	This study focuses on unsupervised polarity identification of Chinese product reviews by using both the rich English knowledge and the limited Chinese knowledge." ></td>
	<td class="line x" title="59:180	The framework of our approach is illustrated in Figure 1." ></td>
	<td class="line x" title="60:180	A Chinese review is translated into the corresponding English review using machine translation services, and then the Chinese review and the English review are analyzed based on Chinese resources and English resources, respectively." ></td>
	<td class="line x" title="61:180	The analysis results are then combined to obtain more accurate results under the assumption that the individual sentiment analysis can complement each other." ></td>
	<td class="line x" title="62:180	Note that in the framework, different machine translation services can be used to obtain different English reviews, and the analysis of English reviews translated by a specific machine translation service is conducted separately." ></td>
	<td class="line x" title="63:180	For simplicity, we consider the English reviews translated by different machine translation services as reviews in different languages, despite the fact that in essence, they are still in English." ></td>
	<td class="line x" title="64:180	Figure 1." ></td>
	<td class="line x" title="65:180	Framework of our approach Formally, give a review rev 0  in the target language (i.e. Chinese), the corresponding review rev i in the ith language is obtained by using a translation function:  rev i =f  i Trans (rev 0 ) where 1ip and p is the total number of machine translation services." ></td>
	<td class="line x" title="66:180	For each review rev k  in the kth language (0kp), we employ the semantic oriented approach to assign a semantic orientation value f  k SO (rev k ) to the review, and the polarity orientation of the review can be simply predicated based on the value by using a threshold." ></td>
	<td class="line x" title="67:180	Given a set of semantic orientation values F SO ={f  k SO (rev k ) | 0kp}, the ensemble methods aim to derive a new semantic orientation value )( 0 revf Ensemble SO based on the values in F SO , which can be used to better classify the review as positive or negative." ></td>
	<td class="line x" title="68:180	The steps of review translation, individual semantic orientation value computation and ensemble combination are described in details in the next sections, respectively." ></td>
	<td class="line x" title="69:180	3.2 Review Translation Translation of a Chinese review into an English review is the first step of the proposed approach." ></td>
	<td class="line x" title="70:180	Manual translation is time-consuming and laborintensive, and it is not feasible to manually translate a large amount of Chinese product reviews in real applications." ></td>
	<td class="line x" title="71:180	Fortunately, machine translation techniques have been well developed in the NLP field, though the translation performance is far from satisfactory." ></td>
	<td class="line x" title="72:180	A few commercial machine translation services can be publicly accessed." ></td>
	<td class="line x" title="73:180	In this study, the following two commercial machine translation services and one baseline system are used to translate Chinese reviews into English reviews." ></td>
	<td class="line x" title="74:180	Google Translate 2  (GoogleTrans): Google Translate is one of the state-of-the-art commercial machine translation systems used today." ></td>
	<td class="line x" title="75:180	Google Translate applies statistical learning techniques to build a translation model based on both monolingual text in the target language and aligned text consisting of examples of human translations between the languages." ></td>
	<td class="line x" title="76:180	Yahoo Babel Fish 3  (YahooTrans): Different from Google Translate, Yaho Babel Fish uses SYSTRANs rule-based translation engine." ></td>
	<td class="line x" title="77:180	SYSTRAN was one of the earliest developers of machine translation software." ></td>
	<td class="line x" title="78:180	SYSTRAN applies complex sets of specific rules defined by linguists to analyze and then transfer the grammatical structure of the source language into the target language." ></td>
	<td class="line x" title="79:180	Baseline Translate (DictTrans): We simply develop a translation method based only on one-toone term translation in a large Chinese-to-English  2  http://translate.google.com/translate_t 3  http://babelfish.yahoo.com/translate_txt Chinese review Chinese Resource English review Machine translation Chinese sentiment analysis Ensemble English sentiment analysis English Resource Polarity Value Polarity Value Pos\Neg 555 dictionary." ></td>
	<td class="line x" title="80:180	Each term in a Chinese review is translated by the first corresponding term in the Chinese-to-English dictionary, without any other processing steps." ></td>
	<td class="line x" title="81:180	In this study, we use the LDC_CE_DIC2.0 4  constructed by LDC as the dictionary for translation, which contains 128366 Chinese terms and their corresponding English terms." ></td>
	<td class="line x" title="82:180	The Chinese-to-English translation performances of the two commercial systems are deemed much better than the weak baseline system." ></td>
	<td class="line x" title="83:180	Google Translate has achieved very good results on the Chinese-to-English translation tracks of NIST open machine translation test (MT) 5  and it ranks the first on most tracks." ></td>
	<td class="line x" title="84:180	In the Chinese-to-English task of MT2005, the BLEU-4 score of Google Translate is 0.3531, and the BLEU-4 score of SYSTRAN is 0.1471." ></td>
	<td class="line x" title="85:180	We can deduce that Google Translate is better than Yahoo Babel Fish, without considering the recent improvements of the two systems." ></td>
	<td class="line x" title="86:180	Here are two running example of Chinese reviews and the translated English reviews (HumanTrans refers to human translation): Positive Example: , HumanTrans: Many advantages and very good shape." ></td>
	<td class="line x" title="87:180	GoogleTrans: Many advantages, the shape is also very good." ></td>
	<td class="line x" title="88:180	YahooTrans: Merit very many, the contour very is also good." ></td>
	<td class="line x" title="89:180	DictTrans: merit very many figure also very good Negative example:  HumanTrans: The memory is too small to support IR." ></td>
	<td class="line x" title="90:180	GoogleTrans: Memory is too small not to support IR." ></td>
	<td class="line x" title="91:180	YahooTrans:The memory too is small does not support infrared." ></td>
	<td class="line x" title="92:180	DictTrans: memory highest small negative not to be in favor of ir." ></td>
	<td class="line x" title="93:180	3.3 Individual Semantic Orientation Value Computation For any specific language, we employ the semantic orientated approach (Kennedy and Inkpen, 2006) to compute the semantic orientation value of a review." ></td>
	<td class="line x" title="94:180	The unsupervised approach is quite  straightforward and it makes use of the following sentiment lexicons: positive Lexicon (Positive_Dic) including terms expressing positive polarity, Negative Lexicon (Negative_Dic) including terms expressing negative polarity, Negation  4  http://projects.ldc.upenn.edu/Chinese/LDC_ch.htm 5  http://www.nist.gov/speech/tests/mt/ Lexicon (Negation_Dic) including terms that are used to reverse the semantic polarity of a particular term, and Intensifier Lexicon (Intensifier_Dic) including terms that are used to change the degree to which a term is positive or negative." ></td>
	<td class="line x" title="95:180	In this study, we conduct our experiments within two languages, and we collect and use the following popular and available Chinese and English sentiment lexicons 6 , without any further filtering and labeling: 1) Chinese lexicons Positive_Dic cn : 3730 Chinese positive terms were collected from the Chinese Vocabulary for Sentiment Analysis (VSA) 7  released by HOWNET." ></td>
	<td class="line x" title="96:180	Negative_Dic cn : 3116 Chinese negative terms were collected from Chinese Vocabulary for Sentiment Analysis (VSA) released by HOWNET." ></td>
	<td class="line x" title="97:180	Negation_Dic cn : 13 negation terms were collected from related papers." ></td>
	<td class="line x" title="98:180	Intensifier_Dic cn : 148 intensifier terms were collected from Chinese Vocabulary for Sentiment Analysis (VSA) released by HOWNET." ></td>
	<td class="line x" title="99:180	2) English lexicons Positive_Dic en : 2718 English positive terms were collected from the feature file subjclueslen1HLTEMNLP05.tff 8  containing the subjectivity clues used in the work (Wilson et al., 2005a; Wilson et al., 2005b)." ></td>
	<td class="line x" title="100:180	The clues in this file were collected from a number of sources." ></td>
	<td class="line x" title="101:180	Some were culled from manually developed resources, e.g. general inquirer 9  (Stone et al., 1966)." ></td>
	<td class="line x" title="102:180	Others were identified automatically using both annotated and unannotated data." ></td>
	<td class="line x" title="103:180	A majority of the clues were collected as part of work reported in Riloff and Wiebe (2003)." ></td>
	<td class="line x" title="104:180	Negative_Dic en : 4910 English negative terms were collected from the same file described above." ></td>
	<td class="line x" title="105:180	Negation_Dic en : 88 negation terms were collected from the feature file valenceshifters.tff used in the work (Wilson et al., 2005a; Wilson et al., 2005b)." ></td>
	<td class="line x" title="106:180	Intensifier_Dic en : 244 intensifier terms were collected from the feature file intensifiers2.tff used in the work (Wilson et al., 2005a; Wilson et al., 2005b)." ></td>
	<td class="line x" title="107:180	6  In this study, we focus on using a few popular resources in both Chinese and English for comparative study, instead of trying to collect and use all available resources." ></td>
	<td class="line x" title="108:180	7  http://www.keenage.com/html/e_index.html 8  http://www.cs.pitt.edu/mpqa/ 9  http://www.wjh.harvard.edu/~inquirer/homecat.htm 556 The semantic orientation value f  k SO (rev k ) for rev k  is computed by summing the polarity values of all words in the review, making use of both the word polarity defined in the positive and negative lexicons and the contextual valence shifters defined in the negation and intensifier lexicons." ></td>
	<td class="line x" title="109:180	The algorithm is illustrated in Figure 2." ></td>
	<td class="line x" title="110:180	Input: a review rev k  in the kth language." ></td>
	<td class="line x" title="111:180	Four lexicons in the kth language: Positive_Dic k , Negative_Dic k , Negation_Dic k , Intensifier_Dic k , which are either Chinese or English lexicons; Output: Polarity Value f  k SO (rev k ); Algorithm Compute_SO: 1." ></td>
	<td class="line x" title="112:180	Tokenize review rev k  into sentence set S and each sentence sS  is tokenized into word set W s ; 2." ></td>
	<td class="line x" title="113:180	For any word w in a sentence sS, compute its SO value SO(w) as follows: 1) if wPositive_Dic k , SO(w)=PosValue; 2) If wNegative_Dic k , SO(w)=NegValue; 3) Otherwise, SO(w)=0; 4) Within the window of q words previous to w, if there is a term wNegation_Dic k , SO(w)= SO(w); 5) Within the window of q words previous to w, if there is a term wIntensifier_Dic k , SO(w) = SO(w); 3." ></td>
	<td class="line x" title="114:180	  = SsWw kk SO s wSOrevf )()( ; Figure 2." ></td>
	<td class="line x" title="115:180	The algorithm for semantic orientation value computation In the above algorithm, PosValue and NegValue are the polarity values for positive words and negative words respectively." ></td>
	<td class="line x" title="116:180	We empirically set PosValue=1 and NegValue= 2 because negative words usually contribute more to the overall semantic orientation of the review than positive words, according to our empirical analysis." ></td>
	<td class="line x" title="117:180	 >1 aims to intensify the polarity value and we simply set  =2." ></td>
	<td class="line x" title="118:180	q is the parameter controlling the window size within which the negation terms and intensifier terms have influence on the polarity words and here q is set to 2 words." ></td>
	<td class="line x" title="119:180	Note that the above parameters are tuned only for Chinese sentiment analysis, and they are used for sentiment analysis in the English language without further tuning." ></td>
	<td class="line x" title="120:180	The tokenization of Chinese reviews involves Chinese word segmentation." ></td>
	<td class="line x" title="121:180	Usually, if the semantic orientation value of a review is less than 0, the review is labeled as negative, otherwise, the review is labeled as positive." ></td>
	<td class="line x" title="122:180	3.4 Ensemble Combination After obtaining the set of semantic orientation values F SO ={f  k SO (rev k ) | 0kp} by using the semantic oriented approach, where p is the number of English translations for each Chinese review, we exploit the following ensemble methods for deriving a new semantic orientation value )( 0 revf Ensemble SO : 1) Average It is the most intuitive combination method and the new value is the average of the values in F SO : 1 )( )( 00 + =  = p revf revf p k kk SO Ensemble SO  Note that after the new value of a review is obtained, the polarity tag of the review is assigned in the same way as described in Section 3.3." ></td>
	<td class="line x" title="123:180	2) Weighted Average This combination method improves the average combination method by associating each individual value with a weight, indicating the relative confidence in the value." ></td>
	<td class="line x" title="124:180	 = = p k kk SOk Ensemble SO revfrevf 0 0 )()(   where  k [0, 1] is the weight associated with f  k SO (rev k )." ></td>
	<td class="line x" title="125:180	The weights can be set in the following two ways: Weighting Scheme1: The weight of f  k SO (rev k ) is set to the accuracy of the individual analysis in the kth language." ></td>
	<td class="line x" title="126:180	Weighting Scheme2: The weight of f  k SO (rev k ) is set to be the maximal correlation coefficient between the analysis results in the kth language and the analysis results in any other language." ></td>
	<td class="line x" title="127:180	The underlying idea is that if the analysis results in one language are highly consistent with the analysis results in another language, the results are deemed to be more reliable." ></td>
	<td class="line x" title="128:180	Given two lists of semantic values for all reviews, we use the Pearsons correlation coefficient to measure the correlation between them." ></td>
	<td class="line x" title="129:180	The weight associated with function f  k SO (rev k ) is then defined as the maximal Pearsons correlation coefficient between the reviews values in the kth language and the reviews values in any other language." ></td>
	<td class="line x" title="130:180	3) Max 557 The new value is the maximum value in F SO : { }pkrevfrevf kk SO Ensemble SO = 0|)(max)( 0  4) Min The new value is the minimum value in F SO : { }pkrevfrevf kk SO Ensemble SO = 0|)(min)( 0  5) Average Max&Min The new value is the average of the maximum value and the minimum value in F SO : {}{} 2 0|)(min0|)(max )( 0 pkrevfpkrevf revf kk SO kk SOEnsemble SO + =  6) Majority Voting This combination method relies on the final polarity tags, instead of the semantic orientation values." ></td>
	<td class="line x" title="131:180	A review can obtain p+1 polarity tags based on the individual analysis results in the p+1 languages." ></td>
	<td class="line x" title="132:180	The polarity tag receiving more votes is chosen as the final polarity tag of the review." ></td>
	<td class="line x" title="133:180	4 Empirical Evaluation 4.1 Dataset and Evaluation Metrics In order to assess the performance of the proposed approach, we collected 1000 product reviews from a popular Chinese IT product web site-IT168 10 . The reviews were posted by users and they focused on such products as mp3 players, mobile phones, digital camera and laptop computers." ></td>
	<td class="line x" title="134:180	Users usually selected for each review an icon indicating postive or negative." ></td>
	<td class="line x" title="135:180	The reviews were first categorized into positive and negative classes according to the associated icon." ></td>
	<td class="line x" title="136:180	The polarity labels for the reviews were then checked by subjects." ></td>
	<td class="line x" title="137:180	Finally, the dataset contained 886 product reviews with accurate polarity labels." ></td>
	<td class="line x" title="138:180	All the 886 reviews were used as test set." ></td>
	<td class="line x" title="139:180	We used the standard precision, recall and Fmeasure to measure the performance of positive and negative class, respectively, and used the MacroF measure and accuracy metric to measure the overall performance of the system." ></td>
	<td class="line x" title="140:180	The metrics are defined the same as in general text categorization." ></td>
	<td class="line x" title="141:180	4.2 Individual Analysis Results In this section, we investigate the following individual sentiment analysis results in each specified language: CN: This method uses only Chinese lexicons to analyze Chinese reviews;  10  http://www.it168.com GoogleEN: This method uses only English lexicons to analyze English reviews translated by GoogleTrans; YahooEN: This method uses only English lexicons to analyze English reviews translated by YahooTrans; DictEN: This method uses only English lexicons to analyze English reviews translated by DictTrans; In addition to the above methods for using English resources, the lexicon-based method investigated in Mihalcea et al.(2007) can also use English resources by directly projecting English lexicons into Chinese lexicons." ></td>
	<td class="line x" title="143:180	We use a large English-to-Chinese dictionary LDC_EC_DIC2.0 11  with 110834 entries for projecting English lexicons into Chinese lexicons via one-to-one translation." ></td>
	<td class="line x" title="144:180	Based on the generated Chinese lexicons, two other individual methods are investigated in the experiments: CN2: This method uses only the generated Chinese Resources to analyze Chinese reviews." ></td>
	<td class="line x" title="145:180	CN3: This method combines the original Chinese lexicons and the generated Chinese lexicons and uses the extended lexicons to analyze Chinese reviews." ></td>
	<td class="line x" title="146:180	Table 1 provides the performance values of all the above individual methods." ></td>
	<td class="line x" title="147:180	Seen from the table, the performances of GoogleEN and YahooEN are much better than the baseline CN method, and even the DictEN performs as well as CN." ></td>
	<td class="line x" title="148:180	The results demonstrate that the use of English resources for sentiment analysis of translated English reviews is an effective way for Chinese sentiment analysis." ></td>
	<td class="line x" title="149:180	We can also see that the English sentiment analysis performance relies positively on the translation performance, and GoogleEN performs the best while DictEN performs the worst, which is consistent with the fact the GoogleTrans is deemed the best of the three machine translation systems, while DictTrans is the weakest one." ></td>
	<td class="line x" title="150:180	Furthermore, the CN method outperforms the CN2 and CN3 methods, and the CN2 method performs the worst, which shows that the generated Chinese lexicons do not give any contributions to the performance of Chinese sentiment analysis." ></td>
	<td class="line x" title="151:180	We explain the results by the fact that the term-based one-to-one translation is inaccurate and the generated Chinese lexicons are not reliable." ></td>
	<td class="line x" title="152:180	Overall, the  11  http://projects.ldc.upenn.edu/Chinese/LDC_ch.htm 558 approach through cross-lingual lexicon translation does not work well for Chinese sentiment analysis in our experiments." ></td>
	<td class="line x" title="153:180	4.3 Ensemble Results In this section, we first use the simple average ensemble method to combine different individual analysis results." ></td>
	<td class="line x" title="154:180	Table 2 provides the performance values of the average ensemble results based on different individual methods." ></td>
	<td class="line x" title="155:180	Seen from Tables 1 and 2, almost all of the average ensembles outperforms the baseline CN method and the corresponding individual methods, which shows that each individual methods have their own evidences for sentiment analysis, and thus fusing the evidences together can improve performance." ></td>
	<td class="line x" title="156:180	For the methods of CN+GoogleEN, CN+YahooEN and CN+DictEN, we can see the ensemble performance is not positively relying on the translation performance: CN+YahooEN performs better than CN+GoogleEN, and even CN+DictEN performs as well as CN+GoogleEN." ></td>
	<td class="line x" title="157:180	The results show that the individual methods in the ensembles can complement each other, and even the combination of two weak individual methods can achieve good performance." ></td>
	<td class="line x" title="158:180	However, the DictEN method is not effective when the ensemble methods have already included GoogleEN and YahooEN." ></td>
	<td class="line x" title="159:180	Overall, the performances of the ensemble methods rely on the performances of the most effective constituent individual methods: the methods including both GoogleEN and YahooEN perform much better than other methods, and CN+GoogleEN+YahooEN performs the best out of all the methods." ></td>
	<td class="line x" title="160:180	We further show the results of four typical average ensembles by varying the combination weights." ></td>
	<td class="line x" title="161:180	The combination weights are respectively specified as  CN+(1- )GoogleEN,  CN+(1 )YahooEN,  CN+(1- )DictEN,  1 CN+ 2 GoogleEN+(1- 1 - 2 )YahooEN." ></td>
	<td class="line x" title="162:180	The results over the MacroF metric are shown in Figures 3 and 4 respectively." ></td>
	<td class="line x" title="163:180	We can see from the figures that GoogleEN and YahooEN are dominant factors in the ensemble methods." ></td>
	<td class="line x" title="164:180	We then investigate to use other ensemble methods introduced in Section 3.4 to combine the CN, GoogleEN and YahooEN methods." ></td>
	<td class="line x" title="165:180	Table 3 gives the comparison results." ></td>
	<td class="line x" title="166:180	The methods of Weighted Average1 and Weighted Average2 are two weighted average ensembles using the two weighing schemes, respectively." ></td>
	<td class="line x" title="167:180	We can see that all the ensemble methods outperform the constituent individual method, while the two weighted average ensembles perform the best." ></td>
	<td class="line x" title="168:180	The results further demonstrate the good effectiveness of the ensemble combination of individual analysis results for Chinese sentiment analysis." ></td>
	<td class="line x" title="169:180	Positive Negative Total Individual Method Precision Recall F-measure Precision Recall F-measure MacroF Accuracy CN 0.681 0.929 0.786 0.882 0.549 0.677 0.732 0.743 CN2 0.615 0.772 0.684 0.678 0.499 0.575 0.630 0.638 CN3 0.702 0.836 0.763 0.788 0.632 0.702 0.732 0.736 GoogleEN 0.764 0.914 0.832 0.888 0.708 0.787 0.810 0.813 YahooEN 0.763 0.871 0.814 0.844 0.720 0.777 0.795 0.797 DictEN 0.738 0.761 0.749 0.743 0.720 0.731 0.740 0.740 Table 1." ></td>
	<td class="line x" title="170:180	Individual analysis results Positive Negative Total Average Ensemble Precision Recall F-measure Precision Recall F-measure MacroF Accuracy GoogleEN+YahooEN 0.820 0.900 0.858 0.885 0.795 0.838 0.848 0.848 GoogleEN+YahooEN +DictEN 0.841 0.845 0.843 0.838 0.834 0.836 0.840 0.840 CN+GoogleEN 0.754 0.949 0.840 0.928 0.678 0.784 0.812 0.816 CN+YahooEN 0.784 0.925 0.848 0.904 0.736 0.811 0.830 0.832 CN+DictEN 0.790 0.867 0.827 0.847 0.761 0.801 0.814 0.815 CN+GoogleEN +YahooEN 0.813 0.927 0.866 0.911 0.779 0.840 0.853 0.854 CN+GoogleEN+ YahooEN+DictEN 0.831 0.891 0.860 0.878 0.811 0.843 0.852 0.852 Table 2." ></td>
	<td class="line x" title="171:180	Average combination results 559 0.72 0.74 0.76 0.78 0.8 0.82 0.84 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  Mac r oF CN+GoogleEN CN+YahooEN CN+DictEN  0 0.3 0.6 0.9 0 0.2 0.4 0.6 0.8 1 0.65 0.7 0.75 0.8 0.85 0.9 MacroF  1  2 0.85-0.9 0.8-0.85 0.75-0.8 0.7-0.75 0.65-0.7  Figure 3." ></td>
	<td class="line x" title="172:180	Ensemble performance vs. weight   for  CN+(1- )GoogleEN/YahooEN/DictEN Figure 4." ></td>
	<td class="line x" title="173:180	Ensemble performance vs. weights  1 and  2  for  1 CN+ 2 GoogleEN+(1- 1 - 2 ) YahooEN  Positive Negative Total Ensemble Method Precision Recall F-measure Precision Recall F-measure MacroF Accuracy Average 0.813 0.927 0.866 0.911 0.779 0.840 0.853 0.854 Weighted Average1 0.825 0.922 0.871 0.908 0.798 0.849 0.860 0.861 Weighted Average2 0.822 0.922 0.869 0.908 0.793 0.847 0.858 0.859 Max 0.765 0.940 0.844 0.919 0.701 0.795 0.820 0.823 Min 0.901 0.787 0.840 0.805 0.910 0.854 0.847 0.848 Average Max&Min 0.793 0.936 0.859 0.918 0.747 0.824 0.841 0.843 Majority Voting 0.765 0.940 0.844 0.919 0.701 0.795 0.820 0.823 Table 3." ></td>
	<td class="line x" title="174:180	Ensemble results for CN & GoogleEN & YahooEN 5 Conclusion and Future Work This paper proposes a novel approach to use English sentiment resources for Chinese sentiment analysis by employing machine translation and ensemble techniques." ></td>
	<td class="line x" title="175:180	Chinese reviews are translated into English reviews and the analysis results of both Chinese reviews and English reviews are combined to improve the overall accuracy." ></td>
	<td class="line x" title="176:180	Experimental results demonstrate the encouraging performance of the proposed approach." ></td>
	<td class="line x" title="177:180	In future work, more additional English resources will be used to further improve the results." ></td>
	<td class="line x" title="178:180	We will also apply the idea to supervised Chinese sentiment analysis." ></td>
	<td class="line x" title="179:180	Acknowledgments This work was supported by the National Science Foundation of China (No.60703064), the Research Fund for the Doctoral Program of Higher Education of China (No.20070001059) and the National High Technology Research and Development Program of China (No.2008AA01Z421)." ></td>
	<td class="line x" title="180:180	We also thank the anonymous reviewers for their useful comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="I08-1039
Learning to Shift the Polarity of Words for Sentiment Classification
Ikeda, Daisuke;Takamura, Hiroya;Ratinov, Lev;Okumura, Manabu;"></td>
	<td class="line x" title="1:199	Learning to Shift the Polarity of Words for Sentiment Classification Daisuke Ikeday Hiroya Takamuraz Lev-Arie Ratinovyy Manabu Okumuraz yDepartment of Computational Intelligence and Systems Science, Tokyo Institute of Technology ikeda@lr.pi.titech.ac.jp yyDepartment of Computer Science, University of Illinois at Urbana-Champaign ratinov2@uiuc.edu zPrecision and Intelligence Laboratory, Tokyo Institute of Technology ftakamura,okug@pi.titech.ac.jp Abstract We propose a machine learning based method of sentiment classification of sentences using word-level polarity." ></td>
	<td class="line x" title="2:199	The polarities of words in a sentence are not always the same as that of the sentence, because there can be polarity-shifters such as negation expressions." ></td>
	<td class="line x" title="3:199	The proposed method models the polarity-shifters." ></td>
	<td class="line x" title="4:199	Our model can be trained in two different ways: word-wise and sentence-wise learning." ></td>
	<td class="line x" title="5:199	In sentence-wise learning, the model can be trained so that the prediction of sentence polarities should be accurate." ></td>
	<td class="line x" title="6:199	The model can also be combined with features used in previous work such as bag-of-words and n-grams." ></td>
	<td class="line x" title="7:199	We empirically show that our method almost always improves the performance of sentiment classification of sentences especially when we have only small amount of training data." ></td>
	<td class="line x" title="8:199	1 Introduction Due to the recent popularity of the internet, individuals have been able to provide various information to the public easily and actively (e.g., by weblogs or online bulletin boards)." ></td>
	<td class="line x" title="9:199	The information often includes opinions or sentiments on a variety of things such as new products." ></td>
	<td class="line x" title="10:199	A huge amount of work has been devoted to analysis of the information, which is called sentiment analysis." ></td>
	<td class="line x" title="11:199	The sentiment analysis has been done at different levels including words, sentences, and documents." ></td>
	<td class="line x" title="12:199	Among them, we focus on the sentiment classification of sentences, the task to classify sentences into positive or negative, because this task is fundamental and has a wide applicability in sentiment analysis." ></td>
	<td class="line x" title="13:199	For example, we can retrieve individuals opinions that are related to a product and can find whether they have the positive attitude to the product." ></td>
	<td class="line x" title="14:199	There has been much work on the identification of sentiment polarity of words." ></td>
	<td class="line x" title="15:199	For instance, beautiful is positively oriented, while dirty is negatively oriented." ></td>
	<td class="line x" title="16:199	We use the term sentiment words to refer to those words that are listed in a predefined polarity dictionary." ></td>
	<td class="line x" title="17:199	Sentiment words are a basic resource for sentiment analysis and thus believed to have a great potential for applications." ></td>
	<td class="line x" title="18:199	However, it is still an open problem how we can effectively use sentiment words to improve performance of sentiment classification of sentences or documents." ></td>
	<td class="line x" title="19:199	The simplest way for that purpose would be the majority voting by the number of positive words and the number of negative words in the given sentence." ></td>
	<td class="line x" title="20:199	However, the polarities of words in a sentence are not always the same as that of the sentence, because there can be polarity-shifters such as negation expressions." ></td>
	<td class="line x" title="21:199	This inconsistency of word-level polarity and sentence-level polarity often causes errors in classification by the simple majority voting method." ></td>
	<td class="line x" title="22:199	A manual list of polarity-shifters, which are the words that can shift the sentiment polarity of another word (e.g., negations), has been suggested." ></td>
	<td class="line x" title="23:199	However, it has limitations due to the diversity of expressions." ></td>
	<td class="line x" title="24:199	Therefore, we propose a machine learning based method that models the polarity-shifters." ></td>
	<td class="line x" title="25:199	The model can be trained in two different ways: word-wise 296 and sentence-wise." ></td>
	<td class="line x" title="26:199	While the word-wise learning focuses on the prediction of polarity shifts, the sentence-wise learning focuses more on the prediction of sentence polarities." ></td>
	<td class="line x" title="27:199	The model can also be combined with features used in previous work such as bag-of-words, n-grams and dependency trees." ></td>
	<td class="line x" title="28:199	We empirically show that our method almost always improves the performance of sentiment classification of sentences especially when we have only small amount of training data." ></td>
	<td class="line x" title="29:199	The rest of the paper is organized as follows." ></td>
	<td class="line x" title="30:199	In Section 2, we briefly present the related work." ></td>
	<td class="line x" title="31:199	In Section 3, we discuss well-known methods that use word-level polarities and describe our motivation." ></td>
	<td class="line x" title="32:199	In Section 4, we describe our proposed model, how to train the model, and how to classify sentences using the model." ></td>
	<td class="line x" title="33:199	We present our experiments and results in Section 5." ></td>
	<td class="line x" title="34:199	Finally in Section 6, we conclude our work and mention possible future work." ></td>
	<td class="line pc" title="35:199	2 Related Work Supervised machine learning methods including Support Vector Machines (SVM) are often used in sentiment analysis and shown to be very promising (Pang et al., 2002; Matsumoto et al., 2005; Kudo and Matsumoto, 2004; Mullen and Collier, 2004; Gamon, 2004)." ></td>
	<td class="line pc" title="36:199	One of the advantages of these methods is that a wide variety of features such as dependency trees and sequences of words can easily be incorporated (Matsumoto et al., 2005; Kudo and Matsumoto, 2004; Pang et al., 2002)." ></td>
	<td class="line x" title="37:199	Our attempt in this paper is not to use the information included in those substructures of sentences, but to use the word-level polarities, which is a resource usually at hand." ></td>
	<td class="line x" title="38:199	Thus our work is an instantiation of the idea to use a resource on one linguistic layer (e.g., word level) to the analysis of another layer (sentence level)." ></td>
	<td class="line x" title="39:199	There have been some pieces of work which focus on multiple levels in text." ></td>
	<td class="line x" title="40:199	Mao and Lebanon (2006) proposed a method that captures local sentiment flow in documents using isotonic conditional random fields." ></td>
	<td class="line x" title="41:199	Pang and Lee (2004) proposed to eliminate objective sentences before the sentiment classification of documents." ></td>
	<td class="line x" title="42:199	McDonald et al.(2007) proposed a model for classifying sentences and documents simultaneously." ></td>
	<td class="line x" title="44:199	They experimented with joint classification of subjectivity for sentence-level, and sentiment for document-level, and reported that their model obtained higher accuracy than the standard document classification model." ></td>
	<td class="line x" title="45:199	Although these pieces of work aim to predict not sentence-level but document-level sentiments, their concepts are similar to ours." ></td>
	<td class="line x" title="46:199	However, all the above methods require annotated corpora for all levels, such as both subjectivity for sentences and sentiments for documents, which are fairly expensive to obtain." ></td>
	<td class="line x" title="47:199	Although we also focus on two different layers, our method does not require such expensive labeled data." ></td>
	<td class="line x" title="48:199	What we require is just sentence-level labeled training data and a polarity dictionary of sentiment words." ></td>
	<td class="line x" title="49:199	3 Simple Voting by Sentiment Words One of the simplest ways to classify sentences using word-level polarities would be a majority voting, where the occurrences of positive words and those of negative words in the given sentence are counted and compared with each other." ></td>
	<td class="line x" title="50:199	However, this majority voting method has several weaknesses." ></td>
	<td class="line x" title="51:199	First, the majority voting cannot take into account at all the phenomenon that the word-level polarity is not always the same as the polarity of the sentence." ></td>
	<td class="line x" title="52:199	Consider the following example: I have not had any distortion problems with this phone and am more pleased with this phone than any Ive used before." ></td>
	<td class="line x" title="53:199	where negative words are underlined and positive words are double-underlined." ></td>
	<td class="line x" title="54:199	The example sentence has the positive polarity, though it locally contains negative words." ></td>
	<td class="line x" title="55:199	The majority voting would misclassify it because of the two negative words." ></td>
	<td class="line x" title="56:199	This kind of inconsistency between sentence-level polarity and word-level polarity often occurs and causes errors in the majority voting." ></td>
	<td class="line x" title="57:199	The reason is that the majority voting cannot take into account negation expressions or adversative conjunctions, e.g., I have not had any  in the example above." ></td>
	<td class="line x" title="58:199	Therefore, taking such polarity-shifting into account is important for classification of sentences using a polarity dictionary." ></td>
	<td class="line x" title="59:199	To circumvent this problem, Kennedy and Inkpen (2006) and Hu and Liu (2004) proposed to use a manually-constructed list of polarity-shifters." ></td>
	<td class="line x" title="60:199	However, it has limitations due to the diversity of expressions." ></td>
	<td class="line x" title="61:199	297 Another weakness of the majority voting is that it cannot be easily combined with existing methods that use the n-gram model or tree structures of the sentence as features." ></td>
	<td class="line x" title="62:199	The method we propose here can easily be combined with existing methods and show better performance." ></td>
	<td class="line x" title="63:199	4 Word-Level Polarity-Shifting Model We assume that when the polarity of a word is different from the polarity of the sentence, the polarity of the word is shifted by its context to adapt to the polarity of the sentence." ></td>
	<td class="line x" title="64:199	Capturing such polarityshifts will improve the classification performance of the majority voting classifier as well as of more sophisticated classifiers." ></td>
	<td class="line x" title="65:199	In this paper, we propose a word polarity-shifting model to capture such phenomena." ></td>
	<td class="line x" title="66:199	This model is a kind of binary classification model which determines whether the polarity is shifted by its context." ></td>
	<td class="line x" title="67:199	The model assigns a score sshift(x,S) to the sentiment word x in the sentence S. If the polarity of x is shifted in S, sshift(x,S) > 0." ></td>
	<td class="line x" title="68:199	If the polarity of x is not shifted in S, sshift(x,S)  0." ></td>
	<td class="line x" title="69:199	Let w be a parameter vector of the model and ` be a pre-defined feature function." ></td>
	<td class="line x" title="70:199	Function sshift is defined as sshift(x,S) = w`(x,S)." ></td>
	<td class="line x" title="71:199	(1) Since this model is a linear discriminative model, there are well-known algorithms to estimate the parameters of the model." ></td>
	<td class="line x" title="72:199	Usually, such models are trained with each occurrence of words as one instance (word-wise learning)." ></td>
	<td class="line x" title="73:199	However, we can train our model more effectively with each sentence being one instance (sentencewise learning)." ></td>
	<td class="line x" title="74:199	In this section, we describe how to train our model in two different ways and how to apply the model to a sentence classification." ></td>
	<td class="line x" title="75:199	4.1 Word-wise Learning In this learning method, we train the word-level polarity-shift model with each occurrence of sentiment words being an instance." ></td>
	<td class="line x" title="76:199	Training examples are automatically extracted by finding sentiment words in labeled sentences." ></td>
	<td class="line x" title="77:199	In the example of Section 3, for instance, both negative words (distortion or problems) and a positive word (pleased) appear in a positive sentence." ></td>
	<td class="line x" title="78:199	We regard distortion and problems, whose polarities are different from that of the sentence, as belonging to the polarityshifted class." ></td>
	<td class="line x" title="79:199	On the contrary, we regard pleased, whose polarity is the same as that of the sentence, as not belonging to polarity-shifted class." ></td>
	<td class="line x" title="80:199	We can use the majority voting by those (possibly polarity-shifted) sentiment words." ></td>
	<td class="line x" title="81:199	Specifically, we first classify each sentiment word in the sentence according to whether the polarity is shifted or not." ></td>
	<td class="line x" title="82:199	Then we use the majority voting to determine the polarity of the sentence." ></td>
	<td class="line x" title="83:199	If the first classifier classifies a positive word into the polarity-shifted class, we treat the word as a negative one." ></td>
	<td class="line x" title="84:199	We expect that the majority voting with polarity-shifting will outperform the simple majority voting without polarityshifting." ></td>
	<td class="line x" title="85:199	We actually use the weighted majority voting, where the polarity-shifting score for each sentiment word is used as the weight of the vote by the word." ></td>
	<td class="line x" title="86:199	We expect that the score works as a confidence measure." ></td>
	<td class="line x" title="87:199	We can formulate this method as follows." ></td>
	<td class="line x" title="88:199	Here, N and P are respectively defined as the sets of negative sentiment words and positive sentiment words." ></td>
	<td class="line x" title="89:199	For instance, x 2 N means that x is a negative word." ></td>
	<td class="line x" title="90:199	We also write x 2 S to express that the word x occurs in S. First, let us define two scores, scorep(S) and scoren(S), for the input sentence S. The scorep(S) and the scoren(S) respectively represent the number of votes for S being positive and the number of votes for S being negative." ></td>
	<td class="line x" title="91:199	If scorep(S) > scoren(S), we regard the sentence S as having the positive polarity, otherwise negative." ></td>
	<td class="line x" title="92:199	We suppose that the following relations hold for the scores: scorep(S) =X x2P\S sshift(x,S) + X x2N\S sshift(x,S), (2) scoren(S) =X x2P\S sshift(x,S) + X x2N\S sshift(x,S)." ></td>
	<td class="line x" title="93:199	(3) When either a polarity-unchanged positive word (sshift(x,S)  0) or a polarity-shifted negative word occurs in the sentence S, scorep(S) increases." ></td>
	<td class="line x" title="94:199	We can easily obtain the following relation between two scores: scorep(S) = scoren(S)." ></td>
	<td class="line x" title="95:199	(4) 298 Since, according to this relation, scorep(S) > scoren(S) is equivalent to scorep(S) > 0, we use only scorep(S) for the rest of this paper." ></td>
	<td class="line x" title="96:199	4.2 Sentence-wise Learning The equation (2) can be rewritten as scorep(S) = X x2S sshift(x,S)I(x) = X x2S w`(x,S)I(x) = w (X x2S `(x,S)I(x) ) , (5) where I(x) is the function defined as follows: I(x) = 8 >< >: +1 if x 2 N, 1 if x 2 P, 0 otherwise." ></td>
	<td class="line x" title="97:199	(6) This scorep(S) can also be seen as a linear discriminative model and the parameters of the model can be estimated directly (i.e., without carrying out wordwise learning)." ></td>
	<td class="line x" title="98:199	Each labeled sentence in a corpus can be used as a training instance for the model." ></td>
	<td class="line x" title="99:199	In this method, the model is learned so that the predictive ability for sentence classification is optimized, instead of the predictive ability for polarityshifting." ></td>
	<td class="line x" title="100:199	Therefore, this model can remain indecisive on the classification of word instances that have little contextual evidence about whether polarityshifting occurs or not." ></td>
	<td class="line x" title="101:199	The model can rely more heavily on word instances that have much evidence." ></td>
	<td class="line x" title="102:199	In contrast, the word-wise learning trains the model with all the sentiment words appearing in a corpus." ></td>
	<td class="line x" title="103:199	It is assumed here that all the sentiment words have relations with the sentence-level polarity, and that we can always find the evidence of the phenomena that the polarity of a word is different from that of a sentence." ></td>
	<td class="line x" title="104:199	Obviously, this assumption is not always correct." ></td>
	<td class="line x" title="105:199	As a result, the word-wise learning sometimes puts a large weight on a context word that is irrelevant to the polarity-shifting." ></td>
	<td class="line x" title="106:199	This might degrade the performance of sentence classification as well as of polarity-shifting." ></td>
	<td class="line x" title="107:199	4.3 Hybrid Model Both methods described in Sections 4.1 and 4.2 are to predict the sentence-level polarity only with the word-level polarity." ></td>
	<td class="line x" title="108:199	On the other hand, several methods that use another set of features, for example, bag-of-words, n-grams or dependency trees, were proposed for the sentence or document classification tasks." ></td>
	<td class="line x" title="109:199	We propose to combine our method with existing methods." ></td>
	<td class="line x" title="110:199	We refer to it as hybrid model." ></td>
	<td class="line x" title="111:199	In recent work, discriminative models including SVM are often used with many different features." ></td>
	<td class="line x" title="112:199	These methods are generally represented as score0p(X) = w0 `0(X), (7) where X indicates the target of classification, for example, a sentence or a document." ></td>
	<td class="line x" title="113:199	If score0p(X) > 0, X is classified into the target class." ></td>
	<td class="line x" title="114:199	`0(X) is a feature function." ></td>
	<td class="line x" title="115:199	When the method uses the bag-ofwords model, `0 maps X to a vector with each element corresponding to a word." ></td>
	<td class="line x" title="116:199	Here, we define new score function scorecomb(S) as a linear combination of scorep(S), the score function of our sentence-wise learning, and score0p(S), the score function of an existing method." ></td>
	<td class="line x" title="117:199	Using this, we can write the function as scorecomb(S) = scorep(S) + (1  )score0p(S) = X x2S w`(x,S)I(x) + (1  )w0 `0(S) = wcomb * X x2S `(x,S)I(x), (1  )`0(S) + ." ></td>
	<td class="line x" title="118:199	(8) Note that hi indicates the concatenation of two vectors, wcomb is defined as hw, w0i and  is a parameter which controls the influence of the word-level polarity-shifting model." ></td>
	<td class="line x" title="119:199	This model is also a discriminative model and we can estimate the parameters with a variety of algorithms including SVMs." ></td>
	<td class="line x" title="120:199	We can incorporate additional information like bagof-words or dependency trees by `0(S)." ></td>
	<td class="line x" title="121:199	4.4 Discussions on the Proposed Model Features such as n-grams or dependency trees can also capture some negations or polarity-shifters." ></td>
	<td class="line x" title="122:199	For example, although satisfy is positive, the bigram model will learn not satisfy as a feature correlated with negative polarity if it appears in the training data." ></td>
	<td class="line x" title="123:199	However, the bigram model cannot generalize the learned knowledge to other features such 299 Table 1: Statistics of the corpus customer movie # of Labeled Sentences 1,700 10,662 Available 1,436 9,492 # of Sentiment Words 3,276 26,493 Inconsistent Words 1,076 10,674 as not great or not disappoint." ></td>
	<td class="line x" title="124:199	On the other hand, our polarity-shifter model learns that the word not causes polarity-shifts." ></td>
	<td class="line x" title="125:199	Therefore, even if there was no not disappoint in training data, our model can determine that not disappoint has correlation with positive class, because the dictionary contains disappoint as a negative word." ></td>
	<td class="line x" title="126:199	For this reason, the polarity-shifting model can be learned even with smaller training data." ></td>
	<td class="line x" title="127:199	What we can obtain from the proposed method is not only a set of polarity-shifters." ></td>
	<td class="line x" title="128:199	We can also obtain the weight vector w, which indicates the strength of each polarity-shifter and is learned so that the predictive ability of sentence classification is optimized especially in the sentence-wise learning." ></td>
	<td class="line x" title="129:199	It is impossible to manually determine such weights for numerous features." ></td>
	<td class="line x" title="130:199	It is also worth noting that all the models proposed in this paper can be represented as a kernel function." ></td>
	<td class="line x" title="131:199	For example, the hybrid model can be seen as the following kernel: Kcomb(S1,S2) =  X xi2S1 X xj2S2 K((xi,S1),(xj,S2)) +(1  )K0(S1,S2)." ></td>
	<td class="line x" title="132:199	(9) Here, K means the kernel function between words and K0 means the kernel function between sentences respectively." ></td>
	<td class="line x" title="133:199	In addition,P xi P xjK((xi,S1),(xj,S2)) can be seen as an instance of convolution kernels, which was proposed by Haussler (1999)." ></td>
	<td class="line x" title="134:199	Convolution kernels are a general class of kernel functions which are calculated on the basis of kernels between substructures of inputs." ></td>
	<td class="line x" title="135:199	Our proposed kernel treats sentences as input, and treats sentiment words as substructures of sentences." ></td>
	<td class="line x" title="136:199	We can use high degree polynomial kernels as both K which is a kernel between substructures, i.e. sentiment words, of sentences, and K0 which is a kernel between sentences to make the classifiers take into consideration the combination of features." ></td>
	<td class="line x" title="137:199	5 Evaluation 5.1 Datasets We used two datasets, customer reviews 1 (Hu and Liu, 2004) and movie reviews 2 (Pang and Lee, 2005) to evaluate sentiment classification of sentences." ></td>
	<td class="line x" title="138:199	Both of these two datasets are often used for evaluation in sentiment analysis researches." ></td>
	<td class="line x" title="139:199	The number of examples and other statistics of the datasets are shown in Table 1." ></td>
	<td class="line x" title="140:199	Our method cannot be applied to sentences which contain no sentiment words." ></td>
	<td class="line x" title="141:199	We therefore eliminated such sentences from the datasets." ></td>
	<td class="line x" title="142:199	Available in Table 1 means the number of examples to which our method can be applied." ></td>
	<td class="line x" title="143:199	Sentiment Words shows the number of sentiment words that are found in the given sentences." ></td>
	<td class="line x" title="144:199	Please remember that sentiment words are defined as those words that are listed in a predefined polarity dictionary in this paper." ></td>
	<td class="line x" title="145:199	Inconsistent Words shows the number of the words whose polarities conflicted with the polarity of the sentence." ></td>
	<td class="line x" title="146:199	We performed 5-fold cross-validation and used the classification accuracy as the evaluation measure." ></td>
	<td class="line x" title="147:199	We extracted sentiment words from General Inquirer (Stone et al., 1996) and constructed a polarity dictionary." ></td>
	<td class="line x" title="148:199	After some preprocessing, the dictionary contains 2,084 positive words and 2,685 negative words." ></td>
	<td class="line x" title="149:199	5.2 Experimental Settings We employed the Max Margin Online Learning Algorithms for parameter estimation of the model (Crammer et al., 2006; McDonald et al., 2007)." ></td>
	<td class="line x" title="150:199	In preliminary experiments, this algorithm yielded equal or better results compared to SVMs." ></td>
	<td class="line x" title="151:199	As the feature representation, `(x,S), of polarity-shifting model, we used the local context of three words to the left and right of the target sentiment word." ></td>
	<td class="line x" title="152:199	We used the polynomial kernel of degree 2 for polarity-shifting model and the linear kernel for oth1http://www.cs.uic.edu/liub/FBS/FBS." ></td>
	<td class="line x" title="153:199	html 2http://www.cs.cornell.edu/people/pabo/ movie-review-data/ 300 Table 2: Experimental results of the sentence classification methods customer movie Baseline 0.638 0.504 BoW 0.790 0.724 2gram 0.809 0.756 3gram 0.800 0.762 Simple-Voting 0.716 0.624 Negation Voting 0.733 0.658 Word-wise 0.783 0.699 Sentence-wise 0.806 0.718 Hybrid BoW 0.827 0.748 Hybrid 2gram 0.840 0.755 Hybrid 3gram 0.837 0.758 Opt 0.840 0.770 ers, and feature vectors are normalized to 1." ></td>
	<td class="line x" title="154:199	In hybrid models, the feature vectors, Px2S `(x,S)I(x) and `0(S) are normalized respectively." ></td>
	<td class="line x" title="155:199	5.3 Comparison of the Methods We compared the following methods:  Baseline classifies all sentences as positive." ></td>
	<td class="line x" title="156:199	 BoW uses unigram features." ></td>
	<td class="line x" title="157:199	2gram uses unigrams and bigrams." ></td>
	<td class="line x" title="158:199	3gram uses unigrams, bigrams, and 3grams." ></td>
	<td class="line x" title="159:199	 Simple-Voting is the most simple majority voting with word-level polarity (Section 3)." ></td>
	<td class="line x" title="160:199	 Negation Voting proposed by Hu and Liu (2004) is the majority voting that takes negations into account." ></td>
	<td class="line x" title="161:199	As negations, we employed not, no, yet, never, none, nobody, nowhere, nothing, and neither, which are taken from (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2006; Hu and Liu, 2004) (Section 3)." ></td>
	<td class="line x" title="162:199	 Word-wise was described in Section 4.1." ></td>
	<td class="line x" title="163:199	 Sentence-wise was described in Section 4.2." ></td>
	<td class="line x" title="164:199	 Hybrid BoW, hybrid 2gram, hybrid 3gram are combinations of sentence-wise model and respectively BoW, 2gram and 3gram (Section 4.3)." ></td>
	<td class="line x" title="165:199	We set  = 0.5." ></td>
	<td class="line x" title="166:199	Table 2 shows the results of these experiments." ></td>
	<td class="line x" title="167:199	Hybrid 3gram, which corresponds to the proposed method, obtained the best accuracy on customer review dataset." ></td>
	<td class="line x" title="168:199	However, on movie review dataset, the proposed method did not outperform 3gram." ></td>
	<td class="line x" title="169:199	In Section 5.4, we will discuss this result in details." ></td>
	<td class="line x" title="170:199	Comparing word-wise to simple-voting, the accuracy increased by about 7 points." ></td>
	<td class="line x" title="171:199	This means that the polarity-shifting model can capture the polarityshifts and it is an important factor for sentiment classification." ></td>
	<td class="line x" title="172:199	In addition, we can see the effectiveness of sentence-wise, by comparing it to word-wise in accuracy." ></td>
	<td class="line x" title="173:199	Opt in Table 2 shows the results of hybrid models with optimal  and combination of models." ></td>
	<td class="line x" title="174:199	The optimal results of hybrid models achieved the best accuracy on both datasets." ></td>
	<td class="line x" title="175:199	We show some dominating polarity-shifters obtained through learning." ></td>
	<td class="line x" title="176:199	We obtained many negations (e.g., no, not, nt, never), modal verbs (e.g., might, would, may), prepositions (e.g., without, despite), comma with a conjunction (e.g., , but as in the case is strong and stylish, but lacks a window), and idiomatic expressions (e.g., hard resist as in it is hard to resist, and real snooze)." ></td>
	<td class="line x" title="177:199	5.4 Effect of Training Data Size When we have a large amount of training data, the ngram classifier can learn well whether each n-gram tends to appear in the positive class or the negative class." ></td>
	<td class="line x" title="178:199	However, when we have only a small amount of training data, the n-gram classifier cannot capture such tendency." ></td>
	<td class="line x" title="179:199	Therefore the external knowledge, such as word-level polarity, could be more valuable information for classification." ></td>
	<td class="line x" title="180:199	Thus it is expected that the sentence-wise model and the hybrid model will outperform n-gram classifier which does not take word-level polarity into account, more largely with few training data." ></td>
	<td class="line x" title="181:199	To verify this conjecture, we conducted experiments by changing the number of the training examples, i.e., the labeled sentences." ></td>
	<td class="line x" title="182:199	We evaluated three models: sentence-wise, 3gram model and hybrid 3gram on both customer review and movie review." ></td>
	<td class="line x" title="183:199	Figures 1 and 2 show the results on customer review and movie review respectively." ></td>
	<td class="line x" title="184:199	When the size of the training data is small, sentence-wise outper301 Figure 1: Experimental results on customer review Figure 2: Experimental results on movie review forms 3gram on both datasets." ></td>
	<td class="line x" title="185:199	We can also see that the advantage of sentence-wise becomes smaller as the amount of training data increases, and that the hybrid 3gram model almost always achieved the best accuracy among the three models." ></td>
	<td class="line x" title="186:199	Similar behaviour was observed when we ran the same experiments with 2gram or BoW model." ></td>
	<td class="line x" title="187:199	From these results, we can conclude that, as we expected above, the wordlevel polarity is especially effective when we have only a limited amount of training data, and that the hybrid model can combine two models effectively." ></td>
	<td class="line x" title="188:199	6 Conclusion We proposed a model that captures the polarityshifting of sentiment words in sentences." ></td>
	<td class="line x" title="189:199	We also presented two different learning methods for the model and proposed an augmented hybrid classifier that is based both on the model and on existing classifiers." ></td>
	<td class="line x" title="190:199	We evaluated our method and reported that the proposed method almost always improved the accuracy of sentence classification compared with other simpler methods." ></td>
	<td class="line x" title="191:199	The improvement was more significant when we have only a limited amount of training data." ></td>
	<td class="line x" title="192:199	For future work, we plan to explore new feature sets appropriate for our model." ></td>
	<td class="line x" title="193:199	The feature sets we used for evaluation in this paper are not necessarily optimal and we can expect a better performance by exploring appropriate features." ></td>
	<td class="line x" title="194:199	For example, dependency relations between words or appearances of conjunctions will be useful." ></td>
	<td class="line x" title="195:199	The position of a word in the given sentence is also an important factor in sentiment analysis (Taboada and Grieve, 2004)." ></td>
	<td class="line x" title="196:199	Furthermore, we should directly take into account the fact that some words do not affect the polarity of the sentence, though the proposed method tackled this problem indirectly." ></td>
	<td class="line x" title="197:199	We cannot avoid this problem to use word-level polarity more effectively." ></td>
	<td class="line x" title="198:199	Lastly, since we proposed a method for the sentence-level sentiment prediction, our next step is to extend the method to the document-level sentiment prediction." ></td>
	<td class="line x" title="199:199	Acknowledgement This research was supported in part by Overseas Advanced Educational Research Practice Support Program by Ministry of Education, Culture, Sports, Science and Technology." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="I08-1040
Unsupervised Classification of Sentiment and Objectivity in Chinese Text
Zagibalov, Taras;Carroll, John A.;"></td>
	<td class="line x" title="1:190	UnsupervisedClassificationofSentimentandObjectivity inChineseText TarasZagibalov JohnCarroll UniversityofSussex DepartmentofInformatics BrightonBN19QH,UK {T.Zagibalov,J.A.Carroll}@sussex.ac.uk Abstract We address the problem of sentiment and objectivity classification of product reviews in Chinese." ></td>
	<td class="line x" title="2:190	Our approach is distinctive in that it treats both positive / negative sentiment and subjectivity / objectivity not as distinct classes but rather as a continuum; we arguethatthis is desirablefrom the perspective of would-be customers who read the reviews." ></td>
	<td class="line x" title="3:190	We use novel unsupervised techniques, including a one-word 'seed' vocabulary and iterative retraining for sentiment processing, and a criterion of 'sentiment density' for determining the extent to which a document is opinionated." ></td>
	<td class="line x" title="4:190	The classifier achieves up to 87% F-measureforsentimentpolaritydetection." ></td>
	<td class="line oc" title="5:190	1 Introduction Automatic classification of sentiment has been a focus of a number of recent research efforts (e.g.(Turney, 2002; Pang et al., 2002; Dave at al., 2003)." ></td>
	<td class="line x" title="7:190	An important potential application of such work is in business intelligence: brands and company image are valuable property,so organizations want to know how they are viewed by the media (what the 'spin' is on news stories, and editorials), business analysts (as expressed in stock market reports), customers (for example on product review sites) and their own employees." ></td>
	<td class="line x" title="8:190	Another important application is to help people find out others' views about products they have purchased(e.g. consumer electronics), services and entertainment (e.g. movies), stocks and shares (from investor bulletin boards), and so on." ></td>
	<td class="line x" title="9:190	In the work reported in this paper we focus onproduct reviews, with the intended usersoftheprocessingbeingwould-becustomers." ></td>
	<td class="line x" title="10:190	Our approach is based on the insight that positive and negative sentiments are extreme points in a continuum of sentiment, and that intermediate points in this continuum are of potential interest." ></td>
	<td class="line x" title="11:190	For instance, in one scenario, someone might want to get an idea of the types of things people are saying about a particular product through reading a sample of reviews covering the spectrum from highly positive, through balanced, to highly negative." ></td>
	<td class="line x" title="12:190	(Wecall a review balanced if it is an opinionated text with an undecided or weak sentiment direction)." ></td>
	<td class="line x" title="13:190	In another scenario, a would-be customer might only be interested in reading balanced reviews, since they often present more reasoned arguments with fewer unsupported claims." ></td>
	<td class="line x" title="14:190	Such a person might therefore want to avoid reviews such asExample(1)writtenbyaChinesepurchaserof amobilephone(ourEnglishgloss)." ></td>
	<td class="line x" title="15:190	(1) q" ></td>
	<td class="line x" title="16:190	H HZ l " ></td>
	<td class="line x" title="17:190	Y9" ></td>
	<td class="line x" title="18:190	ml ^ y " ></td>
	<td class="line x" title="19:190	C`" ></td>
	<td class="line x" title="20:190	vrTQ" ></td>
	<td class="line x" title="21:190	 ^1lT9 ." ></td>
	<td class="line x" title="22:190	 Q1" ></td>
	<td class="line x" title="23:190	  P " ></td>
	<td class="line x" title="24:190	 " ></td>
	<td class="line x" title="25:190	q V Thesoftwareisbad,somesentSMSareneverreceivedbytheaddressee;compatibility isalsobad,onsomemobilephonesthereceivedmessagesareinascrambledencoding!Andsometimesthephone'dies'!Photos arehorrible!Itdoesn'thaveacyclicorpro304 grammablealarm-clock,youhavetosetit everytime,howcumbersome!Thebackcoverdoesnotfit!Theoriginalsoftwarehas manyholes!" ></td>
	<td class="line x" title="26:190	In a third scenario, someone might decide they would like only to read opinionated, weakly negative reviews such as Example (2), since these often contain good argumentation while still identifying themostsalientbadaspectsofaproduct." ></td>
	<td class="line x" title="27:190	(2) 0Q w" ></td>
	<td class="line x" title="28:190	A 1 ,#[/?l" ></td>
	<td class="line x" title="29:190	9 .1 .2" ></td>
	<td class="line x" title="30:190	 1{ .29zP" ></td>
	<td class="line x" title="31:190	HH
" ></td>
	<td class="line x" title="32:190	 'HW" ></td>
	<td class="line x" title="33:190	'0W+" ></td>
	<td class="line x" title="34:190	 q" ></td>
	<td class="line x" title="35:190	W" ></td>
	<td class="line x" title="36:190	9L" ></td>
	<td class="line x" title="37:190	 ^'L^9" ></td>
	<td class="line x" title="38:190	h." ></td>
	<td class="line x" title="39:190	Theresponsetimeofthismobileisvery long,MMSshouldbelessthan30kbonlyto bedownloaded,alsoitdoesn'tsupportMP3 ringtones,(while)thebuilt-intunesarenot good,andfromtimetotimeit'dies',but whenIwasbuyingitIreallylikedit:very original,verynicelymatchingredandwhite colours,ithasitsindividuality,alsoit'snot expensive,butwhenuseditalwayscauses trouble,makesone'sheadache The review contains both positive and negative sentiment coveringdifferentaspects oftheproduct, and the fact that it contains a balance of views means that it is likely to be useful for a would-be customer." ></td>
	<td class="line x" title="40:190	Moving beyond review classification, more advanced tasks such as automatic summarization of reviews (e.g. Feiguina & LaPalme, 2007) might also benefit from techniques which could distinguish more shades of sentiment than justabinarypositive/negativedistinction." ></td>
	<td class="line x" title="41:190	A second dimension, orthogonal to positive / negative, is opinionated / unopinionated (or equivalently subjective / objective)." ></td>
	<td class="line x" title="42:190	When shopping for a product, one might be interested in the physical characteristics of the product or what features the product has, rather than opinions about how well these features work or about how well the product as a whole functions." ></td>
	<td class="line x" title="43:190	Thus, if one is looking for a review that contains more factual information than opinion, one might be interested in reviews like Example(3)." ></td>
	<td class="line x" title="44:190	(3) 9$p" ></td>
	<td class="line x" title="45:190	L" ></td>
	<td class="line x" title="46:190	 7" ></td>
	<td class="line x" title="47:190	1" ></td>
	<td class="line x" title="48:190	9 " ></td>
	<td class="line x" title="49:190	 H" ></td>
	<td class="line x" title="50:190	 " ></td>
	<td class="line x" title="51:190	 '" ></td>
	<td class="line x" title="52:190	|AU" ></td>
	<td class="line x" title="53:190	HWy " ></td>
	<td class="line x" title="54:190	8'1 
" ></td>
	<td class="line x" title="55:190	V" ></td>
	<td class="line x" title="56:190	:Y'b (My)overallfeelingaboutthismobileisnot bad,itfeatures:5alarm-clocksthatswitch thephoneon(off),phonebookfor800items (500people),lunarandsolarcalendars, fastswitchingbetweentimeanddatemodes, WAPnetworking,organizer,notebookand soon." ></td>
	<td class="line x" title="57:190	This review is mostly neutral (unopinionated), but contains information that could be useful to a would-be customer which might not be in a product specification document, e.g. fast switching between different operating modes." ></td>
	<td class="line x" title="58:190	Similarly,wouldbe customers might be interested in retrieving completely unopinionated documents such as technical descriptions and usermanuals.Again, aswith sentiment classification, we argue that opinionated and unopinionated texts are not easily distinguishable separate sets, but form a continuum." ></td>
	<td class="line x" title="59:190	In this continuum, intermediate points are of interest as wellastheextremes." ></td>
	<td class="line x" title="60:190	A major obstacle for automatic classification of sentiment and objectivity is lack of training data, which limits the applicability of approaches based on supervised machine learning." ></td>
	<td class="line x" title="61:190	With the rapid growth in textual data and the emergence of new domains of knowledge it is virtually impossible to maintain corpora of tagged data that cover all  or even most  areas of interest." ></td>
	<td class="line x" title="62:190	The cost of manual taggingalsoaddstotheproblem.Reusingthesame corpus for training classifiers for new domains is also not effective: several studies report decreased accuracy in cross-domain classification (Engstrm, 2004; Aue & Gamon, 2005) a similar problem has also been observed in classification of documents createdoverdifferenttimeperiods(Read,2005)." ></td>
	<td class="line x" title="63:190	Inthispaperwedescribeanunsupervised classification technique which is able to build its own sentiment vocabulary starting from a very small seed vocabulary, using iterative retraining to enlarge the vocabulary.In order to avoid problems of domain dependence, the vocabulary is built using textfrom the samesourceas thetextwhich isto be classified." ></td>
	<td class="line x" title="64:190	In this paper we work with Chinese, but using a very small seed vocabulary may mean that this approach would in principle need very little linguistic adjustment to be applied to a different 305 language." ></td>
	<td class="line x" title="65:190	Written Chinese has some specific features, one of which is the absence of explicitly markedwordboundaries,whichmakesword-based processing problematic." ></td>
	<td class="line x" title="66:190	In keeping with our unsupervised, knowledge-poor approach, we do not use any preliminary word segmentation tools or higher levelgrammaticalanalysis." ></td>
	<td class="line x" title="67:190	The paper is structured as follows." ></td>
	<td class="line x" title="68:190	Section 2 reviews related work in sentiment classification and more generally in unsupervised training of classifiers." ></td>
	<td class="line x" title="69:190	Section 3 describes our datasets, and Section 4 the techniques we use for unsupervised classification and iterative retraining." ></td>
	<td class="line x" title="70:190	Sections 5 and 6 describe a number of experiments into how well the approacheswork,andSection7concludes." ></td>
	<td class="line oc" title="71:190	2 RelatedWork 2.1 Sentiment Classification Most previous work on the problem of categorizing opinionated texts has focused on the binary classification of positive and negative sentiment (Turney, 2002; Pang et al., 2002; Dave at al., 2003)." ></td>
	<td class="line x" title="72:190	However, Pang & Lee (2005) describe an approach closer to ours in which they determine an author's evaluation with respect to a multi-point scale, similar to the 'five-star' sentiment scale widely used on review sites." ></td>
	<td class="line x" title="73:190	However, authors of reviews are inconsistent in assigning fine-grained ratings and quite often star systems are not consistent between critics." ></td>
	<td class="line x" title="74:190	This makes their approach very author-dependent." ></td>
	<td class="line x" title="75:190	The main differences are that Pang and Lee use discrete classes (although more than two), not a continuum as in our approach, and use supervisedmachine learningrather than unsupervised techniques." ></td>
	<td class="line x" title="76:190	A similar approach was adopted by Hagedorn et al.(2007), applied to news stories: they defined five classes encoding sentiment intensity and trained their classifier on a manually tagged training corpus." ></td>
	<td class="line x" title="78:190	They note that world knowledge is necessary for accurate classificationinsuchopen-endeddomains." ></td>
	<td class="line x" title="79:190	There has also been previous work on determining whether a given text is factual or expresses opinion (Yu& Hatzivassiloglu, 2003; Pang & Lee, 2004); again this work uses a binary distinction, and supervised rather than unsupervised approaches." ></td>
	<td class="line x" title="80:190	Recent work on classification of terms with respect to opinion (Esuli & Sebastiani, 2006) uses a three-category system to characterize the opinionrelated properties of word meanings, assigning numerical scores to Positive, Negative and Objective categories." ></td>
	<td class="line x" title="81:190	The visualization of these scores somewhat resembles our graphs in Section 5, although we use two orthogonal scales rather than three categories; we are also concerned with classification ofdocumentsratherthanterms." ></td>
	<td class="line x" title="82:190	2.2 Unsupervised Classification Abney (2002) compares two major kinds of unsupervised approachto classification (co-training and the Yarowskyalgorithm)." ></td>
	<td class="line x" title="83:190	As we do not use multiple classifiers our approach is quite far from cotraining." ></td>
	<td class="line x" title="84:190	But it is close to the paradigm described by Yarowsky (1995) and Turney (2002) as it also employs self-training based on a relatively small seed data set which is incrementally enlarged with unlabelled samples." ></td>
	<td class="line x" title="85:190	But our approach does not use point-wise mutual information." ></td>
	<td class="line x" title="86:190	Instead we use relative frequencies of newly found features in a training subcorpus produced by the previous iteration of the classifier." ></td>
	<td class="line x" title="87:190	Wealso use the smallest possible seed vocabulary, containing just a single word; however there are no restrictions regarding the maximum number of items in the seed vocabulary." ></td>
	<td class="line x" title="88:190	3 Data 3.1 Seed Vocabulary Our approach starts out with a seed vocabulary consisting of a single word, z (good)." ></td>
	<td class="line x" title="89:190	This word is tagged as a positive vocabulary item; initially there are no negative items." ></td>
	<td class="line x" title="90:190	The choice of word was arbitrary, and other words with strongly positive or negative meaning would also be plausible seeds." ></td>
	<td class="line x" title="91:190	Indeed, z might not be the best possible seed, as it is relatively ambiguous: in some contexts it means to like or acts as the adverbial very, and is often used as part of other words (although usually contributing a positive meaning)." ></td>
	<td class="line x" title="92:190	But since it is one of the most frequent units in the Chinese language, it is likely to occur in a relatively large number of reviews, which is important for the rapidgrowthofthevocabularylist." ></td>
	<td class="line x" title="93:190	3.2 TestCorpus Our test corpus is derived from product reviews harvested from the website IT1681." ></td>
	<td class="line x" title="94:190	All the reviews were tagged by their authors as either positive or negative overall." ></td>
	<td class="line x" title="95:190	Most reviews consist of two or three distinct parts: positive opinions, negative opinions, and comments ('other')  although some 1http://product.it168.com 306 reviews have only one part." ></td>
	<td class="line x" title="96:190	Weremoved duplicate reviews automatically using approximate matching, giving a corpus of 29531 reviews of which 23122 are positive (78%) and 6409 are negative (22%)." ></td>
	<td class="line x" title="97:190	The total number of different products in the corpus is 10631, the number of product categories is 255, and most of the reviewed products are either software products or consumer electronics." ></td>
	<td class="line x" title="98:190	Unfortunately, it appears that some users misused the sentiment tagging facility on the website so quite a lot of reviews have incorrect tags." ></td>
	<td class="line x" title="99:190	However, the parts of the reviews are much more reliably identified as being positive or negative so we usedtheseastheitemsofthetestcorpus.Intheexperiments described in this paper we used 2317 reviews of mobile phones of which 1158 are negative and 1159 are positive." ></td>
	<td class="line x" title="100:190	Thus random choice would have approximately 50% accuracy if all itemsweretaggedeitherasnegativeorpositive2." ></td>
	<td class="line x" title="101:190	4 Method 4.1 Sentiment Classification As discussed in Section 1, we do not carry out any word segmentation or grammatical processing of input documents." ></td>
	<td class="line x" title="102:190	We use a very broad notion of words (or phrases) in the Chinese language." ></td>
	<td class="line x" title="103:190	The basic units of processing are 'lexical items', each of which is a sequence of one or more Chinese characters excluding punctuation marks (which may actually form part of a word, a whole word or a sequence of words), and `zones', each of which is a sequence of characters delimited by punctuation marks." ></td>
	<td class="line x" title="104:190	Each zone is classified as either positive or negative based whether positive or negative vocabulary items predominate." ></td>
	<td class="line x" title="105:190	In more detail, a simple maximum match algorithm is used to find all lexical items (character sequences) in the zone that are in the vocabulary list." ></td>
	<td class="line x" title="106:190	As there are two parts of the vocabulary (positiveand negative), wecorrespondinglycalculatetwoscoresusingEquation(1)3, Si= LdL phrase Sd Nd (1) where Ld is the length in characters of a matching lexical item, Lphrase is the length of the current zone 2Thiscorpusispubliclyavailableathttp://www.informatics." ></td>
	<td class="line x" title="107:190	sussex.ac.uk/users/tz21/it168test.zip 3Inthefirstiteration,whenwehaveonlyoneiteminthevocabulary,negativezonesarefoundbymeansofthenegation check(sonot+good=negativeitem)." ></td>
	<td class="line x" title="108:190	in characters, Sd is the current sentiment score of the matching lexical item (initially1.0), and Nd is a negation check coefficient.Thenegation check is a regular expression which determines if the lexical item is preceded by a negation within its enclosing zone." ></td>
	<td class="line x" title="109:190	If a negation is found then Nd is set to 1." ></td>
	<td class="line x" title="110:190	The check looks for six frequently occurring negations:(bu),  (buhui),  (meiyou),  (baituo),  (mianqu),and E  (bimian)." ></td>
	<td class="line x" title="111:190	The sentiment score of a zone is the sum of sentiment scores of all the items found in it." ></td>
	<td class="line x" title="112:190	In fact there are twocompeting sentiment scores for every zone: one positive (the sum of all scores of items found in the positive part of the vocabulary list) and one negative (the sum of the scores for the items in the negative part)." ></td>
	<td class="line x" title="113:190	The sentiment direction of a zone is determined from the maximum of the absolutevaluesofthetwocompetingscoresforthe zone." ></td>
	<td class="line x" title="114:190	This procedure is applied to all zones in a document, classifying each zone as positive, negative, or neither (in cases where there are no positive or negative vocabulary items in the zone)." ></td>
	<td class="line x" title="115:190	To determine the sentiment direction of the whole document, the classifier computes the difference between the number of positive and negative zones." ></td>
	<td class="line x" title="116:190	If the result is greater than zero the document is classifiedas positive,andviceversa.Iftheresultis zero the document is balanced or neutral for sentiment." ></td>
	<td class="line x" title="117:190	4.2 IterativeRetraining The task of iterative retraining is to enlargethe initial seed vocabulary (consisting of a single wordas discussed in Section 3.1) into a comprehensive vocabulary list of sentiment-bearing lexical items." ></td>
	<td class="line x" title="118:190	In each iteration, the current version of the classifier isrunontheproductreviewcorpustoclassifyeach document, resulting in a training subcorpus of positive and a negative documents." ></td>
	<td class="line x" title="119:190	The subcorpus is used to adjust the scores of existing positive and negative vocabulary items and to find new itemsto beincludedinthevocabulary." ></td>
	<td class="line x" title="120:190	Eachlexicalitemthatoccursatleasttwiceinthe corpus is a candidate for inclusion in the vocabulary list." ></td>
	<td class="line x" title="121:190	After candidate items are found, the system calculates their relative frequencies in both the positive and negative parts of the current training subcorpus." ></td>
	<td class="line x" title="122:190	The system also checks for negation whilecountingoccurrences:ifalexicalitemispreceded by a negation, its count is reduced by one." ></td>
	<td class="line x" title="123:190	This results in negative counts (and thus negative relativefrequenciesandscores)forthoseitemsthat 307 are usually used with negation; for example,     (thequalityisfartoobad)isinthe positive part of the vocabulary with a score of 1.70." ></td>
	<td class="line x" title="124:190	This meansthattheitemwasfoundinreviewsclassified by the system as positive but it was preceded by a negation." ></td>
	<td class="line x" title="125:190	If during classification this item is found in a document it will reduce the positive score for that document (as it is in the positive part of the vocabulary), unlessthe item is preceded bya negation." ></td>
	<td class="line x" title="126:190	In this situation the score will be reversed (multiplied by 1), and the positive score will be increasedseeEquation(1)above." ></td>
	<td class="line x" title="127:190	Forallcandidateitemswecomparetheirrelative frequencies in the positive and negative documents inthesubcorpususingEquation(2)." ></td>
	<td class="line x" title="128:190	difference= FpFnF pFn/2 (2) If difference < 1, then the frequencies are similar and the item does not have enough distinguishing power,so it is not included in the vocabulary.Otherwise the the sentiment score of the item is (re-) calculated  according to Equation (3) for positive items,andanalogouslyfornegativeitems." ></td>
	<td class="line x" title="129:190	Fp FpFn (3) Finally, the adjusted vocabulary list with the new scoresisreadyforthenextiteration." ></td>
	<td class="line x" title="130:190	4.3 ObjectivityClassification Given a sentiment classification for each zone in a document, we compute sentiment density as the proportion of opinionated zones with respect to the total number of zones in the document." ></td>
	<td class="line x" title="131:190	Sentiment densitymeasurestheproportionofopinionatedtext in a document, and thus the degree to which the documentasawholeisopinionated." ></td>
	<td class="line x" title="132:190	It should be noted that neither sentiment score nor sentiment density are absolute values, but are relative and only valid for comparing one document with other." ></td>
	<td class="line x" title="133:190	Thus, a sentiment density of 0.5 does not mean that the review is half opinionated, half not." ></td>
	<td class="line x" title="134:190	It means that the review is less opinionatedthanareviewwithdensity0.9." ></td>
	<td class="line x" title="135:190	5 Experiments We ran the system on the product review corpus (Section 3.2) for 20iterations." ></td>
	<td class="line x" title="136:190	Theresults for binary sentiment classification are shown in Table 1." ></td>
	<td class="line x" title="137:190	Wesee increasing F-measure up to iteration 18, after which both precision and recall start to descrease; we therefore use the version of the classifier as it stood after iteration 184." ></td>
	<td class="line x" title="138:190	These figures are only indicative of the classification accuracy of the system." ></td>
	<td class="line x" title="139:190	Accuracy might be lower for unseen text, although since our approach is unsupervised we could in principle perform further retraining iterations on any sample of new text to tune the vocabularylisttoit." ></td>
	<td class="line x" title="140:190	We also computed a (strong) baseline, using as the vocabulary list the NTU Sentiment Dictionary (Ku et al., 2006)5 which is intended to contain only sentiment-related words and phrases." ></td>
	<td class="line x" title="141:190	We assigned each positive and negative vocabulary item a score of 1 or 1 respectively." ></td>
	<td class="line x" title="142:190	This setup achieved 87.77 precision and 77.09 recall on the product review corpus." ></td>
	<td class="line x" title="143:190	InSection1wearguedthatsentimentandobjectivityshouldbothbeconsideredascontinuums,not Table1.Resultsforbinarysentimentclassificationduringiterativeretraining." ></td>
	<td class="line x" title="144:190	4Thesizeofthesentimentvocabularyafteriteration18was 22530(13462positiveand9068negative)." ></td>
	<td class="line x" title="145:190	5Kuetal.automaticallygeneratedthedictionarybyenlarging aninitialmanuallycreatedseedvocabularybyconsultingtwo thesauri,includingtong2yi4ci2ci2lin2andthe AcademiaSinicaBilingualOntologicalWordNet3." ></td>
	<td class="line x" title="146:190	Iteration Precision Recall F-measure 1 77.62 28.43 41.62 2 76.15 73.81 74.96 3 81.15 80.07 80.61 4 83.54 82.79 83.16 5 84.66 83.78 84.22 6 85.51 84.77 85.14 7 86.59 85.76 86.17 8 86.78 86.11 86.44 9 87.15 86.32 86.74 10 87.01 86.37 86.69 11 86.9 86.15 86.53 12 87.05 86.41 86.73 13 86.87 86.19 86.53 14 87.35 86.67 87.01 15 87.13 86.45 86.79 16 87.14 86.5 86.82 17 86.8 86.24 86.52 18 87.57 86.89 87.22 19 87.23 86.67 86.95 20 87.18 86.54 86.86 308 binary distinctions." ></td>
	<td class="line x" title="147:190	Section 4.1 describes how our approach compares the number of positive and negativezonesforadocumentandtreatsthedifference as a measure of the 'positivity' or 'negativity' of a review.The document in Example(2), with 12 zones, is assigned a score of 1 (the least negative score possible): the review contains some positive sentiment but the overall sentiment direction of the review is negative." ></td>
	<td class="line x" title="148:190	In contrast, Example (1) is identified as a highly negative review,as would be expected,withascoreof8,from11zones." ></td>
	<td class="line x" title="149:190	Similarly, with regard to objectivity, the sentiment density of the text in Example (3) is 0.53, which reflects its more factual character compared to Example (1), which has a score of 0.91." ></td>
	<td class="line x" title="150:190	Wecan represent sentiment and objectivity on the followingscales: Negative Balanced Positive Unopinionated Neutral Opinionated The scales are orthogonal, so we can combine themintoasinglecoordinatesystem: Opinionated Negative Positive We would expect most product reviews to be placedtowardsthetopofthethecoordinatesystem (i.e.opinionated),andstretchfromlefttoright." ></td>
	<td class="line x" title="151:190	Figure1plotstheresultsofsentimentandobjectivityclassificationofthetestcorpusinthistwodimensional coordinate system, where X represents sentiment (with scores scaled with respect to the number of zones so that 100 is the most negative possible and +100 the most positive), and Y represents sentiment density (0 being unopinionatedand 1beinghighlyopinionated)." ></td>
	<td class="line x" title="152:190	Most of the reviewsare located in the upper part of the coordinate system, indicating that they have been classified as opinionated, with either positive or negative sentiment direction." ></td>
	<td class="line x" title="153:190	Looking at the overall shape of the plot, more opinionated documents tend to have more explicit sentiment direction, while less opinionated texts stay closer to the balanced/neutralregion(aroundX=0)." ></td>
	<td class="line x" title="154:190	Figure1.Reviewsclassifiedaccordingto sentiment(Xaxis)anddegreeof opinionation(Yaxis)." ></td>
	<td class="line x" title="155:190	6 Discussion As can be seen in Figure 1, the classifier managed to map the reviews onto the coordinate system." ></td>
	<td class="line x" title="156:190	However, there are very few points in the neutral region, that is, on the same X = 0 line as balanced but with low sentiment density." ></td>
	<td class="line x" title="157:190	By inspection, we know that there are neutral reviews in our data set." ></td>
	<td class="line x" title="158:190	Wetherefore conducted a further experiment to investigate what the problem might be." ></td>
	<td class="line x" title="159:190	We took Wikipedia6 articles written in Chinese on mobile telephony and related issues, as well as several articles about the technology,the market and the history of mobile telecommunications, and split them into small parts (about a paragraph long, to make their size close to the size of the reviews) resulting in a corpus of 115documents, which we assume to be mostly unopinionated." ></td>
	<td class="line x" title="160:190	Weprocessed these documents with the trained classifier and found that they were mapped almost exactly where balanced documentsshouldbe(seeFigure2)." ></td>
	<td class="line x" title="161:190	Most of these documents have weak sentiment direction (X = 5 to +10), but are classified as relatively opinionated (Y > 0.5)." ></td>
	<td class="line x" title="162:190	The former is to be expected, whereas the latter is not." ></td>
	<td class="line x" title="163:190	When investigatingthepossiblereasonsforthisbehaviorwenoticed that the classifier found not only feature descriptions (like mz nice touch) or expressions which describe attitude ( (one) like(s)), butalsoproductfeatures(forexample,  MMS or jTV) to be opinionated." ></td>
	<td class="line x" title="164:190	This is because the presence of some advanced features such as MMS inmobilephonesisoftenregardedasapositiveby 6www.wikipedia.org -40 -30 -20 -10 0 10 20 30 40 50 60 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 309 Figure2.Classificationofasampleofarticles fromWikipedia." ></td>
	<td class="line x" title="165:190	Figure3.Classificationofasampleofarticles fromWikipedia,usingtheNTUSentiment Dictionaryasthevocabularylist." ></td>
	<td class="line x" title="166:190	authors of reviews." ></td>
	<td class="line x" title="167:190	In addition, the classifier found words that were used in reviews to describe situations connected with a product and its features: for example,  (service)wasoftenusedindescriptionsofquiteunpleasantsituationswhenauserhad toturntoamanufacturer'spost-salesserviceforrepairorreplacementofa malfunctioningphone,and  (user) was often used to describe what one candowithsomeadvancedfeatures.Thustheclassifier was able to capture some product-specific as well as market-specific sentiment markers, however, it was not able to distinguish the context these generally objective words were used in." ></td>
	<td class="line x" title="168:190	This resulted in relatively high sentiment density of neutral texts which contained these words but used in othertypesofcontext." ></td>
	<td class="line x" title="169:190	To verify this hypothesis we applied the same processing to our corpus derived from Wikipedia articles, but using as the vocabulary list the NTU Sentiment Dictionary." ></td>
	<td class="line x" title="170:190	The results (Figure 3) show that most of the neutral texts are now mapped to the lower part of the opinionation scale (Y < 0.5), as expected." ></td>
	<td class="line x" title="171:190	Therefore, to successfully distinguish between balanced reviewsand neutral documents a classifier should be able to detect when product features are used as sentiment markers and when theyarenot." ></td>
	<td class="line x" title="172:190	7 Conclusionsand FutureWork Wehave described an approach to classification of documents with respect to sentiment polarity and objectivity, representing both as a continuum, and mapping classified documents onto a coordinate system that also represents the difference between balanced and neutral text." ></td>
	<td class="line x" title="173:190	We have presented a novel, unsupervised, iterative retraining procedure for deriving the classifier, starting from the most minimal size seed vocabulary, in conjunction with a simple negation check." ></td>
	<td class="line x" title="174:190	Wehave verified that the approach produces reasonable results." ></td>
	<td class="line x" title="175:190	The approach is extremely minimal in terms of language processing technology, giving it good possibilities for porting to different genres, domains and languages." ></td>
	<td class="line x" title="176:190	We also found that the accuracy of the method depends a lot on the seed word chosen." ></td>
	<td class="line x" title="177:190	If the word has a relatively low frequency or does not have a definite sentiment-related meaning, the results may be very poor.For example,an antonymous wordto (good) in Chinese is(bad), but the latter is notafrequentword:theChineseprefertosay (not good)." ></td>
	<td class="line x" title="178:190	When this word was used as the seed word, accuracy was little more than 15%." ></td>
	<td class="line x" title="179:190	Although the first iteration produced high precision (82%), the size of the extracted subcorpus was only 24 items, resulting in the system being unable to produce a good classifier for the following iterations." ></td>
	<td class="line x" title="180:190	Every new iteration produced an even poorer result as each new extracted corpus was of lower accuracy." ></td>
	<td class="line x" title="181:190	On the other hand, it seems that a seed list consisting of several low-frequency one-character words can compensate each other and produce better results by capturing a larger part of the corpus (thus increasing recall)." ></td>
	<td class="line x" title="182:190	Nevertheless a single word may also produce results even better than those for multiword seed lists." ></td>
	<td class="line x" title="183:190	For example, the two-character wordM((comfortable) as seed reached 91% -40 -30 -20 -10 0 10 20 30 40 50 60 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 -40 -30 -20 -10 0 10 20 30 40 50 60 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 310 accuracy with 90% recall." ></td>
	<td class="line x" title="184:190	We can conclude that our method relies on the quality of the seed word." ></td>
	<td class="line x" title="185:190	Wetherefore need to investigate ways of choosing 'lucky'seedsandavoiding'unlucky'ones." ></td>
	<td class="line x" title="186:190	Future work should also focus on improving classification accuracy: adding a little languagespecific knowledge to be able to detect some word boundariesshouldhelp;wealsoplantoexperiment with more sophisticated methods of sentiment score calculation." ></td>
	<td class="line x" title="187:190	In addition, the notion of 'zone' needs refining and language-specific adjustments (for example, a 'reversed comma' should not be considered to be a zone boundary marker, since thispunctuationmarkisgenerallyusedfortheenumerationofrelatedobjects)." ></td>
	<td class="line x" title="188:190	More experiments are also necessary to determine how the approach works across domains, and further investigation into methods for distinguishingbetweenbalancedandneutraltext." ></td>
	<td class="line x" title="189:190	Finally, we need to produce a new corpus that would enable us to evaluate the performance of a pre-trained version of the classifier that did not have any prior access to the documents it was classifying: we need the reviews to be tagged not in a binary way as they are now, but in a way that reflects the two continuums we use (sentiment and objectivity)." ></td>
	<td class="line x" title="190:190	Acknowledgements The first author is supported by the Ford FoundationInternationalFellowshipsProgram." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="I08-1041
Using Roget's Thesaurus for Fine-grained Emotion Recognition
Aman, Saima;Szpakowicz, Stan;"></td>
	<td class="line x" title="1:138	Using Rogets Thesaurus for Fine-grained Emotion Recognition Saima Aman School of Information Technology and Engineering University of Ottawa, Ottawa, Canada   saman071@site.uottawa.ca Stan Szpakowicz School of Information Technology and Engineering University of Ottawa, Ottawa, Canada ICS, Polish Academy of Sciences Warszawa, Poland szpak@site.uottawa.ca   Abstract Recognizing the emotive meaning of text can add another dimension to the understanding of text." ></td>
	<td class="line x" title="2:138	We study the task of automatically categorizing sentences in a text into Ekmans six basic emotion categories." ></td>
	<td class="line x" title="3:138	We experiment with corpus-based features as well as features derived from two emotion lexicons." ></td>
	<td class="line x" title="4:138	One lexicon is automatically built using the classification system of Rogets Thesaurus, while the other consists of words extracted from WordNet-Affect." ></td>
	<td class="line x" title="5:138	Experiments on the data obtained from blogs show that a combination of corpus-based unigram features with emotion-related features provides superior classification performance." ></td>
	<td class="line x" title="6:138	We achieve Fmeasure values that outperform the rulebased baseline method for all emotion classes." ></td>
	<td class="line x" title="7:138	1 Introduction Recognizing emotions conveyed by a text can provide an insight into the authors intent and sentiment, and can lead to better understanding of the texts content." ></td>
	<td class="line x" title="8:138	Emotion recognition in text has recently attracted increased attention of the NLP community (Alm et al., 2005; Liu et al, 2003; Mihalcea and Liu, 2006); it is also one of the tasks at Semeval-2007 1 . Automatic recognition of emotions can be applied in the development of affective interfaces for  1  Affective Text: Semeval Task at the 4th International Workshop on Semantic Evaluations, 2007, Prague (nlp.cs.swarthmore.edu/semeval/tasks/task14/summary.shtml)." ></td>
	<td class="line x" title="9:138	Computer-Mediated Communication and HumanComputer Interaction." ></td>
	<td class="line x" title="10:138	Other areas that can potentially benefit from automatic emotion analysis are personality modeling and profiling (Liu and Maes, 2004), affective interfaces and communication systems (Liu et al, 2003; Neviarouskaya et al., 2007a) consumer feedback analysis, affective tutoring in e-learning systems (Zhang et al., 2006), and textto-speech synthesis (Alm et al., 2005)." ></td>
	<td class="line x" title="11:138	In this study, we address the task of automatically assigning an emotion label to each sentence in the given dataset, indicating the predominant emotion type expressed in the sentence." ></td>
	<td class="line x" title="12:138	The possible labels are happiness, sadness, anger, disgust, surprise, fear and no-emotion." ></td>
	<td class="line x" title="13:138	Those are Ekmans (1992) six basic emotion categories, and an additional label to account for the absence of a clearly discernible emotion." ></td>
	<td class="line x" title="14:138	We experiment with two types of features for representing text in emotion classification based on machine learning (ML)." ></td>
	<td class="line x" title="15:138	Features of the first type are a corpus-based unigram representation of text." ></td>
	<td class="line x" title="16:138	Features of the second type comprise words that appear in emotion lexicons." ></td>
	<td class="line x" title="17:138	One such lexicon consists of words that we automatically extracted from Rogets Thesaurus (1852)." ></td>
	<td class="line x" title="18:138	We chose words for their semantic similarity to a basic set of terms that represent each emotion category." ></td>
	<td class="line x" title="19:138	Another lexicon builds on lists of words for each emotion category, extracted from WordNet-Affect (Strapparava and Valitutti, 2004)." ></td>
	<td class="line x" title="20:138	We compare the classification results for groups of features of these two types." ></td>
	<td class="line x" title="21:138	We get good results when the features are combined in a series of ML experiments." ></td>
	<td class="line x" title="22:138	312 2 Related Work Research in emotion recognition has focused on discerning emotions along the dimensions of valence (positive / negative) and arousal (calm / excited), and on recognizing distinct emotion categories." ></td>
	<td class="line x" title="23:138	We focus on the latter." ></td>
	<td class="line x" title="24:138	Liu et al.(2003) use a real-world commonsense knowledge base to classify sentences into Ekmans (1992) basic emotion categories." ></td>
	<td class="line x" title="26:138	They use an ensemble of rule-based affect models to determine the emotional affinity of individual sentences." ></td>
	<td class="line x" title="27:138	Neviarouskaya et al.(2007b) also use rules to determine the emotions in sentences in blog posts; their analysis relies on a manually prepared database of words, abbreviations and emoticons labeled with emotion categories." ></td>
	<td class="line x" title="29:138	Since these papers do not report conventional performance metrics such as precision and recall, the effectiveness of their methods cannot be judged empirically." ></td>
	<td class="line x" title="30:138	They also disregard statistical learning methods as ineffective for emotion recognition at sentence level." ></td>
	<td class="line x" title="31:138	They surmise that the small size of the text input (a sentence) gives insufficient data for statistical analysis, and that statistical methods cannot handle negation." ></td>
	<td class="line x" title="32:138	In this paper, we show that ML-based approach with the appropriate combination of features can be applied to distinguishing emotions in text." ></td>
	<td class="line x" title="33:138	Previous work has used lexical resources such as WordNet to automatically acquire emotion-related words for emotion classification experiments." ></td>
	<td class="line x" title="34:138	Starting from a set of primary emotion adjectives, Alm et al.(2005) retrieve similar words from WordNet utilizing all senses of all words in the synsets that contain the adjectives." ></td>
	<td class="line x" title="36:138	They also exploit the synonym and hyponym relations in WordNet to manually find words similar to nominal emotion words." ></td>
	<td class="line x" title="37:138	Kamps and Marx (2002) use WordNets synset relations to determine the affective meaning of words." ></td>
	<td class="line x" title="38:138	They assign multidimensional scores to individual words based on the minimum path length between them and a pair of polar words (such as good and bad) in WordNets structure." ></td>
	<td class="line x" title="39:138	There is also a corpus-driven method of determining the emotional affinity of words: learn probabilistic affective scores of words from large corpora." ></td>
	<td class="line x" title="40:138	Mihalcea and Liu (2006) have used this method to assign a happiness factor to words depending on the frequency of their occurrences in happy-labeled blogposts compared to their total frequency in the corpus." ></td>
	<td class="line x" title="41:138	In this paper, we study a new approach to automatically acquiring a wide variety of words that express emotions or emotion-related concepts, using Rogets Thesaurus (1852)." ></td>
	<td class="line x" title="42:138	3 Emotion-Labeled Data We have based our study on data collected from blogs." ></td>
	<td class="line x" title="43:138	We chose blogs as data source because they are potentially rich in emotion content, and contain good examples of real-world instances of emotions expressed in text." ></td>
	<td class="line x" title="44:138	Additionally, text in blogs does not conform to the style of any particular genre per se, and thus offers a variety in writing styles, choice and combination of words, as well as topics." ></td>
	<td class="line x" title="45:138	So, the methods learned for discerning emotion using blog data are quite general and therefore applicable to a variety of genres rather than to blogs only." ></td>
	<td class="line x" title="46:138	We retrieved blogs using seed words for all emotion categories." ></td>
	<td class="line x" title="47:138	Four human judges manually annotated the blog posts with emotion-related information every sentence received two judgments." ></td>
	<td class="line x" title="48:138	The annotators were required to mark each sentence with one of the eight labels: happiness, sadness, anger, disgust, surprise, fear, mixed-emotion, and no-emotion." ></td>
	<td class="line x" title="49:138	The mixedemotion label was included to handle those sentences that had more than one type of emotion or whose emotion content could not fit into any of the given emotion categories." ></td>
	<td class="line x" title="50:138	Sample sentences from the annotated corpus are shown in Fig." ></td>
	<td class="line x" title="51:138	1." ></td>
	<td class="line x" title="52:138	We measured the inter-annotator agreement using Cohens (1960) kappa." ></td>
	<td class="line x" title="53:138	The average pair-wise agreement for different emotion categories ranged from 0.6 to 0.79." ></td>
	<td class="line x" title="54:138	In the experiments reported in this paper, we use only those sentences for which there was agreement between both judgments (to form a benchmark for the evaluation of the results of automatic classification)." ></td>
	<td class="line x" title="55:138	The distribution of emotion categories in the corpus used in our experiments is shown in Table 1." ></td>
	<td class="line x" title="56:138	313  Emotion Class Number of sentences Happiness 536 Sadness 173 Anger 179 Disgust 172 Surprise 115 Fear 115 No-emotion 600 Table 1." ></td>
	<td class="line x" title="57:138	Distribution of emotion classes 4 A Baseline Approach We are interested in investigating if emotion in text can be discerned on the basis of its lexical content." ></td>
	<td class="line x" title="58:138	A nave approach to determining the emotional orientation of text is to look for obvious emotion words, such as happy, afraid or astonished." ></td>
	<td class="line x" title="59:138	The presence of one or more words of a particular emotion category in a sentence provides a good premise for interpreting the overall emotion of the sentence." ></td>
	<td class="line x" title="60:138	This approach relies on a list of words with prior information about their emotion type, and uses it for sentence-level classification." ></td>
	<td class="line x" title="61:138	The obvious advantage is that no training data are required." ></td>
	<td class="line x" title="62:138	For evaluation purposes, we took this approach to develop a baseline system that counts the number of emotion words of each category in a sentence, and then assigns this sentence the category with the largest number of words." ></td>
	<td class="line x" title="63:138	Ties were resolved by choosing the emotion label according to an arbitrarily predefined ordering of emotion classes." ></td>
	<td class="line x" title="64:138	A sentence containing no emotion word of any type was assigned the no emotion category." ></td>
	<td class="line x" title="65:138	This system worked with word lists 2  extracted  2  Emotion words from WordNet-Affect (http://www.cse.unt.edu/~rada/affectivetext/data/WordNet AffectEmotionLists.tar.gz) from WordNet-Affect (Strapparava and Valitutti, 2004) for six basic emotion categories." ></td>
	<td class="line x" title="66:138	Table 2 shows the precision, recall, and Fmeasure values for the baseline system." ></td>
	<td class="line x" title="67:138	As we have seven classes in our experiments, the class imbalance makes accuracy values less relevant than precision, recall and F-measure." ></td>
	<td class="line x" title="68:138	That is why we do not report accuracy values in our results." ></td>
	<td class="line x" title="69:138	The baseline system shows precision values above 50% for all but two classes." ></td>
	<td class="line x" title="70:138	This shows the usefulness of this approach." ></td>
	<td class="line x" title="71:138	This method, however, fails in the absence of obvious emotion words in the sentence, as indicated by low recall values." ></td>
	<td class="line x" title="72:138	Thus, in order to improve recall, we need to increase the ambit of words that are considered emotion-related." ></td>
	<td class="line x" title="73:138	An alternative approach is to use ML to learn automatically rules that classify emotion in text." ></td>
	<td class="line x" title="74:138	Class Precision Recall F-Measure Happiness 0.589 0.390 0.469 Sadness 0.527 0.283 0.368 Anger 0.681 0.262 0.379 Disgust 0.944 0.099 0.179 Surprise 0.318 0.296 0.306 Fear 0.824 0.365 0.506 No-emotion 0.434 0.867 0.579 Table 2." ></td>
	<td class="line x" title="75:138	Performance metrics of the baseline system 5 Approach Based on Machine Learning We study two types of features: corpus-based features and features based on emotion lexicons." ></td>
	<td class="line x" title="76:138	5.1 Corpus-based features The corpus-based features exploit the statistical characteristics of the data on the basis of the ngram distribution." ></td>
	<td class="line x" title="77:138	In our experiments, we take unigrams (n=1) as features." ></td>
	<td class="line pc" title="78:138	Unigram models have been previously shown to give good results in sentiment classification tasks (Kennedy and Inkpen, 2006; Pang et al., 2002): unigram representations can capture a variety of lexical combinations and distributions, including those of emotion words." ></td>
	<td class="line x" title="79:138	This is particularly important in the case of blogs, whose language is often characterized by frequent use of new words, acronyms (such as lol), onomatopoeic words (haha, grrr), and slang, most of which can be captured in a unigram representaThis was the best summer I have ever experienced." ></td>
	<td class="line x" title="80:138	(happiness) I dont feel like I ever have that kind of privacy where I can talk to God and cry and figure things out." ></td>
	<td class="line x" title="81:138	(sadness) Finally, I got fed up." ></td>
	<td class="line x" title="82:138	(disgust) I cant believe she is finally here!" ></td>
	<td class="line x" title="83:138	(surprise) Fig 1." ></td>
	<td class="line x" title="84:138	Sample sentences from the corpus 314 tion." ></td>
	<td class="line x" title="85:138	Another advantage of a unigram representation is that it does not require any prior knowledge about the data under investigation or the classes to be identified." ></td>
	<td class="line x" title="86:138	For our experiments, we selected all unigrams that occur more than three times in the corpus." ></td>
	<td class="line x" title="87:138	This eliminates rare words, as well as foreign-language words and spelling mistakes, which are quite common in blogs." ></td>
	<td class="line x" title="88:138	We also excluded words that occur in a list of stopwords primarily function words that do not generally have emotional connotations." ></td>
	<td class="line x" title="89:138	We used the SMART list of stopword 3 , with minor modifications." ></td>
	<td class="line x" title="90:138	For instance, we removed from the stop list words such as what and why, which may be used in the context of expressing surprise." ></td>
	<td class="line x" title="91:138	5.2 Features derived from Rogets Thesaurus We utilized Rogets Thesaurus (Jarmasz and Szpakowicz, 2001) to automatically build a lexicon  3  SMART stopwords list." ></td>
	<td class="line x" title="92:138	Used with the SMART information retrieval system at Cornell University (ftp://ftp.cs.cornell.edu/pub/smart/english.stop) of emotion-related words." ></td>
	<td class="line x" title="93:138	The features based on an emotion lexiconrequire prior knowledge about emotion relatedness of words." ></td>
	<td class="line x" title="94:138	We extracted this knowledge from the classification system in Rogets, which groups related concepts into various levels of a hierarchy." ></td>
	<td class="line x" title="95:138	For a detailed account of this classification structure, see Jarmasz and Szpakowicz (2001)." ></td>
	<td class="line x" title="96:138	Rogets structure allows the calculation of semantic relatedness between words, based on the path length between the nodes in the structure that represent those words." ></td>
	<td class="line x" title="97:138	In case of multiple paths, the shortest path is considered." ></td>
	<td class="line x" title="98:138	Jarmasz and Szpakowicz (2004) have introduced a similarity measure derived from path length, which assigns scores ranging from a maximum of 16 to most semantically related words to a minimum of 0 to least related words." ></td>
	<td class="line x" title="99:138	They have shown that on semantic similarity tests this measure outperforms several other methods." ></td>
	<td class="line x" title="100:138	To build a lexicon of emotion-related words utilizing Rogets structure, we need first to make two decisions: select a primary set of emotion words starting with which we can extract other similar Similarity Score Happiness Sadness Anger Disgust Surprise Fear 16 family, home, friends, life, house, loving, partying, bed, pleasure, rest, close, event, lucks, times crying, lost, wounds, bad, pills, falling, messed, spot, unhappy, pass, black, events, hurts, shocked pride, fits, stormed, abandoned, bothered, mental, anger, feelings, distractions shock, disgust, dislike, loathing plans, catch, expected, early, slid, slipped, earlier, caught, act nervous, cry, terror, panic, feelings, run, fog, fire, turn, police, faith, battle, war, sounds 14 love, like, feel, pretty, lovely, better, smiling, nice, beautiful, hope, cutest celebrations, warm, desires ill, bored, feeling, ruin, blow, down, wrong, awful, evil, worry, crushing, bug, death, trouble, dark hate, burn, upset, dislike, wrong, blood, ill, flaws, bar, defects, bitter, growled, black, slow hate, pain, horrifying, ill, pills, sad, wear, blood, appalling, end, work, weighed, regrets, bad left, swing, noticed, worry, times, amazing, stolen, break, interesting, attention falling, life, stunned, pay, broken, hate, blast, times, hanging, hope, broken, blood, blue 12 gift, treats, adorable, fun, hug, kidding, bigger, great, lighting, won, stars, enjoy, favourite, social, divine defeat, nasty, boring, ugly, loser, end, victim, sick, hard, serious, aggravating, bothering, burning lose, throw, offended, hit, power, feel, flaring, pills, broken, life, forgot, ranting feel, fun, lies, drawn, lose, missed, deprived, lack, sighs, defeat, down, hurt, tears, insulted realize, pick, wake, sense, jumped, new, late, magic, omen, forget, popped, feel, question, late, throw fearful, spy, night, upset, feel, chased, hazardous, tomorrow, victim, grim, terrorists, apprehensive Table 3." ></td>
	<td class="line x" title="101:138	Emotion-related words automatically extracted from Rogets Thesaurus 315 words, and choose an appropriate similarity score to serve as cutoff for determining semantic relatedness between words." ></td>
	<td class="line x" title="102:138	The primary set of words that we selected consists of one word for each emotion category, representing the base form of the name of the category: {happy, sad, anger, disgust, surprise, fear}." ></td>
	<td class="line x" title="103:138	Experiments performed on Miller and Charles similarity data (1991), reported in Jarmasz and Szpakowicz (2004), have shown that pairs of words with a semantic similarity value of 16 have high similarity, while those with a score of 12 to 14 have intermediate similarity." ></td>
	<td class="line x" title="104:138	Therefore, we select the score of 12 as cutoff, and include in the lexicon all words that have similarity scores of 12 or higher with respect to the words in the primary set." ></td>
	<td class="line x" title="105:138	This selection of cutoff therefore serves as a form of feature selection." ></td>
	<td class="line x" title="106:138	In Table 3, we present sample words from the lexicon with similarity scores of 16, 14, and 12 for each emotion category." ></td>
	<td class="line x" title="107:138	These words represent three different levels of relatedness to each emotion category." ></td>
	<td class="line x" title="108:138	We are able to identify a large variety of emotion-related words belonging to different parts of speech that go well beyond the stereotypical words associated with different emotions." ></td>
	<td class="line x" title="109:138	We particularly note some generic neutral words, such as feel, life, and times associated with many emotion categories, indicating their conceptual relevance to emotions." ></td>
	<td class="line x" title="110:138	5.3 Features derived from WordNet-Affect WordNet-Affect is an affective lexical resource that assigns a variety of affect-related labels to a subset Model Class Precision Recall F-Measure Baseline F-Measure Happiness 0.840 0.675 0.740 0.469 Sadness 0.619 0.301 0.405 0.368 Anger 0.634 0.358 0.457 0.379 Disgust 0.772 0.453 0.571 0.179 Surprise 0.813 0.339 0.479 0.306 Fear 0.889 0.487 0.629 0.506 Unigrams No-emotion 0.581 0.342 0.431 0.579 Happiness 0.772 0.562 0.650 0.469 Sadness 0.574 0.225 0.324 0.368 Anger 0.638 0.246 0.355 0.379 Disgust 0.729 0.297 0.421 0.179 Surprise 0.778 0.243 0.371 0.306 Fear 0.857 0.470 0.607 0.506 Rogets Thesaurus (RT) Features No-emotion 0.498 0.258 0.340 0.579 Happiness 0.809 0.705 0.754 0.469 Sadness 0.577 0.370 0.451 0.368 Anger 0.636 0.419 0.505 0.379 Disgust 0.686 0.471 0.559 0.179 Surprise 0.717 0.374 0.491 0.306 Fear 0.831 0.513 0.634 0.506 Unigrams + RT Features No-emotion 0.586 0.512 0.546 0.579 Happiness 0.813 0.698 0.751 0.469 Sadness 0.605 0.416 0.493 0.368 Anger 0.650 0.436 0.522 0.379 Disgust 0.672 0.488 0.566 0.179 Surprise 0.723 0.409 0.522 0.306 Fear 0.868 0.513 0.645 0.506 Unigrams + RT Features + WNA Features No-emotion 0.587 0.625 0.605 0.579 * Highest precision, recall, and F-measure values for each class are shown in bold Table 4 ML Classification Results 316 of WordNet synsets comprising affective concepts." ></td>
	<td class="line x" title="111:138	We used lists of words extracted from it for each of the six emotion categories." ></td>
	<td class="line x" title="112:138	6 Experiments and Results We train classifiers with unigram features for each emotion class using Support Vector Machine (SVM) for predicting the emotion category of the sentences in our corpus." ></td>
	<td class="line pc" title="113:138	SVM has been shown to be useful for text classification tasks (Joachims, 1998), and has previously given good performance in sentiment classification experiments (Kennedy and Inkpen, 2006; Mullen and Collier, 2004; Pang and Lee, 2004; Pang et al., 2002)." ></td>
	<td class="line x" title="114:138	In Table 4, we report results from ten-fold cross-validation experiments conducted using the SMO implementation of SVM in Weka (Witten and Frank, 2005)." ></td>
	<td class="line x" title="115:138	In each experiment, we represent a sentence by a vector indicating the number of times each feature occurs." ></td>
	<td class="line x" title="116:138	In the first experiment, we use only corpusbased unigram features." ></td>
	<td class="line x" title="117:138	We obtain high precision values for all emotion classes (as shown in Table 4), and the recall and F-measure values surpass baseline values for all classes except no-emotion." ></td>
	<td class="line x" title="118:138	This validates our premise that unigrams can help learn lexical distributions well to accurately predict emotion categories." ></td>
	<td class="line x" title="119:138	Next, we use as features all words in the emotion lexicon acquired from Rogets Thesaurus (RT)." ></td>
	<td class="line x" title="120:138	The F-measure scores beat the baseline for four out of seven classes." ></td>
	<td class="line x" title="121:138	When we combine both corpus-based unigrams with RT features, we can increase recall values across all seven classes." ></td>
	<td class="line x" title="122:138	Finally, we add features from WordNet-Affect to the feature set containing corpus unigrams and RT features." ></td>
	<td class="line x" title="123:138	This leads to further improvement in overall performance." ></td>
	<td class="line x" title="124:138	Combining all features, we achieve highest recall values across all but one class." ></td>
	<td class="line x" title="125:138	The resulting F-measure values (ranging from 0.493 to 0.751) surpass the baseline values across all seven classes." ></td>
	<td class="line x" title="126:138	This increase was found to be statistically significant (paired t-test, p=0.05)." ></td>
	<td class="line x" title="127:138	7 Discussion We observe that corpus-based features and emotion-related features together contribute to improved performance, better than given by any one type of feature group alone." ></td>
	<td class="line x" title="128:138	Any automatic way of recognizing emotion should inevitably take into account a wide variety of words that are semantically connected to emotions." ></td>
	<td class="line x" title="129:138	While some words are obviously affective, many more are only potentially affective." ></td>
	<td class="line x" title="130:138	The latter derive their affective property from their associations with emotional concepts." ></td>
	<td class="line x" title="131:138	For instance, words like family, friends, home are not inherently emotional, but because of their wellknown semantic association with emotion concepts, their presence in a sentence can be taken as an indicator of emotion expression in the sentence." ></td>
	<td class="line x" title="132:138	We can interpret the results as indicators of how much correlation the classifiers can find between the features and the predicted class." ></td>
	<td class="line x" title="133:138	Considering our best results using all features, we find that this correlation is highest for the happy class, indicated by a precision of 0.813 and recall of 0.698, the highest among all classes." ></td>
	<td class="line x" title="134:138	We can therefore conclude that it is easier to discern happiness in text than Ekmans other basic emotions." ></td>
	<td class="line x" title="135:138	8 Conclusions Working on a corpus of blog sentences annotated with emotion labels, we were able to demonstrate that a combination of corpus-based unigram features and features derived from emotion lexicons can help automatically distinguish basic emotion categories in written text." ></td>
	<td class="line x" title="136:138	When used together in an SVM-based learning environment, these features increased recall in all cases and the resulting F-measure values significantly surpassed the baseline scores for all emotion categories." ></td>
	<td class="line x" title="137:138	In addition, we described a method of building an emotion lexicon derived from Rogets Thesaurus on the basis of semantic relatedness of words to a set of basic emotion words for each emotion category." ></td>
	<td class="line x" title="138:138	The effectiveness of this emotion lexicon was demonstrated in the emotion classification tasks." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="L08-1616
Building Affective Lexicons from Specific Corpora for Automatic Sentiment Analysis
Bestgen, Yves;"></td>
	<td class="line x" title="1:450	<algorithm name='ParsCit' version='080917'> <citationList> <citation valid='true'> <authors> <author>K Adamzik</author> </authors> <title>Textsorten  Texttypologie." ></td>
	<td class="line x" title="2:450	Eine kommentierte Bibliographie</title> <date>1995</date> <location>Nodus, Mnster</location> <contexts> <context>ore than 500 different genre labels from a German dictionary, while a count by (Ferrari and Manzotti, 2002) places the number of Textsorten described or referred to in a large commented bibliography (Adamzik, 1995) at more than 4,500." ></td>
	<td class="line x" title="3:450	Another major obstacle is that some approaches assume that web genres can be adequately categorised on the pageordocumentlevelalone." ></td>
	<td class="line x" title="4:450	Inadditiontotheassignment of genre cate</context> </contexts> <marker>Adamzik, 1995</marker> <rawString>K. Adamzik." ></td>
	<td class="line x" title="5:450	1995." ></td>
	<td class="line x" title="6:450	Textsorten  Texttypologie." ></td>
	<td class="line x" title="7:450	Eine kommentierte Bibliographie." ></td>
	<td class="line x" title="8:450	Nodus, Mnster.</rawString> </citation> <citation valid='true'> <authors> <author>L Bjrneborn</author> </authors> <title>Genre Connectivity and Genre Drift in a Web of Genres</title> <date>2008</date> <booktitle>Genres on the Web." ></td>
	<td class="line x" title="9:450	In preparation</booktitle> <editor>In A. Mehler, S. Sharoff, G. Rehm, and M. Santini, editors</editor> <contexts> <context>line of research proposes to investigate genres at the level of websites." ></td>
	<td class="line x" title="10:450	(Symonenko, 2007), for instance, identifies genre-like regularities in the structure of commercial and educational websites; (Bjrneborn, 2008) examines nine institutional and eight personal meta-genres in university websites; (Littig and Lindemann, 2008) present an approach for the automatic classification of websites into eight super-genr</context> </contexts> <marker>Bjrneborn, 2008</marker> <rawString>L. Bjrneborn." ></td>
	<td class="line x" title="11:450	2008." ></td>
	<td class="line x" title="12:450	Genre Connectivity and Genre Drift in a Web of Genres." ></td>
	<td class="line x" title="13:450	In A. Mehler, S. Sharoff, G. Rehm, and M. Santini, editors, Genres on the Web." ></td>
	<td class="line x" title="14:450	In preparation.</rawString> </citation> <citation valid='true'> <authors> <author>E S Boese</author> </authors> <title>Stereotyping the web: Genre classification of web documents</title> <date>2005</date> <tech>Masters thesis</tech> <institution>Computer Science Department, Colorado State University</institution> <contexts> <context>periments and to evaluate progress." ></td>
	<td class="line x" title="15:450	For instance, it is more or less impossible to compare the genre classification results reported by the studies listed in table 1." ></td>
	<td class="line x" title="16:450	Is the 92% accuracy achieved by (Boese, 2005) better than the 70% accuracy obtained by (Meyer zu Eissen and Stein, 2004)?" ></td>
	<td class="line x" title="17:450	As table 1 illustrates, the following variables differ in approaches: corpus size, number of annotators, number of genres,</context> <context>es blogs, eshops, FAQs, front pages, listings, personal home pages, search pages ca." ></td>
	<td class="line x" title="18:450	90% (Finn and Kushmerick, 2006) 2150 single rater Subjective vs. objective and positive vs. negative ca." ></td>
	<td class="line x" title="19:450	79%/49% (Boese, 2005) 343 The author plus at least one or more raters abstract, call for papers, FAQs, hub/sitemap, job description, resume/C.V., statistics, syllabus, technical paper ca." ></td>
	<td class="line x" title="20:450	90% (Kennedy and Shepherd, 2005)</context> </contexts> <marker>Boese, 2005</marker> <rawString>E. S. Boese." ></td>
	<td class="line x" title="21:450	2005." ></td>
	<td class="line x" title="22:450	Stereotyping the web: Genre classification of web documents." ></td>
	<td class="line x" title="23:450	Masters thesis, Computer Science Department, Colorado State University.</rawString> </citation> <citation valid='true'> <authors> <author>A Brandl</author> </authors> <title>Webangebote und ihre Klassifikation  Typische Merkmale aus Expertenund</title> <date>2002</date> <location>Mnchen</location> <contexts> <context>a set), however, is also problematic." ></td>
	<td class="line x" title="24:450	First, given the recency and the dynamics of web genres, it would be very difficult, probably unfeasible, to get the expertise of the required breadth and depth (Brandl, 2002)." ></td>
	<td class="line x" title="25:450	This option is likely to yield a less representative and more biased set of genre categories than optionA.Moreover,withoutanactualdocumentcollection, there would be no valid reference for genre cat</context> </contexts> <marker>Brandl, 2002</marker> <rawString>A. Brandl." ></td>
	<td class="line x" title="26:450	2002." ></td>
	<td class="line x" title="27:450	Webangebote und ihre Klassifikation  Typische Merkmale aus Expertenund Rezipientenperspektive." ></td>
	<td class="line x" title="28:450	R. Fischer, Mnchen.</rawString> </citation> <citation valid='true'> <authors> <author>P Braslavski</author> </authors> <title>Combining Relevance and Genre-Related Rankings: An Exploratory Study</title> <date>2007</date> <booktitle>Proc." ></td>
	<td class="line x" title="29:450	of the Int." ></td>
	<td class="line x" title="30:450	Workshop Towards Genre-Enabled Search Engines</booktitle> <pages>1--4</pages> <editor>In G. Rehm and M. Santini, editors</editor> <contexts> <context>res and thematic classes (Vidulin et al., 2007), or created hierarchies of genres (Stubbe and Ringlstetter, 2007)." ></td>
	<td class="line x" title="31:450	Others yet used the functional styles belonging to the Russian linguistic tradition (Braslavski, 2007), or used functional classes derived from the lexicographic tradition (Sharoff, 2007b)." ></td>
	<td class="line x" title="32:450	While the authors cited so far created their collections using the individual web page as the main unit of anal</context> <context>al/Promotional; Community; Content Delivery; Entertainment; Error Message; FAQ; Gateway; Index; Informative; Journalistic; Official; Personal; Poetry; Prose Fiction; Scientific; Shopping; User Input (Braslavski, 2007) Official, academic, journalistic, literary, and everyday communication style Table 3: Several recent genre category sets category sets structure and the meaning of individual categories." ></td>
	<td class="line x" title="33:450	In this wa</context> <context>exist that operate on similar levels and that canbemistaken for genresat firstglance." ></td>
	<td class="line x" title="34:450	Amongthese are, for example, functional styles such as official style, academicstyle, or journalistic style(Braslavski, 2007)." ></td>
	<td class="line x" title="35:450	While these terms characterise groups of texts that share a certain property, thesegroupings cannot be consideredgenres themselves, rather, they are, in essence, groups of genres: academic style </context> </contexts> <marker>Braslavski, 2007</marker> <rawString>P. Braslavski." ></td>
	<td class="line x" title="36:450	2007." ></td>
	<td class="line x" title="37:450	Combining Relevance and Genre-Related Rankings: An Exploratory Study." ></td>
	<td class="line x" title="38:450	In G. Rehm and M. Santini, editors, Proc." ></td>
	<td class="line x" title="39:450	of the Int." ></td>
	<td class="line x" title="40:450	Workshop Towards Genre-Enabled Search Engines, pages 14.</rawString> </citation> <citation valid='true'> <authors> <author>M Dimter</author> </authors> <title>TextklassenkonzepteheutigerAlltagssprache Kommunikationssituation, Textfunktion und Textinhalt als Kategorien alltagssprachlicher Textklassifikation</title> <date>1981</date> <volume>32</volume> <location>Niemeyer, Tbingen</location> <contexts> <context>e number of genres or text types (Textsorten in the German textlinguistics tradition) identifiedbylinguists, currentstudiesontheautomaticidentification of genres only use very limited category sets." ></td>
	<td class="line x" title="41:450	(Dimter, 1981) collected a list of more than 500 different genre labels from a German dictionary, while a count by (Ferrari and Manzotti, 2002) places the number of Textsorten described or referred to in a large c</context> </contexts> <marker>Dimter, 1981</marker> <rawString>M.Dimter." ></td>
	<td class="line x" title="42:450	1981." ></td>
	<td class="line x" title="43:450	TextklassenkonzepteheutigerAlltagssprache Kommunikationssituation, Textfunktion und Textinhalt als Kategorien alltagssprachlicher Textklassifikation, volume 32 of Reihe Germanistische Linguistik." ></td>
	<td class="line x" title="44:450	Niemeyer, Tbingen.</rawString> </citation> <citation valid='true'> <authors> <author>L Dong</author> <author>C Watters</author> <author>J Duffy</author> <author>M Shepherd</author> </authors> <title>An Examination of Genre Attributes for Web Page Classification</title> <date>2008</date> <booktitle>In Proc." ></td>
	<td class="line x" title="45:450	of the 41st Hawaii Int." ></td>
	<td class="line x" title="46:450	Conf." ></td>
	<td class="line x" title="47:450	on Systems Sciences (HICSS-41</booktitle> <contexts> <context> (Rehm, 2007), (Levering et al., 2008)." ></td>
	<td class="line x" title="48:450	Recently it has become popular to test classification approaches over several existing web genre collections." ></td>
	<td class="line x" title="49:450	This cross-testing technique has been adopted by (Dong et al., 2008), (Kim and Ross, 2007b), and others." ></td>
	<td class="line x" title="50:450	On the way towards a more objective evaluation of classification results,thistechniquecanbeconsideredasignificantstepforward, but it only partially addresses the </context> </contexts> <marker>Dong, Watters, Duffy, Shepherd, 2008</marker> <rawString>L. Dong, C. Watters, J. Duffy, and M. Shepherd." ></td>
	<td class="line x" title="51:450	2008." ></td>
	<td class="line x" title="52:450	An Examination of Genre Attributes for Web Page Classification." ></td>
	<td class="line x" title="53:450	In Proc." ></td>
	<td class="line x" title="54:450	of the 41st Hawaii Int." ></td>
	<td class="line x" title="55:450	Conf." ></td>
	<td class="line x" title="56:450	on Systems Sciences (HICSS-41).</rawString> </citation> <citation valid='true'> <authors> <author>A Ferrari</author> <author>E Manzotti</author> </authors> <title>Linguistica del testo</title> <date>2002</date> <booktitle>La linguistica italiana alle soglie del 2000 (19871997 e oltre</booktitle> <pages>413--453</pages> <editor>In C. Lavinio, editor</editor> <location>Bulzoni, Roma</location> <contexts> <context>tstudiesontheautomaticidentification of genres only use very limited category sets." ></td>
	<td class="line x" title="57:450	(Dimter, 1981) collected a list of more than 500 different genre labels from a German dictionary, while a count by (Ferrari and Manzotti, 2002) places the number of Textsorten described or referred to in a large commented bibliography (Adamzik, 1995) at more than 4,500." ></td>
	<td class="line x" title="58:450	Another major obstacle is that some approaches assume that web genres</context> </contexts> <marker>Ferrari, Manzotti, 2002</marker> <rawString>A. Ferrari and E. Manzotti." ></td>
	<td class="line x" title="59:450	2002." ></td>
	<td class="line x" title="60:450	Linguistica del testo." ></td>
	<td class="line x" title="61:450	In C. Lavinio, editor, La linguistica italiana alle soglie del 2000 (19871997 e oltre), pages 413453." ></td>
	<td class="line x" title="62:450	Bulzoni, Roma.</rawString> </citation> <citation valid='true'> <authors> <author>A Finn</author> <author>N Kushmerick</author> </authors> <title>Learning to Classify Documents According to Genre</title> <date>2006</date> <journal>Journal of the Am." ></td>
	<td class="line x" title="63:450	Soc." ></td>
	<td class="line x" title="64:450	for Inf." ></td>
	<td class="line x" title="65:450	Science and Tech</journal> <volume>57</volume> <contexts> <context>s # web pages Annotation Labels Accuracy (Santini, 2006) 1400 Annotation by the criterion of objective sources blogs, eshops, FAQs, front pages, listings, personal home pages, search pages ca." ></td>
	<td class="line x" title="66:450	90% (Finn and Kushmerick, 2006) 2150 single rater Subjective vs. objective and positive vs. negative ca." ></td>
	<td class="line x" title="67:450	79%/49% (Boese, 2005) 343 The author plus at least one or more raters abstract, call for papers, FAQs, hub/sitemap, job descr</context> </contexts> <marker>Finn, Kushmerick, 2006</marker> <rawString>A. Finn and N. Kushmerick." ></td>
	<td class="line x" title="68:450	2006." ></td>
	<td class="line x" title="69:450	Learning to Classify Documents According to Genre." ></td>
	<td class="line x" title="70:450	Journal of the Am." ></td>
	<td class="line x" title="71:450	Soc." ></td>
	<td class="line x" title="72:450	for Inf." ></td>
	<td class="line x" title="73:450	Science and Tech., 57(11):15061518.</rawString> </citation> <citation valid='true'> <authors> <author>R Gleim</author> <author>A Mehler</author> <author>H-J Eikmeyer</author> </authors> <title>Representing and Maintaining Large Corpora</title> <date>2007</date> <booktitle>In Proc." ></td>
	<td class="line x" title="74:450	of Corpus Linguistics</booktitle> <location>Birmingham, UK</location> <contexts> <context>us levels of web documents." ></td>
	<td class="line x" title="75:450	HyGraph is such a tool box and comes with a graphical user interface." ></td>
	<td class="line x" title="76:450	It allows for the construction, storage, management, and retrieval of large corpora of web documents (Gleim et al., 2007) and has been designed to support researchers in the overall process of corpus compilation, annotation and analysis." ></td>
	<td class="line x" title="77:450	Following, we describe how some of the typical steps of building corpora of web do</context> </contexts> <marker>Gleim, Mehler, Eikmeyer, 2007</marker> <rawString>R. Gleim, A. Mehler, and H.-J." ></td>
	<td class="line x" title="78:450	Eikmeyer." ></td>
	<td class="line x" title="79:450	2007." ></td>
	<td class="line x" title="80:450	Representing and Maintaining Large Corpora." ></td>
	<td class="line x" title="81:450	In Proc." ></td>
	<td class="line x" title="82:450	of Corpus Linguistics 2007, Birmingham, UK.</rawString> </citation> <citation valid='true'> <authors> <author>J Grimmelmann</author> </authors> <title>The Structure of Search Engine Law</title> <date>2007</date> <journal>Iowa Law Review</journal> <volume>93</volume> <contexts> <context>res, whereupon we will collect a very large set of new documents that will be categorised based on detailed annotation guidelines using the HyGraph tool; legal issues will be taken into account, see (Grimmelmann, 2007; Lehmberg et al., 2008)." ></td>
	<td class="line x" title="83:450	While we will start by applying genre labels to complete HTML documents, we plan to apply similar category sets to page segments as well as to complete websites or hypertexts</context> </contexts> <marker>Grimmelmann, 2007</marker> <rawString>J. Grimmelmann." ></td>
	<td class="line x" title="84:450	2007." ></td>
	<td class="line x" title="85:450	The Structure of Search Engine Law." ></td>
	<td class="line x" title="86:450	Iowa Law Review, 93(1):163.</rawString> </citation> <citation valid='true'> <authors> <author>S Gupta</author> <author>H Becker</author> <author>G Kaiser</author> <author>S Stolfo</author> </authors> <title>Verifying genre-based clustering approach to content extraction</title> <date>2006</date> <booktitle>In Proc." ></td>
	<td class="line x" title="87:450	of the 15th Int." ></td>
	<td class="line x" title="88:450	Conf." ></td>
	<td class="line x" title="89:450	on World Wide Web</booktitle> <pages>875--876</pages> <publisher>ACM Press</publisher> <contexts> <context>e, technical report, PhD thesis, scientific article, weather report)." ></td>
	<td class="line x" title="90:450	In Information Extraction there are several approaches to identifying and extracting useful and relevant content from web pages, (Gupta et al., 2006) used a classification of websites based on genre and layout for this purpose." ></td>
	<td class="line x" title="91:450	In Information Science, automatic genre classification could be useful for the automatic extraction of metadata essentia</context> </contexts> <marker>Gupta, Becker, Kaiser, Stolfo, 2006</marker> <rawString>S. Gupta, H. Becker, G. Kaiser, and S. Stolfo." ></td>
	<td class="line x" title="92:450	2006." ></td>
	<td class="line x" title="93:450	Verifying genre-based clustering approach to content extraction." ></td>
	<td class="line x" title="94:450	In Proc." ></td>
	<td class="line x" title="95:450	of the 15th Int." ></td>
	<td class="line x" title="96:450	Conf." ></td>
	<td class="line x" title="97:450	on World Wide Web, pages 875876." ></td>
	<td class="line x" title="98:450	ACM Press.</rawString> </citation> <citation valid='true'> <authors> <author>R C Holt</author> <author>A Schrr</author> <author>S Elliott Sim</author> </authors> <title>GXL: A Graph-Based Standard Exchange Format for Reengineering</title> <date>2006</date> <journal>Science of Computer Programming</journal> <volume>60</volume> <contexts> <context> that contains all downloaded resources." ></td>
	<td class="line x" title="99:450	Furthermore, the web document graph which is induced by the hyperlinks is extracted and stored in a dedicated XML file using GXL, the Graph eXchange Language (Holt et al., 2006)." ></td>
	<td class="line x" title="100:450	The graph representation distinguishes interand intra-page linking and also performs a basic typing of hyperlinks to capture the hierarchical structure of web documents and their components (Mehl</context> </contexts> <marker>Holt, Schrr, Sim, 2006</marker> <rawString>R. C. Holt, A. Schrr, S. Elliott Sim, , and A. Winter." ></td>
	<td class="line x" title="101:450	2006." ></td>
	<td class="line x" title="102:450	GXL: A Graph-Based Standard Exchange Format for Reengineering." ></td>
	<td class="line x" title="103:450	Science of Computer Programming, 60(2):149170.</rawString> </citation> <citation valid='true'> <authors> <author>M Y Ivory</author> <author>M A Hearst</author> </authors> <title>Statistical Profiles of Highly-Rated Web Sites</title> <date>2002</date> <booktitle>In Proc." ></td>
	<td class="line x" title="104:450	of the SIGCHI Conf." ></td>
	<td class="line x" title="105:450	on Human Factors in Computing Systems</booktitle> <pages>367--374</pages> <publisher>ACM Press</publisher> <location>Minneapolis</location> <contexts> <context>a reference tool that includes example documents for multiple web genres that could be used as typical documents (both current and historical) or best-practice blueprints for new documents, see also (Ivory and Hearst, 2002) and (Rehm, 2007)." ></td>
	<td class="line x" title="106:450	3." ></td>
	<td class="line x" title="107:450	Automatic Web Genre Identification While keywords express the topic of a text, genre expresses its type." ></td>
	<td class="line x" title="108:450	Keywords can be ambiguous, even misleading  this is why keyword-based s</context> </contexts> <marker>Ivory, Hearst, 2002</marker> <rawString>M. Y. Ivory and M. A. Hearst." ></td>
	<td class="line x" title="109:450	2002." ></td>
	<td class="line x" title="110:450	Statistical Profiles of Highly-Rated Web Sites." ></td>
	<td class="line x" title="111:450	In Proc." ></td>
	<td class="line x" title="112:450	of the SIGCHI Conf." ></td>
	<td class="line x" title="113:450	on Human Factors in Computing Systems, pages 367374, Minneapolis." ></td>
	<td class="line x" title="114:450	ACM Press.</rawString> </citation> <citation valid='true'> <authors> <author>J Karlgren</author> <author>D Cutting</author> </authors> <title>Recognizing Text Genres with Simple Metrics Using Discriminant Analysis</title> <date>1994</date> <booktitle>In COLING 94  The 15th Int." ></td>
	<td class="line x" title="115:450	Conf." ></td>
	<td class="line x" title="116:450	on Computational Linguistics</booktitle> <volume>2</volume> <pages>1071--1075</pages> <publisher>ACL</publisher> <location>Kyoto</location> <contexts> <context>of other genres." ></td>
	<td class="line x" title="117:450	Preliminary results in genre-enabled IR were reported by (Karlgren et al., 1998)." ></td>
	<td class="line x" title="118:450	(Xu et al., 2007), (Yeung et al., 2007) and nearly all other approaches since the seminal papers by (Karlgren and Cutting, 1994) and (Kessler et al., 1997) suffer from the same shortcomings: genre category sets are built according to subjective criteria for corpus composition, genre annotation, and genre granularity." ></td>
	<td class="line x" title="119:450	The fiel</context> </contexts> <marker>Karlgren, Cutting, 1994</marker> <rawString>J. Karlgren and D. Cutting." ></td>
	<td class="line x" title="120:450	1994." ></td>
	<td class="line x" title="121:450	Recognizing Text Genres with Simple Metrics Using Discriminant Analysis." ></td>
	<td class="line x" title="122:450	In COLING 94  The 15th Int." ></td>
	<td class="line x" title="123:450	Conf." ></td>
	<td class="line x" title="124:450	on Computational Linguistics, volume 2, pages 10711075, Kyoto, August." ></td>
	<td class="line x" title="125:450	ACL.</rawString> </citation> <citation valid='true'> <authors> <author>J Karlgren</author> <author>I Bretan</author> <author>J Dewe</author> <author>A Hallberg</author> <author>N Wolkert</author> </authors> <title>Iterative Information Retrieval Using Fast Clustering and Usage-Specific Genres</title> <date>1998</date> <booktitle>In Proc." ></td>
	<td class="line x" title="126:450	of the 8th DELOS Workshop on User Interfaces in Digital Libraries</booktitle> <pages>85--92</pages> <contexts> <context>sed together to increase its accuracy, so that queries such as academic papers about global warming could filter out texts of other genres." ></td>
	<td class="line x" title="127:450	Preliminary results in genre-enabled IR were reported by (Karlgren et al., 1998)." ></td>
	<td class="line x" title="128:450	(Xu et al., 2007), (Yeung et al., 2007) and nearly all other approaches since the seminal papers by (Karlgren and Cutting, 1994) and (Kessler et al., 1997) suffer from the same shortcomings: genre </context> </contexts> <marker>Karlgren, Bretan, Dewe, Hallberg, Wolkert, 1998</marker> <rawString>J. Karlgren, I. Bretan, J. Dewe, A. Hallberg, and N. Wolkert." ></td>
	<td class="line x" title="129:450	1998." ></td>
	<td class="line x" title="130:450	Iterative Information Retrieval Using Fast Clustering and Usage-Specific Genres." ></td>
	<td class="line x" title="131:450	In Proc." ></td>
	<td class="line x" title="132:450	of the 8th DELOS Workshop on User Interfaces in Digital Libraries, pages 8592.</rawString> </citation> <citation valid='true'> <authors> <author>A Kennedy</author> <author>M Shepherd</author> </authors> <title>Automatic Identification of Home Pageson the Web</title> <date>2005</date> <booktitle>In Proc." ></td>
	<td class="line x" title="133:450	of the 38th Hawaii Int." ></td>
	<td class="line x" title="134:450	Conf." ></td>
	<td class="line x" title="135:450	on Systems Sciences (HICSS-38</booktitle> <contexts> <context>l of websites (city website, conference website, and personal academic home page), to complex networks, such as, for example, wikis." ></td>
	<td class="line x" title="136:450	While some researchers focused on automatic classification, e.g., (Kennedy and Shepherd, 2005) or the analysis of a single genre (Tavosanis, 2007), others built corpora such as the one by (Kim andRoss, 2007a) that includes70 genresidentified in a large collection of PDF documents; finally, se</context> <context>ve ca." ></td>
	<td class="line x" title="137:450	79%/49% (Boese, 2005) 343 The author plus at least one or more raters abstract, call for papers, FAQs, hub/sitemap, job description, resume/C.V., statistics, syllabus, technical paper ca." ></td>
	<td class="line x" title="138:450	90% (Kennedy and Shepherd, 2005) 321 n.a. home page genres (personal, corporate, organisational) ca." ></td>
	<td class="line x" title="139:450	70% (Lim et al., 2005a, 2005b) 1224 two graduate students personal home page, public home page, commercial home page, bulletin col</context> </contexts> <marker>Kennedy, Shepherd, 2005</marker> <rawString>A. Kennedy and M. Shepherd." ></td>
	<td class="line x" title="140:450	2005." ></td>
	<td class="line x" title="141:450	Automatic Identification of Home Pageson the Web." ></td>
	<td class="line x" title="142:450	In Proc." ></td>
	<td class="line x" title="143:450	of the 38th Hawaii Int." ></td>
	<td class="line x" title="144:450	Conf." ></td>
	<td class="line x" title="145:450	on Systems Sciences (HICSS-38).</rawString> </citation> <citation valid='true'> <authors> <author>B Kessler</author> <author>G Nunberg</author> <author>H Schtze</author> </authors> <title>Automatic Detection of Text Genre</title> <date>1997</date> <booktitle>In Proc." ></td>
	<td class="line x" title="146:450	of the 35th Annual Meeting of the ACL</booktitle> <pages>32--38</pages> <contexts> <context>lts in genre-enabled IR were reported by (Karlgren et al., 1998)." ></td>
	<td class="line x" title="147:450	(Xu et al., 2007), (Yeung et al., 2007) and nearly all other approaches since the seminal papers by (Karlgren and Cutting, 1994) and (Kessler et al., 1997) suffer from the same shortcomings: genre category sets are built according to subjective criteria for corpus composition, genre annotation, and genre granularity." ></td>
	<td class="line x" title="148:450	The field is characterised by small</context> </contexts> <marker>Kessler, Nunberg, Schtze, 1997</marker> <rawString>B. Kessler, G. Nunberg, and H. Schtze." ></td>
	<td class="line x" title="149:450	1997." ></td>
	<td class="line x" title="150:450	Automatic Detection of Text Genre." ></td>
	<td class="line x" title="151:450	In Proc." ></td>
	<td class="line x" title="152:450	of the 35th Annual Meeting of the ACL, pages 3238.</rawString> </citation> <citation valid='true'> <authors> <author>Y Kim</author> <author>S Ross</author> </authors> <title>Searching for Ground Truth: A Stepping Stone in Automated Genre Classification</title> <date>2007</date> <booktitle>Proc." ></td>
	<td class="line x" title="153:450	of the DELOS Conf." ></td>
	<td class="line x" title="154:450	on Digital Libraries</booktitle> <pages>248--261</pages> <editor>In Thanos et al., editor</editor> <contexts> <context>ing et al., 2008)." ></td>
	<td class="line x" title="155:450	Recently it has become popular to test classification approaches over several existing web genre collections." ></td>
	<td class="line x" title="156:450	This cross-testing technique has been adopted by (Dong et al., 2008), (Kim and Ross, 2007b), and others." ></td>
	<td class="line x" title="157:450	On the way towards a more objective evaluation of classification results,thistechniquecanbeconsideredasignificantstepforward, but it only partially addresses the issues underlying the </context> </contexts> <marker>Kim, Ross, 2007</marker> <rawString>Y. Kim and S. Ross." ></td>
	<td class="line x" title="158:450	2007a." ></td>
	<td class="line x" title="159:450	Searching for Ground Truth: A Stepping Stone in Automated Genre Classification." ></td>
	<td class="line x" title="160:450	In Thanos et al., editor, Proc." ></td>
	<td class="line x" title="161:450	of the DELOS Conf." ></td>
	<td class="line x" title="162:450	on Digital Libraries, pages 248261.</rawString> </citation> <citation valid='true'> <authors> <author>Y Kim</author> <author>S Ross</author> </authors> <title>Variations of Word Frequencies in Genre Classification Tasks</title> <date>2007</date> <booktitle>In Proc." ></td>
	<td class="line x" title="163:450	of the DELOS Conf." ></td>
	<td class="line x" title="164:450	on Digital Libraries</booktitle> <location>Tirrenia, Italy</location> <contexts> <context>ing et al., 2008)." ></td>
	<td class="line x" title="165:450	Recently it has become popular to test classification approaches over several existing web genre collections." ></td>
	<td class="line x" title="166:450	This cross-testing technique has been adopted by (Dong et al., 2008), (Kim and Ross, 2007b), and others." ></td>
	<td class="line x" title="167:450	On the way towards a more objective evaluation of classification results,thistechniquecanbeconsideredasignificantstepforward, but it only partially addresses the issues underlying the </context> </contexts> <marker>Kim, Ross, 2007</marker> <rawString>Y. Kim and S. Ross." ></td>
	<td class="line x" title="168:450	2007b." ></td>
	<td class="line x" title="169:450	Variations of Word Frequencies in Genre Classification Tasks." ></td>
	<td class="line x" title="170:450	In Proc." ></td>
	<td class="line x" title="171:450	of the DELOS Conf." ></td>
	<td class="line x" title="172:450	on Digital Libraries, Tirrenia, Italy.</rawString> </citation> <citation valid='true'> <authors> <author>Y-B Lee</author> <author>S Hyon Myaeng</author> </authors> <title>Automatic Identification of Text Genres and Their Roles in Subject-Based Categorization</title> <date>2004</date> <booktitle>In Proc." ></td>
	<td class="line x" title="173:450	of the 37th Hawaii Int." ></td>
	<td class="line x" title="174:450	Conf." ></td>
	<td class="line x" title="175:450	on Systems Sciences (HICSS-37</booktitle> <contexts> <context>% (Meyer zu Eissen and Stein, 2004) 800 three people, one of the authors checked some pages article, discussion, shop, portrayal (non-private), portrayal (private), link collection, download ca." ></td>
	<td class="line x" title="176:450	70% (Lee and Myaeng, 2004) 321 at least two raters reportage-editorial, research article, review, home page, Q&amp;A, specification ca." ></td>
	<td class="line x" title="177:450	90% Table 1: A few recent studies in automatic web genre identification cannot make any assum</context> </contexts> <marker>Lee, Myaeng, 2004</marker> <rawString>Y.-B." ></td>
	<td class="line x" title="178:450	Lee and S. Hyon Myaeng." ></td>
	<td class="line x" title="179:450	2004." ></td>
	<td class="line x" title="180:450	Automatic Identification of Text Genres and Their Roles in Subject-Based Categorization." ></td>
	<td class="line x" title="181:450	In Proc." ></td>
	<td class="line x" title="182:450	of the 37th Hawaii Int." ></td>
	<td class="line x" title="183:450	Conf." ></td>
	<td class="line x" title="184:450	on Systems Sciences (HICSS-37).</rawString> </citation> <citation valid='true'> <authors> <author>T Lehmberg</author> <author>G Rehm</author> <author>A Witt</author> <author>F Zimmermann</author> </authors> <title>Preserving Linguistic Resources: Licensing  Privacy Issues  Mashups." ></td>
	<td class="line x" title="185:450	Library Trends." ></td>
	<td class="line x" title="186:450	In print</title> <date>2008</date> <contexts> <context>ill collect a very large set of new documents that will be categorised based on detailed annotation guidelines using the HyGraph tool; legal issues will be taken into account, see (Grimmelmann, 2007; Lehmberg et al., 2008)." ></td>
	<td class="line x" title="187:450	While we will start by applying genre labels to complete HTML documents, we plan to apply similar category sets to page segments as well as to complete websites or hypertexts." ></td>
	<td class="line x" title="188:450	We also plan to inclu</context> </contexts> <marker>Lehmberg, Rehm, Witt, Zimmermann, 2008</marker> <rawString>T. Lehmberg, G. Rehm, A. Witt, and F. Zimmermann." ></td>
	<td class="line x" title="189:450	2008." ></td>
	<td class="line x" title="190:450	Preserving Linguistic Resources: Licensing  Privacy Issues  Mashups." ></td>
	<td class="line x" title="191:450	Library Trends." ></td>
	<td class="line x" title="192:450	In print.</rawString> </citation> <citation valid='true'> <authors> <author>R Levering</author> <author>M Cutler</author> <author>L Yu</author> </authors> <title>Using Visual Features for Fine-Grained Genre Classification of Web Pages</title> <date>2008</date> <booktitle>In Proc." ></td>
	<td class="line x" title="193:450	of the 41st Hawaii Int." ></td>
	<td class="line x" title="194:450	Conf." ></td>
	<td class="line x" title="195:450	on Systems Sciences (HICSS-41</booktitle> <contexts> <context>ch as the one by (Kim andRoss, 2007a) that includes70 genresidentified in a large collection of PDF documents; finally, several researchers are interested in fine-grained genres, e.g., (Rehm, 2007), (Levering et al., 2008)." ></td>
	<td class="line x" title="196:450	Recently it has become popular to test classification approaches over several existing web genre collections." ></td>
	<td class="line x" title="197:450	This cross-testing technique has been adopted by (Dong et al., 2008), (Kim and Ross, 20</context> </contexts> <marker>Levering, Cutler, Yu, 2008</marker> <rawString>R. Levering, M. Cutler, and L. Yu." ></td>
	<td class="line x" title="198:450	2008." ></td>
	<td class="line x" title="199:450	Using Visual Features for Fine-Grained Genre Classification of Web Pages." ></td>
	<td class="line x" title="200:450	In Proc." ></td>
	<td class="line x" title="201:450	of the 41st Hawaii Int." ></td>
	<td class="line x" title="202:450	Conf." ></td>
	<td class="line x" title="203:450	on Systems Sciences (HICSS-41).</rawString> </citation> <citation valid='true'> <authors> <author>C Su Lim</author> <author>K Joo Lee</author> <author>G Chang Kim</author> </authors> <title>Multiple Sets of Features for Automatic Genre Classification of Web Documents</title> <date>2005</date> <booktitle>Information Processing and Management</booktitle> <pages>41--5</pages> <contexts> <context>s, FAQs, hub/sitemap, job description, resume/C.V., statistics, syllabus, technical paper ca." ></td>
	<td class="line x" title="204:450	90% (Kennedy and Shepherd, 2005) 321 n.a. home page genres (personal, corporate, organisational) ca." ></td>
	<td class="line x" title="205:450	70% (Lim et al., 2005a, 2005b) 1224 two graduate students personal home page, public home page, commercial home page, bulletin collection, link collection, image collection, simple table/lists, input pages, journalistic m</context> <context>ment of categories in order to arrive at a consensus on the (Meyer zu Eissen and Stein, 2004) Help; Article; Discussion; Shop; Portrayal (non-private); Portrayal (private); Link Collection; Download (Lim et al., 2005) Personal homepages; Public homepages; Commercial homepages; Bulletin collections; Link collections; Image collections; Simple tables/lists; Input pages; Journalistic materials; Research reports; Off</context> </contexts> <marker>Lim, Lee, Kim, 2005</marker> <rawString>C. Su Lim, K. Joo Lee, and G. Chang Kim." ></td>
	<td class="line x" title="206:450	2005." ></td>
	<td class="line x" title="207:450	Multiple Sets of Features for Automatic Genre Classification of Web Documents." ></td>
	<td class="line x" title="208:450	Information Processing and Management, 41(5):12631276.</rawString> </citation> <citation valid='true'> <authors> <author>L Littig andC Lindemann</author> </authors> <title>Classificationof WebSites atSuper-GenreLevel</title> <date>2008</date> <booktitle>Genres on the Web." ></td>
	<td class="line x" title="209:450	In preparation</booktitle> <editor>In A. Mehler, S. Sharoff, G. Rehm, and M. Santini, editors</editor> <contexts> <context>fies genre-like regularities in the structure of commercial and educational websites; (Bjrneborn, 2008) examines nine institutional and eight personal meta-genres in university websites; (Littig and Lindemann, 2008) present an approach for the automatic classification of websites into eight super-genres by combining content and structure." ></td>
	<td class="line x" title="210:450	(Mehler, 2008) emphasises the discriminating power of structural informat</context> </contexts> <marker>Lindemann, 2008</marker> <rawString>L. Littig andC.Lindemann." ></td>
	<td class="line x" title="211:450	2008." ></td>
	<td class="line x" title="212:450	Classificationof WebSites atSuper-GenreLevel." ></td>
	<td class="line x" title="213:450	In A. Mehler, S. Sharoff, G. Rehm, and M. Santini, editors, Genres on the Web." ></td>
	<td class="line x" title="214:450	In preparation.</rawString> </citation> <citation valid='true'> <authors> <author>A Mehler</author> <author>R Gleim</author> </authors> <title>The Net for the Graphs  Towards Webgenre Representation for Corpus Linguistic Studies</title> <date>2006</date> <booktitle>Working Papers on the Web as Corpus</booktitle> <pages>191--224</pages> <editor>In M. Baroni and S. Bernardini, editors, WaCky</editor> <publisher>Gedit</publisher> <location>Bologna</location> <contexts> <context> a second category set for the web genre modules/components that occur on the intradocument level, we need a third category set, because web genres can be instantiated on the level of whole websites (Mehler and Gleim, 2006; Symonenko, 2007)." ></td>
	<td class="line x" title="215:450	Ideally, conventionalised connections between these three levels should be represented within the category sets (for example, thataconference websitecontains,amongothers, acall for</context> <context>2006)." ></td>
	<td class="line x" title="216:450	The graph representation distinguishes interand intra-page linking and also performs a basic typing of hyperlinks to capture the hierarchical structure of web documents and their components (Mehler and Gleim, 2006)." ></td>
	<td class="line x" title="217:450	The resulting GXL-file contains metadata and is also used to store any further annotation." ></td>
	<td class="line x" title="218:450	The use of stand-off annotation leaves the original resources untouched and easily accessible for other to</context> </contexts> <marker>Mehler, Gleim, 2006</marker> <rawString>A. Mehler and R. Gleim." ></td>
	<td class="line x" title="219:450	2006." ></td>
	<td class="line x" title="220:450	The Net for the Graphs  Towards Webgenre Representation for Corpus Linguistic Studies." ></td>
	<td class="line x" title="221:450	In M. Baroni and S. Bernardini, editors, WaCky!" ></td>
	<td class="line x" title="222:450	Working Papers on the Web as Corpus, pages 191224." ></td>
	<td class="line x" title="223:450	Gedit, Bologna.</rawString> </citation> <citation valid='true'> <authors> <author>A Mehler</author> <author>R Gleim</author> <author>A Wegner</author> </authors> <title>Structural Uncertainty of Hypertext Types</title> <date>2007</date> <booktitle>Proc." ></td>
	<td class="line x" title="224:450	of the Int." ></td>
	<td class="line x" title="225:450	Workshop Towards Genre-Enabled Search Engines</booktitle> <pages>13--20</pages> <editor>In G. Rehm and M. Santini, editors</editor> <contexts> <context>ork on an intra-document, or page segment level because a single document can contain instances of multiple genres, e.g., contact information, list of publications, C.V., see (Rehm, 2002; Rehm, 2007; Mehler et al., 2007)." ></td>
	<td class="line x" title="226:450	In addition to a second category set for the web genre modules/components that occur on the intradocument level, we need a third category set, because web genres can be instantiated on the level of</context> <context>0 pages for each of the two languages, as well as predicted classes produced by SVM-based classifiers (65,177 pages for English, 29,650 for Russian)." ></td>
	<td class="line x" title="227:450	 The German corpus by (Mehler et al., 2008) and (Mehler et al., 2007) including four web genres: 50 conference websites (2,779 pages, 435 annotated page segments), 68 personal academic homepages (1,569 pages, 292 segments), 52 project websites (1,591 1." ></td>
	<td class="line x" title="228:450	AboutPage  A </context> <context>le 4: An initial list of web genres compiled from previous approaches in a wiki-based discussion pages, 612 segments), 180 city websites (based on 39,862 pages)." ></td>
	<td class="line x" title="229:450	Further, the English corpus built by (Mehler et al., 2007) including two web genres: the genre of 1,460 conference websites (76,011 pages), and personal academic homepages (16,652 pages)." ></td>
	<td class="line x" title="230:450	These corpora are built on the level of websites." ></td>
	<td class="line x" title="231:450	5.3." ></td>
	<td class="line x" title="232:450	Corpus Managem</context> <context>the whole page) manifest a specific category." ></td>
	<td class="line x" title="233:450	Moreover, categorising web documents suffers from heterogeneous, i.e., polymorphicwebpagesthatcontaindifferentpatternsasinstances of multiple categories (Mehler et al., 2007)." ></td>
	<td class="line x" title="234:450	Therefore, the possibility of explicitly demarcating subtrees of the DOM-tree of a page is an important part of annotating web documents." ></td>
	<td class="line x" title="235:450	This functionality is currently under development; its incl</context> </contexts> <marker>Mehler, Gleim, Wegner, 2007</marker> <rawString>A. Mehler, R. Gleim, and A. Wegner." ></td>
	<td class="line x" title="236:450	2007." ></td>
	<td class="line x" title="237:450	Structural Uncertainty of Hypertext Types." ></td>
	<td class="line x" title="238:450	In G. Rehm and M. Santini, editors, Proc." ></td>
	<td class="line x" title="239:450	of the Int." ></td>
	<td class="line x" title="240:450	Workshop Towards Genre-Enabled Search Engines, pages 1320.</rawString> </citation> <citation valid='true'> <authors> <author>A Mehler</author> <author>U Waltinger</author> <author>R Gleim</author> <author>A Wegner</author> </authors> <title>A Model of SemiSupervised Hypertext Zoning</title> <date>2008</date> <booktitle>Modeling, Learning and Processing of Text Technological Data Structures." ></td>
	<td class="line x" title="241:450	In preparation</booktitle> <editor>In A. Mehler, K.-U." ></td>
	<td class="line x" title="242:450	Khnberger, H. Lobin, H. Lngen, A. Storrer, and A. Witt, editors</editor> <contexts> <context>ally checked samples of 250 pages for each of the two languages, as well as predicted classes produced by SVM-based classifiers (65,177 pages for English, 29,650 for Russian)." ></td>
	<td class="line x" title="243:450	 The German corpus by (Mehler et al., 2008) and (Mehler et al., 2007) including four web genres: 50 conference websites (2,779 pages, 435 annotated page segments), 68 personal academic homepages (1,569 pages, 292 segments), 52 project website</context> </contexts> <marker>Mehler, Waltinger, Gleim, Wegner, 2008</marker> <rawString>A. Mehler, U. Waltinger, R. Gleim, and A. Wegner." ></td>
	<td class="line x" title="244:450	2008." ></td>
	<td class="line x" title="245:450	A Model of SemiSupervised Hypertext Zoning." ></td>
	<td class="line x" title="246:450	In A. Mehler, K.-U." ></td>
	<td class="line x" title="247:450	Khnberger, H. Lobin, H. Lngen, A. Storrer, and A. Witt, editors, Modeling, Learning and Processing of Text Technological Data Structures." ></td>
	<td class="line x" title="248:450	In preparation.</rawString> </citation> <citation valid='true'> <authors> <author>A Mehler</author> </authors> <title>Structural Similarities of Complex Networks: A Computational Model by Example of Wiki Graph." ></td>
	<td class="line x" title="249:450	Applied Artificial Intelligence." ></td>
	<td class="line x" title="250:450	In print</title> <date>2008</date> <contexts> <context>personal meta-genres in university websites; (Littig and Lindemann, 2008) present an approach for the automatic classification of websites into eight super-genres by combining content and structure." ></td>
	<td class="line x" title="251:450	(Mehler, 2008) emphasises the discriminating power of structural information and applies his approach to different types of complex documents, from German thematic classes, to web genres at the level of websites (</context> </contexts> <marker>Mehler, 2008</marker> <rawString>A. Mehler." ></td>
	<td class="line x" title="252:450	2008." ></td>
	<td class="line x" title="253:450	Structural Similarities of Complex Networks: A Computational Model by Example of Wiki Graph." ></td>
	<td class="line x" title="254:450	Applied Artificial Intelligence." ></td>
	<td class="line x" title="255:450	In print.</rawString> </citation> <citation valid='true'> <authors> <author>S Meyer zu Eissen</author> <author>B Stein</author> </authors> <title>Genre Classification of Web Pages</title> <date>2004</date> <booktitle>In Proc</booktitle> <contexts> <context>less impossible to compare the genre classification results reported by the studies listed in table 1." ></td>
	<td class="line x" title="256:450	Is the 92% accuracy achieved by (Boese, 2005) better than the 70% accuracy obtained by (Meyer zu Eissen and Stein, 2004)?" ></td>
	<td class="line x" title="257:450	As table 1 illustrates, the following variables differ in approaches: corpus size, number of annotators, number of genres, number of web pages per genre." ></td>
	<td class="line x" title="258:450	Furthermore, studies usually do not make ex</context> <context>llection, simple table/lists, input pages, journalistic material, research report, official materials, FAQs, discussions, product specification, informal texts (poem, fiction, etc.) ca." ></td>
	<td class="line x" title="259:450	75% (Meyer zu Eissen and Stein, 2004) 800 three people, one of the authors checked some pages article, discussion, shop, portrayal (non-private), portrayal (private), link collection, download ca." ></td>
	<td class="line x" title="260:450	70% (Lee and Myaeng, 2004) 321 at least</context> <context> research groups (option C)." ></td>
	<td class="line x" title="261:450	This task is by no means just a simple compilation of labels, but involves the evaluation, and refinement of categories in order to arrive at a consensus on the (Meyer zu Eissen and Stein, 2004) Help; Article; Discussion; Shop; Portrayal (non-private); Portrayal (private); Link Collection; Download (Lim et al., 2005) Personal homepages; Public homepages; Commercial homepages; Bulletin colle</context> </contexts> <marker>Eissen, Stein, 2004</marker> <rawString>S. Meyer zu Eissen and B. Stein." ></td>
	<td class="line x" title="262:450	2004." ></td>
	<td class="line x" title="263:450	Genre Classification of Web Pages." ></td>
	<td class="line x" title="264:450	In Proc.</rawString> </citation> <citation valid='true'> <date></date> <booktitle>of the 27th German Conf." ></td>
	<td class="line x" title="265:450	on AI (KI-2004</booktitle> <location>Ulm</location> <marker></marker> <rawString>of the 27th German Conf." ></td>
	<td class="line x" title="266:450	on AI (KI-2004), Ulm, September.</rawString> </citation> <citation valid='true'> <authors> <author>G Rehm</author> <author>M Santini</author> <author>editors</author> </authors> <date>2007</date> <booktitle>Proc." ></td>
	<td class="line x" title="267:450	of the Int." ></td>
	<td class="line x" title="268:450	Workshop Towards GenreEnabled Search Engines: The Impact of NLP, Borovets</booktitle> <location>Bulgaria</location> <marker>Rehm, Santini, editors, 2007</marker> <rawString>G. Rehm and M. Santini, editors." ></td>
	<td class="line x" title="269:450	2007." ></td>
	<td class="line x" title="270:450	Proc." ></td>
	<td class="line x" title="271:450	of the Int." ></td>
	<td class="line x" title="272:450	Workshop Towards GenreEnabled Search Engines: The Impact of NLP, Borovets, Bulgaria.</rawString> </citation> <citation valid='true'> <authors> <author>G Rehm</author> </authors> <title>Towards Automatic Web Genre Identification  A Corpus-Based Approach in the Domain of Academia by Example of the Academics Personal Homepage</title> <date>2002</date> <booktitle>In Proc." ></td>
	<td class="line x" title="273:450	of the 35th Hawaii Int." ></td>
	<td class="line x" title="274:450	Conf." ></td>
	<td class="line x" title="275:450	on System Sc." ></td>
	<td class="line o" title="276:450	(HICSS-35</booktitle> <contexts> <context>documents, genres also work on an intra-document, or page segment level because a single document can contain instances of multiple genres, e.g., contact information, list of publications, C.V., see (Rehm, 2002; Rehm, 2007; Mehler et al., 2007)." ></td>
	<td class="line x" title="277:450	In addition to a second category set for the web genre modules/components that occur on the intradocument level, we need a third category set, because web genres ca</context> </contexts> <marker>Rehm, 2002</marker> <rawString>G. Rehm." ></td>
	<td class="line x" title="278:450	2002." ></td>
	<td class="line x" title="279:450	Towards Automatic Web Genre Identification  A Corpus-Based Approach in the Domain of Academia by Example of the Academics Personal Homepage." ></td>
	<td class="line x" title="280:450	In Proc." ></td>
	<td class="line x" title="281:450	of the 35th Hawaii Int." ></td>
	<td class="line x" title="282:450	Conf." ></td>
	<td class="line x" title="283:450	on System Sc." ></td>
	<td class="line x" title="284:450	(HICSS-35).</rawString> </citation> <citation valid='true'> <authors> <author>G Rehm</author> </authors> <title>Language-Independent Text Parsing of Arbitrary HTMLDocuments  Towards A Foundation For Web Genre Identification</title> <date>2005</date> <journal>LDV Forum</journal> <volume>20</volume> <contexts> <context>important part of annotating web documents." ></td>
	<td class="line x" title="285:450	This functionality is currently under development; its inclusion in HyGraph will allow for annotating entire pages as well as DOM-based segments, see also (Rehm, 2005)." ></td>
	<td class="line x" title="286:450	HyGraph offers various means of exploring extracted web documents." ></td>
	<td class="line x" title="287:450	It is possible to view the HTML source of web pages, their DOM-trees (figure 5) or their GXL-based representation." ></td>
	<td class="line x" title="288:450	A graphical vie</context> </contexts> <marker>Rehm, 2005</marker> <rawString>G. Rehm." ></td>
	<td class="line x" title="289:450	2005." ></td>
	<td class="line x" title="290:450	Language-Independent Text Parsing of Arbitrary HTMLDocuments  Towards A Foundation For Web Genre Identification." ></td>
	<td class="line x" title="291:450	LDV Forum, 20(2):5374.</rawString> </citation> <citation valid='true'> <authors> <author>G Rehm</author> </authors> <title>Hypertextsorten: Definition  Struktur  Klassifikation." ></td>
	<td class="line x" title="292:450	Books on Demand</title> <date>2007</date> <tech>PhD thesis</tech> <institution>Comp." ></td>
	<td class="line x" title="293:450	Ling., Giessen University</institution> <location>Norderstedt</location> <contexts> <context>s example documents for multiple web genres that could be used as typical documents (both current and historical) or best-practice blueprints for new documents, see also (Ivory and Hearst, 2002) and (Rehm, 2007)." ></td>
	<td class="line x" title="294:450	3." ></td>
	<td class="line x" title="295:450	Automatic Web Genre Identification While keywords express the topic of a text, genre expresses its type." ></td>
	<td class="line x" title="296:450	Keywords can be ambiguous, even misleading  this is why keyword-based searches frequentl</context> <context>ilt corpora such as the one by (Kim andRoss, 2007a) that includes70 genresidentified in a large collection of PDF documents; finally, several researchers are interested in fine-grained genres, e.g., (Rehm, 2007), (Levering et al., 2008)." ></td>
	<td class="line x" title="297:450	Recently it has become popular to test classification approaches over several existing web genre collections." ></td>
	<td class="line x" title="298:450	This cross-testing technique has been adopted by (Dong et al.,</context> <context>enres also work on an intra-document, or page segment level because a single document can contain instances of multiple genres, e.g., contact information, list of publications, C.V., see (Rehm, 2002; Rehm, 2007; Mehler et al., 2007)." ></td>
	<td class="line x" title="299:450	In addition to a second category set for the web genre modules/components that occur on the intradocument level, we need a third category set, because web genres can be instant</context> </contexts> <marker>Rehm, 2007</marker> <rawString>G. Rehm." ></td>
	<td class="line x" title="300:450	2007." ></td>
	<td class="line x" title="301:450	Hypertextsorten: Definition  Struktur  Klassifikation." ></td>
	<td class="line x" title="302:450	Books on Demand, Norderstedt." ></td>
	<td class="line x" title="303:450	(PhD thesis, Comp." ></td>
	<td class="line x" title="304:450	Ling., Giessen University, 2005).</rawString> </citation> <citation valid='true'> <authors> <author>G Rehm</author> </authors> <title>A Comparative Analysis of Genre Category Sets as a Prerequisite for a Reference Corpus of Web Genres</title> <date>2008</date> <booktitle>Genres on the Web." ></td>
	<td class="line x" title="305:450	In preparation</booktitle> <editor>In A. Mehler, S. Sharoff, G. Rehm, and M. Santini, editors</editor> <contexts> <context>with new genre labels that have both adequate namesandoperateonanappropriatelevelofcategorisation." ></td>
	<td class="line x" title="306:450	We are currently discussing the advantages and disadvantages of annotation guidelines for this task (Rehm, 2008)." ></td>
	<td class="line x" title="307:450	5.2." ></td>
	<td class="line x" title="308:450	A Reference Collection of Documents We plan to build the reference corpus in two stages." ></td>
	<td class="line x" title="309:450	First, we will apply the category sets that we are currently working on (see section 5.1) to existing c</context> </contexts> <marker>Rehm, 2008</marker> <rawString>G. Rehm." ></td>
	<td class="line x" title="310:450	2008." ></td>
	<td class="line x" title="311:450	A Comparative Analysis of Genre Category Sets as a Prerequisite for a Reference Corpus of Web Genres." ></td>
	<td class="line x" title="312:450	In A. Mehler, S. Sharoff, G. Rehm, and M. Santini, editors, Genres on the Web." ></td>
	<td class="line x" title="313:450	In preparation.</rawString> </citation> <citation valid='true'> <authors> <author>M Rosso</author> </authors> <title>Using Genre to Improve Web Search</title> <date>2005</date> <tech>Ph.D. thesis</tech> <institution>School of Inf." ></td>
	<td class="line x" title="314:450	and Lib." ></td>
	<td class="line x" title="315:450	Sc., Univ. of North Carolina at Chapel Hill</institution> <contexts> <context>e authors participated in this experiment." ></td>
	<td class="line x" title="316:450	Other studies have shown that user-based genre labeling usually exhibits a certain kind of fragmentation and a low, at most moderate, inter-rater agreement (Rosso, 2005; Santini, 2008)." ></td>
	<td class="line x" title="317:450	In other words, the assignment of genre labels to documents, especially web documents, is a very hard task." ></td>
	<td class="line x" title="318:450	The aim of our experiment was to see whethergenreexpertiseisabletoimprovei</context> </contexts> <marker>Rosso, 2005</marker> <rawString>M. Rosso." ></td>
	<td class="line x" title="319:450	2005." ></td>
	<td class="line x" title="320:450	Using Genre to Improve Web Search." ></td>
	<td class="line x" title="321:450	Ph.D. thesis, School of Inf." ></td>
	<td class="line x" title="322:450	and Lib." ></td>
	<td class="line x" title="323:450	Sc., Univ. of North Carolina at Chapel Hill.</rawString> </citation> <citation valid='true'> <authors> <author>M Rosso</author> </authors> <title>User-Based Identification of Web Genres</title> <date>2008</date> <journal>JASIST</journal> <volume>59</volume> <contexts> <context> this is particularly evident in the experiment reported in section 4." ></td>
	<td class="line x" title="324:450	The situation does not improve when non-experts, and presumably non-prejudiced annotators are asked to label web pages by genre (Rosso, 2008; Santini, 2008)." ></td>
	<td class="line x" title="325:450	The proliferation of genre classes cited in the literature varies in terms of generality (super-genres, genres, subgenres, functional styles, functional classes, relatively arbitrary</context> </contexts> <marker>Rosso, 2008</marker> <rawString>M. Rosso." ></td>
	<td class="line x" title="326:450	2008." ></td>
	<td class="line x" title="327:450	User-Based Identification of Web Genres." ></td>
	<td class="line x" title="328:450	JASIST, 59(5):120.</rawString> </citation> <citation valid='true'> <authors> <author>M Sanderson</author> <author>H Joho</author> </authors> <title>Forming test collections with no system pooling</title> <date>2004</date> <booktitle>Proc." ></td>
	<td class="line x" title="329:450	of the 27th Int." ></td>
	<td class="line x" title="330:450	ACM SIGIR Conf." ></td>
	<td class="line x" title="331:450	on Research and Dev." ></td>
	<td class="line x" title="332:450	in IR</booktitle> <pages>33--40</pages> <editor>In K. Jrvelin, J. Allan, P. Bruza, and M. Sanderson, editors</editor> <contexts> <context>ditorials, short biographies, DIY mini-guides and feature articles (20 web pages per category); (b) seven novel web genres annotated with objective sources (Santini, 2006); (c) the SPIRIT collection (Sanderson and Joho, 2004), which contains random and unclassified web pages." ></td>
	<td class="line x" title="333:450	 The Hierachical Webgenre Collection (Stubbe and Ringlstetter, 2007; Stubbe et al., 2007b), containing 32 genre classes, 40 HTML files per class, </context> </contexts> <marker>Sanderson, Joho, 2004</marker> <rawString>M. Sanderson and H. Joho." ></td>
	<td class="line x" title="334:450	2004." ></td>
	<td class="line x" title="335:450	Forming test collections with no system pooling." ></td>
	<td class="line x" title="336:450	In K. Jrvelin, J. Allan, P. Bruza, and M. Sanderson, editors, Proc." ></td>
	<td class="line x" title="337:450	of the 27th Int." ></td>
	<td class="line x" title="338:450	ACM SIGIR Conf." ></td>
	<td class="line x" title="339:450	on Research and Dev." ></td>
	<td class="line x" title="340:450	in IR, pages 3340.</rawString> </citation> <citation valid='true'> <authors> <author>M Santini</author> </authors> <title>Common Criteria for Genre Classification: Annotation and Granularity</title> <date>2006</date> <booktitle>In Workshop on Text-based IR (TIR 06), Riva del Garda</booktitle> <location>Italy</location> <contexts> <context>e corpus that, although designed to be large, is necessarily limited in size?" ></td>
	<td class="line x" title="341:450	Additionally, as we do not know the distribution of genres on the web, we Authors # web pages Annotation Labels Accuracy (Santini, 2006) 1400 Annotation by the criterion of objective sources blogs, eshops, FAQs, front pages, listings, personal home pages, search pages ca." ></td>
	<td class="line x" title="342:450	90% (Finn and Kushmerick, 2006) 2150 single rater Subjective</context> <context>or English (Santini, 2007), including (a) editorials, short biographies, DIY mini-guides and feature articles (20 web pages per category); (b) seven novel web genres annotated with objective sources (Santini, 2006); (c) the SPIRIT collection (Sanderson and Joho, 2004), which contains random and unclassified web pages." ></td>
	<td class="line x" title="343:450	 The Hierachical Webgenre Collection (Stubbe and Ringlstetter, 2007; Stubbe et al., 2007b), </context> </contexts> <marker>Santini, 2006</marker> <rawString>M. Santini." ></td>
	<td class="line x" title="344:450	2006." ></td>
	<td class="line x" title="345:450	Common Criteria for Genre Classification: Annotation and Granularity." ></td>
	<td class="line x" title="346:450	In Workshop on Text-based IR (TIR 06), Riva del Garda, Italy.</rawString> </citation> <citation valid='true'> <authors> <author>M Santini</author> </authors> <title>Automatic Identification of Genre in Web Pages</title> <date>2007</date> <tech>Ph.D. thesis</tech> <institution>University of Brighton</institution> <contexts> <context> article is the result of a discussion the authors had at the workshop Towards Genre-Enabled Search Engines: The Impact of NLP, held, in conjunction with RANLP 2007, on September 30, 2007 (Rehm and Santini, 2007)." ></td>
	<td class="line x" title="347:450	documents (section 5.2), and a tool for the annotation of the collection with specific categories so that a gold standard benchmark can be built (section 5.3)." ></td>
	<td class="line x" title="348:450	2." ></td>
	<td class="line x" title="349:450	Rationale A classification by genr</context> <context>xclusively) construct this crucial part of the resource; inter-coder reliability will be taken into account." ></td>
	<td class="line x" title="350:450	Among the collections that we plan to process initially are:  The Web Corpus for English (Santini, 2007), including (a) editorials, short biographies, DIY mini-guides and feature articles (20 web pages per category); (b) seven novel web genres annotated with objective sources (Santini, 2006); (c) the S</context> </contexts> <marker>Santini, 2007</marker> <rawString>M. Santini." ></td>
	<td class="line x" title="351:450	2007." ></td>
	<td class="line x" title="352:450	Automatic Identification of Genre in Web Pages." ></td>
	<td class="line x" title="353:450	Ph.D. thesis, University of Brighton.</rawString> </citation> <citation valid='true'> <authors> <author>M Santini</author> </authors> <title>Zero, Single, or Multi?" ></td>
	<td class="line x" title="354:450	Genre of Web Pages through the Users Perspective</title> <date>2008</date> <booktitle>Information Processing and Management</booktitle> <pages>44--2</pages> <contexts> <context>icularly evident in the experiment reported in section 4." ></td>
	<td class="line x" title="355:450	The situation does not improve when non-experts, and presumably non-prejudiced annotators are asked to label web pages by genre (Rosso, 2008; Santini, 2008)." ></td>
	<td class="line x" title="356:450	The proliferation of genre classes cited in the literature varies in terms of generality (super-genres, genres, subgenres, functional styles, functional classes, relatively arbitrary text classes o</context> <context>ticipated in this experiment." ></td>
	<td class="line x" title="357:450	Other studies have shown that user-based genre labeling usually exhibits a certain kind of fragmentation and a low, at most moderate, inter-rater agreement (Rosso, 2005; Santini, 2008)." ></td>
	<td class="line x" title="358:450	In other words, the assignment of genre labels to documents, especially web documents, is a very hard task." ></td>
	<td class="line x" title="359:450	The aim of our experiment was to see whethergenreexpertiseisabletoimproveinter-rater agre</context> </contexts> <marker>Santini, 2008</marker> <rawString>M. Santini." ></td>
	<td class="line x" title="360:450	2008." ></td>
	<td class="line x" title="361:450	Zero, Single, or Multi?" ></td>
	<td class="line x" title="362:450	Genre of Web Pages through the Users Perspective." ></td>
	<td class="line x" title="363:450	Information Processing and Management, 44(2):702737.</rawString> </citation> <citation valid='true'> <authors> <author>S Sharoff</author> </authors> <title>Classifying Web Corpora into Domain and Genre Using Automatic Feature Identification</title> <date>2007</date> <booktitle>In Proc." ></td>
	<td class="line x" title="364:450	of Web as Corpus Workshop</booktitle> <location>Louvain-laNeuve</location> <contexts> <context>e and Ringlstetter, 2007)." ></td>
	<td class="line x" title="365:450	Others yet used the functional styles belonging to the Russian linguistic tradition (Braslavski, 2007), or used functional classes derived from the lexicographic tradition (Sharoff, 2007b)." ></td>
	<td class="line x" title="366:450	While the authors cited so far created their collections using the individual web page as the main unit of analysis, another line of research proposes to investigate genres at the level of website</context> <context>L files per class, in English, collected in 2005/2006." ></td>
	<td class="line x" title="367:450	 The 20-Genre Collection (Vidulin et al., 2007)." ></td>
	<td class="line x" title="368:450	 The Corpus of 400 blog posts (Tavosanis, 2007)." ></td>
	<td class="line x" title="369:450	 The English and Russian web genre corpora (Sharoff, 2007a), including manually checked samples of 250 pages for each of the two languages, as well as predicted classes produced by SVM-based classifiers (65,177 pages for English, 29,650 for Russian)." ></td>
	<td class="line x" title="370:450	 The </context> </contexts> <marker>Sharoff, 2007</marker> <rawString>S. Sharoff." ></td>
	<td class="line x" title="371:450	2007a." ></td>
	<td class="line x" title="372:450	Classifying Web Corpora into Domain and Genre Using Automatic Feature Identification." ></td>
	<td class="line x" title="373:450	In Proc." ></td>
	<td class="line x" title="374:450	of Web as Corpus Workshop, Louvain-laNeuve, September.</rawString> </citation> <citation valid='true'> <authors> <author>S Sharoff</author> </authors> <title>2007b." ></td>
	<td class="line x" title="375:450	In the Garden and in the Jungle: Comparing Genres in the BNC and Internet</title> <date></date> <booktitle>Proc." ></td>
	<td class="line x" title="376:450	of the Colloquium Towards a Reference Corpus of Web Genres</booktitle> <editor>In M. Santini and S. Sharoff, editors</editor> <location>Birmingham, UK</location> <marker>Sharoff, </marker> <rawString>S. Sharoff." ></td>
	<td class="line x" title="377:450	2007b." ></td>
	<td class="line x" title="378:450	In the Garden and in the Jungle: Comparing Genres in the BNC and Internet." ></td>
	<td class="line x" title="379:450	In M. Santini and S. Sharoff, editors, Proc." ></td>
	<td class="line x" title="380:450	of the Colloquium Towards a Reference Corpus of Web Genres, Birmingham, UK, July.</rawString> </citation> <citation valid='true'> <authors> <author>A Stubbe</author> <author>C Ringlstetter</author> </authors> <title>Recognizing Genres</title> <date>2007</date> <booktitle>Proc." ></td>
	<td class="line x" title="381:450	of the Colloquium Towards a Reference Corpus of Web Genres</booktitle> <editor>In M. Santini and S. Sharoff, editors</editor> <location>Birmingham, UK</location> <contexts> <context>gory sets of web genres." ></td>
	<td class="line x" title="382:450	Genres can be analysed at various level of granularity." ></td>
	<td class="line x" title="383:450	Some researches focused on super-genres and thematic classes (Vidulin et al., 2007), or created hierarchies of genres (Stubbe and Ringlstetter, 2007)." ></td>
	<td class="line x" title="384:450	Others yet used the functional styles belonging to the Russian linguistic tradition (Braslavski, 2007), or used functional classes derived from the lexicographic tradition (Sharoff, 2007b)." ></td>
	<td class="line x" title="385:450	While t</context> <context>genres annotated with objective sources (Santini, 2006); (c) the SPIRIT collection (Sanderson and Joho, 2004), which contains random and unclassified web pages." ></td>
	<td class="line x" title="386:450	 The Hierachical Webgenre Collection (Stubbe and Ringlstetter, 2007; Stubbe et al., 2007b), containing 32 genre classes, 40 HTML files per class, in English, collected in 2005/2006." ></td>
	<td class="line x" title="387:450	 The 20-Genre Collection (Vidulin et al., 2007)." ></td>
	<td class="line x" title="388:450	 The Corpus of 400 blog posts (Tav</context> </contexts> <marker>Stubbe, Ringlstetter, 2007</marker> <rawString>A. Stubbe and C. Ringlstetter." ></td>
	<td class="line x" title="389:450	2007." ></td>
	<td class="line x" title="390:450	Recognizing Genres." ></td>
	<td class="line x" title="391:450	In M. Santini and S. Sharoff, editors, Proc." ></td>
	<td class="line x" title="392:450	of the Colloquium Towards a Reference Corpus of Web Genres, Birmingham, UK, July.</rawString> </citation> <citation valid='true'> <authors> <author>A Stubbe</author> <author>C Ringlstetter</author> <author>R Goebel</author> </authors> <title>Elements of a Learning Interface for Genre Qualified Search</title> <date>2007</date> <booktitle>Proc." ></td>
	<td class="line x" title="393:450	of the Int." ></td>
	<td class="line x" title="394:450	Workshop Towards Genre-Enabled Search Engines</booktitle> <pages>21--28</pages> <editor>In G. Rehm and M. Santini, editors</editor> <contexts> <context>llections; Image collections; Simple tables/lists; Input pages; Journalistic materials; Research reports; Official materials; Informative materials; FAQs; Discussions; Product specifications; Others (Stubbe et al., 2007a) Journalism (Commentary; Review; Portrait; Marginal Note; Interview; News; Feature Story; Reportage); Literature (Poem; Prose; Drama); Information (Science Report; Explanation; Recipe; FAQ; Lexicon;</context> <context> sources (Santini, 2006); (c) the SPIRIT collection (Sanderson and Joho, 2004), which contains random and unclassified web pages." ></td>
	<td class="line x" title="395:450	 The Hierachical Webgenre Collection (Stubbe and Ringlstetter, 2007; Stubbe et al., 2007b), containing 32 genre classes, 40 HTML files per class, in English, collected in 2005/2006." ></td>
	<td class="line x" title="396:450	 The 20-Genre Collection (Vidulin et al., 2007)." ></td>
	<td class="line x" title="397:450	 The Corpus of 400 blog posts (Tavosanis, 2007)." ></td>
	<td class="line x" title="398:450	 The </context> </contexts> <marker>Stubbe, Ringlstetter, Goebel, 2007</marker> <rawString>A. Stubbe, C. Ringlstetter, and R. Goebel." ></td>
	<td class="line x" title="399:450	2007a." ></td>
	<td class="line x" title="400:450	Elements of a Learning Interface for Genre Qualified Search." ></td>
	<td class="line x" title="401:450	In G. Rehm and M. Santini, editors, Proc." ></td>
	<td class="line x" title="402:450	of the Int." ></td>
	<td class="line x" title="403:450	Workshop Towards Genre-Enabled Search Engines, pages 2128.</rawString> </citation> <citation valid='true'> <authors> <author>2007b</author> </authors> <title>GenretoClassifyNoiseNoise to Classify Genre</title> <booktitle>In Proc." ></td>
	<td class="line x" title="404:450	of the IJCAI-2007 Workshop on Analytics for Noisy Unstructured Text Data</booktitle> <location>Hyderabad, India</location> <marker>2007b, </marker> <rawString>A.Stubbe, C.Ringlstetter, andK.U.Schulz." ></td>
	<td class="line x" title="405:450	2007b." ></td>
	<td class="line x" title="406:450	GenretoClassifyNoiseNoise to Classify Genre." ></td>
	<td class="line x" title="407:450	In Proc." ></td>
	<td class="line x" title="408:450	of the IJCAI-2007 Workshop on Analytics for Noisy Unstructured Text Data, Hyderabad, India.</rawString> </citation> <citation valid='true'> <authors> <author>S Symonenko</author> </authors> <title>Recognizing Genre-Like Regularities in Website Content Structure</title> <date>2007</date> <booktitle>Proc." ></td>
	<td class="line x" title="409:450	of the Int." ></td>
	<td class="line x" title="410:450	Workshop Towards Genre-Enabled Search Engines</booktitle> <pages>29--36</pages> <editor>In G. Rehm and M. Santini, editors</editor> <contexts> <context>While the authors cited so far created their collections using the individual web page as the main unit of analysis, another line of research proposes to investigate genres at the level of websites." ></td>
	<td class="line x" title="411:450	(Symonenko, 2007), for instance, identifies genre-like regularities in the structure of commercial and educational websites; (Bjrneborn, 2008) examines nine institutional and eight personal meta-genres in university</context> <context>or the web genre modules/components that occur on the intradocument level, we need a third category set, because web genres can be instantiated on the level of whole websites (Mehler and Gleim, 2006; Symonenko, 2007)." ></td>
	<td class="line x" title="412:450	Ideally, conventionalised connections between these three levels should be represented within the category sets (for example, thataconference websitecontains,amongothers, acall for papers, and a sc</context> </contexts> <marker>Symonenko, 2007</marker> <rawString>S. Symonenko." ></td>
	<td class="line x" title="413:450	2007." ></td>
	<td class="line x" title="414:450	Recognizing Genre-Like Regularities in Website Content Structure." ></td>
	<td class="line x" title="415:450	In G. Rehm and M. Santini, editors, Proc." ></td>
	<td class="line x" title="416:450	of the Int." ></td>
	<td class="line x" title="417:450	Workshop Towards Genre-Enabled Search Engines, pages 2936.</rawString> </citation> <citation valid='true'> <authors> <author>M Tavosanis</author> </authors> <title>Juvenile Netspeak and Subgenre Classification Issues in Italian Blogs</title> <date>2007</date> <booktitle>Proc." ></td>
	<td class="line x" title="418:450	of the Int." ></td>
	<td class="line x" title="419:450	Workshop Towards Genre-Enabled Search Engines</booktitle> <pages>37--43</pages> <editor>In G. Rehm and M. Santini, editors</editor> <contexts> <context>cademic home page), to complex networks, such as, for example, wikis." ></td>
	<td class="line x" title="420:450	While some researchers focused on automatic classification, e.g., (Kennedy and Shepherd, 2005) or the analysis of a single genre (Tavosanis, 2007), others built corpora such as the one by (Kim andRoss, 2007a) that includes70 genresidentified in a large collection of PDF documents; finally, several researchers are interested in fine-grained gen</context> <context>007; Stubbe et al., 2007b), containing 32 genre classes, 40 HTML files per class, in English, collected in 2005/2006." ></td>
	<td class="line x" title="421:450	 The 20-Genre Collection (Vidulin et al., 2007)." ></td>
	<td class="line x" title="422:450	 The Corpus of 400 blog posts (Tavosanis, 2007)." ></td>
	<td class="line x" title="423:450	 The English and Russian web genre corpora (Sharoff, 2007a), including manually checked samples of 250 pages for each of the two languages, as well as predicted classes produced by SVM-based class</context> </contexts> <marker>Tavosanis, 2007</marker> <rawString>M. Tavosanis." ></td>
	<td class="line x" title="424:450	2007." ></td>
	<td class="line x" title="425:450	Juvenile Netspeak and Subgenre Classification Issues in Italian Blogs." ></td>
	<td class="line x" title="426:450	In G. Rehm and M. Santini, editors, Proc." ></td>
	<td class="line x" title="427:450	of the Int." ></td>
	<td class="line x" title="428:450	Workshop Towards Genre-Enabled Search Engines, pages 3743.</rawString> </citation> <citation valid='true'> <authors> <author>V Vidulin</author> <author>M Lutrek</author> <author>M Gams</author> </authors> <title>Using Genres to Improve Search Engines</title> <date>2007</date> <booktitle>Proc." ></td>
	<td class="line x" title="429:450	of the Int." ></td>
	<td class="line x" title="430:450	Workshop Towards Genre-Enabled Search Engines</booktitle> <pages>45--51</pages> <editor>In G. Rehm and M. Santini, editors</editor> <contexts> <context>heir annotation criteria or the composition of their category sets of web genres." ></td>
	<td class="line x" title="431:450	Genres can be analysed at various level of granularity." ></td>
	<td class="line x" title="432:450	Some researches focused on super-genres and thematic classes (Vidulin et al., 2007), or created hierarchies of genres (Stubbe and Ringlstetter, 2007)." ></td>
	<td class="line x" title="433:450	Others yet used the functional styles belonging to the Russian linguistic tradition (Braslavski, 2007), or used functional classes </context> <context>al Dictionary; Presentation; Statistics; Code); Documentation (Law; Official Report; Protocol); Directory (Person; Catalog; Resources; Timeline); Communcation (Mail/Talk; Forum; Blog; Form); Nothing (Vidulin et al., 2007) Pornographic; Blog; Childrens; Commercial/Promotional; Community; Content Delivery; Entertainment; Error Message; FAQ; Gateway; Index; Informative; Journalistic; Official; Personal; Poetry; Prose F</context> <context>achical Webgenre Collection (Stubbe and Ringlstetter, 2007; Stubbe et al., 2007b), containing 32 genre classes, 40 HTML files per class, in English, collected in 2005/2006." ></td>
	<td class="line x" title="434:450	 The 20-Genre Collection (Vidulin et al., 2007)." ></td>
	<td class="line x" title="435:450	 The Corpus of 400 blog posts (Tavosanis, 2007)." ></td>
	<td class="line x" title="436:450	 The English and Russian web genre corpora (Sharoff, 2007a), including manually checked samples of 250 pages for each of the two languages, as wel</context> </contexts> <marker>Vidulin, Lutrek, Gams, 2007</marker> <rawString>V. Vidulin, M. Lutrek, and M. Gams." ></td>
	<td class="line x" title="437:450	2007." ></td>
	<td class="line x" title="438:450	Using Genres to Improve Search Engines." ></td>
	<td class="line x" title="439:450	In G. Rehm and M. Santini, editors, Proc." ></td>
	<td class="line x" title="440:450	of the Int." ></td>
	<td class="line x" title="441:450	Workshop Towards Genre-Enabled Search Engines, pages 4551.</rawString> </citation> <citation valid='true'> <authors> <author>J Xu</author> <author>Y Cao</author> <author>H Li</author> <author>N Craswell</author> <author>Y Huang</author> </authors> <title>Searching Documents Based on Relevance and Type</title> <date>2007</date> <booktitle>Proc." ></td>
	<td class="line x" title="442:450	of the 29th European Conf." ></td>
	<td class="line x" title="443:450	on IR Research (ECIR 2007</booktitle> <pages>629--636</pages> <editor>In G. Amati, C. Carpineto, and G. Romano, editors</editor> <contexts> <context>its accuracy, so that queries such as academic papers about global warming could filter out texts of other genres." ></td>
	<td class="line x" title="444:450	Preliminary results in genre-enabled IR were reported by (Karlgren et al., 1998)." ></td>
	<td class="line x" title="445:450	(Xu et al., 2007), (Yeung et al., 2007) and nearly all other approaches since the seminal papers by (Karlgren and Cutting, 1994) and (Kessler et al., 1997) suffer from the same shortcomings: genre category sets are b</context> </contexts> <marker>Xu, Cao, Li, Craswell, Huang, 2007</marker> <rawString>J. Xu, Y. Cao, H. Li, N. Craswell, and Y. Huang." ></td>
	<td class="line x" title="446:450	2007." ></td>
	<td class="line x" title="447:450	Searching Documents Based on Relevance and Type." ></td>
	<td class="line x" title="448:450	In G. Amati, C. Carpineto, and G. Romano, editors, Proc." ></td>
	<td class="line x" title="449:450	of the 29th European Conf." ></td>
	<td class="line x" title="450:450	on IR Research (ECIR 2007), pages 629636.</rawString> </citation> </citationList> </algorithm>" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-1034
When Specialists and Generalists Work Together: Overcoming Domain Dependence in Sentiment Tagging
Andreevskaia, Alina;Bergler, Sabine;"></td>
	<td class="line x" title="1:177	Proceedings of ACL-08: HLT, pages 290298, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:177	c2008 Association for Computational Linguistics When Specialists and Generalists Work Together: Overcoming Domain Dependence in Sentiment Tagging Alina Andreevskaia Concordia University Montreal, Quebec andreev@cs.concordia.ca Sabine Bergler Concordia University Montreal, Canada bergler@cs.concordia.ca Abstract This study presents a novel approach to the problem of system portability across different domains: a sentiment annotation system that integrates a corpus-based classifier trained on a small set of annotated in-domain data and a lexicon-based system trained on WordNet." ></td>
	<td class="line x" title="3:177	The paper explores the challenges of system portability across domains and text genres (movie reviews, news, blogs, and product reviews), highlights the factors affecting system performance on out-of-domain and smallset in-domain data, and presents a new system consisting of the ensemble of two classifiers with precision-based vote weighting, that provides significant gains in accuracy and recall over the corpus-based classifier and the lexicon-based system taken individually." ></td>
	<td class="line x" title="4:177	1 Introduction One of the emerging directions in NLP is the development of machine learning methods that perform well not only on the domain on which they were trained, but also on other domains, for which training data is not available or is not sufficient to ensure adequate machine learning." ></td>
	<td class="line x" title="5:177	Many applications require reliable processing of heterogeneous corpora, such as the World Wide Web, where the diversity of genres and domains present in the Internet limits the feasibility of in-domain training." ></td>
	<td class="line x" title="6:177	In this paper, sentiment annotation is defined as the assignment of positive, negative or neutral sentiment values to texts, sentences, and other linguistic units." ></td>
	<td class="line x" title="7:177	Recent experiments assessing system portability across different domains, conducted by Aue and Gamon (2005), demonstrated that sentiment annotation classifiers trained in one domain do not perform well on other domains." ></td>
	<td class="line x" title="8:177	A number of methods has been proposed in order to overcome this system portability limitation by using out-of-domain data, unlabelled in-domain corpora or a combination of in-domain and out-of-domain examples (Aue and Gamon, 2005; Bai et al., 2005; Drezde et al., 2007; Tan et al., 2007)." ></td>
	<td class="line x" title="9:177	In this paper, we present a novel approach to the problem of system portability across different domains by developing a sentiment annotation system that integrates a corpus-based classifier with a lexicon-based system trained on WordNet." ></td>
	<td class="line x" title="10:177	By adopting this approach, we sought to develop a system that relies on both general and domainspecific knowledge, as humans do when analyzing a text." ></td>
	<td class="line x" title="11:177	The information contained in lexicographical sources, such as WordNet, reflects a lay persons general knowledge about the world, while domainspecific knowledge can be acquired through classifier training on a small set of in-domain data." ></td>
	<td class="line x" title="12:177	The first part of this paper reviews the extant literature on domain adaptation in sentiment analysis and highlights promising directions for research." ></td>
	<td class="line x" title="13:177	The second part establishes a baseline for system evaluation by drawing comparisons of system performance across four different domains/genres movie reviews, news, blogs, and product reviews." ></td>
	<td class="line x" title="14:177	The final, third part of the paper presents our system, composed of an ensemble of two classifiers  one trained on WordNet glosses and synsets and the other trained on a small in-domain training set." ></td>
	<td class="line x" title="15:177	290 2 Domain Adaptation in Sentiment Research Most text-level sentiment classifiers use standard machine learning techniques to learn and select features from labeled corpora." ></td>
	<td class="line x" title="16:177	Such approaches work well in situations where large labeled corpora are available for training and validation (e.g., movie reviews), but they do not perform well when training data is scarce or when it comes from a different domain (Aue and Gamon, 2005; Read, 2005), topic (Read, 2005) or time period (Read, 2005)." ></td>
	<td class="line x" title="17:177	There are two alternatives to supervised machine learning that can be used to get around this problem: on the one hand, general lists of sentiment clues/features can be acquired from domain-independent sources such as dictionaries or the Internet, on the other hand, unsupervised and weakly-supervised approaches can be used to take advantage of a small number of annotated in-domain examples and/or of unlabelled indomain data." ></td>
	<td class="line x" title="18:177	The first approach, using general word lists automatically acquired from the Internet or from dictionaries, outperforms corpus-based classifiers when such classifiers use out-of-domain training data or when the training corpus is not sufficiently large to accumulate the necessary feature frequency information." ></td>
	<td class="line oc" title="19:177	But such general word lists were shown to perform worse than statistical models built on sufficiently large in-domain training sets of movie reviews (Pang et al., 2002)." ></td>
	<td class="line x" title="20:177	On other domains, such as product reviews, the performance of systems that use general word lists is comparable to the performance of supervised machine learning approaches (Gamon and Aue, 2005)." ></td>
	<td class="line x" title="21:177	The recognition of major performance deficiencies of supervised machine learning methods with insufficient or out-of-domain training brought about an increased interest in unsupervised and weaklysupervised approaches to feature learning." ></td>
	<td class="line x" title="22:177	For instance, Aue and Gamon (2005) proposed training on a samll number of labeled examples and large quantities of unlabelled in-domain data." ></td>
	<td class="line x" title="23:177	This system performed well even when compared to systems trained on a large set of in-domain examples: on feedback messages from a web survey on knowledge bases, Aue and Gamon report 73.86% accuracy using unlabelled data compared to 77.34% for in-domain and 72.39% for the best out-of-domain training on a large training set." ></td>
	<td class="line x" title="24:177	Drezde et al.(2007) applied structural correspondence learning (Drezde et al., 2007) to the task of domain adaptation for sentiment classification of product reviews." ></td>
	<td class="line x" title="26:177	They showed that, depending on the domain, a small number (e.g., 50) of labeled examples allows to adapt the model learned on another corpus to a new domain." ></td>
	<td class="line x" title="27:177	However, they note that the success of such adaptation and the number of necessary in-domain examples depends on the similarity between the original domain and the new one." ></td>
	<td class="line x" title="28:177	Similarly, Tan et al.(2007) suggested to combine out-of-domain labeled examples with unlabelled ones from the target domain in order to solve the domain-transfer problem." ></td>
	<td class="line x" title="30:177	They applied an outof-domain-trained SVM classifier to label examples from the target domain and then retrained the classifier using these new examples." ></td>
	<td class="line x" title="31:177	In order to maximize the utility of the examples from the target domain, these examples were selected using Similarity Ranking and Relative Similarity Ranking algorithms (Tan et al., 2007)." ></td>
	<td class="line x" title="32:177	Depending on the similarity between domains, this method brought up to 15% gain compared to the baseline SVM." ></td>
	<td class="line x" title="33:177	Overall, the development of semi-supervised approaches to sentiment tagging is a promising direction of the research in this area but so far, based on reported results, the performance of such methods is inferior to the supervised approaches with indomain training and to the methods that use general word lists." ></td>
	<td class="line x" title="34:177	It also strongly depends on the similarity between the domains as has been shown by (Drezde et al., 2007; Tan et al., 2007)." ></td>
	<td class="line x" title="35:177	3 Factors Affecting System Performance The comparison of system performance across different domains involves a number of factors that can significantly affect system performance  from training set size to level of analysis (sentence or entire document), document domain/genre and many other factors." ></td>
	<td class="line x" title="36:177	In this section we present a series of experiments conducted to assess the effects of different external factors (i.e., factors unrelated to the merits of the system itself) on system performance in order to establish the baseline for performance comparisons across different domains/genres." ></td>
	<td class="line oc" title="37:177	291 3.1 Level of Analysis Research on sentiment annotation is usually conducted at the text (Aue and Gamon, 2005; Pang et al., 2002; Pang and Lee, 2004; Riloff et al., 2006; Turney, 2002; Turney and Littman, 2003) or at the sentence levels (Gamon and Aue, 2005; Hu and Liu, 2004; Kim and Hovy, 2005; Riloff et al., 2006)." ></td>
	<td class="line o" title="38:177	It should be noted that each of these levels presents different challenges for sentiment annotation." ></td>
	<td class="line x" title="39:177	For example, it has been observed that texts often contain multiple opinions on different topics (Turney, 2002; Wiebe et al., 2001), which makes assignment of the overall sentiment to the whole document problematic." ></td>
	<td class="line x" title="40:177	On the other hand, each individual sentence contains a limited number of sentiment clues, which often negatively affects the accuracy and recall if that single sentiment clue encountered in the sentence was not learned by the system." ></td>
	<td class="line x" title="41:177	Since the comparison of sentiment annotation system performance on texts and on sentences has not been attempted to date, we also sought to close this gap in the literature by conducting the first set of our comparative experiments on data sets of 2,002 movie review texts and 10,662 movie review snippets (5331 with positive and 5331 with negative sentiment) provided by Bo Pang (http://www.cs.cornell.edu/People/pabo/moviereview-data/)." ></td>
	<td class="line x" title="42:177	3.2 Domain Effects The second set of our experiments explores system performance on different domains at sentence level." ></td>
	<td class="line x" title="43:177	For this we used four different data sets of sentences annotated with sentiment tags:  A set of movie review snippets (further: movie) from (Pang and Lee, 2005)." ></td>
	<td class="line x" title="44:177	This dataset of 10,662 snippets was collected automatically from www.rottentomatoes.com website." ></td>
	<td class="line x" title="45:177	All sentences in reviews marked rotten were considered negative and snippets from fresh reviews were deemed positive." ></td>
	<td class="line x" title="46:177	In order to make the results obtained on this dataset comparable to other domains, a randomly selected subset of 1066 snippets was used in the experiments." ></td>
	<td class="line x" title="47:177	 A balanced corpus of 800 manually annotated sentences extracted from 83 newspaper texts (further, news)." ></td>
	<td class="line x" title="48:177	The full set of sentences was annotated by one judge." ></td>
	<td class="line x" title="49:177	200 sentences from this corpus (100 positive and 100 negative) were also randomly selected from the corpus for an inter-annotator agreement study and were manually annotated by two independent annotators." ></td>
	<td class="line x" title="50:177	The pairwise agreement between annotators was calculated as the percent of same tags divided by the number of sentences with this tag in the gold standard." ></td>
	<td class="line x" title="51:177	The pair-wise agreement between the three annotators ranged from 92.5 to 95.9% (=0.74 and 0.75 respectively) on positive vs. negative tags." ></td>
	<td class="line x" title="52:177	 A set of sentences taken from personal weblogs (further, blogs) posted on LiveJournal (http://www.livejournal.com) and on http://www.cyberjournalist.com." ></td>
	<td class="line x" title="53:177	This corpus is composed of 800 sentences (400 sentences with positive and 400 sentences with negative sentiment)." ></td>
	<td class="line x" title="54:177	In order to establish the interannotator agreement, two independent judges were asked to annotate 200 sentences from this corpus." ></td>
	<td class="line x" title="55:177	The agreement between the two annotators on positive vs. negative tags reached 99% (=0.97)." ></td>
	<td class="line x" title="56:177	 A set of 1200 product review (PR) sentences extracted from the annotated corpus made available by Bing Liu (Hu and Liu, 2004) (http://www.cs.uic.edu/ liub/FBS/FBS.html)." ></td>
	<td class="line x" title="57:177	The data set sizes are summarized in Table 1." ></td>
	<td class="line x" title="58:177	Movies News Blogs PR Text level 2002 texts n/a n/a n/a Sentence level 10662 800 800 1200 snippets sent." ></td>
	<td class="line x" title="59:177	sent." ></td>
	<td class="line x" title="60:177	sent." ></td>
	<td class="line x" title="61:177	Table 1: Datasets 3.3 Establishing a Baseline for a Corpus-based System (CBS) Supervised statistical methods have been very successful in sentiment tagging of texts: on movie review texts they reach accuracies of 85-90% (Aue and Gamon, 2005; Pang and Lee, 2004)." ></td>
	<td class="line x" title="62:177	These methods perform particularly well when a large volume of labeled data from the same domain as the 292 test set is available for training (Aue and Gamon, 2005)." ></td>
	<td class="line x" title="63:177	For this reason, most of the research on sentiment tagging using statistical classifiers was limited to product and movie reviews, where review authors usually indicate their sentiment in a form of a standardized score that accompanies the texts of their reviews." ></td>
	<td class="line x" title="64:177	The lack of sufficient data for training appears to be the main reason for the virtual absence of experiments with statistical classifiers in sentiment tagging at the sentence level." ></td>
	<td class="line x" title="65:177	To our knowledge, the only work that describes the application of statistical classifiers (SVM) to sentence-level sentiment classification is (Gamon and Aue, 2005)1." ></td>
	<td class="line x" title="66:177	The average performance of the system on ternary classification (positive, negative, and neutral) was between 0.50 and 0.52 for both average precision and recall." ></td>
	<td class="line x" title="67:177	The results reported by (Riloff et al., 2006) for binary classification of sentences in a related domain of subjectivity tagging (i.e., the separation of sentiment-laden from neutral sentences) suggest that statistical classifiers can perform well on this task: the authors have reached 74.9% accuracy on the MPQA corpus (Riloff et al., 2006)." ></td>
	<td class="line x" title="68:177	In order to explore the performance of different approaches in sentiment annotation at the text and sentence levels, we used a basic Nave Bayes classifier." ></td>
	<td class="line x" title="69:177	It has been shown that both Nave Bayes and SVMs perform with similar accuracy on different sentiment tagging tasks (Pang and Lee, 2004)." ></td>
	<td class="line x" title="70:177	These observations were confirmed with our own experiments with SVMs and Nave Bayes (Table 3)." ></td>
	<td class="line x" title="71:177	We used the Weka package (http://www.cs.waikato.ac.nz/ml/weka/) with default settings." ></td>
	<td class="line x" title="72:177	In the sections that follow, we describe a set of comparative experiments with SVMs and Nave Bayes classifiers (1) on texts and sentences and (2) on four different domains (movie reviews, news, blogs, and product reviews)." ></td>
	<td class="line x" title="73:177	System runs with unigrams, bigrams, and trigrams as features and with different training set sizes are presented." ></td>
	<td class="line x" title="74:177	1Recently, a similar task has been addressed by the Affective Text Task at SemEval-1 where even shorter units  headlines  were classified into positive, negative and neutral categories using a variety of techniques (Strapparava and Mihalcea, 2007)." ></td>
	<td class="line x" title="75:177	4 Experiments 4.1 System Performance on Texts vs. Sentences The experiments comparing in-domain trained system performance on texts vs. sentences were conducted on 2,002 movie review texts and on 10,662 movie review snippets." ></td>
	<td class="line x" title="76:177	The results with 10-fold cross-validation are reported in Table 22." ></td>
	<td class="line x" title="77:177	Trained on Texts Trained on Sent." ></td>
	<td class="line x" title="78:177	Tested on Tested on Tested on Tested on Texts Sent." ></td>
	<td class="line x" title="79:177	Texts Sent." ></td>
	<td class="line x" title="80:177	1gram 81.1 69.0 66.8 77.4 2gram 83.7 68.6 71.2 73.9 3gram 82.5 64.1 70.0 65.4 Table 2: Accuracy of Nave Bayes on movie reviews." ></td>
	<td class="line x" title="81:177	Consistent with findings in the literature (Cui et al., 2006; Dave et al., 2003; Gamon and Aue, 2005), on the large corpus of movie review texts, the indomain-trained system based solely on unigrams had lower accuracy than the similar system trained on bigrams." ></td>
	<td class="line x" title="82:177	But the trigrams fared slightly worse than bigrams." ></td>
	<td class="line x" title="83:177	On sentences, however, we have observed an inverse pattern: unigrams performed better than bigrams and trigrams." ></td>
	<td class="line x" title="84:177	These results highlight a special property of sentence-level annotation: greater sensitivity to sparseness of the model: On texts, classifier error on one particular sentiment marker is often compensated by a number of correctly identified other sentiment clues." ></td>
	<td class="line x" title="85:177	Since sentences usually contain a much smaller number of sentiment clues than texts, sentence-level annotation more readily yields errors when a single sentiment clue is incorrectly identified or missed by the system." ></td>
	<td class="line x" title="86:177	Due to lower frequency of higher-order n-grams (as opposed to unigrams), higher-order ngram language models are more sparse, which increases the probability of missing a particular sentiment marker in a sentence (Table 33)." ></td>
	<td class="line x" title="87:177	Very large 2All results are statistically significant at = 0.01 with two exceptions: the difference between trigrams and bigrams for the system trained and tested on texts is statistically significant at alpha=0.1 and for the system trained on sentences and tested on texts is not statistically significant at  = 0.01." ></td>
	<td class="line x" title="88:177	3The results for movie reviews are lower than those reported in Table 2 since the dataset is 10 times smaller, which results in less accurate classification." ></td>
	<td class="line x" title="89:177	The statistical significance of the 293 training sets are required to overcome this higher ngram sparseness in sentence-level annotation." ></td>
	<td class="line x" title="90:177	Dataset Movie News Blogs PRs Dataset size 1066 800 800 1200 unigrams SVM 68.5 61.5 63.85 76.9 NB 60.2 59.5 60.5 74.25 nb features 5410 4544 3615 2832 bigrams SVM 59.9 63.2 61.5 75.9 NB 57.0 58.4 59.5 67.8 nb features 16286 14633 15182 12951 trigrams SVM 54.3 55.4 52.7 64.4 NB 53.3 57.0 56.0 69.7 nb features 20837 18738 19847 19132 Table 3: Accuracy of unigram, bigram and trigram models across domains." ></td>
	<td class="line x" title="91:177	4.2 System Performance on Different Domains In the second set of experiments we sought to compare system results on sentences using in-domain and out-of-domain training." ></td>
	<td class="line x" title="92:177	Table 4 shows that indomain training, as expected, consistently yields superior accuracy than out-of-domain training across all four datasets: movie reviews (Movies), news, blogs, and product reviews (PRs)." ></td>
	<td class="line x" title="93:177	The numbers for in-domain trained runs are highlighted in bold." ></td>
	<td class="line x" title="94:177	Test Data Training Data Movies News Blogs PRs Movies 68.5 55.2 53.2 60.7 News 55.0 61.5 56.25 57.4 Blogs 53.7 49.9 63.85 58.8 PRs 55.8 55.9 56.25 76.9 Table 4: Accuracy of SVM with unigram model results depends on the genre and size of the n-gram: on product reviews, all results are statistically significant at  = 0.025 level; on movie reviews, the difference between Nave Bayes and SVM is statistically significant at  = 0.01 but the significance diminishes as the size of the n-gram increases; on news, only bi-grams produce a statistically significant ( = 0.01) difference between the two machine learning methods, while on blogs the difference between SVMs and Nave Bayes is most pronounced when unigrams are used ( = 0.025)." ></td>
	<td class="line x" title="95:177	It is interesting to note that on sentences, regardless of the domain used in system training and regardless of the domain used in system testing, unigrams tend to perform better than higher-order ngrams." ></td>
	<td class="line x" title="96:177	This observation suggests that, given the constraints on the size of the available training sets, unigram-based systems may be better suited for sentence-level sentiment annotation." ></td>
	<td class="line x" title="97:177	5 Lexicon-Based Approach The search for a base-learner that can produce greatest synergies with a classifier trained on small-set in-domain data has turned our attention to lexiconbased systems." ></td>
	<td class="line x" title="98:177	Since the benefits from combining classifiers that always make similar decisions is minimal, the two (or more) base-learners should complement each other (Alpaydin, 2004)." ></td>
	<td class="line x" title="99:177	Since a system based on a fairly different learning approach is more likely to produce a different decision under a given set of circumstances, the diversity of approaches integrated in the ensemble of classifiers was expected to have a beneficial effect on the overall system performance." ></td>
	<td class="line x" title="100:177	A lexicon-based approach capitalizes on the fact that dictionaries, such as WordNet (Fellbaum, 1998), contain a comprehensive and domainindependent set of sentiment clues that exist in general English." ></td>
	<td class="line x" title="101:177	A system trained on such general data, therefore, should be less sensitive to domain changes." ></td>
	<td class="line x" title="102:177	This robustness, however is expected to come at some cost, since some domain-specific sentiment clues may not be covered in the dictionary." ></td>
	<td class="line x" title="103:177	Our hypothesis was, therefore, that a lexiconbased system will perform worse than an in-domain trained classifier but possibly better than a classifier trained on out-of domain data." ></td>
	<td class="line x" title="104:177	One of the limitations of general lexicons and dictionaries, such as WordNet (Fellbaum, 1998), as training sets for sentiment tagging systems is that they contain only definitions of individual words and, hence, only unigrams could be effectively learned from dictionary entries." ></td>
	<td class="line x" title="105:177	Since the structure of WordNet glosses is fairly different from that of other types of corpora, we developed a system that used the list of human-annotated adjectives from (Hatzivassiloglou and McKeown, 1997) as a seed list and then learned additional unigrams 294 from WordNet synsets and glosses with up to 88% accuracy, when evaluated against General Inquirer (Stone et al., 1966) (GI) on the intersection of our automatically acquired list with GI." ></td>
	<td class="line x" title="106:177	In order to expand the list coverage for our experiments at the text and sentence levels, we then augmented the list by adding to it all the words annotated with Positiv or Negativ tags in GI, that were not picked up by the system." ></td>
	<td class="line x" title="107:177	The resulting list of features contained 11,000 unigrams with the degree of membership in the category of positive or negative sentiment assigned to each of them." ></td>
	<td class="line x" title="108:177	In order to assign the membership score to each word, we did 58 system runs on unique nonintersecting seed lists drawn from manually annotated list of positive and negative adjectives from (Hatzivassiloglou and McKeown, 1997)." ></td>
	<td class="line x" title="109:177	The 58 runs were then collapsed into a single set of 7,813 unique words." ></td>
	<td class="line x" title="110:177	For each word we computed a score by subtracting the total number of runs assigning this word a negative sentiment from the total of the runs that consider it positive." ></td>
	<td class="line x" title="111:177	The resulting measure, termed Net Overlap Score (NOS), reflected the number of ties linking a given word with other sentimentladen words in WordNet, and hence, could be used as a measure of the words centrality in the fuzzy category of sentiment." ></td>
	<td class="line x" title="112:177	The NOSs were then normalized into the interval from -1 to +1 using a sigmoid fuzzy membership function (Zadeh, 1975)4." ></td>
	<td class="line x" title="113:177	Only words with fuzzy membership degree not equal to zero were retained in the list." ></td>
	<td class="line x" title="114:177	The resulting list contained 10,809 sentiment-bearing words of different parts of speech." ></td>
	<td class="line x" title="115:177	The sentiment determination at the sentence and text level was then done by summing up the scores of all identified positive unigrams (NOS>0) and all negative unigrams (NOS<0) (Andreevskaia and Bergler, 2006)." ></td>
	<td class="line x" title="116:177	5.1 Establishing a Baseline for the Lexicon-Based System (LBS) The baseline performance of the Lexicon-Based System (LBS) described above is presented in Table 5, along with the performance results of the indomainand out-of-domain-trained SVM classifier." ></td>
	<td class="line x" title="117:177	Table 5 confirms the predicted pattern: the LBS performs with lower accuracy than in-domain4With coefficients: =1, =15." ></td>
	<td class="line x" title="118:177	Movies News Blogs PRs LBS 57.5 62.3 63.3 59.3 SVM in-dom." ></td>
	<td class="line x" title="119:177	68.5 61.5 63.85 76.9 SVM out-of-dom." ></td>
	<td class="line x" title="120:177	55.8 55.9 56.25 60.7 Table 5: System accuracy on best runs on sentences trained corpus-based classifiers, and with similar or better accuracy than the corpus-based classifiers trained on out-of-domain data." ></td>
	<td class="line x" title="121:177	Thus, the lexiconbased approach is characterized by a bounded but stable performance when the system is ported across domains." ></td>
	<td class="line x" title="122:177	These performance characteristics of corpus-based and lexicon-based approaches prompt further investigation into the possibility to combine the portability of dictionary-trained systems with the accuracy of in-domain trained systems." ></td>
	<td class="line x" title="123:177	6 Integrating the Corpus-based and Dictionary-based Approaches The strategy of integration of two or more systems in a single ensemble of classifiers has been actively used on different tasks within NLP." ></td>
	<td class="line x" title="124:177	In sentiment tagging and related areas, Aue and Gamon (2005) demonstrated that combining classifiers can be a valuable tool in domain adaptation for sentiment analysis." ></td>
	<td class="line x" title="125:177	In the ensemble of classifiers, they used a combination of nine SVM-based classifiers deployed to learn unigrams, bigrams, and trigrams on three different domains, while the fourth domain was used as an evaluation set." ></td>
	<td class="line x" title="126:177	Using then an SVM meta-classifier trained on a small number of target domain examples to combine the nine base classifiers, they obtained a statistically significant improvement on out-of-domain texts from book reviews, knowledge-base feedback, and product support services survey data." ></td>
	<td class="line x" title="127:177	No improvement occurred on movie reviews." ></td>
	<td class="line x" title="128:177	Pang and Lee (2004) applied two different classifiers to perform sentiment annotation in two sequential steps: the first classifier separated subjective (sentiment-laden) texts from objective (neutral) ones and then they used the second classifier to classify the subjective texts into positive and negative." ></td>
	<td class="line x" title="129:177	Das and Chen (2004) used five classifiers to determine market sentiment on Yahoo!" ></td>
	<td class="line x" title="130:177	postings." ></td>
	<td class="line x" title="131:177	Simple majority vote was applied to make decisions within 295 the ensemble of classifiers and achieved accuracy of 62% on ternary in-domain classification." ></td>
	<td class="line x" title="132:177	In this study we describe a system that attempts to combine the portability of a dictionary-trained system (LBS) with the accuracy of an in-domain trained corpus-based system (CBS)." ></td>
	<td class="line x" title="133:177	The selection of these two classifiers for this system, thus, was theorybased." ></td>
	<td class="line x" title="134:177	The section that follows describes the classifier integration and presents the performance results of the system consisting of an ensemble CBS and LBS classifier and a precision-based vote weighting procedure." ></td>
	<td class="line x" title="135:177	6.1 The Classifier Integration Procedure and System Evaluation The comparative analysis of the corpus-based and lexicon-based systems described above revealed that the errors produced by CBS and LBS were to a great extent complementary (i.e., where one classifier makes an error, the other tends to give the correct answer)." ></td>
	<td class="line x" title="136:177	This provided further justification to the integration of corpus-based and lexicon-based approaches in a single system." ></td>
	<td class="line x" title="137:177	Table 6 below illustrates the complementarity of the performance CBS and LBS classifiers on the positive and negative categories." ></td>
	<td class="line x" title="138:177	In this experiment, the corpus-based classifier was trained on 400 annotated product review sentences5." ></td>
	<td class="line x" title="139:177	The two systems were then evaluated on a test set of another 400 product review sentences." ></td>
	<td class="line x" title="140:177	The results reported in Table 6 are statistically significant at  = 0.01." ></td>
	<td class="line x" title="141:177	CBS LBS Precision positives 89.3% 69.3% Precision negatives 55.5% 81.5% Pos/Neg Precision 58.0% 72.1% Table 6: Base-learners precision and recall on product reviews on test data." ></td>
	<td class="line x" title="142:177	Table 6 shows that the corpus-based system has a very good precision on those sentences that it classifies as positive but makes a lot of errors on those sentences that it deems negative." ></td>
	<td class="line x" title="143:177	At the same time, the lexicon-based system has low precision on positives 5The small training set explains relatively low overall performance of the CBS system." ></td>
	<td class="line x" title="144:177	and high precision on negatives6." ></td>
	<td class="line x" title="145:177	Such complementary distribution of errors produced by the two systems was observed on different data sets from different domains, which suggests that the observed distribution pattern reflects the properties of each of the classifiers, rather than the specifics of the domain/genre." ></td>
	<td class="line x" title="146:177	In order to take advantage of the observed complementarity of the two systems, the following procedure was used." ></td>
	<td class="line x" title="147:177	First, a small set of in-domain data was used to train the CBS system." ></td>
	<td class="line x" title="148:177	Then both CBS and LBS systems were run separately on the same training set, and for each classifier, the precision measures were calculated separately for those sentences that the classifier considered positive and those it considered negative." ></td>
	<td class="line x" title="149:177	The chance-level performance (50%) was then subtracted from the precision figures to ensure that the final weights reflect by how much the classifiers precision exceeds the chance level." ></td>
	<td class="line x" title="150:177	The resulting chance-adjusted precision numbers of the two classifiers were then normalized, so that the weights of CBS and LBS classifiers sum up to 100% on positive and to 100% on negative sentences." ></td>
	<td class="line x" title="151:177	These weights were then used to adjust the contribution of each classifier to the decision of the ensemble system." ></td>
	<td class="line x" title="152:177	The choice of the weight applied to the classifier decision, thus, varied depending on whether the classifier scored a given sentence as positive or as negative." ></td>
	<td class="line x" title="153:177	The resulting system was then tested on a separate test set of sentences7." ></td>
	<td class="line x" title="154:177	The small-set training and evaluation experiments with the system were performed on different domains using 3-fold validation." ></td>
	<td class="line x" title="155:177	The experiments conducted with the Ensemble system were designed to explore system performance under conditions of limited availability of annotated data for classifier training." ></td>
	<td class="line x" title="156:177	For this reason, the numbers reported for the corpus-based classifier do not reflect the full potential of machine learning approaches when sufficient in-domain training data is available." ></td>
	<td class="line x" title="157:177	Table 7 presents the results of these experiments by domain/genre." ></td>
	<td class="line x" title="158:177	The results 6These results are consistent with an observation in (Kennedy and Inkpen, 2006), where a lexicon-based system performed with a better precision on negative than on positive texts." ></td>
	<td class="line x" title="159:177	7The size of the test set varied in different experiments due to the availability of annotated data for a particular domain." ></td>
	<td class="line x" title="160:177	296 are statistically significant at  = 0.01, except the runs on movie reviews where the difference between the LBS and Ensemble classifiers was significant at  = 0.05." ></td>
	<td class="line x" title="161:177	LBS CBS Ensemble News Acc 67.8 53.2 73.3 F 0.82 0.71 0.85 Movies Acc 54.5 53.5 62.1 F 0.73 0.72 0.77 Blogs Acc 61.2 51.1 70.9 F 0.78 0.69 0.83 PRs Acc 59.5 58.9 78.0 F 0.77 0.75 0.88 Average Acc 60.7 54.2 71.1 F 0.77 0.72 0.83 Table 7: Performance of the ensemble classifier Table 7 shows that the combination of two classifiers into an ensemble using the weighting technique described above leads to consistent improvement in system performance across all domains/genres." ></td>
	<td class="line x" title="162:177	In the ensemble system, the average gain in accuracy across the four domains was 16.9% relative to CBS and 10.3% relative to LBS." ></td>
	<td class="line x" title="163:177	Moreover, the gain in accuracy and precision was not offset by decreases in recall: the net gain in recall was 7.4% relative to CBS and 13.5% vs. LBS." ></td>
	<td class="line x" title="164:177	The ensemble system on average reached 99.1% recall." ></td>
	<td class="line x" title="165:177	The F-measure has increased from 0.77 and 0.72 for LBS and CBS classifiers respectively to 0.83 for the whole ensemble system." ></td>
	<td class="line x" title="166:177	7 Discussion The development of domain-independent sentiment determination systems poses a substantial challenge for researchers in NLP and artificial intelligence." ></td>
	<td class="line x" title="167:177	The results presented in this study suggest that the integration of two fairly different classifier learning approaches in a single ensemble of classifiers can yield substantial gains in system performance on all measures." ></td>
	<td class="line x" title="168:177	The most substantial gains occurred in recall, accuracy, and F-measure." ></td>
	<td class="line x" title="169:177	This study permits to highlight a set of factors that enable substantial performance gains with the ensemble of classifiers approach." ></td>
	<td class="line x" title="170:177	Such gains are most likely when (1) the errors made by the classifiers are complementary, i.e., where one classifier makes an error, the other tends to give the correct answer, (2) the classifier errors are not fully random and occur more often in a certain segment (or category) of classifier results, and (3) there is a way for a system to identify that low-precision segment and reduce the weights of that classifiers results on that segment accordingly." ></td>
	<td class="line x" title="171:177	The two classifiers used in this study  corpus-based and lexicon-based  provided an interesting illustration of potential performance gains associated with these three conditions." ></td>
	<td class="line x" title="172:177	The use of precision of classifier results on the positives and negatives proved to be an effective technique for classifier vote weighting within the ensemble." ></td>
	<td class="line x" title="173:177	8 Conclusion This study contributes to the research on sentiment tagging, domain adaptation, and the development of ensembles of classifiers (1) by proposing a novel approach for sentiment determination at sentence level and delineating the conditions under which greatest synergies among combined classifiers can be achieved, (2) by describing a precision-based technique for assigning differential weights to classifier results on different categories identified by the classifier (i.e., categories of positive vs. negative sentences), and (3) by proposing a new method for sentiment annotation in situations where the annotated in-domain data is scarce and insufficient to ensure adequate performance of the corpus-based classifier, which still remains the preferred choice when large volumes of annotated data are available for system training." ></td>
	<td class="line x" title="174:177	Among the most promising directions for future research in the direction laid out in this paper is the deployment of more advanced classifiers and feature selection techniques that can further enhance the performance of the ensemble of classifiers." ></td>
	<td class="line x" title="175:177	The precision-based vote weighting technique may prove to be effective also in situations, where more than two classifiers are integrated into a single system." ></td>
	<td class="line x" title="176:177	We expect that these more advanced ensemble-ofclassifiers systems would inherit the benefits of multiple complementary approaches to sentiment annotation and will be able to achieve better and more stable accuracy on in-domain, as well as on out-ofdomain data." ></td>
	<td class="line x" title="177:177	297" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-1036
A Joint Model of Text and Aspect Ratings for Sentiment Summarization
Titov, Ivan;McDonald, Ryan;"></td>
	<td class="line x" title="1:223	Proceedings of ACL-08: HLT, pages 308316, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:223	c2008 Association for Computational Linguistics A Joint Model of Text and Aspect Ratings for Sentiment Summarization Ivan Titov Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 titov@uiuc.edu Ryan McDonald Google Inc. 76 Ninth Avenue New York, NY 10011 ryanmcd@google.com Abstract Online reviews are often accompanied with numerical ratings provided by users for a set of service or product aspects." ></td>
	<td class="line x" title="3:223	We propose a statistical model which is able to discover corresponding topics in text and extract textual evidence from reviews supporting each of these aspect ratings  a fundamental problem in aspect-based sentiment summarization (Hu and Liu, 2004a)." ></td>
	<td class="line x" title="4:223	Our model achieves high accuracy, without any explicitly labeled data except the user provided opinion ratings." ></td>
	<td class="line x" title="5:223	The proposed approach is general and can be used for segmentation in other applications where sequential data is accompanied with correlated signals." ></td>
	<td class="line x" title="6:223	1 Introduction Usergeneratedcontentrepresentsauniquesourceof information in which user interface tools have facilitated the creation of an abundance of labeled content, e.g., topics in blogs, numerical product and service ratings in user reviews, and helpfulness rankings in online discussion forums." ></td>
	<td class="line x" title="7:223	Many previous studies on user generated content have attempted to predict these labels automatically from the associated text." ></td>
	<td class="line x" title="8:223	However, these labels are often present in the data already, which opens another interesting line of research: designing models leveraging these labelings to improve a wide variety of applications." ></td>
	<td class="line x" title="9:223	In this study, we look at the problem of aspectbased sentiment summarization (Hu and Liu, 2004a; Popescu and Etzioni, 2005; Gamon et al., 2005; Nikos Fine Dining Food 4/5 Best fish in the city, Excellent appetizers Decor 3/5 Cozy with an old world feel, Too dark Service 1/5 Our waitress was rude, Awful service Value 5/5 Good Greek food for the $, Great price! Figure 1: An example aspect-based summary." ></td>
	<td class="line x" title="10:223	Carenini et al., 2006; Zhuang et al., 2006).1 An aspect-based summarization system takes as input a set of user reviews for a specific product or service and produces a set of relevant aspects, the aggregated sentiment for each aspect, and supporting textual evidence." ></td>
	<td class="line x" title="11:223	For example, figure 1 summarizes a restaurant using aspects food, decor, service, and value plus a numeric rating out of 5." ></td>
	<td class="line x" title="12:223	Standard aspect-based summarization consists of two problems." ></td>
	<td class="line x" title="13:223	The first is aspect identification and mention extraction." ></td>
	<td class="line x" title="14:223	Here the goal is to find the set of relevant aspects for a rated entity and extract all textual mentions that are associated with each." ></td>
	<td class="line x" title="15:223	Aspects can be fine-grained, e.g., fish, lamb, calamari, or coarse-grained, e.g., food, decor, service." ></td>
	<td class="line x" title="16:223	Similarly, extracted text can range from a single word to phrases and sentences." ></td>
	<td class="line x" title="17:223	The second problem is sentiment classification." ></td>
	<td class="line x" title="18:223	Once all the relevant aspects and associated pieces of texts are extracted, the system should aggregate sentiment over each aspect to provide the user with an average numeric or symbolic rating." ></td>
	<td class="line oc" title="19:223	Sentiment classification is a well studied problem (Wiebe, 2000; Pang et al., 2002; Turney, 2002) and in many domains users explicitly 1We use the term aspect to denote properties of an object that can be rated by a user as in Snyder and Barzilay (2007)." ></td>
	<td class="line x" title="20:223	Other studies use the term feature (Hu and Liu, 2004b)." ></td>
	<td class="line x" title="21:223	308 Food: 5; Decor: 5; Service: 5; Value: 5 The chicken was great." ></td>
	<td class="line x" title="22:223	On top of that our service was excellent and the price was right." ></td>
	<td class="line x" title="23:223	Cant wait to go back!" ></td>
	<td class="line x" title="24:223	Food: 2; Decor: 1; Service: 3; Value: 2 We went there for our anniversary." ></td>
	<td class="line x" title="25:223	My soup was cold and expensive plus it felt like they hadnt painted since 1980." ></td>
	<td class="line x" title="26:223	Food: 3; Decor: 5; Service: 4; Value: 5 The food is only mediocre, but well worth the cost." ></td>
	<td class="line x" title="27:223	Wait staff was friendly." ></td>
	<td class="line x" title="28:223	Lots of fun decorations." ></td>
	<td class="line x" title="29:223	 Food The chicken was great, My soup wascold, The food is only mediocre Decor it felt like they hadnt painted since1980, Lots of fun decorations Service service was excellent,Wait staff was friendly Value the price was right, My soup was coldand expensive, well worth the cost Figure 2: Extraction problem: Produce aspect mentions from a corpus of aspect rated reviews." ></td>
	<td class="line x" title="30:223	provide ratings for each aspect making automated means unnecessary.2 Aspect identification has also been thoroughly studied (Hu and Liu, 2004b; Gamon et al., 2005; Titov and McDonald, 2008), but again, ontologies and users often provide this information negating the need for automation." ></td>
	<td class="line x" title="31:223	Though it may be reasonable to expect a user to provide a rating for each aspect, it is unlikely that a user will annotate every sentence and phrase in a review as being relevant to some aspect." ></td>
	<td class="line x" title="32:223	Thus, it can be argued that the most pressing challenge in an aspect-based summarization system is to extract all relevant mentions for each aspect, as illustrated in figure 2." ></td>
	<td class="line x" title="33:223	When labeled data exists, this problem can be solved effectively using a wide variety of methods available for text classification and information extraction (Manning and Schutze, 1999)." ></td>
	<td class="line x" title="34:223	However, labeled data is often hard to come by, especially when one considers all possible domains of products and services." ></td>
	<td class="line x" title="35:223	Instead, we propose an unsupervised model that leverages aspect ratings that frequently accompany an online review." ></td>
	<td class="line x" title="36:223	In order to construct such model, we make two assumptions." ></td>
	<td class="line x" title="37:223	First, ratable aspects normally represent coherent topics which can be potentially discovered from co-occurrence information in the text." ></td>
	<td class="line x" title="38:223	Second, we hypothesize that the most predictive features of an aspect rating are features derived from the text segments discussing the corresponding aspect." ></td>
	<td class="line x" title="39:223	Motivated by these observations, we construct ajointstatisticalmodeloftextandsentimentratings." ></td>
	<td class="line x" title="40:223	The model is at heart a topic model in that it assigns words to a set of induced topics, each of which may represent one particular aspect." ></td>
	<td class="line x" title="41:223	The model is extended through a set of maximum entropy classifiers, one per each rated aspect, that are used to pre2E.g., http://zagat.com and http://tripadvisor.com." ></td>
	<td class="line x" title="42:223	dict the sentiment rating towards each of the aspects." ></td>
	<td class="line x" title="43:223	However, only the words assigned to an aspects corresponding topic are used in predicting the rating for that aspect." ></td>
	<td class="line x" title="44:223	As a result, the model enforces that words assigned to an aspects topic are predictive of the associated rating." ></td>
	<td class="line x" title="45:223	Our approach is more general than the particular statistical model we consider in this paper." ></td>
	<td class="line x" title="46:223	For example, other topic models can be used as a part of our model and the proposed class of modelscan beemployed inothertasksbeyond sentiment summarization, e.g., segmentation of blogs on the basis of topic labels provided by users, or topic discovery on the basis of tags given by users on social bookmarking sites.3 The rest of the paper is structured as follows." ></td>
	<td class="line x" title="47:223	Section 2 begins with a discussion of the joint textsentiment model approach." ></td>
	<td class="line x" title="48:223	In Section 3 we provide both a qualitative and quantitative evaluation of the proposed method." ></td>
	<td class="line x" title="49:223	We conclude in Section 4 with an examination of related work." ></td>
	<td class="line x" title="50:223	2 The Model In this section we describe a new statistical model called the Multi-Aspect Sentiment model (MAS), whichconsistsoftwoparts." ></td>
	<td class="line x" title="51:223	Thefirstpartisbasedon Multi-Grain Latent Dirichlet Allocation (Titov and McDonald, 2008), whichhasbeenpreviouslyshown to build topics that are representative of ratable aspects." ></td>
	<td class="line x" title="52:223	The second part is a set of sentiment predictors per aspect that are designed to force specific topics in the model to be directly correlated with a particular aspect." ></td>
	<td class="line x" title="53:223	2.1 Multi-Grain LDA The Multi-Grain Latent Dirichlet Allocation model (MG-LDA) is an extension of Latent Dirichlet Allocation (LDA) (Blei et al., 2003)." ></td>
	<td class="line x" title="54:223	As was demon3See e.g. del.ico.us (http://del.ico.us)." ></td>
	<td class="line x" title="55:223	309 strated in Titov and McDonald (2008), the topics produced by LDA do not correspond to ratable aspects of entities." ></td>
	<td class="line x" title="56:223	In particular, these models tend to build topics that globally classify terms into product instances (e.g., Creative Labs Mp3 players versus iPods, or New York versus Paris Hotels)." ></td>
	<td class="line x" title="57:223	To combat this, MG-LDA models two distinct types of topics: global topics and local topics." ></td>
	<td class="line x" title="58:223	As in LDA, the distribution of global topics is fixed for a document (a user review)." ></td>
	<td class="line x" title="59:223	However, the distribution of local topics is allowed to vary across the document." ></td>
	<td class="line x" title="60:223	A word in the document is sampled either from the mixture of global topics or from the mixture of local topics specific to the local context of the word." ></td>
	<td class="line x" title="61:223	It was demonstrated in Titov and McDonald (2008) that ratable aspects will be captured by local topics and global topics will capture properties of reviewed items." ></td>
	<td class="line x" title="62:223	For example, consider an extract from a review of a London hotel: public transport in London is straightforward, the tube station is about an 8 minute walk or you can get a bus for 1.50." ></td>
	<td class="line x" title="63:223	It can be viewed as a mixture of topic London shared bytheentirereview(words: London, tube, ), and the ratable aspect location, specific for the local context of the sentence (words: transport, walk, bus)." ></td>
	<td class="line x" title="64:223	Local topics are reused between very different types of items, whereas global topics correspond only to particular types of items." ></td>
	<td class="line x" title="65:223	In MG-LDA a document is represented as a set of sliding windows, each covering T adjacent sentences within a document.4 Each windowvin documentdhas an associated distribution over local topicslocd,v and a distribution defining preference for local topics versus global topics pid,v. A word can be sampled using any window covering its sentence s, where the window is chosen according to a categorical distributiond,s. Importantly, the fact that windows overlap permits the model to exploit a larger co-occurrence domain." ></td>
	<td class="line x" title="66:223	These simple techniques are capable of modeling local topics without more expensive modeling of topic transitions used in (Griffiths et al., 2004; Wang and McCallum, 2005; Wallach, 2006; Gruber et al., 2007)." ></td>
	<td class="line x" title="67:223	Introduction of a symmetrical Dirichlet priorDir() for the distributiond,s can control the smoothness of transitions." ></td>
	<td class="line x" title="68:223	4Ourparticularimplementationisoversentences,butsliding windows in theory can be over any sized fragment of text." ></td>
	<td class="line x" title="69:223	(a) (b) Figure 3: (a) MG-LDA model." ></td>
	<td class="line x" title="70:223	(b) An extension of MGLDA to obtain MAS." ></td>
	<td class="line x" title="71:223	The formal definition of the model with Kgl global and Kloc local topics is as follows: First, draw Kgl word distributions for global topics glz from a Dirichlet prior Dir(gl) and Kloc word distributions for local topics locz from Dir(loc)." ></td>
	<td class="line x" title="72:223	Then, for each documentd:  Choose a distribution of global topicsgld Dir(gl)." ></td>
	<td class="line x" title="73:223	 For each sentence s choose a distribution over sliding windowsd,s(v) Dir()." ></td>
	<td class="line x" title="74:223	 For each sliding windowv  chooselocd,v Dir(loc),  choosepid,v Beta(mix)." ></td>
	<td class="line x" title="75:223	 For each wordiin sentencesof documentd  choose windowvd,i d,s,  chooserd,i pid,vd,i,  ifrd,i = glchoose global topiczd,i gld ,  ifrd,i=locchoose local topiczd,ilocd,vd,i,  choose wordwd,i from the word distributionrd,izd,i. Beta(mix)is a prior Beta distribution for choosing between local and global topics." ></td>
	<td class="line x" title="76:223	In Figure 3a the corresponding graphical model is presented." ></td>
	<td class="line x" title="77:223	2.2 Multi-Aspect Sentiment Model MG-LDA constructs a set of topics that ideally correspond to ratable aspects of an entity (often in a many-to-one relationship of topics to aspects)." ></td>
	<td class="line x" title="78:223	A major shortcoming of this model  and all other unsupervised models  is that this correspondence is not explicit, i.e., how does one say that topic X is really about aspect Y?" ></td>
	<td class="line x" title="79:223	However, we can observe that numeric aspect ratings are often included in our data by users who left the reviews." ></td>
	<td class="line x" title="80:223	We then make the assumption that the text of the review discussing an aspect is predictive of its rating." ></td>
	<td class="line x" title="81:223	Thus, if we model the prediction of aspect ratings jointly with the construction of explicitly associated topics, then such a 310 model should benefit from both higher quality topics and a direct assignment from topics to aspects." ></td>
	<td class="line x" title="82:223	This is the basic idea behind the Multi-Aspect Sentiment model (MAS)." ></td>
	<td class="line x" title="83:223	In its simplest form, MAS introduces a classifier for each aspect, which is used to predict its rating." ></td>
	<td class="line x" title="84:223	Each classifier is explicitly associated to a single topic in the model and only words assigned to that topic can participate in the prediction of the sentiment rating for the aspect." ></td>
	<td class="line x" title="85:223	However, it has been observed that ratings for different aspects can be correlated (Snyder and Barzilay, 2007), e.g., very negative opinion about room cleanliness is likely to result not only in a low rating for the aspect rooms, but also is very predictive of low ratings for the aspectsserviceanddining." ></td>
	<td class="line x" title="86:223	Thiscomplicatesdiscovery of the corresponding topics, as in many reviews the most predictive features for an aspect rating might correspond to another aspect." ></td>
	<td class="line x" title="87:223	Another problem with this overly simplistic model is the presence of opinions about an item in general without referring to any particular aspect." ></td>
	<td class="line x" title="88:223	For example, this product is the worst I have ever purchased is a good predictor of low ratings for every aspect." ></td>
	<td class="line x" title="89:223	In such cases, non-aspect background words will appear to be the mostpredictive." ></td>
	<td class="line x" title="90:223	Therefore,theuseoftheaspectsentiment classifiers based only on the words assigned to the corresponding topics is problematic." ></td>
	<td class="line x" title="91:223	Such a model will not be able to discover coherent topics associated with each aspect, because in many cases the most predictive fragments for each aspect rating will not be the ones where this aspect is discussed." ></td>
	<td class="line x" title="92:223	Ourproposalistoestimatethedistributionofpossible values of an aspect rating on the basis of the overall sentiment rating and to use the words assigned to the corresponding topic to compute corrections for this aspect." ></td>
	<td class="line x" title="93:223	An aspect rating is typically correlated to the overall sentiment rating5 and the fragments discussing this particular aspect will help to correct the overall sentiment in the appropriate direction." ></td>
	<td class="line x" title="94:223	For example, if a review of a hotel is generally positive, but it includes a sentence the neighborhood is somewhat seedy then this sentence is predictive of rating for an aspect location being below other ratings." ></td>
	<td class="line x" title="95:223	This rectifies the aforementioned 5In the dataset used in our experiments all three aspect ratings are equivalent for 5,250 reviews out of 10,000." ></td>
	<td class="line x" title="96:223	problems." ></td>
	<td class="line x" title="97:223	First, aspect sentiment ratings can often be regarded as conditionally independent given the overall rating, therefore the model will not be forced to include in an aspect topic any words from other aspect topics." ></td>
	<td class="line x" title="98:223	Secondly, the fragments discussing overall opinion will influence the aspect rating only through the overall sentiment rating." ></td>
	<td class="line x" title="99:223	The overall sentiment is almost always present in the real data along with the aspect ratings, but it can be coarsely discretized and we preferred to use a latent overall sentiment." ></td>
	<td class="line x" title="100:223	The MAS model is presented in Figure 3b." ></td>
	<td class="line x" title="101:223	Note that for simplicity we decided to omit in the figure the components of the MG-LDA model other than variables r, z and w, though they are present in the statistical model." ></td>
	<td class="line x" title="102:223	MAS also allows for extra unassociated local topics in order to capture aspects not explicitly rated by the user." ></td>
	<td class="line x" title="103:223	As in MG-LDA, MAS has global topics which are expected to capture topics correspondingtoparticulartypesofitems,suchLondon hotelsorseaside resortsforthehoteldomain." ></td>
	<td class="line x" title="104:223	In figure 3b we shaded the aspect ratingsya, assuming that every aspect rating is present in the data (though in practice they might be available only for some reviews)." ></td>
	<td class="line x" title="105:223	In this model the distribution of the overall sentiment rating yov is based on all the n-gram featuresofareviewtext." ></td>
	<td class="line x" title="106:223	Thenthedistributionofya, for every rated aspecta, can be computed from the distribution of yov and from any n-gram feature where at least one word in the n-gram is assigned to the associated aspect topic (r = loc, z = a)." ></td>
	<td class="line x" title="107:223	Instead of having a latent variable yov,6 we use a similar model which does not have an explicit notion ofyov." ></td>
	<td class="line x" title="108:223	The distribution of a sentiment ratingya for each rated aspectais computed from two scores." ></td>
	<td class="line x" title="109:223	The first score is computed on the basis of all the ngrams, but using a common set of weights independent of the aspecta." ></td>
	<td class="line x" title="110:223	Another score is computed only using n-grams associated with the related topic, but an aspect-specific set of weights is used in this computation." ></td>
	<td class="line x" title="111:223	More formally, we consider the log-linear distribution: P(ya = y|w,r,z)exp(bay+ summationdisplay fw Jf,y+paf,r,zJaf,y), (1) where w, r, z are vectors of all the words in a docu6Preliminary experiments suggested that this is also a feasible approach, but somewhat more computationally expensive." ></td>
	<td class="line x" title="112:223	311 ment, assignments of context (global or local) and topics for all the words in the document, respectively." ></td>
	<td class="line x" title="113:223	bay is the bias term which regulates the prior distribution P(ya = y), f iterates through all the n-grams, Jy,f and Jay,f are common weights and aspect-specific weights for n-gram feature f. paf,r,z is equal to a fraction of words in n-gram feature f assigned to the aspect topic (r = loc,z = a)." ></td>
	<td class="line x" title="114:223	2.3 Inference in MAS Exact inference in the MAS model is intractable." ></td>
	<td class="line x" title="115:223	Following Titov and McDonald (2008) we use a collapsed Gibbs sampling algorithm that was derived for the MG-LDA model based on the Gibbs sampling method proposed for LDA in (Griffiths and Steyvers, 2004)." ></td>
	<td class="line x" title="116:223	Gibbs sampling is an example of a Markov Chain Monte Carlo algorithm (Geman and Geman, 1984)." ></td>
	<td class="line x" title="117:223	It is used to produce a sample from a joint distribution when only conditional distributions of each variable can be efficiently computed." ></td>
	<td class="line x" title="118:223	In Gibbs sampling, variables are sequentially sampled from their distributions conditioned on all other variables in the model." ></td>
	<td class="line x" title="119:223	Such a chain of model states converges to a sample from the joint distribution." ></td>
	<td class="line x" title="120:223	A naive application of this technique to LDA would imply that both assignments of topics to words z and distributions  and  should be sampled." ></td>
	<td class="line x" title="121:223	However, (Griffiths and Steyvers, 2004) demonstrated that anefficient collapsedGibbssamplercan be constructed, where only assignments z need to be sampled, whereas the dependency on distributionsand can be integrated out analytically." ></td>
	<td class="line x" title="122:223	In the case of MAS we also use maximum aposteriori estimates of the sentiment predictor parameters bay, Jy,f and Jay,f. The MAP estimates for parameters bay, Jy,f and Jay,f are obtained by using stochastic gradient ascent." ></td>
	<td class="line x" title="123:223	The direction of the gradient is computed simultaneously with running a chain by generating several assignments at each step and averaging over the corresponding gradient estimates." ></td>
	<td class="line x" title="124:223	For details on computing gradients for loglinear graphical models with Gibbs sampling we refer the reader to (Neal, 1992)." ></td>
	<td class="line x" title="125:223	Space constraints do not allow us to present either the derivation or a detailed description of the sampling algorithm." ></td>
	<td class="line x" title="126:223	However, note that the conditional distribution used in sampling decomposes into two parts: P(vd,i = v,rd,i = r,zd,i = z|v,r,z,w,y)  d,iv,r,z d,ir,z, (2) where v, r and z are vectors of assignments of sliding windows, context (global or local) and topics for all the words in the collection except for the considered word at positioniin documentd;yis the vector of sentiment ratings." ></td>
	<td class="line x" title="127:223	The first factor d,iv,r,z is responsibleformodelingco-occurrencesonthewindowanddocumentlevelandcoherenceofthetopics." ></td>
	<td class="line x" title="128:223	This factor is proportional to the conditional distribution used in the Gibbs sampler of the MG-LDA model (Titov and McDonald, 2008)." ></td>
	<td class="line x" title="129:223	The last factor quantifies the influence of the assignment of the word (d,i) on the probability of the sentiment ratings." ></td>
	<td class="line x" title="130:223	It appears only if ratings are known (observable) and equals: d,ir,z = productdisplay a P(yda|w,r,rd,i = r,z,zd,i = z) P(yda|w,r,z,rd,i = gl) , wheretheprobabilitydistributioniscomputedasdefined in expression (1), yda is the rating for the ath aspect of reviewd." ></td>
	<td class="line x" title="131:223	3 Experiments In this section we present qualitative and quantitative experiments." ></td>
	<td class="line x" title="132:223	For the qualitative analysis we show that topics inferred by the MAS model correspond directly to the associated aspects." ></td>
	<td class="line x" title="133:223	For the quantitative analysis we show that the MAS model induces a distribution over the rated aspects which can be used to accurately predict whether a text fragment is relevant to an aspect or not." ></td>
	<td class="line x" title="134:223	3.1 Qualitative Evaluation To perform qualitative experiments we used a set of reviews of hotels taken from TripAdvisor.com7 that contained 10,000 reviews (109,024 sentences, 2,145,313 words in total)." ></td>
	<td class="line x" title="135:223	Every review was rated with at least three aspects: service, location and rooms." ></td>
	<td class="line x" title="136:223	Each rating is an integer from 1 to 5." ></td>
	<td class="line x" title="137:223	The dataset was tokenized and sentence split automatically." ></td>
	<td class="line x" title="138:223	7(c) 2005-06, TripAdvisor, LLC All rights reserved 312 rated aspect top words service staff friendly helpful service desk concierge excellent extremely hotel great reception english pleasant help location hotel walk location station metro walking away right minutes close bus city located just easy restaurants local rooms room bathroom shower bed tv small water clean comfortable towels bath nice large pillows space beds tub topics breakfast free coffee internet morning access buffet day wine nice lobby complimentary included good fruit $ night parking rate price paid day euros got cost pay hotel worth euro expensive car extra deal booked room noise night street air did door floor rooms open noisy window windows hear outside problem quiet sleep global moscow st russian petersburg nevsky russia palace hermitage kremlin prospect river prospekt kempinski topics paris tower french eiffel dame notre rue st louvre rer champs opera elysee george parisian du pantheon cafes Table 1: Top words from MAS for hotel reviews." ></td>
	<td class="line x" title="139:223	Krooms top words 2 rooms clean hotel room small nice comfortable modern good quite large lobby old decor spacious decorated bathroom size room noise night street did air rooms door open noisy window floor hear windows problem outside quiet sleep bit light 3 room clean bed comfortable rooms bathroom small beds nice large size tv spacious good double big space huge king room floor view rooms suite got views given quiet building small balcony upgraded nice high booked asked overlooking room bathroom shower air water did like hot small towels door old window toilet conditioning open bath dirty wall tub 4 room clean rooms comfortable bed small beds nice bathroom size large modern spacious good double big quiet decorated check arrived time day airport early room luggage took late morning got long flight ready minutes did taxi bags went room noise night street did air rooms noisy open door hear windows window outside quiet sleep problem floor conditioning bathroom room shower tv bed small water towels bath tub large nice toilet clean space toiletries flat wall sink screen Table 2: Top words for aspect rooms with different number of topicsKrooms." ></td>
	<td class="line x" title="140:223	We ran the sampling chain for 700 iterations to produce a sample." ></td>
	<td class="line x" title="141:223	Distributions of words in each topic were estimated as the proportion of words assignedtoeachtopic, takingintoaccounttopicmodel priors gl and loc." ></td>
	<td class="line x" title="142:223	The sliding windows were chosen to cover 3 sentences for all the experiments." ></td>
	<td class="line x" title="143:223	All the priors were chosen to be equal to 0.1." ></td>
	<td class="line x" title="144:223	We used 15 local topics and 30 global topics." ></td>
	<td class="line x" title="145:223	In the model, the first three local topics were associated to the rating classifiers for each aspects." ></td>
	<td class="line x" title="146:223	As a result, we would expect these topics to correspond to the service, location, and rooms aspects respectively." ></td>
	<td class="line x" title="147:223	Unigram and bigram features were used in the sentiment predictors in the MAS model." ></td>
	<td class="line x" title="148:223	Before applying the topic models we removed punctuation and also removed stop words using the standard list of stop words,8 however, all the words and punctuation were used in the sentiment predictors." ></td>
	<td class="line x" title="149:223	It does not take many chain iterations to discover initial topics." ></td>
	<td class="line x" title="150:223	This happens considerably faster than the appropriate weights of the sentiment predictor being learned." ></td>
	<td class="line x" title="151:223	This poses a problem, because, in the beginning, the sentiment predictors are not accurate enough to force the model to discover appropriate topics associated with each of the rated aspects." ></td>
	<td class="line x" title="152:223	And assoonastopicareformed, aspectsentimentpredictors cannot affect them anymore because they do not 8http://www.dcs.gla.ac.uk/idom/ir resources/linguistic utils/ stop words have access to the true words associated with their aspects." ></td>
	<td class="line x" title="153:223	To combat this problem we first train the sentiment classifiers by assuming thatpaf,r,z is equal for all the local topics, which effectively ignores the topic model." ></td>
	<td class="line x" title="154:223	Then we use the estimated parameters within the topic model.9 Secondly, we modify the sampling algorithm." ></td>
	<td class="line x" title="155:223	The conditional probability used in sampling, expression (2), is proportional to the product of two factors." ></td>
	<td class="line x" title="156:223	The first factor, d,iv,r,z, expresses a preference for topics likely from the co-occurrence information, whereas the second one, d,ir,z, favors the choice of topics which are predictive of the observable sentiment ratings." ></td>
	<td class="line x" title="157:223	We used (d,ir,z)1+0.95tq in the sampling distribution instead of d,ir,z, where t is the iteration number." ></td>
	<td class="line x" title="158:223	q was chosen to be 4, though the quality of the topics seemed to be indistinguishable with any q between 3 and 10." ></td>
	<td class="line x" title="159:223	This can be thought of as having 1 + 0.95tq ratings instead of a single vector assigned to each review, i.e., focusing the model on prediction of the ratings rather than finding the topic labels which are good at explaining co-occurrences of words." ></td>
	<td class="line x" title="160:223	These heuristics influence sampling only during the first iterations of the chain." ></td>
	<td class="line x" title="161:223	Top words for some of discovered local topics, in9Initial experiments suggested that instead of doing this pre-training we could start with very large priors loc and mix, and then reduce them through the course of training." ></td>
	<td class="line x" title="162:223	However, this is significantly more computationally expensive." ></td>
	<td class="line x" title="163:223	313  0  10  20  30  40  50  60  70  80  90  100  0  10  20  30  40  50  60  70  80  90  100 Recall Precision topic modelmaxent classifier topic model maxent classifier  0  10  20  30  40  50  60  70  80  90  100  0  10  20  30  40  50  60  70  80  90  100 Recall Precision maxent classifier 1 topic2 topics 3 topics4 topics  0  10  20  30  40  50  60  70  80  90  100  0  10  20  30  40  50  60  70  80  90  100 Recall Precision (a) (b) (c) Figure 4: (a) Aspect service." ></td>
	<td class="line x" title="164:223	(b) Aspect location." ></td>
	<td class="line x" title="165:223	(c) Aspect rooms." ></td>
	<td class="line x" title="166:223	cludingthefirst3topicsassociatedwiththeratedaspects, and also top words for some of global topics are presented in Table 1." ></td>
	<td class="line x" title="167:223	We can see that the model discovered as its first three topics the correct associated aspects: service, location, and rooms." ></td>
	<td class="line x" title="168:223	Other local topics, as for the MG-LDA model, correspond to otheraspectsdiscussedinreviews(breakfast, prices, noise), and as it was previously shown in Titov and McDonald (2008), aspects for global topics correspond to the types of reviewed items (hotels in Russia, Paris hotels) or background words." ></td>
	<td class="line x" title="169:223	Notice though, that the 3rd local topic induced for the rating rooms is slightly narrow." ></td>
	<td class="line x" title="170:223	This can be explained by the fact that the aspect rooms is a central aspect of hotel reviews." ></td>
	<td class="line x" title="171:223	A very significant fraction of text in every review can be thought of as a part of the aspect rooms." ></td>
	<td class="line x" title="172:223	These portions of reviews discuss different coherent sub-aspects related to the aspect rooms, e.g., the previously discovered topic noise." ></td>
	<td class="line x" title="173:223	Therefore, it is natural to associate several topics to such central aspects." ></td>
	<td class="line x" title="174:223	To test this we varied the number of topics associated with the sentiment predictor for the aspect rooms." ></td>
	<td class="line x" title="175:223	Top words for resulting topics are presented in Table 2." ></td>
	<td class="line x" title="176:223	It can be observed that the topic model discovered appropriate topics while the number of topics was below 4." ></td>
	<td class="line x" title="177:223	With 4 topics a semantically unrelated topic (check-in/arrival) is induced." ></td>
	<td class="line x" title="178:223	Manual selection of the number of topics is undesirable, but this problem can be potentially tackled with Dirichlet Process priors or a topic split criterion based on the accuracy of the sentiment predictor in the MAS model." ></td>
	<td class="line x" title="179:223	We found that both service and location did not benefit by the assignment of additional topics to their sentiment rating models." ></td>
	<td class="line x" title="180:223	The experimental results suggest that the MAS model is reliable in the discovery of topics corresponding to the rated aspects." ></td>
	<td class="line x" title="181:223	In the next section we will show that the induced topics can be used to accurately extract fragments for each aspect." ></td>
	<td class="line x" title="182:223	3.2 Sentence Labeling A primary advantage of MAS over unsupervised models, such as MG-LDA or clustering, is that topics are linked to a rated aspect, i.e., we know exactly which topics model which aspects." ></td>
	<td class="line x" title="183:223	As a result, these topics can be directly used to extract textual mentions that are relevant for an aspect." ></td>
	<td class="line x" title="184:223	To test this, we hand labeled 779 random sentences from the dataset considered in the previous set of experiments." ></td>
	<td class="line x" title="185:223	The sentences were labeled with one or more aspects." ></td>
	<td class="line x" title="186:223	Among them, 164, 176 and 263 sentences were labeled as related to aspects service, location and rooms, respectively." ></td>
	<td class="line x" title="187:223	The remaining sentences were not relevant to any of the rated aspects." ></td>
	<td class="line x" title="188:223	We compared two models." ></td>
	<td class="line x" title="189:223	The first model uses the first three topics of MAS to extract relevant mentionsbasedontheprobabilityofthattopic/aspectbeingpresentinthesentence." ></td>
	<td class="line x" title="190:223	Toobtaintheseprobabilities we used estimators based on the proportion of words in the sentence assigned to an aspects topic and normalized within local topics." ></td>
	<td class="line x" title="191:223	To improve the reliability of the estimator we produced 100 samples for each document while keeping assignments ofthetopicstoallotherwordsinthecollectionfixed." ></td>
	<td class="line x" title="192:223	The probability estimates were then obtained by averaging over these samples." ></td>
	<td class="line x" title="193:223	We did not perform any model selection on the basis of the hand-labeled data, and tested only a single model of each type." ></td>
	<td class="line x" title="194:223	314 For the second model we trained a maximum entropy classifier, one per each aspect, using 10-fold cross validation and unigram/bigram features." ></td>
	<td class="line x" title="195:223	Note that this is a supervised system and as such represents an upper-bound in performance one might expect when comparing an unsupervised model such as MAS." ></td>
	<td class="line x" title="196:223	We chose this comparison to demonstrate that our model can find relevant text mentions with high accuracy relative to a supervised model." ></td>
	<td class="line x" title="197:223	It is difficult to compare our model to other unsupervised systems such as MG-LDA or LDA." ></td>
	<td class="line x" title="198:223	Again, this is because those systems have no mechanism for directly correlating topics or clusters to corresponding aspects, highlighting the benefit of MAS." ></td>
	<td class="line x" title="199:223	The resulting precision-recall curves for the aspects service, location and rooms are presented in Figure 4." ></td>
	<td class="line x" title="200:223	In Figure 4c, we varied the number of topics associated with the aspect rooms.10 The average precision we obtained (the standard measure proportional to the area under the curve) is 75.8%, 85.5% for aspects service and location, respectively." ></td>
	<td class="line x" title="201:223	For the aspect rooms these scores are equal to 75.0%, 74.5%, 87.6%, 79.8% with 14 topics per aspect, respectively." ></td>
	<td class="line x" title="202:223	The logistic regression models achieve 80.8%, 94.0% and 88.3% for the aspects service, location and rooms." ></td>
	<td class="line x" title="203:223	We can observe that the topic model, which does not use any explicitly aspect-labeled text, achieves accuracies lower than, but comparable to a supervised model." ></td>
	<td class="line x" title="204:223	4 Related Work There is a growing body of work on summarizing sentiment by extracting and aggregating sentiment over ratable aspects and providing corresponding textual evidence." ></td>
	<td class="line x" title="205:223	Text excerpts are usually extracted through string matching (Hu and Liu, 2004a; Popescu and Etzioni, 2005), sentence clustering (Gamon et al., 2005), or through topic models (Mei et al., 2007; Titov and McDonald, 2008)." ></td>
	<td class="line x" title="206:223	String extraction methods are limited to fine-grained aspects whereasclusteringandtopicmodelapproachesmust resort to ad-hoc means of labeling clusters or topics." ></td>
	<td class="line x" title="207:223	However, this is the first work we are aware of that uses a pre-defined set of aspects plus an associated signal to learn a mapping from text to an aspect for 10To improve readability we smoothed the curve for the aspect rooms." ></td>
	<td class="line x" title="208:223	the purpose of extraction." ></td>
	<td class="line x" title="209:223	A closely related model to ours is that of Mei et al.(2007) which performs joint topic and sentiment modeling of collections." ></td>
	<td class="line x" title="211:223	Our model differs from theirs in many respects: Mei et al. only model sentiment predictions for the entire document and not on the aspect level; They treat sentiment predictions as unobserved variables, whereas we treat them as observed signals that help to guide the creation of topics; They model co-occurrences solely on the documentlevel, whereasourmodelisbasedonMG-LDA and models both local and global contexts." ></td>
	<td class="line x" title="212:223	Recently, Blei and McAuliffe (2008) proposed an approach for joint sentiment and topic modeling that can be viewed as a supervised LDA (sLDA) model that tries to infer topics appropriate for use in a givenclassificationorregressionproblem." ></td>
	<td class="line x" title="213:223	MASand sLDA are similar in that both use sentiment predictions as an observed signal that is predicted by the model." ></td>
	<td class="line x" title="214:223	However, Blei et al. do not consider multiaspect ranking or look at co-occurrences beyond the document level, both of which are central to our model." ></td>
	<td class="line x" title="215:223	Parallel to this study Branavan et al.(2008) also showed that joint models of text and user annotations benefit extractive summarization." ></td>
	<td class="line x" title="217:223	In particular, they used signals from pros-cons lists whereas our models use aspect rating signals." ></td>
	<td class="line x" title="218:223	5 Conclusions In this paper we presented a joint model of text and aspect ratings for extracting text to be displayed in sentimentsummaries." ></td>
	<td class="line x" title="219:223	Themodelusesaspectratings todiscoverthecorrespondingtopicsandcanthusextract fragments of text discussing these aspects without the need of annotated data." ></td>
	<td class="line x" title="220:223	We demonstrated that the model indeed discovers corresponding coherent topics and achieves accuracy in sentence labeling comparable to a standard supervised model." ></td>
	<td class="line x" title="221:223	The primary area of future work is to incorporate the model into an end-to-end sentiment summarization system in order to evaluate it at that level." ></td>
	<td class="line x" title="222:223	Acknowledgments This work benefited from discussions with Sasha Blair-Goldensohn and Fernando Pereira." ></td>
	<td class="line x" title="223:223	315" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-2034
Lyric-based Song Sentiment Classification with Sentiment Vector Space Model
Xia, Yunqing;Wang, Linlin;Wong, Kam-Fai;Xu, Mingxing;"></td>
	<td class="line x" title="1:111	Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 133136, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:111	c2008 Association for Computational Linguistics Sentiment Vector Space Model for Lyric-based Song Sentiment Classification   Yunqing Xia Linlin Wang Center for Speech and language Tech." ></td>
	<td class="line x" title="3:111	State Key Lab of Intelligent Tech." ></td>
	<td class="line x" title="4:111	and Sys." ></td>
	<td class="line x" title="5:111	RIIT, Tsinghua University Dept. of CST, Tsinghua University Beijing 100084, China Beijing 100084, China yqxia@tsinghua.edu.cn wangll07@mails.tsinghua.edu.cn  Kam-Fai Wong Mingxing Xu Dept. of SE&EM Dept. of CST The Chinese University of Hong Kong Tsinghua University Shatin, Hong Kong Beijing 100084, China kfwong@se.cuhk.edu.hk xumx@tsinghua.edu.cn   Abstract Lyric-based song sentiment classification seeks to assign songs appropriate sentiment labels such as light-hearted and heavy-hearted." ></td>
	<td class="line x" title="6:111	Four problems render vector space model (VSM)-based text classification approach ineffective: 1) Many words within song lyrics actually contribute little to sentiment; 2) Nouns and verbs used to express sentiment are ambiguous; 3) Negations and modifiers around the sentiment keywords make particular contributions to sentiment; 4) Song lyric is usually very short." ></td>
	<td class="line x" title="7:111	To address these problems, the sentiment vector space model (s-VSM) is proposed to represent song lyric document." ></td>
	<td class="line x" title="8:111	The preliminary experiments prove that the sVSM model outperforms the VSM model in the lyric-based song sentiment classification task." ></td>
	<td class="line x" title="9:111	1 Introduction Song sentiment classification nowadays becomes a hot research topic due largely to the increasing demand of ubiquitous song access, especially via mobile phone." ></td>
	<td class="line x" title="10:111	In their music phone W910i, Sony and Ericsson provide Sense Me component to catch owners mood and play songs accordingly." ></td>
	<td class="line x" title="11:111	Song sentiment classification is the key technology for song recommendation." ></td>
	<td class="line x" title="12:111	Many research works have been reported to achieve this goal using audio signal (Knees et al., 2007)." ></td>
	<td class="line x" title="13:111	But research efforts on lyric-based song classification are very few." ></td>
	<td class="line x" title="14:111	Preliminary experiments show that VSM-based text classification method (Joachims, 2002) is ineffective in song sentiment classification (see Section 5) due to the following four reasons." ></td>
	<td class="line x" title="15:111	Firstly, the VSM model considers all content words within song lyric as features in text classification." ></td>
	<td class="line x" title="16:111	But in fact many words in song lyric actually make little contribution to sentiment expressing." ></td>
	<td class="line x" title="17:111	Using all content words as features, the VSM-based classification methods perform poorly in song sentiment classification." ></td>
	<td class="line x" title="18:111	Secondly, observation on lyrics of thousands of Chinese pop songs reveals that sentiment-related nouns and verbs usually carry multiple senses." ></td>
	<td class="line x" title="19:111	Unfortunately, the ambiguity is not appropriately handled in the VSM model." ></td>
	<td class="line x" title="20:111	Thirdly, negations and modifiers are constantly found around the sentiment words in song lyric to inverse, to strengthen or to weaken the sentiments that the sentences carry." ></td>
	<td class="line x" title="21:111	But the VSM model is not capable of reflecting these functions." ></td>
	<td class="line x" title="22:111	Lastly, song lyric is usually very short, namely 50 words on average in length, rendering serious sparse data problem in VSM-based classification." ></td>
	<td class="line x" title="23:111	To address the aforementioned problems of the VSM model, the sentiment vector space model (sVSM) is proposed in this work." ></td>
	<td class="line x" title="24:111	We adopt the sVSM model to extract sentiment features from song lyrics and implement the SVM-light (Joachims, 2002) classification algorithm to assign sentiment labels to given songs." ></td>
	<td class="line x" title="25:111	133 2 Related Works Song sentiment classification has been investigated since 1990s in audio signal processing community and research works are mostly found relying on audio signal to make a decision using machine learning algorithms (Li and Ogihara, 2006; Lu et al., 2006)." ></td>
	<td class="line x" title="26:111	Typically, the sentiment classes are defined based on the Thayers arousal-valence emotion plane (Thayer, 1989)." ></td>
	<td class="line x" title="27:111	Instead of assigning songs one of the four typical sentiment labels, Lu et al.(2006) propose the hierarchical framework to perform song sentiment classification with two steps." ></td>
	<td class="line x" title="29:111	In the first step the energy level is detected with intensity features and the stress level is determined in the second step with timbre and rhythm features." ></td>
	<td class="line x" title="30:111	It is proved difficult to detect stress level using audio as classification proof." ></td>
	<td class="line x" title="31:111	Song sentiment classification using lyric as proof is recently investigated by Chen et al.(2006)." ></td>
	<td class="line x" title="33:111	They adopt the hierarchical framework and make use of song lyric to detect stress level in the second step." ></td>
	<td class="line x" title="34:111	In fact, many literatures have been produced to address the sentiment analysis problem in natural language processing research." ></td>
	<td class="line pc" title="35:111	Three approaches are dominating, i.e. knowledge-based approach (Kim and Hovy, 2004), information retrieval-based approach (Turney and Littman, 2003) and machine learning approach (Pang et al., 2002), in which the last approach is found very popular." ></td>
	<td class="line oc" title="36:111	Pang et al.(2002) adopt the VSM model to represent product reviews and apply text classification algorithms such as Nave Bayes, maximum entropy and support vector machines to predict sentiment polarity of given product review." ></td>
	<td class="line x" title="38:111	Chen et al.(2006) also apply the VSM model in lyric-based song sentiment classification." ></td>
	<td class="line x" title="40:111	However, our experiments show that song sentiment classification with the VSM model delivers disappointing quality (see Section 5)." ></td>
	<td class="line x" title="41:111	Error analysis reveals that the VSM model is problematic in representing song lyric." ></td>
	<td class="line x" title="42:111	It is necessary to design a new lyric representation model for song sentiment classification." ></td>
	<td class="line x" title="43:111	3 Sentiment Vector Space Model We propose the sentiment vector space model (sVSM) for song sentiment classification." ></td>
	<td class="line x" title="44:111	Principles of the s-VSM model are listed as follows." ></td>
	<td class="line x" title="45:111	(1) Only sentiment-related words are used to produce sentiment features for the s-VSM model." ></td>
	<td class="line x" title="46:111	(2) The sentiment words are appropriately disambiguated with the neighboring negations and modifiers." ></td>
	<td class="line x" title="47:111	(3) Negations and modifiers are included in the sVSM model to reflect the functions of inversing, strengthening and weakening." ></td>
	<td class="line x" title="48:111	Sentiment unit is found the appropriate element complying with the above principles." ></td>
	<td class="line x" title="49:111	To be general, we first present the notation for sentiment lexicon as follows.,,1},{    ,,1},{   ,,1},{  };,,{ LlmM JjnN IicCMNCL l j i == == ===  in which L represents sentiment lexicon, C sentiment word set, N negation set and M modifier set." ></td>
	<td class="line x" title="51:111	These words can be automatically extracted from a semantic dictionary and each sentiment word is assigned a sentiment label, namely light-hearted or heavy-hearted according to its lexical definition." ></td>
	<td class="line x" title="52:111	Given a piece of song lyric, denoted as follows, HhwW h ,,1},{ == in which W denotes a set of words that appear in the song lyric, the semantic lexicon is in turn used to locate sentiment units denoted as follows." ></td>
	<td class="line x" title="53:111	MWmNWnCWc mncuU vlvjvi vlvjviv  == ,,, ,,,   ;  ;,  },,{}{  Note that sentiment units are unambiguous sentiment expressions, each of which contains one sentiment word and possibly one modifier and one negation." ></td>
	<td class="line x" title="54:111	Negations and modifiers are helpful to determine the unique meaning of the sentiment words within certain context window, e.g. 3 preceding words and 3 succeeding words in our case." ></td>
	<td class="line x" title="55:111	Then, the s-VSM model is presented as follows." ></td>
	<td class="line x" title="56:111	))(),,(),(( 21 UfUfUfV TS = . in which V S  represents the sentiment vector for the given song lyric and f i (U) sentiment features which are usually certain statistics on sentiment units that appear in lyric." ></td>
	<td class="line x" title="57:111	We classify the sentiment units according to occurrence of sentiment words, negations and modifiers." ></td>
	<td class="line x" title="58:111	If the sentiment word is mandatory for any sentiment unit, eight kinds of sentiment units are obtained." ></td>
	<td class="line x" title="59:111	Let f PSW  denote count of positive senti134 ment words (PSW), f NSW count of negative sentiment words (NSW), f NEG count of negations (NEG) and f MOD  count of modifiers (MOD)." ></td>
	<td class="line x" title="60:111	Eight sentiment features are defined in Table 1." ></td>
	<td class="line x" title="61:111	f i  Number of sentiment units satisfying  f 1  f PSW  >0, f NSW =f NEG =f MOD =0 f 2  f PSW =0, f NSW  >0, f NEG = f MOD =0 f 3  f PSW  >0, f NSW  =0,  f NEG >0, f MOD =0 f 4  f PSW =0, f NSW  >0, f NEG  >0, f MOD =0 f 5  f PSW  >0, f NSW  =0, f NEG  =0, f MOD  >0 f 6  f PSW =0, f NSW  >0, f NEG  =0, f MOD  >0 f 7  f PSW  >0, f NSW  =0, f NEG  >0, f MOD  >0 f 8  f PSW  =0, f NSW  >0, f NEG  >0, f MOD  >0 Table 1." ></td>
	<td class="line x" title="62:111	Definition of sentiment features." ></td>
	<td class="line x" title="63:111	Note that one sentiment unit contains only one sentiment word." ></td>
	<td class="line x" title="64:111	Thus it is not possible that f PSW  and f NSW  are both bigger than zero." ></td>
	<td class="line x" title="65:111	Obviously, sparse data problem can be well addressed using statistics on sentiment units rather than on individual words or sentiment units." ></td>
	<td class="line x" title="66:111	4  Lyric-based Song Sentiment Classification Song sentiment classification based on lyric can be viewed as a text classification task thus can be handled by some standard classification algorithms." ></td>
	<td class="line x" title="67:111	In this work, the SVM-light algorithm is implemented to accomplish this task due to its excellence in text classification." ></td>
	<td class="line x" title="68:111	Note that song sentiment classification differs from the traditional text classification in feature extraction." ></td>
	<td class="line x" title="69:111	In our case, sentiment units are first detected and the sentiment features are then generated based on sentiment units." ></td>
	<td class="line x" title="70:111	As the sentiment units carry unambiguous sentiments, it is deemed that the s-VSM is model is promising to carry out the song sentiment classification task effectively." ></td>
	<td class="line x" title="71:111	5 Evaluation To evaluate the s-VSM model, a song corpus, i.e. 5SONGS, is created manually." ></td>
	<td class="line x" title="72:111	It covers 2,653 Chinese pop songs, in which 1,632 are assigned label of light-hearted (positive class) and 1,021 assigned heavy-hearted (negative class)." ></td>
	<td class="line x" title="73:111	We randomly select 2,001 songs (around 75%) for training and the rest for testing." ></td>
	<td class="line x" title="74:111	We adopt the standard evaluation criteria in text classification, namely precision (p), recall (r), f-1 measure (f) and accuracy (a) (Yang and Liu, 1999)." ></td>
	<td class="line x" title="75:111	In our experiments, three approaches are implemented in song sentiment classification, i.e. audiobased (AB) approach, knowledge-based (KB) approach and machine learning (ML) approach, in which the latter two approaches are also referred to as text-based (TB) approach." ></td>
	<td class="line x" title="76:111	The intentions are 1) to compare AB approach against the two TB approaches, 2) to compare the ML approach against the KB approach, and 3) to compare the VSMbased ML approach against the s-VSM-based one." ></td>
	<td class="line x" title="77:111	Audio-based (AB) Approach We extract 10 timbre features and 2 rhythm features (Lu et al., 2006) from audio data of each song." ></td>
	<td class="line x" title="78:111	Thus each song is represented by a 12-dimension vector." ></td>
	<td class="line x" title="79:111	We run SVM-light algorithm to learn on the training samples and classify test ones." ></td>
	<td class="line x" title="80:111	Knowledge-based (KB) Approach We make use of HowNet (Dong and dong, 2006), to detect sentiment words, to recognize the neighboring negations and modifiers, and finally to locate sentiment units within song lyric." ></td>
	<td class="line x" title="81:111	Sentiment (SM) of the sentiment unit (SU) is determined considering sentiment words (SW), negation (NEG) and modifiers (MOD) using the following rule." ></td>
	<td class="line x" title="82:111	(1) SM(SU) = label(SW); (2) SM(SU) = SM(SU) iff SU contains NEG; (3) SM(SU) = degree(MOD)*SM(SU) iff SU contains MOD." ></td>
	<td class="line x" title="83:111	In the above rule, label(x) is the function to read sentiment label({1, -1}) of given word in the sentiment lexicon and degree(x) to read its modification degree({1/2, 2})." ></td>
	<td class="line x" title="84:111	As the sentiment labels are integer numbers, the following formula is adopted to obtain label of the given song lyric." ></td>
	<td class="line x" title="85:111	      =  i i SUSMsignlabel )(  Machine Learning (ML) Approach The ML approach adopts text classification algorithms to predict sentiment label of given song lyric." ></td>
	<td class="line x" title="86:111	The SVM-light algorithm is implemented based on VSM model and s-VSM model, respectively." ></td>
	<td class="line x" title="87:111	For the VSM model, we apply (CHI) algorithm (Yang and Pedersen, 1997) to select effective sentiment word features." ></td>
	<td class="line x" title="88:111	For the s-VSM model, we adopt HowNet as the sentiment lexicon to create sentiment vectors." ></td>
	<td class="line x" title="89:111	Experimental results are presented Table 2." ></td>
	<td class="line x" title="90:111	135  p R f-1 a Audio-based 0.504 0.701 0.586 0.504 Knowledge-based 0.726 0.584 0.647 0.714 VSM-based 0.587 1.000 0.740 0.587 s-VSM-based 0.783 0.750 0.766 0.732 Table 2." ></td>
	<td class="line x" title="91:111	Experimental results Table 2 shows that the text-based methods outperform the audio-based method." ></td>
	<td class="line x" title="92:111	This justifies our claim that lyric is better than audio in song sentiment detection." ></td>
	<td class="line x" title="93:111	The second observation is that machine learning approach outperforms the knowledge-based approach." ></td>
	<td class="line x" title="94:111	The third observation is that s-VSM-based method outperforms VSMbased method on f-1 score." ></td>
	<td class="line x" title="95:111	Besides, we surprisingly find that VSM-based method assigns all test samples light-hearted label thus recall reaches 100%." ></td>
	<td class="line x" title="96:111	This makes results of VSM-based method unreliable." ></td>
	<td class="line x" title="97:111	We look into the model file created by the SVM-light algorithm and find that 1,868 of 2,001 VSM training vectors are selected as support vectors while 1,222 s-VSM support vectors are selected." ></td>
	<td class="line x" title="98:111	This indicates that the VSM model indeed suffers the problems mentioned in Section 1 in lyric-based song sentiment classification." ></td>
	<td class="line x" title="99:111	As a comparison, the s-VSM model produces more discriminative support vectors for the SVM classifier thus yields reliable predictions." ></td>
	<td class="line x" title="100:111	6  Conclusions and Future Works The s-VSM model is presented in this paper as a document representation model to address the problems encountered in song sentiment classification." ></td>
	<td class="line x" title="101:111	This model considers sentiment units in feature definition and produces more discriminative support vectors for song sentiment classification." ></td>
	<td class="line x" title="102:111	Some conclusions can be drawn from the preliminary experiments on song sentiment classification." ></td>
	<td class="line x" title="103:111	Firstly, text-based methods are more effective than the audio-based method." ></td>
	<td class="line x" title="104:111	Secondly, the machine learning approach outperforms the knowledgebased approach." ></td>
	<td class="line x" title="105:111	Thirdly, s-VSM model is more reliable and more accurate than the VSM model." ></td>
	<td class="line x" title="106:111	We are thus encouraged to carry out more research to further refine the s-VSM model in sentiment classification." ></td>
	<td class="line x" title="107:111	In the future, we will incorporate some linguistic rules to improve performance of sentiment unit detection." ></td>
	<td class="line x" title="108:111	Meanwhile, sentiment features in the s-VSM model are currently equally weighted." ></td>
	<td class="line x" title="109:111	We will adopt some estimation techniques to assess their contributions for the s-VSM model." ></td>
	<td class="line x" title="110:111	Finally, we will also explore how the sVSM model improves quality of polarity classification in opinion mining." ></td>
	<td class="line x" title="111:111	Acknowledgement Research work in this paper is partially supported by NSFC (No. 60703051) and Tsinghua University under the Basic Research Foundation (No. JC2007049)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-2065
Multi-domain Sentiment Classification
Li, Shoushan;Zong, Chengqing;"></td>
	<td class="line x" title="1:89	Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 257260, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:89	c2008 Association for Computational Linguistics Multi-domain Sentiment Classification  Shoushan Li and Chengqing Zong National Laboratory of Pattern Recognition Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China {sshanli,cqzong}@nlpr.ia.ac.cn      Abstract This paper addresses a new task in sentiment classification, called multi-domain sentiment classification, that aims to improve performance through fusing training data from multiple domains." ></td>
	<td class="line x" title="3:89	To achieve this, we propose two approaches of fusion, feature-level and classifier-level, to use training data from multiple domains simultaneously." ></td>
	<td class="line x" title="4:89	Experimental studies show that multi-domain sentiment classification using the classifier-level approach performs much better than single domain classification (using the training data individually)." ></td>
	<td class="line oc" title="5:89	1 Introduction Sentiment classification is a special task of text categorization that aims to classify documents according to their opinion of, or sentiment toward a given subject (e.g., if an opinion is supported or not) (Pang et al., 2002)." ></td>
	<td class="line p" title="6:89	This task has created a considerable interest due to its wide applications." ></td>
	<td class="line x" title="7:89	Sentiment classification is a very domainspecific problem; training a classifier using the data from one domain may fail when testing against data from another." ></td>
	<td class="line x" title="8:89	As a result, real application systems usually require some labeled data from multiple domains, guaranteeing an acceptable performance for different domains." ></td>
	<td class="line x" title="9:89	However, each domain has a very limited amount of training data due to the fact that creating largescale high-quality labeled corpora is difficult and time-consuming." ></td>
	<td class="line x" title="10:89	Given the limited multi-domain training data, an interesting task arises, how to best make full use of all training data to improve sentiment classification performance." ></td>
	<td class="line x" title="11:89	We name this new task, multi-domain sentiment classification." ></td>
	<td class="line x" title="12:89	In this paper, we propose two approaches to multi-domain sentiment classification." ></td>
	<td class="line x" title="13:89	In the first, called feature-level fusion, we combine the feature sets from all the domains into one feature set." ></td>
	<td class="line x" title="14:89	Using the unified feature set, we train a classifier using all the training data regardless of domain." ></td>
	<td class="line x" title="15:89	In the second approach, classifier-level fusion, we train a base classifier using the training data from each domain and then apply combination methods to combine the base classifiers." ></td>
	<td class="line oc" title="16:89	2 Related Work Sentiment classification has become a hot topic since the publication work that discusses classification of movie reviews by Pang et al.(2002)." ></td>
	<td class="line o" title="18:89	This was followed by a great many studies into sentiment classification focusing on many domains besides that of movie." ></td>
	<td class="line x" title="19:89	Research into sentiment classification over multiple domains remains sparse." ></td>
	<td class="line x" title="20:89	It is worth noting that Blitzer et al.(2007) deal with the domain adaptation problem for sentiment classification where labeled data from one domain is used to train a classifier for classifying data from a different domain." ></td>
	<td class="line x" title="22:89	Our work focuses on the problem of how to make multiple domains help each other when all contain some labeled samples." ></td>
	<td class="line x" title="23:89	These two problems are both important for real applications of sentiment classification." ></td>
	<td class="line x" title="24:89	3 Our Approaches 3.1 Problem Statement In a standard supervised classification problem, we seek a predictor f (also called a classifier) that 257 maps an input vector x to the corresponding class label y. The predictor is trained on a finite set of labeled examples { (,) ii X Y } (i=1,,n) and its objective is to minimize expected error, i.e., l arg min ( ( ), ) n ii f i f LfX Y  =    Where L is a prescribed loss function and H is a set of functions called the hypothesis space, which consists of functions from x to y. In sentiment classification, the input vector of one document is constructed from weights of terms." ></td>
	<td class="line x" title="25:89	The terms 1 ( ,, ) N tt are possibly words, word n-grams, or even phrases extracted from the training data, with N being the number of terms." ></td>
	<td class="line x" title="26:89	The output label y has a value of 1 or -1 representing a positive or negative sentiment classification." ></td>
	<td class="line x" title="27:89	In multi-domain classification, m different domains are indexed by k={1,,m}, each with k n training samples (,) kk ii XY {1, . . ., } kk in= . A straightforward approach is to train a predictor k f for the k-th domain only using the training data {( , )} kk ii XY . We call this approach single domain classification and show its architecture in Figure 1." ></td>
	<td class="line x" title="29:89	Figure 1: The architecture of single domain classification." ></td>
	<td class="line x" title="30:89	3.2 Feature-level Fusion Approach Although terms are extracted from multiple domains, some occur in all domains and convey the same sentiment (this can be called global sentiment information)." ></td>
	<td class="line x" title="31:89	For example, some terms like excellent and perfect express positive sentiment information independent of domain." ></td>
	<td class="line x" title="32:89	To learn the global sentiment information more correctly, we can pool the training data from all domains for training." ></td>
	<td class="line x" title="33:89	Our first approach is using a common set of terms 1 (',,' ) all N tt to construct a uniform feature vector 'x  and then train a predictor using all training data: m 11 arg min ( ( ' ), ) k kk all all k nm all i i f ki f LfX Y  == =    We call this approach feature-level fusion and show its architecture in Figure 2." ></td>
	<td class="line x" title="34:89	The common set of terms is the union of the term sets from multiple domains." ></td>
	<td class="line x" title="35:89	Figure 2: The architecture of the feature-level fusion approach  Feature-level fusion approach is simple to implement and needs no extra labeled data." ></td>
	<td class="line x" title="36:89	Note that training data from different domains contribute differently to the learning process for a specific domain." ></td>
	<td class="line x" title="37:89	For example, given data from three domains, books, DVDs and kitchen, we decide to train a classifier for classifying reviews from books." ></td>
	<td class="line x" title="38:89	As the training data from DVDs is much more similar to books than that from kitchen (Blitzer et al., 2007), we should give the data from DVDs a higher weight." ></td>
	<td class="line x" title="39:89	Unfortunately, the feature-level fusion approach lacks the capacity to do this." ></td>
	<td class="line x" title="40:89	A more qualified approach is required to deal with the differences among the classification abilities of training data from different domains." ></td>
	<td class="line x" title="41:89	3.3 Classifier-level Fusion Approach As mentioned in sub-Section 2.1, single domain classification is used to train a single classifier for each domain using the training data in the corresponding domain." ></td>
	<td class="line x" title="42:89	As all these single classifiers aim to determine the sentiment orientation of a document, a single classifier can certainly be used to classify documents from other domains." ></td>
	<td class="line x" title="43:89	Given multiple single classifiers, our second approach is to combine them to be a multiple classifier system for sentiment classification." ></td>
	<td class="line x" title="44:89	We call this approach classifier-level fusion and show its architecture in Figure 3." ></td>
	<td class="line x" title="45:89	This approach consists of two main steps: Training Data from Domain 1 Training Data from Domain 2 Training Data from Domain m Classifier 1 Classifier 2 Classifier m Testing Data from Domain 1 Testing Data from Domain 2 Testing Data from Domain m    . . ." ></td>
	<td class="line x" title="46:89	Training Data from Domain 1 Training Data from Domain 2 Training Data from Domain m Classifier Testing Data from Domain 1 Testing Data from Domain 2 Testing Data from Domain m    . . ." ></td>
	<td class="line x" title="47:89	Training Data from all Domains using a Uniform Feature Vector 258 (1) train multiple base classifiers (2) combine the base classifiers." ></td>
	<td class="line x" title="48:89	In the first step, the base classifiers are multiple single classifiers k f  (k=1,,m) from all domains." ></td>
	<td class="line x" title="49:89	In the second step, many combination methods can be applied to combine the base classifiers." ></td>
	<td class="line x" title="50:89	A well-known method called meta-learning (ML) has been shown to be very effective (Vilalta and Drissi, 2002)." ></td>
	<td class="line x" title="51:89	The key idea behind this method is to train a meta-classifier with input attributes that are the output of the base classifiers." ></td>
	<td class="line x" title="52:89	Figure 3: The architecture of the classifier-level fusion approach  Formally, let 'k X denote a feature vector of a sample from the development data of the '-thk domain ( ' 1,, )km= . The output of the -thk base classifier k f on this sample is the probability distribution over the set of classes 12 {, ,, } n cc c , i.e., '1' ' ( )  ( | ),, ( | ) kk k k kn k pX pcX pc X=< > For the '-thk domain, we train a meta-classifier '  ( ' 1,, ) k f km= using the development data from the '-thk domain with the meta-level feature vector ' meta m n k XR   '1' ' '  (),.,(),.,() meta kkkmk X pXpXpX=< > Each meta-classifier is then used to test the testing data from the same domain." ></td>
	<td class="line x" title="53:89	Different from the feature-level approach, the classifier-level approach treats the training data from different domains individually and thus has the ability to take the differences in classification abilities into account." ></td>
	<td class="line x" title="54:89	4 Experiments Data Set:  We carry out our experiments on the labeled product reviews from four domains: books, DVDs, electronics, and kitchen appliances 1 . Each domain contains 1,000 positive and 1,000 negative reviews." ></td>
	<td class="line pc" title="55:89	Experiment Implementation: We apply SVM algorithm to construct our classifiers which has been shown to perform better than many other classification algorithms (Pang et al., 2002)." ></td>
	<td class="line x" title="56:89	Here, we use LIBSVM 2 with a linear kernel function for training and testing." ></td>
	<td class="line x" title="57:89	In our experiments, the data in each domain are partitioned randomly into training data, development data and testing data with the proportion of 70%, 20% and 10% respectively." ></td>
	<td class="line x" title="58:89	The development data are used to train the meta-classifier." ></td>
	<td class="line x" title="59:89	Baseline: The baseline uses the single domain classification approach mentioned in sub-Section 2.1." ></td>
	<td class="line x" title="60:89	We test four different feature sets to construct our feature vector." ></td>
	<td class="line x" title="61:89	First, we use unigrams (e.g., happy) as features and perform the standard feature selection process to find the optimal feature set of unigrams (1Gram)." ></td>
	<td class="line x" title="62:89	The selection method is Bi-Normal Separation (BNS) that is reported to be excellent in many text categorization tasks (Forman, 2003)." ></td>
	<td class="line x" title="63:89	The criterion of the optimization is to find the set of unigrams with the best performance on the development data through selecting the features with high BNS scores." ></td>
	<td class="line x" title="64:89	Then, we get the optimal word bi-gram (e.g., very happy) (2Gram) and mixed feature set (1+2Gram) in the same way." ></td>
	<td class="line x" title="65:89	The fourth feature set (1Gram+2Gram) also consists of unigrams and bi-grams just like the third one." ></td>
	<td class="line x" title="66:89	The difference between them lies in their selection strategy." ></td>
	<td class="line x" title="67:89	The third feature set is obtained through selecting the unigrams and bi-grams with high BNS scores while the fourth one is obtained through simply uniting the two optimal sets of 1Gram and 2Gram." ></td>
	<td class="line x" title="68:89	From Table 1, we see that 1Gram+2Gram features perform much better than other types of features, which implies that we need to select good unigram and bi-gram features separately before combine them." ></td>
	<td class="line x" title="69:89	Although the size of our training data are smaller than that reported in Blitzer et al.  1  This data set is collected by Blitzer et al.(2007): http://www.seas.upenn.edu/~mdredze/datasets/sentiment/ 2  LIBSVM is an integrated software for SVM: http://www.csie.ntu.edu.tw/~cjlin/libsvm/ Training Data from Domain 1 Training Data from Domain 2 Training Data from Domain m Multiple Classifier System 1 Testing Data from Domain 1 Testing Data from Domain 2 Testing Data from Domain m    . . ." ></td>
	<td class="line x" title="71:89	Base Classifier 1 Base Classifier 2 Base Classifier m    . . ." ></td>
	<td class="line x" title="72:89	Multiple Classifier System 2 Multiple Classifier System m Development Data from Domain 1 Development Data from Domain 2 Development Data from Domain m    . . ." ></td>
	<td class="line x" title="73:89	259 (2007) (70% vs. 80%), the classification performance is comparative to theirs." ></td>
	<td class="line x" title="74:89	We implement the fusion using 1+2Gram and 1Gram+2Gram respectively." ></td>
	<td class="line x" title="75:89	From Figure 4, we see that both the two fusion approaches generally outperform single domain classification when using 1+2Gram features." ></td>
	<td class="line x" title="76:89	They increase the average accuracy from 0.8 to 0.82375 and 0.83875, a significant relative error reduction of 11.87% and 19.38% over baseline." ></td>
	<td class="line x" title="77:89	1+2Gram Features 76.5 81 80 83 82.5 82.5 82.5 81 83 84 86 83 72 74 76 78 80 82 84 86 88 Books DVDs Electronics Kitchen Ac cur acy (%)  1Gram+2Gram Features 79 84.5 84 82 84.5 85 83 82 83.5 86 88 89 74 76 78 80 82 84 86 88 90 Books DVDs Electronics Kitchen Ac cura cy( %) Single domain classification Feature-level fusion Classifier-level fusion with ML  Figure 4: Accuracy results on the testing data using multi-domain classification with different approaches." ></td>
	<td class="line x" title="78:89	However, when the performance of baseline increases, the feature level approach fails to help the performance improvement in three domains." ></td>
	<td class="line x" title="79:89	This is mainly because the base classifiers perform extremely unbalanced on the testing data of these domains." ></td>
	<td class="line x" title="80:89	For example, the four base classifiers from Books, DVDs, Electronics, and Kitchen achieve the accuracies of 0.675, 0.62, 0.85, and 0.79 on the testing data from Electronics respectively." ></td>
	<td class="line x" title="81:89	Dealing with such an unbalanced performance, we definitely need to put enough high weight on the training data from Electronics." ></td>
	<td class="line x" title="82:89	However, the feature-level fusion approach simply pools all training data from different domains and treats them equally." ></td>
	<td class="line x" title="83:89	Thus it can not capture the unbalanced information." ></td>
	<td class="line x" title="84:89	In contrast, metalearning is able to learn the unbalance automatically through training the meta-classifier using the development data." ></td>
	<td class="line x" title="85:89	Therefore, it can still increase the average accuracy from 0.8325 to 0.8625, an impressive relative error reduction of 17.91% over baseline." ></td>
	<td class="line x" title="86:89	5 Conclusion In this paper, we propose two approaches to multidomain classification task on sentiment classification." ></td>
	<td class="line x" title="87:89	Empirical studies show that the classifierlevel approach generally outperforms the feature approach." ></td>
	<td class="line x" title="88:89	Compared to single domain classification, multi-domain classification with the classifier-level approach can consistently achieve much better results." ></td>
	<td class="line x" title="89:89	Acknowledgments The research work described in this paper has been partially supported by the Natural Science Foundation of China under Grant No. 60575043, and 60121302, National High-Tech Research and Development Program of China under Grant No. 2006AA01Z194, National Key Technologies R&D Program of China under Grant No. 2006BAH03B02, and Nokia (China) Co. Ltd as well." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1017
Review Sentiment Scoring via a Parse-and-Paraphrase Paradigm
Liu, Jingjing;Seneff, Stephanie;"></td>
	<td class="line x" title="1:229	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 161169, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:229	c 2009 ACL and AFNLP Review Sentiment Scoring via a Parse-and-Paraphrase Paradigm   Jingjing Liu, Stephanie Seneff MIT Computer Science & Artificial Intelligence Laboratory 32 Vassar Street, Cambridge, MA 02139 {jingl, seneff}@csail.mit.edu    Abstract  This paper presents a parse-and-paraphrase paradigm to assess the degrees of sentiment for product reviews." ></td>
	<td class="line x" title="3:229	Sentiment identification has been well studied; however, most previous work provides binary polarities only (positive and negative), and the polarity of sentiment is simply reversed when a negation is detected." ></td>
	<td class="line x" title="4:229	The extraction of lexical features such as unigram/bigram also complicates the sentiment classification task, as linguistic structure such as implicit long-distance dependency is often disregarded." ></td>
	<td class="line x" title="5:229	In this paper, we propose an approach to extracting adverb-adjective-noun phrases based on clause structure obtained by parsing sentences into a hierarchical representation." ></td>
	<td class="line x" title="6:229	We also propose a robust general solution for modeling the contribution of adverbials and negation to the score for degree of sentiment." ></td>
	<td class="line x" title="7:229	In an application involving extracting aspect-based pros and cons from restaurant reviews, we obtained a 45% relative improvement in recall through the use of parsing methods, while also improving precision." ></td>
	<td class="line x" title="8:229	1 Introduction Online product reviews have provided an extensive collection of free-style texts as well as product ratings prepared by general users, which in return provide grassroots contributions to users interested in a particular product or service as assistance." ></td>
	<td class="line x" title="9:229	Yet, valuable as they are, free-style reviews contain much noisy data and are tedious to read through in order to reach an overall conclusion." ></td>
	<td class="line x" title="10:229	Thus, we conducted this study to automatically process and evaluate product reviews in order to generate both numerical evaluation and textual summaries of users opinions, with the ultimate goal of adding value to real systems such as a restaurant-guide dialogue system." ></td>
	<td class="line oc" title="11:229	Sentiment summarization has been well studied in the past decade (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004a, 2004b; Carenini et al., 2006; Liu et al., 2007)." ></td>
	<td class="line x" title="12:229	The polarity of users sentiments in each segment of review texts is extracted, and the polarities of individual sentiments are aggregated among all the sentences/segments of texts to give a numerical scaling on sentiment orientation." ></td>
	<td class="line x" title="13:229	Most of the work done for sentiment analysis so far has employed shallow parsing features such as part-of-speech tagging." ></td>
	<td class="line x" title="14:229	Frequent adjectives and nouns/noun phrases are extracted as opinion words and representative product features." ></td>
	<td class="line x" title="15:229	However, the linguistic structure of the sentence is usually not taken into consideration." ></td>
	<td class="line x" title="16:229	High level linguistic features, if well utilized and accurately extracted, can provide much insight into the semantic meaning of user opinions and contribute to the task of sentiment identification." ></td>
	<td class="line x" title="17:229	Furthermore, in addition to adjectives and nouns, adverbials and negation also play an important role in determining the degree of the orientation level." ></td>
	<td class="line x" title="18:229	For example, very good and good certainly express different degrees of positive sentiment." ></td>
	<td class="line x" title="19:229	Also, in previous studies, when negative expressions are identified, the polarity of sentiment in the associated segment of text is simply reversed." ></td>
	<td class="line x" title="20:229	However, semantic expressions are quite different from the absolute opposite values in mathematics." ></td>
	<td class="line x" title="21:229	For example, not bad does not express the opposite meaning of bad, which would be highly positive." ></td>
	<td class="line x" title="22:229	Simply reversing the polarity of sentiment on the appearance of negations may result in inaccurate interpretation of sentiment expressions." ></td>
	<td class="line x" title="23:229	Thus, a system which attempts to quantify sentiment while ignoring adverbials is missing a significant component of the sentiment score, especially if the adverbial is a negative word." ></td>
	<td class="line x" title="24:229	161 Another challenging aspect of negation is proper scoping of the negative reference over the right constituent, which we argue, can be handled quite well with careful linguistic analysis." ></td>
	<td class="line x" title="25:229	Take the sentence I dont think the place is very clean as example." ></td>
	<td class="line x" title="26:229	A linguistic approach associating long-distance elements with semantic relations can identify that the negation not scopes over the complement clause, thus extracting not very clean instead of very clean." ></td>
	<td class="line x" title="27:229	Our goal in modeling adverbials is to investigate whether a simple linear correction model can capture the polarity contribution of all adverbials." ></td>
	<td class="line x" title="28:229	Furthermore, is it also appropriate to adjust for multiple adverbs, including negation, via a linear additive model?" ></td>
	<td class="line x" title="29:229	I.e., can not very good be modeled as not(very(good))?" ></td>
	<td class="line x" title="30:229	The fact that not very good seems to be less negative than not good suggests that such an algorithm might work well." ></td>
	<td class="line x" title="31:229	From these derivations we have developed a model which treats negations in the exact same way as modifying adverbs, via an accumulative linear offset model." ></td>
	<td class="line x" title="32:229	This yields a very generic and straightforward solution to modeling the strength of sentiment expression." ></td>
	<td class="line x" title="33:229	In this paper we utilize a parse-and-paraphrase paradigm to identify semantically related phrases in review texts, taking quantifiers (e.g., modifying adverbs) and qualifiers (e.g., negations) into special consideration." ></td>
	<td class="line x" title="34:229	The approach makes use of a lexicalized probabilistic syntactic grammar to identify and extract sets of adverb-adjectivenoun phrases that match review-related patterns." ></td>
	<td class="line x" title="35:229	Such patterns are constructed based on wellformed linguistic structure; thus, relevant phrases can be extracted reliably." ></td>
	<td class="line x" title="36:229	We also propose a cumulative linear offset model to calculate the degree of sentiment for joint adjectives and quantifiers/qualifiers." ></td>
	<td class="line x" title="37:229	The proposed sentiment prediction model takes modifying adverbs and negations as universal scales on strength of sentiment, and conducts cumulative calculation on the degree of sentiment for the associated adjective." ></td>
	<td class="line x" title="38:229	With this model, we can provide not only qualitative textual summarization such as good food and bad service, but also a numerical scoring of sentiment, i.e., how good the food is and how bad the service is. 2 Related Work There have been many studies on sentiment classification and opinion summarization (Pang and Lee, 2004, 2005; Gamon et al., 2005; Popescu and Etzioni, 2005; Liu et al., 2005; Zhuang et al., 2006; Kim and Hovy, 2006)." ></td>
	<td class="line x" title="39:229	Specifically, aspect rating as an interesting topic has also been widely studied (Titov and McDonald, 2008a; Snyder and Barzilay, 2007; Goldberg and Zhu, 2006)." ></td>
	<td class="line x" title="40:229	Recently, Baccianella et." ></td>
	<td class="line x" title="41:229	al." ></td>
	<td class="line x" title="42:229	(2009) conducted a study on multi-facet rating of product reviews with special emphasis on how to generate vectorial representations of the text by means of POS tagging, sentiment analysis, and feature selection for ordinal regression learning." ></td>
	<td class="line x" title="43:229	Titov and McDonald (2008b) proposed a joint model of text and aspect ratings which utilizes a modified LDA topic model to build topics that are representative of ratable aspects, and builds a set of sentiment predictors." ></td>
	<td class="line x" title="44:229	Branavan et al.(2008) proposed a method for leveraging unstructured annotations in product reviews to infer semantic document properties, by clustering user annotations into semantic properties and tying the induced clusters to hidden topics in the text." ></td>
	<td class="line x" title="46:229	3 System Overview Our review summarization task is to extract sets of descriptor-topic pairs (e.g., excellent service) from a set of reviews (e.g., for a particular restaurant), and to cluster the extracted phrases into representative aspects on a set of dimensions (e.g., food, service and atmosphere)." ></td>
	<td class="line x" title="47:229	Driven by this motivation, we propose a three-stage system that automatically processes reviews." ></td>
	<td class="line x" title="48:229	A block diagram is given in Figure 1." ></td>
	<td class="line x" title="49:229	Figure 1." ></td>
	<td class="line x" title="50:229	Framework of review processing." ></td>
	<td class="line x" title="51:229	The first stage is sentence-level data filtering." ></td>
	<td class="line x" title="52:229	Review data published by general users is often in free-style, and a large fraction of the data is either ill-formed or not relevant to the task." ></td>
	<td class="line x" title="53:229	We classify these as out of domain sentences." ></td>
	<td class="line x" title="54:229	To filter out such noisy data, we collect unigram statistics on all the relevant words in the corpus, and select high frequency adjectives and nouns." ></td>
	<td class="line x" title="55:229	Any sentence that contains none of the highfrequency nouns or adjectives is rejected from further analysis." ></td>
	<td class="line x" title="56:229	The remaining in-domain sentences are subjected to the second stage, parse 162 analysis and semantic understanding, for topic extraction." ></td>
	<td class="line x" title="57:229	From the parsable sentences we extract descriptor-topic phrase patterns based on a carefully-designed generation grammar." ></td>
	<td class="line x" title="58:229	We then apply LM (language model) based topic clustering to group the extracted phrases into representative aspects." ></td>
	<td class="line x" title="59:229	The third stage scores the degree of sentiment for adjectives, as well as the strength of sentiment for modifying adverbs and negations, which further refine the degree of sentiment of the associated adjectives." ></td>
	<td class="line x" title="60:229	We then run a linear additive model to assign a combined sentiment score for each extracted phrase." ></td>
	<td class="line x" title="61:229	The rest of the paper is structured as follows: In Section 4, we explain the linguistic analysis." ></td>
	<td class="line x" title="62:229	In Section 5, we describe the cumulative model for assessing the degree of sentiment." ></td>
	<td class="line x" title="63:229	Section 6 provides a systematic evaluation, conducted on real data in the restaurant review domain harvested from the Web." ></td>
	<td class="line x" title="64:229	Section 7 provides a discussion analyzing the results." ></td>
	<td class="line x" title="65:229	Section 8 summarizes the paper as well as pointing to future work." ></td>
	<td class="line x" title="66:229	4 Linguistic Analysis 4.1 Parse-and-Paraphrase Our linguistic analysis is based on a parse-andparaphrase paradigm." ></td>
	<td class="line x" title="67:229	Instead of the flat structure of a surface string, the parser provides a hierarchical representation, which we call a linguistic frame (Xu et al., 2008)." ></td>
	<td class="line x" title="68:229	It preserves linguistic structure by encoding different layers of semantic dependencies." ></td>
	<td class="line x" title="69:229	The grammar captures syntactic structure through a set of carefully constructed context free grammar rules, and employs a feature-passing mechanism to enforce long distance constraints." ></td>
	<td class="line x" title="70:229	The grammar is lexicalized, and uses a statistical model to rank order competing hypotheses." ></td>
	<td class="line x" title="71:229	It knows explicitly about 9,000 words, with all unknown words being interpreted as nouns." ></td>
	<td class="line x" title="72:229	The grammar probability model was trained automatically on the corpus of review sentences." ></td>
	<td class="line x" title="73:229	To produce the phrases, a set of generation rules is carefully constructed to only extract sets of related adverbs, adjectives and nouns." ></td>
	<td class="line x" title="74:229	The adjective-noun relationships are captured from the following linguistic patterns: (1) all adjectives attached directly to a noun in a noun phrase, (2) adjectives embedded in a relative clause modifying a noun, and (3) adjectives related to nouns in a subject-predicate relationship in a clause." ></td>
	<td class="line x" title="75:229	These patterns are compatible, i.e., if a clause contains both a modifying adjective and a predicate adjective related to the same noun, two adjective-noun pairs are generated by different patterns." ></td>
	<td class="line x" title="76:229	As in, The efficient waitress was nonetheless very courteous. It is a parse-andparaphrase-like paradigm: the paraphrase tries to preserve the original words intact, while reordering them and/or duplicating them into multiple NP units." ></td>
	<td class="line x" title="77:229	Since they are based on syntactic structure, the generation rules can also be applied in any other domain involving opinion mining." ></td>
	<td class="line x" title="78:229	An example linguistic frame is shown in Figure 2, which encodes the sentence The caesar with salmon or chicken is really quite good. In this example, for the adjective good, the nearby noun chicken would be associated with it if only proximity is considered." ></td>
	<td class="line x" title="79:229	From the linguistic frame, however, we can easily associate caesar with good by extracting the head of the topic sub-frame and the head of the predicate subframe, which are encoded in the same layer (root layer) of the linguistic frame." ></td>
	<td class="line x" title="80:229	Also, we can tell from the predicate sub-frame that there is an adverb quite modifying the head word good." ></td>
	<td class="line x" title="81:229	The linguistic frame also encodes an adverb really in the upstairs layer." ></td>
	<td class="line x" title="82:229	A well-constructed generation grammar can create customized adverb-adjective-noun phrases such as quite good caesar or really quite good caesar." ></td>
	<td class="line x" title="83:229	{c cstatement   :topic {q caesar              :quantifier 'def'              :pred {p with :topic {q salmon                                          :pred {p conjunction                                            :or {q chicken  }}}}   :adv 'really'    :pred {p adj_complement             :pred {p adjective                     :adv 'quite'                 :pred {p quality :topic 'good' }}}} Figure 2." ></td>
	<td class="line x" title="84:229	Linguistic frame for The caesar with salmon or chicken is really quite good. Interpreting negation in English is not straightforward, and it is often impossible to do correctly without a deep linguistic analysis." ></td>
	<td class="line x" title="85:229	Xuehui Wu (2005) wrote: The scope of negation is a complex linguistic phenomenon." ></td>
	<td class="line x" title="86:229	It is easy to perceive but hard to be defined from a syntactic point of view." ></td>
	<td class="line x" title="87:229	Misunderstanding or ambiguity may occur when the negative scope is not understood clearly and correctly. The majority rule for negation is that it scopes over the remainder of its containing clause, and this works well for most cases." ></td>
	<td class="line x" title="88:229	For example, Figure 3 shows 163 the linguistic frame for the sentence Their menu was a good one that didnt try to do too much. {c cstatement :topic {q menu   :poss 'their' } }    :complement {q pronoun   :name one              :adj_clause {c cstatement                            :conjn 'that'                            :negate 'not'                            :pred {p try :to_clause  {p do                                            :topic {q object                                            :adv 'too'                                            :quant 'much' }}}}    :pred {p adjective                 :pred {p quality :topic 'good' }}} Figure 3." ></td>
	<td class="line x" title="89:229	Linguistic frame for Their menu was a good one that didnt try to do too much. Traditional approaches which do not consider the linguistic structure would treat the appearance of not as a negation and simply reverse the sentiment of the sentence to negative polarity, which is wrong as the sentence actually expresses positive opinion for the topic menu." ></td>
	<td class="line x" title="90:229	In our approach, the negation not is identified as under the sub-frame of the complement clause, instead of in the same or higher layer of the adjective sub-frame; thus it is considered as unrelated to the adjective good." ></td>
	<td class="line x" title="91:229	In this way we can successfully predict the scope of the reference of the negation over the correct constituent of a sentence and create proper association between negation and its modified words." ></td>
	<td class="line x" title="92:229	4.2 LM-based Topic Clustering To categorize the extracted phrases into representative aspects, we automatically group the identified topics into a set of clusters based on LM probabilities." ></td>
	<td class="line x" title="93:229	The LM-based algorithm assumes that topics which are semantically related have high probability of co-occurring with similar descriptive words." ></td>
	<td class="line x" title="94:229	For example, delicious might co-occur frequently with both pizza and dessert." ></td>
	<td class="line x" title="95:229	By examining the distribution of bigram probability of these topics with corresponding descriptive words, we can group pizza and dessert into the same cluster of food." ></td>
	<td class="line x" title="96:229	We select a small set of the most common topics, i.e., topics with the highest frequency counts, and put them into an initial set I. Then, for each candidate topic g1872g1855 outside set I, we calculate its probability given each topic g1872g1861 within the initial set I, given by:        g1842g4666g1872g3030| g1872g3036g4667g3404 g1842g4666g1872g3030|g1853g4667g1842g4666g1853|g1872g3036g4667g3028g1488g3002                       g3404  g3017g4666g3028,g3047g3278g4667g3017g4666g3028g4667 g3017g4666g3028,g3047g3284g4667g3017g4666g3047 g3284g4667g3028g1488g3002                     g3404 g2869g3017g4666g3047 g3284g4667  g2869g3017g4666g3028g4667g1842g4666g1853,g1872g3030g4667g1842g4666g1853,g1872g3036g4667g3028g1488g3002        (1) where A represents the set of all the adjectives in the corpus." ></td>
	<td class="line x" title="97:229	For each candidate topic g1872g1855 , we choose the cluster of the initial topic g1872g1861  with which it has the highest probability score." ></td>
	<td class="line x" title="98:229	There are also cases where a meaningful adjective occurs in the absence of an associated topic, e.g., It is quite expensive. We call such cases the widow-adjective case." ></td>
	<td class="line x" title="99:229	Without hardcoded ontology matching, it is difficult to identify expensive as a price-related expression." ></td>
	<td class="line x" title="100:229	To discover such cases and associate them with related topics, we propose a surrogate topic matching approach based on bigram probability." ></td>
	<td class="line x" title="101:229	As aforementioned, the linguistic frame organizes all adjectives into separate clauses." ></td>
	<td class="line x" title="102:229	Thus, we create a surrogate topic category in the linguistic frames for widow-adjective cases, which makes it easy to detect descriptors that are affiliated with uninformative topics like the pronoun it." ></td>
	<td class="line x" title="103:229	We then have it generate phrases such as expensive surrogate_topic and use bigram probability statistics to automatically map each sufficiently strongly associated adjective to its most common topic among our major classes, e.g., mapping expensive with its surrogate topic price." ></td>
	<td class="line x" title="104:229	Therefore, we can generate sets of additional phrases in which the topic is hallucinated from the widow-adjective." ></td>
	<td class="line x" title="105:229	5 Assessment of Sentiment Strength 5.1 Problem Formulation Given the sets of adverb-adjective-noun phrases extracted by linguistic analysis, our goal is to assign a score for the degree of sentiment to each phrase and calculate an average rating for each aspect." ></td>
	<td class="line x" title="106:229	An example summary is given in Table 1." ></td>
	<td class="line x" title="107:229	Table 1." ></td>
	<td class="line x" title="108:229	Example of review summary." ></td>
	<td class="line x" title="109:229	Aspect Extracted phrases Rating Atmosphere very nice ambiance, outdoor patio 4.8 Food not bad meal, quite authentic food 4.1 Place not great place, very smoky restaurant 2.8 Price so high bill, high cost, not cheap price 2.2 To calculate the numerical degree of sentiment, there are three major problems to solve: 1) how to associate numerical scores with textual sentiment; 2) whether to calculate sentiment scores for adjectives and adverbs jointly or separately; 3) 164 whether to treat negations as special cases or in the same way as modifying adverbs." ></td>
	<td class="line x" title="110:229	There have been studies on building sentiment lexicons to define the strength of sentiment of words." ></td>
	<td class="line x" title="111:229	Esuli and Sebastiani (2006) constructed a lexical resource, SentiWordNet, a WordNet-like lexicon emphasizing sentiment orientation of words and providing numerical scores of how objective, positive and negative these words are." ></td>
	<td class="line x" title="112:229	However, lexicon-based methods can be tedious and inefficient and may not be accurate due to the complex cross-relations in dictionaries like WordNet." ></td>
	<td class="line x" title="113:229	Instead, our primary approach to sentiment scoring is to make use of collective data such as user ratings." ></td>
	<td class="line x" title="114:229	In product reviews collected from online forums, the format of a review entry often consists of three parts: pros/cons, free-style text and user rating." ></td>
	<td class="line x" title="115:229	We assume that user rating is normally consistent with the tone of the review text published by the same user." ></td>
	<td class="line x" title="116:229	By associating user ratings with each phrase extracted from review texts, we can easily associate numerical scores with textual sentiment." ></td>
	<td class="line x" title="117:229	A simple strategy of rating assignment is to take each extracted adverb-adjective pair as a composite unit." ></td>
	<td class="line x" title="118:229	However, this method is likely to lead to a large number of rare combinations, thus suffering from sparse data problems." ></td>
	<td class="line x" title="119:229	Therefore, an interesting question to ask is whether it is feasible to assign to each adverb a perturbation score, which adjusts the score of the associated adjective up or down by a fixed scalar value." ></td>
	<td class="line x" title="120:229	This approach thus hypothesizes that very expensive is as much worse than expensive as very romantic is better than romantic." ></td>
	<td class="line x" title="121:229	This allows us to pool all instances of a given adverb regardless of which adjective it is associated with, in order to compute the absolute value of the perturbation score for that adverb." ></td>
	<td class="line x" title="122:229	Therefore, we consider adverbs and adjectives separately when calculating the sentiment score, treating each modifying adverb as a universal quantifier which consistently scales up/down the strength of sentiment for the adjectives it modifies." ></td>
	<td class="line x" title="123:229	Furthermore, instead of treating negation as a special case, the universal model works for all adverbials." ></td>
	<td class="line x" title="124:229	The model hypothesizes that not bad is as much better than bad as not good is worse than good, i.e., negations push positive/negative adjectives to the other side of sentiment polarity by a universal scale." ></td>
	<td class="line x" title="125:229	This again, allows us to pool all instances of a given negation and compute the absolute value of the perturbation score for that negation, in the same way as dealing with modifying adverbs." ></td>
	<td class="line x" title="126:229	5.2 Linear Additive Model For each adjective, we average all its ratings given by: g1845g1855g1867g1870g1857g4666g1853g1856g1862g4667g3404   g3263g3289g3293 g3284   g3045g3284g3284g1488g3265   g3263g3289g3293 g3284 g3293g3284             (2) where g1842 represents the set of appearances of adjective g1853g1856g1862, g1870g3036 represents the associated user rating in each appearance of g1853g1856g1862, g1840 represents the number of entities (e.g., restaurants) in the entire data set, and g1866g3045g3284 represents the number of entities with rating g1870g3036." ></td>
	<td class="line x" title="127:229	The score is averaged over all the appearances, weighted by the frequency count of each category of rating to remove bias towards any category." ></td>
	<td class="line x" title="128:229	As for adverbs, using a slightly modified version of equation (2), we can get a rating table for all adverb-adjective pairs." ></td>
	<td class="line x" title="129:229	For each adverb adv, we get a list of all its possible combinations with adjectives." ></td>
	<td class="line x" title="130:229	Then, for each adj in the list, we calculate the distance between the rating of adv-adj and the rating of the adj alone." ></td>
	<td class="line x" title="131:229	We then aggregate the distances among all the pairs of adv-adj and adj in the list, weighted by the frequency count of each adv-adj pair: g1845g1855g1867g1870g1857g4666g1853g1856g1874g4667g3404  g3030g3042g3048g3041g3047g4666g3028g3031g3049,g3028g3031g3037g3295g4667 g3030g3042g3048g3041g3047g3435g3028g3031g3049,g3028g3031g3037 g3285g3439g3285g1488g3250g3047g1488g3002  g1842g1867g1864g4666g1853g1856g1862g3047g4667g4666g1870g4666g1853g1856g1874,g1853g1856g1862g3047g4667g3398g1870g4666g1853g1856g1862g3047g4667g4667          (3) where g1855g1867g1873g1866g1872g4666g1853g1856g1874, g1853g1856g1862g3047g4667 represents the count of the combination g1853g1856g1874g3398g1853g1856g1862g3047, g1827 represents the set of adjectives that co-occur with g1853g1856g1874 , g1870g4666g1853g1856g1874,g1853g1856g1862g3047g4667 represents the sentiment rating of the combination g1853g1856g1874g3398g1853g1856g1862g3047 , and g1870g4666g1853g1856g1862g3047g4667 represents the sentiment rating of the adjective g1853g1856g1862g3047 alone." ></td>
	<td class="line x" title="132:229	g1842g1867g1864g4666g1853g1856g1862g3047g4667 represents the polarity of g1853g1856g1862g3047, assigned as 1 if g1853g1856g1862g3047 is positive, and -1 if negative." ></td>
	<td class="line x" title="133:229	Specifically, negations are well handled by the same scoring strategy, treated exactly the same as modifying adverbs, except that they get such strong negative scores that the sentiment of the associated adjectives is pushed to the other side of the polarity scale." ></td>
	<td class="line x" title="134:229	After obtaining the strength rating for adverbs and the sentiment rating for adjectives, the next step is to assign the strength of sentiment to each phrase (negation-adverb-adjective-noun) extracted by linguistic analysis, as given by: g1845g1855g1867g1870g1857g4672g1866g1857g1859g3435g1853g1856g1874g4666g1853g1856g1862g4667g3439g4673g3404g1870g4666g1853g1856g1862g4667g3397 g1842g1867g1864g4666g1853g1856g1862g4667g1870g4666g1853g1856g1874g4667g3397g1842g1867g1864g4666g1853g1856g1862g4667g1870g4666g1866g1857g1859g4667       (4) 165 where g1870g4666g1853g1856g1862g4667 represents the rating of adjective g1853g1856g1862, g1870g4666g1853g1856g1874g4667 represents the rating of adverb g1853g1856g1874, and g1870g4666g1866g1857g1859g4667 represents the rating of negation g1866g1857g1859." ></td>
	<td class="line x" title="135:229	g1842g1867g1864g4666g1853g1856g1862g4667 represents the polarity of g1853g1856g1862, assigned as 1 if g1853g1856g1862 is positive, and -1 if negative." ></td>
	<td class="line x" title="136:229	Thus, if g1853g1856g1862 is positive, we assign a combined rating g1870g4666g1853g1856g1862g4667g3397g1870g4666g1853g1856g1874g4667 to this phrase." ></td>
	<td class="line x" title="137:229	If it is negative, we assign g1870g4666g1853g1856g1862g4667g3398g1870g4666g1853g1856g1874g4667." ></td>
	<td class="line x" title="138:229	Specifically, if it is a negation case, we further assign a linear offset g1870g4666g1866g1857g1859g4667 if g1853g1856g1862 is positive or g3398g1870g4666g1866g1857g1859g4667 if g1853g1856g1862 is negative." ></td>
	<td class="line x" title="139:229	For example, given the ratings <good: 4.5>, <bad: 1.5>, <very: 0.5> and <not: -3.0>, we would assign 5.0 to very good (score(very(good))=4.5+0.5), 1.0 to very bad (score(very(bad))=1.5-0.5), and 2.0 to not very good (score(not(very(good)))= 4.5+0.53.0)." ></td>
	<td class="line x" title="140:229	The corresponding sequence of different degrees of sentiment is: very good: 5.0 > good: 4.5 > not very good: 2.0 > bad: 1.5 > very bad: 1.0." ></td>
	<td class="line x" title="141:229	6 Experiments In this section we present a systematic evaluation of the proposed approaches conducted on real data." ></td>
	<td class="line x" title="142:229	We crawled a data collection of 137,569 reviews on 24,043 restaurants in 9 cities in the U.S. from an online restaurant evaluation website1." ></td>
	<td class="line x" title="143:229	Most of the reviews have both pros/cons and free-style text." ></td>
	<td class="line x" title="144:229	For the purpose of evaluation, we take those reviews containing pros/cons as the experimental set, which is 72.7% (99,147 reviews) of the original set." ></td>
	<td class="line x" title="145:229	6.1 Topic Extraction Based on the experimental set, we first filtered out-of-domain sentences based on frequency count, leaving a set of 857,466 in-domain sentences (67.5%)." ></td>
	<td class="line x" title="146:229	This set was then subjected to parse analysis; 78.6% of them are parsable." ></td>
	<td class="line x" title="147:229	Given the parsing results in the format of linguistic frame, we used a set of language generation rules to extract relevant adverb-adjectivenoun phrases." ></td>
	<td class="line x" title="148:229	We then selected the most frequent 6 topics that represented appropriate dimensions for the restaurant domain (place, food, service, price, atmosphere and portion) as the initial set, and clustered the extracted topic mentions into different aspect categories by creating a set of topic mappings with the LMbased clustering method." ></td>
	<td class="line x" title="149:229	Phrases not belonging to any category are filtered out." ></td>
	<td class="line x" title="150:229	1  http://www.citysearch.com To evaluate the performance of the proposed approach (LING) to topic extraction, we compare it with a baseline method similar to (Hu and Liu, 2004a, 2004b; Liu et al., 2005)." ></td>
	<td class="line x" title="151:229	We performed part-of-speech tagging on both parsable and unparsable sentences, extracted each pair of noun and adjective that has the smallest proximity, and filtered out those with low frequency counts." ></td>
	<td class="line x" title="152:229	Adverbs and negation words that are adjacent to the identified adjectives were also extracted along with the adjective-noun pairs." ></td>
	<td class="line x" title="153:229	We call this the neighbor baseline (NB)." ></td>
	<td class="line x" title="154:229	The proposed method is unable to make use of the non-parsable sentences, which make up over 20% of the data." ></td>
	<td class="line x" title="155:229	Hence, it seems plausible to utilize a back-off mechanism for these sentences via a combined system (COMB) incorporating NB only for the sentences that fail to parse." ></td>
	<td class="line x" title="156:229	In considering how to construct the ground truth set of pros and cons for particular aspects, our goal was to minimize error as much as possible without requiring exorbitant amounts of manual labeling." ></td>
	<td class="line x" title="157:229	We also wanted to assure that the methods were equally fair to both systems (LING and NB)." ></td>
	<td class="line x" title="158:229	To these ends, we decided to pool together all of the topic mappings and surrogate topic hallucinations obtained automatically from both systems, and then to manually edit the resulting list to eliminate any that were deemed unreasonable." ></td>
	<td class="line x" title="159:229	We then applied these edited mappings in an automatic procedure to the adjective-noun pairs in the user-provided pros and cons of all the restaurant reviews." ></td>
	<td class="line x" title="160:229	The resulting aspect-categorized phrase lists are taken as the ground truth." ></td>
	<td class="line x" title="161:229	Each system then used its own (unedited) set of mappings in processing the associated review texts." ></td>
	<td class="line x" title="162:229	We also needed an algorithm to decide on a particular set of reviews for consideration, again, with the goal of omitting bias towards either of the two systems." ></td>
	<td class="line x" title="163:229	We decided to retain as the evaluation set all reviews which obtained at least one topic extraction from both systems." ></td>
	<td class="line x" title="164:229	Thus the two systems processed exactly the same data with exactly the same definitions of ground truth." ></td>
	<td class="line x" title="165:229	Performance was evaluated on this set of 62,588 reviews in terms of recall (percentage of topics in the ground truth that are also identified from the review body) and precision (percentage of extracted topics that are also in the ground truth)." ></td>
	<td class="line x" title="166:229	These measures are computed separately for each review, and then averaged over all reviews." ></td>
	<td class="line x" title="167:229	As shown in Table 2, without clustering, the LING approach gets 4.6% higher recall than the 166 NB baseline." ></td>
	<td class="line x" title="168:229	And the recall from the COMB approach is 3.9% higher than that from the LING approach and 8.5% higher than that from the NB baseline." ></td>
	<td class="line x" title="169:229	With topic clustering, the COMB approach also gets the highest recall, with a 4.9% and 17.5% increase from the LING approach and the NB baseline respectively." ></td>
	<td class="line x" title="170:229	The precision is quite close among the different approaches, around 60%." ></td>
	<td class="line x" title="171:229	Table 2 also shows that the topic clustering approach increases the recall by 4.8% for the NB baseline, 12.8% for the LING approach, and 13.8% for the COMB approach." ></td>
	<td class="line x" title="172:229	Table 2." ></td>
	<td class="line x" title="173:229	Experimental results of topic extraction by the NB baseline, the proposed LING approach and a combined system (COMB)." ></td>
	<td class="line x" title="174:229	No Clustering NB LING COMB Recall 39.6% 44.2% 48.1% Precision 60.2% 60.0% 59.8%  With Clustering NB LING COMB Recall 44.4% 57.0% 61.9% Precision 56.8% 61.1% 60.8% 6.2 Sentiment Scoring To score the degree of sentiment for each extracted phrase, we built a table of sentiment score (<adjective: score>) for adjectives and a table of strength score (<adverb: score>) for adverbs." ></td>
	<td class="line x" title="175:229	The pros/cons often contain short and wellstructured phrases, and have better parsing quality than the long and complex sentences in freestyle texts; pros/cons also have clear sentiment orientations." ></td>
	<td class="line x" title="176:229	Thus, we use pros/cons to score the sentiment of adjectives, which requires strong polarity association." ></td>
	<td class="line x" title="177:229	To obtain reliable ratings, we associate the adjectives in the pros of review entries which have a user rating 4 or 5, and associate the adjectives in the cons of review entries with user ratings 1 or 2 (the scale of user rating is 1 to 5)." ></td>
	<td class="line x" title="178:229	Reviews with rating 3 are on the boundary of sentiment, so we associate both sides with the overall rating." ></td>
	<td class="line x" title="179:229	On the other hand, the frequencies of adverbs in free-style texts are much higher than those in pros/cons, as pros/cons mostly contain adjective-noun patterns." ></td>
	<td class="line x" title="180:229	Thus, we use free-style texts instead of pros/cons to score the strength of adverbs." ></td>
	<td class="line x" title="181:229	Partial results of the sentiment scoring are shown in Tables 3 and 4." ></td>
	<td class="line x" title="182:229	As shown in Table 3, the polarity of sentiment as well as the degree of polarity of an adjective can be distinguished by its score." ></td>
	<td class="line x" title="183:229	The higher the sentiment score is, the more positive the adjective is. Table 3." ></td>
	<td class="line x" title="184:229	Sentiment scoring for selected adjectives." ></td>
	<td class="line x" title="185:229	Adjective Rating Adjective Rating Excellent  5.0 Awesome  4.8 Easy  4.1 Great  4.4 Good  3.9 Limited  3.4 Inattentive  2.75 Overpriced  2.3 Rude  1.69 Horrible  1.3 Table 4 gives the scores of strength for most common adverbs." ></td>
	<td class="line x" title="186:229	The higher the strength score is, the more the adverb scales up/down the degree of sentiment of the adjective it modifies." ></td>
	<td class="line x" title="187:229	While not gets a strong negative score, some adverbs such as a little (-0.65) and a bit (0.83) also get negative scores, indicating slightly less sentiment for the associated adjectives." ></td>
	<td class="line x" title="188:229	Table 4." ></td>
	<td class="line x" title="189:229	Strength scoring for selected adverbs." ></td>
	<td class="line x" title="190:229	Adverb Rating Adverb Rating Super  0.58 Fairly  0.13 Extremely  0.54 Pretty 0.07 Incredibly  0.49 A little  -0.65 Very 0.44 A bit -0.83 Really  0.39 Not -3.10 To evaluate the performance of sentiment scoring, we randomly selected a subset of 1,000 adjective-noun phrases and asked two annotators to independently rate the sentiment of each phrase on a scale of 1 to 5." ></td>
	<td class="line x" title="191:229	We compared the sentiment scoring between our system and the annotations in a measurement of mean distance: g1856g1861g1871g1872g1853g1866g1855g1857 g3404 g2869|g3020| |g1870g3036g3043g3043g3106g3020 g3398g1870g3028g3043|      (5) where g1845  represents the set of phrases, g1868 represents each phrase in the set g1845, g1870g3036g3043 represents the rating on phrase g1868 from our sentiment scoring system, and g1870g3028g3043 represents the annotated rating on phrase g1868." ></td>
	<td class="line x" title="192:229	As shown in Table 5, the obtained mean distance between the scoring from our approach and that from each annotation set is 0.46 and 0.43 respectively, based on the absolute rating scale from 1 to 5." ></td>
	<td class="line x" title="193:229	This shows that the scoring of sentiment from our system is quite close to human annotation." ></td>
	<td class="line x" title="194:229	The kappa agreement between the two annotation sets is 0.68, indicating high consistency between the annotators." ></td>
	<td class="line x" title="195:229	The reliability of these results gives us sufficient confidence to make use of the scores of sentiments for summarization." ></td>
	<td class="line x" title="196:229	To examine the prediction of sentiment polarity, for each annotation set, we pooled the phrases with rating 4/5 into positive, rating 1/2 into negative, and rating 3 into neutral." ></td>
	<td class="line x" title="197:229	Then we rounded up the sentiment scores from our system to integers and pooled the scores into three polar167 ity sets (positive, negative and neutral) in the same way." ></td>
	<td class="line x" title="198:229	As shown in Table 5, the obtained kappa agreement between the result from our system and that from each annotation set is 0.55 and 0.60 respectively." ></td>
	<td class="line x" title="199:229	This shows reasonably high agreement on the polarity of sentiment between our system and human evaluation." ></td>
	<td class="line x" title="200:229	Table 5." ></td>
	<td class="line x" title="201:229	Comparison of sentiment scoring between the proposed approach and two annotation sets." ></td>
	<td class="line x" title="202:229	Annotation 1 Annotation 2 Mean distance 0.46 0.43 Kappa agreement 0.55 0.60 Table 6." ></td>
	<td class="line x" title="203:229	Experimental results of topic extraction based on sentiment polarity matching." ></td>
	<td class="line x" title="204:229	No Clustering NB LING COMB Recall 34.5% 38.9% 42.2% Precision 53.8% 54.0% 53.3%  With Clustering NB LING COMB Recall 37.4% 49.7% 54.1% Precision 48.5% 52.9% 51.4% To evaluate the combination of topic extraction and sentiment identification, we repeated the topic extraction experiments presented in Table 2, but this time requiring as well a correct polarity assignment to obtain a match with the pros/cons ground truth." ></td>
	<td class="line x" title="205:229	As shown in Table 6, the COMB approach gets the highest recall both with and without topic clustering, and the recall from the LING approach is higher than that from the NB baseline in both cases as well, indicating the superiority of the proposed approach." ></td>
	<td class="line x" title="206:229	The precision is stable among the different approaches, consistent with the case without the consideration of sentiment polarity." ></td>
	<td class="line x" title="207:229	7 Discussion It is surprising that the parse-and-paraphrase method performs so well, despite the fact that it utilizes less than 80% of the data (parsable set)." ></td>
	<td class="line x" title="208:229	In this section, we will discuss two experiments that were done to tease apart the contributions of different variables." ></td>
	<td class="line x" title="209:229	In both experiments, we compared the change in relative improvement in recall between NB and LING, relative to the values in Table 6, in the with-clustering condition." ></td>
	<td class="line x" title="210:229	In the table, LING obtains a score of 49.7% for recall, which is a 33% relative increase from the score for NB (37.4%)." ></td>
	<td class="line x" title="211:229	Three distinct factors could play a role in the improvement: the widowadjective topic hallucinations, the topic mapping for clustering, and the extracted phrases themselves." ></td>
	<td class="line x" title="212:229	An experiment involving omitting topic hallucinations from widow adjectives determined that these account for 12% of the relative increase." ></td>
	<td class="line x" title="213:229	To evaluate the contribution of clustering, we replaced the mapping tables used by both systems with the edited one used by the ground truth computation." ></td>
	<td class="line x" title="214:229	Thus, both systems made use of the same mapping table, removing this variable from consideration." ></td>
	<td class="line x" title="215:229	This improved the performance of both systems (NB and LING), but resulted in a decrease of LINGs relative improvement by 17%." ></td>
	<td class="line x" title="216:229	This implies that LINGs mapping table is superior." ></td>
	<td class="line x" title="217:229	Since both systems use the same sentiment scores for adjectives and adverbs, the remainder of the difference (71%) must be due simply to higher quality extracted phrases." ></td>
	<td class="line x" title="218:229	We suspected that over-generated phrases (the 40% of phrases that find no mappings in the pros/cons) might not really be a problem." ></td>
	<td class="line x" title="219:229	To test this hypothesis, we selected 100 reviews for their high density of extracted phrases, and manually evaluated all the over-generated phrases." ></td>
	<td class="line x" title="220:229	We found that over 80% were well formed, correct, and informative." ></td>
	<td class="line x" title="221:229	Therefore, a lower precision here does not necessarily mean poor performance, but instead shows that the pros/cons provided by users are often incomplete." ></td>
	<td class="line x" title="222:229	By extracting summaries from review texts we can recover additional valuable information." ></td>
	<td class="line x" title="223:229	8 Conclusions & Future Work This paper presents a parse-and-paraphrase approach to assessing the degree of sentiment for product reviews." ></td>
	<td class="line x" title="224:229	A general purpose context free grammar is employed to parse review sentences, and semantic understanding methods are developed to extract representative negation-adverbadjective-noun phrases based on well-defined semantic rules." ></td>
	<td class="line x" title="225:229	A language modeling-based method is proposed to cluster topics into respective categories." ></td>
	<td class="line x" title="226:229	We also introduced in this paper a cumulative linear offset model for supporting the assessment of the strength of sentiment in adjectives and quantifiers/qualifiers (including negations) on a numerical scale." ></td>
	<td class="line x" title="227:229	We demonstrated that the parse-and-paraphrase method can perform substantially better than a neighbor baseline on topic extraction from reviews even with less data." ></td>
	<td class="line x" title="228:229	The future work focuses in two directions: (1) building a relational database from the summaries and ratings and using it to enhance users experiences in a multimodal spoken dialogue system; and (2) applying our techniques to other domains to demonstrate generality." ></td>
	<td class="line x" title="229:229	168" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1019
Sentiment Analysis of Conditional Sentences
Narayanan, Ramanathan;Liu, Bing;Choudhary, Alok;"></td>
	<td class="line x" title="1:344	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 180189, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:344	c 2009 ACL and AFNLP Sentiment Analysis of Conditional Sentences   Ramanathan Narayanan Dept. of EECS Northwestern University ramanathan.an@gmail.com Bing Liu *  Dept. of Computer Science Univ. of Illinois at Chicago liub@cs.uic.edu Alok Choudhary Dept. of EECS Northwestern University alokchoudhary01@gmail.com     Abstract This paper studies sentiment analysis of conditional sentences." ></td>
	<td class="line x" title="3:344	The aim is to determine whether opinions expressed on different topics in a conditional sentence are positive, negative or neutral." ></td>
	<td class="line x" title="4:344	Conditional sentences are one of the commonly used language constructs in text." ></td>
	<td class="line x" title="5:344	In a typical document, there are around 8% of such sentences." ></td>
	<td class="line x" title="6:344	Due to the condition clause, sentiments expressed in a conditional sentence can be hard to determine." ></td>
	<td class="line x" title="7:344	For example, in the sentence, if your Nokia phone is not good, buy this great Samsung phone, the author is positive about Samsung phone but does not express an opinion on Nokia phone (although the owner of the Nokia phone may be negative about it)." ></td>
	<td class="line x" title="8:344	However, if the sentence does not have if, the first clause is clearly negative." ></td>
	<td class="line x" title="9:344	Although if commonly signifies a conditional sentence, there are many other words and constructs that can express conditions." ></td>
	<td class="line x" title="10:344	This paper first presents a linguistic analysis of such sentences, and then builds some supervised learning models to determine if sentiments expressed on different topics in a conditional sentence are positive, negative or neutral." ></td>
	<td class="line x" title="11:344	Experimental results on conditional sentences from 5 diverse domains are given to demonstrate the effectiveness of the proposed approach." ></td>
	<td class="line x" title="12:344	1 Introduction Sentiment analysis (also called opinion mining) has been an active research area in recent years." ></td>
	<td class="line oc" title="13:344	There are many research directions, e.g., sentiment classification (classifying an opinion document as positive or negative) (e.g., Pang, Lee and Vaithyanathan, 2002; Turney, 2002), subjectivity classification (determining whether a sentence is subjective or objective, and its associated opinion) (Wiebe and Wilson, 2002; Yu and Hatzivassiloglou, 2003; Wilson et al, 2004; Kim and Hovy, 2004; Riloff and Wiebe, 2005), feature/topic-based sentiment analysis (assigning positive or negative sentiments to topics or product features) (Hu and Liu 2004; Popescu and Etzioni, 2005; Carenini et al., 2005; Ku et al., 2006; Kobayashi, Inui and Matsumoto, 2007; Titov and McDonald." ></td>
	<td class="line x" title="14:344	2008)." ></td>
	<td class="line x" title="15:344	Formal definitions of different aspects of the sentiment analysis problem and discussions of major research directions and algorithms can be found in (Liu, 2006; Liu, 2009)." ></td>
	<td class="line x" title="16:344	A comprehensive survey of the field can be found in (Pang and Lee, 2008)." ></td>
	<td class="line x" title="17:344	Our work is in the area of topic/feature-based sentiment analysis or opinion mining (Hu and Liu, 2004)." ></td>
	<td class="line x" title="18:344	The existing research focuses on solving the general problem." ></td>
	<td class="line x" title="19:344	However, we argue that it is unlikely to have a one-technique-fit-all solution because different types of sentences express sentiments/opinions in different ways." ></td>
	<td class="line x" title="20:344	A divide-and-conquer approach is needed, e.g., focused studies on different types of sentences." ></td>
	<td class="line x" title="21:344	This paper focuses on one type of sentences, i.e., conditional sentences, which have some unique characteristics that make it hard to determine the orientation of sentiments on topics/features in such sentences." ></td>
	<td class="line x" title="22:344	By sentiment orientation, we mean positive, negative or neutral opinions." ></td>
	<td class="line x" title="23:344	By topic, we mean the target on which an opinion has been expressed." ></td>
	<td class="line x" title="24:344	In the product domain, a topic is usually a product feature (i.e., a component or attribute)." ></td>
	<td class="line x" title="25:344	For example, in the sentence, I do not like the sound quality, but love the design of this MP3 player, the product features (topics) are sound quality and design of the MP3 player as opinions have been expressed on them." ></td>
	<td class="line x" title="26:344	The sentiment is positive on design but negative on sound quality." ></td>
	<td class="line x" title="27:344	Conditional sentences are sentences that describe implications or hypothetical situations and their consequences." ></td>
	<td class="line x" title="28:344	In the English language, a variety of conditional connectives can be used to form these sentences." ></td>
	<td class="line x" title="29:344	A conditional sentence contains two clauses: the condition clause and *  This work was done when Bing Liu was on sabbatical leave at Northwestern University." ></td>
	<td class="line x" title="30:344	180 the consequent clause, that are dependent on each other." ></td>
	<td class="line x" title="31:344	Their relationship has significant implications on whether the sentence describes an opinion." ></td>
	<td class="line x" title="32:344	One simple observation is that sentiment words (also known as opinion words) (e.g., great, beautiful, bad) alone cannot distinguish an opinion sentence from a non-opinion one." ></td>
	<td class="line x" title="33:344	A conditional sentence may contain many sentiment words or phrases, but express no opinion." ></td>
	<td class="line x" title="34:344	Example 1: If someone makes a beautiful and reliable car, I will buy it expresses no sentiment towards any particular car, although beautiful and reliable are positive sentiment words." ></td>
	<td class="line x" title="35:344	This, however, does not mean that a conditional sentence cannot express opinions/sentiments." ></td>
	<td class="line x" title="36:344	Example 2: If your Nokia phone is not good, buy this great Samsung phone is positive about the Samsung phone but does not express an opinion on the Nokia phone (although the owner of the Nokia phone may be negative about it)." ></td>
	<td class="line x" title="37:344	Clearly, if the sentence does not have if, the first clause is negative." ></td>
	<td class="line x" title="38:344	Hence, a method for determining sentiments in normal sentences will not work for conditional sentences." ></td>
	<td class="line x" title="39:344	The examples below further illustrate the point." ></td>
	<td class="line x" title="40:344	In many cases, both the condition and consequent together determine the opinion." ></td>
	<td class="line x" title="41:344	Example 3: If you are looking for a phone with good voice quality, dont buy this Nokia phone is negative about the voice quality of the Nokia phone, although there is a positive sentiment word good in the conditional clause modifying voice quality." ></td>
	<td class="line x" title="42:344	However, in the following example, the opinion is just the opposite." ></td>
	<td class="line x" title="43:344	Example 4: If you want a phone with good voice quality, buy this Nokia phone is positive about the voice quality of the Nokia phone." ></td>
	<td class="line x" title="44:344	As we can see, sentiment analysis of conditional sentences is a challenging problem." ></td>
	<td class="line x" title="45:344	One may ask whether there is a large percentage of conditional sentences to warrant a focused study." ></td>
	<td class="line x" title="46:344	Indeed, there is a fairly large proportion of such sentences in evaluative text." ></td>
	<td class="line x" title="47:344	They can have a major impact on the sentiment analysis accuracy." ></td>
	<td class="line x" title="48:344	Table 1 shows the percentage of conditional sentences (sentences containing the words if, unless, assuming, etc) and also the total number of sentences from which we computed the percentage in several user-forums." ></td>
	<td class="line x" title="49:344	The figures definitely suggest that there is considerable benefit to be gained by developing techniques that can analyze conditional sentences." ></td>
	<td class="line x" title="50:344	To the best of our knowledge, there is no focused study on conditional sentences." ></td>
	<td class="line x" title="51:344	This paper makes such an attempt." ></td>
	<td class="line x" title="52:344	Specifically, we determine whether a conditional sentence (which is also called a conditional in the linguistic literature) expresses positive, negative or neutral opinions on some topics/features." ></td>
	<td class="line x" title="53:344	Since our focus is on studying how conditions and consequents affect sentiments, we assume that topics are given, which are product attributes since our data sets are user comments on different products." ></td>
	<td class="line x" title="54:344	Our study is conducted from two perspectives." ></td>
	<td class="line x" title="55:344	We start with the linguistic angle to gain a good understanding of existing work on different types of conditionals." ></td>
	<td class="line x" title="56:344	As conditionals can be expressed with other words or phrases than if, we will study how they behave compared to if." ></td>
	<td class="line x" title="57:344	We will also show that the distribution of these conditionals based on our data sets." ></td>
	<td class="line x" title="58:344	With the linguistic knowledge, we perform a computational study using machine learning." ></td>
	<td class="line x" title="59:344	A set of features for learning is designed to capture the essential determining information." ></td>
	<td class="line x" title="60:344	Note that the features here are data attributes used in learning rather than product attributes or features." ></td>
	<td class="line x" title="61:344	Three classification strategies are designed to study how to best perform the classification task due to the complex situation of two clauses and their interactions in conditional sentences." ></td>
	<td class="line x" title="62:344	These three classification strategies are clause-based, consequent-based and whole-sentence-based." ></td>
	<td class="line x" title="63:344	Clause-based classification classifies each clause separately and then combines their results." ></td>
	<td class="line x" title="64:344	Consequent-based classification only uses consequents for classification as it is observed that in conditional sentences, it is often the consequents that decide the opinion." ></td>
	<td class="line x" title="65:344	Whole-sentence-based classification treats the entire sentence as a whole in classification." ></td>
	<td class="line x" title="66:344	Experimental results on conditional sentences from diverse domains demonstrate the effectiveness of these classification models." ></td>
	<td class="line x" title="67:344	The results indicate that the wholesentence-based classifier performs the best." ></td>
	<td class="line x" title="68:344	Since this paper only studies conditional sentences, a natural question is whether the proposed technique can be easily integrated into an overall sentiment analysis or opinion mining system." ></td>
	<td class="line x" title="69:344	The answer is yes because a large proportion of conditional sentences can be detected using conditional connectives." ></td>
	<td class="line x" title="70:344	Keyword search is Table 1: Percent of conditional sentences Source % of cond." ></td>
	<td class="line x" title="71:344	(total #." ></td>
	<td class="line x" title="72:344	of sent.)" ></td>
	<td class="line x" title="73:344	Cellphone 8.6 (47711) Automobile 5.0 (8113) LCD TV 9.92 (258078) Audio Systems 8.1 (5702) Medicine 8.29 (160259) 181 thus sufficient to identify such sentences for special handling using the proposed approach." ></td>
	<td class="line x" title="74:344	There are, however, some subtle conditionals which do not use normal conditional connectives and will need an additional module to identify them, but such sentences are very rare as Table 2 indicates." ></td>
	<td class="line x" title="75:344	2 The Problem Statement The paper follows the feature-based sentiment analysis model in (Hu and Liu 2004; Popescu and Etzioni, 2005)." ></td>
	<td class="line x" title="76:344	We are particularly interested in sentiments on products and services, which are called objects or entities." ></td>
	<td class="line x" title="77:344	Each object is described by its parts and attributes, which are collectively called features in (Hu and Liu, 2004; Liu, 2006)." ></td>
	<td class="line x" title="78:344	For example, in the sentence, If this camera has great picture quality, I will buy it, picture quality is a feature of the camera." ></td>
	<td class="line x" title="79:344	For formal definitions of objects and features, please refer to (Liu, 2006; Liu, 2009)." ></td>
	<td class="line x" title="80:344	In this paper, we use the term topic to mean feature as the feature here can confuse with the feature used in machine learning." ></td>
	<td class="line x" title="81:344	The term topic has also been used by some researchers (e.g., Kim and Hovy, 2004; Stoyanov and Cardie, 2008)." ></td>
	<td class="line x" title="82:344	Our objective is to predict the sentiment orientation (positive, negative or neutral) on each topic that has been commented on in a sentence." ></td>
	<td class="line x" title="83:344	The problem of automatically identifying features or topics being spoken about in a sentence has been studied in (Hu and Liu, 2004; Popescu and Etzioni, 2005; Stoyanov and Cardie, 2008)." ></td>
	<td class="line x" title="84:344	In this work, we do not attempt to identify such topics automatically." ></td>
	<td class="line x" title="85:344	Instead, we assume that they are given because our objective is to study how the interaction of the condition and consequent clauses affects sentiments." ></td>
	<td class="line x" title="86:344	For this purpose, we manually identify all the topics." ></td>
	<td class="line x" title="87:344	3 Conditional Sentences This section presents the linguistic perspective of conditional sentences." ></td>
	<td class="line x" title="88:344	3.1 Conditional Connectives A large majority of conditional sentences are introduced by the subordinating conjunction If." ></td>
	<td class="line x" title="89:344	However, there are also many other conditional connectives, e.g., even if, unless, in case, assuming/supposing, as long as, etc. Table 2 shows the distribution of conditional sentences with various connectives in our data." ></td>
	<td class="line x" title="90:344	Detailed linguistic discussions of them are beyond the scope of this paper." ></td>
	<td class="line x" title="91:344	Interested readers, please refer to (Declerck and Reed, 2001)." ></td>
	<td class="line x" title="92:344	Below, we briefly discuss some important ones and their interpretations." ></td>
	<td class="line x" title="93:344	If: This is the most commonly used conditional connective." ></td>
	<td class="line x" title="94:344	In addition to its own usage, it can also be used to replace other conditional connectives, except some semantically richer connectives (Declerck and Reed, 2001)." ></td>
	<td class="line x" title="95:344	Most (but not all) conditional sentences can be logically expressed in the form If P then Q, where P is the condition clause and Q is the consequent clause." ></td>
	<td class="line x" title="96:344	For practical purposes, we can automatically segment the condition and consequent clauses using simple rules generated by observing grammatical and linguistic patterns." ></td>
	<td class="line x" title="97:344	Unless: Most conditional sentences containing unless can be replaced with equivalent sentences with an if and a not." ></td>
	<td class="line x" title="98:344	For example, the sentence Unless you need clarity, buy the cheaper model can be expressed with If you dont need clarity, buy the cheaper model." ></td>
	<td class="line x" title="99:344	Even if: Linguistic theories claim that even if is a special case of a conditional which may not always imply an if-then relationship (Gauker 2005)." ></td>
	<td class="line x" title="100:344	However, in our datasets, we have observed that the usage of even if almost always translates into a conditional." ></td>
	<td class="line x" title="101:344	Replacing even if by if will yield a sentence that is semantically similar enough for the purpose of sentiment analysis." ></td>
	<td class="line x" title="102:344	Only if, provided/providing that, on condition that: Conditionals involving these phrases typically express a necessary condition, e.g., I will buy this camera only if they can reduce the price." ></td>
	<td class="line x" title="103:344	In such sentences, only usually does not affect whether the sentence is opinionated or not." ></td>
	<td class="line x" title="104:344	In case: Conditional sentences containing in case usually describe a precaution (I will close the window in case it rains), prevention (I wore sunglasses in case I was recognized), or a relevance conditional (In case you need a car, you can rent one)." ></td>
	<td class="line x" title="105:344	Identifying the conditional and consequent clauses is not straightforward in many cases." ></td>
	<td class="line x" title="106:344	Further, in these instances, replacing in case with if may not convey the intended meaning of the conditional." ></td>
	<td class="line x" title="107:344	We have ignored these cases in Table 2: Percentage of sentences with some main conditional connectives Conditional Connective % of sentences If 6.42 Unless 0.32 Even if 0.17 Until 0.10 As (so) long as 0.09 Assuming/supposing 0.04 In case 0.04 Only if 0.03 182 our analysis as we believe that they need a separate study, and also such sentences are rare." ></td>
	<td class="line x" title="108:344	As (so) long as: Sentences with these connectives behave similarly to if and can usually be replaced with if." ></td>
	<td class="line x" title="109:344	Assuming/Supposing: These are a category of conditionals that behave quite differently." ></td>
	<td class="line x" title="110:344	The participles supposing and assuming create conditional sentences where the conditional clause and the consequent clause can be syntactically independent." ></td>
	<td class="line x" title="111:344	It is quite difficult to distinguish those conditional sentences which contain an explicit consequent clause and fit within our analysis framework." ></td>
	<td class="line x" title="112:344	In our data, most of such sentences have no consequent, thus representing assumptions rather than opinions." ></td>
	<td class="line x" title="113:344	We omit these sentences in our study (they are also rare)." ></td>
	<td class="line x" title="114:344	3.2 Types of Conditionals There are extensive studies of conditional sentences (also known as conditionals) in linguistics." ></td>
	<td class="line x" title="115:344	Various theories have led to a number of classification systems." ></td>
	<td class="line x" title="116:344	Popular types of conditionals include actualization conditionals, inferential conditionals, implicative conditionals, etc (Declerck and Reed, 2001)." ></td>
	<td class="line x" title="117:344	However, these classifications are mainly based on semantic meanings which are difficult to recognize by a computer program." ></td>
	<td class="line x" title="118:344	To build classification models, we instead exploit canonical tense patterns of conditionals, which are often used in pedagogic grammar books." ></td>
	<td class="line x" title="119:344	They are defined based on tense and are associated with general meanings." ></td>
	<td class="line x" title="120:344	However, as described in (Declerck and Reed, 2001), their meanings are much more complex and numerous than their associated general meanings." ></td>
	<td class="line x" title="121:344	However, the advantage of this classification is that different types can be detected easily because they depend on tense which can be produced by a part-of-speech tagger." ></td>
	<td class="line x" title="122:344	As we will see in Section 5, canonical tense patterns help sentiment classification significantly." ></td>
	<td class="line x" title="123:344	Below, we introduce the four canonical tense patterns." ></td>
	<td class="line x" title="124:344	Zero Conditional:  This conditional form is used to describe universal statements like facts, rules and certainties." ></td>
	<td class="line x" title="125:344	In a zero conditional, both the condition and consequent clauses are in the simple present tense." ></td>
	<td class="line x" title="126:344	An example of such sentences is: If you heat water, it boils." ></td>
	<td class="line x" title="127:344	First Conditional: Conditional sentences of this type are also called potential or indicative conditionals." ></td>
	<td class="line x" title="128:344	They are used to express a hypothetical situation that is probably true, but the truth of which is unverified." ></td>
	<td class="line x" title="129:344	In the first conditional, the condition is in the simple present tense, and the consequent can be either in past tense or present tense, usually with a modal auxiliary verb preceding the main verb, e.g., If the acceleration is good, I will buy it." ></td>
	<td class="line x" title="130:344	Second Conditional: This is usually used to describe less probable situations, for stating preferences and imaginary events." ></td>
	<td class="line x" title="131:344	The condition clause of a second conditional sentence is in the past subjunctive (past tense), and the consequent clause contains a conditional verb modifier (like would, should, might), in addition to the main verb, e.g., If the cell phone was robust, I would consider buying it." ></td>
	<td class="line x" title="132:344	Third conditional: This is usually used to describe contrary-to-fact (impossible) past events." ></td>
	<td class="line x" title="133:344	The past perfect tense is used in the condition clause, and the consequent clause is in the present perfect tense, e.g., If I had bought the a767, I would have hated it." ></td>
	<td class="line x" title="134:344	Based on the above definitions, we have developed approximate part-of-speech (POS) tags 1  for the condition and the consequent of each pattern (Table 3), which do not cover all sentences, but overall they cover a majority of the sentences." ></td>
	<td class="line x" title="135:344	For those not covered cases, the problem is mainly due to incomplete sentences and wrong grammars, which are typical for informal writings in forum postings and blogs." ></td>
	<td class="line x" title="136:344	For example, the sentence, Great car if you need powerful acceleration, does not fall into any category, but it actually means It is a great car if you need powerful acceleration, which is a zero conditional." ></td>
	<td class="line x" title="137:344	To handle such sentences, we designed a set of rules to assign them some default types:  If condition contains VB/VBP/VBZ  0 conditional  If consequent contains VB/VBP/VBS  0 conditional  If condition contains VBG  1 st  conditional  If condition contains VBD  2 nd  conditional  If conditional contains VBN  3 rd  conditional." ></td>
	<td class="line x" title="138:344	1  The list of Part-Of-Speech (POS) tags can be found at: http://www.ling.upenn.edu/courses/Fall_2003/ling001/ penn_treebank_pos.html Table 3: Tenses for identifying conditional types Type Linguistic Rule Condition POS tags Consequent POS tags 0 If + simple present  simple present VB/VBP/VBZ VB/VBP/ VBZ 1 If + simple present  will + bare infinitive VB/VBP/VBZ /VBG MD + VB 2 If + past tense  would + infinitive VBD MD + VB 3 If + past perfect  present perfect VBD+VBN MD + VBD 183 By using these rules, we can increase the sentence coverage from 73% to 95%." ></td>
	<td class="line x" title="139:344	4 Sentiment Analysis of Conditionals We now describe our computational study." ></td>
	<td class="line x" title="140:344	We take a machine learning approach to predict sentiment orientations." ></td>
	<td class="line x" title="141:344	Below, we first describe features used and then classification strategies." ></td>
	<td class="line x" title="142:344	4.1 Feature construction I.  Sentiment words/phrases and their locations: Sentiment words are words used to express positive or negative opinions, which are instrumental for sentiment classification for obvious reasons." ></td>
	<td class="line x" title="143:344	We obtained a list of over 6500 sentiment words gathered from various sources." ></td>
	<td class="line x" title="144:344	The bulk of it is from http://www.cs.pitt.edu/mpqa." ></td>
	<td class="line x" title="145:344	We also added some of our own." ></td>
	<td class="line x" title="146:344	Our list is mainly from the work in (Hu and Liu, 2004; Ding, Liu and Yu, 2008)." ></td>
	<td class="line x" title="147:344	In addition to words, there are phrases that describe opinions." ></td>
	<td class="line x" title="148:344	We have identified a set of such phrases." ></td>
	<td class="line x" title="149:344	Although obtaining these phrases was time-consuming, it was only a one-time effort." ></td>
	<td class="line x" title="150:344	We will make this list available as a community resource." ></td>
	<td class="line x" title="151:344	It is possible that there is a better automated method for finding such phrases, such as the methods in (Kanayama and Nasukawa, 2006; Breck, Choi and Cardie, 2007)." ></td>
	<td class="line x" title="152:344	However, automatically generating sentiment phrases has not been the focus of this work as our objective is to study how the two clauses interact to determine opinions given the sentiment words and phrases are known." ></td>
	<td class="line x" title="153:344	Our list of phrases is by no means complete and we will continue to expand it in the future." ></td>
	<td class="line x" title="154:344	For each sentence, we also identify whether it contains sentiment words/phrases in its condition or consequent clause." ></td>
	<td class="line x" title="155:344	It was observed that the presence of a sentiment word/phrase in the consequent clause has more effect on the sentiment of a sentence." ></td>
	<td class="line x" title="156:344	II." ></td>
	<td class="line x" title="157:344	POS tags of sentiment words: Sentiment words may be used in several contexts, not all of which may correspond to an opinion." ></td>
	<td class="line x" title="158:344	For example, I trust Motorola and He has a trust fund both contain the word trust." ></td>
	<td class="line x" title="159:344	But only the former contains an opinion." ></td>
	<td class="line x" title="160:344	In such cases, the POS tags can provide useful information." ></td>
	<td class="line x" title="161:344	III." ></td>
	<td class="line x" title="162:344	Words indicating no opinion: Similar to how sentiment words are related to opinions, there are also a number of words which imply the opposite." ></td>
	<td class="line x" title="163:344	Words like wondering, thinking, debating are used when the user is posing a question or expressing doubts." ></td>
	<td class="line x" title="164:344	Thus such phrases usually do not contribute an opinion, especially if they are in the vicinity of the if connective." ></td>
	<td class="line x" title="165:344	We search a window of 3 words on either side of if to determine if there is any such word." ></td>
	<td class="line x" title="166:344	We have compiled a list of these words as well and use it in our experiments." ></td>
	<td class="line x" title="167:344	IV." ></td>
	<td class="line x" title="168:344	Tense patterns: These are the canonical tense patterns in Section 3.2." ></td>
	<td class="line x" title="169:344	They are used to generate a set of features." ></td>
	<td class="line x" title="170:344	We identify the first verb in both the condition and consequent clauses by searching for the relevant POS tags in Table 3." ></td>
	<td class="line x" title="171:344	We also search for the words preceding the main verb to find modal auxiliary verbs, which are also used as features." ></td>
	<td class="line x" title="172:344	V. Special characters: The presence or absence of ? and !." ></td>
	<td class="line x" title="173:344	VI." ></td>
	<td class="line x" title="174:344	Conditional connectives: The conditional connective used in the sentence (if, even if, unless, only if, etc) is also taken as a feature." ></td>
	<td class="line x" title="175:344	VII." ></td>
	<td class="line x" title="176:344	Length of condition and consequent clauses: Using simple linguistic and punctuation rules, we automatically segment a sentence into condition and consequent clauses." ></td>
	<td class="line x" title="177:344	The numbers of words in the condition and consequent clauses are then used as features." ></td>
	<td class="line x" title="178:344	We observed that when the condition clause is short, it usually has no impact on whether the sentence expresses an opinion." ></td>
	<td class="line x" title="179:344	VIII." ></td>
	<td class="line x" title="180:344	Negation words: The use of negation words like not, dont, never, etc, often alter the sentiment orientation of a sentence." ></td>
	<td class="line x" title="181:344	For example, the addition of not before a sentiment word can change the orientation of a sentence from positive to negative." ></td>
	<td class="line x" title="182:344	We consider a window of 3-6 words before an opinion word, and search for these kinds of words." ></td>
	<td class="line x" title="183:344	The following two features are singled out for easy reference later." ></td>
	<td class="line x" title="184:344	They are only used in one classification strategy." ></td>
	<td class="line x" title="185:344	The first feature is an indicator, and the second feature has a parameter (which will be evaluated separately)." ></td>
	<td class="line x" title="186:344	(1)." ></td>
	<td class="line x" title="187:344	Topic location: This feature indicates whether the topic is in the conditional clause or the consequent clause." ></td>
	<td class="line x" title="188:344	(2)." ></td>
	<td class="line x" title="189:344	Opinion weight: This feature considers only sentiment words in the vicinity of the topic, since they are more likely to influence the opinion on the topic." ></td>
	<td class="line x" title="190:344	A window size is used to control what we mean by vicinity." ></td>
	<td class="line x" title="191:344	The following formula is used to assign a weight to each sentiment word, which is inversely proportional to the distance (D op ) of the sentiment word to the topic mention." ></td>
	<td class="line x" title="192:344	Sentiment 184 value is +1 for a positive word and -1 for a negative word." ></td>
	<td class="line x" title="193:344	Sentwords are the set of known sentiment words and phrases." ></td>
	<td class="line x" title="194:344	}{, 1 sentwordsop D weight op op   =   4.2 Classification Strategies Since we are interested in topic-based sentiment analysis, how to perform classification becomes an interesting issue." ></td>
	<td class="line x" title="195:344	Due to the two clauses, it may not be sufficient to classify the whole sentence as positive or negative as in the same sentence, some topics may be positive and some may be negative." ></td>
	<td class="line x" title="196:344	We propose three strategies." ></td>
	<td class="line x" title="197:344	Clause-based classification: Since there are two clauses in a conditional sentence, in this case we build two classifiers, one for the condition and one for the consequent." ></td>
	<td class="line x" title="198:344	Condition classifier: This method classifies the condition clause as expressing positive, negative or neutral opinion." ></td>
	<td class="line x" title="199:344	Training data: Each training sentence is represented as a feature vector." ></td>
	<td class="line x" title="200:344	Its class is positive, negative or neutral depending on whether the conditional clause is positive, negative or neutral while considering both clauses." ></td>
	<td class="line x" title="201:344	Testing: For each test sentence, the resulting classifier predicts the opinion of the condition clause." ></td>
	<td class="line x" title="202:344	Topic class prediction: To predict the opinion on a topic, if the topic is in the condition clause, it takes the predicted class of the clause." ></td>
	<td class="line x" title="203:344	Consequent classifier: This classifier classifies the consequent clause as expressing positive, negative or neutral opinion." ></td>
	<td class="line x" title="204:344	Training data: Each training sentence is represented as a feature vector." ></td>
	<td class="line x" title="205:344	Its class is positive, negative or neutral depending on whether the consequent clause is positive, negative or neutral while considering both clauses." ></td>
	<td class="line x" title="206:344	Testing: For each test sentence, the resulting classifier predicts the opinion of the consequent clause." ></td>
	<td class="line x" title="207:344	Topic class prediction: To predict the opinion on a topic, if the topic is in the consequent clause, it takes the predicted class of the clause." ></td>
	<td class="line x" title="208:344	The combination of these two classifiers is called the clause-based classifier." ></td>
	<td class="line x" title="209:344	It works as follows: If a topic is in the conditional clause, the condition classifier is used, and if a topic is in the consequent clause, the consequent classifier is used." ></td>
	<td class="line x" title="210:344	Consequent-based classification: It is observed that in most cases, the condition clause contains no opinion whereas the consequent clause reflects the sentiment of the entire sentence." ></td>
	<td class="line x" title="211:344	Thus, this method uses (in a different way) only the above consequent classifier." ></td>
	<td class="line x" title="212:344	If it classifies the consequent of a testing conditional sentence as positive, all the topics in the whole sentence are assigned the positive orientation, and likewise for negative and neutral." ></td>
	<td class="line x" title="213:344	Whole-sentence-based classification: In this case, a single classifier is built to predict the opinion on each topic in a sentence." ></td>
	<td class="line x" title="214:344	Training data: In addition to the normal features, the two features (1) and (2) in Section 4.1 are used for this classifier." ></td>
	<td class="line x" title="215:344	If a sentence contains multiple topics, multiple training instances of the same sentence are created in the training data." ></td>
	<td class="line x" title="216:344	Each instance represents one specific topic." ></td>
	<td class="line x" title="217:344	The class of the instance depends on whether the opinion on the topic is positive, negative or neutral." ></td>
	<td class="line x" title="218:344	Testing: For each topic in each test sentence, the resulting classifier predicts its opinion." ></td>
	<td class="line x" title="219:344	Topic class prediction: This is not needed as the prediction has been done in testing." ></td>
	<td class="line x" title="220:344	5 Results and Discussions 5.1 Data sets Our data consists of conditional sentences from 5 different user forums: Cellphone, Automobile, LCD TV, Audio systems and Medicine." ></td>
	<td class="line x" title="221:344	We obtained user postings from these forums and extracted the conditional sentences." ></td>
	<td class="line x" title="222:344	We then manually annotated 1378 sentences from this corpus." ></td>
	<td class="line x" title="223:344	We also annotated the conditional and consequent clauses and identified the topics (or product features) being commented upon, and their sentiment orientations." ></td>
	<td class="line x" title="224:344	In our annotation, we observed that sentences with no sentiment words or phrases almost never express opinions, i.e., only around 3% of them express opinions." ></td>
	<td class="line x" title="225:344	There are around 26% sentences containing no sentiment words or phrases in our data." ></td>
	<td class="line x" title="226:344	To make the problem challenging, we restrict our attention to only those sentences that contain at least one sentiment word or phrase." ></td>
	<td class="line x" title="227:344	We have annotated topics from around 900 such sentences." ></td>
	<td class="line x" title="228:344	Table 4 shows the class distributions of this data." ></td>
	<td class="line x" title="229:344	At the clause level (topics are not considered), we observe that conditional clauses contain few opinions." ></td>
	<td class="line x" title="230:344	At the topic-level, 43.5% of the topics have positive opinions, 26.4% of the topics have negative opinions, and the rest have no opinions." ></td>
	<td class="line x" title="231:344	185 Table 4: Distribution of classes For the annotation of data, we assume that topics are known." ></td>
	<td class="line x" title="232:344	One student annotated the topics first." ></td>
	<td class="line x" title="233:344	Then two students annotated the sentiments on the topics." ></td>
	<td class="line x" title="234:344	If a student found that a topic annotation is wrong, he will let us know." ></td>
	<td class="line x" title="235:344	Some mistakes and missing topics were found but there were mainly due to oversights rather than disagreements." ></td>
	<td class="line x" title="236:344	The agreement on sentiment annotations were computed using the Kappa score." ></td>
	<td class="line x" title="237:344	We achieved the Kappa score of 0.63, which indicates strong agreements." ></td>
	<td class="line x" title="238:344	The conflicting cases were then solved through discussion to reach consensus." ></td>
	<td class="line x" title="239:344	We did not find anything that the annotators absolutely disagree with each other." ></td>
	<td class="line x" title="240:344	5.2 Experimental results We now present the results for different combinations of features and classification strategies." ></td>
	<td class="line x" title="241:344	For model building, we used Support Vector Machines (SVM), and the LIBSVM implementation (Chang and Lin, 2001) with a Gaussian kernel, which produces the best results." ></td>
	<td class="line x" title="242:344	All the results are obtained via 10-fold cross validation." ></td>
	<td class="line x" title="243:344	Two-class classification: We first discuss the results for a simpler version of the problem that involves only sentences with positive or negative orientations on some topics (at least one of the clauses must have a positive/negative opinion on a topic)." ></td>
	<td class="line x" title="244:344	Neutral sentences are not used (~28% of the total)." ></td>
	<td class="line x" title="245:344	The results of all three classifiers are given in Table 5." ></td>
	<td class="line x" title="246:344	The feature sets have been described in Section 4.1." ></td>
	<td class="line x" title="247:344	For all the experiments below, features (1) and (2) are only used by the whole-sentence-based classifier, but not used by the other two classifiers for obvious reasons." ></td>
	<td class="line x" title="248:344	{I+II}: This setting uses sentiment words and phrases, their positions and POS tags as features (we used Brills POS tagger)." ></td>
	<td class="line x" title="249:344	This can be seen as the baseline." ></td>
	<td class="line x" title="250:344	We observe that both the consequent-based and whole-sentence-based classifiers perform dramatically better than the clause-based classifier." ></td>
	<td class="line x" title="251:344	The consequent-based classifier and the whole-sentence-based classifier perform similarly (with the latter being slightly better)." ></td>
	<td class="line x" title="252:344	The precision, recall, and F-score are computed as the average of the two classes." ></td>
	<td class="line x" title="253:344	{I+II+III}: In this setting, the list of special non-sentiment related words is added to the feature set." ></td>
	<td class="line x" title="254:344	All three classifiers improve slightly." ></td>
	<td class="line x" title="255:344	{I+II+III+IV}: This setting includes all the canonical tense based features." ></td>
	<td class="line x" title="256:344	We see marked improvements for the consequent-based and wholesentence-based classifiers both in term of accuracy and F-score, which are statistically significant compared to those of {I+II+III} at the 95% confidence level based on paired t-test." ></td>
	<td class="line x" title="257:344	All: When all the features are used, the results of all the classifiers improve further." ></td>
	<td class="line x" title="258:344	Two main observations worth mentioning: 1." ></td>
	<td class="line x" title="259:344	Both the consequent-based and wholesentence-based classifiers outperform the clause-based classifier dramatically." ></td>
	<td class="line x" title="260:344	This confirms our observation that the consequent usually plays the key role in determining the sentiment of the sentence." ></td>
	<td class="line x" title="261:344	This is further reinforced by the fact that the consequent-based classifier actually performs similarly to the whole-sentence-based classifier." ></td>
	<td class="line x" title="262:344	The condition clause seems to give no help." ></td>
	<td class="line x" title="263:344	2." ></td>
	<td class="line x" title="264:344	The second observation is that the linguistic knowledge of canonical tense patterns helps significantly." ></td>
	<td class="line x" title="265:344	This shows that the linguistic knowledge is very useful." ></td>
	<td class="line x" title="266:344	We also noticed that many misclassifications are caused by grammatical errors, use of slang phrases and improper punctuations, which are typical of postings on the Web." ></td>
	<td class="line x" title="267:344	Due to language irregularities (e.g., wrong grammar, missing punctuations, sarcasm, exclamations), the POS tagger makes many mistakes as well causing some errors in the tense based features." ></td>
	<td class="line x" title="268:344	Three-class classification: We now move to the more difficult and realistic case of three classes: positive, negative and neutral (no-opinion)." ></td>
	<td class="line x" title="269:344	Table 6 shows the results." ></td>
	<td class="line x" title="270:344	The trend is similar except that the whole-sentence-based classifier now performs markedly better than the consequentbased classifier." ></td>
	<td class="line x" title="271:344	We believe that this is because the neutral class needs information from both the condition and consequent clauses." ></td>
	<td class="line x" title="272:344	This is evident from the fact that there is little or no improvement after {I+II} for the consequent-based classifier." ></td>
	<td class="line x" title="273:344	We also observe that the accuracies and Fscores for the three-class classification are lower than those for the two-class classification." ></td>
	<td class="line x" title="274:344	This is understandable due to the difficulty of determining whether a sentence has opinion or not." ></td>
	<td class="line x" title="275:344	Again, statistical test shows that the canonical tensebased features help significantly." ></td>
	<td class="line x" title="276:344	As mentioned in Section 4.1, the wholesentence-based classifier only considers those sentiment words in the vicinity of the topic under  Positive Negative Neutral Condition 6.9% 6.7% 86.4% Consequent 49.3% 16.5% 34% Topic-level 43.5% 26.4% 29.9% 186 investigation." ></td>
	<td class="line x" title="277:344	For this, we search a window of n words on either side of the topic mention." ></td>
	<td class="line x" title="278:344	To study the effect of varying n, we performed an experiment with various values of the window size and measured the overall accuracy for each case." ></td>
	<td class="line x" title="279:344	Table 7 shows how the accuracy changes as we increase the window size." ></td>
	<td class="line x" title="280:344	We found that a window size of 6-10 yielded good accuracies." ></td>
	<td class="line x" title="281:344	This is because lower values of n lead to loss of information regarding sentiment words as some sentiment words could be far from the topic." ></td>
	<td class="line x" title="282:344	We finally used 8, which gave the best results." ></td>
	<td class="line x" title="283:344	We also investigated ways of using the negation word in the sentence to correctly predict the sentiment." ></td>
	<td class="line x" title="284:344	One method is to use the negation word as a feature, as described in Section 4.1." ></td>
	<td class="line x" title="285:344	Another technique is to reverse the orientation of the prediction for those sentences which contain negation words." ></td>
	<td class="line x" title="286:344	We found that the former technique yielded better results." ></td>
	<td class="line x" title="287:344	The results reported so far are based on the former approach." ></td>
	<td class="line x" title="288:344	6 Related Work There are several research directions in sentiment analysis (or opinion mining)." ></td>
	<td class="line oc" title="289:344	One of the main directions is sentiment classification, which classifies the whole opinion document (e.g., a product review) as positive or negative (e.g., Pang et al, 2002; Turney, 2002; Dave et al, 2003; Ng et al. 2006; McDonald et al, 2007)." ></td>
	<td class="line x" title="290:344	It is clearly different from our work as we are interested in conditional sentences." ></td>
	<td class="line x" title="291:344	Another important direction is classifying sentences as subjective or objective, and classifying subjective sentences or clauses as positive or negative (Wiebe et al, 1999; Wiebe and Wilson, 2002, Yu and Hatzivassiloglou, 2003; Wilson et al, 2004; Kim and Hovy, 2004; Riloff and Wiebe, 2005; Gamon et al 2005; McDonald et al, 2007)." ></td>
	<td class="line x" title="292:344	Although these works deal with sentences, they aim to solve the general problem." ></td>
	<td class="line x" title="293:344	This paper argues that there is unlikely a onetechnique-fit-all solution, and advocates dealing with specific types of sentences differently by exploiting their unique characteristics." ></td>
	<td class="line x" title="294:344	Conditional sentences are the focus of this paper." ></td>
	<td class="line x" title="295:344	To the best of our knowledge, there is no focused study on them." ></td>
	<td class="line x" title="296:344	Several researchers also studied feature/topicbased sentiment analysis (e.g., Hu and Liu, 2004; Popescu and Etzioni, 2005; Ku et al, 2006; Carenini et al, 2006; Mei et al, 2007; Ding, Liu and Yu, 2008; Titov and R. McDonald, 2008; Stoyanov and Cardie, 2008; Lu and Zhai, 2008)." ></td>
	<td class="line x" title="297:344	Their objective is to extract topics or product features in sentences and determine whether the sentiments expressed on them are positive or negative." ></td>
	<td class="line x" title="298:344	Again, no focused study has been made to handle conditional sentences." ></td>
	<td class="line x" title="299:344	Effectively handling of conditional sentences can help their effort significantly." ></td>
	<td class="line x" title="300:344	Table 5: Two-class classification  positive and negative  Clause-based classifier Consequent-based classifier Whole-sentence-based classifier Acc." ></td>
	<td class="line x" title="301:344	Prec." ></td>
	<td class="line x" title="302:344	Rec." ></td>
	<td class="line x" title="303:344	F Acc." ></td>
	<td class="line x" title="304:344	Prec." ></td>
	<td class="line x" title="305:344	Rec." ></td>
	<td class="line x" title="306:344	F Acc." ></td>
	<td class="line x" title="307:344	Prec." ></td>
	<td class="line x" title="308:344	Rec." ></td>
	<td class="line x" title="309:344	F I+II (senti." ></td>
	<td class="line x" title="310:344	words+POS) 39.9 42.8 34.0 37.9 69.1 72.9 67.1 69.8 68.9 73.7 68.13 70.8 I+II+III (+ non-senti." ></td>
	<td class="line x" title="311:344	words)  41.5 44.9 37.1 40.6 69.3 73.9 66.3   69.9 69.2 73.7 63.5 71.0 I+II+III+IV (+ tenses) 42.7 45.2 38.5 41.6 72.7 76.4 72.0 74.1   71.1 77.9 72.2 74.9 All 43.2 46.1 38.9 42.2 73.3 77.0 72.7 74.8 72.3 77.8 73.6 75.6 Table 6: Three-class classification  positive, negative and neutral (no opinion)  Clause-based classifier Consequent-based classifier Whole-sentence-based classifier Acc." ></td>
	<td class="line x" title="312:344	Prec." ></td>
	<td class="line x" title="313:344	Rec." ></td>
	<td class="line x" title="314:344	F Acc." ></td>
	<td class="line x" title="315:344	Prec." ></td>
	<td class="line x" title="316:344	Rec." ></td>
	<td class="line x" title="317:344	F Acc." ></td>
	<td class="line x" title="318:344	Prec." ></td>
	<td class="line x" title="319:344	Rec." ></td>
	<td class="line x" title="320:344	F I+II (senti." ></td>
	<td class="line x" title="321:344	words+POS) 45.2 41.3 35.1 37.9 54.6 57.7 52.9 55.2 59.1 58.1 56.4 57.2 I+II+III (+ non-senti." ></td>
	<td class="line x" title="322:344	words)  46.9 42.8 37.8 40.1 55.3 60.0 51.3 55.3 61.4 60.1 60.8 60.4 I+II+III+IV (+ tenses) 50.3 48.7 40.9 44.5 57.3 64.0 50.0 56.1 64.6 63.3 63.9 63.6 All 53.3 49.8 44.1 46.8 58.7 64.5 50.1 56.4 67.8 66.9 65.1 66.0 Table 7: Accuracy of the whole-sentence-based classifier with varying window sizes (n) Window size 1 2 3 4 5 6 7 8 9 10 Accuracy 66.1 62.6 64.1 64.8 65.3 65.7 66.3 67.3 66.9 66.8 187 In this work, we used many sentiment words and phrases." ></td>
	<td class="line x" title="323:344	These words and phrases are usually compiled using different approaches (Hatzivassiloglou and McKeown, 1997; Kaji and Kitsuregawa, 2006; Kanayama and Nasukawa, 2006; Esuli and Sebastiani, 2006; Breck et al, 2007; Ding, Liu and Yu." ></td>
	<td class="line x" title="324:344	2008; Qiu et al, 2009)." ></td>
	<td class="line x" title="325:344	There are several existing lists produced by researchers." ></td>
	<td class="line x" title="326:344	We used the one from the MPQA corpus (http://www.cs.pitt.edu/mpqa) with added phrases of our own from (Ding, Liu and Yu." ></td>
	<td class="line x" title="327:344	2008)." ></td>
	<td class="line x" title="328:344	In our work, we also assume that the topics are known." ></td>
	<td class="line x" title="329:344	(Hu and Liu, 2004; Popescu and Etzioni, 2005; Kobayashi, Inui and Matsumoto, 2007; Stoyanov and Cardie, 2008) have studied topic/feature extraction." ></td>
	<td class="line x" title="330:344	One existing focused study is on comparative and superlative sentences (Jindal and Liu, 2006; Bos and Nissim, 2006; Fiszman et al, 2007; Ganapathibhotla and Liu, 2008)." ></td>
	<td class="line x" title="331:344	Their work identifies comparative sentences, extracts comparative relations in the sentences and analyzes comparative opinions (Ganapathibhotla and Liu, 2008)." ></td>
	<td class="line x" title="332:344	An example comparative sentence is Honda looks better than Toyota." ></td>
	<td class="line x" title="333:344	As we can see, comparative sentences are entirely different from conditional sentences." ></td>
	<td class="line x" title="334:344	Thus, their methods cannot be directly applied to conditional sentences." ></td>
	<td class="line x" title="335:344	7 Conclusion To perform sentiment analysis accurately, we argue that a divide-and-conquer approach is needed, i.e., focused study on each type of sentences." ></td>
	<td class="line x" title="336:344	It is unlikely that there is a one-size-fit-all solution." ></td>
	<td class="line x" title="337:344	This paper studied one type, i.e., conditional sentences, which have some unique characteristics that need special handling." ></td>
	<td class="line x" title="338:344	Our study was carried out from both the linguistic and computational perspectives." ></td>
	<td class="line x" title="339:344	In the linguistic study, we focused on canonical tense patterns, which have been showed useful in classification." ></td>
	<td class="line x" title="340:344	In the computational study, we built SVM models to automatically predict whether opinions on topics are positive, negative or neutral." ></td>
	<td class="line x" title="341:344	Experimental results have shown the effectiveness of the models." ></td>
	<td class="line x" title="342:344	In our future work, we will further improve the classification accuracy and study related problems, e.g., identifying topics/features." ></td>
	<td class="line x" title="343:344	Although there are some special conditional sentences that do not use easily recognizable conditional connectives and identifying them are useful, such sentences are very rare and spending time and effort on them may not be cost-effective at the moment." ></td>
	<td class="line x" title="344:344	Acknowledgements This work was supported in part by DOE SCIDAC-2: Scientific Data Management Center for Enabling Technologies (CET) grant DE-FC0207ER25808, DOE FASTOS award number DEFG02-08ER25848, NSF HECURA CCF0621443, NSF SDCI OCI-0724599, and NSF ST-HEC CCF-0444405." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1061
Topic-wise, Sentiment-wise, or Otherwise? Identifying the Hidden Dimension for Unsupervised Text Classification
Dasgupta, Sajib;Ng, Vincent;"></td>
	<td class="line x" title="1:238	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 580589, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:238	c 2009 ACL and AFNLP Topic-wise, Sentiment-wise, or Otherwise?" ></td>
	<td class="line x" title="3:238	Identifying the Hidden Dimension for Unsupervised Text Classification Sajib Dasgupta and Vincent Ng Human Language Technology Research Institute University of Texas at Dallas Richardson, TX 75083-0688 {sajib,vince}@hlt.utdallas.edu Abstract While traditional work on text clustering has largely focused on grouping documents by topic, it is conceivable that a user may want to cluster documents along other dimensions, such as the authors mood, gender, age, or sentiment." ></td>
	<td class="line x" title="4:238	Without knowing the users intention, a clustering algorithm will only group documents along the most prominent dimension, which may not be the one the user desires." ></td>
	<td class="line x" title="5:238	To address this problem, we propose a novel way of incorporating user feedback into a clustering algorithm, which allows a user to easily specify the dimension along which she wants the data points to be clustered via inspecting only a small number of words." ></td>
	<td class="line x" title="6:238	This distinguishes our method from existing ones, which typically require a large amount of effort on the part of humans in the form of document annotation or interactive construction of the feature space." ></td>
	<td class="line x" title="7:238	We demonstrate the viability of our method on several challenging sentiment datasets." ></td>
	<td class="line x" title="8:238	1 Introduction Text clustering is one of the most important applications in Natural Language Processing (NLP)." ></td>
	<td class="line x" title="9:238	A common approach to this problem consists of (1) computing the similarity between each pair of documents, each of which is typically represented as a bag of words; and (2) using an unsupervised clustering algorithm to partition the documents." ></td>
	<td class="line x" title="10:238	The majority of existing work on text clustering has focused on topic-based clustering, where high accuracies can be achieved even for datasets with a large number of classes (e.g., 20 Newsgroups)." ></td>
	<td class="line x" title="11:238	On the other hand, there has been relatively little work on sentiment-based clustering and the related task of unsupervised polarity classification, where the goal is to cluster (or classify) a set of documents (e.g., reviews) according to the polarity (e.g., thumbs up or thumbs down) expressed by the author in an unsupervised manner." ></td>
	<td class="line oc" title="12:238	Despite the large amount of recent work on sentiment analysis and opinion mining, much of it has focused on supervised methods (e.g., Pang et al.(2002), Kim and Hovy (2004), Mullen and Collier (2004))." ></td>
	<td class="line n" title="14:238	One weakness of these existing supervised polarity classification systems is that they are typically domainand language-specific." ></td>
	<td class="line x" title="15:238	Hence, when given a new domain or language, one needs to go through the expensive process of collecting a large amount of annotated data in order to train a high-performance polarity classifier." ></td>
	<td class="line x" title="16:238	Some recent attempts have been made to leverage existing sentiment corpora or lexica to automatically create annotated resources for new domains or languages." ></td>
	<td class="line x" title="17:238	However, such methods require the existence of either a parallel corpus/machine translation engine for projecting/translating annotations/lexica from a resource-rich language to the target language (Banea et al., 2008; Wan, 2008), or a domain that is similar enough to the target domain (Blitzer et al., 2007)." ></td>
	<td class="line x" title="18:238	When the target domain or language fails to meet this requirement, sentiment-based clustering or unsupervised polarity classification become appealing alternatives." ></td>
	<td class="line x" title="19:238	Unfortunately, to our knowledge, these tasks are largely under-investigated in the NLP community." ></td>
	<td class="line x" title="20:238	Turneys (2002) work is perhaps one of the most notable examples of unsupervised polarity classification." ></td>
	<td class="line x" title="21:238	However, while his system learns the semantic orientation of the phrases in a review in an unsupervised manner, this information is used to predict the polarity of a review heuristically." ></td>
	<td class="line x" title="22:238	Despite its practical significance, sentimentbased clustering is a challenging task." ></td>
	<td class="line x" title="23:238	To illustrate its difficulty, consider the task of clustering a set of movie reviews." ></td>
	<td class="line x" title="24:238	Since each review may contain a description of the plot and the authors 580 sentiment, a clustering algorithm may cluster reviews along either the plot dimension or the sentiment dimension; and without knowing the users intention, they will be clustered along the most prominent dimension." ></td>
	<td class="line x" title="25:238	Assuming the usual bagof-words representation, the most prominent dimension will more likely be plot, as it is not uncommon for a review to be devoted almost exclusively to the plot, with the author briefly expressing her sentiment only at the end of the review." ></td>
	<td class="line x" title="26:238	Even if the reviews contain mostly subjective material, the most prominent dimension may still not be sentiment, due to the fact that many reviews are sentimentally ambiguous." ></td>
	<td class="line x" title="27:238	Specifically, a reviewer may have negative opinions on the actors but at the same time talk enthusiastically about how much she enjoyed the plot." ></td>
	<td class="line x" title="28:238	The presence of both positive and negative sentiment-bearing words in these reviews renders the sentiment dimension hidden (i.e., less prominent) as far as clustering is concerned." ></td>
	<td class="line x" title="29:238	Therefore, there is no guarantee that the clustering algorithm will automatically produce a sentiment-based clustering of the reviews." ></td>
	<td class="line x" title="30:238	Hence, it is important for a user to provide feedback on the clustering process to ensure that the reviews are clustered along the sentiment dimension, possibly in an interactive manner." ></td>
	<td class="line x" title="31:238	One way to do this would be to ask the user to annotate a small number of reviews with polarity information, possibly through an active learning procedure to minimize human intervention (Dredze and Crammer, 2008)." ></td>
	<td class="line x" title="32:238	Another way would be to have the user explicitly identify the relevant features (in our case, the sentiment-bearing words) at the beginning of the clustering process (Liu et al., 2004), or incrementally construct the set of relevant features in an interactive fashion (Bekkerman et al., 2007; Raghavan and Allan, 2007; Roth and Small, 2009)." ></td>
	<td class="line x" title="33:238	In addition, the user may supply constraints on which pairs of documents must or must not appear in the same cluster (Wagstaff et al., 2001), or simply tell the algorithm whether two clusters should be merged or split during the clustering process (Balcan and Blum, 2008)." ></td>
	<td class="line x" title="34:238	It is worth noting that many of these feedback mechanisms were developed by machine learning researchers for general clustering tasks and not for sentiment-based clustering." ></td>
	<td class="line x" title="35:238	Our goal in this paper is to propose a novel mechanism allowing a user to cluster a set of documents along the desired dimension, which may be a hidden dimension, with very limited user feedback." ></td>
	<td class="line x" title="36:238	In comparison to the aforementioned feedback mechanisms, ours is arguably much simpler: we only require that the user select a dimension by examining a small number of features for each dimension, as opposed to having the user generate the feature space in an interactive manner or identify clusters that need to be merged or split." ></td>
	<td class="line x" title="37:238	In particular, identifying clusters for merging or splitting in Balcan and Blums algorithm may not be as easy as it appears: for each MERGE or SPLIT decision the user makes, she has to sample a large number of documents from the cluster(s), read through the documents, and base her decision on the extent to which the documents are (dis)similar to each other." ></td>
	<td class="line x" title="38:238	Perhaps more importantly, our human experiments involving five users indicate that all of them can easily identify the sentiment dimension based on the features, thus providing suggestive evidence that our method is viable." ></td>
	<td class="line x" title="39:238	In sum, our contributions in this paper are threefold." ></td>
	<td class="line x" title="40:238	First, we propose a novel feedback mechanism for clustering allowing a user to easily specify the dimension along which she wants data points to be clustered and apply the mechanism to the challenging, yet under-investigated problem of sentiment-based clustering." ></td>
	<td class="line x" title="41:238	Second, spectral learning, which is the core of our method, has not been applied extensively to NLP problems, and we hope that our work can increase the awareness of this powerful machine learning technique in the NLP community." ></td>
	<td class="line x" title="42:238	Finally, we demonstrate the viability of our method not only by evaluating its performance on sentiment datasets, but also via a set of human experiments, which is typically absent in papers that involve algorithms for incorporating user feedback." ></td>
	<td class="line x" title="43:238	The rest of the paper is organized as follows." ></td>
	<td class="line x" title="44:238	Section 2 presents the basics of spectral clustering, which will facilitate the discussion of our feedback mechanism in Section 3." ></td>
	<td class="line x" title="45:238	We describe our human experiments and evaluation results on several sentiment datasets in Section 4, and present our conclusions in Section 5." ></td>
	<td class="line x" title="46:238	2 Spectral Clustering When given a clustering task, an important question to ask is: which clustering algorithm should we use?" ></td>
	<td class="line x" title="47:238	A popular choice is k-means." ></td>
	<td class="line x" title="48:238	Nevertheless, it is well-known that k-means has the major drawback of not being able to separate data points 581 that are not linearly separable in the given feature space (e.g., see Dhillon et al.(2004) and Cai et al.(2005))." ></td>
	<td class="line x" title="51:238	Spectral clustering algorithms were developed in response to this problem with k-means." ></td>
	<td class="line x" title="52:238	The central idea behind spectral clustering is to (1) construct a low-dimensional space from the original (typically high-dimensional) space while retaining as much information about the original space as possible, and (2) cluster the data points in this low-dimensional space." ></td>
	<td class="line x" title="53:238	The rest of this section provides the details of spectral clustering." ></td>
	<td class="line x" title="54:238	2.1 Algorithm Although there are several well-known spectral clustering algorithms in the literature (e.g., Weiss (1999), Shi and Malik (2000), Kannan et al.(2004)), we adopt the one proposed by Ng et al.(2002), as it is arguably the most widely-used." ></td>
	<td class="line x" title="57:238	The algorithm takes as input a similarity matrix S created by applying a user-defined similarity function to each pair of data points." ></td>
	<td class="line x" title="58:238	Below are the main steps of the algorithm: 1." ></td>
	<td class="line x" title="59:238	Create the diagonal matrix D whose (i,i)th entry is the sum of the i-th row of S, and then construct the Laplacian matrix L = D1/2SD1/2." ></td>
	<td class="line x" title="60:238	2. Find the eigenvalues and eigenvectors of L. 3." ></td>
	<td class="line x" title="61:238	Create a new matrix from the m eigenvectors that correspond to the m largest eigenvalues.1 4." ></td>
	<td class="line x" title="62:238	Each data point is now rank-reduced to a point in the m-dimensional space." ></td>
	<td class="line x" title="63:238	Normalize each point to unit length (while retaining the sign of each value)." ></td>
	<td class="line x" title="64:238	5." ></td>
	<td class="line x" title="65:238	Cluster the resulting data points using kmeans." ></td>
	<td class="line x" title="66:238	In essence, each dimension in the reduced space is defined by exactly one eigenvector." ></td>
	<td class="line x" title="67:238	The reason why eigenvectors with large eigenvalues are used is that they capture the largest variance in the data." ></td>
	<td class="line x" title="68:238	As a result, each of them can be thought of as revealing an important dimension of the data." ></td>
	<td class="line x" title="69:238	2.2 Clustering with Eigenvectors As Ng et al.(2002) point out, different authors still disagree on which eigenvectors to use, and how to derive clusters from them." ></td>
	<td class="line x" title="71:238	There are two common methods for deriving clusters using the eigenvectors." ></td>
	<td class="line x" title="72:238	These methods will serve as our baselines in our evaluation." ></td>
	<td class="line x" title="73:238	1For brevity, we will refer to the eigenvector with the n-th largest eigenvalue simply as the n-th eigenvector." ></td>
	<td class="line x" title="74:238	Method 1: Using the second eigenvector only The first method is to use only the second eigenvector, e2, to partition the points." ></td>
	<td class="line x" title="75:238	Besides revealing one of the most important dimensions of the data, this eigenvector induces an intuitively ideal partition of the data  the partition induced by the minimum normalized cut of the similarity graph2, where the nodes are the data points and the edge weights are the pairwise similarity values of the points (Shi and Malik, 2000)." ></td>
	<td class="line x" title="76:238	Clustering in a onedimensional space is trivial: since we have a linearization of the points, all we need to do is to determine a threshold for partitioning the points." ></td>
	<td class="line x" title="77:238	However, we follow Ng et al.(2002) and cluster using 2-means in this one-dimensional space." ></td>
	<td class="line x" title="79:238	Method 2: Using m eigenvectors Recall from Section 2.1 that after eigendecomposing the Laplacian matrix, each data point is represented by m co-ordinates." ></td>
	<td class="line x" title="80:238	In the second method, we simply use 2-means to cluster the data points in this m-dimensional space, effectively exploiting all of the m eigenvectors." ></td>
	<td class="line x" title="81:238	3 Our Approach As mentioned before, sentiment-based clustering is challenging, in part due to the fact that the reviews can be clustered along more than one dimension." ></td>
	<td class="line x" title="82:238	In this section, we propose and incorporate a user feedback mechanism into a spectral clustering algorithm, which makes it easy for a user to specify the dimension along which she wants to cluster the data points." ></td>
	<td class="line x" title="83:238	Recall that our method first applies spectral clustering to reveal the most important dimensions of the data, and then lets the user select the desired dimension." ></td>
	<td class="line x" title="84:238	To motivate the importance of user feedback, it helps to understand why the two baseline clustering algorithms described in Section 2.2, which are also based on spectral methods but do not rely on user feedback, may not always yield a sentiment-based clustering." ></td>
	<td class="line x" title="85:238	To begin with, consider the first method, where only the second eigenvector is used to induce the partition." ></td>
	<td class="line x" title="86:238	Recall that the second eigenvector reveals the most prominent dimension of the data." ></td>
	<td class="line x" title="87:238	Hence, if sentiment is not the most prominent dimension (which can happen if the non-sentiment-bearing 2Using the normalized cut (as opposed to the usual cut) ensures that the size of the two clusters are relatively balanced, avoiding trivial cuts where one cluster is empty and the other is full." ></td>
	<td class="line x" title="88:238	See Shi and Malik (2000) for details." ></td>
	<td class="line x" title="89:238	582 words outnumber the sentiment-bearing words in the bag-of-words representation of a review), then the resulting clustering of the reviews may not be sentiment-oriented." ></td>
	<td class="line x" title="90:238	A similar line of reasoning can be used to explain why the second baseline clustering algorithm, which clusters based on all of the eigenvectors in the low-dimensional space, may not always work well." ></td>
	<td class="line x" title="91:238	Since each eigenvector corresponds to a different dimension (and, in particular, some of them correspond to non-sentiment dimensions), using all of them to represent a review may hamper the accurate computation of the similarity of two reviews as far as clustering along the sentiment dimension is concerned." ></td>
	<td class="line x" title="92:238	In the rest of this section, we discuss the major steps of our user-feedback mechanism in detail." ></td>
	<td class="line x" title="93:238	Step 1: Identify the important dimensions To identify the important dimensions of the given reviews, we take the top eigenvectors computed from the eigen-decomposition of the Laplacian matrix, which is in turn formed from the input similarity matrix." ></td>
	<td class="line x" title="94:238	We compute the similarity between two reviews by taking the dot product of their feature vectors (see Section 4.1 for details on feature vector generation)." ></td>
	<td class="line x" title="95:238	Following Ng et al., we set the diagonal entries of the similarity matrix to 0." ></td>
	<td class="line x" title="96:238	Step 2: Identify the relevant features Given the eigen-decomposition from Step 1, we first obtain the second through the fifth eigenvectors3, which as mentioned above, correspond to the most important dimensions of the data." ></td>
	<td class="line x" title="97:238	Then, we ask the user to select one of the four dimensions defined by these eigenvectors according to their relevance to sentiment." ></td>
	<td class="line x" title="98:238	One way to do this is to (1) induce one partition of the reviews from each of the four eigenvectors, using a procedure identical to Method 1 in Section 2.2, and (2) have the user inspect the four partitions and decide which corresponds most closely to a sentimentbased clustering." ></td>
	<td class="line x" title="99:238	The main drawback associated with this kind of user feedback is that the user may have to read a large number of reviews in order to make a decision." ></td>
	<td class="line x" title="100:238	Hence, to reduce human effort, we employ an alternative procedure: we (1) identify the most informative features for characterizing each partition, and (2) have the user inspect just the features rather than the reviews." ></td>
	<td class="line x" title="101:238	While traditional feature selection techniques such as log-likelihood ratio and information 3The first eigenvector is not used because it is a constant vector, meaning that it cannot be used to partition the data." ></td>
	<td class="line x" title="102:238	gain can be applied to identify these informative features (see Yang and Pedersen (1997) for an overview), we employ a more sophisticated feature-ranking method that we call maximum margin feature ranking (MMFR)." ></td>
	<td class="line x" title="103:238	Recall that a maximum margin classifier (e.g., a support vector machine) separates data points from two classes while maximizing the margin of separation." ></td>
	<td class="line x" title="104:238	Specifically, a maximum margin hyperplane is defined by w  x  b = 0, where x is a feature vector representing an arbitrary data point, and w (a weight vector) and b (a scalar) are parameters that are learned by solving the following constrained optimization problem: argmin 12bardblwbardbl2 + C summationdisplay i i subject to ci(wxi b)  1i, 1  i  n, where ci  {+1,1} is the class of the i-th training point xi, i is the degree of misclassification of xi, and C is a regularization parameter that balances training error and model complexity." ></td>
	<td class="line x" title="105:238	We use w to identify the most informative features for a partition." ></td>
	<td class="line x" title="106:238	Note that a feature with a large positive weight is strongly indicative of the positive class, whereas a feature with a large negative weight is strongly indicative of the negative class." ></td>
	<td class="line x" title="107:238	In other words, the most informative features are those with large absolute weight values." ></td>
	<td class="line x" title="108:238	We exploit this observation and identify the most informative features for a partition by (1) training an SVM classifier4 on the partition, where data points in the same cluster belong to the same class; (2) sorting the features according to the SVMlearned feature weights; and (3) generating two ranked lists of informative features using the top and bottom 100 features, respectively." ></td>
	<td class="line x" title="109:238	Given the ranked lists generated for each of the four partitions, the user will select one of the partitions/dimensions as most relevant to sentiment by inspecting as many features in the ranked lists as needed." ></td>
	<td class="line x" title="110:238	After picking the most relevant dimension, the user will label one of the two feature lists associated with this dimension as POSITIVE and the other as NEGATIVE." ></td>
	<td class="line x" title="111:238	Since each feature list represents one of the clusters, the cluster associated with the positive list is labeled POSITIVE and 4All the SVM classifiers in this paper are trained using the SVMlight package (Joachims, 1999), with the learning parameters set to their default values." ></td>
	<td class="line x" title="112:238	583 the cluster associated with the negative list is labeled NEGATIVE." ></td>
	<td class="line x" title="113:238	In comparison to existing user feedback mechanisms for assisting a clustering algorithm, ours requires comparatively little human intervention: we only require that the user select a dimension by examining a small number of features, as opposed to having the user construct the feature space or identify clusters that need to be merged or split as is required with other methods." ></td>
	<td class="line x" title="114:238	Step 3: Identify the unambiguous reviews There is a caveat, however." ></td>
	<td class="line x" title="115:238	As mentioned in the introduction, many reviews contain both positive and negative sentiment-bearing words." ></td>
	<td class="line x" title="116:238	These ambiguous reviews are more likely to be clustered incorrectly than their unambiguous counterparts." ></td>
	<td class="line x" title="117:238	Now, since the ranked lists of features are derived from the partition, the presence of these ambiguous reviews can adversely affect the identification of informative features using MMFR." ></td>
	<td class="line x" title="118:238	As a result, we remove the ambiguous reviews before deriving informative features from a partition." ></td>
	<td class="line x" title="119:238	We employ a simple method for identifying unambiguous reviews." ></td>
	<td class="line x" title="120:238	In the computation of eigenvalues, each data point factors out the orthogonal projections of each of the other data points with which they have an affinity." ></td>
	<td class="line x" title="121:238	Ambiguous data points receive the orthogonal projections from both the positive and negative data points, and hence they have near zero values in the pivot eigenvectors." ></td>
	<td class="line x" title="122:238	We exploit this important information." ></td>
	<td class="line x" title="123:238	The basic idea is that the data points with near zero values in the eigenvectors are more ambiguous than those with large absolute values." ></td>
	<td class="line x" title="124:238	As a result, we posit 250 reviews from each cluster whose corresponding values in the eigenvector are farthest away from zero as unambiguous, and induce the ranked list of features only from the resulting 500 unambiguous reviews.5 Step 4: Cluster along the selected dimension Finally, we employ the 2-means algorithm to cluster all the reviews along the dimension (i.e., the eigenvector) selected by the user, regardless of whether a review is ambiguous or not." ></td>
	<td class="line x" title="125:238	5Note that 500 is a somewhat arbitrary choice." ></td>
	<td class="line x" title="126:238	Underlying this choice is our assumption that a fraction of the reviews is unambiguous." ></td>
	<td class="line x" title="127:238	As we will see in the evaluation section, these 500 reviews can be classified with a high accuracy; consequently, the features induced from the resulting clusters are also of high quality." ></td>
	<td class="line x" title="128:238	Additional experiments reveal that the list of top-ranking features does not change significantly when induced from a smaller number of unambiguous reviews." ></td>
	<td class="line x" title="129:238	4 Evaluation 4.1 Experimental Setup Datasets." ></td>
	<td class="line pc" title="130:238	We use five sentiment classification datasets, including the widely-used movie review dataset [MOV] (Pang et al., 2002) as well as four datasets containing reviews of four different types of products from Amazon [books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT)] (Blitzer et al., 2007)." ></td>
	<td class="line o" title="131:238	Each dataset has 2000 labeled reviews (1000 positives and 1000 negatives)." ></td>
	<td class="line x" title="132:238	To illustrate the difference between topic-based clustering and sentiment-based clustering, we will also show topic-based clustering results on POL, a dataset created by taking all the documents from two sections of 20 Newsgroups, namely, sci.crypt and talks.politics." ></td>
	<td class="line x" title="133:238	To preprocess a document, we first tokenize and downcase it, and then represent it as a vector of unigrams, using frequency as presence." ></td>
	<td class="line x" title="134:238	In addition, we remove from the vector punctuation, numbers, words of length one, and words that occur in only a single review." ></td>
	<td class="line x" title="135:238	Following the common practice in the information retrieval community, we also exclude words with high document frequency, many of which are stopwords or domainspecific general-purpose words (e.g., movies in the movie domain)." ></td>
	<td class="line x" title="136:238	A preliminary examination of our evaluation datasets reveals that these words typically comprise 12% of a vocabulary." ></td>
	<td class="line x" title="137:238	The decision of exactly how many terms to remove from each dataset is subjective: a large corpus typically requires more removals than a small corpus." ></td>
	<td class="line x" title="138:238	To be consistent, we simply sort the vocabulary by document frequency and remove the top 1.5%." ></td>
	<td class="line x" title="139:238	Evaluation metrics." ></td>
	<td class="line x" title="140:238	We employ two evaluation metrics." ></td>
	<td class="line x" title="141:238	First, we report results in terms of the accuracy achieved on the 2000 labeled reviews for each dataset." ></td>
	<td class="line x" title="142:238	Second, following Kamvar et al.(2003), we evaluate the clusters produced by our approach against the gold-standard clusters using the Adjusted Rand Index (ARI)." ></td>
	<td class="line x" title="144:238	ARI ranges from 1 to 1; better clusterings have higher ARI values." ></td>
	<td class="line x" title="145:238	4.2 Baseline Systems Clustering using the second eigenvector only." ></td>
	<td class="line x" title="146:238	As our first baseline, we adopt Shi and Maliks approach and cluster the reviews using only the second eigenvector, e2, as described in Section 2.2." ></td>
	<td class="line x" title="147:238	Results on POL and the five sentiment datasets are 584 Accuracy Adjusted Rand Index System Variation POL MOV KIT BOO DVD ELE POL MOV KIT BOO DVD ELE Baseline: 2nd eigenvector 93.7 70.9 69.7 58.9 55.3 50.8 0.76 0.17 0.15 0.03 0.01 0.01 Baseline: m eigenvectors 95.9 59.3 63.2 60.1 62.5 63.8 0.84 0.03 0.07 0.04 0.06 0.08 Our approach 93.7 70.9 69.7 69.5 70.8 65.8 0.76 0.17 0.15 0.15 0.17 0.10 Table 1: Results in terms of accuracy and Adjusted Rand Index for the six datasets." ></td>
	<td class="line x" title="148:238	shown in row 1 of Table 1.6 As we can see, this baseline achieves an accuracy of 90% on POL, but a much lower accuracy (of 5070%) on the sentiment datasets." ></td>
	<td class="line x" title="149:238	The same performance trend can be observed with ARI." ></td>
	<td class="line x" title="150:238	These results provide support for the claim that sentiment-based clustering is more difficult than topic-based clustering." ></td>
	<td class="line x" title="151:238	In addition, it is worth noting that the baseline achieves much lower accuracies and ARI values on BOO, DVD, and ELE than on the remaining two sentiment datasets." ></td>
	<td class="line x" title="152:238	Since e2 captures the most prominent dimension, these results suggest that sentiment dimension is not the most prominent dimension in these three datasets." ></td>
	<td class="line x" title="153:238	In fact, this is intuitively plausible." ></td>
	<td class="line x" title="154:238	For instance, in the book domain, positive book reviews typically contain a short description of the content, with the reviewer only briefly expressing her sentiment somewhere in the review." ></td>
	<td class="line x" title="155:238	Similarly for the electronics domain: electronic product reviews are typically aspect-oriented, with the reviewer talking about the pros and cons of each aspect of the product (e.g., battery, durability)." ></td>
	<td class="line x" title="156:238	Since the reviews are likely to contain both positive and negative sentiment-bearing words, the sentiment-based clustering is unlikely to be captured by e2." ></td>
	<td class="line x" title="157:238	Clustering using top five eigenvectors." ></td>
	<td class="line x" title="158:238	As our second baseline, we represent each data point using the top five eigenvectors (i.e., e1 through e5), and cluster them using 2-means in this 5dimensional space, as described in Section 2.2." ></td>
	<td class="line x" title="159:238	Hence, this can be thought of as an ensemble approach, where the clustering decision is collectively made by the five eigenvectors." ></td>
	<td class="line x" title="160:238	Results are shown in row 2 of Table 1." ></td>
	<td class="line x" title="161:238	In comparison to the first baseline, we see improvements in accuracy and ARI for the three datasets on which the first baseline performs poorly (i.e., BOO, DVD, and ELE), with the most drastic improvement observed on ELE." ></td>
	<td class="line x" title="162:238	On the other hand, performance on the remaining two senti6Owing to the randomness in the choice of seeds for 2means, these and all other experimental results involving 2means are averaged over ten independent runs." ></td>
	<td class="line x" title="163:238	ment datasets deteriorates." ></td>
	<td class="line x" title="164:238	These results can be attributed to the fact that for BOO, DVD, and ELE, e2 does not capture the sentiment dimension, but since some other eigenvector in the ensemble does, we see improvements." ></td>
	<td class="line x" title="165:238	On the other hand, e2 has already captured the sentiment dimension in MOV and KIT; as a result, employing additional dimensions, which may not be sentiment-related, may only introduce noise into the computation of the similarities between the reviews." ></td>
	<td class="line x" title="166:238	4.3 Our Approach Human experiments." ></td>
	<td class="line x" title="167:238	Unlike the two baselines, our approach requires users to specify which of the four dimensions (defined by the second through fifth eigenvectors) are most closely related to sentiment by inspecting a set of features derived from the unambiguous reviews for each dimension using MMFR." ></td>
	<td class="line x" title="168:238	To better understand how easy it is for a human to select the desired dimension given the features, we performed the experiment independently with five humans (all of whom are computer science graduate students not affiliated with this research) and computed the agreement rate." ></td>
	<td class="line x" title="169:238	More specifically, for each dataset, we showed each human judge the top 100 features for each cluster according to MMFR (see Tables 46 for a snippet)." ></td>
	<td class="line x" title="170:238	In addition, we informed them of the intended dimension: for example, for POL, the judge was told that the intended clustering is Politics vs. Science." ></td>
	<td class="line x" title="171:238	Also, if she determined that more than one dimension was relevant to the intended clustering, she was instructed to rank these dimensions in terms of their degree of relevance, where the most relevant one would appear first in the list." ></td>
	<td class="line x" title="172:238	The dimensions (expressed in terms of the IDs of the eigenvectors) selected by each of the five judges for each dataset are shown in Table 2." ></td>
	<td class="line x" title="173:238	The agreement rate (shown in the last row of the table) was computed based on only the highestranked dimension selected by each judge." ></td>
	<td class="line x" title="174:238	As we can see, perfect agreement is achieved for four of the five sentiment datasets, and for the remaining two datasets, near-perfect agreement is achieved." ></td>
	<td class="line x" title="175:238	585 Judge POL MOV KIT BOO DVD ELE 1 2,3,4 2 2 4 3 3 2 2,4 2 2 4 3 3 3 4 2,4 4 4 3 3 4 2,3 2 2 4 3 3,4 5 2 2 2 4 3 3 Agr 80% 100% 80% 100% 100% 100% Table 2: Human agreement rate." ></td>
	<td class="line x" title="176:238	POL MOV KIT BOO DVD ELE Acc 99.8 87.0 87.6 86.2 87.4 77.6 Table 3: Accuracies on unambiguous documents." ></td>
	<td class="line x" title="177:238	These results together with the fact that it took 5 6 minutes to identify the relevant dimension, indicate that asking a human to determine the intended dimension based on solely the informative features is a viable task." ></td>
	<td class="line x" title="178:238	Clustering results." ></td>
	<td class="line x" title="179:238	Next, we cluster all 2000 documents for each dataset using the dimension selected by the majority of the human judges." ></td>
	<td class="line x" title="180:238	The clustering results are shown in row 3 of Table 1." ></td>
	<td class="line x" title="181:238	In comparison to the better baseline for each dataset, we see that our approach performs substantially better on BOO, DVD and ELE, at almost the same level on MOV and KIT, but slightly worse on POL." ></td>
	<td class="line x" title="182:238	Note that the improvements observed for BOO, DVD and ELE can be attributed to the failure of e2 to capture the sentiment dimension." ></td>
	<td class="line x" title="183:238	Perhaps most importantly, by exploiting human feedback, our approach has achieved more stable performance across the datasets than the baselines, with accuracies ranging from 65.8% to 93.7% and ARI ranging from 0.10 to 0.76." ></td>
	<td class="line x" title="184:238	Role of unambiguous documents." ></td>
	<td class="line x" title="185:238	Recall that the features with the largest MMFR were computed from the unambiguous documents only." ></td>
	<td class="line x" title="186:238	To get an intuitive understanding of the role of unambiguous documents in our approach, we show in Table 3 the accuracy when the unambiguous documents in each dataset were clustered using the eigenvector selected by the majority of the judges." ></td>
	<td class="line x" title="187:238	As we can see, the accuracy of each dataset is higher than the corresponding accuracy shown in row 3 of Table 1." ></td>
	<td class="line x" title="188:238	In fact, an accuracy of more than 85% was achieved on all but one dataset." ></td>
	<td class="line x" title="189:238	This suggests that our method of identifying unambiguous documents is useful." ></td>
	<td class="line x" title="190:238	Note that it is crucial to be able to achieve a high accuracy on the unambiguous documents: if clustering accuracy is low, the features induced from the clusters may not be an accurate representation of the corresponding dimension, and the human judge may have a difficult time identifying the intended dimension." ></td>
	<td class="line x" title="191:238	In fact, some human judges reported difficulty in identifying the correct dimension for the ELE dataset, and this can be attributed in part to the low accuracy achieved on the unambiguous documents." ></td>
	<td class="line x" title="192:238	Features as summary." ></td>
	<td class="line x" title="193:238	Recall that the method we proposed represents each dimension with a small number of features and asks a user to select the desired dimension by inspecting the corresponding feature lists." ></td>
	<td class="line x" title="194:238	In other words, each feature list serves as a summary of its corresponding dimension, and inspecting the features induced for each dimension can give us insights into the different dimensions of a dataset." ></td>
	<td class="line x" title="195:238	Hence, if a user is not sure how she wants the data points to be clustered (due to lack of knowledge of the data, for instance), our automatically induced features may serve as an overview of the different dimensions of the data." ></td>
	<td class="line x" title="196:238	To better understand whether these features can indeed provide a user with additional useful information about a dataset, we show in Tables 46 the top ten features induced for each cluster and each dimension for the six datasets." ></td>
	<td class="line x" title="197:238	As an example, consider the MOV dataset." ></td>
	<td class="line x" title="198:238	Inspecting the induced features, we can determine that it has a sentiment dimension (e2), as well as a humor vs. thriller dimension (e4)." ></td>
	<td class="line x" title="199:238	In other words, if we cluster along e2, we get a sentiment-based clustering; and if we cluster along e4, we obtain a genre-based (humor vs. thriller) clustering." ></td>
	<td class="line x" title="200:238	User feedback vs. labeled data." ></td>
	<td class="line x" title="201:238	Recall that our two baselines are unsupervised, whereas our approach can be characterized as semi-supervised, as it relies on user feedback to select the intended dimension." ></td>
	<td class="line x" title="202:238	Hence, it should not be surprising to see that the average clustering performance of our approach is better than that of the baselines." ></td>
	<td class="line x" title="203:238	To do a fairer comparison, we conduct another experiment in which we compare our approach against a semi-supervised sentiment classification system, which uses transductive SVM as the underlying semi-supervised learner." ></td>
	<td class="line x" title="204:238	More specifically, the goal of this experiment is to determine how many labeled documents are needed in order for the transductive learner to achieve the same level of performance as our approach." ></td>
	<td class="line x" title="205:238	To answer this question, we first give the transductive learner access to the 2000 documents for each dataset as 586 POL MOV e2 e3 e4 e5 e2 e3 e4 e5 C1 C1 C1 C1 C1 C1 C1 C1 serder beyer serbs escrow relationship production jokes starts armenian arabs palestinians serial son earth kids person turkey andi muslims algorithm tale sequences live saw armenians research wrong chips husband aliens animation feeling muslims israelis department ensure perfect war disney lives sdpa tim bosnia care drama crew animated told argic uci live strong focus alien laughs happen davidian ab matter police strong planet production am dbd@ura z@virginia freedom omissions beautiful horror voice felt troops holocaust politics excepted nature evil hilarious happened C2 C2 C2 C2 C2 C2 C2 C2 sternlight escrow standard internet worst sex thriller comic wouldn sternlight sternlight uucp stupid romantic killer sequences pgp algorithm des uk waste school murder michael crypto access escrow net bunch relationship crime supporting algorithm net employer quote wasn friends police career isn des net ac video jokes car production likely privacy york co worse laughs dead peter access uk jake didn boring sexual killed style idea systems code ai guess cute starts latest cryptograph pgp algorithm mit anyway mother violence entertaining Table 4: Top ten features induced for each dimension for the POL and MOV domains." ></td>
	<td class="line x" title="206:238	The shaded columns correspond to the dimensions selected by the human judges." ></td>
	<td class="line x" title="207:238	e2, , e5 are the top eigenvectors; C1 and C2 are the clusters." ></td>
	<td class="line x" title="208:238	BOO ELE e2 e3 e4 e5 e2 e3 e4 e5 C1 C1 C1 C1 C1 C1 C1 C1 history series loved must mouse music easy amazon must man highly wonderful cable really used cable modern history easy old cables ipod card card important character enjoyed feel case too fine recommend text death children away red little using dvd reference between again children monster headphones problems camera excellent war although year picture hard fine fast provides seems excellent someone kit excellent drive far business political understand man overall need computer printer both american three made paid fit install picture C2 C2 C2 C2 C2 C2 C2 C2 plot buy money boring working worked money phone didn bought bad series never problem worth off thought information nothing history before never amazon worked boring easy waste pages phone item over power got money buy information days amazon return battery character recipes anything between headset working years unit couldn pictures doesn highly money support much set ll look already page months months headphones phones ending waste instead excellent return returned sony range fan copy seems couldn second another received little Table 5: Top ten features induced for each dimension for the BOO and ELE domains." ></td>
	<td class="line x" title="209:238	The shaded columns correspond to the dimensions selected by the human judges." ></td>
	<td class="line x" title="210:238	e2, , e5 are the top eigenvectors; C1 and C2 are the clusters." ></td>
	<td class="line x" title="211:238	unlabeled data." ></td>
	<td class="line x" title="212:238	Next, we randomly sample 50 unlabeled documents and assign them the true label." ></td>
	<td class="line x" title="213:238	We then re-train the classifier and compute its accuracy on the 2000 documents." ></td>
	<td class="line x" title="214:238	We keep adding more labeled data (50 in each iteration) until it reaches the accuracy achieved by our system." ></td>
	<td class="line x" title="215:238	Results of this experiment are shown in Table 7." ></td>
	<td class="line x" title="216:238	Owing in the randomness involved in the selection of unlabeled documents, these results are averaged over ten independent runs." ></td>
	<td class="line x" title="217:238	As we can see, our 587 KIT DVD e2 e3 e4 e5 e2 e3 e4 e5 C1 C1 C1 C1 C1 C1 C1 C1 love works really pan worth music video money clean water nice oven bought collection music quality nice clean works cooking series excellent found video size work too made money wonderful feel worth set ice quality pans season must bought found kitchen makes small better fan loved workout version easily thing sturdy heat collection perfect daughter picture sturdy need little cook music highly recommend waste recommend keep think using tv makes our special price best item clean thought special disappointed sound C2 C2 C2 C2 C2 C2 C2 C2 months price ve love young worst series saw still item years coffee between money cast watched back set love too actors thought fan loved never ordered never recommend men boring stars enjoy worked amazon clean makes cast nothing original whole money gift months over seems minutes comedy got did got over size job waste actors family amazon quality pan little beautiful saw worth series return received been maker around pretty classic season machine knives pans cup director reviews action liked Table 6: Top ten features induced for each dimension for the KIT and DVD domains." ></td>
	<td class="line x" title="218:238	The shaded columns correspond to the dimensions selected by the human judges." ></td>
	<td class="line x" title="219:238	e2, , e5 are the top eigenvectors; C1 and C2 are the clusters." ></td>
	<td class="line x" title="220:238	POL MOV KIT BOO DVD ELE # labels 400 150 200 350 350 200 Table 7: Transductive SVM results." ></td>
	<td class="line x" title="221:238	user feedback is equivalent to the effort of handannotating 275 documents per dataset on average." ></td>
	<td class="line x" title="222:238	Multiple relevant dimensions." ></td>
	<td class="line x" title="223:238	As seen from Table 2, some human judges selected more than one dimension for some datasets (e.g., 2,3,4 for POL; 2,4 for MOV; and 3,4 for ELE)." ></td>
	<td class="line x" title="224:238	However, we never took into account these extra dimensions in our previous experiments." ></td>
	<td class="line x" title="225:238	To better understand whether these extra dimensions can help improve accuracy and ARI, we conduct another experiment in which we apply 2-means to cluster the documents in a space that is defined by all of the selected dimensions." ></td>
	<td class="line x" title="226:238	The final accuracy turns out to be 95.9%, 70.9%, and 67.5% for POL, MOV, and ELE respectively, which is considerably better than using only the optimal dimension and suggests that the extra dimensions contain useful information." ></td>
	<td class="line x" title="227:238	5 Conclusions Unsupervised clustering algorithms typically group objects along the most prominent dimension, in part owing to their objective of simultaneously maximizing inter-cluster similarity and intra-cluster dissimilarity." ></td>
	<td class="line x" title="228:238	Hence, if the users intended clustering dimension is not the most prominent dimension, these unsupervised clustering algorithms will fail miserably." ></td>
	<td class="line x" title="229:238	To address this problem, we proposed to integrate a novel user feedback mechanism into a spectral clustering algorithm, which allows us to mine the intended, possibly hidden, dimension of the data and produce the desired clustering." ></td>
	<td class="line x" title="230:238	This mechanism differs from competing methods in that it requires very limited feedback: to select the intended dimension, the user only needs to inspect a small number of features." ></td>
	<td class="line x" title="231:238	We demonstrated its viability via a set of human and automatic experiments with unsupervised sentiment classification, obtaining promising results." ></td>
	<td class="line x" title="232:238	In future work, we plan to explore several extensions to our proposed method." ></td>
	<td class="line x" title="233:238	First, we plan to use our user-feedback method in combination with existing methods (e.g., Bekkerman et al.(2007)) for improving its performance." ></td>
	<td class="line x" title="235:238	For instance, instead of having the user construct a relevant feature space from scratch, she can simply extend the set of informative features identified for the user-selected dimension." ></td>
	<td class="line x" title="236:238	Second, since none of the steps in our method is specifically designed for sentiment classification, we plan to apply it to other non-topic-based text classification tasks." ></td>
	<td class="line x" title="237:238	588 Acknowledgments We thank the three anonymous reviewers for their invaluable comments on an earlier draft of the paper." ></td>
	<td class="line x" title="238:238	This work was supported in part by NSF Grant IIS-0812261." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1159
Phrase Dependency Parsing for Opinion Mining
Wu, Yuanbin;Zhang, Qi;Huang, Xuangjing;Wu, Lide;"></td>
	<td class="line x" title="1:218	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 15331541, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:218	c 2009 ACL and AFNLP Phrase Dependency Parsing for Opinion Mining Yuanbin Wu, Qi Zhang, Xuanjing Huang, Lide Wu Fudan University School of Computer Science {ybwu,qi zhang,xjhuang,ldwu}@fudan.edu.cn Abstract In this paper, we present a novel approach for mining opinions from product reviews, where it converts opinion mining task to identify product features, expressions of opinions and relations between them." ></td>
	<td class="line x" title="3:218	By taking advantage of the observation that a lot of product features are phrases, a concept of phrase dependency parsing is introduced, which extends traditional dependency parsing to phrase level." ></td>
	<td class="line x" title="4:218	This concept is then implemented for extracting relations between product features and expressions of opinions." ></td>
	<td class="line x" title="5:218	Experimental evaluations show that the mining task can benefit from phrase dependency parsing." ></td>
	<td class="line x" title="6:218	1 Introduction As millions of users contribute rich information to the Internet everyday, an enormous number of product reviews are freely written in blog pages, Web forums and other consumer-generated mediums (CGMs)." ></td>
	<td class="line x" title="7:218	This vast richness of content becomes increasingly important information source for collecting and tracking customer opinions." ></td>
	<td class="line x" title="8:218	Retrieving this information and analyzing this content are impossible tasks if they were to be manually done." ></td>
	<td class="line x" title="9:218	However, advances in machine learning and natural language processing present us with a unique opportunity to automate the decoding of consumers opinions from online reviews." ></td>
	<td class="line x" title="10:218	Previous works on mining opinions can be divided into two directions: sentiment classification and sentiment related information extraction." ></td>
	<td class="line oc" title="11:218	The former is a task of identifying positive and negative sentiments from a text which can be a passage, a sentence, a phrase and even a word (Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005)." ></td>
	<td class="line x" title="12:218	The latter focuses on extracting the elements composing a sentiment text." ></td>
	<td class="line x" title="13:218	The elements include source of opinions who expresses an opinion (Choi et al., 2005); target of opinions which is a receptor of an opinion (Popescu and Etzioni, 2005); opinion expression which delivers an opinion (Wilson et al., 2005b)." ></td>
	<td class="line x" title="14:218	Some researchers refer this information extraction task as opinion extraction or opinion mining." ></td>
	<td class="line x" title="15:218	Comparing with the former one, opinion mining usually produces richer information." ></td>
	<td class="line x" title="16:218	In this paper, we define an opinion unit as a triple consisting of a product feature, an expression of opinion, and an emotional attitude(positive or negative)." ></td>
	<td class="line x" title="17:218	We use this definition as the basis for our opinion mining task." ></td>
	<td class="line x" title="18:218	Since a product review may refer more than one product feature and express different opinions on each of them, the relation extraction is an important subtask of opinion mining." ></td>
	<td class="line x" title="19:218	Consider the following sentences: 1." ></td>
	<td class="line x" title="20:218	I highly [recommend](1) the Canon SD500(1) to anybody looking for a compact camera that can take [good](2) pictures(2)." ></td>
	<td class="line x" title="21:218	2." ></td>
	<td class="line x" title="22:218	This camera takes [amazing](3) image qualities(3) and its size(4) [cannot be beat](4)." ></td>
	<td class="line x" title="23:218	The phrases underlined are the product features, marked with square brackets are opinion expressions." ></td>
	<td class="line x" title="24:218	Product features and opinion expressions with identical superscript compose a relation." ></td>
	<td class="line x" title="25:218	For the first sentence, an opinion relation exists between the Canon SD500 and recommend, but not between picture and recommend." ></td>
	<td class="line x" title="26:218	The example shows that more than one relation may appear in a sentence, and the correct relations are not simple Cartesian product of opinion expressions and product features." ></td>
	<td class="line x" title="27:218	Simple inspection of the data reveals that product features usually contain more than one word, such as LCD screen, image color, Canon PowerShot SD500, and so on." ></td>
	<td class="line x" title="28:218	An incomplete product feature will confuse the successive analysis." ></td>
	<td class="line x" title="29:218	For example, in passage Image color is dis1533 appointed, the negative sentiment becomes obscure if only image or color is picked out." ></td>
	<td class="line x" title="30:218	Since a product feature could not be represented by a single word, dependency parsing might not be the best approach here unfortunately, which provides dependency relations only between words." ></td>
	<td class="line x" title="31:218	Previous works on relation extraction usually use the head word to represent the whole phrase and extract features from the word level dependency tree." ></td>
	<td class="line x" title="32:218	This solution is problematic because the information provided by the phrase itself can not be used by this kind of methods." ></td>
	<td class="line x" title="33:218	And, experimental results show that relation extraction task can benefit from dependencies within a phrase." ></td>
	<td class="line x" title="34:218	To solve this issue, we introduce the concept of phrase dependency parsing and propose an approach to construct it." ></td>
	<td class="line x" title="35:218	Phrase dependency parsing segments an input sentence into phrases and links segments with directed arcs." ></td>
	<td class="line x" title="36:218	The parsing focuses on the phrases and the relations between them, rather than on the single words inside each phrase." ></td>
	<td class="line x" title="37:218	Because phrase dependency parsing naturally divides the dependencies into local and global, a novel tree kernel method has also been proposed." ></td>
	<td class="line x" title="38:218	The remaining parts of this paper are organized as follows: In Section 2 we discuss our phrase dependency parsing and our approach." ></td>
	<td class="line x" title="39:218	In Section 3, experiments are given to show the improvements." ></td>
	<td class="line x" title="40:218	In Section 4, we present related work and Section 5 concludes the paper." ></td>
	<td class="line x" title="41:218	2 The Approach Fig." ></td>
	<td class="line x" title="42:218	1 gives the architecture overview for our approach, which performs the opinion mining task in three main steps: (1) constructing phrase dependency tree from results of chunking and dependency parsing; (2) extracting candidate product features and candidate opinion expressions; (3) extracting relations between product features and opinion expressions." ></td>
	<td class="line x" title="43:218	2.1 Phrase Dependency Parsing 2.1.1 Overview of Dependency Grammar Dependency grammar is a kind of syntactic theories presented by Lucien Tesni`ere(1959)." ></td>
	<td class="line x" title="44:218	In dependency grammar, structure is determined by the relation between a head and its dependents." ></td>
	<td class="line x" title="45:218	In general, the dependent is a modifier or complement; the head plays a more important role in determining the behaviors of the pair." ></td>
	<td class="line x" title="46:218	Therefore, criPhrase Dependency Parsing  Review Crawler Review Database  ChunkingDependencyParsing  CandidateProduct Features Identification CandidateOpinion Expressions Extraction Relation ExtractionOpinionDatabase Phrase Dependency Tree Figure 1: The architecture of our approach." ></td>
	<td class="line x" title="47:218	teria of how to establish dependency relations and how to distinguish the head and dependent in such relations is central problem for dependency grammar. Fig." ></td>
	<td class="line x" title="48:218	2(a) shows the dependency representation of an example sentence." ></td>
	<td class="line x" title="49:218	The root of the sentence is enjoyed." ></td>
	<td class="line x" title="50:218	There are seven pairs of dependency relationships, depicted by seven arcs from heads to dependents." ></td>
	<td class="line x" title="51:218	2.1.2 Phrase Dependency Parsing Currently, the mainstream of dependency parsing is conducted on lexical elements: relations are built between single words." ></td>
	<td class="line x" title="52:218	A major information loss of this word level dependency tree compared with constituent tree is that it doesnt explicitly provide local structures and syntactic categories (i.e. NP, VP labels) of phrases (Xia and Palmer, 2001)." ></td>
	<td class="line x" title="53:218	On the other hand, dependency tree provides connections between distant words, which are useful in extracting long distance relations." ></td>
	<td class="line x" title="54:218	Therefore, compromising between the two, we extend the dependency tree node with phrases." ></td>
	<td class="line x" title="55:218	That implies a noun phrase Cannon SD500 PowerShot can be a dependent that modifies a verb phrase head really enjoy using with relation type dobj." ></td>
	<td class="line x" title="56:218	The feasibility behind is that a phrase is a syntactic unit regardless of the length or syntactic category (Santorini and Kroch, 2007), and it is acceptable to substitute a single word by a phrase with same syntactic category in a sentence." ></td>
	<td class="line x" title="57:218	Formally, we define the dependency parsing with phrase nodes as phrase dependency parsing." ></td>
	<td class="line x" title="58:218	A dependency relationship which is an asymmetric binary relationship holds between two phrases." ></td>
	<td class="line x" title="59:218	One is called head, which is the central phrase in the relation." ></td>
	<td class="line x" title="60:218	The other phrase is called dependent, which modifies the head." ></td>
	<td class="line x" title="61:218	A label representing the 1534 enjoyed We nsubjreallyadvmod usingpartmod SD500 thedet CanonPowerShotnn nn dobj enjoyed nsubj really using partmod We VP NP SD500 thedet CanonPowerShotnn nn NP advmod dobj (a) (c)(b)   NP SEGMENT:      [We]  VP SEGMENT:      [really]       [enjoyed ]      [using]  NP SEGMENT:      [the]       [Canon]      [PowerShot]       [SD500] Figure 2: Example of Phrase Dependency Parsing." ></td>
	<td class="line x" title="62:218	relation type is assigned to each dependency relationship, such as subj (subject), obj (object), and so on." ></td>
	<td class="line x" title="63:218	Fig.2(c) shows an example of phrase dependency parsing result." ></td>
	<td class="line x" title="64:218	By comparing the phrase dependency tree and the word level dependency tree in Fig.2, the former delivers a more succinct tree structure." ></td>
	<td class="line x" title="65:218	Local words in same phrase are compacted into a single node." ></td>
	<td class="line x" title="66:218	These words provide local syntactic and semantic effects which enrich the phrase they belong to." ></td>
	<td class="line x" title="67:218	But they should have limited influences on the global tree topology, especially in applications which emphasis the whole tree structures, such as tree kernels." ></td>
	<td class="line x" title="68:218	Pruning away local dependency relations by additional phrase structure information, phrase dependency parsing accelerates following processing of opinion relation extraction . T construct phrase dependency tree, we propose a method which combines results from an existing shallow parser and a lexical dependency parser." ></td>
	<td class="line x" title="69:218	A phrase dependency tree is defined as T = (V,E), where V is the set of phrases, E is the dependency relations among the phrases in V representing by direct edges." ></td>
	<td class="line x" title="70:218	To reserve the word level dependencies inside a phrase, we define a nested structure for a phrase Ti in V : Ti = (Vi,Ei)." ></td>
	<td class="line x" title="71:218	Vi = {v1,v2, ,vm} is the internal words, Ei is the internal dependency relations." ></td>
	<td class="line x" title="72:218	We conduct the phrase dependency parsing in this way: traverses word level dependency tree in preorder (visits root node first, then traverses the children recursively)." ></td>
	<td class="line x" title="73:218	When visits a node R, searches in its children and finds the node set D which are in the same phrase with R according Algorithm 1 Pseudo-Code for constructing the phrase dependency tree INPUT: Tprime = (Vprime,Eprime) a word level dependency tree P = phrases OUTPUT: phrase dependency tree T = (V,E) where V = {T1(V1,E1),T2(V2,E2), ,Tn(Vn,En)} Initialize: V {({vprime},{})|vprime  Vprime} E {(Ti,Tj)|(vprimei,vprimej)  Eprime,vprimei  Vi,vprimej  Vj} R = (Vr,Er) root of T PhraseDPTree(R, P) 1: Find pi  P where word[R]  pi 2: for each S = (Vs,Es),(R,S) E do 3: if word[S]  pi then 4: Vr  Vr vs;vs  Vs 5: Er  Er (vr,root[S]);vr  Vr 6: V V S 7: E E +(R,l);(S,l) E 8: E E (R,S) 9: end if 10: end for 11: for each (R,S) E do 12: PhraseDPTree(S,P) 13: end for 14: return (V,E) to the shallow parsing result." ></td>
	<td class="line x" title="74:218	Compacts D and R into a single node." ></td>
	<td class="line x" title="75:218	Then traverses all the remaining children in the same way." ></td>
	<td class="line x" title="76:218	The algorithm is shown in Alg." ></td>
	<td class="line x" title="77:218	1." ></td>
	<td class="line x" title="78:218	The output of the algorithm is still a tree, for we only cut edges which are compacted into a phrase, the connectivity is keeped." ></td>
	<td class="line x" title="79:218	Note that there will be inevitable disagrees between shallow parser and lexical dependency parser, the algorithm implies that we simply follow the result of the latter one: the phrases from shallow parser will not appear in the final result if they cannot be found in the procedure." ></td>
	<td class="line x" title="80:218	Consider the following example: We really enjoyed using the Canon PowerShot SD500. Fig.2 shows the procedure of phrase dependency parsing." ></td>
	<td class="line x" title="81:218	Fig.2(a) is the result of the lexical dependency parser." ></td>
	<td class="line x" title="82:218	Shallow parsers result is shown in Fig.2(b)." ></td>
	<td class="line x" title="83:218	Chunk phrases NP(We), VP(really enjoyed using) and NP(the Canon PowerShot SD500) are nodes in the output phrase dependency tree." ></td>
	<td class="line x" title="84:218	When visiting node enjoyed in Fig.2(a), the shallow parser tells that really and using which are children of enjoy are in the same phrase with their parent, then the three nodes are packed." ></td>
	<td class="line x" title="85:218	The final phrase dependency parsing tree is shown in the Fig." ></td>
	<td class="line x" title="86:218	2(c)." ></td>
	<td class="line x" title="87:218	1535 2.2 Candidate Product Features and Opinion Expressions Extraction In this work, we define that product features are products, product parts, properties of products, properties of parts, company names and related objects." ></td>
	<td class="line x" title="88:218	For example,in consumer electronic domain, Canon PowerShot, image quality,camera, laptop are all product features." ></td>
	<td class="line x" title="89:218	From analyzing the labeled corpus, we observe that more than 98% of product features are in a single phrase, which is either noun phrase (NP) or verb phrase (VP)." ></td>
	<td class="line x" title="90:218	Based on it, all NPs and VPs are selected as candidate product features." ></td>
	<td class="line x" title="91:218	While prepositional phrases (PPs) and adjectival phrases (ADJPs) are excluded." ></td>
	<td class="line x" title="92:218	Although it can cover nearly all the true product features, the precision is relatively low." ></td>
	<td class="line x" title="93:218	The large amount of noise candidates may confuse the relation extraction classifier." ></td>
	<td class="line x" title="94:218	To shrink the size of candidate set, we introduce language model by an intuition that the more likely a phrase to be a product feature, the more closely it related to the product review." ></td>
	<td class="line x" title="95:218	In practice, for a certain domain of product reviews, a language model is build on easily acquired unlabeled data." ></td>
	<td class="line x" title="96:218	Each candidate NP or VP chunk in the output of shallow parser is scored by the model, and cut off if its score is less than a threshold." ></td>
	<td class="line x" title="97:218	Opinion expressions are spans of text that express a comment or attitude of the opinion holder, which are usually evaluative or subjective phrases." ></td>
	<td class="line x" title="98:218	We also analyze the labeled corpus for opinion expressions and observe that many opinion expressions are used in multiple domains, which is identical with the conclusion presented by Kobayashi et al.(2007)." ></td>
	<td class="line x" title="100:218	They collected 5,550 opinion expressions from various sources . The coverage of the dictionary is high in multiple domains." ></td>
	<td class="line x" title="101:218	Motivated by those observations, we use a dictionary which contains 8221 opinion expressions to select candidates (Wilson et al., 2005b)." ></td>
	<td class="line x" title="102:218	An assumption we use to filter candidate opinion expressions is that opinion expressions tend to appear closely with product features, which is also used to extract product features by Hu and Liu (2004)." ></td>
	<td class="line x" title="103:218	In our experiments, the tree distance between product feature and opinion expression in a relation should be less than 5 in the phrase dependency parsing tree." ></td>
	<td class="line x" title="104:218	2.3 Relation Extraction This section describes our method on extracting relations between opinion expressions and product features using phrase dependency tree." ></td>
	<td class="line x" title="105:218	Manually built patterns were used in previous works which have an obvious drawback that those patterns can hardly cover all possible situations." ></td>
	<td class="line x" title="106:218	By taking advantage of the kernel methods which can search a feature space much larger than that could be represented by a feature extraction-based approach, we define a new tree kernel over phrase dependency trees and incorporate this kernel within an SVM to extract relations between opinion expressions and product features." ></td>
	<td class="line x" title="107:218	The potential relation set consists of the all combinations between candidate product features and candidate opinion expressions in a sentence." ></td>
	<td class="line x" title="108:218	Given a phrase dependency parsing tree, we choose the subtree rooted at the lowest common parent(LCP) of opinion expression and product feature to represent the relation." ></td>
	<td class="line x" title="109:218	Dependency tree kernels has been proposed by (Culotta and Sorensen, 2004)." ></td>
	<td class="line x" title="110:218	Their kernel is defined on lexical dependency tree by the convolution of similarities between all possible subtrees." ></td>
	<td class="line x" title="111:218	However, if the convolution containing too many irrelevant subtrees, over-fitting may occur and decreases the performance of the classifier." ></td>
	<td class="line x" title="112:218	In phrase dependency tree, local words in a same phrase are compacted, therefore it provides a way to treat local dependencies and global dependencies differently (Fig." ></td>
	<td class="line x" title="113:218	3)." ></td>
	<td class="line x" title="114:218	As a consequence, these two kinds of dependencies will not disturb each other in measuring similarity." ></td>
	<td class="line x" title="115:218	Later experiments prove the validity of this statement." ></td>
	<td class="line x" title="116:218	B A C D E B A C D E Phrase Local dependencies Global dependencies Figure 3: Example of local dependencies and global dependencies." ></td>
	<td class="line x" title="117:218	We generalize the definition by (Culotta and Sorensen, 2004) to fit the phrase dependency tree." ></td>
	<td class="line x" title="118:218	Use the symbols in Section 2.1.2, T i and T j are two trees with root Ri and Rj, K(T i,T j) is the kernel function for them." ></td>
	<td class="line x" title="119:218	Firstly, each tree node Tk  T i is augmented with a set of features F, and an instance of F for Tk is Fk = {fk}." ></td>
	<td class="line x" title="120:218	A match function m(Ti,Tj) is defined on comparing a subset of nodes features M  F. And in the same way, a similarity function s(Ti,Tj) are de1536 fined on S  F m(Ti,Tj) = braceleftBigg 1 if fim = fjm fm  M 0 otherwise (1) and s(Ti,Tj) = summationdisplay fsS C(fis,fjs) (2) where C(fis,fjs) = braceleftBigg 1 if fis = fjs 0 otherwise (3) For the given phrase dependency parsing trees, the kernel function K(T i,T j) is defined as follow: K(T i,T j) =    0 if m(Ri,Rj) = 0 s(Ri,Rj)+Kin(Ri,Rj) +Kc(Ri.C,Rj.C) otherwise (4) where Kin(Ri,Rj) is a kernel function over Ri = (V ir,Eir) and Rj = (V jr ,Ejr)s internal phrase structures, Kin(Ri,Rj) = K(Ri,Rj) (5) Kc is the kernel function over Ri and Rjs children." ></td>
	<td class="line x" title="121:218	Denote a is a continuous subsequence of indices a,a+1,a+l(a) for Ris children where l(a) is its length, as is the s-th element in a. And likewise b for Rj." ></td>
	<td class="line x" title="122:218	Kc(Ri.C,Rj.C) =summationtext a,b,l(a)=l(b)  l(a)K(Ri.[a],Rj.[b]) producttexts=1l(a) m(Ri.[as],Rj.[bs]) (6) where the constant 0 <  < 1 normalizes the effects of children subsequences length." ></td>
	<td class="line x" title="123:218	Compared with the definitions in (Culotta and Sorensen, 2004), we add term Kin to handle the internal nodes of a pharse, and make this extension still satisfy the kernel function requirements (composition of kernels is still a kernel (Joachims et al., 2001))." ></td>
	<td class="line x" title="124:218	The consideration is that the local words should have limited effects on whole tree structures." ></td>
	<td class="line x" title="125:218	So the kernel is defined on external children (Kc) and internal nodes (Kin) separately, Table 1: Statistics for the annotated corpus Category # Products # Sentences Cell Phone 2 1100 Diaper 1 375 Digital Camera 4 1470 DVD Player 1 740 MP3 Player 3 3258 as the result, the local words are not involved in subsequences of external children for Kc." ></td>
	<td class="line x" title="126:218	After the kernel computing through training instances, support vector machine (SVM) is used for classification." ></td>
	<td class="line x" title="127:218	3 Experiments and Results In this section, we describe the annotated corpus and experiment configurations including baseline methods and our results on in-domain and crossdomain." ></td>
	<td class="line x" title="128:218	3.1 Corpus We conducted experiments with labeled corpus which are selected from Hu and Liu (2004), Jindal and Liu (2008) have built." ></td>
	<td class="line x" title="129:218	Their documents are collected from Amazon.com and CNet.com, where products have a large number of reviews." ></td>
	<td class="line x" title="130:218	They also manually labeled product features and polarity orientations." ></td>
	<td class="line x" title="131:218	Our corpus is selected from them, which contains customer reviews of 11 products belong to 5 categories(Diaper, Cell Phone, Digital Camera, DVD Player, and MP3 Player)." ></td>
	<td class="line x" title="132:218	Table 1 gives the detail statistics." ></td>
	<td class="line x" title="133:218	Since we need to evaluate not only the product features but also the opinion expressions and relations between them, we asked two annotators to annotate them independently." ></td>
	<td class="line x" title="134:218	The annotators started from identifying product features." ></td>
	<td class="line x" title="135:218	Then for each product feature, they annotated the opinion expression which has relation with it." ></td>
	<td class="line x" title="136:218	Finally, one annotator A1 extracted 3595 relations, while the other annotator A2 extracted 3745 relations, and 3217 cases of them matched." ></td>
	<td class="line x" title="137:218	In order to measure the annotation quality, we use the following metric to measure the inter-annotator agreement, which is also used by Wiebe et al.(2005)." ></td>
	<td class="line x" title="139:218	agr(a||b) = |A matches B||A| 1537 Table 2: Results for extracting product features and opinion expressions P R F Product Feature 42.8% 85.5% 57.0% Opinion Expression 52.5% 75.2% 61.8% Table 3: Features used in SVM-1: o denotes an opinion expression and t a product feature 1) Positions of o/t in sentence(start, end, other); 2) The distance between o and t (1, 2, 3, 4, other); 3) Whether o and t have direct dependency relation; 4) Whether o precedes t; 5) POS-Tags of o/t. where agr(a||b) represents the inter-annotator agreement between annotator a and b, A and B are the sets of anchors annotated by annotators a and b. agr(A1||A2) was 85.9% and agr(A2||A1) was 89.5%." ></td>
	<td class="line x" title="140:218	It indicates that the reliability of our annotated corpus is satisfactory." ></td>
	<td class="line x" title="141:218	3.2 Preprocessing Results Results of extracting product features and opinion expressions are shown in Table 2." ></td>
	<td class="line x" title="142:218	We use precision, recall and F-measure to evaluate performances." ></td>
	<td class="line x" title="143:218	The candidate product features are extracted by the method described in Section 2.2, whose result is in the first row." ></td>
	<td class="line x" title="144:218	6760 of 24414 candidate product features remained after the filtering, which means we cut 72% of irrelevant candidates with a cost of 14.5%(1-85.5%) loss in true answers." ></td>
	<td class="line x" title="145:218	Similar to the product feature extraction, the precision of extracting opinion expression is relatively low, while the recall is 75.2%." ></td>
	<td class="line x" title="146:218	Since both product features and opinion expressions extractions are preprocessing steps, recall is more important." ></td>
	<td class="line x" title="147:218	3.3 Relation Extraction Experiments 3.3.1 Experiments Settings In order to compare with state-of-the-art results, we also evaluated the following methods." ></td>
	<td class="line x" title="148:218	1." ></td>
	<td class="line x" title="149:218	Adjacent method extracts relations between a product feature and its nearest opinion expression, which is also used in (Hu and Liu, 2004)." ></td>
	<td class="line x" title="150:218	2." ></td>
	<td class="line x" title="151:218	SVM-1." ></td>
	<td class="line x" title="152:218	To compare with tree kernel based Table 4: Features used in SVM-PTree Features for match function 1) The syntactic category of the tree node (e.g. NP, VP, PP, ADJP)." ></td>
	<td class="line x" title="153:218	2) Whether it is an opinion expression node 3) Whether it is a product future node." ></td>
	<td class="line x" title="154:218	Features for similarity function 1) The syntactic category of the tree node (e.g. NP, VP, PP, ADJP)." ></td>
	<td class="line x" title="155:218	2) POS-Tag of the head word of nodes internal phrases." ></td>
	<td class="line x" title="156:218	3) The type of phrase dependency edge linking to nodes parent." ></td>
	<td class="line x" title="157:218	4) Feature 2) for the nodes parent 5) Feature 3) for the nodes parent approaches, we evaluated an SVM1 result with a set of manually selected features(Table 3), which are also used in (Kobayashi et al., 2007)." ></td>
	<td class="line x" title="158:218	3." ></td>
	<td class="line x" title="159:218	SVM-2 is designed to compare the effectiveness of cross-domain performances." ></td>
	<td class="line x" title="160:218	The features used are simple bag of words and POS-Tags between opinion expressions and product features." ></td>
	<td class="line x" title="161:218	4." ></td>
	<td class="line x" title="162:218	SVM-WTree uses head words of opinion expressions and product features in the word-level dependency tree, as the previous works in information extraction." ></td>
	<td class="line x" title="163:218	Then conducts tree kernel proposed by Culotta and Sorensen (2004)." ></td>
	<td class="line x" title="164:218	5." ></td>
	<td class="line x" title="165:218	SVM-PTree denotes the results of our treekernel based SVM, which is described in the Section 2.3." ></td>
	<td class="line x" title="166:218	Stanford parser (Klein and Manning, 2002) and Sundance (Riloff and Phillips, 2004) are used as lexical dependency parser and shallow parser." ></td>
	<td class="line x" title="167:218	The features in match function and similarity function are shown in Table 4." ></td>
	<td class="line x" title="168:218	6. OERight is the result of SVM-PTree with correct opinion expressions." ></td>
	<td class="line x" title="169:218	7." ></td>
	<td class="line x" title="170:218	PFRight is the result of SVM-PTree with correct product features." ></td>
	<td class="line x" title="171:218	Table 5 shows the performances of different relation extraction methods with in-domain data." ></td>
	<td class="line x" title="172:218	For each domain, we conducted 5-fold cross validation." ></td>
	<td class="line x" title="173:218	Table 6 shows the performances of the extraction methods on cross-domain data." ></td>
	<td class="line x" title="174:218	We use the digital camera and cell phone domain as training set." ></td>
	<td class="line x" title="175:218	The other domains are used as testing set." ></td>
	<td class="line x" title="176:218	1libsvm 2.88 is used in our experiments 1538 Table 5: Results of different methods Cell Phone MP3 Player Digital Camera DVD Player Diaper Methods P R F P R F P R F P R F P R F Adjacent 40.3% 60.5% 48.4% 26.5% 59.3% 36.7% 32.7% 59.1% 42.1% 31.8% 68.4% 43.4% 23.4% 78.8% 36.1% SVM-1 69.5% 42.3% 52.6% 60.7% 30.6% 40.7% 61.4% 32.4% 42.4% 56.0% 27.6% 37.0% 29.3% 14.1% 19.0% SVM-2 60.7% 19.7% 29.7% 63.6% 23.8% 34.6% 66.9% 23.3% 34.6% 66.7% 13.2% 22.0% 79.2% 22.4% 34.9% SVM-WTree 52.6% 52.7% 52.6% 46.4% 43.8% 45.1% 49.1% 46.0% 47.5% 35.9% 32.0% 33.8% 36.6% 31.7% 34.0% SVM-PTree 55.6% 57.2% 56.4% 51.7% 50.7% 51.2% 54.0% 49.9% 51.9% 37.1% 35.4% 36.2% 37.3% 30.5% 33.6% OERight 66.7% 69.5% 68.1% 65.6% 65.9% 65.7% 64.3% 61.0% 62.6% 59.9% 63.9% 61.8% 55.8% 58.5% 57.1% PFRight 62.8% 62.1% 62.4% 61.3% 56.8% 59.0% 59.7% 56.2% 57.9% 46.9% 46.6% 46.7% 58.5% 51.3% 53.4% Table 6: Results for total performance with cross domain training data Diaper DVD Player MP3 Player Methods P R F P R F P R F Adjacent 23.4% 78.8% 36.1% 31.8% 68.4% 43.4% 26.5% 59.3% 36.7% SVM-1 22.4% 30.6% 25.9% 52.8% 30.9% 39.0% 55.9% 36.8% 44.4% SVM-2 71.9% 15.1% 25.0% 51.2% 13.2% 21.0% 63.1% 22.0% 32.6% SVM-WTree 38.7% 52.4% 44.5% 30.7% 59.2% 40.4% 38.1% 47.2% 42.2% SVM-PTree 37.3% 53.7% 44.0% 59.2% 48.3% 46.3% 43.0% 48.9% 45.8% 3.3.2 Results Discussion Table 5 presents different methods results in five domains." ></td>
	<td class="line x" title="177:218	We observe that the three learning based methods(SVM-1, SVM-WTree, SVM-PTree) perform better than the Adjacent baseline in the first three domains." ></td>
	<td class="line x" title="178:218	However, in other domains, directly adjacent method is better than the learning based methods." ></td>
	<td class="line x" title="179:218	The main difference between the first three domains and the last two domains is the size of data(Table 1)." ></td>
	<td class="line x" title="180:218	It implies that the simple Adjacent method is also competent when the training set is small." ></td>
	<td class="line x" title="181:218	A further inspection into the result of first 3 domains, we can also conclude that: 1) Tree kernels(SVM-WTree and SVM-PTree) are better than Adjacent, SVM-1 and SVM-2 in all domains." ></td>
	<td class="line x" title="182:218	It proofs that the dependency tree is important in the opinion relation extraction." ></td>
	<td class="line x" title="183:218	The reason for that is a connection between an opinion and its target can be discovered with various syntactic structures." ></td>
	<td class="line x" title="184:218	2) The kernel defined on phrase dependency tree (SVM-PTree) outperforms kernel defined on word level dependency tree(SVMWTree) by 4.8% in average." ></td>
	<td class="line x" title="185:218	We believe the main reason is that phrase dependency tree provides a more succinct tree structure, and the separative treatment of local dependencies and global dependencies in kernel computation can indeed improve the performance of relation extraction." ></td>
	<td class="line x" title="186:218	To analysis the results of preprocessing steps influences on the following relation extraction, we provide 2 additional experiments which the product features and opinion expressions are all correctly extracted respectively: OERight and PFRight." ></td>
	<td class="line x" title="187:218	These two results show that given an exactly extraction of opinion expression and product feature, the results of opinion relation extraction will be much better." ></td>
	<td class="line x" title="188:218	Further, opinion expressions are more influential which naturally means the opinion expressions are crucial in opinion relation extraction." ></td>
	<td class="line x" title="189:218	For evaluations on cross domain, the Adjacent method doesnt need training data, its results are the same as the in-domain experiments." ></td>
	<td class="line x" title="190:218	Note in Table 3 and Table 4, we dont use domain related features in SVM-1, SVM-WTree, SVMPTree, but SVM-2s features are domain dependent." ></td>
	<td class="line x" title="191:218	Since the cross-domain training set is larger than the original one in Diaper and DVD domain, the models are trained more sufficiently." ></td>
	<td class="line x" title="192:218	The final results on cross-domain are even better than in-domain experiments on SVM-1, SVM-WTree, and SVM-PTree with percentage of 4.6%, 8.6%, 10.3% in average." ></td>
	<td class="line x" title="193:218	And the cross-domain training set is smaller than in-domain in MP3, but it also achieve competitive performance with the 1539 in-domain." ></td>
	<td class="line x" title="194:218	On the other hand, SVM-2s result decreased compared with the in-domain experiments because the test domain changed." ></td>
	<td class="line x" title="195:218	At the same time, SVM-PTree outperforms other methods which is similar in in-domain experiments." ></td>
	<td class="line x" title="196:218	4 Related Work Opinion mining has recently received considerable attention." ></td>
	<td class="line oc" title="197:218	Amount of works have been done on sentimental classification in different levels (Zhang et al., 2009; Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005)." ></td>
	<td class="line x" title="198:218	While we focus on extracting product features, opinion expressions and mining relations in this paper." ></td>
	<td class="line x" title="199:218	Kobayashi et al.(2007) presented their work on extracting opinion units including: opinion holder, subject, aspect and evaluation." ></td>
	<td class="line x" title="201:218	Subject and aspect belong to product features, while evaluation is the opinion expression in our work." ></td>
	<td class="line x" title="202:218	They converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which combines contextual clues and statistical clues." ></td>
	<td class="line x" title="203:218	Their experimental results showed that the model using contextual clues improved the performance." ></td>
	<td class="line x" title="204:218	However since the contextual information in a domain is specific, the model got by their approach can not easily converted to other domains." ></td>
	<td class="line x" title="205:218	Choi et al.(2006) used an integer linear programming approach to jointly extract entities and relations in the context of opinion oriented information extraction." ></td>
	<td class="line x" title="207:218	They identified expressions of opinions, sources of opinions and the linking relation that exists between them." ></td>
	<td class="line x" title="208:218	The sources of opinions denote to the person or entity that holds the opinion." ></td>
	<td class="line x" title="209:218	Another area related to our work is opinion expressions identification (Wilson et al., 2005a; Breck et al., 2007)." ></td>
	<td class="line x" title="210:218	They worked on identifying the words and phrases that express opinions in text." ></td>
	<td class="line x" title="211:218	According to Wiebe et al.(2005), there are two types of opinion expressions, direct subjective expressions and expressive subjective elements." ></td>
	<td class="line x" title="213:218	5 Conclusions In this paper, we described our work on mining opinions from unstructured documents." ></td>
	<td class="line x" title="214:218	We focused on extracting relations between product features and opinion expressions." ></td>
	<td class="line x" title="215:218	The novelties of our work included: 1) we defined the phrase dependency parsing and proposed an approach to construct the phrase dependency trees; 2) we proposed a new tree kernel function to model the phrase dependency trees." ></td>
	<td class="line x" title="216:218	Experimental results show that our approach improved the performances of the mining task." ></td>
	<td class="line x" title="217:218	6 Acknowledgement This work was (partially) funded by Chinese NSF 60673038, Doctoral Fund of Ministry of Education of China 200802460066, and Shanghai Science and Technology Development Funds 08511500302." ></td>
	<td class="line x" title="218:218	The authors would like to thank the reviewers for their useful comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E09-1046
Generating a Non-English Subjectivity Lexicon: Relations That Matter
Jijkoun, Valentin;Hofmann, Katja;"></td>
	<td class="line x" title="1:182	Proceedings of the 12th Conference of the European Chapter of the ACL, pages 398405, Athens, Greece, 30 March  3 April 2009." ></td>
	<td class="line x" title="2:182	c2009 Association for Computational Linguistics Generating a Non-English Subjectivity Lexicon: Relations That Matter Valentin Jijkoun and Katja Hofmann ISLA, University of Amsterdam Amsterdam, The Netherlands {jijkoun,k.hofmann}@uva.nl Abstract We describe a method for creating a nonEnglish subjectivity lexicon based on an English lexicon, an online translation service and a general purpose thesaurus: Wordnet." ></td>
	<td class="line x" title="3:182	We use a PageRank-like algorithm to bootstrap from the translation of the English lexicon and rank the words in the thesaurus by polarity using the network of lexical relations in Wordnet." ></td>
	<td class="line x" title="4:182	We apply our method to the Dutch language." ></td>
	<td class="line x" title="5:182	The best results are achieved when using synonymy and antonymy relations only, and ranking positive and negative words simultaneously." ></td>
	<td class="line x" title="6:182	Our method achieves an accuracy of 0.82 at the top 3,000 negative words, and 0.62 at the top 3,000 positive words." ></td>
	<td class="line x" title="7:182	1 Introduction One of the key tasks in subjectivity analysis is the automatic detection of subjective (as opposed to objective, factual) statements in written documents (Mihalcea and Liu, 2006)." ></td>
	<td class="line x" title="8:182	This task is essential for applications such as online marketing research, where companies want to know what customers say about the companies, their products,specificproductsfeatures,andwhethercomments made are positive or negative." ></td>
	<td class="line x" title="9:182	Another application is in political research, where public opinion could be assessed by analyzing usergenerated online data (blogs, discussion forums, etc.)." ></td>
	<td class="line x" title="10:182	Most current methods for subjectivity identification rely on subjectivity lexicons, which list words that are usually associated with positive or negative sentiments or opinions (i.e., words with polarity)." ></td>
	<td class="line oc" title="11:182	Such a lexicon can be used, e.g., to classify individual sentences or phrases as subjective or not, and as bearing positive or negative sentiments (Pang et al., 2002; Kim and Hovy, 2004; Wilson et al., 2005a)." ></td>
	<td class="line x" title="12:182	For English, manually created subjectivity lexicons have been available for a while, but for many other languages such resources are still missing." ></td>
	<td class="line x" title="13:182	We describe a language-independent method for automatically bootstrapping a subjectivity lexicon, and apply and evaluate it for the Dutch language." ></td>
	<td class="line x" title="14:182	The method starts with an English lexicon of positive and negative words, automatically translated into the target language (Dutch in our case)." ></td>
	<td class="line x" title="15:182	A PageRank-like algorithm is applied to the Dutch wordnet in order to filter and expand the set of words obtained through translation." ></td>
	<td class="line x" title="16:182	The Dutch lexicon is then created from the resulting ranking ofthewordnetnodes." ></td>
	<td class="line x" title="17:182	Ourmethodhasseveralbenefits:  It is applicable to any language for which a wordnet and an automatic translation service or a machine-readable dictionary (from English) are available." ></td>
	<td class="line x" title="18:182	For example, the EuroWordnet project (Vossen, 1998), e.g., provides wordnets for 7 languages, and free online translation services such as the one we have used in this paper are available for many other languages as well." ></td>
	<td class="line x" title="19:182	 Themethodranksall(oralmostall)entriesof a wordnet by polarity (positive or negative), which makes it possible to experiment with different settings of the precision/coverage threshold in applications that use the lexicon." ></td>
	<td class="line x" title="20:182	We apply our method to the most recent version of Cornetto (Vossen et al., 2007), an extension of the Dutch WordNet, and we experiment with various parameters of the algorithm, in order to arrive at a good setting for porting the method to other languages." ></td>
	<td class="line x" title="21:182	Specifically, we evaluate the quality of the resulting Dutch subjectivity lexicon using different subsets of wordnet relations and information in the glosses (definitions)." ></td>
	<td class="line x" title="22:182	We also examine 398 the effect of the number of iterations on the performance of our method." ></td>
	<td class="line x" title="23:182	We find that best performance is achieved when using only synonymy and antonymy relations and, moreover, the algorithm converges after about 10 iterations." ></td>
	<td class="line x" title="24:182	The remainder of the paper is organized as follows." ></td>
	<td class="line x" title="25:182	We summarize related work in section 2, present our method in section 3 and describe the manual assessment of the lexicon in section 4." ></td>
	<td class="line x" title="26:182	We discuss experimental results in section 5 and conclude in section 6." ></td>
	<td class="line x" title="27:182	2 Related work Creating subjectivity lexicons for languages other than English has only recently attracted attention oftheresearchcommunity." ></td>
	<td class="line x" title="28:182	(Mihalceaetal., 2007) describes experiments with subjectivity classification for Romanian." ></td>
	<td class="line x" title="29:182	The authors start with an Englishsubjectivitylexiconwith6,856entries,OpinionFinder (Wiebe and Riloff, 2005), and automatically translate it into Romanian using two bilingual dictionaries, obtaining a Romanian lexicon with 4,983 entries." ></td>
	<td class="line x" title="30:182	A manual evaluation of a sample of 123 entries of this lexicon showed that 50% of the entries do indicate subjectivity." ></td>
	<td class="line x" title="31:182	In (Banea et al., 2008) a different approach based on boostrapping was explored for Romanian." ></td>
	<td class="line x" title="32:182	The method starts with a small seed set of 60 words, which is iteratively (1) expanded by adding synonyms from an online Romanian dictionary, and (2) filtered by removing words which are not similar (at a preset threshold) to the original seed, according to an LSA-based similarity measure computed on a half-million word corpus of Romanian." ></td>
	<td class="line x" title="33:182	The lexicon obtained after 5 iterations of the method was used for sentencelevel sentiment classification, indicating an 18% improvement over the lexicon of (Mihalcea et al., 2007)." ></td>
	<td class="line x" title="34:182	Both these approaches produce unordered sets of positive and negative words." ></td>
	<td class="line x" title="35:182	Our method, on the other hand, assigns polarity scores to words and produces a ranking of words by polarity, which provides a more flexible experimental framework for applications that will use the lexicon." ></td>
	<td class="line x" title="36:182	Esuli and Sebastiani (Esuli and Sebastiani, 2007) apply an algorithm based on PageRank to ranksynsetsinEnglishWordNetaccordingtopositive and negativite sentiments." ></td>
	<td class="line x" title="37:182	The authors view WordNet as a graph where nodes are synsets and synsets are linked with the synsets of terms used in their glosses (definitions)." ></td>
	<td class="line x" title="38:182	The algorithm is initialized with positivity/negativity scores provided in SentiWordNet (Esuli and Sebastiani, 2006), an English sentiment lexicon." ></td>
	<td class="line x" title="39:182	The weights are then distributed through the graph using an the algorithm similar to PageRank." ></td>
	<td class="line x" title="40:182	Authors conclude that larger initial seed sets result in a better ranking produced by the method." ></td>
	<td class="line x" title="41:182	The algorithm is always run twice, once for positivity scores, and once for negativity scores; this is different in our approach, which ranks words from negative to positive in one run." ></td>
	<td class="line x" title="42:182	See section 5.4 for a more detailed comparison between the existing approaches outlined above and our approach." ></td>
	<td class="line x" title="43:182	3 Approach Our approach extends the techniques used in (Esuli and Sebastiani, 2007; Banea et al., 2008) forminingEnglishandRomaniansubjectivitylexicons." ></td>
	<td class="line x" title="44:182	3.1 Boostrapping algorithm We hypothesize that concepts (synsets) that are closely related in a wordnet have similar meaning andthussimilarpolarity." ></td>
	<td class="line x" title="45:182	Todeterminerelatedness between concepts, we view a wordnet as a graph of lexical relations between words and synsets:  nodes correspond to lexical units (words) and synsets; and  directed arcs correspond to relations between synsets (hyponymy, meronymy, etc.) and between synsets and words they contain; in one of our experiments, following (Esuli and Sebastiani, 2007), we also include relations betweensynsetsandallwordsthatoccurintheir glosses (definitions)." ></td>
	<td class="line x" title="46:182	Nodes and arcs of such a graph are assigned weights, which are then propagated through the graph by iteratively applying a PageRank-like algorithm." ></td>
	<td class="line x" title="47:182	Initially, weights are assigned to nodes and arcs inthegraphusingtranslationsfromanEnglishpolarity lexicon as follows:  words that are translations of the positive words from the English lexicon are assigned a weight of 1, words that are translations of the negative words are initialized to -1; in general, weight of a word indicates its polarity; 399  All arcs are assigned a weight of 1, except for antonymy relations which are assigned a weight of -1; the intuition behind the arc weights is simple: arcs with weight 1 would usually connect synsets of the same (or similar) polarity, while arcs with weight -1 would connect synsets with opposite polarities." ></td>
	<td class="line x" title="48:182	We use the following notation." ></td>
	<td class="line x" title="49:182	Our algorithm is iterative and k = 0,1, denotes an iteration." ></td>
	<td class="line x" title="50:182	Let aki be the weight of the node i at the k-th iteration." ></td>
	<td class="line x" title="51:182	Let wjm be the weight of the arc that connectsnodej withnodem;weassumetheweightis 0 if the arc does not exist." ></td>
	<td class="line x" title="52:182	Finally,  is a damping factor of the PageRank algorithm, set to 0.8." ></td>
	<td class="line x" title="53:182	This factor balances the impact of the initial weight of a node with the impact of weight received through connections to other nodes." ></td>
	<td class="line x" title="54:182	Thealgorithmproceedsbyupdatingtheweights of nodes iteratively as follows: ak+1i =   summationdisplay j akj wjisummationtext m |wjm| +(1)a0i Furthermore, at each iterarion, all weights ak+1i are normalized by maxj |ak+1j |." ></td>
	<td class="line x" title="55:182	The equation above is a straightforward extension of the PageRank method for the case when arcs of the graph are weighted." ></td>
	<td class="line x" title="56:182	Nodes propagate theirpolaritymasstoneighboursthroughoutgoing arcs." ></td>
	<td class="line x" title="57:182	The mass transferred depends on the weight of the arcs." ></td>
	<td class="line x" title="58:182	Note that for arcs with negative weight (in our case, antonymy relation), the polarity of transferred mass is inverted: i.e., synsets with negative polarity will enforce positive polarity in their antonyms." ></td>
	<td class="line x" title="59:182	We iterate the algorithm and read off the resulting weight of the word nodes." ></td>
	<td class="line x" title="60:182	We assume words with the lowest resulting weight to have negative polarity, and word nodes with the highest weight positive polarity." ></td>
	<td class="line x" title="61:182	The output of the algorithm is a list of words ordered by polarity score." ></td>
	<td class="line x" title="62:182	3.2 Resources used WeuseanEnglishsubjectivitylexiconofOpinionFinder (Wilson et al., 2005b) as the starting point ofourmethod." ></td>
	<td class="line x" title="63:182	Thelexiconcontains2,718English words with positive polarity and 4,910 words with negative polarity." ></td>
	<td class="line x" title="64:182	We use a free online translation service1 to translate positive and negative polarity words into Dutch, resulting in 974 and 1,523 1http://translate.google.com Dutch words, respectively." ></td>
	<td class="line x" title="65:182	We assumed that a word was translated into Dutch successfully if the translation occurred in the Dutch wordnet (therefore, theresultofthetranslationissmallerthanthe original English lexicon)." ></td>
	<td class="line x" title="66:182	The Dutch wordnet we used in our experiments is the most recent version of Cornetto (Vossen et al., 2007)." ></td>
	<td class="line x" title="67:182	This wordnet contains 103,734 lexical units (words), 70,192 synsets, and 157,679 relations between synsets." ></td>
	<td class="line x" title="68:182	4 Manual assessments To assess the quality of our method we re-used assessments made for earlier work on comparing two resources in terms of their usefulness for automatically generating subjectivity lexicons (Jijkoun and Hofmann, 2008)." ></td>
	<td class="line x" title="69:182	In this setting, the goal was to compare two versions of the Dutch Wordnet: the first from 2001 and the other from 2008." ></td>
	<td class="line x" title="70:182	We applied the method described in section 3 to both resources and generated two subjectivity rankings." ></td>
	<td class="line x" title="71:182	From each ranking, we selected the 2000 words ranked as most negative and the 1500 words ranked as most positive, respectively." ></td>
	<td class="line x" title="72:182	More negative than positive words were chosen to reflect the original distribution of positive vs. negative words." ></td>
	<td class="line x" title="73:182	In addition, we selected words for assessment from the remaining parts of the ranked lists, randomly sampling chunks of 3000 words at intervals of 10000 words with a sampling rate of 10%." ></td>
	<td class="line x" title="74:182	The selection was made in this way because we were mostly interested in negative and positive words, i.e., the words near either end of the rankings." ></td>
	<td class="line x" title="75:182	4.1 Assessment procedure Human annotators were presented with a list of words in random order, for each word its part-ofspeech tag was indicated." ></td>
	<td class="line x" title="76:182	Annotators were asked to identify positive and negative words in this list, i.e., words that indicate positive (negative) emotions, evaluations, or positions." ></td>
	<td class="line x" title="77:182	Annotators were asked to classify each word on the list into one of five classes: ++ thewordispositiveinmostcontexts(strongly positive) + the word is positive in some contexts (weakly positive) 0 the word is hardly ever positive or negative (neutral) 400  the a word is negative in some contexts (weakly negative)  the word is negative in most contexts (strongly negative) Cases where assessors were unable to assign a wordtooneoftheclasses, wereseparatelymarked as such." ></td>
	<td class="line x" title="78:182	Forthepurposeofthisstudywewereonlyinterested in identifying subjective words without considering subjectivity strength." ></td>
	<td class="line x" title="79:182	Furthermore, a pilot study showed assessments of the strength of subjectivity to be a much harder task (54% interannotator agreement) than distinguishing between positive, neutral and negative words only (72% agreement)." ></td>
	<td class="line x" title="80:182	We therefore collapsed the classes of strongly and weakly subjective words for evaluation." ></td>
	<td class="line x" title="81:182	These results for three classes are reported and used in the remainder of this paper." ></td>
	<td class="line x" title="82:182	4.2 Annotators The data were annotated by two undergraduate university students, both native speakers of Dutch." ></td>
	<td class="line x" title="83:182	Annotators were recruited through a university mailing list." ></td>
	<td class="line x" title="84:182	Assessment took a total of 32 working hours (annotating at approximately 450-500 words per hour) which were distributed over a total of 8 annotation sessions." ></td>
	<td class="line x" title="85:182	4.3 Inter-annotator Agreement In total, 9,089 unique words were assessed, of which 6,680 words were assessed by both annotators." ></td>
	<td class="line x" title="86:182	For 205 words, one or both assessors could not assign an appropriate class; these words were excluded from the subsequent study, leaving us with 6,475 words with double assessments." ></td>
	<td class="line x" title="87:182	Table 1 shows the number of assessed words and inter-annotator agreement overall and per part-of-speech." ></td>
	<td class="line x" title="88:182	Overall agreement is 69% (Cohens =0.52)." ></td>
	<td class="line x" title="89:182	The highest agreement is for adjectives, at 76% (=0.62) . This is the same level of agreement as reported in (Kim and Hovy, 2004) for English." ></td>
	<td class="line x" title="90:182	Agreement is lowest for verbs (55%, =0.29) and adverbs (56%, =0.18), which is slightly less than the 62% agreement on verbs reported by Kim and Hovy." ></td>
	<td class="line x" title="91:182	Overall we judge agreement to be reasonable." ></td>
	<td class="line x" title="92:182	Table 2 shows the confusion matrix between the two assessors." ></td>
	<td class="line x" title="93:182	We see that one assessor judged more words as subjective overall, and that more words are judged as negative than positive (this POS Count % agreement  noun 3670 70% 0.51 adjective 1697 76% 0.62 adverb 25 56% 0.18 verb 1083 55% 0.29 overall 6475 69% 0.52 Table 1: Inter-annotator agreement per part-ofspeech." ></td>
	<td class="line x" title="94:182	can be explained by our sampling method described above)." ></td>
	<td class="line x" title="95:182	 0 + Total  1803 137 39 1979 0 1011 1857 649 3517 + 81 108 790 979 Total 2895 2102 1478 6475 Table 2: Contingency table for all words assessed by two annotators." ></td>
	<td class="line x" title="96:182	5 Experiments and results We evaluated several versions of the method of section 3 in order to find the best setting." ></td>
	<td class="line x" title="97:182	Our baseline is a ranking of all words in the wordnet with the weight -1 assigned to the translations of English negative polarity words, 1 assigned to the translations of positive words, and 0 assigned to the remaining words." ></td>
	<td class="line x" title="98:182	This corresponds to simply translating the English subjectivity lexicon." ></td>
	<td class="line x" title="99:182	In the run all.100 we applied our method to all words,synsetsandrelationsfromtheDutchWordnet to create a graph with 153,386 nodes (70,192 synsets, 83,194 words) and 362,868 directed arcs (103,734 word-to-synset, 103,734 synset-to-word, 155,400 synset-to-synset relations)." ></td>
	<td class="line x" title="100:182	We used 100 iterations of the PageRank algorihm for this run (and all runs below, unless indicated otherwise)." ></td>
	<td class="line x" title="101:182	In the run syn.100 we only used synset-toword, word-to-synset relations and 2,850 nearsynonymy relations between synsets." ></td>
	<td class="line x" title="102:182	We added 1,459 near-antonym relations to the graph to produce the run syn+ant.100." ></td>
	<td class="line x" title="103:182	In the run syn+hyp.100 we added 66,993 hyponymy and 66,993 hyperonymy relations to those used in run syn.100." ></td>
	<td class="line x" title="104:182	We also experimented with the information provided in the definitions (glosses) of synset." ></td>
	<td class="line x" title="105:182	The glosses were available for 68,122 of the 70,192 401 synsets." ></td>
	<td class="line x" title="106:182	Following (Esuli and Sebastiani, 2007), we assumed that there is a semantic relationship between a synset and each word used in its gloss." ></td>
	<td class="line x" title="107:182	Thus, the run gloss.100 uses a graph with 70,192 synsets, 83,194 words and 350,855 directed arcs from synsets to lemmas of all words in their glosses." ></td>
	<td class="line x" title="108:182	To create these arcs, glosses were lemmatized and lemmas not found in the wordnet were ignored." ></td>
	<td class="line x" title="109:182	Toseeiftheinformationintheglossescancomplement the wordnet relations, we also generated a hybrid run syn+ant+gloss.100 that used arcs derived from word-to-synset, synset-to-word, synonymy, antonymy relations and glosses." ></td>
	<td class="line x" title="110:182	Finally, we experimented with the number of iterations of PageRank in two setting: using all wordnet relations and using only synonyms and antonyms." ></td>
	<td class="line x" title="111:182	5.1 Evaluation measures We used several measures to evaluate the quality of the word rankings produced by our method." ></td>
	<td class="line x" title="112:182	We consider the evaluation of a ranking parallel to the evaluation for a binary classification problem, where words are classified as positive (resp." ></td>
	<td class="line x" title="113:182	negative) if the assigned score exceeds a certain threshold value." ></td>
	<td class="line x" title="114:182	We can select a specific threshold and classify all words exceeding this score as positive." ></td>
	<td class="line x" title="115:182	There will be a certain amount of correctly classified words (true positives), and some incorrectly classified words (false positives)." ></td>
	<td class="line x" title="116:182	As we move the threshold to include a larger portion of the ranking, both the number of true positives and the number of false positives increase." ></td>
	<td class="line x" title="117:182	Wecanvisualizethequalityofrankingsbyplotting their ROC curves, which show the relation between true positive rate (portion of the data correctly labeled as positive instances) and false positive rate (portion of the data incorrectly labeled as positive instances) at all possible threshold settings." ></td>
	<td class="line x" title="118:182	To compare rankings, we compute the area under the ROC curve (AUC), a measure frequently used to evaluate the performance of ranking classifiers." ></td>
	<td class="line x" title="119:182	The AUC value corresponds to the probability that a randomly drawn positive instance will be ranked higher than a randomly drawn negative instance." ></td>
	<td class="line x" title="120:182	Thus, an AUC of 0.5 corresponds to random performance, a value of 1.0 corresponds to perfect performance." ></td>
	<td class="line x" title="121:182	When evaluating word rankings, we compute AUC and AUC+ as evaluaRun k Dk AUC AUC+ baseline 0.395 0.303 0.701 0.733 syn.10 0.641 0.180 0.829 0.837 gloss.100 0.637 0.181 0.829 0.835 all.100 0.565 0.218 0.792 0.787 syn.100 0.645 0.177 0.831 0.839 syn+ant.100 0.650 0.175 0.833 0.841 syn+ant+gloss.100 0.643 0.178 0.831 0.838 syn+hyp.100 0.594 0.203 0.807 0.810 Table 3: Evaluation results tion measures for the tasks of identifying words with negative (resp., positive) polarity." ></td>
	<td class="line x" title="122:182	Other measures commonly used to evaluate rankings are Kendalls rank correlation, or Kendalls tau coefficient, and Kendalls distance (Fagin et al., 2004; Esuli and Sebastiani, 2007)." ></td>
	<td class="line x" title="123:182	When comparing rankings, Kendalls measures look at the number of pairs of ranked items that agree or disagree with the ordering in the gold standard." ></td>
	<td class="line x" title="124:182	The measures can deal with partially ordered sets (i.e., rankings with ties): only pairs that are ordered in the gold standard are used." ></td>
	<td class="line x" title="125:182	Let T = {(ai,bi)}i denote the set of pairs ordered in the gold standard, i.e., ai g bi." ></td>
	<td class="line x" title="126:182	Let C = {(a,b)  T | a r b} be the set of concordant pairs, i.e., pairs ordered the same way in the gold standard and in the ranking." ></td>
	<td class="line x" title="127:182	Let D = {(a,b)  T | b r a} be the set of discordant pairs and U = T \ (C  D) the set of pairs ordered in the gold standard, but tied in the ranking." ></td>
	<td class="line x" title="128:182	Kendalls rank correlation coefficient k and Kendalls distance Dk are defined as follows: k = |C|  |D||T| Dk = |D|+p |U||T| where p is a penalization factor for ties, which we set to 0.5, following (Esuli and Sebastiani, 2007)." ></td>
	<td class="line x" title="129:182	The value of k ranges from -1 (perfect disagreement) to 1 (perfect agreement), with 0 indicating an almost random ranking." ></td>
	<td class="line x" title="130:182	The value of Dk ranges from 0 (perfect agreement) to 1 (perfect disagreement)." ></td>
	<td class="line x" title="131:182	When applying Kendalls measures we assume that the gold standard defines a partial order: for twowordsaandb,a g bholdswhena  Ng,b  Ug Pg or when a  Ug,b  Pg; here Ng,Ug,Pg are sets of words judged as negative, neutral and positive, respectively, by human assessors." ></td>
	<td class="line x" title="132:182	5.2 Types of wordnet relations TheresultsinTable3indicatethatthemethodperforms best when only synonymy and antonymy 402 Negative polarity False positive rate True positive rate 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 baseline all.100 gloss.100 syn+ant.100 syn+hyp.100 Positive polarity False positive rate True positive rate 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 baseline all.100 gloss.100 syn+ant.100 syn+hyp.100 Figure 1: ROC curves showing the impact of using different sets of relations for negative and positive polarity." ></td>
	<td class="line x" title="133:182	Graphs were generated using ROCR (Sing et al., 2005)." ></td>
	<td class="line x" title="134:182	relations are considered for ranking." ></td>
	<td class="line x" title="135:182	Adding hyponyms and hyperonyms, or adding relations between synsets and words in their glosses substantially decrease the performance, according to all four evaluation measures." ></td>
	<td class="line x" title="136:182	With all relations, the performance degrades even further." ></td>
	<td class="line x" title="137:182	Our hypothesis is that with many relations the polarity mass of the seed words is distributed too broadly." ></td>
	<td class="line x" title="138:182	This is supported by the drop in the performance early in the ranking at the negative side of runs with all relations and with hyponyms (Figure 1, left)." ></td>
	<td class="line x" title="139:182	Another possible explanation can be that words with many incoming arcs (but without strong connections to the seed words) get substantial weights, thereby decreasing the quality of the ranking." ></td>
	<td class="line x" title="140:182	Antonymy relations also prove useful, as using them in addition to synonyms results in a small improvement." ></td>
	<td class="line x" title="141:182	This justifies our modification of the PageRank algorithm, when we allow negative node and arc weights." ></td>
	<td class="line x" title="142:182	In the best setting (syn+ant.100), our method achieves an accuracy of 0.82 at top 3,000 negative words, and 0.62 at top 3,000 positive words (estimated from manual assessments of a sample, see section 4)." ></td>
	<td class="line x" title="143:182	Moreover, Figure 1 indicates that the accuracy of the seed set (i.e., the baseline translations of the English lexicon) is maintained at the positive and negative ends of the ranking for most variants of the method." ></td>
	<td class="line x" title="144:182	5.3 The number of iterations In Figure 2 we plot how the AUC measure changes when the number of PageRank iterations increases (for positive polarity; the plots are almost identical for negative polarity)." ></td>
	<td class="line x" title="145:182	Although the absolutemaximumofAUCisachievedat110iteration (60 iterations for positive polarity), the AUC clearly converges after 20 iterations." ></td>
	<td class="line x" title="146:182	We conclude that after 20 iterations all useful information has been propagated through the graph." ></td>
	<td class="line x" title="147:182	Moreover, our version of PageRank reaches a stable weight distribution and, at the same time, produces the best ranking." ></td>
	<td class="line x" title="148:182	5.4 Comparison to previous work Although the values in the evaluation results are, obviously, language-dependent, we tried to replicate the methods used in the literature for Romanian and English (section 2), to the degree possible." ></td>
	<td class="line x" title="149:182	Our baseline replicates the method of (Mihalcea et al., 2007): i.e., a simple translation of the English lexicon into the target language." ></td>
	<td class="line x" title="150:182	The run syn.10 is similar to the iterative method used in (Banea et al., 2008), except that we do not perform a corpus-based filtering." ></td>
	<td class="line x" title="151:182	We run PageRank for 10 iterations, so that polarity is propagated from the seed words to all their 5-step-synonymy neighbours." ></td>
	<td class="line x" title="152:182	Table 3 indicates that increasing the number of iterations in the method of (Banea et 403 0 50 100 150 200 0.70 0.75 0.80 0.85 0.90 Number of iterations AUC all relations synsets+antonyms Figure2: Thenumberofiterationsandtheranking quality (AUC), for positive polarity." ></td>
	<td class="line x" title="153:182	Rankings for negative polarity behave similarly." ></td>
	<td class="line x" title="154:182	al., 2008) might help to generate a better subjectivity lexicon." ></td>
	<td class="line x" title="155:182	The run gloss.100 is similar to the PageRankbased method of (Esuli and Sebastiani, 2007)." ></td>
	<td class="line x" title="156:182	The main difference is that Esuli and Sebastiani used the extended English WordNet, where words in all glosses are manually assigned to their correct synsets: the PageRank method then uses relations between synsets and synsets of words in their glosses." ></td>
	<td class="line x" title="157:182	Since such a resource is not available for our target language (Dutch), we used relations between synsets and words in their glosses, instead." ></td>
	<td class="line x" title="158:182	With this simplification, the PageRank method using glosses produces worse results than the method using synonyms." ></td>
	<td class="line x" title="159:182	Further experiments with the extended English WordNet are necessary to investigate whether this decrease can be attributed to the lack of disambiguation for glosses." ></td>
	<td class="line x" title="160:182	An important difference between our method and (Esuli and Sebastiani, 2007) is that the latter produces two independent rankings: one for positive and one for negative words." ></td>
	<td class="line x" title="161:182	To evaluate the effect of this choice, we generated runs gloss.100.N and gloss.100.P that used only negative (resp., only positive) seed words." ></td>
	<td class="line x" title="162:182	We compare these runs with the run gloss.100 (that starts with both positive and negative seeds) in Table 4." ></td>
	<td class="line x" title="163:182	To allow a fair comparison of the generated rankings, the evaluation measures in this case are calculated separately for two binary classification problems: words with negative polarity versus all words, and words with positive polarity versus all." ></td>
	<td class="line x" title="164:182	The results in Table 4 clearly indicate that inRun k Dk AUC gloss.100 0.669 0.166 0.829 gloss.100.N 0.562 0.219 0.782 +k D+k AUC+ gloss.100 0.665 0.167 0.835 gloss.100.P 0.580 0.210 0.795 Table4: Comparisonofseparateandsimultaneous rankings of negative and positive words." ></td>
	<td class="line x" title="165:182	formation about words of one polarity class helps to identify words of the other polarity: negative words are unlikely to be also positive, and vice versa." ></td>
	<td class="line x" title="166:182	This supports our design choice: ranking words from negative to positive in one run of the method." ></td>
	<td class="line x" title="167:182	6 Conclusion We have presented a PageRank-like algorithm that bootstraps a subjectivity lexicon from a list of initial seed examples (automatic translations of words in an English subjectivity lexicon)." ></td>
	<td class="line x" title="168:182	The algorithm views a wordnet as a graph where words and concepts are connected by relations such as synonymy, hyponymy, meronymy etc. We initializethealgorithmbyassigninghighweightstopositive seed examples and low weights to negative seedexamples." ></td>
	<td class="line x" title="169:182	Theseweightsarethenpropagated through the wordnet graph via the relations." ></td>
	<td class="line x" title="170:182	After a number of iterations words are ranked according to their weight." ></td>
	<td class="line x" title="171:182	We assume that words with lower weights are likely negative and words with high weights are likely positive." ></td>
	<td class="line x" title="172:182	We evaluated several variants of the method for the Dutch language, using the most recent version of Cornetto, an extension of Dutch WordNet." ></td>
	<td class="line x" title="173:182	The evaluation was based on the manual assessment of 9,089 words (with inter-annotator agreement 69%, =0.52)." ></td>
	<td class="line x" title="174:182	Best results were achieved when the method used only synonymy and antonymy relations, and was ranking positive and negative words simultaneously." ></td>
	<td class="line x" title="175:182	In this setting, the method achieves an accuracy of 0.82 at the top 3,000 negative words, and 0.62 at the top 3,000 positive words." ></td>
	<td class="line x" title="176:182	Our method is language-independent and can easily be applied to other languages for which wordnets exist." ></td>
	<td class="line x" title="177:182	We plan to make the implementation of the method publicly available." ></td>
	<td class="line x" title="178:182	An additional important outcome of our experiments is the first (to our knowledge) manually annotated sentiment lexicon for the Dutch language." ></td>
	<td class="line x" title="179:182	404 The lexicon contains 2,836 negative polarity and 1,628 positive polarity words." ></td>
	<td class="line x" title="180:182	The lexicon will be made publicly available as well." ></td>
	<td class="line x" title="181:182	Our future work will focus on using the lexicon for sentenceand phrase-level sentiment extraction for Dutch." ></td>
	<td class="line x" title="182:182	Acknowledgments This work was supported by projects DuOMAn and Cornetto, carried out within the STEVIN programme which is funded by the Dutch and Flemish Governments (http:// www.stevin-tst.org), and by the NetherlandsOrganizationforScientificResearch(NWO) under project number 612.061.814." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-1001
Subjectivity Recognition on Word Senses via Semi-supervised Mincuts
Su, Fangzhong;Markert, Katja;"></td>
	<td class="line x" title="1:199	Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 19, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:199	c 2009 Association for Computational Linguistics Subjectivity Recognition on Word Senses via Semi-supervised Mincuts Fangzhong Su School of Computing University of Leeds fzsu@comp.leeds.ac.uk Katja Markert School of Computing University of Leeds markert@comp.leeds.ac.uk Abstract We supplement WordNet entries with information on the subjectivity of its word senses." ></td>
	<td class="line x" title="3:199	Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data." ></td>
	<td class="line x" title="4:199	The resulting data sparseness problem is aggravated by the fact that dictionary definitions are very short." ></td>
	<td class="line x" title="5:199	We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure." ></td>
	<td class="line x" title="6:199	The experimental results show that it outperformssupervisedminimumcutaswellasstandard supervised, non-graph classification, reducing the error rate by 40%." ></td>
	<td class="line x" title="7:199	In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data." ></td>
	<td class="line x" title="8:199	1 Introduction There is considerable academic and commercial interest in processing subjective content in text, where subjective content refers to any expression of a private state such as an opinion or belief (Wiebe et al., 2005)." ></td>
	<td class="line x" title="9:199	Important strands of work include the identification of subjective content and the determination of its polarity, i.e. whether a favourable or unfavourable opinion is expressed." ></td>
	<td class="line oc" title="10:199	Automatic identification of subjective content often relies on word indicators, such as unigrams (Pang et al., 2002) or predetermined sentiment lexica (Wilson et al., 2005)." ></td>
	<td class="line x" title="11:199	Thus, the word positive in the sentence This deal is a positive development for our company. gives a strong indication that the sentence contains a favourable opinion." ></td>
	<td class="line x" title="12:199	However, such word-based indicators can be misleading for two reasons." ></td>
	<td class="line x" title="13:199	First, contextual indicators such as irony and negation can reverse subjectivity or polarity indications (Polanyi and Zaenen, 2004)." ></td>
	<td class="line x" title="14:199	Second, different word senses of a single word can actually be of different subjectivity or polarity." ></td>
	<td class="line x" title="15:199	A typical subjectivity-ambiguous word, i.e. a word that has at least one subjective and at least one objective sense, is positive, as shown by the two example senses given below.1 (1) positive, electropositivehaving a positive electric charge;protons are positive (objective) (2) plus, positiveinvolving advantage or good; a plus (or positive) factor (subjective) We concentrate on this latter problem by automatically creating lists of subjective senses, instead of subjective words, via adding subjectivity labels for senses to electronic lexica, using the example of WordNet." ></td>
	<td class="line x" title="16:199	This is important as the problem of subjectivity-ambiguity is frequent: We (Su and Markert, 2008) find that over 30% of words in our dataset are subjectivity-ambiguous." ></td>
	<td class="line x" title="17:199	Information on subjectivity of senses can also improve other tasks such as word sense disambiguation (Wiebe and Mihalcea, 2006)." ></td>
	<td class="line x" title="18:199	Moreover, Andreevskaia and Bergler (2006) show that the performance of automaticannotationofsubjectivityatthewordlevelcan be hurt by the presence of subjectivity-ambiguous words in the training sets they use." ></td>
	<td class="line x" title="19:199	1All examples in this paper are from WordNet 2.0." ></td>
	<td class="line x" title="20:199	1 We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Ouralgorithmoutperformssupervisedminimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%." ></td>
	<td class="line x" title="21:199	In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data." ></td>
	<td class="line x" title="22:199	Our approach also outperforms prior approaches to the subjectivity recognition of word senses and performs well across two different data sets." ></td>
	<td class="line x" title="23:199	The remainder of this paper is organized as follows." ></td>
	<td class="line x" title="24:199	Section 2 discusses previous work." ></td>
	<td class="line x" title="25:199	Section 3 describes our proposed semi-supervised minimum cut framework in detail." ></td>
	<td class="line x" title="26:199	Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5." ></td>
	<td class="line oc" title="27:199	2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level." ></td>
	<td class="line x" title="28:199	An up-to-date overview is given in Pang and Lee (2008)." ></td>
	<td class="line x" title="29:199	Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do." ></td>
	<td class="line x" title="30:199	We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences, used in Pang and Lee (2004)) at the word sense level." ></td>
	<td class="line x" title="31:199	At the word level Takamura et al.(2005) use a semi-supervised spin model for word polarity determination, where the graph 2It can be argued that subjectivity labels are maybe rather more graded than the clear-cut binary distinction we assign." ></td>
	<td class="line x" title="33:199	However, in Su and Markert (2008a) as well as Wiebe and Mihalcea (2006) we find that human can assign the binary distinction to word senses with a high level of reliability." ></td>
	<td class="line x" title="34:199	is constructed using a variety of information such as gloss co-occurrences and WordNet links." ></td>
	<td class="line x" title="35:199	Apart from using a different graph-based model from ours, theyassumethatsubjectivityrecognitionhasalready been achieved prior to polarity recognition and test against word lists containing subjective words only." ></td>
	<td class="line x" title="36:199	However, Kim and Hovy (2004) and Andreevskaia and Bergler (2006) show that subjectivity recognition might be the harder problem with lower human agreement and automatic performance." ></td>
	<td class="line x" title="37:199	In addition, we deal with classification at the word sense level, treating also subjectivity-ambiguous words, which goes beyond the work in Takamura et al.(2005)." ></td>
	<td class="line x" title="39:199	Word Sense Level: There are three prior approaches addressing word sense subjectivity or polarity classification." ></td>
	<td class="line x" title="40:199	Esuli and Sebastiani (2006) determine the polarity (positive/negative/objective) of word senses in WordNet." ></td>
	<td class="line x" title="41:199	However, there is no evaluation as to the accuracy of their approach." ></td>
	<td class="line x" title="42:199	They then extend their work (Esuli and Sebastiani, 2007) by applying the Page Rank algorithm to rank the WordNet senses in terms of how strongly a sense possesses a given semantic property (e.g., positive or negative)." ></td>
	<td class="line x" title="43:199	Apart from us tackling subjectivity instead of polarity, their Page Rank graph is also constructed focusing on WordNet glosses (linking glosses containing the same words), whereas we concentrate on the use of WordNet relations." ></td>
	<td class="line x" title="44:199	Both Wiebe and Mihalcea (2006) and our prior work (Su and Markert, 2008) present an annotation scheme for word sense subjectivity and algorithms for automatic classification." ></td>
	<td class="line x" title="45:199	Wiebe and Mihalcea (2006) use an algorithm relying on distributional similarity and an independent, large manually annotated opinion corpus (MPQA) (Wiebe et al., 2005)." ></td>
	<td class="line x" title="46:199	Oneofthedisadvantagesoftheiralgorithmis thatitisrestrictedtosensesthathavedistributionally similar words in the MPQA corpus, excluding 23% of their test data from automatic classification." ></td>
	<td class="line x" title="47:199	Su and Markert (2008) present supervised classifiers, which rely mostly on WordNet glosses and do not effectively exploit WordNets relation structure." ></td>
	<td class="line x" title="48:199	3 Semi-Supervised Mincuts 3.1 Minimum Cuts: The Main Idea Binary classification with minimum cuts (Mincuts) in graphs is based on the idea that similar items 2 should be grouped in the same cut." ></td>
	<td class="line x" title="49:199	All items in the training/test data are seen as vertices in a graph with undirected weighted edges between them specifying how strong the similarity/association between two vertices is. We use minimum s-t cuts: the graph contains two particular vertices s (source, corresponds to subjective) and t (sink, corresponds to objective) and each vertex u is connected to s and t via a weighted edge that can express how likely u is to be classified as s or t in isolation." ></td>
	<td class="line x" title="50:199	Binary classification of the vertices is equivalent to splitting the graph into two disconnected subsets of all vertices, S and T with s  S and t  T. This corresponds to removing a set of edges from the graph." ></td>
	<td class="line x" title="51:199	As similar items should be in the same part of the split, the best split is one which removes edges with low weights." ></td>
	<td class="line x" title="52:199	In other words, a minimum cut problem is to find a partition of the graph which minimizes the following formula, where w(u,v) expresses the weight of an edge between two vertices." ></td>
	<td class="line oc" title="53:199	W(S,T) = summationdisplay uS,vT w(u,v) Globally optimal minimum cuts can be found in polynomial time and near-linear running time in practice, using the maximum flow algorithm (Pang and Lee, 2004; Cormen et al., 2002)." ></td>
	<td class="line x" title="54:199	3.2 Why might Semi-supervised Minimum Cuts Work?" ></td>
	<td class="line x" title="55:199	We propose semi-supervised mincuts for subjectivity recognition on senses for several reasons." ></td>
	<td class="line x" title="56:199	First, our problem satisfies two major conditions necessary for using minimum cuts." ></td>
	<td class="line x" title="57:199	It is a binary classification problem (subjective vs. objective senses) as is needed to divide the graph into two components." ></td>
	<td class="line x" title="58:199	Our dataset also lends itself naturally to s-t Mincuts as we have two different views on the data." ></td>
	<td class="line x" title="59:199	Thus, the edges of a vertex (=sense) to the source/sink can be seen as the probability of a sense being subjective or objective without taking similaritytoothersensesintoaccount, forexampleviaconsidering only the sense gloss." ></td>
	<td class="line x" title="60:199	In contrast, the edges between two senses can incorporate the WordNet relation hierarchy, which is a good source of similarity for our problem as many WordNet relations are subjectivity-preserving, i.e. if two senses are connected via such a relation they are likely to be both subjective or both objective.3 An example here is the antonym relation, where two antonyms such as goodmorally admirable and evil, wickedmorally bad or wrong are both subjective." ></td>
	<td class="line x" title="61:199	Second, Mincuts can be easily expanded into a semi-supervised framework (Blum and Chawla, 2001)." ></td>
	<td class="line x" title="62:199	This is essential as the existing labeled datasets for our problem are small." ></td>
	<td class="line x" title="63:199	In addition, glosses are short, leading to sparse high dimensional vectors in standard feature representations." ></td>
	<td class="line x" title="64:199	Also, WordNet connections between different parts of the WordNet hierarchy can also be sparse, leading to relatively isolated senses in a graph in a supervised framework." ></td>
	<td class="line x" title="65:199	Semi-supervised Mincuts allow us to import unlabeled data that can serve as bridges to isolated components." ></td>
	<td class="line x" title="66:199	More importantly, as the unlabeled data can be chosen to be related to the labeled and test data, they might help pull test data to the right cuts (categories)." ></td>
	<td class="line x" title="67:199	3.3 Formulation of Semi-supervised Mincuts The formulation of our semi-supervised Mincut for sense subjectivity classification involves the following steps, which we later describe in more detail." ></td>
	<td class="line x" title="68:199	1." ></td>
	<td class="line x" title="69:199	We define two vertices s (source) and t (sink), which correspond to the subjective and objective category, respectively." ></td>
	<td class="line x" title="70:199	Following the definition in Blum and Chawla (2001), we call the vertices s and t classification vertices, and all other vertices (labeled, test, and unlabeled data) example vertices." ></td>
	<td class="line x" title="71:199	Each example vertex corresponds to one WordNet sense and is connected to both s and t via a weighted edge." ></td>
	<td class="line x" title="72:199	The latter guarantees that the graph is connected." ></td>
	<td class="line x" title="73:199	2." ></td>
	<td class="line x" title="74:199	For the test and unlabeled examples, we see the edges to the classification vertices as the probability of them being subjective/objective disregarding other example vertices." ></td>
	<td class="line x" title="75:199	We use a supervised classifier to set these edge weights." ></td>
	<td class="line x" title="76:199	Forthelabeledtrainingexamples, theyareconnected by edges with a high constant weight to the classification vertices that they belong to." ></td>
	<td class="line x" title="77:199	3." ></td>
	<td class="line x" title="78:199	WordNet relations are used to construct the edges between two example vertices." ></td>
	<td class="line x" title="79:199	Such 3See Kamps et al.(2004) for an early indication of such properties for some WordNet relations." ></td>
	<td class="line x" title="81:199	3 edges can exist between any pair of example vertices, for example between two unlabeled examples." ></td>
	<td class="line x" title="82:199	4." ></td>
	<td class="line x" title="83:199	After graph construction we then employ a maximum-flow algorithm to find the minimum s-t cuts of the graph." ></td>
	<td class="line x" title="84:199	The cut in which the sourcevertexsliesisclassified assubjective, andthecutinwhichthesinkvertextliesisobjective." ></td>
	<td class="line x" title="85:199	We now describe the above steps in more detail." ></td>
	<td class="line x" title="86:199	Selection of unlabeled data: Random selection of unlabeled data might hurt the performance of Mincuts, as they might not be related to any sense in our training/test data (denoted by A)." ></td>
	<td class="line x" title="87:199	Thus a basic principle is that the selected unlabeled senses should be related to the training/test data by WordNet relations." ></td>
	<td class="line x" title="88:199	WethereforesimplyscaneachsenseinA, and collect all senses related to it via one of the WordNet relations in Table 1." ></td>
	<td class="line x" title="89:199	All such senses that are not in A are collected in the unlabeled data set." ></td>
	<td class="line x" title="90:199	Weighting of edges to the classification vertices: The edge weight to s and t represents how likely it is that an example vertex is initially put in the cut in which s (subjective) or t (objective) lies." ></td>
	<td class="line x" title="91:199	For unlabeled and test vertices, we use a supervised classifier (SVM4) with the labeled data as training data to assign the edge weights." ></td>
	<td class="line x" title="92:199	The SVM is also used as a baseline and its features are described in Section 4.3." ></td>
	<td class="line x" title="93:199	As we do not wish the Mincut to reverse labels of the labeled training data, we assign a high constant weight of 5 to the edge between a labeledvertexanditscorrespondingclassificationvertex, and a low weight of 0.01 to the edge to the other classification vertex." ></td>
	<td class="line x" title="94:199	Assigning weights to WordNet relations: We connect two vertices that are linked by one of the ten WordNet relations in Table 1 via an edge." ></td>
	<td class="line x" title="95:199	Not all WordNet relations we use are subjectivitypreserving to the same degree: for example, hyponyms (such as simpleton) of objective senses (such as person) do not have to be objective." ></td>
	<td class="line x" title="96:199	However, we aim for high graph connectivity and we can assign different weights to different relations 4We employ LIBSVM, available at http://www.csie." ></td>
	<td class="line x" title="97:199	ntu.edu.tw/cjlin/libsvm/." ></td>
	<td class="line x" title="98:199	Linear kernel and probability estimates are used in this work." ></td>
	<td class="line x" title="99:199	to reflect the degree to which they are subjectivitypreserving." ></td>
	<td class="line x" title="100:199	Therefore, we experiment with two methods of weight assignment." ></td>
	<td class="line x" title="101:199	Method 1 (NoSL) assigns the same constant weight of 1.0 to all WordNet relations." ></td>
	<td class="line x" title="102:199	Method 2 (SL) reflects different degrees of preserving subjectivity." ></td>
	<td class="line x" title="103:199	To do this, we adapt an unsupervised method of generating a large noisy set of subjective and objective senses from our previous work (Su and Markert, 2008)." ></td>
	<td class="line x" title="104:199	This method uses a list of subjective words (SL)5 to classify each WordNet sense with at least two subjective words in its gloss as subjective and all other senses as objective." ></td>
	<td class="line x" title="105:199	We then count how often two senses related via a given relation have the same or a different subjectivity label." ></td>
	<td class="line x" title="106:199	The weight is computed by #same/(#same+#different)." ></td>
	<td class="line x" title="107:199	Results are listed in Table 1." ></td>
	<td class="line x" title="108:199	Table 1: Relation weights (Method 2) Method #Same #Different Weight Antonym 2,808 309 0.90 Similar-to 6,887 1,614 0.81 Derived-from 4,630 947 0.83 Direct-Hypernym 71,915 8,600 0.89 Direct-Hyponym 71,915 8,600 0.89 Attribute 350 109 0.76 Also-see 1,037 337 0.75 Extended-Antonym 6,917 1,651 0.81 Domain 4,387 892 0.83 Domain-member 4,387 892 0.83 Example graph: An example graph is shown in Figure 1." ></td>
	<td class="line x" title="109:199	The three example vertices correspond to the senses religiousextremely scrupulous and conscientious, scrupuloushaving scruples; arising from a sense of right and wrong; principled; and flicker, spark, glinta momentary flash of light respectively." ></td>
	<td class="line x" title="110:199	The vertex scrupulous is unlabeled data derived from the vertex religious(a test item) by the relation similar-to." ></td>
	<td class="line x" title="111:199	4 Experiments and Evaluation 4.1 Datasets We conduct the experiments on two different gold standard datasets." ></td>
	<td class="line x" title="112:199	One is the Micro-WNOp corpus, 5Available at http://www.cs.pitt.edu/mpqa 4 scrupulous religious subjective objective flicker 0.24 0.76 0.83 0.17 0.16 0.84 0.81similar-to Figure 1: Graph of Word Senses which is representative of the part-of-speech distribution in WordNet 6." ></td>
	<td class="line x" title="113:199	It includes 298 words with 703 objective and 358 subjective WordNet senses." ></td>
	<td class="line x" title="114:199	The second one is the dataset created by Wiebe and Mihalcea (2006).7 It only contains noun and verb senses, and includes 60 words with 236 objective and 92 subjective WordNet senses." ></td>
	<td class="line x" title="115:199	As the Micro-WNOp set is larger and also contains adjective and adverb senses, we describe our results in more detail on that corpus in the Section 4.3 and 4.4." ></td>
	<td class="line x" title="116:199	In Section 4.5, we shortly discuss results on Wiebe&Mihalceas dataset." ></td>
	<td class="line x" title="117:199	4.2 Baseline and Evaluation We compare to a baseline that assigns the most frequent category objective to all senses, which achievesanaccuracyof66.3%and72.0%onMicroWNOp and Wiebe&Mihalceas dataset respectively." ></td>
	<td class="line x" title="118:199	We use the McNemar test at the significance level of 5% for significance statements." ></td>
	<td class="line x" title="119:199	All evaluations are carried out by 10-fold cross-validation." ></td>
	<td class="line x" title="120:199	4.3 Standard Supervised Learning We use an SVM classifier to compare our proposed semi-supervised Mincut approach to a reasonable 6Available at http://www.comp.leeds.ac.uk/ markert/data." ></td>
	<td class="line x" title="121:199	This dataset was first used with a different annotation scheme in Esuli and Sebastiani (2007) and we also used it in Su and Markert (2008)." ></td>
	<td class="line x" title="122:199	7Available at http://www.cs.pitt.edu/wiebe/ pubs/papers/goldstandard.total.acl06." ></td>
	<td class="line x" title="123:199	baseline.8 Three different feature types are used." ></td>
	<td class="line x" title="124:199	LexicalFeatures(L):a bag-of-words representation of the sense glosses with stop word filtering." ></td>
	<td class="line x" title="125:199	RelationFeatures (R): First, we use two features for each of the ten WordNet relations in Table 1, describing how many relations of that type the sense hastosensesinthesubjectiveorobjectivepartofthe training set, respectively." ></td>
	<td class="line x" title="126:199	This provides a non-graph summary of subjectivity-preserving links." ></td>
	<td class="line x" title="127:199	Second, we manually collected a small set (denoted by SubjSet) of seven subjective verb and noun senses which are close to the root in WordNets hypernym tree." ></td>
	<td class="line x" title="128:199	A typical example element of SubjSet is psychological feature a feature of the mental life of a living organism, which indicates subjectivity for its hyponyms such as hope  the general feeling that some desire will be fulfilled." ></td>
	<td class="line x" title="129:199	A binary feature describes whether a noun/verb sense is a hyponym of an element of SubjSet." ></td>
	<td class="line x" title="130:199	Monosemous Feature (M): for each sense, we scan if a monosemous word is part of its synset." ></td>
	<td class="line x" title="131:199	If so, we further check if the monosemous word is collected in the subjective word list (SL)." ></td>
	<td class="line x" title="132:199	The intuition is that if a monosemous word is subjective, obviously its (single) sense is subjective." ></td>
	<td class="line x" title="133:199	For example, the sense uncompromising, inflexiblenot making concessions is subjective, as uncompromising is a monosemous word and also in SL." ></td>
	<td class="line x" title="134:199	We experiment with different combinations of features and the results are listed in Table 2, prefixed by SVM." ></td>
	<td class="line x" title="135:199	All combinations perform significantly better than the more frequent category baseline and similarly to the supervised Naive Bayes classifier (see S&M in Table 2) we used in Su and Markert (2008)." ></td>
	<td class="line x" title="136:199	However, improvements by adding more features remain small." ></td>
	<td class="line x" title="137:199	In addition, we compare to a supervised classifier (see Lesk in Table 2) that just assigns each sense the subjectivity label of its most similar sense in the training data, using Lesks similarity measure from Pedersens WordNet similarity package9." ></td>
	<td class="line x" title="138:199	We use Lesk as it is one of the few measures applicable across all parts-of-speech." ></td>
	<td class="line x" title="139:199	8This SVM is also used to provide the edge weights to the classification vertices in the Mincut approach." ></td>
	<td class="line x" title="140:199	9Availableathttp://www.d.umn.edu/tpederse/ similarity.html." ></td>
	<td class="line x" title="141:199	5 Table 2: Results of SVM and Mincuts with different settings of feature Method Subjective Objective Accuracy Precision Recall F-score Precision Recall F-score Baseline N/A 0 N/A 66.3% 100% 79.7% 66.3% S&M 66.2% 64.5% 65.3% 82.2% 83.2% 82.7% 76.9% Lesk 65.6% 50.3% 56.9% 77.5% 86.6% 81.8% 74.4% SVM-L 69.6% 37.7% 48.9% 74.3% 91.6% 82.0% 73.4% L-SL 82.0% 43.3% 56.7% 76.7% 95.2% 85.0% 77.7% L-NoSL 80.8% 43.6% 56.6% 76.7% 94.7% 84.8% 77.5% SVM-LM 68.9% 42.2% 52.3% 75.4% 90.3% 82.2% 74.1% LM-SL 83.2% 44.4% 57.9% 77.1% 95.4% 85.3% 78.2% LM-NoSL 83.6% 44.1% 57.8% 77.1% 95.6% 85.3% 78.2% SVM-LR 68.4% 45.3% 54.5% 76.2% 89.3% 82.3% 74.5% LR-SL 82.7% 65.4% 73.0% 84.1% 93.0% 88.3% 83.7% LR-NoSL 82.4% 65.4% 72.9% 84.0% 92.9% 88.2% 83.6% SVM-LRM 69.8% 47.2% 56.3% 76.9% 89.6% 82.8% 75.3% LRM-SL 85.5% 65.6% 74.2% 84.4% 94.3% 89.1% 84.6% LRM-NoSL 84.6% 65.9% 74.1% 84.4% 93.9% 88.9% 84.4% 1 L, R and M correspond to the lexical, relation and monosemous features respectively." ></td>
	<td class="line x" title="142:199	2 SVM-LcorrespondstousinglexicalfeaturesonlyfortheSVMclassifier." ></td>
	<td class="line x" title="143:199	Likewise,SVMLRM corresponds to using a combination for lexical, relation, and monosemous features for the SVM classifier." ></td>
	<td class="line x" title="144:199	3 L-SL corresponds to the Mincut that uses only lexical features for the SVM classifier, and subjective list (SL) to infer the weight of WordNet relations." ></td>
	<td class="line x" title="145:199	Likewise, LM-NoSL corresponds to the Mincut algorithm that uses lexical and monosemous features for the SVM, and predefined constants for WordNet relations (without subjective list)." ></td>
	<td class="line x" title="146:199	4.4 Semi-supervised Graph Mincuts Using our formulation in Section 3.3, we import 3,220 senses linked by the ten WordNet relations to any senses in Micro-WNOp as unlabeled data." ></td>
	<td class="line x" title="147:199	We construct edge weights to classification vertices using the SVM discussed above and use WordNet relations for links between example vertices, weighted by either constants (NoSL) or via the method illustrated in Table 1 (SL)." ></td>
	<td class="line x" title="148:199	The results are also summarized in Table 2." ></td>
	<td class="line x" title="149:199	Semi-supervised Mincuts always significantly outperform the corresponding SVM classifiers, regardless of whether the subjectivity list is used for setting edge weights." ></td>
	<td class="line x" title="150:199	We can also see that we achieve good results without using any other knowledge sources (setting LR-NoSL)." ></td>
	<td class="line x" title="151:199	The example in Figure 1 explains why semisupervised Mincuts outperforms the supervised approach." ></td>
	<td class="line x" title="152:199	The vertex religious is initially assigned the subjective/objective probabilities 0.24/0.76 by theSVMclassifier, leadingtoawrongclassification." ></td>
	<td class="line x" title="153:199	However, inourgraph-basedMincutframework, the vertex religious might link to other vertices (for example, it links to the vertex scrupulous in the unlabeled data by the relation similar-to)." ></td>
	<td class="line x" title="154:199	The mincut algorithm will put vertices religious and scrupulousinthesamecut(subjectivecategory)as this results in the least cost 0.93 (ignoring the cost of assigning the unrelated sense of flicker)." ></td>
	<td class="line x" title="155:199	In other words, the edges between the vertices are likely to correct some initially wrong classification and pull the vertices into the right cuts." ></td>
	<td class="line x" title="156:199	In the following we will analyze the best minimum cut algorithm LRM-SL in more detail." ></td>
	<td class="line x" title="157:199	We measure its accuracy for each part-of-speech in the Micro-WNOp dataset." ></td>
	<td class="line x" title="158:199	The number of noun, adjective, adverb and verb senses in Micro-WNOp is 484, 265, 31 and 281, respectively." ></td>
	<td class="line x" title="159:199	The result is listed in Table 3." ></td>
	<td class="line x" title="160:199	The significantly better performance of semi-supervised mincuts holds across all parts-ofspeech but the small set of adverbs, where there is no significant difference between the baseline, SVM and the Mincut algorithm." ></td>
	<td class="line x" title="161:199	6 Table 3: Accuracy for Different Part-Of-Speech Method Noun Adjective Adverb Verb Baseline 76.9% 61.1% 77.4% 72.6% SVM 81.4% 63.4% 83.9% 75.1% Mincut 88.6% 78.9% 77.4% 84.0% We will now investigate how LRM-SL performs with different sizes of labeled and unlabeled data." ></td>
	<td class="line x" title="162:199	All learning curves are generated via averaging 10 learning curves from 10-fold cross-validation." ></td>
	<td class="line x" title="163:199	Performance with different sizes of labeled data: we randomly generate subsets of labeled data A1, A2 An, and guarantee that A1  A2  An." ></td>
	<td class="line x" title="164:199	Results for the best SVM (LRM) and the best minimum cut (LRM-SL) are listed in Table 4, and the corresponding learning curve is shown in Figure 2." ></td>
	<td class="line x" title="165:199	As can be seen, the semi-supervised Mincuts is consistently better than SVM." ></td>
	<td class="line x" title="166:199	Moreover, the semisupervised Mincut with only 200 labeled data items performs even better than SVM with 954 training items (78.9% vs 75.3%), showing that our semisupervised framework allows for a training data reduction of more than 80%." ></td>
	<td class="line x" title="167:199	Table 4: Accuracy with different sizes of labeled data # labeled data SVM Mincuts 100 69.1% 72.2% 200 72.6% 78.9% 400 74.4% 82.7% 600 75.5% 83.7% 800 76.0% 84.1% 900 75.6% 84.8% 954 (all) 75.3% 84.6% Performance with different sizes of unlabeled data: We propose two different settings." ></td>
	<td class="line x" title="168:199	Option1: Use a subset of the ten relations to generate the unlabeled data (and edges between example vertices)." ></td>
	<td class="line x" title="169:199	For example, we first use {antonym, similar-to} only to obtain a unlabeled dataset U1, then use a larger subset of the relations like {antonym, similar-to, direct-hyponym, directhypernym} to generate another unlabeled dataset U2, and so forth." ></td>
	<td class="line x" title="170:199	Obviously, Ui is a subset of Ui+1." ></td>
	<td class="line x" title="171:199	Option2: Use all the ten relations to generate the unlabeled data U. We then randomly select subsets of U, such as subset U1, U2 and U3, and guarantee that U1  U2  U3  U.  68  71  74  77  80  83  86  89  100  200  300  400  500  600  700  800  900  1000 Accuracy(%) Size of Labeled Data Mincuts SVM Figure 2: Learning curve with different sizes of labeled data The results are listed in Table 5 and Table 6 respectively." ></td>
	<td class="line x" title="172:199	The corresponding learning curves are shown in Figure 3." ></td>
	<td class="line x" title="173:199	We see that performance improves with the increase of unlabeled data." ></td>
	<td class="line x" title="174:199	In addition, the curves seem to converge when the size of unlabeled data is larger than 3,000." ></td>
	<td class="line x" title="175:199	From the results in Tabel 5 one can also see that hyponymy is the relation accounting for the largest increase." ></td>
	<td class="line x" title="176:199	Table 6: Accuracy with different sizes of unlabeled data (random selection) # unlabeled data Accuracy 0 75.9% 200 76.5% 500 78.6% 1000 80.2% 2000 82.8% 3000 84.0% 3220 84.6% Furthermore, these results also show that a supervised mincut without unlabeled data performs only on a par with other supervised classifiers (75.9%)." ></td>
	<td class="line x" title="177:199	The reason is that if we exclude the unlabeled data, there are only 67 WordNet relations/edges between senses in the small Micro-WNOp dataset." ></td>
	<td class="line x" title="178:199	In contrast, the use of unlabeled data adds more edges (4,586) to the graph, which strongly affects the graph cut partition (see also Figure 1)." ></td>
	<td class="line x" title="179:199	4.5 Comparison to Prior Approaches In our previous work (Su and Markert, 2008), we report 76.9% as the best accuracy on the same Micro7 Table 5: Accuracy with different sizes of unlabeled data from WordNet relation Relation # unlabeled data Accuracy {} 0 75.3% {similar-to} 418 79.1% {similar-to, antonym} 514 79.5% {similar-to, antonym, direct-hypernym, directhyponym} 2,721 84.4% {similar-to, antonym, direct-hypernym, directhyponym, also-see, extended-antonym} 3,004 84.4% {similar-to, antonym, direct-hypernym, directhyponym, also-see, extended-antonym, derived-from, attribute, domain, domain-member} 3,220 84.6%  75  77  79  81  83  85  87  89  0  500  1000  1500  2000  2500  3000  3500 Accuracy(%) Size of Unlabeled Data Option1Option2 Figure3: Learningcurvewithdifferentsizesofunlabeled data WNOp dataset used in the previous sections, using a supervised Naive Bayes (S&M in Tabel 2)." ></td>
	<td class="line x" title="180:199	Our best result from Mincuts is significantly better at 84.6% (see LRM-SL in Table 2)." ></td>
	<td class="line x" title="181:199	For comparison to Wiebe and Mihalcea (2006), we use their dataset for testing, henceforth called Wiebe (see Section 4.1 for a description)." ></td>
	<td class="line x" title="182:199	Wiebe and Mihalcea (2006) report their results in precision andrecallcurvesforsubjectivesenses,suchasaprecision of about 55% at a recall of 50% for subjective senses." ></td>
	<td class="line x" title="183:199	Their F-score for subjective senses seems to remain relatively static at 0.52 throughout their precision/recall curve." ></td>
	<td class="line x" title="184:199	We run our best Mincut LRM-SL algorithm with two different settings on Wiebe." ></td>
	<td class="line x" title="185:199	Using MicroWNOp as training set and Wiebe as test set, we achieveanaccuracyof83.2%,whichissimilartothe results on the Micro-WNOp dataset." ></td>
	<td class="line x" title="186:199	At the recall of 50% we achieve a precision of 83.6% (in comparisontotheirprecisionof55%atthesamerecall)." ></td>
	<td class="line x" title="187:199	Our F-score is 0.63 (vs. 0.52)." ></td>
	<td class="line x" title="188:199	Tocheckwhetherthehighperformanceisjustdue to our larger training set, we also conduct 10-fold cross-validation on Wiebe." ></td>
	<td class="line x" title="189:199	The accuracy achieved is 81.1% and the F-score 0.56 (vs. 0.52), suggesting that our algorithm performs better." ></td>
	<td class="line x" title="190:199	Our algorithm can be used on all WordNet senses whereas theirs is restricted to senses that have distributionally similar words in the MPQA corpus (see Section 2)." ></td>
	<td class="line x" title="191:199	However, they use an unsupervised algorithm i.e. they do not need labeled word senses, although they do need a large, manually annotated opinion corpus." ></td>
	<td class="line x" title="192:199	5 Conclusion and Future Work We propose a semi-supervised minimum cut algorithm for subjectivity recognition on word senses." ></td>
	<td class="line x" title="193:199	The experimental results show that our proposed approach is significantly better than a standard supervised classification framework as well as a supervised Mincut." ></td>
	<td class="line x" title="194:199	Overall, we achieve a 40% reduction in error rates (from an error rate of about 25% to an error rate of 15%)." ></td>
	<td class="line x" title="195:199	To achieve the results of standard supervised approaches with our model, we need less than20%oftheirtrainingdata." ></td>
	<td class="line x" title="196:199	Inaddition, wecompare our algorithm to previous state-of-the-art approaches, showing that our model performs better on the same datasets." ></td>
	<td class="line x" title="197:199	Future work will explore other graph construction methods, such as the use of morphological relations as well as thesaurus and distributional similarity measures." ></td>
	<td class="line x" title="198:199	We will also explore other semisupervised algorithms." ></td>
	<td class="line x" title="199:199	8" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-1031
Predicting Risk from Financial Reports with Regression
Kogan, Shimon;Levin, Dimitry;Routledge, Bryan R.;Sagi, Jacob S.;Smith, Noah A.;"></td>
	<td class="line x" title="1:185	Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 272280, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:185	c 2009 Association for Computational Linguistics Predicting Risk from Financial Reports with Regression Shimon Kogan McCombs School of Business University of Texas at Austin Austin, TX 78712, USA shimon.kogan@mccombs.utexas.edu Dimitry Levin Mellon College of Science Carnegie Mellon University Pittsburgh, PA 15213, USA dimitrylevin@gmail.com Bryan R. Routledge Tepper School of Business Carnegie Mellon University Pittsburgh, PA 15213, USA routledge@cmu.edu Jacob S. Sagi Owen Graduate School of Management Vanderbilt University Nashville, TN 37203, USA Jacob.Sagi@Owen.Vanderbilt.edu Noah A. Smith School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA nasmith@cs.cmu.edu Abstract We address a text regression problem: given a piece of text, predict a real-world continuous quantity associated with the texts meaning." ></td>
	<td class="line x" title="3:185	In this work, the text is an SEC-mandated financial report published annually by a publiclytraded company, and the quantity to be predicted is volatility of stock returns, an empirical measure of financial risk." ></td>
	<td class="line x" title="4:185	We apply wellknown regression techniques to a large corpus of freely available financial reports, constructing regression models of volatility for the period following a report." ></td>
	<td class="line x" title="5:185	Our models rival past volatility (a strong baseline) in predicting the target variable, and a single model that uses both can significantly outperform past volatility." ></td>
	<td class="line x" title="6:185	Interestingly, our approach is more accurate for reports after the passage of the Sarbanes-Oxley Act of 2002, giving some evidence for the success of that legislation in making financial reports more informative." ></td>
	<td class="line x" title="7:185	1 Introduction We consider a text regression problem: given a piece of text, predict a R-valued quantity associated with that text." ></td>
	<td class="line x" title="8:185	Specifically, we use a companys annual financial report to predict the financial risk of investment in that company, as measured empirically by a quantity known as stock return volatility." ></td>
	<td class="line x" title="9:185	Predicting financial risk is of clear interest to anyone who invests money in stocks and central to modern portfolio choice." ></td>
	<td class="line x" title="10:185	Financial reports are a government-mandated artifact of the financial world thatone might hypothesizecontain a large amount of information about companies and their value." ></td>
	<td class="line x" title="11:185	Indeed, it is an important question whether mandated disclosures are informative, since they are meant to protect investors but are costly to produce." ></td>
	<td class="line x" title="12:185	The intrinsic properties of the problem are attractive as a test-bed for NLP research." ></td>
	<td class="line x" title="13:185	First, there is no controversy about the usefulness or existential reality of the output variable (volatility)." ></td>
	<td class="line x" title="14:185	Statistical NLP often deals in the prediction of variables ranging from text categories to linguistic structures to novel utterances." ></td>
	<td class="line x" title="15:185	While many of these targets are uncontroversially useful, they often suffer from evaluation difficulties and disagreement among annotators." ></td>
	<td class="line x" title="16:185	The output variable in this work is a statistic summarizing facts about the real world; it is not subject to any kind of human expertise, knowledge, or intuition." ></td>
	<td class="line x" title="17:185	Hence this prediction task provides a new, objective test-bed for any kind of linguistic analysis." ></td>
	<td class="line x" title="18:185	Second, many NLP problems rely on costly annotated resources (e.g., treebanks or aligned bilingual corpora)." ></td>
	<td class="line x" title="19:185	Because the text and historical financial data used in this work are freely available (by law) and are generated as a by-product of the American 272 economy, old and new data can be obtained by anyone with relatively little effort." ></td>
	<td class="line x" title="20:185	In this paper, we demonstrate that predicting financial volatility automatically from a financial report is a novel, challenging, and easily evaluated natural language understanding task." ></td>
	<td class="line x" title="21:185	We show that a very simple representation of the text (essentially, bags of unigrams and bigrams) can rival and, in combination, improve over a strong baseline that does not use the text." ></td>
	<td class="line x" title="22:185	Analysis of the learned models provides insights about what can make this problem more or less difficult, and suggests that disclosurerelated legislation led to more transparent reporting." ></td>
	<td class="line x" title="23:185	2 Stock Return Volatility Volatility is often used in finance as a measure of risk." ></td>
	<td class="line x" title="24:185	It is measured as the standard deviation of a stocks returns over a finite period of time." ></td>
	<td class="line x" title="25:185	A stock will have high volatility when its price fluctuates widely and low volatility when its price remains more or less constant." ></td>
	<td class="line x" title="26:185	Let rt = PtPt1 1 be the return on a given stock between the close of trading day t1 and day t, where Pt is the (dividend-adjusted) closing stock price at datet." ></td>
	<td class="line x" title="27:185	The measured volatility over the time period from day t to day t is equal to the sample s.d.: v[t,t] = radicaltpradicalvertex radicalvertexradicalbt summationdisplay i=0 (rtir)2 slashBigg  (1) where r is the sample mean of rt over the period." ></td>
	<td class="line x" title="28:185	In this work, the above estimate will be treated as the true output variable on training and testing data." ></td>
	<td class="line x" title="29:185	It is important to note that predicting volatility is not the same as predicting returns or value." ></td>
	<td class="line x" title="30:185	Rather than trying to predict how well a stock will perform, we are trying to predict how stable its price will be over a future time period." ></td>
	<td class="line x" title="31:185	It is, by now, received wisdom in the field of economics that predicting a stocks performance, based on easily accessible public information, is difficult." ></td>
	<td class="line x" title="32:185	This is an attribute of well-functioning (or efficient) markets and a cornerstone of the so-called efficient market hypothesis (Fama, 1970)." ></td>
	<td class="line x" title="33:185	By contrast, the idea that one can predict a stocks level of risk using public information is uncontroversial and a basic assumption made by many economically sound pricing models." ></td>
	<td class="line x" title="34:185	A large body of research in finance suggests that the two types of quantities are very different: while predictability of returns could be easily traded away by the virtue of buying/selling stocks that are underor over-valued (Fama, 1970), similar trades are much more costly to implement with respect to predictability of volatility (Dumas et al., 2007)." ></td>
	<td class="line x" title="35:185	By focusing on volatility prediction, we avoid taking a stance on whether or not the United States stock market is informationally efficient." ></td>
	<td class="line x" title="36:185	3 Problem Formulation Given a text document d, we seek to predict the value of a continuous variable v. We do this via a parameterized function f: v = f(d;w) (2) where w Rd are the parameters or weights." ></td>
	<td class="line x" title="37:185	Our approach is to learn a human-interpretable w from a collection of N training examples {di,vi}Ni=1, where each di is a document and each viR. Support vector regression (Drucker et al., 1997) is a well-known method for training a regression model." ></td>
	<td class="line x" title="38:185	SVR is trained by solving the following optimization problem: min wRd 1 2bardblwbardbl 2+C N Nsummationdisplay i=1 max parenleftBig 0, vextendsinglevextendsingle vextendsinglevif(di;w) vextendsinglevextendsingle vextendsingleepsilon1 parenrightBig bracehtipupleft bracehtipdownrightbracehtipdownleft bracehtipupright epsilon1-insensitive loss function(3) where C is a regularization constant and epsilon1 controls the training error.1 The training algorithm finds weights w that define a function f minimizing the (regularized) empirical risk." ></td>
	<td class="line x" title="39:185	Let h be a function from documents into some vector-space representationRd. In SVR, the function f takes the form: f(d;w) = h(d)latticetopw = Nsummationdisplay i=1 iK(d,di) (4) where Equation 4 re-parameterizes f in terms of a kernel function K with dual weights i. K can 1Given the embedding h of documents in Rd, epsilon1 defines a slab (region between two parallel hyperplanes, sometimes called the epsilon1-tube) in Rd+1 through which each h(di),f(di;w) must pass in order to have zero loss." ></td>
	<td class="line x" title="40:185	273 year words documents words/doc." ></td>
	<td class="line x" title="41:185	1996 5.5M 1,408 3,893 1997 9.3M 2,260 4,132 1998 11.8M 2,462 4,808 1999 14.5M 2,524 5,743 2000 13.4M 2,425 5,541 2001 15.4M 2,596 5,928 2002 22.7M 2,846 7,983 2003 35.3M 3,612 9,780 2004 38.9M 3,559 10,936 2005 41.9M 3,474 12,065 2006 38.8M 3,308 11,736 total 247.7M 26,806 9,240 Table 1: Dimensions of the dataset used in this paper, after filtering and tokenization." ></td>
	<td class="line x" title="42:185	The near doubling in average document size during 20023 is possibly due to the passage of the Sarbanes-Oxley Act of 2002 in the wake of Enrons accounting scandal (and numerous others)." ></td>
	<td class="line x" title="43:185	be seen as a similarity function between two documents." ></td>
	<td class="line x" title="44:185	At test time, a new example is compared to a subset of the training examples (those with inegationslash= 0); typically with SVR this set is sparse." ></td>
	<td class="line x" title="45:185	With the linear kernel, the primal and dual weights relate linearly: w = Nsummationdisplay i=1 ih(di) (5) The full details of SVR and its implementation are beyond the scope of this paper; interested readers are referred to Scholkopf and Smola (2002)." ></td>
	<td class="line x" title="46:185	SVMlight (Joachims, 1999) is a freely available implementation of SVR training that we used in our experiments.2 4 Dataset In the United States, the Securities Exchange Commission mandates that all publicly-traded corporations produce annual reports known as Form 10K. The report typically includes information about the history and organization of the company, equity and subsidiaries, as well as financial information." ></td>
	<td class="line x" title="47:185	These reports are available to the public and published on the SECs web site.3 The structure of the 10-K is specified in detail in the legislation." ></td>
	<td class="line x" title="48:185	We have collected 54,379 reports published over the period 2Available at http://svmlight.joachims.org." ></td>
	<td class="line x" title="49:185	3http://www.sec.gov/edgar.shtml 19962006 from 10,492 different companies." ></td>
	<td class="line x" title="50:185	Each report comes with a date of publication, which is important for tying the text back to the financial variables we seek to predict." ></td>
	<td class="line x" title="51:185	From the perspective of predicting future events, one section of the 10-K is of special interest: Section 7, known as managements discussion and analysis of financial conditions and results of operations (MD&A), and in particular Subsection 7A, quantitative and qualitative disclosures about market risk. Because Section 7 is where the most important forward-looking content is most likely to be found, we filter other sections from the reports." ></td>
	<td class="line x" title="52:185	The filtering is done automatically using a short, hand-written Perl script that seeks strings loosely matching the Section 7, 7A, and 8 headers, finds the longest reasonable Section 7 match (in words) of more than 1,000 whitespace-delineated tokens." ></td>
	<td class="line x" title="53:185	Section 7 typically begins with an introduction like this (from ABCs 1998 Form 10-K, before tokenization for readability; boldface added): The following discussion and analysis of ABCs consolidated financial condition and consolidated results of operation should be read in conjunction with ABCs Consolidated Financial Statements and Notes thereto included elsewhere herein." ></td>
	<td class="line x" title="54:185	This discussion contains certain forward-looking statements which involve risks and uncertainties." ></td>
	<td class="line x" title="55:185	ABCs actual results could differ materially from the results expressed in, or implied by, such statements." ></td>
	<td class="line x" title="56:185	See Regarding ForwardLooking Statements. Not all of the documents downloaded pass the filter at all, and for the present work we have only used documents that do pass the filter." ></td>
	<td class="line x" title="57:185	(One reason for the failure of the filter is that many 10-K reports include Section 7 by reference, so the text is not directly included in the document.)" ></td>
	<td class="line x" title="58:185	In addition to the reports, we used the Center for Research in Security Prices (CRSP) US Stocks Database to obtain the price return series along with other firm characteristics.4 We proceeded to calculate two volatilities for each firm/report observation: the twelve months prior to the report (v(12)) and the twelve months after the report (v(+12))." ></td>
	<td class="line x" title="59:185	4The text and volatility data are publicly available at http: //www.ark.cs.cmu.edu/10K." ></td>
	<td class="line x" title="60:185	274 Tokenization was applied to the text, including punctuation removal, downcasing, collapsing all digit sequences,5 and heuristic removal of remnant markup." ></td>
	<td class="line x" title="61:185	Table 1 gives statistics on the corpora used in this research; this is a subset of the corpus for which there is no missing volatility information." ></td>
	<td class="line x" title="62:185	The drastic increase in length during the 2002 2003 period might be explained by the passage by the US Congress of the Sarbanes-Oxley Act of 2002 (and related SEC and exchange rules), which imposed revised standards on reporting practices of publicly-traded companies in the US." ></td>
	<td class="line x" title="63:185	5 Baselines and Evaluation Method Volatility displays an effect known as autoregressive conditional heteroscedasticity (Engle, 1982)." ></td>
	<td class="line x" title="64:185	This means that the variance in a stocks return tends to change gradually." ></td>
	<td class="line x" title="65:185	Large changes in price are presaged by other changes, and periods of stability tend to continue." ></td>
	<td class="line x" title="66:185	Volatility is, generally speaking, not constant, yet prior volatility (e.g., v(12)) is a very good predictor of future volatility (e.g., v(+12))." ></td>
	<td class="line x" title="67:185	At the granularity of a year, which we consider here because the 10-K reports are annual, there are no existing models of volatility that are widely agreed to be significantly more accurate than our historical volatility baseline." ></td>
	<td class="line x" title="68:185	We tested a state-of-theart model known as GARCH(1,1) (Engle, 1982; Bollerslev, 1986) and found that it was no stronger than our historical volatility baseline on this sample." ></td>
	<td class="line x" title="69:185	Throughout this paper, we will report performance using the mean squared error between the predicted and true log-volatilities:6 MSE = 1Nprime Nprimesummationdisplay i=1 (log(vi)log(vi))2 (6) where Nprime is the size of the test set, given in Table 1." ></td>
	<td class="line x" title="70:185	6 Experiments In our experiments, we vary h (the function that maps inputs to a vector space) and the subset of the 5While numerical information is surely informative about risk, recall that our goal is to find indicators of risk expressed in the text; automatic predictors of risk from numerical data would use financial data streams directly, not text reports." ></td>
	<td class="line x" title="71:185	6We work in the log domain because it is standard in finance, due to the dynamic range of actual volatilities; the distribution over logv across companies tends to have a bell shape." ></td>
	<td class="line x" title="72:185	data used for training." ></td>
	<td class="line x" title="73:185	We will always report performance over test sets consisting of one years worth of data (the subcorpora described in Table 1)." ></td>
	<td class="line x" title="74:185	In this work, we focus on predicting the volatility over the year following the report (v(+12))." ></td>
	<td class="line x" title="75:185	In all experiments, epsilon1 = 0.1 and C is set using the default choice of SVMlight, which is the inverse of the average of h(d)latticetoph(d) over the training data.7 6.1 Feature Representation We first consider how to represent the 10-K reports." ></td>
	<td class="line x" title="76:185	We adopt various document representations, all using word features." ></td>
	<td class="line x" title="77:185	Let M be the vocabulary size derived from the training data.8 Let freq(xj;d) denote the number of occurrences of the jth word in the vocabulary in document d.  TF: hj(d) = 1|d|freq(xj;d),j{1,,M}." ></td>
	<td class="line x" title="78:185	 TFIDF: hj(d) = 1|d|freq(xj;d)log(N/|{d : freq(xj;d) > 0}|), where N is the number of documents in the training set." ></td>
	<td class="line x" title="79:185	This is the classic TFIDF score." ></td>
	<td class="line x" title="80:185	 LOG1P: hj(d) = log(1 + freq(xj;d))." ></td>
	<td class="line x" title="81:185	Rather than normalizing word frequencies as for TF, this score dampens them with a logarithm." ></td>
	<td class="line x" title="82:185	We also include a variant of LOG1P where terms are the union of unigrams and bigrams." ></td>
	<td class="line x" title="83:185	Note that each of these preserves sparsity; when freq(xj;d) = 0, hj(d) = 0 in all cases." ></td>
	<td class="line x" title="84:185	For interpretability of results, we use a linear kernel." ></td>
	<td class="line x" title="85:185	The usual bias weight b is included." ></td>
	<td class="line x" title="86:185	We found it convenient to work in the logarithmic domain for the predicted variable, predicting logv instead of v, since volatility is always nonnegative." ></td>
	<td class="line x" title="87:185	In this setting, the predicted volatility takes the form: log v = b+ Msummationdisplay j=1 wjhj(d) (7) Because the goal of this work is to explore how text might be used to predict volatility, we also wish 7These values were selected after preliminary and cursory exploration with 19962000 as training data and 2001 as the test set." ></td>
	<td class="line x" title="88:185	While the effects of epsilon1 and C were not large, further improvements may be possible with more careful tuning." ></td>
	<td class="line x" title="89:185	8Preliminary experiments that filtered common or rare words showed a negligible or deleterious effect on performance." ></td>
	<td class="line x" title="90:185	275 features 2001 2002 2003 2004 2005 2006 micro-ave. history v(12) (baseline) 0.1747 0.1600 0.1873 0.1442 0.1365 0.1463 0.1576 v(12) (SVR with bias) 0.2433 0.4323 0.1869 0.2717 0.3184 5.6778 1.2061 v(12) (SVR without bias) 0.2053 0.1653 0.2051 0.1337 0.1405 0.1517 0.1655 words TF 0.2219 0.2571 0.2588 0.2134 0.1850 0.1862 0.2197 TFIDF 0.2033 0.2118 0.2178 0.1660 0.1544 0.1599 0.1842 LOG1P 0.2107 0.2214 0.2040 0.1693 0.1581 0.1715 0.1873 LOG1P, bigrams 0.1968 0.2015 0.1729 0.1500 0.1394 0.1532 0.1667 both TF+ 0.1885 0.1616 0.1925 0.1230 0.1272 0.1402 0.1541 TFIDF+ 0.1919 0.1618 0.1965 0.1246 0.1276 0.1403 0.1557 LOG1P+ 0.1846 0.1764 0.1671 0.1309 0.1319 0.1458 0.1542 LOG1P+, bigrams 0.1852 0.1792 0.1599 0.1352 0.1307 0.1448 0.1538 Table 2: MSE (Eq." ></td>
	<td class="line x" title="91:185	6) of different models on test data predictions." ></td>
	<td class="line x" title="92:185	Lower values are better." ></td>
	<td class="line x" title="93:185	Boldface denotes improvements over the baseline, and  denotes significance compared to the baseline under a permutation test (p < 0.05)." ></td>
	<td class="line x" title="94:185	to see whether text adds information beyond what can be predicted using historical volatility alone (the baseline, v(12))." ></td>
	<td class="line x" title="95:185	We therefore consider models augmented with an additional feature, defined as hM+1 = logv(12)." ></td>
	<td class="line x" title="96:185	Since this is historical information, it is always available when the 10-K report is published." ></td>
	<td class="line x" title="97:185	These models are denoted TF+, TFIDF+, and LOG1P+." ></td>
	<td class="line x" title="98:185	The performance of these models, compared to the baseline from Section 5, is shown in Table 2." ></td>
	<td class="line x" title="99:185	We used as training examples all reports from the five-year period preceding the test year (so six experiments on six different training and test sets are shown in the figure)." ></td>
	<td class="line x" title="100:185	We also trained SVR models on the single feature v(12), with and without bias weights (b in Eq." ></td>
	<td class="line x" title="101:185	7); these are usually worse and never signficantly better than the baseline." ></td>
	<td class="line x" title="102:185	Strikingly, the models that use only the text to predict volatility come very close to the historical baseline in some years." ></td>
	<td class="line x" title="103:185	That a text-only method (LOG1P with bigrams) for predicting future risk comes within 5% of the error of a strong baseline (20036) shows promise for the overall approach." ></td>
	<td class="line x" title="104:185	A combined model improves substantially over the baseline in four out of six years (20036), and this difference is usually robust to the representation used." ></td>
	<td class="line x" title="105:185	Table 3 shows the most strongly weighted terms in each of the text-only LOG1P models (including bigrams)." ></td>
	<td class="line x" title="106:185	These weights are recovered using the relationship expressed in Eq." ></td>
	<td class="line x" title="107:185	5." ></td>
	<td class="line x" title="108:185	6.2 Training Data Effects It is well known that more training data tend to improve the performance of a statistical method; however, the standard assumption is that the training data are drawn from the same distribution as the test data." ></td>
	<td class="line x" title="109:185	In this work, where we seek to predict the future based on data from past, that assumption is obviously violated." ></td>
	<td class="line x" title="110:185	It is therefore an open question whether more data (i.e., looking farther into the past) is helpful for predicting volatility, or whether it is better to use only the most recent data." ></td>
	<td class="line x" title="111:185	Table 4 shows how performance varies when one, two, or five years of historical training data are used, averaged across test years." ></td>
	<td class="line x" title="112:185	In most cases, using more training data (from a longer historical period) is helpful, but not always." ></td>
	<td class="line x" title="113:185	One interesting trend, not shown in the aggregate statistics of Table 4, is that recency of the training set affected performance much more strongly in earlier train/test splits (20013) than later ones (20046)." ></td>
	<td class="line x" title="114:185	This experiment leads us to conclude that temporal changes in financial reporting make training data selection nontrivial." ></td>
	<td class="line x" title="115:185	Changes in the macro economy and specific businesses make older reports less relevant for prediction." ></td>
	<td class="line x" title="116:185	For example, regulatory changes like Sarbanes-Oxley, variations in the business cycle, and technological innovation like the Internet influence both the volatility and the 10-K text." ></td>
	<td class="line x" title="117:185	6.3 Effects of Sarbanes-Oxley We noted earlier that the passage of the SarbanesOxley Act of 2002, which sought to reform financial reporting, had a clear effect on the lengths of the 10-K reports in our collection." ></td>
	<td class="line x" title="118:185	But are the reports more informative?" ></td>
	<td class="line x" title="119:185	This question is important, because producing reports is costly; we present an empirical argument based on our models that the legis276 19962000 19972001 19982002 19992003 20002004 20012005 net loss 0.026 year # 0.028 loss 0.023 loss 0.026 loss 0.025 loss 0.026 high v year # 0.024 net loss 0.023 net loss 0.020 net loss 0.020 net loss 0.017 net loss 0.018  loss 0.020 expenses 0.020 expenses 0.017 expenses 0.017 year # 0.016 going concern 0.014 expenses 0.019 loss 0.020 year # 0.015 going concern 0.015 expenses 0.015 expenses 0.014 co venants 0.017 experienced 0.015 oblig ations 0.015 year # 0.015 going concern 0.014 agoing 0.014 diluted 0.014 of $# 0.015 financing 0.014 financing 0.014 agoing 0.013 personnel 0.013 con vertible 0.014 co venants 0.015 con vertible 0.014 agoing 0.014 administrati ve 0.013 financing 0.013 date 0.014 additional 0.014 additional 0.014 additional 0.013 personnel 0.013 administrati ve 0.012 longterm -0.014 mer ger agreement -0.015 unsecured -0.012 distrib utions -0.012 distrib utions -0.011 policies -0.011 rates -0.015 dividends -0.015 earnings -0.012 annual -0.012 insurance -0.011 by the -0.011 dividend -0.015 unsecured -0.017 distrib utions -0.012 dividend -0.012 critical accounting -0.012 earnings -0.011 unsecured -0.015 dividend -0.017 dividends -0.015 dividends -0.012 lower interest -0.012 dividends -0.012 mer ger agreement -0.017 properties -0.018 income -0.016 rates -0.013 dividends -0.013 unsecured -0.012 properties -0.018 net income -0.019 properties -0.016 properties -0.015 properties -0.014 properties -0.013 income -0.021 income -0.021 net income -0.019 rate -0.019 rate -0.017 rate -0.014  rate -0.022 rate -0.025 rate -0.022 net income -0.023 net income -0.021 net income -0.018 low v Table 3: Most strongly-weighted terms in models learned from various time periods (LO G1 Pmodel with unigrams and bigrams)." ></td>
	<td class="line x" title="120:185	# denotes an ydigit sequence." ></td>
	<td class="line x" title="121:185	features 1 2 5 TF+ 0.1509 0.1450 0.1541 TFIDF+ 0.1512 0.1455 0.1557 LOG1P+ 0.1621 0.1611 0.1542 LOG1P+, bigrams 0.1617 0.1588 0.1538 Table 4: MSE of volatility predictions using reports from varying historical windows (1, 2, and 5 years), microaveraged across six train/test scenarios." ></td>
	<td class="line x" title="122:185	Boldface marks best in a row." ></td>
	<td class="line x" title="123:185	The historical baseline achieves 0.1576 MSE (see Table 2)." ></td>
	<td class="line x" title="124:185	lation has actually been beneficial." ></td>
	<td class="line x" title="125:185	Our experimental results in Section 6.1, in which volatility in the years 20042006 was more accurately predicted from the text than in 20012002, suggest that the Sarbanes-Oxley Act led to more informative reports." ></td>
	<td class="line x" title="126:185	We compared the learned weights (LOG1P+, unigrams) between the six overlapping five-year windows ending in 20002005; measured in L1 distance, these were, in consecutive order, 52.2, 59.9, 60.7, 55.3, 52.3; the biggest differences came between 2001 and 2002 and between 2002 and 2003." ></td>
	<td class="line x" title="127:185	(Firms are most likely to have begun compliance with the new law in 2003 or 2004.)" ></td>
	<td class="line x" title="128:185	The same pattern held when only words appearing in all five models were considered." ></td>
	<td class="line x" title="129:185	Variation in the recency/training set size tradeoff (6.2), particularly during 20023, also suggests that there were substantial changes in the reports during that time." ></td>
	<td class="line x" title="130:185	6.4 Qualitative Evaluation One of the advantages of a linear model is that we can explore what each model discovers about different unigram and bigram terms." ></td>
	<td class="line x" title="131:185	Some manually selected examples of terms whose learned weights (w) show interesting variation patterns over time are shown in Figure 1, alongside term frequency patterns, for the text-only LOG1P model (with bigrams)." ></td>
	<td class="line x" title="132:185	These examples were suggested by experts in finance from terms with weights that were both large and variable (across training sets)." ></td>
	<td class="line x" title="133:185	A particularly interesting case, in light of Sarbanes-Oxley, is the term accounting policies." ></td>
	<td class="line x" title="134:185	Sarbanes-Oxley mandated greater discussion of accounting policy in the 10-K MD&A section." ></td>
	<td class="line x" title="135:185	Before 2002 this term indicates high volatility, perhaps due to complicated off-balance sheet transactions or unusual accounting policies." ></td>
	<td class="line x" title="136:185	Starting in 2002, explicit mention of accounting policies indi277 0 0.2 0.4 0.6 0.8 ave. term frequency -0.015 -0.010 -0.005 0 0.005 w accounting policies estimates -0.010 -0.005 0 0.005 w reit mortgages -0.010 -0.005 0 0.005 0.010 96-00 97-01 98-02 99-03 00-04 01-05 w higher margin lower margin 0 0.05 0.10 0.15 0.20 ave. term frequency 0 2 4 6 8 ave. term frequency Figure 1: Left: learned weights for selected terms across models trained on data from different time periods (x-axis)." ></td>
	<td class="line x" title="137:185	These weights are from the LOG1P (unigrams and bigrams) models trained on five-year periods, the same models whose extreme weights are summarized in Tab." ></td>
	<td class="line x" title="138:185	3." ></td>
	<td class="line x" title="139:185	Note that all weights are within 00.026." ></td>
	<td class="line x" title="140:185	Right: the terms average frequencies (by document) over the same periods." ></td>
	<td class="line x" title="141:185	cates lower volatility." ></td>
	<td class="line x" title="142:185	The frequency of the term also increases drastically over the same period, suggesting that the earlier weights may have been inflated." ></td>
	<td class="line x" title="143:185	A more striking example is estimates, which averages one occurrence per document even in the 19962000 period, experiences the same term frequency explosion, and goes through a similar weight change, from strongly indicating high volatility to strongly indicating low volatility." ></td>
	<td class="line x" title="144:185	As a second example, consider the terms mortgages and reit (Real Estate Investment Trust, a tax designation for businesses that invest in real estate)." ></td>
	<td class="line x" title="145:185	Given the importance of the housing and mortgage market over the past few years, it is interesting to note that the weight on both of these terms increases over the period from a strong low volatility term to a weak indicator of high volatility." ></td>
	<td class="line x" title="146:185	It will be interesting to see how the dramatic decline in housing prices in late 2007, and the fallout created in credit markets in 2008, is reflected in future models." ></td>
	<td class="line x" title="147:185	Finally, notice that high margin and low margin, whose frequency patterns are fairly flat switch places, over the sample: first indicating high and low volatility, respectively, then low and high." ></td>
	<td class="line x" title="148:185	There is no a priori reason to expect high or low margins would be associated with high or low stock volatility." ></td>
	<td class="line x" title="149:185	However, this is an interesting example where bigrams are helpful (the word margin by itself is uninformative) and indicates that predicting risk is highly time-dependent." ></td>
	<td class="line x" title="150:185	6.5 Delisting An interesting but relatively infrequent phenomenon is the delisting of a company, i.e., when it ceases to be traded on a particular exchange due to dissolution after bankruptcy, a merger, or violation of exchange rules." ></td>
	<td class="line x" title="151:185	The relationship between volatility and delisting has been studied by Merton (1974), among others." ></td>
	<td class="line x" title="152:185	Our dataset includes a small number of cases where the volatility figures for the period following the publication of a 10-K report are unavailable because the company was delisted." ></td>
	<td class="line x" title="153:185	Learning to predict delisting is extremely difficult because fewer than 4% of the 20016 10-K reports precede delisting." ></td>
	<td class="line x" title="154:185	Using the LOG1P representation, we built a linear SVM classifier for each year in 20016 (trained on the five preceding years data) to predict whether a company will be delisted following its 10-K report." ></td>
	<td class="line x" title="155:185	Performance for various precision measures is shown in Table 5." ></td>
	<td class="line x" title="156:185	Notably, for 20014 we achieve 278 precision (%) at  01 02 03 04 05 06 recall = 10% 80 93 79 100 47 21 n = 5 100 100 40 100 60 80 n = 10 80 90 70 90 60 70 n = 100 38 48 53 29 24 20 oracle F1 (%) 35 42 44 36 31 16 6 bulletin, creditors, dip, otc 5 court 4 chapter, debtors, filing, prepetition 3 bankruptcy 2 concern, confirmation, going, liquidation 1 debtorinpossession, delisted, nasdaq, petition Table 5: Left: precision of delisting predictions." ></td>
	<td class="line x" title="157:185	The oracle F1 row shows the maximal F1 score obtained for any n. Right: Words most strongly predicting delisting of a company." ></td>
	<td class="line x" title="158:185	The number is how many of the six years (20016) the word is among the ten most strongly weighted." ></td>
	<td class="line x" title="159:185	There were no clear patterns across years for words predicting that a company would not be delisted." ></td>
	<td class="line x" title="160:185	The word otc refers to over-the-counter trading, a high-risk market." ></td>
	<td class="line x" title="161:185	above 75% precision at 10% recall." ></td>
	<td class="line x" title="162:185	Our best (oracle) F1 scores occur in 2002 and 2003, suggesting again a difference in reports around Sarbanes-Oxley." ></td>
	<td class="line x" title="163:185	Table 5 shows words associated with delisting." ></td>
	<td class="line x" title="164:185	7 Related Work In NLP, regression is not widely used, since most natural language-related data are discrete." ></td>
	<td class="line x" title="165:185	Regression methods were pioneered by Yang and Chute (1992) and Yang and Chute (1993) for information retrieval purposes, but the predicted continuous variable was not an end in itself in that work." ></td>
	<td class="line x" title="166:185	Blei and McAuliffe (2007) used latent topic variables to predict movie reviews and popularity from text." ></td>
	<td class="line x" title="167:185	Lavrenko et al.(2000b) and Lavrenko et al.(2000a) modeled influences between text and time series financial data (stock prices) using language models." ></td>
	<td class="line x" title="170:185	Farther afield, Albrecht and Hwa (2007) used SVR to train machine translation evaluation metrics to match human evaluation scores and compared techniques using correlation." ></td>
	<td class="line x" title="171:185	Regression has also been used to order sentences in extractive summarization (Biadsy et al., 2008)." ></td>
	<td class="line x" title="172:185	While much of the information relevant for investors is communicated through text (rather than numbers), only recently is this link explored." ></td>
	<td class="line x" title="173:185	Some papers relate news articles to earning forecasts, stock returns, volatility, and volume (Koppel and Shtrimberg, 2004; Tetlock, 2007; Tetlock et al., 2008; Gaa, 2007; Engelberg, 2007)." ></td>
	<td class="line x" title="174:185	Das and Chen (2001) and Antweiler and Frank (2004) ask whether messages posted on message boards can help explain stock performance, while Li (2005) measures the association between frequency of words associated with risk and subsequent stock returns." ></td>
	<td class="line x" title="175:185	Weiss-Hanley and Hoberg (2008) study initial public offering disclosures using word statistics." ></td>
	<td class="line oc" title="176:185	Many researchers have focused the related problem of predicting sentiment and opinion in text (Pang et al., 2002; Wiebe and Riloff, 2005), sometimes connected to extrinsic values like prediction markets (Lerman et al., 2008)." ></td>
	<td class="line x" title="177:185	In contrast to text regression, text classification comprises a widely studied set of problems involving the prediction of categorial variables related to text." ></td>
	<td class="line oc" title="178:185	Applications have included the categorization of documents by topic (Joachims, 1998), language (Cavnar and Trenkle, 1994), genre (Karlgren and Cutting, 1994), author (Bosch and Smith, 1998), sentiment (Pang et al., 2002), and desirability (Sahami et al., 1998)." ></td>
	<td class="line x" title="179:185	Text categorization has served as a test application for nearly every machine learning technique for discrete classification." ></td>
	<td class="line x" title="180:185	8 Conclusion We have introduced and motivated a new kind of task for NLP: text regression, in which text is used to make predictions about measurable phenomena in the real world." ></td>
	<td class="line x" title="181:185	We applied the technique to predicting financial volatility from companies 10-K reports, and found text regression model predictions to correlate with true volatility nearly as well as historical volatility, and a combined model to perform even better." ></td>
	<td class="line x" title="182:185	Further, improvements in accuracy and changes in models after the passage of the SarbanesOxley Act suggest that financial reporting reform has had interesting and measurable effects." ></td>
	<td class="line x" title="183:185	Acknowledgments The authors are grateful to Jamie Callan, Chester Spatt, Anthony Tomasic, Yiming Yang, and Stanley Zin for helpful discussions, and to the anonymous reviewers for useful feedback." ></td>
	<td class="line x" title="184:185	This research was supported by grants from the Institute for Quantitative Research in Finanace and from the Center for Analytical Research in Technology at the Tepper School of Business, Carnegie Mellon University." ></td>
	<td class="line x" title="185:185	279" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-1055
An Iterative Reinforcement Approach for Fine-Grained Opinion Mining
Du, Weifu;Tan, Songbo;"></td>
	<td class="line x" title="1:133	Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 486493, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:133	c 2009 Association for Computational Linguistics An Iterative Reinforcement Approach for Fine-Grained Opinion Mining  Weifu Du Haerbin Institute of Technology Haerbin, China duweifu@software.ict.ac.cn Songbo Tan Institute of Computing Technology Beijing, China tansongbo@software.ict.ac.cn  Abstract With the in-depth study of sentiment analysis research, finer-grained opinion mining, which aims to detect opinions on different review features as opposed to the whole review level, has been receiving more and more attention in the sentiment analysis research community recently." ></td>
	<td class="line x" title="3:133	Most of existing approaches rely mainly on the template extraction to identify the explicit relatedness between product feature and opinion terms, which is insufficient to detect the implicit review features and mine the hidden sentiment association in reviews, which satisfies (1) the review features are not appear explicit in the review sentences; (2) it can be deduced by the opinion words in its context." ></td>
	<td class="line x" title="4:133	From an information theoretic point of view, this paper proposed an iterative reinforcement framework based on the improved information bottleneck algorithm to address such problem." ></td>
	<td class="line x" title="5:133	More specifically, the approach clusters product features and opinion words simultaneously and iteratively by fusing both their semantic information and co-occurrence information." ></td>
	<td class="line x" title="6:133	The experimental results demonstrate that our approach outperforms the template extraction based approaches." ></td>
	<td class="line x" title="7:133	1 Introduction In the Web2.0 era, the Internet turns from a static information media into a platform for dynamic information exchanging, on which people can express their views and show their selfhood." ></td>
	<td class="line x" title="8:133	More and more people are willing to record their feelings (blog), give voice to public affairs (news review), express their likes and dislikes on products (product review), and so on." ></td>
	<td class="line x" title="9:133	In the face of the volume of sentimental information available on the Internet continues to increase, there is growing interest in helping people better find, filter, and manage these resources." ></td>
	<td class="line x" title="10:133	Automatic opinion mining (Turney et al., 2003; Ku et al., 2006; Devitt et al., 2007) can play an important role in a wide variety of more flexible and dynamic information management tasks." ></td>
	<td class="line x" title="11:133	For example, with the help of sentiment analysis system, in the field of public administration, the administrators can receive the feedbacks on one policy in a timelier manner; in the field of business, manufacturers can perform more targeted updates on products to improve the consumer experience." ></td>
	<td class="line oc" title="12:133	The research of opinion mining began in 1997, the early research results mainly focused on the polarity of opinion words (Hatzivassiloglou et al., 1997) and treated the text-level opinion mining as a classification of either positive or negative on the number of positive or negative opinion words in one text (Turney et al., 2003; Pang et al., 2002; Zagibalov et al., 2008;)." ></td>
	<td class="line x" title="13:133	With the in-depth study of opinion mining, researchers committed their efforts for more accurate results: the research of sentiment summarization (Philip et al., 2004; Hu et al., KDD 2004), domain transfer problem of the sentiment analysis (Kanayama et al., 2006; Tan et al., 2007; Blitzer et al., 2007; Tan et al., 2008; Andreevskaia et al., 2008; Tan et al., 2009) and finegrained opinion mining (Hatzivassiloglou et al., 2000; Takamura et al., 2007; Bloom et al., 2007; Wang et al., 2008; Titov et al., 2008) are the main branches of the research of opinion mining." ></td>
	<td class="line x" title="14:133	In this paper, we focus on the fine-grained (feature-level) opinion mining." ></td>
	<td class="line x" title="15:133	For many applications (e.g. the task of public affairs review analysis and the products review analysis), simply judging the sentiment orientation of a review unit is not sufficient." ></td>
	<td class="line x" title="16:133	Researchers (Kushal, 2003; Hu et al., KDD 2004; Hu et al., AAAI 2004; Popescu et al., 2005) began to work on finer-grained opinion mining which predicts the sentiment orientation related to different review features." ></td>
	<td class="line x" title="17:133	The task is known as feature-level opinion mining." ></td>
	<td class="line x" title="18:133	486 In feature-level opinion mining, most of the existing researches associate product features and opinion words by their explicit co-occurrence." ></td>
	<td class="line x" title="19:133	Template extraction based method (Popescu et al., 2005) and association rule mining based method (Hu et al., AAAI 2004) are the representative ones." ></td>
	<td class="line x" title="20:133	These approaches did good jobs for identifying the review features that appear explicitly in reviews, however, real reviews from customers are usually complicated." ></td>
	<td class="line x" title="21:133	In some cases, the review features are implicit in the review sentences, but can be deduced by the opinion words in its context." ></td>
	<td class="line x" title="22:133	The detection of such hidden sentiment association is a big challenge in feature-level opinion mining on Chinese reviews due to the nature of Chinese language (Qi et al., 2008)." ></td>
	<td class="line x" title="23:133	Obviously, neither the template extraction based method nor the association rule mining based method is effective for such cases." ></td>
	<td class="line x" title="24:133	Moreover, in some cases, even if the review features appear explicitly in the review sentences, the co-occurrence information between review features and opinion words is too quantitatively sparse to be utilized." ></td>
	<td class="line x" title="25:133	So we consider whether it is a more sensible way to construct or cluster review feature groups and opinion words groups to mine the implicit or hidden sentiment association in the reviews." ></td>
	<td class="line x" title="26:133	The general approach will cluster the two types of objects separately, which neglects the highly interrelationship." ></td>
	<td class="line x" title="27:133	To address this problem, in this paper, we propose an iterative reinforcement framework, under which we cluster product features and opinion words simultaneously and iteratively by fusing both their semantic information and sentiment link information." ></td>
	<td class="line x" title="28:133	We take improved information bottleneck algorithm (Tishby, 1999) as the kernel of the proposed framework." ></td>
	<td class="line x" title="29:133	The information bottleneck approach was presented by Tishby (1999)." ></td>
	<td class="line x" title="30:133	The basic idea of the approach is that it treats the clustering problems from the information compressing point of view, and takes this problem as a case of much more fundamental problem: what are the features of the variable X that are relevant for the prediction of another, relevance, variable Y?" ></td>
	<td class="line x" title="31:133	Based on the information theory, the problem can be formulated as: find a compressed representation of the variable X, denoted C, such that the mutual information between C and Y is as high as possible, under a constraint on the mutual information between X and C. For our case, take the hotel reviews as example, X is one type of objects of review features (e.g. facilities, service, surrounding environment, etc) or opinion words (e.g. perfect, circumspect, quiet, etc), and Y is another one." ></td>
	<td class="line x" title="32:133	Given some review features (or opinion words) gained from review corpus, we want to assemble them into categories, conserving the information about opinion words (or review features) as high as possible." ></td>
	<td class="line x" title="33:133	The information bottleneck algorithm has some benefits, mainly including (1) it treats the trade-off of precision versus complexity of clustering model through the rate distortion theory, which is a subfield of information theory; (2) it defines the distance or similarity in a well-defined way based on the mutual information." ></td>
	<td class="line x" title="34:133	The efficiency of information bottleneck algorithm (Slonim and Tishby, 2000) motivates us to take it as the kernel of our framework." ></td>
	<td class="line x" title="35:133	As far as we know, this approach has not been employed in opinion mining yet." ></td>
	<td class="line x" title="36:133	In traditional information bottleneck approach, the distance between two data objects is measured by the Jensen-Shannon divergence (Lin, 1991), which aims to measure the divergence between two probability distributions." ></td>
	<td class="line x" title="37:133	We alter this measure to integrate more semantic information, which will be illustrated in detail in the following sections, and the experimental result shows the effectiveness of the alteration." ></td>
	<td class="line x" title="38:133	It would be worthwhile to highlight several aspects of our work here: null We propose an iterative reinforcement framework, and under this framework, review feature words and opinion words are organized into categories in a simultaneous and iterative manner." ></td>
	<td class="line x" title="39:133	null In the process of clustering, the semantic information and the co-occurrence information are integrated." ></td>
	<td class="line x" title="40:133	null The experimental results on real Chinese web reviews demonstrate that proposed method outperforms the template extraction based algorithm." ></td>
	<td class="line x" title="41:133	2 Proposed Algorithm 2.1 The Problem In product reviews, opinion words are used to express opinion, sentiment or attitude of reviewers." ></td>
	<td class="line x" title="42:133	Although some review units may express general opinions toward a product, most review units are 487 regarding to specific features of the product." ></td>
	<td class="line x" title="43:133	A product is always reviewed under a certain feature set F. Suppose we have got a lexical list O which includes all the opinion expressions and their sentiment polarities." ></td>
	<td class="line x" title="44:133	For the feature-level opinion mining, identifying the sentiment association between F and O is essential." ></td>
	<td class="line x" title="45:133	The key points in the whole process are as follows: null get opinion word set O (with polarity labels) null get product feature set F null identify relationships between F and O The focus of the paper is on the latter two steps, especially for the case of hidden sentiment association that the review features are implicit in the review sentences, but can be deduced by the opinion words in its context." ></td>
	<td class="line x" title="46:133	In contrast to existing explicit adjacency approaches, the proposed approach detects the sentiment association between F and O based on review feature categories and opinion word groups gained from the review corpus." ></td>
	<td class="line x" title="47:133	To this end, we first consider two sets of association objects: the set of product feature words F = {f 1 ,f 2 ,,f m } and the set of opinion words O = {o 1 ,o 2 ,o n }." ></td>
	<td class="line x" title="48:133	A weighted bipartite graph from F and O can be built, denoted by G = {F, O, R}." ></td>
	<td class="line x" title="49:133	Here R = [r ij ] is the m*n link weight matrix containing all the pair-wise weights between set F and O. The weight can be calculated with different weighting schemes, in this paper, we set r ij  by the co-appearance frequency of f i  and o j  in clause level." ></td>
	<td class="line x" title="50:133	We take F and O as two random variables, and the question of constructing or clustering the object groups can be defined as finding compressed representation of each variable that reserves the information about another variable as high as possible." ></td>
	<td class="line x" title="51:133	Take F as an example, we want to find its compression, denoted as C, such that the mutual information between C and O is as high as possible, under a constraint on the mutual information between F and C. We propose an iterative reinforcement framework to deal with the tasks." ></td>
	<td class="line x" title="52:133	An improved information bottleneck algorithm is employed in this framework, which will be illustrated in detail in the following sections." ></td>
	<td class="line x" title="53:133	2.2 Information Bottleneck Algorithm The information bottleneck method (IB) was presented by Tishby et al.(1999)." ></td>
	<td class="line x" title="55:133	According to Shannons information theory (Cover and Thomas, 1991), for the two random variables X, Y, the mutual information I(X;Y) between the random variables X, Y is given by the symmetric functional: , (|) (;) ()(|)log () xXyY py x IXY pxpyx py  =          (1) and the mutual information between them measures the relative entropy between their joint distribution p(x, y) and the product of respective marginal distributions p(x)p(y), which is the only consistent statistical measure of the information that variable X contains about variable Y (and vice versa)." ></td>
	<td class="line x" title="56:133	Roughly speaking, some of the mutual information will be lost in the process of compression, e.g.(,) ( ,)I CY I XY  (C is a compressed representation of X)." ></td>
	<td class="line x" title="58:133	This representation is defined through a (possibly stochastic) mapping between each value x X to each representative value cC . Formally, this mapping can be characterized by a conditional distribution p(c|x), inducing a soft partitioning of X values, Specifically, each value of X _is associated with all the codebook elements (C values), with some normalized probability." ></td>
	<td class="line x" title="59:133	_ The IB method is based on the following simple idea." ></td>
	<td class="line x" title="60:133	Given the empirical joint distribution of two variables, one variable is compressed so that the mutual information about the other variable is preserved as much as possible." ></td>
	<td class="line x" title="61:133	The method can be considered as finding a minimal sufficient partition or efficient relevant coding of one variable with respect to the other one." ></td>
	<td class="line x" title="62:133	This problem can be solved by introducing a Lagrange multiplier  , and then minimizing the functional: [(|)] (, ) (,)Lpc x ICX ICY=                        (2) This solution is given in terms of the three distributions that characterize every cluster cC , the prior probability for this cluster, p(c), its membership probabilities p(c|x), and its distribution over the relevance variable p(y|c)." ></td>
	<td class="line x" title="63:133	In general, the membership probabilities, p(c|x) is soft, i.e. every x X can be assigned to every cC in some (normalized) probability." ></td>
	<td class="line x" title="64:133	The information bottleneck principle determines the distortion measure between the points x and c to be the [] (|) (|)| (|) (|)log (|) KL y py x D pyx pyc pyx pyc =  , the Kullback-Leibler divergence (Cover and Thomas, 1991) between the conditional distributions p(y|x) 488 and p(y|c)." ></td>
	<td class="line x" title="65:133	Specifically, the formal optimal solution is given by the following equations which must be solved together." ></td>
	<td class="line x" title="66:133	() (|) exp( [( |)| ( |)]) (,) 1 (|) (|)()(|) () () ( | ) () KL x x pc pc x D py x py c Zx pyc pc xpxpy x pc pc pc x px    =    =    =      (3) Where (,)Z x is a normalization factor, and the single positive (Lagrange) parameter determines the softness of the classification." ></td>
	<td class="line x" title="67:133	Intuitively, in this procedure the information contained in X about Y squeezed through a compact bottleneck of clusters C, that is forced to represent the relevant part in X w.r.t to Y. An important special case is the hard clustering case where C is a deterministic function of X. That is, p(c|x) can only take values of zero or one, This restriction, which corresponds to the limit  in Eqs 3 meaning every x X  is assigned to exactly one cluster cC  with a probability of one and to all the others with a probability of zero." ></td>
	<td class="line x" title="68:133	This yields a natural distance measure between distributions which can be easily implemented in an agglomerative hierarchical clustering procedure (Slonim and Tishby, 1999)." ></td>
	<td class="line x" title="69:133	1, (|) 0, 1 (|) (,) () () () xc xc if x c pc x otherwise pyc pxy pc pc px    =      =    =                                    (4) The algorithm starts with a trivial partitioning into |X| singleton clusters, where each cluster contains exactly one element of X. At each step we merge two components of the current partition into a single new component in a way that locally minimizes the loss of mutual information about the categories." ></td>
	<td class="line x" title="70:133	Every merger, * (, ) ij cc c , is formally defined by the following equation: * * ** * 1, (|) 0, () () (| ) (| ) (| ) () () () () () ij j i ij ij xcorxc pc x otherwise pc pc p yc pyc pyc pc pc pc pc pc   =       =+    =+         (5) The decrease in the mutual information I(C, Y) due to this merger is defined by (, ) ( ,) ( ,) i j before after I cc IC Y IC Y               (6) When (,) before I CY and (,) after I CYare the information values before and after the merger, respectively." ></td>
	<td class="line x" title="71:133	After a little algebra, one can see ( ) (, ) () ( ) (| ),(| ) ij i j JS i j I cc pc pc D pyc pyc +   (7) Where the functional D JS   is the Jensen-Shannon divergence (Lin, 1991), defined as ^^ ,||| JS i j i KL i j KL j D pp D p p D p p   =+        (8) where in our case {}{ } {} ** ^ ,(|),(|) () () ,, () () (| ) (| ) ij i j j i ij iij j pp pyc pyc pc pc pc pc ppyc pyc              =+                       (9) By introducing the information optimization criterion the resulting similarity measure directly emerges from the analysis." ></td>
	<td class="line x" title="72:133	The algorithm is now very simple." ></td>
	<td class="line x" title="73:133	At each step we perform the best possible merger, i.e. merge the clusters {, } ij cc which minimize (, ) ij I cc . 2.3 Improved Information Bottleneck Algorithm for Semantic Information In traditional information bottleneck approach, the distance between two data objects is measured by the difference of information values before and after the merger, which is measured by the JensenShannon divergence." ></td>
	<td class="line x" title="74:133	This divergence aims to measure the divergence between two probability distributions." ></td>
	<td class="line x" title="75:133	For our case, the divergence is based on the co-occurrence information between the two variables F and O. While the co-occurrence in corpus is usually quantitatively sparse; additionally, Statistics based 489 on word-occurrence loses semantic related information." ></td>
	<td class="line x" title="76:133	To avoid such reversed effects, in the proposed framework we combine the co-occurrence information and semantic information as the final distance between the two types of objects." ></td>
	<td class="line x" title="77:133	(, ) (, ) (1 )(, ) {}{} i j semantic i j ij ij ij DX X D X X IX X where X F X F X O X O   = +       (10) In equation 10, the distance between two data objects X i  and X j  is denoted as a linear combination of semantic distance and information value difference." ></td>
	<td class="line x" title="78:133	The parameter   reflects the contribution of different distances to the final distance." ></td>
	<td class="line x" title="79:133	Input: Joint probability distribution p(f,o) Output: A partition of F into m clusters, m {1,,|F|}, and a partition of O into n clusters  n{1,,|O|} 1." ></td>
	<td class="line x" title="80:133	t0 2." ></td>
	<td class="line x" title="81:133	Repeat a. Construct CF t F t  b. i, j=1,,|CF t |, i<j, calculate          (,)(1 )(,) tttt ij semantic i j i j dDcfcf Icfcf+ c. for m|CF t |-1 to 1 1) find the indices {i, j}, for which d ij t  is minimized 2) merge {cf i t , cf j t }into cf * t  3) update CF t {CF t  -{cf i t , cf j t }}U {cf * t } 4) update d ij t  costs w.r.t cf * t  d. Construct CO t O t  e. i, j=1,,|CO t |, i<j,calculate      (, )(1 )(, ) tttt ij semantic i j i j dDcoco Icoco+ f. for n|CO t |-1 to 1 1) find the indices {i, j}, for which d ij t  is minimized 2) merge {co i t ,co j t }into co * t  3) update CO t  {CO t  -{co i t ,co j t }}U {co * t } 4) update d ij t  costs w.r.t co * t  g. tt+1 3." ></td>
	<td class="line x" title="82:133	until (CF t  = CF t-1  and CO t  =CO t-1 ) Figure 1: Pseudo-code of semantic information bottleneck in iterative reinforcement framework  The semantic distance can be got by the usage of lexicon, such as WordNet (Budanitsky and Hirst, 2006)." ></td>
	<td class="line x" title="83:133	In this paper, we use the Chinese lexicon HowNet 1 . The basic idea of the iterative reinforcement principle is to propagate the clustered results between different type data objects by updating their inter-relationship spaces." ></td>
	<td class="line x" title="84:133	The clustering process can begin from an arbitrary type of data object." ></td>
	<td class="line x" title="85:133	The clustering results of one data object type update the interrelationship thus reinforce the data object categorization of another type." ></td>
	<td class="line x" title="86:133	The process is iterative until clustering results of both object types converge." ></td>
	<td class="line x" title="87:133	Suppose we begin the clustering process from data objects in set F, and then the steps can be expressed as Figure 1." ></td>
	<td class="line x" title="88:133	After the iteration, we can get the strongest n links between product feature categories and opinion word groups." ></td>
	<td class="line x" title="89:133	That constitutes our set of sentiment association." ></td>
	<td class="line x" title="90:133	3 Experimental Setup In this section we describe our experiments and the data used in these experiments." ></td>
	<td class="line x" title="91:133	3.1 Data Our experiments take hotel reviews (in Chinese) as example." ></td>
	<td class="line x" title="92:133	The corpus used in the experiments is composed by 4000 editor reviews on hotel, including 857,692 Chinese characters." ></td>
	<td class="line x" title="93:133	They are extracted from www.ctrip.com." ></td>
	<td class="line x" title="94:133	Each review contains a users rating represented by stars, the number of the star denotes the users satisfaction." ></td>
	<td class="line x" title="95:133	The detailed information is illustrated in Table 1,  Table 1: The detail information of corpus Users rating Number 1 star 555 2 star 1375 3 star 70 4 star 2000  Then we use ICTCLAS 2 , a Chinese word segmentation software to extract candidate review features and opinion words." ></td>
	<td class="line x" title="96:133	Usually, adjectives are normally used to express opinions in reviews." ></td>
	<td class="line x" title="97:133	Therefore, most of the existing researches take adjectives as opinion words." ></td>
	<td class="line x" title="98:133	In the research of Hu et al.(2004), they proposed that  1  http://www.keenage.com/ 2  www.searchforum.org.cn 490 other components of a sentence are unlikely to be product features except for nouns and noun phrases." ></td>
	<td class="line x" title="100:133	Some researchers (Fujii and Ishikawa, 2006) targeted nouns, noun phrases and verb phrases." ></td>
	<td class="line x" title="101:133	The adding of verb phrases caused the identification of more possible product features, while brought lots of noises." ></td>
	<td class="line x" title="102:133	So in this paper, we follow the points of Hus, extracting nouns and noun phrases as candidate product feature words." ></td>
	<td class="line x" title="103:133	Take the whole set of nouns and noun phrases as candidate features will bring some noise." ></td>
	<td class="line x" title="104:133	In order to reduce such adverse effects, we use the function of Named Entity Recognition (NER) in ICTCLAS to filter out named entities, including: person, location, organization." ></td>
	<td class="line x" title="105:133	Since the NEs have small probability of being product features, we prune the candidate nouns or noun phrases which have the above NE taggers." ></td>
	<td class="line x" title="106:133	Table 2: The number of candidate review features and opinion words in our corpus Extracted Instance Total NonRepeated Candidate review feature 86,623 15,249 Opinion word 26,721 1,231  By pruning candidate product feature words, we get the set of product feature words F. And the set of opinion words O is composed by all the adjectives in reviews." ></td>
	<td class="line x" title="107:133	The number of candidate product feature words and opinion words extracted from the corpus are shown as Table 2: 3.2 Experimental Procedure We evaluate our approach from two perspectives: 1) Effectiveness of product feature category construction by mutual reinforcement based clustering; 2) Precision of sentiment association between product feature categories and opinion word groups; 4 Experimental Results and Discussion 4.1 Evaluation of Review Feature Category Construction To calculate agreement between the review feature category construction results and the correct labels, we make use of the Rand index (Rand, 1971)." ></td>
	<td class="line x" title="108:133	This allows for a measure of agreement between two partitions, P 1  and P 2 , of the same data set D. Each partition is viewed as a collection of n*(n-1)/2 pair wise decisions, where n is the size of D. For each pair of points d i  and d j  in D, P i  either assigns them to the same cluster or to different clusters." ></td>
	<td class="line x" title="109:133	Let a be the number of decisions where d i  is in the same cluster as d j  in P 1  and in P 2 . Let b be the number of decisions where the two instances are placed in different clusters in both partitions." ></td>
	<td class="line x" title="110:133	Total agreement can then be calculated using 12 (, ) (1)/2 ab Rand P P nn + =                              (11) In our case, the parts of product feature words in the pre-constructed evaluation set are used to represent the data set D; a and b represent the partition agreements between the pairs of any two words in the parts and in the clustering results respectively." ></td>
	<td class="line x" title="111:133	In equation 10, the parameter  reflects the respective contribution of semantic information and co-occurrence information to the final distance." ></td>
	<td class="line x" title="112:133	When 0 = or 1 = , the co-occurrence information or the semantic information will be utilized alone." ></td>
	<td class="line x" title="113:133	In order to get the optimal combination of the two type of distance, we adjust the parameter  from 0 to 1(stepped by 0.2), and the accuracy of feature category construction with different  are shown in Figure 2:   Figure 2: The accuracy of review feature category construction with the variation of the parameter   From this figure, we can find that the semantic information ( =1) contributes much more to the accuracy of review feature category construction than the co-occurrence information (  =0), and when  =0, the approach is equivalent to the traditional information bottleneck approach." ></td>
	<td class="line x" title="114:133	We consider this is due partly to the sparseness of the cor491 pus, by enlarging the scale of the corpus or using the search engine (e.g. google etc), we can get more accurate results." ></td>
	<td class="line x" title="115:133	Additionally, by a sensible adjust on the parameter  (in this experiment, we set   as 0.6), we can get higher accuracy than the two baselines ( =0 and  =1), which indicates the necessity and effectiveness of the integration of semantic information and co-occurrence information in the proposed approach." ></td>
	<td class="line x" title="116:133	4.2 Evaluation of Sentiment Association We use precision to evaluate the performance of sentiment association." ></td>
	<td class="line x" title="117:133	An evaluation set is constructed manually first, in which there are not only the categories that every review feature word belong to, but also the relationship between each category and opinion word." ></td>
	<td class="line x" title="118:133	Then we define precision as: number of correctly associated pairs Precision number of detected pairs =                                                                           (12) A comparative result is got by the means of template-extraction based approach on the same test set." ></td>
	<td class="line x" title="119:133	By the usage of regular expression, the nouns (phrase) and gerund (phrase) are extracted as the review features, and the nearest adjectives are extracted as the related opinion words." ></td>
	<td class="line x" title="120:133	Because the modifiers of adjectives in reviews also contain rich sentiment information and express the view of customs, we extract adjectives and their modifiers simultaneously, and take them as the opinion words." ></td>
	<td class="line x" title="121:133	Table 3: Performance comparison in sentiment association Approach Pairs Precision Template extraction 27,683 65.89% Proposed approach 141,826 78.90%  Table 3 shows the advantage of our approach over the extraction by explicit adjacency." ></td>
	<td class="line x" title="122:133	Using the same product feature categorization, our sentiment association approach get a more accurate pair set than the direct extraction based on explicit adjacency." ></td>
	<td class="line x" title="123:133	The precision we obtained by the iterative reinforcement approach is 78.90%, almost 13 points higher than the adjacency approach." ></td>
	<td class="line x" title="124:133	This indicates that there are a large number of hidden sentiment associations in the real custom reviews, which underlines the importance and value of our work." ></td>
	<td class="line x" title="125:133	5 Conclusions and Further Work In this paper, we propose a novel iterative reinforcement framework based on improved information bottleneck approach to deal with the featurelevel product opinion-mining problem." ></td>
	<td class="line x" title="126:133	We alter traditional information bottleneck method by integration with semantic information, and the experimental result demonstrates the effectiveness of the alteration." ></td>
	<td class="line x" title="127:133	The main contribution of our work mainly including: null We propose an iterative reinforcement information bottleneck framework, and in this framework, review feature words and opinion words are organized into categories in a simultaneous and iterative manner." ></td>
	<td class="line x" title="128:133	null In the process of clustering, the semantic information and the co-occurrence information are integrated." ></td>
	<td class="line x" title="129:133	null The experimental results based on real Chinese web reviews demonstrate that our method outperforms the template extraction based algorithm." ></td>
	<td class="line x" title="130:133	Although our methods for candidate product feature extraction and filtering (see in 3.1) can partly identify real product features, it may lose some data and remain some noises." ></td>
	<td class="line x" title="131:133	Well conduct deeper research in this area in future work." ></td>
	<td class="line x" title="132:133	Additionally, we plan to exploit more information, such as background knowledge, to improve the performance." ></td>
	<td class="line x" title="133:133	6 Acknowledgments This work was mainly supported by two funds, i.e., 0704021000 and 60803085, and one another project, i.e., 2004CB318109." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-1056
For a few dollars less: Identifying review pages sans human labels
Barbosa, Luciano;Kumar, Ravi;Pang, Bo;Tomkins, Andrew;"></td>
	<td class="line x" title="1:259	Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 494502, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:259	c 2009 Association for Computational Linguistics For a few dollars less: Identifying review pages sans human labels Luciano Barbosa Dept. of Computer Science University of Utah Salt Lake City, UT 84112, USA." ></td>
	<td class="line x" title="3:259	lbarbosa@cs.utah.edu Ravi Kumar Bo Pang Andrew Tomkins Yahoo!" ></td>
	<td class="line x" title="4:259	Research 701 First Ave Sunnyvale, CA 94089, USA." ></td>
	<td class="line x" title="5:259	{ravikumar,bopang,atomkins}@yahoo-inc.com Abstract We address the problem of large-scale automatic detection of online reviews without using any human labels." ></td>
	<td class="line x" title="6:259	We propose an efficient method that combines two basic ideas: Building a classifier from a large number of noisy examples and using the structure of the website to enhance the performance of this classifier." ></td>
	<td class="line x" title="7:259	Experiments suggest that our method is competitive against supervised learning methods that mandate expensive human effort." ></td>
	<td class="line x" title="8:259	1 Introduction Shoppers are migrating to the web and online reviews are playing a critical role in affecting their shopping decisions, online and offline." ></td>
	<td class="line x" title="9:259	According to two surveys published by comScore (2007) and Horrigan (2008), 81% of web users have done online research on a product at least once." ></td>
	<td class="line x" title="10:259	Among readers of online reviews, more than 70% reported that the reviews had a significant influence on their purchases." ></td>
	<td class="line x" title="11:259	Realizingthiseconomicpotential, search engines have been scrambling to cater to such user needs in innovative ways." ></td>
	<td class="line x" title="12:259	For example, in response to a product-related query, a search engine might wanttosurfaceonlyreviewpages, perhapsviaafilter by option, to the user." ></td>
	<td class="line x" title="13:259	More ambitiously, they might want to dissect the reviews, segregate them into novice and expert judgments, distill sentiments, and present an aggregated wisdom of the crowds opinion to the user." ></td>
	<td class="line x" title="14:259	Identifying review pages is the indispensable enabler to fulfill any such ambition; nonetheless, this problem does not seem to have been addressed at web scale before." ></td>
	<td class="line x" title="15:259	Detecting review webpages in a few, review-only websites is an easy, manually-doable task." ></td>
	<td class="line x" title="16:259	A large fraction of the interesting review content, however, is present on pages outside such websites." ></td>
	<td class="line x" title="17:259	This is where the task becomes challenging." ></td>
	<td class="line x" title="18:259	Review pages might constitute a minority and can be buried in a multitude of ways among non-review pages  for instance, the movie review pages in nytimes." ></td>
	<td class="line x" title="19:259	com, which are scattered among all news articles, or the product review pages in amazon.com, which areaccessiblefromtheproductdescriptionpage." ></td>
	<td class="line x" title="20:259	An automatic and scalable method to identify reviews is thus a practical necessity for the next-generation search engines." ></td>
	<td class="line x" title="21:259	The problem is actually more general than detecting reviews: it applies to detecting any horizontal category such as buying guides, forums, discussion boards, FAQs, etc. Given the nature of these problems, it is tempting to use supervised classification." ></td>
	<td class="line x" title="22:259	A formidable barrier is the labeling task itself since human labels need time and money." ></td>
	<td class="line x" title="23:259	On the other hand, it is easier to generate an enormous number of lowquality labeled examples through purely automatic methods." ></td>
	<td class="line x" title="24:259	This prompts the question: Can we do review detection by focusing just on the textual content of a large number of automatically obtained but low-quality labeled examples, perhaps also utilizing the site structure specific to each website?" ></td>
	<td class="line x" title="25:259	And how will it compare to the best supervised classification method?" ></td>
	<td class="line x" title="26:259	We address these questions in this paper." ></td>
	<td class="line x" title="27:259	Main contributions." ></td>
	<td class="line x" title="28:259	We propose the first end-toend method that can operate at web scale to efficiently detect review pages." ></td>
	<td class="line x" title="29:259	Our method is based on using simple URL-based clues to automatically 494 partition a large collection of webpages into two noisy classes: One that consists mostly of review webpages and another that consists of a mixture of some review but predominantly non-review webpages (more details in Section 4.2)." ></td>
	<td class="line x" title="30:259	We analyze the use of a naive Bayes classifier in this noisy setting and present a simple algorithm for review page classification." ></td>
	<td class="line x" title="31:259	We further enhance the performanceofthisclassifierbyincorporatinginformationaboutthestructureofthewebsitethatismanifested through the URLs of the webpages." ></td>
	<td class="line x" title="32:259	We do this by partitioning the website into clusters of webpages, where the clustering delicately balances the information in the site-unaware labels provided by the classifier in the previous step and the site structure encoded in the URL tokens; a decision tree is used to accomplish this." ></td>
	<td class="line x" title="33:259	Our classification method for noisily-labeled examples and the use of sitespecific cues to improve upon a site-independent classifier are general techniques that may be applicable in other large-scale web analyses." ></td>
	<td class="line x" title="34:259	Experiments on 2000 hand-labeled webpages from 40 websites of varying sizes show that besides being computationally efficient, our human-labelfree method not only outperforms those based on off-the-shelf subjectivity detection but also remains competitive against the state-of-the-art supervised text classification that relies on editorial labels." ></td>
	<td class="line x" title="35:259	2 Related work The related work falls into roughly four categories: Documentand sentence-level subjectivity detection, sentiment analysis in the context of reviews, learning from noisy labeled examples, and exploiting site structure for classification." ></td>
	<td class="line x" title="36:259	Giventhesubjectivenatureofreviews, documentlevel subjectivity classification is closely related to our work." ></td>
	<td class="line x" title="37:259	There have been a number of approaches proposed to address document-level subjectivity in news articles, weblogs, etc.(Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004; Finn and Kushmerick, 2006; Ni et al., 2007; Stepinski and Mittal, 2007)." ></td>
	<td class="line x" title="39:259	Ng et al.(2006) experiment with review identificationforknowndomainsusingdatasetswith clean labels (e.g., movie reviews vs. movie-related non-reviews), a setting different from that of ours." ></td>
	<td class="line x" title="41:259	Pang and Lee (2008b) present a method on reranking documents that are web search results for a specific query (containing the word review) based onthesubjective/objectivedistinction." ></td>
	<td class="line x" title="42:259	Giventhenature of the query, they implicitly detect reviews from unknown sources." ></td>
	<td class="line x" title="43:259	But their re-ranking algorithm only applies to webpages known to be (roughly) related to the same narrow subject." ></td>
	<td class="line x" title="44:259	Since the webpages in our datasets cover not only a diverse range of websites but also a diverse range of topics, their approach does not apply." ></td>
	<td class="line x" title="45:259	To the best of our knowledge, there has been no work on identifying review pages at the scale and diversity we consider." ></td>
	<td class="line x" title="46:259	Subjectivity classification of within-document items, such as terms, has been an active line of research (Wiebe et al.(2004) present a survey)." ></td>
	<td class="line x" title="48:259	Identifying subjective sentences in a document via offthe-shelf packages is an alternative way of detecting reviews without (additional) human annotations." ></td>
	<td class="line x" title="49:259	In particular, the OpinionFinder system (Riloff and Wiebe, 2003; Wiebe and Riloff, 2005) is a state-ofthe-art knowledge-rich sentiment-analysis system." ></td>
	<td class="line x" title="50:259	We will use it as one of our baselines and compare its performance with our methods." ></td>
	<td class="line n" title="51:259	There has been a great deal of previous work in sentiment analysis that worked with reviews, but they were typically restricted to using reviews extracted from one or two well-known sources, bypassing automatic review detection." ></td>
	<td class="line oc" title="52:259	Examples of such early work include (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005)." ></td>
	<td class="line x" title="53:259	See Pang and Lee (2008a) for a more comprehensive survey." ></td>
	<td class="line x" title="54:259	Building a collection of diverse review webpages, not limited to one or two hosts, can better facilitate such research." ></td>
	<td class="line x" title="55:259	Learning from noisy examples has been studied for a long time in the learning theory community (Angluin and Laird, 1988)." ></td>
	<td class="line x" title="56:259	Learning naive Bayes classifiers from noisy data (either features or labels or both) was studied by Yang et al.(2003)." ></td>
	<td class="line x" title="58:259	Their focus, however, is to reconstruct the underlying conditional probability distributions from the observed noisydataset." ></td>
	<td class="line x" title="59:259	We, ontheotherhand, relyonthevolume of labels to drown the noise." ></td>
	<td class="line x" title="60:259	Along this spirit, Snowetal.(2008)showthatobtainingmultiplelowquality labels (through Mechanical Turk) can approach high-quality editorial labels." ></td>
	<td class="line x" title="61:259	Unlike in their setting, we do not have multiple low-quality labels for the same URL." ></td>
	<td class="line x" title="62:259	The extensive body of work in 495 semi-supervised learning or learning from one class is also somewhat relevant to our work." ></td>
	<td class="line x" title="63:259	A major difference is that they tend to work with small amount of clean, labeled data." ></td>
	<td class="line x" title="64:259	In addition, many semisupervised/transductive learning algorithms are not efficient for web-scale data." ></td>
	<td class="line x" title="65:259	Using site structure for web analysis tasks has been addressed in a variety of contexts." ></td>
	<td class="line x" title="66:259	For example, Kening et al.(2005) exploit the structure of a website to improve classification." ></td>
	<td class="line x" title="68:259	On a related note, co-training has also been used to utilize inter-page link information in addition to intra-page textual content: Blum and Mitchell (1998) use anchor texts pointing to a webpage as the alternative view of the page in the context of webpage classification." ></td>
	<td class="line x" title="69:259	Their algorithm is largely site-unaware in that it does not explicitly exploit site structures." ></td>
	<td class="line x" title="70:259	Utilizing site structures also has remote connections to wrapper induction, and there is extensive literature on this topic." ></td>
	<td class="line x" title="71:259	Unfortunately, the methods in all of these work require human labeling, which is precisely what our work is trying to circumvent." ></td>
	<td class="line x" title="72:259	3 Methodology In this section we describe our basic methodology for identifying review pages." ></td>
	<td class="line x" title="73:259	Our method consists of two main steps." ></td>
	<td class="line x" title="74:259	The first is to use a large amount of noisy training examples to learn a basic classifier for review webpages; we adapt a simple naive Bayes classifier for this purpose." ></td>
	<td class="line x" title="75:259	The second is to improve the performance of this basic classifier by exploiting the website structure; we use a decision tree for this." ></td>
	<td class="line x" title="76:259	Let P be the set of all webpages." ></td>
	<td class="line x" title="77:259	Let C+ denote the positive class, i.e., the set of all review pages and let C denote the negative class, i.e., the set of all non-review pages." ></td>
	<td class="line x" title="78:259	Each webpage p is exactly in one of C+ or C, and is labeled +1 or 1 respectively." ></td>
	<td class="line x" title="79:259	3.1 Learning from large amounts of noisy data Previous work using supervised or semi-supervised learning approaches for sentiment analysis assumes relatively high-quality labels that are produced either via human annotation or automatically generated through highly accurate rules (e.g., assigning positive or negative label to a review according to automatically extracted star ratings)." ></td>
	<td class="line x" title="80:259	Weexamineadifferentscenariowherewecanautomatically generate large amount of relatively lowquality labels." ></td>
	<td class="line x" title="81:259	Section 4.2 describes the process in more detail, but briefly, in a collection of pages crawled from sites that are very likely to host reviews, those with the word review in their URLs are very likely to contain reviews (the noisy positive set tildewideC+) and the rest of the pages on those sites are less likely to contain reviews (the more noisy negative set tildewideC)." ></td>
	<td class="line x" title="82:259	More formally, for a webpage p, suppose Pr[p  C+ | p  tildewideC+] =  and Pr[p  C+ | p  tildewideC] = , where 1 >  greaternotequal  > 0." ></td>
	<td class="line x" title="83:259	Canwestilllearnsomethingusefulfrom tildewideC+ and tildewideC despite the labels being highly noisy?" ></td>
	<td class="line x" title="84:259	The following analysis is based on a naive Bayes classifier." ></td>
	<td class="line x" title="85:259	We chose naive Bayes classifier since the learning phase can easily be parallelized." ></td>
	<td class="line x" title="86:259	Given a webpage (or a document) p represented as a bag of features {fi}, we wish to assign a class argmaxc{C+,C} Pr[c | p] to this webpage." ></td>
	<td class="line x" title="87:259	Naive Bayes classifiers assume fis to be conditionally independent and we have Pr[p | c] = producttextPr[fi | c]." ></td>
	<td class="line x" title="88:259	Let ri = Pr[fi | C+]/Pr[fi | C] denote the contribution of each feature towards classification, and rc = Pr[C+]/Pr[C] denote the ratio of class priors." ></td>
	<td class="line x" title="89:259	First note that log Pr[C+|p]Pr[C|p] = log parenleftBigPr[C +] Pr[C]  Pr[p|C+] Pr[p|C] parenrightBig = log parenleftBigPr[C +] Pr[C]  producttextr i parenrightBig = logrc +summationtextlogri." ></td>
	<td class="line x" title="90:259	A webpage p receives label +1 iff Pr[C+ | p] > Pr[C | p], and by above, if and only if summationtextlogri > logrc." ></td>
	<td class="line x" title="91:259	When we do not have a reasonable estimate of Pr[C+] and Pr[C], as in our setting, the best we can do is to assume rc = 1." ></td>
	<td class="line x" title="92:259	In this case, p receives label +1 if and only ifsummationtextlogri > 0." ></td>
	<td class="line x" title="93:259	Thus, a feature fi with logri > 0 has a positive contribution towards p being labeled +1; call fi to be a positive feature." ></td>
	<td class="line x" title="94:259	Typically we use relative-frequency estimation of Pr[c] and Pr[fi | c] for c  {C+,C}." ></td>
	<td class="line x" title="95:259	Now, how does the estimation from a dataset with noisy labels compare with the estimation from a dataset with clean labels?" ></td>
	<td class="line x" title="96:259	To examine this, we calculate the following: Pr[fi | tildewideC+] = Pr[fi | C+] + (1)Pr[fi | C], Pr[fi | tildewideC] =  Pr[fi | C+] + (1)Pr[fi | C]." ></td>
	<td class="line x" title="97:259	Let ri = Pr[fi|eC+]Pr[f i|eC] = ri+(1)ri+(1)." ></td>
	<td class="line x" title="98:259	Clearly ri is monotonic but not linear in ri." ></td>
	<td class="line x" title="99:259	Furthermore, it is bounded: 496 (1)/(1)  ri  /." ></td>
	<td class="line x" title="100:259	However, ri > 1  ri + (1) > ri + (1)  ()ri > ()  ri > 1,where the last step used  > ." ></td>
	<td class="line x" title="101:259	Thus, the sign of log ri is the same as that of logri, i.e., a feature contributing positively tosummationtextlogri will continue to contribute positively tosummationtextlog ri (although its magnitude is distorted) and vice versa." ></td>
	<td class="line x" title="102:259	Theaboveanalysismotivatesanalternativemodel to naive Bayes." ></td>
	<td class="line x" title="103:259	Instead of each feature fi placing a weighted vote log ri in the final decision, we trust only the sign of log ri, and let each feature fi place a vote for the class C+ (respectively, C) if log ri > 0 (respectively, log ri < 0)." ></td>
	<td class="line x" title="104:259	Intuitively, this model just compares the number of positive features and thenumberofnegativefeatures,ignoringthemagnitude (since it is distorted anyway)." ></td>
	<td class="line x" title="105:259	This is precisely our algorithm: For a given threshold , the final label nbu(p) of a webpage p is given by nbu(p) = sgn(summationtextsgn(log ri)), where sgn is the sign function." ></td>
	<td class="line x" title="106:259	For comparison purposes, we also indicate the weighted version: nbw(p) = sgn(summationtextlog ri )." ></td>
	<td class="line x" title="107:259	If  = 0, we omit  and use nb to denote a generic label assigned by any of the above algorithms." ></td>
	<td class="line x" title="108:259	Note that even though our discussions were for two-class and in particular, review classification, they are equally applicable to a wide range of classification tasks in large-scale web-content analysis." ></td>
	<td class="line x" title="109:259	Our analysis of learning from automatically generated noisy examples is thus of independent interest." ></td>
	<td class="line x" title="110:259	3.2 Utilizing site structure Can the structure of a website be exploited to improve the classification of webpages given by nb()?" ></td>
	<td class="line x" title="111:259	While not all websites are well-organized, quite a number of them exhibit certain structure that makes it possible to identify large subsites that contain only review pages." ></td>
	<td class="line x" title="112:259	Typically but not always this structure is manifested through the tokens in the URL corresponding to the webpage." ></td>
	<td class="line x" title="113:259	For instance, the pattern http://www.zagat.com/verticals/ PropertyDetails.aspx?VID=a&R=b, where a,b are numbers, is indicative of all webpages in zagat.com that are reviews of restaurants." ></td>
	<td class="line x" title="114:259	In fact, we can think of this as a generalization of having the keyword review in the URL." ></td>
	<td class="line x" title="115:259	Now, suppose we have an initial labeling nb(p)  {1} for each webpage p produced by a classifier (as in the previous section, or one that is trained on a small set of human annotated pages), can we further improve the labeling using the pattern in the URL structure?" ></td>
	<td class="line x" title="116:259	It is not immediate how to best use the URL structure to identify the review subsites." ></td>
	<td class="line x" title="117:259	First, URLs contain irrelevant information (e.g., the token verticals in the above example), thus clustering by simple cosine similarity may not discover the review subsites." ></td>
	<td class="line x" title="118:259	Second, the subsite may not correspond to a subtree in the URL hierarchy, i.e., it is not reasonable to expect all the review URLs to share a common prefix." ></td>
	<td class="line x" title="119:259	Third, the URLs contain a mixture of path components (e.g., www.zagat.com/verticals/ PropertyDetails.aspx) and key-value pairs (e.g., VID=a and R=b) and hence each token (regardless of its position) in the URL could play a role in determining the review subsite." ></td>
	<td class="line x" title="120:259	Furthermore, conjunction of presence/absence of certain tokens in the URL may best correspond to subsite membership." ></td>
	<td class="line x" title="121:259	In light of these, we represent each URL (and hence the corresponding webpage) by a bag {gi} of tokens obtained from the URL." ></td>
	<td class="line x" title="122:259	We perform a crude form of feature selection by dropping tokens that are either ubiquitous (occurring in more than 99% of URLs) or infrequent (occurring in fewer than 1% of URLs) in a website; neither yields useful information." ></td>
	<td class="line x" title="123:259	Our overall approach will be to use gis to partition P into clusters {Ci} of webpages such that each cluster Ci is predominantly labeled as either review or non-review by nb()." ></td>
	<td class="line x" title="124:259	This automatically yields a new label cls(p) for each page p, which is the majority label of the cluster of p: cls(p) = sgn parenleftBigsummationtext qC(p) nb(q) parenrightBig , where C(p) is the cluster of p. To this end, we use a decision tree classifier to build the clusters." ></td>
	<td class="line x" title="125:259	This classifier will use the features {gi} and the target labels nb()." ></td>
	<td class="line x" title="126:259	The classifier is trained on all the webpages in the website and in the obtained decision tree, each leaf, consisting of pages with the same set of feature values leading down the path, corresponds to a cluster of webpages." ></td>
	<td class="line x" title="127:259	Note that the clusters delicately balance the information in the siteunaware labels nb() and the site structure encoded 497 in the URLs (given by gis)." ></td>
	<td class="line x" title="128:259	Thus the label cls(p) can be thought of as a smoothed version of nb(p)." ></td>
	<td class="line x" title="129:259	Eventhoughwecanexpectmostclusterstobehomogeneous (i.e., pure reviews or non-reviews), the above method can produce clusters that are inherently heterogeneous." ></td>
	<td class="line x" title="130:259	This can happen if the website URLs are organized such that many subsites contain both review and non-review webpages." ></td>
	<td class="line x" title="131:259	To take this into account, we propose the following hybrid approach that interpolates between the unsmoothed labels given by nb() and the smoothed labels given by cls()." ></td>
	<td class="line x" title="132:259	For a cluster Ci, the discrepancy disc(Ci) = summationtextpCi[cls(p) negationslash= nb(p)]; this quantity measures the number of disagreements between the majority label cls(p) and the original label nb(p) for each page p in the cluster." ></td>
	<td class="line x" title="133:259	The decision tree guarantees disc(Ci)  |Ci|/2." ></td>
	<td class="line x" title="134:259	We call a cluster Ci to be -homogeneous if disc(Ci)  |Ci|, where   [0,1/2]." ></td>
	<td class="line x" title="135:259	For a fixed , the hybrid label of a webpage p is given by hyb(p) = braceleftbigg cls(p) if C(p) is -homogeneous, nb(p) otherwise." ></td>
	<td class="line x" title="136:259	Note that hyb1/2(p) = cls(p) and hyb0(p) = nb(p)." ></td>
	<td class="line x" title="137:259	Note that in the above discussions, any clustering method that can incorporate the site-unaware labels nb() and the site-specific tokens in gis could have been used; off-the-shelf decision tree was merely a specific way to realize this." ></td>
	<td class="line x" title="138:259	4 Data It is crucial for this study to create a dataset that is representative of a diverse range of websites that host reviews over different topics in different styles." ></td>
	<td class="line x" title="139:259	We are not aware of any extensive index of online review websites and we do not want to restrict our study to a few well-known review aggregation websites (such as yelp.com or zagat.com) since this will not represent the less popular and more specialized ones." ></td>
	<td class="line x" title="140:259	Instead, we utilized user-generated tags for webpages, available on social bookmarking websites such as del.icio.us." ></td>
	<td class="line x" title="141:259	We obtained (a sample of) a snapshot of URLtag pairs from del.icio.us." ></td>
	<td class="line x" title="142:259	We took the top one thousand sites with review* tags; these websites hopefully represent a broad coverage." ></td>
	<td class="line x" title="143:259	We were able to crawl over nine hundred of these sites and the resulting collection of webpages served as the basis of the experiments in this paper." ></td>
	<td class="line x" title="144:259	We refer to these websites (or the webpages from these sites, when it is clear from the context) as Sall." ></td>
	<td class="line x" title="145:259	4.1 Gold-standard test set When the websites are as diverse as represented in Sall, there is no perfect automatic way to generate the ground truth labels." ></td>
	<td class="line x" title="146:259	Thus we sampled a number of pages for human labeling as follows." ></td>
	<td class="line x" title="147:259	First, we set aside 40 sites as the test sites (S40)." ></td>
	<td class="line x" title="148:259	In order to represent different types of websites (to the best we can), we sampled the 40 sites so that S40 covers different size ranges, since large-scale websites and small-scale websites are often quite different in style, topic, and content." ></td>
	<td class="line x" title="149:259	We uniformly sampled 10 sites from each of the four size categories (roughly, sites with 1005K, 5K25K, 25K 100K, and 100K+ webpages)1." ></td>
	<td class="line x" title="150:259	Indeed, S40 (as did Sall) covered a wide range of topics (e.g., games, books, restaurants, movies, music, and electronics) and styles (e.g., dedicated review sites, product sites thatincludeuserreviews,newspaperswithmoviereview sections, religious sites hosting book reviews, and non-English review sites)." ></td>
	<td class="line x" title="151:259	Wethensampled50pagestobelabeledfromeach site in S40." ></td>
	<td class="line x" title="152:259	Since there are some fairly large sites that have only a small number of review pages, a uniform sampling may yield no review webpages from those sites." ></td>
	<td class="line x" title="153:259	To reflect the natural distribution on a website and to represent pages from both classes, the webpages were sampled in the following way." ></td>
	<td class="line x" title="154:259	For each website in S40, 25 pages were uniformly sampled (representing the natural distribution) and 25 pages were sampled from among equivalence classes based on URLs so that pages from each major URL pattern were represented." ></td>
	<td class="line x" title="155:259	Here, each webpage in the site is represented by a URL signature containing the most frequent tokens that occur in the URLs in that site and all pages with the same signature form an equivalence class." ></td>
	<td class="line x" title="156:259	For our purposes, a webpage is considered a review if it contains significant amount of textual information expressing subjective opinions on or personal experiences with a given product / service." ></td>
	<td class="line x" title="157:259	When in doubt, the guiding principle is whether 1As we do not want to waste human annotation on sites with no reviews at all, a quick pre-screening process eliminated candidate sites that did not seem to host any reviews." ></td>
	<td class="line x" title="158:259	498 a page can be a satisfactory result page for users searching for reviews." ></td>
	<td class="line x" title="159:259	More specifically, the human annotation labeled each webpage, after thoroughly examining the content, with one of the following seven intuitive labels: single (contains exactly one review), multiple (concatenation of more than one review), no (clearly not a review page), empty (looks like a page that could contain reviews but had none), login (a valid user login needed to look at the content), hub (a pointer to one or more review pages), and ambiguous (border-line case, e.g., a webpagewithaonelinereview)." ></td>
	<td class="line x" title="160:259	Thefirsttwolabels were treated as +1 (i.e., reviews) and the last five labels were treated as 1 (i.e., non-reviews)." ></td>
	<td class="line x" title="161:259	Out of the 2000 pages, we obtained 578 pages labeled +1 and the 1422 pages labeled 1." ></td>
	<td class="line x" title="162:259	On a pilot study using two human judges, we obtained 78% inter-judge agreement for the seven labels and 92% inter-judge agreement if we collapse the labels to 1." ></td>
	<td class="line x" title="163:259	Percentages of reviews in our samples from different sites range from 14.6% to 93.9%." ></td>
	<td class="line x" title="164:259	Preprocessing for text-based analysis." ></td>
	<td class="line x" title="165:259	We processed the crawled webpages using lynx to extract the text content." ></td>
	<td class="line x" title="166:259	To discard templated content, which is an annoying issue in large-scale web processing, and HTML artifacts, we used the following preprocessing." ></td>
	<td class="line x" title="167:259	First, the HTML tags <p>, <br>, </tr>, and </td> were interpreted as paragraph breaks, the . inside a paragraph was interpreted as a sentence break, and whitespace was used to tokenize words in a sentence." ></td>
	<td class="line x" title="168:259	A sentence is considered good if it has at least seven alphabetic words and a paragraph is considered good if it has at least two good sentences." ></td>
	<td class="line x" title="169:259	After extracting the text using lynx, only the good paragraphs were retained." ></td>
	<td class="line x" title="170:259	This effectively removes most of the templated content (e.g., navigational phrases) and retains most of the natural language texts." ></td>
	<td class="line x" title="171:259	Because of this preprocessing, 485 pages out of 2000 turned out to be empty and these were discarded (human labels on 97% of these empty pages were 1)." ></td>
	<td class="line x" title="172:259	4.2 Dataset with noisy labels As discussed in Section 3.1, our goal is to obtain a large noisy set of positive and negative labeled examples." ></td>
	<td class="line x" title="173:259	We obtained these labels for the webpages in the training sites, Srest, which is essentially Sall \ S40." ></td>
	<td class="line x" title="174:259	First, the URLs in Srest were tokenized using a unigram model based on an English dictionary; this is so that strings such as reviewoftheday are properly interpreted." ></td>
	<td class="line x" title="175:259	tildewideC+: To be labeled +1, the path-component of the URL of the webpage has to contain the token review." ></td>
	<td class="line x" title="176:259	Our assumption is that such pages are highly likely to be review pages." ></td>
	<td class="line x" title="177:259	On a uniform sample of 100 such pages in Sall, 90% were found to be genuine reviews." ></td>
	<td class="line x" title="178:259	Thus, we obtained a collection of webpages with slightly noisy positive labels." ></td>
	<td class="line x" title="179:259	tildewideC: The rest of the pages in Srest were labeled 1." ></td>
	<td class="line x" title="180:259	Clearly this is a noisy negative set since not all pages containing reviews have review as part of their URLs (recall the example from zagat.com); thus many pages in tildewideC can still be reviews." ></td>
	<td class="line x" title="181:259	While the negative labels in Srest are more noisy than the positive labels, we believe most of the nonreview pages are in tildewideC, and as most websites contain a significant number of non-review pages, the percentage of reviews in tildewideC is smaller than that in tildewideC+ (the assumption  greaternotequal  in Section 3.1)." ></td>
	<td class="line x" title="182:259	We collected all the paragraphs (as defined earlier) from both tildewideC+ and tildewideC separately." ></td>
	<td class="line x" title="183:259	We eliminated duplicate paragraphs (this further mitigates the templates issue, especially for sites generated by content-management software), and trained a unigram language model as in Section 3.1." ></td>
	<td class="line x" title="184:259	5 Evaluations The evaluations were conducted on the 1515 labeled (non-empty) pages in S40 described in Section 4.1." ></td>
	<td class="line x" title="185:259	We report the accuracy (acc.)" ></td>
	<td class="line x" title="186:259	as well as precision (prec.), recall (rec.), and f-measure (fmeas.)" ></td>
	<td class="line x" title="187:259	for C+." ></td>
	<td class="line x" title="188:259	Trivial baselines." ></td>
	<td class="line x" title="189:259	Out of the 1515 labeled pages, 565 were labeled +1 and 950 were labeled 1." ></td>
	<td class="line x" title="190:259	Table 1 summarizes the performance of baselines that always predict one of the classes and a baseline that randomly select a class according to the class distribution S40." ></td>
	<td class="line x" title="191:259	As we can see, the best accuracy is .63, the best f-measure is .54, and they cannot be achieved by the same baseline." ></td>
	<td class="line x" title="192:259	Before presentacc." ></td>
	<td class="line x" title="193:259	prec." ></td>
	<td class="line x" title="194:259	rec." ></td>
	<td class="line x" title="195:259	fmeas." ></td>
	<td class="line x" title="196:259	always C .63 0 always C+ .37 .37 1 .54 random .53 .37 .37 .37 Table 1: Trivial baseline performances." ></td>
	<td class="line x" title="197:259	499 ing the main results of our methods, we introduce a much stronger baseline that utilizes a knowledgerich subjectivity detection package." ></td>
	<td class="line x" title="198:259	5.1 Using subjectivity detectors This baseline is motivated by the fact that reviews oftencontainextensivesubjectivecontent." ></td>
	<td class="line x" title="199:259	Thereare many existing techniques that detect subjectivity in text." ></td>
	<td class="line x" title="200:259	OpinionFinder (http://www.cs.pitt." ></td>
	<td class="line x" title="201:259	edu/mpqa/opinionfinderrelease/) is a well-known system that processes documents and automatically identifies subjective sentences in them." ></td>
	<td class="line x" title="202:259	OpinionFinder uses two subjective sentence classifiers (Riloff and Wiebe, 2003; Wiebe and Riloff, 2005)." ></td>
	<td class="line x" title="203:259	The first (denoted opfA) focuses on yielding the highest accuracy; the second (denoted opfB) optimizes precision at the expense of recall." ></td>
	<td class="line x" title="204:259	The methods underlying OpinionFinder incorporate extensive tools from linguistics (including, speech activity verbs, psychological verbs, FrameNet verbs andadjectiveswithframeexperiencer,amongothers) and machine learning." ></td>
	<td class="line x" title="205:259	In terms of performance, previous work has shown that OpinionFinder is a challenging system to improve upon for review retrieval (Pang and Lee, 2008b)." ></td>
	<td class="line x" title="206:259	Computationally, OpinionFinderisveryexpensiveandhenceunattractive for large-scale webpage analysis (running OpinionFinder on 1515 pages took about five hours)." ></td>
	<td class="line x" title="207:259	Therefore, we also propose a light-weight subjectivity detection mechanism called lwd, which counts the number of opinion words in each sentence in the text." ></td>
	<td class="line x" title="208:259	The opinion words (5403 of them) were obtained from an existing subjectivity lexicon (http: //www.cs.pitt.edu/mpqa)." ></td>
	<td class="line x" title="209:259	We ran both opfA and opfB on the tokenized text (running them on raw HTML produced worse results)." ></td>
	<td class="line x" title="210:259	Each sentence in the text was labeled subjective or objective." ></td>
	<td class="line x" title="211:259	We experimented with two ways to label a document using sentence-level subjectivity labels." ></td>
	<td class="line x" title="212:259	We labeled a document +1 if it contained at least k subjective sentences (denoted as opfstar(k), where k > 0 is the absolute threshold), or at least f fraction of its sentences were labeled subjective (denoted as opfstar(f), where f  (0,1] is the relative threshold)." ></td>
	<td class="line x" title="213:259	We conducted exhaustive parameter search with both opfA and opfB." ></td>
	<td class="line x" title="214:259	For instance, the performances of opfA as a function of the thresholds, both absolute and relative, is shown in Figure 1." ></td>
	<td class="line x" title="215:259	Table 2 summarizes the best performances of opfstar(k) (first two rows) and opfstar(f) (next two rows), in terms of accuracy and f-measure (boldfaced)." ></td>
	<td class="line x" title="216:259	Similarly, for lwd, we labeled a document +1 if at least k sentences have at least lscript opinion words (denoted lwd(k,lscript).)" ></td>
	<td class="line x" title="217:259	Table 2 once again shows the best performing parameters for both accuracy and f-measure for lwd." ></td>
	<td class="line x" title="218:259	Our results indicate that a simple method such as lwd can come very close to a sophisticated system such as opfstar." ></td>
	<td class="line x" title="219:259	acc." ></td>
	<td class="line x" title="220:259	prec." ></td>
	<td class="line x" title="221:259	rec." ></td>
	<td class="line x" title="222:259	fmeas." ></td>
	<td class="line x" title="223:259	opfA(2) .704 .597 .634 .615 opfB(2) .659 .526 .857 .652 opfA(.17) .652 .529 .614 .568 opfB(.36) .636 .523 .797 .632 lwd(1,4) .716 .631 .572 .600 lwd(1,1) .666 .538 .740 .623 Table 2: Best performances of opfstar and lwd methods." ></td>
	<td class="line x" title="224:259	Figure 1: Performance of opfA as a function of thresholds: Absolute and relative." ></td>
	<td class="line x" title="225:259	5.2 Main results As stated earlier, we do not have any prior knowledge about the value of  and hence have to work with  = 0." ></td>
	<td class="line x" title="226:259	To investigate the implications of this assumption, we study the performance of nbu and nbw as a function of ." ></td>
	<td class="line x" title="227:259	The accuracy and fmeasures are plotted in Figure 2." ></td>
	<td class="line x" title="228:259	There are three 500 acc." ></td>
	<td class="line x" title="229:259	prec." ></td>
	<td class="line x" title="230:259	rec." ></td>
	<td class="line x" title="231:259	fmeas." ></td>
	<td class="line x" title="232:259	nbu .753 .652 .726 .687 cls .756 .696 .616 .654 hyb1/3 .777 .712 .674 .693 Table 3: Performance of our methods." ></td>
	<td class="line x" title="233:259	conclusions that can be drawn from this study: (i) The peak values of accuracy and f-measure are comparable for both nbu and nbw, (ii) at  = 0, nbu is much better than nbw, in terms of both accuracy and f-measure, and(iii)thebestperformanceof nbu occurs at   0." ></td>
	<td class="line x" title="234:259	Given the difficulty of obtaining  if one were to use nbw, the above conclusions validate our intuition and the algorithm in Section 3.1." ></td>
	<td class="line x" title="235:259	Figure 2: Performance as threshold changes: Comparing nbu (marked as (u)) with nbw (marked as (w))." ></td>
	<td class="line x" title="236:259	Table 3 shows the performance of the site-specific method outlined in Section 3.2." ></td>
	<td class="line x" title="237:259	The clusters were generated using the unpruned J48 decision tree in Weka (www.cs.waikato.ac.nz/ml/ weka)." ></td>
	<td class="line x" title="238:259	In our experiments, we set  = 1/3 as a natural choice for the hybrid method." ></td>
	<td class="line x" title="239:259	As we see the performance of nbu is about 7% better than the best performance using a subjectivity-based method (in terms of accuracy)." ></td>
	<td class="line x" title="240:259	The performance of the smoothed labels (decision tree-based clustering) is comparable to that of nbu." ></td>
	<td class="line x" title="241:259	However, the hybrid method hyb1/3 yields an additional 3% relative improvement over nbu." ></td>
	<td class="line x" title="242:259	Paired t-test over the accuracies for these 40 sites shows both hyb1/3 and nbu to be statistically significantly better than the opfstar with best accuracy (with p < 0.05, p < 0.005, respectively), and hyb1/3 to be statistically significantly better than nbu (with p < 0.05)." ></td>
	<td class="line x" title="243:259	5.3 Cross-validation on S40 While the main focus of our paper is to study how to detect reviews without human labels, we present cross validation results on S40 as a comparison point." ></td>
	<td class="line x" title="244:259	The goal of this experiment is to get a sense of the best possible accuracy and f-measure numbers using labeled data and the state-of-theart method for text classification, namely, SVMs." ></td>
	<td class="line x" title="245:259	In other words, the performance numbers obtained through SVMs and cross-validation can be thought of as realistic upper bounds on the performance of content-based review detection." ></td>
	<td class="line x" title="246:259	We used SVMlight (svmlight.joachims.org) for this purpose." ></td>
	<td class="line x" title="247:259	The cross-validation experiment was conducted as follows." ></td>
	<td class="line x" title="248:259	We split the data by site to simulate the more realistic setting where pages in the test set do not necessarily come from a known site." ></td>
	<td class="line x" title="249:259	Each fold consisted of one site from each size category; thus, 36 of the 40 sites in S40 were used for training and the remainder for testing." ></td>
	<td class="line x" title="250:259	Over ten folds, the averageperformancewas: accuracy.795, precision.759, recall .658, and f-measure .705." ></td>
	<td class="line x" title="251:259	Thus our methods in Section 3 come reasonably close to the upper bound given by SVMs and human-labeled data." ></td>
	<td class="line x" title="252:259	In fact, while the supervised SVMs statistically significantly outperform nbu, they are statistically indistinguishable from hyb1/3 via paired t-test over site-level accuracies." ></td>
	<td class="line x" title="253:259	6 Conclusions In this paper we proposed an automatic method to perform efficient and large-scale detection of reviews." ></td>
	<td class="line x" title="254:259	Our method is based on two principles: Building a classifier from a large number of noisy labeled examples and using the site structure to improve the performance of this classifier." ></td>
	<td class="line x" title="255:259	Extensive experiments suggest that our method is competitive against supervised learning methods that depend on expensive human labels." ></td>
	<td class="line x" title="256:259	There are several interesting avenues for future research, including improving the current method for exploiting the site structure." ></td>
	<td class="line x" title="257:259	On a separate note, previous research has explicitly studied sentiment analysis as an application of transfer learning (Blitzer et al., 2007)." ></td>
	<td class="line x" title="258:259	Given the diverse range of topics present in our dataset, addressing topic-dependency is also an interesting future research direction." ></td>
	<td class="line x" title="259:259	501" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-2046
Improving SCL Model for Sentiment-Transfer Learning
Tan, Songbo;Cheng, Xue-Qi;"></td>
	<td class="line x" title="1:84	Proceedings of NAACL HLT 2009: Short Papers, pages 181184, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:84	c 2009 Association for Computational Linguistics Improving SCL Model for Sentiment-Transfer Learning  Songbo Tan Institute of Computing Technology Beijing, China tansongbo@software.ict.ac.cn Xueqi Cheng Institute of Computing Technology Beijing, China cxq@ict.ac.cn  ABSTRACT In recent years, Structural Correspondence Learning (SCL) is becoming one of the most promising techniques for sentiment-transfer learning." ></td>
	<td class="line x" title="3:84	However, SCL model treats each feature as well as each instance by an equivalent-weight strategy." ></td>
	<td class="line x" title="4:84	To address the two issues effectively, we proposed a weighted SCL model (W-SCL), which weights the features as well as the instances." ></td>
	<td class="line x" title="5:84	More specifically, W-SCL assigns a smaller weight to high-frequency domain-specific (HFDS) features and assigns a larger weight to instances with the same label as the involved pivot feature." ></td>
	<td class="line x" title="6:84	The experimental results indicate that proposed W-SCL model could overcome the adverse influence of HFDS features, and leverage knowledge from labels of instances and pivot features." ></td>
	<td class="line oc" title="7:84	1   Introduction In the community of sentiment analysis (Turney 2002; Pang et al., 2002; Tang et al., 2009), transferring a sentiment classifier from one source domain to another target domain is still far from a trivial work, because sentiment expression often behaves with strong domain-specific nature." ></td>
	<td class="line x" title="8:84	Up to this time, many researchers have proposed techniques to address this problem, such as classifiers adaptation, generalizable features detection and so on (DaumeIII et al., 2006; Jiang et al., 2007; Tan et al., 2007; Tan et al., 2008; Tan et al., 2009)." ></td>
	<td class="line x" title="9:84	Among these techniques, SCL (Structural Correspondence Learning) (Blitzer et al., 2006) is regarded as a promising method to tackle transfer-learning problem." ></td>
	<td class="line x" title="10:84	The main idea behind SCL model is to identify correspondences among features from different domains by modeling their correlations with pivot features (or generalizable features)." ></td>
	<td class="line x" title="11:84	Pivot features behave similarly in both domains." ></td>
	<td class="line x" title="12:84	If non-pivot features from different domains are correlated with many of the same pivot features, then we assume them to be corresponded with each other, and treat them similarly when training a sentiment classifier." ></td>
	<td class="line x" title="13:84	However, SCL model treats each feature as well as each instance by an equivalent-weight strategy." ></td>
	<td class="line x" title="14:84	From the perspective of feature, this strategy fails to overcome the adverse influence of highfrequency domain-specific (HFDS) features." ></td>
	<td class="line x" title="15:84	For example, the words stock or market occurs frequently in most of stock reviews, so these nonsentiment features tend to have a strong correspondence with pivot features." ></td>
	<td class="line x" title="16:84	As a result, the representative ability of the other sentiment features will inevitably be weakened to some degree." ></td>
	<td class="line x" title="17:84	To address this issue, we proposed Frequently Exclusively-occurring Entropy (FEE) to pick out HFDS features, and proposed a feature-weighted SCL model (FW-SCL) to adjust the influence of HFDS features in building correspondence." ></td>
	<td class="line x" title="18:84	The main idea of FW-SCL is to assign a smaller weight to HFDS features so that the adverse influence of HFDS features can be decreased." ></td>
	<td class="line x" title="19:84	From the other perspective, the equivalentweight strategy of SCL model ignores the labels (positive or negative) of labeled instances." ></td>
	<td class="line x" title="20:84	Obviously, this is not a good idea." ></td>
	<td class="line x" title="21:84	In fact, positive pivot features tend to occur in positive instances, so the correlations built on positive instances are more reliable than that built on negative instances; and vice versa." ></td>
	<td class="line x" title="22:84	Consequently, utilization of labels of instances and pivot features can decrease the adverse influence of some co-occurrences, such as co-occurrences involved with positive pivot features and negative instances, or involved with negative pivot features and positive instances." ></td>
	<td class="line x" title="23:84	In order to take into account the labels of labeled instances, we proposed an instanceweighted SCL model (IW-SCL), which assigns a larger weight to instances with the same label as the involved pivot feature." ></td>
	<td class="line x" title="24:84	In this time, we obtain a combined model: feature-weighted and instanceweighted SCL model (FWIW-SCL)." ></td>
	<td class="line x" title="25:84	For the sake 181 of convenience, we simplify FWIW-SCL as W-SCL in the rest of this paper." ></td>
	<td class="line x" title="26:84	2   Structural Correspondence Learning In the section, we provide the detailed procedures for SCL model." ></td>
	<td class="line x" title="27:84	First we need to pick out pivot features." ></td>
	<td class="line x" title="28:84	Pivot features occur frequently in both the source and the target domain." ></td>
	<td class="line x" title="29:84	In the community of sentiment analysis, generalizable sentiment words are good candidates for pivot features, such as good and excellent." ></td>
	<td class="line x" title="30:84	In the rest of this paper, we use K to stand for the number of pivot features." ></td>
	<td class="line x" title="31:84	Second, we need to compute the pivot predictors (or mapping vectors) using selected pivot features." ></td>
	<td class="line x" title="32:84	The pivot predictors are the key job, because they directly decide the performance of SCL." ></td>
	<td class="line x" title="33:84	For each pivot feature k, we use a loss function L k , () 2 1)( wxwxpL i i T ikk +=         (1) where the function p k (x i ) indicates whether the pivot feature k occurs in the instance x i , otherwise xif xp ik ik 0 1 1 )( >     = , where the weight vector w encodes the correspondence of the non-pivot features with the pivot feature k (Blitzer et al., 2006)." ></td>
	<td class="line x" title="34:84	Finally we use the augmented space [x T ,  x T W] T  to train the classifier on the source labeled data and predict the examples on the target domain, where W=[w 1 ,w 2 , , w K ]." ></td>
	<td class="line x" title="35:84	3   Feature-Weighted SCL Model 3.1 Measure to pick out HFDS features In order to pick out HFDS features, we proposed Frequently Exclusively-occurring Entropy (FEE)." ></td>
	<td class="line x" title="36:84	Our measure includes two criteria: occur in one domain as frequently as possible, while occur on another domain as rarely as possible." ></td>
	<td class="line x" title="37:84	To satisfy this requirement, we proposed the following formula: () ()         += )(),(min )(),(max log)(),(maxlog wPwP wPwP wPwPf no no now (2) where P o (w) and P n (w) indicate the probability of word w in the source domain and the target domain respectively:  () ()  + + = 2 )( )( o o o N wN wP                     (3) ( ) ()  + + = 2 )( )( n n n N wN wP                     (4) where N o (w) and N n (w) is the number of examples with word w in the source domain and the target domain respectively; N o  and N n  is the number of examples in the source domain and the target domain respectively." ></td>
	<td class="line x" title="38:84	In order to overcome overflow, we set  =0.0001 in our experiment reported in section 5." ></td>
	<td class="line x" title="39:84	To better understand this measure, lets take a simple example (see Table 1)." ></td>
	<td class="line x" title="40:84	Given a source dataset with 1000 documents and a target dataset with 1000 documents, 12 candidate features, and a task to pick out 2 HFDS features." ></td>
	<td class="line x" title="41:84	According to our understanding, the best choice is to pick out w 4  and w 8 .  According to formula (2), fortunately, we successfully pick out w 4 , and w 8 . This simple example validates the effectiveness of proposed FEE formula." ></td>
	<td class="line x" title="42:84	Table 1: A simple example for FEE FEE Words N o (w) N n (w) Score Rank w 1  100 100 -2.3025 6 w 2  100 90 -2.1971 4 w 3  100 45 -1.5040 3 w 4  100 4 0.9163 1 w 5  50 50 -2.9956 8 w 6  50 45 -2.8903 7 w 7  50 23 -2.2192 5 w 8  50 2 0.2231 2 w 9  4 4 -5.5214 11 w 10 4 3 -5.2337 10 w 11 4 2 -4.8283 9 w 12 1 1 -6.9077 12 3.2 Feature-Weighted SCL model To adjust the influence of HFDS features in building correspondence, we proposed featureweighted SCL model (FW-SCL), ( ) 2 1)( wxwxpL i il l llikk  +=      (5) where the function p k (x i ) indicates whether the pivot feature k occurs in the instance x i ; otherwise xif xp ik ik 0 1 1 )( >     = , and  l  is the parameter to control the weight of the HFDS feature l, 182  otherwise Zlif HFDS l     =  1    where Z HFDS  indicates the HFDS feature set and  is located in the range [0,1]." ></td>
	<td class="line x" title="43:84	When  =0, it indicates that no HFDS features are used to build the correspondence vectors; while  =1 indicates that all features are equally used to build the correspondence vectors, that is to say, proposed FW-SCL algorithm is simplified as traditional SCL algorithm." ></td>
	<td class="line x" title="44:84	Consequently, proposed FW-SCL algorithm could be regarded as a generalized version of traditional SCL algorithm." ></td>
	<td class="line x" title="45:84	4 Instance-Weighted SCL Model The traditional SCL model does not take into account the labels (positive or negative) of instances on the source domain and pivot features." ></td>
	<td class="line x" title="46:84	Although the labels of pivot features are not given at first, it is very easy to obtain these labels because the number of pivot features is typically very small." ></td>
	<td class="line x" title="47:84	Obviously, positive pivot features tend to occur in positive instances, so the correlations built on positive instances are more reliable than the correlations built on negative instances; and vice versa." ></td>
	<td class="line x" title="48:84	As a result, the ideal choice is to assign a larger weight to the instances with the same label as the involved pivot feature, while assign a smaller weight to the instances with the different label as the involved pivot feature." ></td>
	<td class="line x" title="49:84	This strategy can make correlations more reliable." ></td>
	<td class="line x" title="50:84	This is the key idea of instance-weighted SCL model (IWSCL)." ></td>
	<td class="line x" title="51:84	Combining the idea of feature-weighted SCL model (FW-SCL), we obtain the featureweighted and instance-weighted SCL model (FWIW-SCL), ()()() () ()()()   ++= 1),(11 1),( 2 jl l lljkj il l llikik xwxpxk wxwxpxkL   (6) where   is the instance weight and the function p k (x i ) indicates whether the pivot feature k occurs in the instance x i ; otherwise xif xp ik ik 0 1 1 )( >     = and  l  is the parameter to control the weight of the HFDS feature l,  otherwise Zlif HFDS l     =  1   , where Z HFDS  indicates the HFDS feature set and  is located in the range [0,1]." ></td>
	<td class="line x" title="52:84	In equation (6), the function  (z,y) indicates whether the two variables z and y have the same non-zero value, () otherwise 0y and zzif z,y =    =  0 1  ; and the function  (z) is a hinge function, whose variables are either pivot features or instances, labelnegativez has aif unknown labelpositivez has aif z   1 0 1 )(       = . For the sake of convenience, we simplify FWIW-SCL as W-SCL." ></td>
	<td class="line x" title="53:84	5   Experimental Results 5.1 Datasets We collected three Chinese domain-specific datasets: Education Reviews (Edu, from http://blog.sohu.com/learning/), Stock Reviews (Sto, from http://blog.sohu.com/stock/) and Computer Reviews (Comp, from http://detail.zol.com.cn/)." ></td>
	<td class="line x" title="54:84	All of these datasets are annotated by three linguists." ></td>
	<td class="line x" title="55:84	We use ICTCLAS (a Chinese text POS tool, http://ictclas.org/) to parse Chinese words." ></td>
	<td class="line x" title="56:84	The dataset Edu includes 1,012 negative reviews and 254 positive reviews." ></td>
	<td class="line x" title="57:84	The average size of reviews is about 600 words." ></td>
	<td class="line x" title="58:84	The dataset Sto consists of 683 negative reviews and 364 positive reviews." ></td>
	<td class="line x" title="59:84	The average length of reviews is about 460 terms." ></td>
	<td class="line x" title="60:84	The dataset Comp contains 390 negative reviews and 544 positive reviews." ></td>
	<td class="line x" title="61:84	The average length of reviews is about 120 words." ></td>
	<td class="line x" title="62:84	5.2 Comparison Methods In our experiments, we run one supervised baseline, i.e., Nave Bayes (NB), which only uses one source-domain labeled data as training data." ></td>
	<td class="line x" title="63:84	For transfer-learning baseline, we implement traditional SCL model (T-SCL) (Blitzer et al., 2006)." ></td>
	<td class="line x" title="64:84	Like TSVM, it makes use of the sourcedomain labeled data as well as the target-domain unlabeled data." ></td>
	<td class="line x" title="65:84	5.3 Does proposed method work?" ></td>
	<td class="line x" title="66:84	To conduct our experiments, we use sourcedomain data as unlabeled set or labeled training set, and use target-domain data as unlabeled set or testing set." ></td>
	<td class="line x" title="67:84	Note that we use 100 manualannotated pivot features for T-SCL, FW-SCL and W-SCL in the following experiments." ></td>
	<td class="line x" title="68:84	We select 183 pivot features use three criteria: a) is a sentiment word; b) occurs frequently in both domains; c) has similar occurring probability." ></td>
	<td class="line x" title="69:84	For T-SCL, FWSCL and W-SCL, we use prototype classifier (Sebastiani, 2002) to train the final model." ></td>
	<td class="line x" title="70:84	Table 2 shows the results of experiments comparing proposed method with supervised learning, transductive learning and T-SCL." ></td>
	<td class="line x" title="71:84	For FW-SCL, the Z HFDS  is set to 200 and   is set to 0.1; For W-SCL, the Z HFDS  is set to 200,   is set to 0.1, and   is set to 0.9." ></td>
	<td class="line x" title="72:84	As expected, proposed method FW-SCL does indeed provide much better performance than supervised baselines, TSVM and T-SCL model." ></td>
	<td class="line x" title="73:84	For example, the average accuracy of FW-SCL beats supervised baselines by about 12 percents, beats TSVM by about 11 percents and beats TSCL by about 10 percents." ></td>
	<td class="line x" title="74:84	This result indicates that proposed FW-SCL model could overcome the shortcomings of HFDS features in building correspondence vectors." ></td>
	<td class="line x" title="75:84	More surprisingly, instance-weighting strategy can further boost the performance of FW-SCL by about 4 percents." ></td>
	<td class="line x" title="76:84	This result indicates that the labels of instances and pivot features are very useful in building the correlation vectors." ></td>
	<td class="line x" title="77:84	This result also verifies our analysis in section 4: positive pivot features tend to occur in positive instances, so the correlations built on positive instances are more reliable than the correlations built on negative instances, and vice versa." ></td>
	<td class="line x" title="78:84	Table 2: Accuracy of different methods  NB T-SCL FW-SCL W-SCL Edu->Sto 0.6704 0.7965 0.7917 0.8108 Edu->Comp 0.5085 0.8019 0.8993 0.9025 Sto->Edu 0.6824 0.7712 0.9072 0.9368 Sto->Comp 0.5053 0.8126 0.8126 0.8693 Comp->Sto 0.6580 0.6523 0.7010 0.7717 Comp->Edu 0.6114 0.5976 0.9112 0.9408 Average 0.6060 0.7387 0.8372 0.8720 Although SCL is a method designed for transfer learning, but it cannot provide better performance than TSVM." ></td>
	<td class="line x" title="79:84	This result verifies the analysis in section 3: a small amount of HFDS features occupy a large amount of weight in classification model, but hardly carry corresponding sentiment." ></td>
	<td class="line x" title="80:84	In another word, very few top-frequency words degrade the representative ability of SCL model for sentiment classification." ></td>
	<td class="line x" title="81:84	6 Conclusion Remarks In this paper, we proposed a weighted SCL model (W-SCL) for domain adaptation in the context of sentiment analysis." ></td>
	<td class="line x" title="82:84	On six domaintransfer tasks, W-SCL consistently produces much better performance than the supervised, semisupervised and transfer-learning baselines." ></td>
	<td class="line x" title="83:84	As a result, we can say that proposed W-SCL model offers a better choice for sentiment-analysis applications that require high-precision classification but hardly have any labeled training data." ></td>
	<td class="line x" title="84:84	7 Acknowledgments This work was mainly supported by two funds, i.e., 0704021000 and 60803085, and one another project, i.e., 2004CB318109." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1027
Co-Training for Cross-Lingual Sentiment Classification
Wan, Xiaojun;"></td>
	<td class="line x" title="1:195	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 235243, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:195	c2009 ACL and AFNLP Co-Training for Cross-Lingual Sentiment Classification  Xiaojun Wan Institute of Compute Science and Technology & Key Laboratory of Computational Linguistics, MOE Peking University, Beijing 100871, China wanxiaojun@icst.pku.edu.cn   Abstract The lack of Chinese sentiment corpora limits the research progress on Chinese sentiment classification." ></td>
	<td class="line x" title="3:195	However, there are many freely available English sentiment corpora on the Web." ></td>
	<td class="line x" title="4:195	This paper focuses on the problem of cross-lingual sentiment classification, which leverages an available English corpus for Chinese sentiment classification by using the English corpus as training data." ></td>
	<td class="line x" title="5:195	Machine translation services are used for eliminating the language gap between the training set and test set, and English features and Chinese features are considered as two independent views of the classification problem." ></td>
	<td class="line x" title="6:195	We propose a cotraining approach to making use of unlabeled Chinese data." ></td>
	<td class="line x" title="7:195	Experimental results show the effectiveness of the proposed approach, which can outperform the standard inductive classifiers and the transductive classifiers." ></td>
	<td class="line x" title="8:195	1 Introduction Sentiment classification is the task of identifying the sentiment polarity of a given text." ></td>
	<td class="line x" title="9:195	The sentiment polarity is usually positive or negative and the text genre is usually product review." ></td>
	<td class="line x" title="10:195	In recent years, sentiment classification has drawn much attention in the NLP field and it has many useful applications, such as opinion mining and summarization (Liu et al., 2005; Ku et al., 2006; Titov and McDonald, 2008)." ></td>
	<td class="line x" title="11:195	To date, a variety of corpus-based methods have been developed for sentiment classification." ></td>
	<td class="line x" title="12:195	The methods usually rely heavily on an annotated corpus for training the sentiment classifier." ></td>
	<td class="line x" title="13:195	The sentiment corpora are considered as the most valuable resources for the sentiment classification task." ></td>
	<td class="line x" title="14:195	However, such resources in different languages are very imbalanced." ></td>
	<td class="line x" title="15:195	Because most previous work focuses on English sentiment classification, many annotated corpora for English sentiment classification are freely available on the Web." ></td>
	<td class="line x" title="16:195	However, the annotated corpora for Chinese sentiment classification are scarce and it is not a trivial task to manually label reliable Chinese sentiment corpora." ></td>
	<td class="line x" title="17:195	The challenge before us is how to leverage rich English corpora for Chinese sentiment classification." ></td>
	<td class="line x" title="18:195	In this study, we focus on the problem of cross-lingual sentiment classification, which leverages only English training data for supervised sentiment classification of Chinese product reviews, without using any Chinese resources." ></td>
	<td class="line x" title="19:195	Note that the above problem is not only defined for Chinese sentiment classification, but also for various sentiment analysis tasks in other different languages." ></td>
	<td class="line x" title="20:195	Though pilot studies have been performed to make use of English corpora for subjectivity classification in other languages (Mihalcea et al., 2007; Banea et al., 2008), the methods are very straightforward by directly employing an inductive classifier (e.g. SVM, NB), and the classification performance is far from satisfactory because of the language gap between the original language and the translated language." ></td>
	<td class="line x" title="21:195	In this study, we propose a co-training approach to improving the classification accuracy of polarity identification of Chinese product reviews." ></td>
	<td class="line x" title="22:195	Unlabeled Chinese reviews can be fully leveraged in the proposed approach." ></td>
	<td class="line x" title="23:195	First, machine translation services are used to translate English training reviews into Chinese reviews and also translate Chinese test reviews and additional unlabeled reviews into English reviews." ></td>
	<td class="line x" title="24:195	Then, we can view the classification problem in two independent views: Chinese view with only Chinese features and English view with only English features." ></td>
	<td class="line x" title="25:195	We then use the co-training approach to making full use of the two redundant views of features." ></td>
	<td class="line x" title="26:195	The SVM classifier is adopted as the basic classifier in the proposed approach." ></td>
	<td class="line x" title="27:195	Experimental results show that the proposed approach can outperform the baseline inductive classifiers and the more advanced transductive classifiers." ></td>
	<td class="line x" title="28:195	The rest of this paper is organized as follows: Section 2 introduces related work." ></td>
	<td class="line x" title="29:195	The proposed 235 co-training approach is described in detail in Section 3." ></td>
	<td class="line x" title="30:195	Section 4 shows the experimental results." ></td>
	<td class="line x" title="31:195	Lastly we conclude this paper in Section 5." ></td>
	<td class="line x" title="32:195	2 Related Work 2.1 Sentiment Classification Sentiment classification can be performed on words, sentences or documents." ></td>
	<td class="line x" title="33:195	In this paper we focus on document sentiment classification." ></td>
	<td class="line x" title="34:195	The methods for document sentiment classification can be generally categorized into lexicon-based and corpus-based." ></td>
	<td class="line x" title="35:195	Lexicon-based methods usually involve deriving a sentiment measure for text based on sentiment lexicons." ></td>
	<td class="line x" title="36:195	Turney (2002) predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs, which is denoted as the semantic oriented method." ></td>
	<td class="line x" title="37:195	Kim and Hovy (2004) build three models to assign a sentiment category to a given sentence by combining the individual sentiments of sentimentbearing words." ></td>
	<td class="line x" title="38:195	Hiroshi et al.(2004) use the technique of deep language analysis for machine translation to extract sentiment units in text documents." ></td>
	<td class="line x" title="40:195	Kennedy and Inkpen (2006) determine the sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers." ></td>
	<td class="line x" title="41:195	Devitt and Ahmad (2007) explore a computable metric of positive or negative polarity in financial news text." ></td>
	<td class="line x" title="42:195	Corpus-based methods usually consider the sentiment analysis task as a classification task and they use a labeled corpus to train a sentiment classifier." ></td>
	<td class="line oc" title="43:195	Since the work of Pang et al.(2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005; Read, 2005)." ></td>
	<td class="line x" title="45:195	Most recently, McDonald et al.(2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity." ></td>
	<td class="line x" title="47:195	Blitzer et al.(2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products." ></td>
	<td class="line x" title="49:195	Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpus-based classifier and a lexicon-based classifier with precision-based vote weighting." ></td>
	<td class="line x" title="50:195	Chinese sentiment analysis has also been studied (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 2007) and most such work uses similar lexiconbased or corpus-based methods for Chinese sentiment classification." ></td>
	<td class="line x" title="51:195	To date, several pilot studies have been performed to leverage rich English resources for sentiment analysis in other languages." ></td>
	<td class="line x" title="52:195	Standard Nave Bayes and SVM classifiers have been applied for subjectivity classification in Romanian (Mihalcea et al., 2007; Banea et al., 2008), and the results show that automatic translation is a viable alternative for the construction of resources and tools for subjectivity analysis in a new target language." ></td>
	<td class="line x" title="53:195	Wan (2008) focuses on leveraging both Chinese and English lexicons to improve Chinese sentiment analysis by using lexicon-based methods." ></td>
	<td class="line x" title="54:195	In this study, we focus on improving the corpus-based method for crosslingual sentiment classification of Chinese product reviews by developing novel approaches." ></td>
	<td class="line x" title="55:195	2.2 Cross-Domain Text Classification Cross-domain text classification can be considered as a more general task than cross-lingual sentiment classification." ></td>
	<td class="line x" title="56:195	In the problem of crossdomain text classification, the labeled and unlabeled data come from different domains, and their underlying distributions are often different from each other, which violates the basic assumption of traditional classification learning." ></td>
	<td class="line x" title="57:195	To date, many semi-supervised learning algorithms have been developed for addressing the cross-domain text classification problem by transferring knowledge across domains, including Transductive SVM (Joachims, 1999), EM(Nigam et al., 2000), EM-based Nave Bayes classifier (Dai et al., 2007a), Topic-bridged PLSA (Xue et al., 2008), Co-Clustering based classification (Dai et al., 2007b), two-stage approach (Jiang and Zhai, 2007)." ></td>
	<td class="line x" title="58:195	DaumIII and Marcu (2006) introduce a statistical formulation of this problem in terms of a simple mixture model." ></td>
	<td class="line x" title="59:195	In particular, several previous studies focus on the problem of cross-lingual text classification, which can be considered as a special case of general cross-domain text classification." ></td>
	<td class="line x" title="60:195	Bel et al.(2003) present practical and cost-effective solutions." ></td>
	<td class="line x" title="62:195	A few novel models have been proposed to address the problem, e.g. the EM-based algorithm (Rigutini et al., 2005), the information bottleneck approach (Ling et al., 2008), the multilingual domain models (Gliozzo and Strapparava, 2005), etc. To the best of our knowledge, cotraining has not yet been investigated for crossdomain or cross-lingual text classification." ></td>
	<td class="line x" title="63:195	236 3 The Co-Training Approach 3.1 Overview The purpose of our approach is to make use of the annotated English corpus for sentiment polarity identification of Chinese reviews in a supervised framework, without using any Chinese resources." ></td>
	<td class="line x" title="64:195	Given the labeled English reviews and unlabeled Chinese reviews, two straightforward methods for addressing the problem are as follows: 1) We first learn a classifier based on the labeled English reviews, and then translate Chinese reviews into English reviews." ></td>
	<td class="line x" title="65:195	Lastly, we use the classifier to classify the translated English reviews." ></td>
	<td class="line x" title="66:195	2) We first translate the labeled English reviews into Chinese reviews, and then learn a classifier based on the translated Chinese reviews with labels." ></td>
	<td class="line x" title="67:195	Lastly, we use the classifier to classify the unlabeled Chinese reviews." ></td>
	<td class="line x" title="68:195	The above two methods have been used in (Banea et al., 2008) for Romanian subjectivity analysis, but the experimental results are not very promising." ></td>
	<td class="line x" title="69:195	As shown in our experiments, the above two methods do not perform well for Chinese sentiment classification, either, because the underlying distribution between the original language and the translated language are different." ></td>
	<td class="line x" title="70:195	In order to address the above problem, we propose to use the co-training approach to make use of some amounts of unlabeled Chinese reviews to improve the classification accuracy." ></td>
	<td class="line x" title="71:195	The co-training approach can make full use of both the English features and the Chinese features in a unified framework." ></td>
	<td class="line x" title="72:195	The framework of the proposed approach is illustrated in Figure 1." ></td>
	<td class="line x" title="73:195	The framework consists of a training phase and a classification phase." ></td>
	<td class="line x" title="74:195	In the training phase, the input is the labeled English reviews and some amounts of unlabeled Chinese reviews 1 . The labeled English reviews are translated into labeled Chinese reviews, and the unlabeled Chinese reviews are translated into unlabeled English reviews, by using machine translation services." ></td>
	<td class="line x" title="75:195	Therefore, each review is associated with an English version and a Chinese version." ></td>
	<td class="line x" title="76:195	The English features and the Chinese features for each review are considered two independent and redundant views of the review." ></td>
	<td class="line x" title="77:195	The co-training algorithm is then applied to learn two classifiers  1  The unlabeled Chinese reviews used for co-training do not include the unlabeled Chinese reviews for testing, i.e., the Chinese reviews for testing are blind to the training phase." ></td>
	<td class="line x" title="78:195	and finally the two classifiers are combined into a single sentiment classifier." ></td>
	<td class="line x" title="79:195	In the classification phase, each unlabeled Chinese review for testing is first translated into English review, and then the learned classifier is applied to classify the review into either positive or negative." ></td>
	<td class="line x" title="80:195	The steps of review translation and the cotraining algorithm are described in details in the next sections, respectively." ></td>
	<td class="line x" title="81:195	Figure 1." ></td>
	<td class="line x" title="82:195	Framework of the proposed approach 3.2 Review Translation In order to overcome the language gap, we must translate one language into another language." ></td>
	<td class="line x" title="83:195	Fortunately, machine translation techniques have been well developed in the NLP field, though the translation performance is far from satisfactory." ></td>
	<td class="line x" title="84:195	A few commercial machine translation services can be publicly accessed, e.g. Google Translate 2 , Yahoo Babel Fish 3  and Windows Live Translate 4 .  2  http://translate.google.com/translate_t 3  http://babelfish.yahoo.com/translate_txt 4  http://www.windowslivetranslator.com/ Unlabeled Chinese Reviews Labeled English Reviews Machine Translation (CN-EN) Co-Training Machine Translation (EN-CN) Labeled Chinese Reviews Unlabeled English Reviews Pos\Neg Chinese View English View Test Chinese Review Sentiment Classifier Machine Translation (CN-EN) Test English Review Training Phase Classification Phase 237 In this study, we adopt Google Translate for both English-to-Chinese Translation and Chinese-toEnglish Translation, because it is one of the state-of-the-art commercial machine translation systems used today." ></td>
	<td class="line x" title="85:195	Google Translate applies statistical learning techniques to build a translation model based on both monolingual text in the target language and aligned text consisting of examples of human translations between the languages." ></td>
	<td class="line x" title="86:195	3.3 The Co-Training Algorithm The co-training algorithm (Blum and Mitchell, 1998) is a typical bootstrapping method, which starts with a set of labeled data, and increase the amount of annotated data using some amounts of unlabeled data in an incremental way." ></td>
	<td class="line x" title="87:195	One important aspect of co-training is that two conditional independent views are required for cotraining to work, but the independence assumption can be relaxed." ></td>
	<td class="line x" title="88:195	Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001)." ></td>
	<td class="line x" title="89:195	In the context of cross-lingual sentiment classification, each labeled English review or unlabeled Chinese review has two views of features: English features and Chinese features." ></td>
	<td class="line x" title="90:195	Here, a review is used to indicate both its Chinese version and its English version, until stated otherwise." ></td>
	<td class="line x" title="91:195	The co-training algorithm is illustrated in Figure 2." ></td>
	<td class="line x" title="92:195	In the algorithm, the class distribution in the labeled data is maintained by balancing the parameter values of p and n at each iteration." ></td>
	<td class="line x" title="93:195	The intuition of the co-training algorithm is that if one classifier can confidently predict the class of an example, which is very similar to some of labeled ones, it can provide one more training example for the other classifier." ></td>
	<td class="line x" title="94:195	But, of course, if this example happens to be easy to be classified by the first classifier, it does not mean that this example will be easy to be classified by the second classifier, so the second classifier will get useful information to improve itself and vice versa (Kiritchenko and Matwin, 2001)." ></td>
	<td class="line x" title="95:195	In the co-training algorithm, a basic classification algorithm is required to construct C en  and C cn . Typical text classifiers include Support Vector Machine (SVM), Nave Bayes (NB), Maximum Entropy (ME), K-Nearest Neighbor (KNN), etc. In this study, we adopt the widely-used SVM classifier (Joachims, 2002)." ></td>
	<td class="line x" title="96:195	Viewing input data as two sets of vectors in a feature space, SVM constructs a separating hyperplane in the space by maximizing the margin between the two data sets." ></td>
	<td class="line x" title="97:195	The English or Chinese features used in this study include both unigrams and bigrams 5  and the feature weight is simply set to term frequency 6 . Feature selection methods (e.g. Document Frequency (DF), Information Gain (IG), and Mutual Information (MI)) can be used for dimension reduction." ></td>
	<td class="line x" title="98:195	But we use all the features in the experiments for comparative analysis, because there is no significant performance improvement after applying the feature selection techniques in our empirical study." ></td>
	<td class="line x" title="99:195	The output value of the SVM classifier for a review indicates the confidence level of the reviews classification." ></td>
	<td class="line x" title="100:195	Usually, the sentiment polarity of a review is indicated by the sign of the prediction value." ></td>
	<td class="line x" title="101:195	Given: F en  and F cn  are redundantly sufficient sets of features, where F en  represents the English features, F cn  represents the Chinese features; L is a set of labeled training reviews; U is a set of unlabeled reviews; Loop for I iterations: 1." ></td>
	<td class="line x" title="102:195	Learn the first classifier C en  from L based on F en ; 2." ></td>
	<td class="line x" title="103:195	Use C en  to label reviews from U based on F en ; 3." ></td>
	<td class="line x" title="104:195	Choose p positive and n negative the most confidently predicted reviews E en  from U; 4." ></td>
	<td class="line x" title="105:195	Learn the second classifier C cn  from L based on F cn ; 5." ></td>
	<td class="line x" title="106:195	Use C cn  to label reviews from U based on F cn ; 6." ></td>
	<td class="line x" title="107:195	Choose p positive and n negative the most confidently predicted reviews E cn  from U; 7." ></td>
	<td class="line x" title="108:195	Removes reviews E en E cn  from U 7 ; 8." ></td>
	<td class="line x" title="109:195	Add reviews E en E cn  with the corresponding labels to L; Figure 2." ></td>
	<td class="line x" title="110:195	The co-training algorithm In the training phase, the co-training algorithm learns two separate classifiers: C en  and C cn .  5  For Chinese text, a unigram refers to a Chinese word and a bigram refers to two adjacent Chinese words." ></td>
	<td class="line x" title="111:195	6  Term frequency performs better than TFIDF by our empirical analysis." ></td>
	<td class="line x" title="112:195	7  Note that the examples with conflicting labels are not included in E en E cn In other words, if an example is in both E en  and E cn , but the labels for the example is conflicting, the example will be excluded from E en E cn." ></td>
	<td class="line x" title="113:195	238 Therefore, in the classification phase, we can obtain two prediction values for a test review." ></td>
	<td class="line x" title="114:195	We normalize the prediction values into [-1, 1] by dividing the maximum absolute value." ></td>
	<td class="line x" title="115:195	Finally, the average of the normalized values is used as the overall prediction value of the review." ></td>
	<td class="line x" title="116:195	4 Empirical Evaluation 4.1 Evaluation Setup 4.1.1 Data set The following three datasets were collected and used in the experiments: Test Set (Labeled Chinese Reviews): In order to assess the performance of the proposed approach, we collected and labeled 886 product reviews (451 positive reviews + 435 negative reviews) from a popular Chinese IT product web site-IT168 8 . The reviews focused on such products as mp3 players, mobile phones, digital camera and laptop computers." ></td>
	<td class="line x" title="117:195	Training Set (Labeled English Reviews): There are many labeled English corpora available on the Web and we used the corpus constructed for multi-domain sentiment classification (Blitzer et al., 2007) 9 , because the corpus was large-scale and it was within similar domains as the test set." ></td>
	<td class="line x" title="118:195	The dataset consisted of 8000 Amazon product reviews (4000 positive reviews + 4000 negative reviews) for four different product types: books, DVDs, electronics and kitchen appliances." ></td>
	<td class="line x" title="119:195	Unlabeled Set (Unlabeled Chinese Reviews): We downloaded additional 1000 Chinese product reviews from IT168 and used the reviews as the unlabeled set." ></td>
	<td class="line x" title="120:195	Therefore, the unlabeled set and the test set were in the same domain and had similar underlying feature distributions." ></td>
	<td class="line x" title="121:195	Each Chinese review was translated into English review, and each English review was translated into Chinese review." ></td>
	<td class="line x" title="122:195	Therefore, each review has two independent views: English view and Chinese view." ></td>
	<td class="line x" title="123:195	A review is represented by both its English view and its Chinese view." ></td>
	<td class="line x" title="124:195	Note that the training set and the unlabeled set are used in the training phase, while the test set is blind to the training phase." ></td>
	<td class="line x" title="125:195	4.1.2 Evaluation Metric We used the standard precision, recall and Fmeasure to measure the performance of positive and negative class, respectively, and used the  8  http://www.it168.com 9  http://www.cis.upenn.edu/~mdredze/datasets/sentiment/ accuracy metric to measure the overall performance of the system." ></td>
	<td class="line x" title="126:195	The metrics are defined the same as in general text categorization." ></td>
	<td class="line x" title="127:195	4.1.3 Baseline Methods In the experiments, the proposed co-training approach (CoTrain) is compared with the following baseline methods: SVM(CN): This method applies the inductive SVM with only Chinese features for sentiment classification in the Chinese view." ></td>
	<td class="line x" title="128:195	Only Englishto-Chinese translation is needed." ></td>
	<td class="line x" title="129:195	And the unlabeled set is not used." ></td>
	<td class="line x" title="130:195	SVM(EN): This method applies the inductive SVM with only English features for sentiment classification in the English view." ></td>
	<td class="line x" title="131:195	Only Chineseto-English translation is needed." ></td>
	<td class="line x" title="132:195	And the unlabeled set is not used." ></td>
	<td class="line x" title="133:195	SVM(ENCN1): This method applies the inductive SVM with both English and Chinese features for sentiment classification in the two views." ></td>
	<td class="line x" title="134:195	Both English-to-Chinese and Chinese-toEnglish translations are required." ></td>
	<td class="line x" title="135:195	And the unlabeled set is not used." ></td>
	<td class="line x" title="136:195	SVM(ENCN2): This method combines the results of SVM(EN) and SVM(CN) by averaging the prediction values in the same way with the co-training approach." ></td>
	<td class="line x" title="137:195	TSVM(CN): This method applies the transductive SVM with only Chinese features for sentiment classification in the Chinese view." ></td>
	<td class="line x" title="138:195	Only English-to-Chinese translation is needed." ></td>
	<td class="line x" title="139:195	And the unlabeled set is used." ></td>
	<td class="line x" title="140:195	TSVM(EN): This method applies the transductive SVM with only English features for sentiment classification in the English view." ></td>
	<td class="line x" title="141:195	Only Chinese-to-English translation is needed." ></td>
	<td class="line x" title="142:195	And the unlabeled set is used." ></td>
	<td class="line x" title="143:195	TSVM(ENCN1): This method applies the transductive SVM with both English and Chinese features for sentiment classification in the two views." ></td>
	<td class="line x" title="144:195	Both English-to-Chinese and Chinese-toEnglish translations are required." ></td>
	<td class="line x" title="145:195	And the unlabeled set is used." ></td>
	<td class="line x" title="146:195	TSVM(ENCN2): This method combines the results of TSVM(EN) and TSVM(CN) by averaging the prediction values." ></td>
	<td class="line x" title="147:195	Note that the first four methods are straightforward methods used in previous work, while the latter four methods are strong baselines because the transductive SVM has been widely used for improving the classification accuracy by leveraging additional unlabeled examples." ></td>
	<td class="line x" title="148:195	239 4.2 Evaluation Results 4.2.1 Method Comparison In the experiments, we first compare the proposed co-training approach (I=40 and p=n=5) with the eight baseline methods." ></td>
	<td class="line x" title="149:195	The three parameters in the co-training approach are empirically set by considering the total number (i.e. 1000) of the unlabeled Chinese reviews." ></td>
	<td class="line x" title="150:195	In our empirical study, the proposed approach can perform well with a wide range of parameter values, which will be shown later." ></td>
	<td class="line x" title="151:195	Table 1 shows the comparison results." ></td>
	<td class="line x" title="152:195	Seen from the table, the proposed co-training approach outperforms all eight baseline methods over all metrics." ></td>
	<td class="line x" title="153:195	Among the eight baselines, the best one is TSVM(ENCN2), which combines the results of two transductive SVM classifiers." ></td>
	<td class="line x" title="154:195	Actually, TSVM(ENCN2) is similar to CoTrain because CoTrain also combines the results of two classifiers in the same way." ></td>
	<td class="line x" title="155:195	However, the co-training approach can train two more effective classifiers, and the accuracy values of the component English and Chinese classifiers are 0.775 and 0.790, respectively, which are higher than the corresponding TSVM classifiers." ></td>
	<td class="line x" title="156:195	Overall, the use of transductive learning and the combination of English and Chinese views are beneficial to the final classification accuracy, and the cotraining approach is more suitable for making use of the unlabeled Chinese reviews than the transductive SVM." ></td>
	<td class="line x" title="157:195	4.2.2 Influences of Iteration Number (I) Figure 3 shows the accuracy curve of the cotraining approach (Combined Classifier) with different numbers of iterations." ></td>
	<td class="line x" title="158:195	The iteration number I is varied from 1 to 80." ></td>
	<td class="line x" title="159:195	When I is set to 1, the co-training approach is degenerated into SVM(ENCN2)." ></td>
	<td class="line x" title="160:195	The accuracy curves of the component English and Chinese classifiers learned in the co-training approach are also shown in the figure." ></td>
	<td class="line x" title="161:195	We can see that the proposed co-training approach can outperform the best baselineTSVM(ENCN2) after 20 iterations." ></td>
	<td class="line x" title="162:195	After a large number of iterations, the performance of the cotraining approach decreases because noisy training examples may be selected from the remaining unlabeled set." ></td>
	<td class="line x" title="163:195	Finally, the performance of the approach does not change any more, because the algorithm runs out of all possible examples in the unlabeled set." ></td>
	<td class="line x" title="164:195	Fortunately, the proposed approach performs well with a wide range of iteration numbers." ></td>
	<td class="line x" title="165:195	We can also see that the two component classifier has similar trends with the cotraining approach." ></td>
	<td class="line x" title="166:195	It is encouraging that the component Chinese classifier alone can perform better than the best baseline when the iteration number is set between 40 and 70." ></td>
	<td class="line x" title="167:195	4.2.3 Influences of Growth Size (p, n) Figure 4 shows how the growth size at each iteration (p positive and n negative confident examples) influences the accuracy of the proposed co-training approach." ></td>
	<td class="line x" title="168:195	In the above experiments, we set p=n, which is considered as a balanced growth." ></td>
	<td class="line x" title="169:195	When p differs very much from n, the growth is considered as an imbalanced growth." ></td>
	<td class="line x" title="170:195	Balanced growth of (2, 2), (5, 5), (10, 10) and (15, 15) examples and imbalanced growth of (1, 5), (5, 1) examples are compared in the figure." ></td>
	<td class="line x" title="171:195	We can see that the performance of the cotraining approach with the balanced growth can be improved after a few iterations." ></td>
	<td class="line x" title="172:195	And the performance of the co-training approach with large p and n will more quickly become unchanged, because the approach runs out of the limited examples in the unlabeled set more quickly." ></td>
	<td class="line x" title="173:195	However, the performance of the co-training approaches with the two imbalanced growths is always going down quite rapidly, because the labeled unbalanced examples hurt the performance badly at each iteration." ></td>
	<td class="line x" title="174:195	Positive Negative Total Method Precision Recall F-measure Precision Recall F-measure Accuracy SVM(CN) 0.733 0.865 0.793 0.828 0.674 0.743 0.771 SVM(EN) 0.717 0.803 0.757 0.766 0.671 0.716 0.738 SVM(ENCN1) 0.744 0.820 0.781 0.792 0.708 0.748 0.765 SVM(ENCN2) 0.746 0.847 0.793 0.816 0.701 0.754 0.775 TSVM(CN) 0.724 0.878 0.794 0.838 0.653 0.734 0.767 TSVM(EN) 0.732 0.860 0.791 0.823 0.674 0.741 0.769 TSVM(ENCN1) 0.743 0.878 0.805 0.844 0.685 0.756 0.783 TSVM(ENCN2) 0.744 0.896 0.813 0.863 0.680 0.761 0.790 CoTrain (I=40; p=n=5) 0.768 0.905 0.831 0.879 0.717 0.790 0.813 Table 1." ></td>
	<td class="line x" title="175:195	Comparison results 240 0.72 0.73 0.74 0.75 0.76 0.77 0.78 0.79 0.8 0.81 0.82 1 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 Iteration Number (I ) Acc u ra c y English Classifier(CoTrain) Chinese Classifier(CoTrain) Combined Classifier(CoTrain) TSVM(ENCN2)  Figure 3." ></td>
	<td class="line x" title="176:195	Accuracy vs. number of iterations for co-training (p=n=5) 0.5 0.55 0.6 0.65 0.7 0.75 0.8 1 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 Iteration Number (I ) Ac cura c y (p=2,n=2) (p=5,n=5) (p=10,n=10) (p=15,n=15) (p=1,n=5) (p=5,n=1)  Figure 4." ></td>
	<td class="line x" title="177:195	Accuracy vs. different (p, n) for co-training 0.76 0.77 0.78 0.79 0.8 0.81 0.82 25% 50% 75% 100% Feature size Acc u r a c y TSVM(ENCN1) TSVM(ENCN2) CoTrain (I=40; p=n=5)  Figure 5." ></td>
	<td class="line x" title="178:195	Influences of feature size  241 4.2.4 Influences of Feature Selection In the above experiments, all features (unigram + bigram) are used." ></td>
	<td class="line x" title="179:195	As mentioned earlier, feature selection techniques are widely used for dimension reduction." ></td>
	<td class="line x" title="180:195	In this section, we further conduct experiments to investigate the influences of feature selection techniques on the classification results." ></td>
	<td class="line x" title="181:195	We use the simple but effective document frequency (DF) for feature selection." ></td>
	<td class="line x" title="182:195	Figures 6 show the comparison results of different feature sizes for the co-training approach and two strong baselines." ></td>
	<td class="line x" title="183:195	The feature size is measured as the proportion of the selected features against the total features (i.e. 100%)." ></td>
	<td class="line x" title="184:195	We can see from the figure that the feature selection technique has very slight influences on the classification accuracy of the methods." ></td>
	<td class="line x" title="185:195	It can be seen that the co-training approach can always outperform the two baselines with different feature sizes." ></td>
	<td class="line x" title="186:195	The results further demonstrate the effectiveness and robustness of the proposed cotraining approach." ></td>
	<td class="line x" title="187:195	5 Conclusion and Future Work In this paper, we propose to use the co-training approach to address the problem of cross-lingual sentiment classification." ></td>
	<td class="line x" title="188:195	The experimental results show the effectiveness of the proposed approach." ></td>
	<td class="line x" title="189:195	In future work, we will improve the sentiment classification accuracy in the following two ways: 1) The smoothed co-training approach used in (Mihalcea, 2004) will be adopted for sentiment classification." ></td>
	<td class="line x" title="190:195	The approach has the effect of smoothing the learning curves." ></td>
	<td class="line x" title="191:195	During the bootstrapping process of smoothed co-training, the classifier at each iteration is replaced with a majority voting scheme applied to all classifiers constructed at previous iterations." ></td>
	<td class="line x" title="192:195	2) The feature distributions of the translated text and the natural text in the same language are still different due to the inaccuracy of the machine translation service." ></td>
	<td class="line x" title="193:195	We will employ the structural correspondence learning (SCL) domain adaption algorithm used in (Blitzer et al., 2007) for linking the translated text and the natural text." ></td>
	<td class="line x" title="194:195	Acknowledgments This work was supported by NSFC (60873155), RFDP (20070001059), Beijing Nova Program (2008B03), National High-tech R&D Program (2008AA01Z421) and NCET (NCET-08-0006)." ></td>
	<td class="line x" title="195:195	We also thank the anonymous reviewers for their useful comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1028
A Non-negative Matrix Tri-factorization Approach to Sentiment Classification with Lexical Prior Knowledge
Li, Tao;Zhang, Yi;Sindhwani, Vikas;"></td>
	<td class="line x" title="1:191	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 244252, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:191	c2009 ACL and AFNLP A Non-negative Matrix Tri-factorization Approach to Sentiment Classification with Lexical Prior Knowledge Tao Li Yi Zhang School of Computer Science Florida International University {taoli,yzhan004}@cs.fiu.edu Vikas Sindhwani Mathematical Sciences IBM T.J. Watson Research Center vsindhw@us.ibm.com Abstract Sentiment classi cation refers to the task of automatically identifying whether a given piece of text expresses positive or negative opinion towards a subject at hand." ></td>
	<td class="line x" title="3:191	The proliferation of user-generated web content such as blogs, discussion forums and online review sites has made it possible to perform large-scale mining of public opinion." ></td>
	<td class="line x" title="4:191	Sentiment modeling is thus becoming a critical component of market intelligence and social media technologies that aim to tap into the collective wisdom of crowds." ></td>
	<td class="line x" title="5:191	In this paper, we consider the problem of learning high-quality sentiment models with minimal manual supervision." ></td>
	<td class="line x" title="6:191	We propose a novel approach to learn from lexical prior knowledge in the form of domain-independent sentimentladen terms, in conjunction with domaindependent unlabeled data and a few labeled documents." ></td>
	<td class="line x" title="7:191	Our model is based on a constrained non-negative tri-factorization of the term-document matrix which can be implemented using simple update rules." ></td>
	<td class="line x" title="8:191	Extensive experimental studies demonstrate the effectiveness of our approach on a variety of real-world sentiment prediction tasks." ></td>
	<td class="line x" title="9:191	1 Introduction Web 2.0 platforms such as blogs, discussion forums and other such social media have now given a public voice to every consumer." ></td>
	<td class="line x" title="10:191	Recent surveys have estimated that a massive number of internet users turn to such forums to collect recommendations for products and services, guiding their own choices and decisions by the opinions that other consumers have publically expressed." ></td>
	<td class="line x" title="11:191	Gleaning insights by monitoring and analyzing large amounts of such user-generated data is thus becoming a key competitive differentiator for many companies." ></td>
	<td class="line x" title="12:191	While tracking brand perceptions in traditional media is hardly a new challenge, handling the unprecedented scale of unstructured user-generated web content requires new methodologies." ></td>
	<td class="line x" title="13:191	These methodologies are likely to be rooted in natural language processing and machine learning techniques." ></td>
	<td class="line x" title="14:191	Automatically classifying the sentiment expressed in a blog around selected topics of interest is a canonical machine learning task in this discussion." ></td>
	<td class="line x" title="15:191	A standard approach would be to manually label documents with their sentiment orientation and then apply off-the-shelf text classi cation techniques." ></td>
	<td class="line x" title="16:191	However, sentiment is often conveyed with subtle linguistic mechanisms such as the use of sarcasm and highly domain-speci c contextual cues." ></td>
	<td class="line x" title="17:191	This makes manual annotation of sentiment time consuming and error-prone, presenting a bottleneck in learning high quality models." ></td>
	<td class="line x" title="18:191	Moreover, products and services of current focus, and associated community of bloggers with their idiosyncratic expressions, may rapidly evolve over time causing models to potentially lose performance and become stale." ></td>
	<td class="line x" title="19:191	This motivates the problem of learning robust sentiment models from minimal supervision." ></td>
	<td class="line pc" title="20:191	In their seminal work, (Pang et al., 2002) demonstrated that supervised learning signi cantly outperformed a competing body of work where hand-crafted dictionaries are used to assign sentiment labels based on relative frequencies of positive and negative terms." ></td>
	<td class="line x" title="21:191	As observed by (Ng et al., 2006), most semi-automated dictionary-based approaches yield unsatisfactory lexicons, with either high coverage and low precision or vice versa." ></td>
	<td class="line x" title="22:191	However, the treatment of such dictionaries as forms of prior knowledge that can be incorporated in machine learning models is a relatively less explored topic; even lesser so in conjunction with semi-supervised models that attempt to utilize un244 labeled data." ></td>
	<td class="line x" title="23:191	This is the focus of the current paper." ></td>
	<td class="line x" title="24:191	Our models are based on a constrained nonnegative tri-factorization of the term-document matrix, which can be implemented using simple update rules." ></td>
	<td class="line x" title="25:191	Treated as a set of labeled features, the sentiment lexicon is incorporated as one set of constraints that enforce domain-independent prior knowledge." ></td>
	<td class="line x" title="26:191	A second set of constraints introduce domain-speci c supervision via a few document labels." ></td>
	<td class="line x" title="27:191	Together these constraints enable learning from partial supervision along both dimensions of the term-document matrix, in what may be viewed more broadly as a framework for incorporating dual-supervision in matrix factorization models." ></td>
	<td class="line x" title="28:191	We provide empirical comparisons with several competing methodologies on four, very different domains  blogs discussing enterprise software products, political blogs discussing US presidential candidates, amazon.com product reviews and IMDB movie reviews." ></td>
	<td class="line x" title="29:191	Results demonstrate the effectiveness and generality of our approach." ></td>
	<td class="line x" title="30:191	The rest of the paper is organized as follows." ></td>
	<td class="line x" title="31:191	We begin by discussing related work in Section 2." ></td>
	<td class="line x" title="32:191	Section 3 gives a quick background on Nonnegative Matrix Tri-factorization models." ></td>
	<td class="line x" title="33:191	In Section 4, we present a constrained model and computational algorithm for incorporating lexical knowledge in sentiment analysis." ></td>
	<td class="line x" title="34:191	In Section 5, we enhance this model by introducing document labels as additional constraints." ></td>
	<td class="line x" title="35:191	Section 6 presents an empirical study on four datasets." ></td>
	<td class="line x" title="36:191	Finally, Section 7 concludes this paper." ></td>
	<td class="line x" title="37:191	2 Related Work We point the reader to a recent book (Pang and Lee, 2008) for an in-depth survey of literature on sentiment analysis." ></td>
	<td class="line x" title="38:191	In this section, we briskly cover related work to position our contributions appropriately in the sentiment analysis and machine learning literature." ></td>
	<td class="line x" title="39:191	Methods focussing on the use and generation of dictionaries capturing the sentiment of words have ranged from manual approaches of developing domain-dependent lexicons (Das and Chen, 2001) to semi-automated approaches (Hu and Liu, 2004; Zhuang et al., 2006; Kim and Hovy, 2004), and even an almost fully automated approach (Turney, 2002)." ></td>
	<td class="line nc" title="40:191	Most semi-automated approaches have met with limited success (Ng et al., 2006) and supervised learning models have tended to outperform dictionary-based classi cation schemes (Pang et al., 2002)." ></td>
	<td class="line x" title="41:191	A two-tier scheme (Pang and Lee, 2004) where sentences are  rst classi ed as subjective versus objective, and then applying the sentiment classi er on only the subjective sentences further improves performance." ></td>
	<td class="line x" title="42:191	Results in these papers also suggest that using more sophisticated linguistic models, incorporating parts-of-speech and n-gram language models, do not improve over the simple unigram bag-of-words representation." ></td>
	<td class="line x" title="43:191	In keeping with these  ndings, we also adopt a unigram text model." ></td>
	<td class="line x" title="44:191	A subjectivity classi cation phase before our models are applied may further improve the results reported in this paper, but our focus is on driving the polarity prediction stage with minimal manual effort." ></td>
	<td class="line x" title="45:191	In this regard, our model brings two interrelated but distinct themes from machine learning to bear on this problem: semi-supervised learning and learning from labeled features." ></td>
	<td class="line x" title="46:191	The goal of the former theme is to learn from few labeled examples by making use of unlabeled data, while the goal of the latter theme is to utilize weak prior knowledge about term-class af nities (e.g., the term  awful indicates negative sentiment and therefore may be considered as a negatively labeled feature)." ></td>
	<td class="line x" title="47:191	Empirical results in this paper demonstrate that simultaneously attempting both these goals in a single model leads to improvements over models that focus on a single goal." ></td>
	<td class="line x" title="48:191	(Goldberg and Zhu, 2006) adapt semi-supervised graph-based methods for sentiment analysis but do not incorporate lexical prior knowledge in the form of labeled features." ></td>
	<td class="line x" title="49:191	Most work in machine learning literature on utilizing labeled features has focused on using them to generate weakly labeled examples that are then used for standard supervised learning: (Schapire et al., 2002) propose one such framework for boosting logistic regression; (Wu and Srihari, 2004) build a modi ed SVM and (Liu et al., 2004) use a combination of clustering and EM based methods to instantiate similar frameworks." ></td>
	<td class="line x" title="50:191	By contrast, we incorporate lexical knowledge directly as constraints on our matrix factorization model." ></td>
	<td class="line x" title="51:191	In recent work, Druck et al.(Druck et al., 2008) constrain the predictions of a multinomial logistic regression model on unlabeled instances in a Generalized Expectation formulation for learning from labeled features." ></td>
	<td class="line x" title="53:191	Unlike their approach which uses only unlabeled instances, our method uses both labeled and unlabeled documents in conjunction with labeled and 245 unlabeled words." ></td>
	<td class="line x" title="54:191	The matrix tri-factorization models explored in this paper are closely related to the models proposed recently in (Li et al., 2008; Sindhwani et al., 2008)." ></td>
	<td class="line x" title="55:191	Though, their techniques for proving algorithm convergence and correctness can be readily adapted for our models, (Li et al., 2008) do not incorporate dual supervision as we do." ></td>
	<td class="line x" title="56:191	On the other hand, while (Sindhwani et al., 2008) do incorporate dual supervision in a non-linear kernelbased setting, they do not enforce non-negativity or orthogonality  aspects of matrix factorization models that have shown bene ts in prior empirical studies, see e.g., (Ding et al., 2006)." ></td>
	<td class="line x" title="57:191	We also note the very recent work of (Sindhwani and Melville, 2008) which proposes a dualsupervision model for semi-supervised sentiment analysis." ></td>
	<td class="line x" title="58:191	In this model, bipartite graph regularization is used to diffuse label information along both sides of the term-document matrix." ></td>
	<td class="line x" title="59:191	Conceptually, their model implements a co-clustering assumption closely related to Singular Value Decomposition (see also (Dhillon, 2001; Zha et al., 2001) for more on this perspective) while our model is based on Non-negative Matrix Factorization." ></td>
	<td class="line x" title="60:191	In another recent paper (Sandler et al., 2008), standard regularization models are constrained using graphs of word co-occurences." ></td>
	<td class="line x" title="61:191	These are very recently proposed competing methodologies, and we have not been able to address empirical comparisons with them in this paper." ></td>
	<td class="line x" title="62:191	Finally, recent efforts have also looked at transfer learning mechanisms for sentiment analysis, e.g., see (Blitzer et al., 2007)." ></td>
	<td class="line x" title="63:191	While our focus is on single-domain learning in this paper, we note that cross-domain variants of our model can also be orthogonally developed." ></td>
	<td class="line x" title="64:191	3 Background 3.1 Basic Matrix Factorization Model Our proposed models are based on non-negative matrix Tri-factorization (Ding et al., 2006)." ></td>
	<td class="line x" title="65:191	In these models, an m n term-document matrix X is approximated by three factors that specify soft membership of terms and documents in one of kclasses: X FSGT ." ></td>
	<td class="line x" title="66:191	(1) where F is an m k non-negative matrix representing knowledge in the word space, i.e., i-th row of F represents the posterior probability of word i belonging to the k classes, G is an n k nonnegative matrix representing knowledge in document space, i.e., the i-th row of G represents the posterior probability of document i belonging to the k classes, and S is an k k nonnegative matrix providing a condensed view of X. The matrix factorization model is similar to the probabilistic latent semantic indexing (PLSI) model (Hofmann, 1999)." ></td>
	<td class="line x" title="67:191	In PLSI, X is treated as the joint distribution between words and documents by the scaling X ! flX = X/i j Xi j thus i j flXi j = 1)." ></td>
	<td class="line x" title="68:191	flX is factorized as flX WSDT , k Wik = 1, k D jk = 1, k Skk = 1." ></td>
	<td class="line x" title="69:191	(2) where X is the m n word-document semantic matrix, X = WSD, W is the word classconditional probability, and D is the document class-conditional probability and S is the class probability distribution." ></td>
	<td class="line x" title="70:191	PLSI provides a simultaneous solution for the word and document class conditional distribution." ></td>
	<td class="line x" title="71:191	Our model provides simultaneous solution for clustering the rows and the columns of X. To avoid ambiguity, the orthogonality conditions FT F = I, GT G = I." ></td>
	<td class="line x" title="72:191	(3) can be imposed to enforce each row of F and G to possess only one nonzero entry." ></td>
	<td class="line x" title="73:191	Approximating the term-document matrix with a tri-factorization while imposing non-negativity and orthogonality constraints gives a principled framework for simultaneously clustering the rows (words) and columns (documents) of X. In the context of coclustering, these models return excellent empirical performance, see e.g., (Ding et al., 2006)." ></td>
	<td class="line x" title="74:191	Our goal now is to bias these models with constraints incorporating (a) labels of features (coming from a domain-independent sentiment lexicon), and (b) labels of documents for the purposes of domainspeci c adaptation." ></td>
	<td class="line x" title="75:191	These enhancements are addressed in Sections 4 and 5 respectively." ></td>
	<td class="line x" title="76:191	4 Incorporating Lexical Knowledge We used a sentiment lexicon generated by the IBM India Research Labs that was developed for other text mining applications (Ramakrishnan et al., 2003)." ></td>
	<td class="line x" title="77:191	It contains 2,968 words that have been human-labeled as expressing positive or negative sentiment." ></td>
	<td class="line x" title="78:191	In total, there are 1,267 positive (e.g.  great ) and 1,701 negative (e.g.,  bad ) unique 246 terms after stemming." ></td>
	<td class="line x" title="79:191	We eliminated terms that were ambiguous and dependent on context, such as  dear and   ne . It should be noted, that this list was constructed without a speci c domain in mind; which is further motivation for using training examples and unlabeled data to learn domain speci c connotations." ></td>
	<td class="line x" title="80:191	Lexical knowledge in the form of the polarity of terms in this lexicon can be introduced in the matrix factorization model." ></td>
	<td class="line x" title="81:191	By partially specifying term polarities via F, the lexicon in uences the sentiment predictions G over documents." ></td>
	<td class="line x" title="82:191	4.1 Representing Knowledge in Word Space Let F0 represent prior knowledge about sentimentladen words in the lexicon, i.e., if word i is a positive word (F0)i1 = 1 while if it is negative (F0)i2 = 1." ></td>
	<td class="line x" title="83:191	Note that one may also use soft sentiment polarities though our experiments are conducted with hard assignments." ></td>
	<td class="line x" title="84:191	This information is incorporated in the tri-factorization model via a squared loss term, min F;G;S kX FSGTk2 + Trbracketleftbig(F F0)TC1(F F0)bracketrightbig (4) where the notation Tr(A) means trace of the matrix A. Here,  > 0 is a parameter which determines the extent to which we enforce F F0, C1 is a m m diagonal matrix whose entry (C1)ii = 1 if the category of the i-th word is known (i.e., speci ed by the i-th row of F0) and (C1)ii = 0 otherwise." ></td>
	<td class="line x" title="85:191	The squared loss terms ensure that the solution for F in the otherwise unsupervised learning problem be close to the prior knowledge F0." ></td>
	<td class="line x" title="86:191	Note that if C1 = I, then we know the class orientation of all the words and thus have a full speci cation of F0, Eq.(4) is then reduced to min F;G;S kX FSGTk2 + kF F0k2 (5) The above model is generic and it allows certain  exibility." ></td>
	<td class="line x" title="87:191	For example, in some cases, our prior knowledge on F0 is not very accurate and we use smaller  so that the  nal results are not dependent on F0 very much, i.e., the results are mostly unsupervised learning results." ></td>
	<td class="line x" title="88:191	In addition, the introduction of C1 allows us to incorporate partial knowledge on word polarity information." ></td>
	<td class="line x" title="89:191	4.2 Computational Algorithm The optimization problem in Eq.( 4) can be solved using the following update rules G jk G jk (X T FS)jk (GGT XT FS)jk , (6) Sik  Sik (F T XG)ik (FT FSGT G)ik ." ></td>
	<td class="line x" title="90:191	(7) Fik Fik (XGS T + C1F0)ik (FFT XGST + C1F)ik ." ></td>
	<td class="line x" title="91:191	(8) The algorithm consists of an iterative procedure using the above three rules until convergence." ></td>
	<td class="line x" title="92:191	We call this approach Matrix Factorization with Lexical Knowledge (MFLK) and outline the precise steps in the table below." ></td>
	<td class="line x" title="93:191	Algorithm 1 Matrix Factorization with Lexical Knowledge (MFLK) begin 1." ></td>
	<td class="line x" title="94:191	Initialization: Initialize F = F0 G to K-means clustering results, S = (FT F) 1FT XG(GT G) 1." ></td>
	<td class="line x" title="95:191	2. Iteration: Update G:  xing F,S, updating G Update F:  xing S,G, updating F Update S:  xing F,G, updating S end 4.3 Algorithm Correctness and Convergence Updating F,G,S using the rules above leads to an asymptotic convergence to a local minima." ></td>
	<td class="line x" title="96:191	This can be proved using arguments similar to (Ding et al., 2006)." ></td>
	<td class="line x" title="97:191	We outline the proof of correctness for updating F since the squared loss term that involves F is a new component in our models." ></td>
	<td class="line x" title="98:191	Theorem 1 The above iterative algorithm converges." ></td>
	<td class="line x" title="99:191	Theorem 2 At convergence, the solution satisfies the Karuch, Kuhn, Tucker optimality condition, i.e., the algorithm converges correctly to a local optima." ></td>
	<td class="line x" title="100:191	Theorem 1 can be proved using the standard auxiliary function approach used in (Lee and Seung, 2001)." ></td>
	<td class="line x" title="101:191	Proof of Theorem 2." ></td>
	<td class="line x" title="102:191	Following the theory of constrained optimization (Nocedal and Wright, 1999), 247 we minimize the following function L(F) =jjX FSGTjj2 +Trbracketleftbig(F F0)TC1(F F0)bracketrightbig Note that the gradient of L is, L F = 2XGS T + 2FSGT GST + 2C1(F F0)." ></td>
	<td class="line x" title="103:191	(9) The KKT complementarity condition for the nonnegativity of Fik gives [ 2XGST + FSGT GST + 2C1(F F0)]ikFik = 0." ></td>
	<td class="line x" title="104:191	(10) This is the  xed point relation that local minima for F must satisfy." ></td>
	<td class="line x" title="105:191	Given an initial guess of F, the successive update of F using Eq.(8) will converge to a local minima." ></td>
	<td class="line x" title="106:191	At convergence, we have Fik = Fik (XGS T + C1F0)ik (FFT XGST + C1F)ik . which is equivalent to the KKT condition of Eq.(10)." ></td>
	<td class="line x" title="107:191	The correctness of updating rules for G in Eq.(6) and S in Eq.(7) have been proved in (Ding et al., 2006)." ></td>
	<td class="line x" title="108:191	u Note that we do not enforce exact orthogonality in our updating rules since this often implies softer class assignments." ></td>
	<td class="line x" title="109:191	5 Semi-Supervised Learning With Lexical Knowledge So far our models have made no demands on human effort, other than unsupervised collection of the term-document matrix and a one-time effort in compiling a domain-independent sentiment lexicon." ></td>
	<td class="line x" title="110:191	We now assume that a few documents are manually labeled for the purposes of capturing some domain-speci c connotations leading to a more domain-adapted model." ></td>
	<td class="line x" title="111:191	The partial labels on documents can be described using G0 where (G0)i1 = 1 if the document expresses positive sentiment, and (G0)i2 = 1 for negative sentiment." ></td>
	<td class="line x" title="112:191	As with F0, one can also use soft sentiment labeling for documents, though our experiments are conducted with hard assignments." ></td>
	<td class="line x" title="113:191	Therefore, the semi-supervised learning with lexical knowledge can be described as min F;G;S kX FSGTk2 + Trbracketleftbig(F F0)TC1(F F0)bracketrightbig+ Trbracketleftbig(G G0)TC2(G G0)bracketrightbig Where  > 0, > 0 are parameters which determine the extent to which we enforce F  F0 and G G0 respectively, C1 and C2 are diagonal matrices indicating the entries of F0 and G0 that correspond to labeled entities." ></td>
	<td class="line x" title="114:191	The squared loss terms ensure that the solution for F,G, in the otherwise unsupervised learning problem, be close to the prior knowledge F0 and G0." ></td>
	<td class="line x" title="115:191	5.1 Computational Algorithm The optimization problem in Eq.( 4) can be solved using the following update rules G jk G jk (X T FS + C2G0)jk (GGT XT FS + GGTC2G0)jk (11) Sik  Sik (F T XG)ik (FT FSGT G)ik ." ></td>
	<td class="line x" title="116:191	(12) Fik Fik (XGS T + C1F0)ik (FFT XGST + C1F)ik ." ></td>
	<td class="line x" title="117:191	(13) Thus the algorithm for semi-supervised learning with lexical knowledge based on our matrix factorization framework, referred as SSMFLK, consists of an iterative procedure using the above three rules until convergence." ></td>
	<td class="line x" title="118:191	The correctness and convergence of the algorithm can also be proved using similar arguments as what we outlined earlier for MFLK in Section 4.3." ></td>
	<td class="line x" title="119:191	A quick word about computational complexity." ></td>
	<td class="line x" title="120:191	The term-document matrix is typically very sparse with z nm non-zero entries while k is typically also much smaller than n,m. By using sparse matrix multiplications and avoiding dense intermediate matrices, the updates can be very ef ciently and easily implemented." ></td>
	<td class="line x" title="121:191	In particular, updating F,S,G each takes O(k2(m + n) + kz) time per iteration which scales linearly with the dimensions and density of the data matrix." ></td>
	<td class="line x" title="122:191	Empirically, the number of iterations before practical convergence is usually very small (less than 100)." ></td>
	<td class="line x" title="123:191	Thus, computationally our approach scales to large datasets even though our experiments are run on relatively small-sized datasets." ></td>
	<td class="line x" title="124:191	6 Experiments 6.1 Datasets Description Four different datasets are used in our experiments." ></td>
	<td class="line pc" title="125:191	Movies Reviews: This is a popular dataset in sentiment analysis literature (Pang et al., 2002)." ></td>
	<td class="line o" title="126:191	It consists of 1000 positive and 1000 negative movie reviews drawn from the IMDB archive of the rec.arts.movies.reviews newsgroups." ></td>
	<td class="line x" title="127:191	248 Lotus blogs: The data set is targeted at detecting sentiment around enterprise software, specifically pertaining to the IBM Lotus brand (Sindhwani and Melville, 2008)." ></td>
	<td class="line x" title="128:191	An unlabeled set of blog posts was created by randomly sampling 2000 posts from a universe of 14,258 blogs that discuss issues relevant to Lotus software." ></td>
	<td class="line x" title="129:191	In addition to this unlabeled set, 145 posts were chosen for manual labeling." ></td>
	<td class="line x" title="130:191	These posts came from 14 individual blogs, 4 of which are actively posting negative content on the brand, with the rest tending to write more positive or neutral posts." ></td>
	<td class="line x" title="131:191	The data was collected by downloading the latest posts from each bloggers RSS feeds, or accessing the blogs archives." ></td>
	<td class="line x" title="132:191	Manual labeling resulted in 34 positive and 111 negative examples." ></td>
	<td class="line x" title="133:191	Political candidate blogs: For our second blog domain, we used data gathered from 16,742 political blogs, which contain over 500,000 posts." ></td>
	<td class="line x" title="134:191	As with the Lotus dataset, an unlabeled set was created by randomly sampling 2000 posts." ></td>
	<td class="line x" title="135:191	107 posts were chosen for labeling." ></td>
	<td class="line x" title="136:191	A post was labeled as having positive or negative sentiment about a speci c candidate (Barack Obama or Hillary Clinton) if it explicitly mentioned the candidate in positive or negative terms." ></td>
	<td class="line x" title="137:191	This resulted in 49 positively and 58 negatively labeled posts." ></td>
	<td class="line x" title="138:191	Amazon Reviews: The dataset contains product reviews taken from Amazon.com from 4 product types: Kitchen, Books, DVDs, and Electronics (Blitzer et al., 2007)." ></td>
	<td class="line x" title="139:191	The dataset contains about 4000 positive reviews and 4000 negative reviews and can be obtained from http://www.cis.upenn." ></td>
	<td class="line x" title="140:191	edu/mdredze/datasets/sentiment/." ></td>
	<td class="line x" title="141:191	For all datasets, we picked 5000 words with highest document-frequency to generate the vocabulary." ></td>
	<td class="line x" title="142:191	Stopwords were removed and a normalized term-frequency representation was used." ></td>
	<td class="line x" title="143:191	Genuinely unlabeled posts for Political and Lotus were used for semi-supervised learning experiments in section 6.3; they were not used in section 6.2 on the effect of lexical prior knowledge." ></td>
	<td class="line x" title="144:191	In the experiments, we set , the parameter determining the extent to which to enforce the feature labels, to be 1/2, and , the corresponding parameter for enforcing document labels, to be 1." ></td>
	<td class="line x" title="145:191	6.2 Sentiment Analysis with Lexical Knowledge Of course, one can remove all burden on human effort by simply using unsupervised techniques." ></td>
	<td class="line x" title="146:191	Our interest in the  rst set of experiments is to explore the bene ts of incorporating a sentiment lexicon over unsupervised approaches." ></td>
	<td class="line x" title="147:191	Does a one-time effort in compiling a domainindependent dictionary and using it for different sentiment tasks pay off in comparison to simply using unsupervised methods?" ></td>
	<td class="line x" title="148:191	In our case, matrix tri-factorization and other co-clustering methods form the obvious unsupervised baseline for comparison and so we start by comparing our method (MFLK) with the following methods:  Four document clustering methods: Kmeans, Tri-Factor Nonnegative Matrix Factorization (TNMF) (Ding et al., 2006), Information-Theoretic Co-clustering (ITCC) (Dhillon et al., 2003), and Euclidean Co-clustering algorithm (ECC) (Cho et al., 2004)." ></td>
	<td class="line x" title="149:191	These methods do not make use of the sentiment lexicon." ></td>
	<td class="line x" title="150:191	Feature Centroid (FC): This is a simple dictionary-based baseline method." ></td>
	<td class="line x" title="151:191	Recall that each word can be expressed as a  bagof-documents vector." ></td>
	<td class="line x" title="152:191	In this approach, we compute the centroids of these vectors, one corresponding to positive words and another corresponding to negative words." ></td>
	<td class="line x" title="153:191	This yields a two-dimensional representation for documents, on which we then perform K-means clustering." ></td>
	<td class="line x" title="154:191	Performance Comparison Figure 1 shows the experimental results on four datasets using accuracy as the performance measure." ></td>
	<td class="line x" title="155:191	The results are obtained by averaging 20 runs." ></td>
	<td class="line x" title="156:191	It can be observed that our MFLK method can effectively utilize the lexical knowledge to improve the quality of sentiment prediction." ></td>
	<td class="line x" title="157:191	Movies Lotus Political Amazon0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Accuracy   MFLK FC TNMF ECC ITCC KMeans Figure 1: Accuracy results on four datasets 249 Size of Sentiment Lexicon We also investigate the effects of the size of the sentiment lexicon on the performance of our model." ></td>
	<td class="line x" title="158:191	Figure 2 shows results with random subsets of the lexicon of increasing size." ></td>
	<td class="line x" title="159:191	We observe that generally the performance increases as more and more lexical supervision is provided." ></td>
	<td class="line x" title="160:191	0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 Fraction of sentiment words labeled Accuracy   Movies Lotus Political Amazon Figure 2: MFLK accuracy as size of sentiment lexicon (i.e., number of words in the lexicon) increases on the four datasets Robustness to Vocabulary Size High dimensionality and noise can have profound impact on the comparative performance of clustering and semi-supervised learning algorithms." ></td>
	<td class="line x" title="161:191	We simulate scenarios with different vocabulary sizes by selecting words based on information gain." ></td>
	<td class="line x" title="162:191	It should, however, be kept in mind that in a truely unsupervised setting document labels are unavailable and therefore information gain cannot be practically computed." ></td>
	<td class="line x" title="163:191	Figure 3 and Figure 4 show results for Lotus and Amazon datasets respectively and are representative of performance on other datasets." ></td>
	<td class="line x" title="164:191	MLFK tends to retain its position as the best performing method even at different vocabulary sizes." ></td>
	<td class="line x" title="165:191	ITCC performance is also noteworthy given that it is a completely unsupervised method." ></td>
	<td class="line x" title="166:191	0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 Fraction of Original Vocabulary Accuracy   MFLK FC TNMF KMeans ITCC ECC Figure 3: Accuracy results on Lotus dataset with increasing vocabulary size 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 Fraction of Original Vocabulary Accuracy   MFLK FC TNMF KMeans ITCC ECC Figure 4: Accuracy results on Amazon dataset with increasing vocabulary size 6.3 Sentiment Analysis with Dual Supervision We now assume that together with labeled features from the sentiment lexicon, we also have access to a few labeled documents." ></td>
	<td class="line x" title="167:191	The natural question is whether the presence of lexical constraints leads to better semi-supervised models." ></td>
	<td class="line x" title="168:191	In this section, we compare our method (SSMFLK) with the following three semi-supervised approaches: (1) The algorithm proposed in (Zhou et al., 2003) which conducts semi-supervised learning with local and global consistency (Consistency Method); (2) Zhu et al.s harmonic Gaussian  eld method coupled with the Class Mass Normalization (HarmonicCMN) (Zhu et al., 2003); and (3) Greens function learning algorithm (Greens Function) proposed in (Ding et al., 2007)." ></td>
	<td class="line x" title="169:191	We also compare the results of SSMFLK with those of two supervised classi cation methods: Support Vector Machine (SVM) and Naive Bayes." ></td>
	<td class="line x" title="170:191	Both of these methods have been widely used in sentiment analysis." ></td>
	<td class="line pc" title="171:191	In particular, the use of SVMs in (Pang et al., 2002) initially sparked interest in using machine learning methods for sentiment classi cation." ></td>
	<td class="line n" title="172:191	Note that none of these competing methods utilizes lexical knowledge." ></td>
	<td class="line x" title="173:191	The results are presented in Figure 5, Figure 6, Figure 7, and Figure 8." ></td>
	<td class="line x" title="174:191	We note that our SSMFLK method either outperforms all other methods over the entire range of number of labeled documents (Movies, Political), or ultimately outpaces other methods (Lotus, Amazon) as a few document labels come in." ></td>
	<td class="line x" title="175:191	Learning Domain-Specific Connotations In our  rst set of experiments, we incorporated the sentiment lexicon in our models and learnt the sentiment orientation of words and documents via F,G factors respectively." ></td>
	<td class="line x" title="176:191	In the second set of 250 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 Number of documents labeled as a fraction of the original set of labeled documents Accuracy   SSMFLK Consistency Method HomonicCMN Green Function SVM Naive Bays Figure 5: Accuracy results with increasing number of labeled documents on Movies dataset 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50.3 0.4 0.5 0.6 0.7 0.8 0.9 Number of documents labeled as a fraction of the original set of labeled documents Accuracy   SSMFLK Consistency Method HomonicCMN Green Function SVM Naive Bayes Figure 6: Accuracy results with increasing number of labeled documents on Lotus dataset experiments, we additionally introduced labeled documents for domain-speci c adjustments." ></td>
	<td class="line x" title="177:191	Between these experiments, we can now look for words that switch sentiment polarity." ></td>
	<td class="line x" title="178:191	These words are interesting because their domain-speci c connotation differs from their lexical orientation." ></td>
	<td class="line x" title="179:191	For amazon reviews, the following words switched polarity from positive to negative: fan, important, learning, cons, fast, feature, happy, memory, portable, simple, small, work while the following words switched polarity from negative to positive: address, finish, lack, mean, budget, rent, throw." ></td>
	<td class="line x" title="180:191	Note that words like fan, memory probably refer to product or product components (i.e., computer fan and memory) in the amazon review context but have a very different connotation say in the context of movie reviews where they probably refer to movie fanfare and memorable performances." ></td>
	<td class="line x" title="181:191	We were surprised to see happy switch polarity!" ></td>
	<td class="line x" title="182:191	Two examples of its negative-sentiment usage are: I ended up buying a Samsung and I couldnt be more happy and BORING, not one single exciting thing about this book." ></td>
	<td class="line x" title="183:191	I was happy when my lunch break ended so I could go back to work and stop reading." ></td>
	<td class="line x" title="184:191	0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 Number of documents labeled as a fraction of the original set of labeled documents Accuracy   SSMFLK Consistency Method HomonicCMN Green Function SVM Naive Bays Figure 7: Accuracy results with increasing number of labeled documents on Political dataset 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 Number of documents labeled as a fraction of the original set of labeled documents Accuracy   SSMFLK Consistency Method HomonicCMN Green Function SVM Naive Bays Figure 8: Accuracy results with increasing number of labeled documents on Amazon dataset 7 Conclusion The primary contribution of this paper is to propose and benchmark new methodologies for sentiment analysis." ></td>
	<td class="line x" title="185:191	Non-negative Matrix Factorizations constitute a rich body of algorithms that have found applicability in a variety of machine learning applications: from recommender systems to document clustering." ></td>
	<td class="line x" title="186:191	We have shown how to build effective sentiment models by appropriately constraining the factors using lexical prior knowledge and document annotations." ></td>
	<td class="line x" title="187:191	To more effectively utilize unlabeled data and induce domain-speci c adaptation of our models, several extensions are possible: facilitating learning from related domains, incorporating hyperlinks between documents, incorporating synonyms or co-occurences between words etc. As a topic of vigorous current activity, there are several very recently proposed competing methodologies for sentiment analysis that we would like to benchmark against." ></td>
	<td class="line x" title="188:191	These are topics for future work." ></td>
	<td class="line x" title="189:191	Acknowledgement: The work of T. Li is partially supported by NSF grants DMS-0844513 and CCF-0830659." ></td>
	<td class="line x" title="190:191	We would also like to thank Prem Melville and Richard Lawrence for their support." ></td>
	<td class="line x" title="191:191	251" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1029
Discovering the Discriminative Views: Measuring Term Weights for Sentiment Analysis
Kim, Jungi;Li, Jin-Ji;Lee, Jong-Hyeok;"></td>
	<td class="line x" title="1:211	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 253261, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:211	c2009 ACL and AFNLP Discovering the Discriminative Views: Measuring Term Weights for Sentiment Analysis Jungi Kim, Jin-Ji Li and Jong-Hyeok Lee Division of Electrical and Computer Engineering Pohang University of Science and Technology, Pohang, Republic of Korea {yangpa,ljj,jhlee}@postech.ac.kr Abstract This paper describes an approach to utilizing term weights for sentiment analysis tasks and shows how various term weighting schemes improve the performance of sentiment analysis systems." ></td>
	<td class="line x" title="3:211	Previously, sentiment analysis was mostly studied under data-driven and lexicon-based frameworks." ></td>
	<td class="line x" title="4:211	Such work generally exploits textual features for fact-based analysis tasks or lexical indicators from a sentiment lexicon." ></td>
	<td class="line x" title="5:211	We propose to model term weighting into a sentiment analysis system utilizing collection statistics, contextual and topicrelated characteristics as well as opinionrelated properties." ></td>
	<td class="line x" title="6:211	Experiments carried out on various datasets show that our approach effectively improves previous methods." ></td>
	<td class="line x" title="7:211	1 Introduction With the explosion in the amount of commentaries on current issues and personal views expressed in weblogs on the Internet, the field of studying how to analyze such remarks and sentiments has been increasing as well." ></td>
	<td class="line x" title="8:211	The field of opinion mining and sentiment analysis involves extracting opinionated pieces of text, determining the polarities and strengths, and extracting holders and targets of the opinions." ></td>
	<td class="line x" title="9:211	Much research has focused on creating testbeds for sentiment analysis tasks." ></td>
	<td class="line x" title="10:211	Most notable and widely used are Multi-Perspective Question Answering (MPQA) and Movie-review datasets." ></td>
	<td class="line x" title="11:211	MPQA is a collection of newspaper articles annotated with opinions and private states at the subsentence level (Wiebe et al., 2003)." ></td>
	<td class="line oc" title="12:211	Movie-review dataset consists of positive and negative reviews from the Internet Movie Database (IMDb) archive (Pang et al., 2002)." ></td>
	<td class="line x" title="13:211	Evaluation workshops such as TREC and NTCIR have recently joined in this new trend of research and organized a number of successful meetings." ></td>
	<td class="line x" title="14:211	At the TREC Blog Track meetings, researchers have dealt with the problem of retrieving topically-relevant blog posts and identifying documents with opinionated contents (Ounis et al., 2008)." ></td>
	<td class="line x" title="15:211	NTCIR Multilingual Opinion Analysis Task (MOAT) shared a similar mission, where participants are provided with a number of topics and a set of relevant newspaper articles for each topic, and asked to extract opinion-related properties from enclosed sentences (Seki et al., 2008)." ></td>
	<td class="line x" title="16:211	Previous studies for sentiment analysis belong to either the data-driven approach where an annotated corpus is used to train a machine learning (ML) classifier, or to the lexicon-based approach where a pre-compiled list of sentiment terms is utilized to build a sentiment score function." ></td>
	<td class="line x" title="17:211	This paper introduces an approach to the sentiment analysis tasks with an emphasis on how to represent and evaluate the weights of sentiment terms." ></td>
	<td class="line x" title="18:211	We propose a number of characteristics of good sentiment terms from the perspectives of informativeness, prominence, topicrelevance, and semantic aspects using collection statistics, contextual information, semantic associations as well as opinionrelated properties of terms." ></td>
	<td class="line x" title="19:211	These term weighting features constitute the sentiment analysis model in our opinion retrieval system." ></td>
	<td class="line x" title="20:211	We test our opinion retrieval system with TREC and NTCIR datasets to validate the effectiveness of our term weighting features." ></td>
	<td class="line x" title="21:211	We also verify the effectiveness of the statistical features used in datadriven approaches by evaluating an ML classifier with labeled corpora." ></td>
	<td class="line x" title="22:211	2 Related Work Representing text with salient features is an important part of a text processing task, and there exists many works that explore various features for 253 text analysis systems (Sebastiani, 2002; Forman, 2003)." ></td>
	<td class="line x" title="23:211	Sentiment analysis task have also been using various lexical, syntactic, and statistical features (Pang and Lee, 2008)." ></td>
	<td class="line oc" title="24:211	Pang et al.(2002) employed n-gram and POS features for ML methods to classify movie-review data." ></td>
	<td class="line x" title="26:211	Also, syntactic features such as the dependency relationship of words and subtrees have been shown to effectively improve the performances of sentiment analysis (Kudo and Matsumoto, 2004; Gamon, 2004; Matsumoto et al., 2005; Ng et al., 2006)." ></td>
	<td class="line x" title="27:211	While these features are usually employed by data-driven approaches, there are unsupervised approaches for sentiment analysis that make use of a set of terms that are semantically oriented toward expressing subjective statements (Yu and Hatzivassiloglou, 2003)." ></td>
	<td class="line x" title="28:211	Accordingly, much research has focused on recognizing terms semantic orientations and strength, and compiling sentiment lexicons (Hatzivassiloglou and Mckeown, 1997; Turney and Littman, 2003; Kamps et al., 2004; Whitelaw et al., 2005; Esuli and Sebastiani, 2006)." ></td>
	<td class="line x" title="29:211	Interestingly, there are conflicting conclusions about the usefulness of the statistical features in sentiment analysis tasks (Pang and Lee, 2008)." ></td>
	<td class="line pc" title="30:211	Pang et al.(2002) presents empirical results indicating that using term presence over term frequency is more effective in a data-driven sentiment classification task." ></td>
	<td class="line o" title="32:211	Such a finding suggests that sentiment analysis may exploit different types of characteristics from the topical tasks, that, unlike fact-based text analysis tasks, repetition of terms does not imply a significance on the overall sentiment." ></td>
	<td class="line x" title="33:211	On the other hand, Wiebe et al.(2004) have noted that hapax legomena (terms that only appear once in a collection of texts) are good signs for detecting subjectivity." ></td>
	<td class="line x" title="35:211	Other works have also exploited rarely occurring terms for sentiment analysis tasks (Dave et al., 2003; Yang et al., 2006)." ></td>
	<td class="line x" title="36:211	The opinion retrieval task is a relatively recent issue that draws both the attention of IR and NLP communities." ></td>
	<td class="line x" title="37:211	Its task is to find relevant documents that also contain sentiments about a given topic." ></td>
	<td class="line x" title="38:211	Generally, the opinion retrieval task has been approached as a twostage task: first, retrieving topically relevant documents, then reranking the documents by the opinion scores (Ounis et al., 2006)." ></td>
	<td class="line x" title="39:211	This approach is also appropriate for evaluation systems such as NTCIR MOAT that assumes that the set of topically relevant documents are already known in advance." ></td>
	<td class="line x" title="40:211	On the other hand, there are also some interesting works on modeling the topic and sentiment of documents in a unified way (Mei et al., 2007; Zhang and Ye, 2008)." ></td>
	<td class="line x" title="41:211	3 Term Weighting and Sentiment Analysis In this section, we describe the characteristics of terms that are useful in sentiment analysis, and present our sentiment analysis model as part of an opinion retrieval system and an ML sentiment classifier." ></td>
	<td class="line x" title="42:211	3.1 Characteristics of Good Sentiment Terms This section examines the qualities of useful terms for sentiment analysis tasks and corresponding features." ></td>
	<td class="line x" title="43:211	For the sake of organization, we categorize the sources of features into either global or local knowledge, and either topic-independent or topic-dependent knowledge." ></td>
	<td class="line x" title="44:211	Topic-independently speaking, a good sentiment term is discriminative and prominent, such that the appearance of the term imposes greater influence on the judgment of the analysis system." ></td>
	<td class="line x" title="45:211	The rare occurrence of terms in document collections has been regarded as a very important feature in IR methods, and effective IR models of today, either explicitly or implicitly, accommodate this feature as an Inverse Document Frequency (IDF) heuristic (Fang et al., 2004)." ></td>
	<td class="line x" title="46:211	Similarly, prominence of a term is recognized by the frequency of the term in its local context, formulated as Term Frequency (TF) in IR." ></td>
	<td class="line x" title="47:211	If a topic of the text is known, terms that are relevant and descriptive of the subject should be regarded to be more useful than topically-irrelevant and extraneous terms." ></td>
	<td class="line x" title="48:211	One way of measuring this is using associations between the query and terms." ></td>
	<td class="line x" title="49:211	Statistical measures of associations between terms include estimations by the co-occurrence in the whole collection, such as Point-wise Mutual Information (PMI) and Latent Semantic Analysis (LSA)." ></td>
	<td class="line x" title="50:211	Another method is to use proximal information of the query and the word, using syntactic structure such as dependency relations of words that provide the graphical representation of the text (Mullen and Collier, 2004)." ></td>
	<td class="line x" title="51:211	The minimum spans of words in such graph may represent their associations in the text." ></td>
	<td class="line x" title="52:211	Also, the distance between words in the local context or in the thesauruslike dictionaries such as WordNet may be approximated as such measure." ></td>
	<td class="line x" title="53:211	254 3.2 Opinion Retrieval Model The goal of an opinion retrieval system is to find a set of opinionated documents that are relevant to a given topic." ></td>
	<td class="line x" title="54:211	We decompose the opinion retrieval system into two tasks: the topical retrieval task and the sentiment analysis task." ></td>
	<td class="line x" title="55:211	This two-stage approach for opinion retrieval has been taken by many systems and has been shown to perform well (Ounis et al., 2006)." ></td>
	<td class="line x" title="56:211	The topic and the sentiment aspects of the opinion retrieval task are modeled separately, and linearly combined together to produce a list of topically-relevant and opinionated documents as below." ></td>
	<td class="line x" title="57:211	ScoreOpRet(D,Q) = Scorerel(D,Q)+(1)Scoreop(D,Q) The topic-relevance model Scorerel may be substituted by any IR system that retrieves relevant documents for the query Q. For tasks such as NTCIR MOAT, relevant documents are already known in advance and it becomes unnecessary to estimate the relevance degree of the documents." ></td>
	<td class="line x" title="58:211	We focus on modeling the sentiment aspect of the opinion retrieval task, assuming that the topicrelevance of documents is provided in some way." ></td>
	<td class="line x" title="59:211	To assign documents with sentiment degrees, we estimate the probability of a document D to generate a query Q and to possess opinions as indicated by a random variable Op.1 Assuming uniform prior probabilities of documentsD, queryQ, and Op, and conditional independence between Q and Op, the opinion score function reduces to estimating the generative probability of Q and Op given D. Scoreop(D,Q)p(D|Op,Q)p(Op,Q|D) If we regard that the document D is represented as a bag of words and that the words are uniformly distributed, then p(Op,Q|D) = X wD p(Op,Q|w)p(w|D) = X wD p(Op|w)p(Q|w)p(w|D) (1) Equation 1 consists of three factors: the probability of a word to be opinionated (P(Op|w)), the likelihood of a query given a word (P(Q|w)), and the probability of a document generating a word (P(w|D))." ></td>
	<td class="line x" title="60:211	Intuitively speaking, the probability of a document embodying topically related opinion is estimated by accumulating the probabilities of all 1Throughout this paper, Op indicates Op = 1." ></td>
	<td class="line x" title="61:211	words from the document to have sentiment meanings and associations with the given query." ></td>
	<td class="line x" title="62:211	In the following sections, we assess the three factors of the sentiment models from the perspectives of term weighting." ></td>
	<td class="line x" title="63:211	3.2.1 Word Sentiment Model Modeling the sentiment of a word has been a popular approach in sentiment analysis." ></td>
	<td class="line x" title="64:211	There are many publicly available lexicon resources." ></td>
	<td class="line x" title="65:211	The size, format, specificity, and reliability differ in all these lexicons." ></td>
	<td class="line x" title="66:211	For example, lexicon sizes range from a few hundred to several hundred thousand." ></td>
	<td class="line x" title="67:211	Some lexicons assign real number scores to indicate sentiment orientations and strengths (i.e. probabilities of having positive and negative sentiments) (Esuli and Sebastiani, 2006) while other lexicons assign discrete classes (weak/strong, positive/negative) (Wilson et al., 2005)." ></td>
	<td class="line x" title="68:211	There are manually compiled lexicons (Stone et al., 1966) while some are created semi-automatically by expanding a set of seed terms (Esuli and Sebastiani, 2006)." ></td>
	<td class="line x" title="69:211	The goal of this paper is not to create or choose an appropriate sentiment lexicon, but rather it is to discover useful term features other than the sentiment properties." ></td>
	<td class="line x" title="70:211	For this reason, one sentiment lexicon, namely SentiWordNet, is utilized throughout the whole experiment." ></td>
	<td class="line x" title="71:211	SentiWordNet is an automatically generated sentiment lexicon using a semi-supervised method (Esuli and Sebastiani, 2006)." ></td>
	<td class="line x" title="72:211	It consists of WordNet synsets, where each synset is assigned three probability scores that add up to 1: positive, negative, and objective." ></td>
	<td class="line x" title="73:211	These scores are assigned at sense level (synsets in WordNet), and we use the following equations to assess the sentiment scores at the word level." ></td>
	<td class="line x" title="74:211	p(Pos|w) = max ssynset(w) SWNPos(s) p(Neg|w) = max ssynset(w) SWNNeg(s) p(Op|w) = max(p(Pos|w),p(Neg|w)) where synset(w) is the set of synsets of w and SWNPos(s), SWNNeg(s) are positive and negative scores of a synset in SentiWordNet." ></td>
	<td class="line x" title="75:211	We assess the subjective score of a word as the maximum value of the positive and the negative scores, because a word has either a positive or a negative sentiment in a given context." ></td>
	<td class="line x" title="76:211	The word sentiment model can also make use of other types of sentiment lexicons." ></td>
	<td class="line x" title="77:211	The sub255 jectivity lexicon used in OpinionFinder2 is compiled from several manually and automatically built resources." ></td>
	<td class="line x" title="78:211	Each word in the lexicon is tagged with the strength (strong/weak) and polarity (Positive/Negative/Neutral)." ></td>
	<td class="line x" title="79:211	The word sentiment can be modeled as below." ></td>
	<td class="line x" title="80:211	P(Pos|w) = 8 >< >: 1.0 if w is Positive and Strong 0.5 if w is Positive and Weak 0.0 otherwise P(Op|w) = max(p(Pos|w),p(Neg|w)) 3.2.2 Topic Association Model If a topic is given in the sentiment analysis, terms that are closely associated with the topic should be assigned heavy weighting." ></td>
	<td class="line x" title="81:211	For example, sentiment words such as scary and funny are more likely to be associated with topic words such as book and movie than grocery or refrigerator." ></td>
	<td class="line x" title="82:211	In the topic association model, p(Q|w) is estimated from the associations between the word w and a set of query terms Q. p(Q|w) = P qQAsc-Score(q,w) |Q|  X qQ Asc-Score(q,w) Asc-Score(q,w) is the association score between q and w, and|Q|is the number of query words." ></td>
	<td class="line x" title="83:211	To measure associations between words, we employ statistical approaches using document collections such as LSA and PMI, and local proximity features using the distance in dependency trees or texts." ></td>
	<td class="line x" title="84:211	Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997) creates a semantic space from a collection of documents to measure the semantic relatedness of words." ></td>
	<td class="line x" title="85:211	Point-wise Mutual Information (PMI) is a measure of associations used in information theory, where the association between two words is evaluated with the joint and individual distributions of the two words." ></td>
	<td class="line x" title="86:211	PMI-IR (Turney, 2001) uses an IR system and its search operators to estimate the probabilities of two terms and their conditional probabilities." ></td>
	<td class="line x" title="87:211	Equations for association scores using LSA and PMI are given below." ></td>
	<td class="line x" title="88:211	Asc-ScoreLSA(w1,w2) = 1 + LSA(w1,w2)2 Asc-ScorePMI(w1,w2) = 1 + PMI-IR(w1,w2)2 2http://www.cs.pitt.edu/mpqa/ For the experimental purpose, we used publicly available online demonstrations for LSA and PMI." ></td>
	<td class="line x" title="89:211	For LSA, we used the online demonstration mode from the Latent Semantic Analysis page from the University of Colorado at Boulder.3 For PMI, we used the online API provided by the CogWorks Lab at the Rensselaer Polytechnic Institute.4 Word associations between two terms may also be evaluated in the local context where the terms appear together." ></td>
	<td class="line x" title="90:211	One way of measuring the proximity of terms is using the syntactic structures." ></td>
	<td class="line x" title="91:211	Given the dependency tree of the text, we model the association between two terms as below." ></td>
	<td class="line x" title="92:211	Asc-ScoreDTP(w1,w2) = ( 1.0 min." ></td>
	<td class="line x" title="93:211	span in dep." ></td>
	<td class="line x" title="94:211	treeDsyn 0.5 otherwise where, Dsyn is arbitrarily set to 3." ></td>
	<td class="line x" title="95:211	Another way is to use co-occurrence statistics as below." ></td>
	<td class="line x" title="96:211	Asc-ScoreWP(w1,w2) = ( 1.0 if distance betweenw1andw2 K 0.5 otherwise where K is the maximum window size for the co-occurrence and is arbitrarily set to 3 in our experiments." ></td>
	<td class="line x" title="97:211	The statistical approaches may suffer from data sparseness problems especially for named entity terms used in the query, and the proximal clues cannot sufficiently cover all termquery associations." ></td>
	<td class="line x" title="98:211	To avoid assigning zero probabilities, our topic association models assign 0.5 to word pairs with no association and 1.0 to words with perfect association." ></td>
	<td class="line x" title="99:211	Note that proximal features using co-occurrence and dependency relationships were used in previous work." ></td>
	<td class="line x" title="100:211	For opinion retrieval tasks, Yang et al.(2006) and Zhang and Ye (2008) used the cooccurrence of a query word and a sentiment word within a certain window size." ></td>
	<td class="line x" title="102:211	Mullen and Collier (2004) manually annotated named entities in their dataset (i.e. title of the record and name of the artist for music record reviews), and utilized presence and position features in their ML approach." ></td>
	<td class="line x" title="103:211	3.2.3 Word Generation Model Our word generation model p(w|d) evaluates the prominence and the discriminativeness of a word 3http://lsa.colorado.edu/, default parameter settings for the semantic space (TASA, 1st year college level) and number of factors (300)." ></td>
	<td class="line x" title="104:211	4http://cwl-projects.cogsci.rpi.edu/msr/, PMI-IR with the Google Search Engine." ></td>
	<td class="line x" title="105:211	256 w in a document d. These issues correspond to the core issues of traditional IR tasks." ></td>
	<td class="line x" title="106:211	IR models, such as Vector Space (VS), probabilistic models such as BM25, and Language Modeling (LM), albeit in different forms of approach and measure, employ heuristics and formal modeling approaches to effectively evaluate the relevance of a term to a document (Fang et al., 2004)." ></td>
	<td class="line x" title="107:211	Therefore, we estimate the word generation model with popular IR models the relevance scores of a document d given w as a query.5 p(w|d)IR-SCORE(w,d) In our experiments, we use the Vector Space model with Pivoted Normalization (VS), Probabilistic model (BM25), and Language modeling with Dirichlet Smoothing (LM)." ></td>
	<td class="line x" title="108:211	VSPN(w,d) = 1 +ln(1 +ln(c(w,d))) (1s) +s |d|avgdl lnN + 1df(w) BM25(w,d) = lnNdf(w) + 0.5df(w) + 0.5  (k1 + 1)c(w,d) k1  (1b) +b |d|avgdl  +c(w,d) LMDI(w,d) = ln  1 + c(w,d)c(w,C) ! +ln |d|+ c(w,d) is the frequency of w in d, | d | is the number of unique terms in d, avgdl is the average | d | of all documents, N is the number of documents in the collection, df(w) is the number of documents with w, C is the entire collection, and k1 and b are constants 2.0 and 0.75." ></td>
	<td class="line x" title="109:211	3.3 Data-driven Approach To verify the effectiveness of our term weighting schemes in experimental settings of the datadriven approach, we carry out a set of simple experiments with ML classifiers." ></td>
	<td class="line oc" title="110:211	Specifically, we explore the statistical term weighting features of the word generation model with Support Vector machine (SVM), faithfully reproducing previous work as closely as possible (Pang et al., 2002)." ></td>
	<td class="line x" title="111:211	Each instance of train and test data is represented as a vector of features." ></td>
	<td class="line x" title="112:211	We test various combinations of the term weighting schemes listed below." ></td>
	<td class="line x" title="113:211	 PRESENCE: binary indicator for the presence of a term  TF: term frequency 5With proper assumptions and derivations, p(w | d) can be derived to language modeling approaches." ></td>
	<td class="line x" title="114:211	Refer to (Zhai and Lafferty, 2004)." ></td>
	<td class="line x" title="115:211	 VS.TF: normalized tf as in VS  BM25.TF: normalized tf as in BM25  IDF: inverse document frequency  VS.IDF: normalized idf as in VS  BM25.IDF: normalized idf as in BM25 4 Experiment Our experiments consist of an opinion retrieval task and a sentiment classification task." ></td>
	<td class="line x" title="116:211	We use MPQA and movie-review corpora in our experiments with an ML classifier." ></td>
	<td class="line x" title="117:211	For the opinion retrieval task, we use the two datasets used by TREC blog track and NTCIR MOAT evaluation workshops." ></td>
	<td class="line x" title="118:211	The opinion retrieval task at TREC Blog Track consists of three subtasks: topic retrieval, opinion retrieval, and polarity retrieval." ></td>
	<td class="line x" title="119:211	Opinion and polarity retrieval subtasks use the relevant documents retrieved at the topic retrieval stage." ></td>
	<td class="line x" title="120:211	On the other hand, the NTCIR MOAT task aims to find opinionated sentences given a set of documents that are already hand-assessed to be relevant to the topic." ></td>
	<td class="line x" title="121:211	4.1 Opinion Retieval Task  TREC Blog Track 4.1.1 Experimental Setting TREC Blog Track uses the TREC Blog06 corpus (Macdonald and Ounis, 2006)." ></td>
	<td class="line x" title="122:211	It is a collection of RSS feeds (38.6 GB), permalink documents (88.8GB), and homepages (28.8GB) crawled on the Internet over an eleven week period from December 2005 to February 2006." ></td>
	<td class="line x" title="123:211	Non-relevant content of blog posts such as HTML tags, advertisement, site description, and menu are removed with an effective internal spam removal algorithm (Nam et al., 2009)." ></td>
	<td class="line x" title="124:211	While our sentiment analysis model uses the entire relevant portion of the blog posts, further stopword removal and stemming is done for the blog retrieval system." ></td>
	<td class="line x" title="125:211	For the relevance retrieval model, we faithfully reproduce the passage-based language model with pseudo-relevance feedback (Lee et al., 2008)." ></td>
	<td class="line x" title="126:211	We use in total 100 topics from TREC 2007 and 2008 blog opinion retrieval tasks (07:901-950 and 08:1001-1050)." ></td>
	<td class="line x" title="127:211	We use the topics from Blog 07 to optimize the parameter for linearly combining the retrieval and opinion models, and use Blog 08 topics as our test data." ></td>
	<td class="line x" title="128:211	Topics are extracted only from the Title field, using the Porter stemmer and a stopword list." ></td>
	<td class="line x" title="129:211	257 Table 1: Performance of opinion retrieval models using Blog 08 topics." ></td>
	<td class="line x" title="130:211	The linear combination parameter  is optimized on Blog 07 topics." ></td>
	<td class="line x" title="131:211	indicates statistical significance at the 1% level over the baseline." ></td>
	<td class="line x" title="132:211	Model MAP R-prec P@10 TOPIC REL." ></td>
	<td class="line x" title="133:211	0.4052 0.4366 0.6440 BASELINE 0.4141 0.4534 0.6440 VS 0.4196 0.4542 0.6600 BM25 0.4235 0.4579 0.6600 LM 0.4158 0.4520 0.6560 PMI 0.4177 0.4538 0.6620 LSA 0.4155 0.4526 0.6480 WP 0.4165 0.4533 0.6640 BM25PMI 0.4238 0.4575 0.6600 BM25LSA 0.4237 0.4578 0.6600 BM25WP 0.4237 0.4579 0.6600 BM25PMIWP 0.4242 0.4574 0.6620 BM25LSAWP 0.4238 0.4576 0.6580 4.1.2 Experimental Result Retrieval performances using different combinations of term weighting features are presented in Table 1." ></td>
	<td class="line x" title="134:211	Using only the word sentiment model is set as our baseline." ></td>
	<td class="line x" title="135:211	First, each feature of the word generation and topic association models are tested; all features of the models improve over the baseline." ></td>
	<td class="line x" title="136:211	We observe that the features of our word generation model is more effective than those of the topic association model." ></td>
	<td class="line x" title="137:211	Among the features of the word generation model, the most improvement was achieved with BM25, improving the MAP by 2.27%." ></td>
	<td class="line x" title="138:211	Features of the topic association model show only moderate improvements over the baseline." ></td>
	<td class="line x" title="139:211	We observe that these features generally improve P@10 performance, indicating that they increase the accuracy of the sentiment analysis system." ></td>
	<td class="line x" title="140:211	PMI out-performed LSA for all evaluation measures." ></td>
	<td class="line x" title="141:211	Among the topic association models, PMI performs the best in MAP and R-prec, while WP achieved the biggest improvement in P@10." ></td>
	<td class="line x" title="142:211	Since BM25 performs the best among the word generation models, its combination with other features was investigated." ></td>
	<td class="line x" title="143:211	Combinations of BM25 with the topic association models all improve the performance of the baseline and BM25." ></td>
	<td class="line x" title="144:211	This demonstrates that the word generation model and the topic association model are complementary to each other." ></td>
	<td class="line x" title="145:211	The best MAP was achieved with BM25, PMI, and WP (+2.44% over the baseline)." ></td>
	<td class="line x" title="146:211	We observe that PMI and WP also complement each other." ></td>
	<td class="line x" title="147:211	4.2 Sentiment Analysis Task  NTCIR MOAT 4.2.1 Experimental Setting Another set of experiments for our opinion analysis model was carried out on the NTCIR-7 MOAT English corpus." ></td>
	<td class="line x" title="148:211	The English opinion corpus for NTCIR MOAT consists of newspaper articles from the Mainichi Daily News, Korea Times, Xinhua News, Hong Kong Standard, and the Straits Times." ></td>
	<td class="line x" title="149:211	It is a collection of documents manually assessed for relevance to a set of queries from NTCIR-7 Advanced Cross-lingual Information Access (ACLIA) task." ></td>
	<td class="line x" title="150:211	The corpus consists of 167 documents, or 4,711 sentences for 14 test topics." ></td>
	<td class="line x" title="151:211	Each sentence is manually tagged with opinionatedness, polarity, and relevance to the topic by three annotators from a pool of six annotators." ></td>
	<td class="line x" title="152:211	For preprocessing, no removal or stemming is performed on the data." ></td>
	<td class="line x" title="153:211	Each sentence was processed with the Stanford English parser6 to produce a dependency parse tree." ></td>
	<td class="line x" title="154:211	Only the Title fields of the topics were used." ></td>
	<td class="line x" title="155:211	For performance evaluations of opinion and polarity detection, we use precision, recall, and Fmeasure, the same measure used to report the official results at the NTCIR MOAT workshop." ></td>
	<td class="line x" title="156:211	There are lenient and strict evaluations depending on the agreement of the annotators; if two out of three annotators agreed upon an opinion or polarity annotation then it is used during the lenient evaluation, similarly three out of three agreements are used during the strict evaluation." ></td>
	<td class="line x" title="157:211	We present the performances using the lenient evaluation only, for the two evaluations generally do not show much difference in relative performance changes." ></td>
	<td class="line x" title="158:211	Since MOAT is a classification task, we use a threshold parameter to draw a boundary between opinionated and non-opinionated sentences." ></td>
	<td class="line x" title="159:211	We report the performance of our system using the NTCIR-7 dataset, where the threshold parameter is optimized using the NTCIR-6 dataset." ></td>
	<td class="line x" title="160:211	4.2.2 Experimental Result We present the performance of our sentiment analysis system in Table 2." ></td>
	<td class="line x" title="161:211	As in the experiments with 6http://nlp.stanford.edu/software/lex-parser.shtml 258 Table 2: Performance of the Sentiment Analysis System on NTCIR7 dataset." ></td>
	<td class="line x" title="162:211	System parameters are optimized for F-measure using NTCIR6 dataset with lenient evaluations." ></td>
	<td class="line x" title="163:211	Opinionated Model Precision Recall F-Measure BASELINE 0.305 0.866 0.451 VS 0.331 0.807 0.470 BM25 0.327 0.795 0.464 LM 0.325 0.794 0.461 LSA 0.315 0.806 0.453 PMI 0.342 0.603 0.436 DTP 0.322 0.778 0.455 VSLSA 0.335 0.769 0.466 VSPMI 0.311 0.833 0.453 VSDTP 0.342 0.745 0.469 VSLSADTP 0.349 0.719 0.470 VSPMIDTP 0.328 0.773 0.461 the TREC dataset, using only the word sentiment model is used as our baseline." ></td>
	<td class="line x" title="164:211	Similarly to the TREC experiments, the features of the word generation model perform exceptionally better than that of the topic association model." ></td>
	<td class="line x" title="165:211	The best performing feature of the word generation model is VS, achieving a 4.21% improvement over the baselines f-measure." ></td>
	<td class="line x" title="166:211	Interestingly, this is the tied top performing f-measure over all combinations of our features." ></td>
	<td class="line x" title="167:211	While LSA and DTP show mild improvements, PMI performed worse than baseline, with higher precision but a drop in recall." ></td>
	<td class="line x" title="168:211	DTP was the best performing topic association model." ></td>
	<td class="line x" title="169:211	When combining the best performing feature of the word generation model (VS) with the features of the topic association model, LSA, PMI and DTP all performed worse than or as well as the VS in f-measure evaluation." ></td>
	<td class="line x" title="170:211	LSA and DTP improves precision slightly, but with a drop in recall." ></td>
	<td class="line x" title="171:211	PMI shows the opposite tendency." ></td>
	<td class="line x" title="172:211	The best performing system was achieved using VS, LSA and DTP at both precision and f-measure evaluations." ></td>
	<td class="line x" title="173:211	4.3 Classification task  SVM 4.3.1 Experimental Setting To test our SVM classifier, we perform the classification task." ></td>
	<td class="line x" title="174:211	Movie Review polarity dataset7 was 7http://www.cs.cornell.edu/people/pabo/movie-reviewdata/ Table 3: Average ten-fold cross-validation accuracies of polarity classification task with SVM." ></td>
	<td class="line oc" title="175:211	Accuracy Features Movie-review MPQA PRESENCE 82.6 76.8 TF 71.1 76.5 VS.TF 81.3 76.7 BM25.TF 81.4 77.9 IDF 61.6 61.8 VS.IDF 83.6 77.9 BM25.IDF 83.6 77.8 VS.TFVS.IDF 83.8 77.9 BM25.TFBM25.IDF 84.1 77.7 BM25.TFVS.IDF 85.1 77.7 first introduced by Pang et al.(2002) to test various ML-based methods for sentiment classification." ></td>
	<td class="line x" title="177:211	It is a balanced dataset of 700 positive and 700 negative reviews, collected from the Internet Movie Database (IMDb) archive." ></td>
	<td class="line x" title="178:211	MPQA Corpus8 contains 535 newspaper articles manually annotated at sentence and subsentence level for opinions and other private states (Wiebe et al., 2005)." ></td>
	<td class="line oc" title="179:211	To closely reproduce the experiment with the best performance carried out in (Pang et al., 2002) using SVM, we use unigram with the presence feature." ></td>
	<td class="line x" title="180:211	We test various combinations of our features applicable to the task." ></td>
	<td class="line x" title="181:211	For evaluation, we use ten-fold cross-validation accuracy." ></td>
	<td class="line x" title="182:211	4.3.2 Experimental Result We present the sentiment classification performances in Table 3." ></td>
	<td class="line oc" title="183:211	As observed by Pang et al.(2002), using the raw tf drops the accuracy of the sentiment classification (-13.92%) of movie-review data." ></td>
	<td class="line x" title="185:211	Using the raw idf feature worsens the accuracy even more (-25.42%)." ></td>
	<td class="line x" title="186:211	Normalized tf-variants show improvements over tf but are worse than presence." ></td>
	<td class="line x" title="187:211	Normalized idf features produce slightly better accuracy results than the baseline." ></td>
	<td class="line x" title="188:211	Finally, combining any normalized tf and idf features improved the baseline (high 83%low 85%)." ></td>
	<td class="line x" title="189:211	The best combination was BM25.TFVS.IDF." ></td>
	<td class="line x" title="190:211	MPQA corpus reveals similar but somewhat uncertain tendency." ></td>
	<td class="line x" title="191:211	8http://www.cs.pitt.edu/mpqa/databaserelease/ 259 4.4 Discussion Overall, the opinion retrieval and the sentiment analysis models achieve improvements using our proposed features." ></td>
	<td class="line x" title="192:211	Especially, the features of the word generation model improve the overall performances drastically." ></td>
	<td class="line x" title="193:211	Its effectiveness is also verified with a data-driven approach; the accuracy of a sentiment classifier trained on a polarity dataset was improved by various combinations of normalized tf and idf statistics." ></td>
	<td class="line x" title="194:211	Differences in effectiveness of VS, BM25, and LM come from parameter tuning and corpus differences." ></td>
	<td class="line x" title="195:211	For the TREC dataset, BM25 performed better than the other models, and for the NTCIR dataset, VS performed better." ></td>
	<td class="line x" title="196:211	Our features of the topic association model show mild improvement over the baseline performance in general." ></td>
	<td class="line x" title="197:211	PMI and LSA, both modeling the semantic associations between words, show different behaviors on the datasets." ></td>
	<td class="line x" title="198:211	For the NTCIR dataset, LSA performed better, while PMI is more effective for the TREC dataset." ></td>
	<td class="line x" title="199:211	We believe that the explanation lies in the differences between the topics for each dataset." ></td>
	<td class="line x" title="200:211	In general, the NTCIR topics are general descriptive words such as regenerative medicine, American economy after the 911 terrorist attacks, and lawsuit brought against Microsoft for monopolistic practices. The TREC topics are more namedentity-like terms such as Carmax, Wikipedia primary source, Jiffy Lube, Starbucks, and Windows Vista. We have experimentally shown that LSA is more suited to finding associations between general terms because its training documents are from a general domain.9 Our PMI measure utilizes a web search engine, which covers a variety of named entity terms." ></td>
	<td class="line x" title="201:211	Though the features of our topic association model, WP and DTP, were evaluated on different datasets, we try our best to conjecture the differences." ></td>
	<td class="line x" title="202:211	WP on TREC dataset shows a small improvement of MAP compared to other topic association features, while the precision is improved the most when this feature is used alone." ></td>
	<td class="line x" title="203:211	The DTP feature displays similar behavior with precision." ></td>
	<td class="line x" title="204:211	It also achieves the best f-measure over other topic association features." ></td>
	<td class="line x" title="205:211	DTP achieves higher relative improvement (3.99% F-measure verse 2.32% MAP), and is more effective for improving the performance in combination with LSA and PMI." ></td>
	<td class="line x" title="206:211	9TASA Corpus, http://lsa.colorado.edu/spaces.html 5 Conclusion In this paper, we proposed various term weighting schemes and how such features are modeled in the sentiment analysis task." ></td>
	<td class="line x" title="207:211	Our proposed features include corpus statistics, association measures using semantic and local-context proximities." ></td>
	<td class="line x" title="208:211	We have empirically shown the effectiveness of the features with our proposed opinion retrieval and sentiment analysis models." ></td>
	<td class="line x" title="209:211	There exists much room for improvement with further experiments with various term weighting methods and datasets." ></td>
	<td class="line x" title="210:211	Such methods include, but by no means limited to, semantic similarities between word pairs using lexical resources such as WordNet (Miller, 1995) and data-driven methods with various topic-dependent term weighting schemes on labeled corpus with topics such as MPQA." ></td>
	<td class="line x" title="211:211	Acknowledgments This work was supported in part by MKE & IITA through IT Leading R&D Support Project and in part by the BK 21 Project in 2009." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1078
A Framework of Feature Selection Methods for Text Categorization
Li, Shoushan;Xia, Rui;Zong, Chengqing;Huang, Chu-Ren;"></td>
	<td class="line x" title="1:174	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 692700, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:174	c2009 ACL and AFNLP A Framework of Feature Selection Methods for Text Categorization   Shoushan Li1  Rui Xia2  Chengqing Zong2  Chu-Ren Huang1  1 Department of Chinese and Bilingual Studies The Hong Kong Polytechnic University {shoushan.li,churenhuang} @gmail.com  2 National Laboratory of Pattern Recognition  Institute of Automation  Chinese Academy of Sciences {rxia,cqzong}@nlpr.ia.ac.cn    Abstract In text categorization, feature selection (FS) is a strategy that aims at making text classifiers more efficient and accurate." ></td>
	<td class="line x" title="3:174	However, when dealing with a new task, it is still difficult to quickly select a suitable one from various FS methods provided by many previous studies." ></td>
	<td class="line x" title="4:174	In this paper, we propose a theoretic framework of FS methods based on two basic measurements: frequency measurement and ratio measurement." ></td>
	<td class="line x" title="5:174	Then six popular FS methods are in detail discussed under this framework." ></td>
	<td class="line x" title="6:174	Moreover, with the guidance of our theoretical analysis, we propose a novel method called weighed frequency and odds (WFO) that combines the two measurements with trained weights." ></td>
	<td class="line x" title="7:174	The experimental results on data sets from both topic-based and sentiment classification tasks show that this new method is robust across different tasks and numbers of selected features." ></td>
	<td class="line x" title="8:174	1 Introduction With the rapid growth of online information, text classification, the task of assigning text documents to one or more predefined categories, has become one of the key tools for automatically handling and organizing text information." ></td>
	<td class="line x" title="9:174	The problems of text classification normally involve the difficulty of extremely high dimensional feature space which sometimes makes learning algorithms intractable." ></td>
	<td class="line x" title="10:174	A standard procedure to reduce the feature dimensionality is called feature selection (FS)." ></td>
	<td class="line x" title="11:174	Various FS methods, such as document frequency (DF), information gain (IG), mutual information (MI), 2 -test (CHI), Bi-Normal Separation (BNS), and weighted log-likelihood ratio (WLLR), have been proposed for the tasks (Yang and Pedersen, 1997; Nigam et al., 2000; Forman, 2003) and make text classification more efficient and accurate." ></td>
	<td class="line x" title="12:174	However, comparing these FS methods appears to be difficult because they are usually based on different theories or measurements." ></td>
	<td class="line x" title="13:174	For example, MI and IG are based on information theory, while CHI is mainly based on the measurements of statistic independence." ></td>
	<td class="line x" title="14:174	Previous comparisons of these methods have mainly depended on empirical studies that are heavily affected by the experimental sets." ></td>
	<td class="line x" title="15:174	As a result, conclusions from those studies are sometimes inconsistent." ></td>
	<td class="line x" title="16:174	In order to better understand the relationship between these methods, building a general theoretical framework provides a fascinating perspective." ></td>
	<td class="line x" title="17:174	Furthermore, in real applications, selecting an appropriate FS method remains hard for a new task because too many FS methods are available due to the long history of FS studies." ></td>
	<td class="line x" title="18:174	For example, merely in an early survey paper (Sebastiani, 2002), eight methods are mentioned." ></td>
	<td class="line x" title="19:174	These methods are provided by previous work for dealing with different text classification tasks but none of them is shown to be robust across different classification applications." ></td>
	<td class="line x" title="20:174	In this paper, we propose a framework with two basic measurements for theoretical comparison of six FS methods which are widely used in text classification." ></td>
	<td class="line x" title="21:174	Moreover, a novel method is set forth that combines the two measurements and tunes their influences considering different application domains and numbers of selected features." ></td>
	<td class="line x" title="22:174	The remainder of this paper is organized as follows." ></td>
	<td class="line x" title="23:174	Section 2 introduces the related work on 692 feature selection for text classification." ></td>
	<td class="line x" title="24:174	Section 3 theoretically analyzes six FS methods and proposes a new FS approach." ></td>
	<td class="line x" title="25:174	Experimental results are presented and analyzed in Section 4." ></td>
	<td class="line x" title="26:174	Finally, Section 5 draws our conclusions and outlines the future work." ></td>
	<td class="line x" title="27:174	2 Related Work FS is a basic problem in pattern recognition and has been a fertile field of research and development since the 1970s." ></td>
	<td class="line x" title="28:174	It has been proven to be effective on removing irrelevant and redundant features, increasing efficiency in learning tasks, and improving learning performance." ></td>
	<td class="line x" title="29:174	FS methods fall into two broad categories, the filter model and the wrapper model (John et al., 1994)." ></td>
	<td class="line x" title="30:174	The wrapper model requires one predetermined learning algorithm in feature selection and uses its performance to evaluate and determine which features are selected." ></td>
	<td class="line x" title="31:174	And the filter model relies on general characteristics of the training data to select some features without involving any specific learning algorithm." ></td>
	<td class="line x" title="32:174	There is evidence that wrapper methods often perform better on small scale problems (John et al, 1994), but on large scale problems, such as text classification, wrapper methods are shown to be impractical because of its high computational cost." ></td>
	<td class="line x" title="33:174	Therefore, in text classification, filter methods using feature scoring metrics are popularly used." ></td>
	<td class="line x" title="34:174	Below we review some recent studies of feature selection on both topic-based and sentiment classification." ></td>
	<td class="line x" title="35:174	In the past decade, FS studies mainly focus on topic-based classification where the classification categories are related to the subject content, e.g., sport or education." ></td>
	<td class="line x" title="36:174	Yang and Pedersen (1997) investigate five FS metrics and report that good FS methods improve the categorization accuracy with an aggressive feature removal using DF, IG, and CHI." ></td>
	<td class="line x" title="37:174	More recently, Forman (2003) empirically compares twelve FS methods on 229 text classification problem instances and proposes a new method called 'Bi-Normal Separation' (BNS)." ></td>
	<td class="line x" title="38:174	Their experimental results show that BNS can perform very well in the evaluation metrics of recall rate and F-measure." ></td>
	<td class="line x" title="39:174	But for the metric of precision, it often loses to IG." ></td>
	<td class="line x" title="40:174	Besides these two comparison studies, many others contribute to this topic (Yang and Liu, 1999; Brank et al., 2002; Gabrilovich and Markovitch, 2004) and more and more new FS methods are generated, such as, Gini index (Shang et al., 2007), Distance to Transition Point (DTP) (Moyotl-Hernandez and Jimenez-Salazar, 2005), Strong Class Information Words (SCIW) (Li and Zong, 2005) and parameter tuning based FS for Rocchio classifier (Moschitti, 2003)." ></td>
	<td class="line oc" title="41:174	Recently, sentiment classification has become popular because of its wide applications (Pang et al., 2002)." ></td>
	<td class="line x" title="42:174	Its criterion of classification is the attitude expressed in the text (e.g., recommended or not recommended, positive or negative) rather than some facts (e.g., sport or education)." ></td>
	<td class="line x" title="43:174	To our best knowledge, yet no related work has focused on comparison studies of FS methods on this special task." ></td>
	<td class="line x" title="44:174	There are only some scattered reports in their experimental studies." ></td>
	<td class="line x" title="45:174	Riloff et al.(2006) report that the traditional FS method (only using IG method) performs worse than the baseline in some cases." ></td>
	<td class="line x" title="47:174	However, Cui et al.(2006) present the experiments on the sentiment classification for large-scale online product reviews to show that using the FS method of CHI does not degrade the performance but can significantly reduce the dimension of the feature vector." ></td>
	<td class="line x" title="49:174	Moreover, Ng et al.(2006) examine the FS of the weighted log-likelihood ratio (WLLR) on the movie review dataset and achieves an accuracy of 87.1%, which is higher than the result reported by Pang and Lee (2004) with the same dataset." ></td>
	<td class="line x" title="51:174	From the analysis above, we believe that the performance of the sentiment classification system is also dramatically affected by FS." ></td>
	<td class="line x" title="52:174	3 Our Framework In the selection process, each feature (term, or single word) is assigned with a score according to a score-computing function." ></td>
	<td class="line x" title="53:174	Then those with higher scores are selected." ></td>
	<td class="line x" title="54:174	These mathematical definitions of the score-computing functions are often defined by some probabilities which are estimated by some statistic information in the documents across different categories." ></td>
	<td class="line x" title="55:174	For the convenience of description, we give some notations of these probabilities below." ></td>
	<td class="line x" title="56:174	( )P t : the probability that a document x  contains term t ; ( )iP c : the probability that a document x  does not belong to category ic ; ( , )iP t c : the joint probability that a document x contains term t  and also belongs to category ic ; ( | )iP c t : the probability that a document x belongs to category ic g712under the condition that it contains term t. 693 ( | )iP t c : the probability that, a document x does not contain term t with the condition that x belongs to category ic ; Some other probabilities, such as ( )P t , ( )iP c , ( | )iP t c , ( | )iP t c , ( | )iP c t ,  and ( | )iP c t , are similarly defined." ></td>
	<td class="line x" title="57:174	In order to estimate these probabilities, statistical information from the training data is needed, and notations about the training data are given as follows: 1{ } m i ic = : the set of categories; iA : the number of the documents that contain the term t  and also belong to category ic ; iB : the number of the documents that contain the term t  but do not belong to category ic ; iN : the total number of the documents that belong to category ic ; allN : the total number of all documents from the training data." ></td>
	<td class="line x" title="58:174	iC : the number of the documents that do not contain the term t  but belong to category ic , i.e., i iN A iD : the number of the documents that neither contain the term t  nor belong to category ic , i.e., all i iN N B  ; In this section, we would analyze theoretically six popular methods, namely DF, MI, IG, CHI, BNS, and WLLR." ></td>
	<td class="line x" title="59:174	Although these six FS methods are defined differently with different scoring measurements, we believe that they are strongly related." ></td>
	<td class="line x" title="60:174	In order to connect them, we define two basic measurements which are discussed as follows." ></td>
	<td class="line x" title="61:174	The first measurement is to compute the document frequency in one category, i.e., iA . The second measurement is the ratio between the document frequencies in one category and the other categories, i.e., /i iA B . The terms with a high ratio are often referred to as the terms with high category information." ></td>
	<td class="line x" title="62:174	These two measurements form the basis for all the measurements that are used by the FS methods throughout this paper." ></td>
	<td class="line x" title="63:174	In particular, we show that DF and MI are using the first and second measurement respectively." ></td>
	<td class="line x" title="64:174	Other complicated FS methods are combinations of these two measurements." ></td>
	<td class="line x" title="65:174	Thus, we regard the two measurements as basic, which are referred to as the frequency measurement and ratio measurement." ></td>
	<td class="line x" title="66:174	3.1 Document Frequency (DF) DF is the number of documents in which a term occurs." ></td>
	<td class="line x" title="67:174	It is defined as 1( ) m iiDF A== The terms with low or high document frequency are often referred to as rare or common terms, respectively." ></td>
	<td class="line x" title="68:174	It is easy to see that this FS method is based on the first basic measurement." ></td>
	<td class="line x" title="69:174	It assumes that the terms with higher document frequency are more informative for classification." ></td>
	<td class="line x" title="70:174	But sometimes this assumption does not make any sense, for example, the stop words (e.g., the, a, an) hold very high DF scores, but they seldom contribute to classification." ></td>
	<td class="line x" title="71:174	In general, this simple method performs very well in some topic-based classification tasks (Yang and Pedersen, 1997)." ></td>
	<td class="line x" title="72:174	3.2 Mutual Information (MI) The mutual information between term t  and class ic  is defined as ( | )( , ) log ( ) i i P t cI t c P t= And it is estimated as log ( )( )i all i i i i A NMI A C A B = + + Let us consider the following formula (using Bayes theorem) ( | ) ( | )( , ) log log ( ) ( ) i i i i P t c P c tI t c P t P c= = Therefore, ( , )= log ( | ) log ( )i i iI t c P c t P c And it is estimated as log log       log log 1      log(1 ) log / i i i i all i i i i all i i i all A NMI A B N A B N A N N A B N = + +=   =  +   From this formula, we can see that the MI score is based on the second basic measurement." ></td>
	<td class="line x" title="73:174	This method assumes that the term with higher category ratio is more effective for classification." ></td>
	<td class="line x" title="74:174	It is reported that this method is biased towards low frequency terms and the bias becomes extreme when ( )P t  is near zero." ></td>
	<td class="line x" title="75:174	It can be seen in the following formula (Yang and Pedersen, 1997) ( , ) log( ( | )) log( ( ))i iI t c P t c P t=  694 Therefore, this method might perform badly when common terms are informative for classification." ></td>
	<td class="line x" title="76:174	Taking into account mutual information of all categories, two types of MI score are commonly used: the maximum score ( )maxI t  and the average score ( )avgI t , i.e., 1( ) max { ( , )} m max i iI t I t c== , 1( ) ( ) ( , ) m avg i iiI t P c I t c==  . We choose the maximum score since it performs better than the average score (Yang and Pedersen, 1997)." ></td>
	<td class="line x" title="77:174	It is worth noting that the same choice is made for other methods, including CHI, BNS, and WLLR in this paper." ></td>
	<td class="line x" title="78:174	3.3 Information Gain (IG) IG measures the number of bits of information obtained for category prediction by recognizing the presence or absence of a term in a document (Yang and Pedersen, 1997)." ></td>
	<td class="line x" title="79:174	The function is 1 1 1 ( ) { ( ) log ( )}             +{ ( )[ ( | )log ( | )]            ( )[ ( | ) log ( | )]} m i ii m i ii m i ii G t P c P c P t P c t P c t P t P c t P c t = = = =  +     And it is estimated as 1 1 1 1 1 { log }     +( / )[ log ]   ( / )[ log ] m i i i all all m m i i i alli i i i i i m m i i i alli i i i i i N NIG N N A AA N A B A B C CC N C D C D = = = = = =  + + + + +      From the definition, we know that the information gain is the weighted average of the mutual information ( , )iI t c and ( , )iI t c  where the weights are the joint probabilities ( , )iP t c and ( , )iP t c : 1 1( ) ( , ) ( , ) ( , ) ( , ) m m i i i ii iG t P t c I t c P t c I t c= == +  Since ( , )iP t c is closely related to the document frequency iA  and the mutual information ( , )iI t c  is shown to be based on the second measurement, we can say that the IG score is influenced by the two basic measurements." ></td>
	<td class="line x" title="80:174	3.4 2  Statistic (CHI) The CHI measurement (Yang and Pedersen, 1997) is defined as 2( ) ( ) ( ) ( ) ( ) all i i i i i i i i i i i i N A D C BCHI A C B D A B C D  = +  +  +  + In order to get the relationship between CHI and the two measurements, the above formula is rewritten as follows 2[ ( ) ( ) ] ( ) ( ) [ ( )] all i all i i i i i i all i i i all i i N A N N B N A BCHI N N N A B N A B     =    +   +  For simplicity, we assume that there are two categories and the numbers of the training documents in the two categories are the same ( 2all iN N= )." ></td>
	<td class="line x" title="81:174	The CHI score then can be written as  2 2 2 ( ) ( ) [2 ( )] 2 ( / 1) 2( / 1) [ / ( / 1)] i i i i i i i i i i i i i i i i i i i N A BCHI A B N A B N A B NA B A B A B A = +   + = +    +  From the above formula, we see that the CHI score is related to both the frequency measurement iA  and ratio measurement /i iA B . Also, when keeping the same ratio value, the terms with higher document frequencies will yield higher CHI scores." ></td>
	<td class="line x" title="82:174	3.5 Bi-Normal Separation (BNS) BNS method is originally proposed by Forman (2003) and it is defined as 1 1( , ) ( ( | )) ( ( | ) i i iBNS t c F P t c F P t c  =  It is calculated using the following formula 1 1( ) ( )i i i all i A BBNS F F N N N  =   where ( )F x  is the cumulative probability function of standard normal distribution." ></td>
	<td class="line x" title="83:174	For simplicity, we assume that there are two categories and the numbers of the training documents in the two categories are the same, i.e., 2all iN N=  and we also assume that i iA B> . It should be noted that this assumption is only to allow easier analysis but will not be applied in our experiment implementation." ></td>
	<td class="line x" title="84:174	In addition, we only consider the case when / 0.5i iA N  . In fact, most terms take the document frequency iA which is less than half of iN . Under these conditions, the BNS score can be shown in Figure 1 where the area of the shadow part represents ( / / )i i i iA N B N  and the length of the projection to the x  axis represents the BNS score." ></td>
	<td class="line x" title="85:174	695 From Figure 1, we can easily draw the two following conclusions: 1) Given the same value of iA , the BNS score increases with the increase of i iA B . 2) Given the same value of i iA B , BNS score increase with the decrease of iA .  Figure 1." ></td>
	<td class="line x" title="86:174	View of BNS using the normal probability distribution." ></td>
	<td class="line x" title="87:174	Both the left and right graphs have shadowed areas of the same size." ></td>
	<td class="line x" title="88:174	And the value of i iA B  can be rewritten as the following 1(1 ) / i i i i i i i i i A BA B A A A A B  =  =   The above analysis gives the following conclusions regarding the relationship between BNS and the two basic measurements: 1) Given the same iA , the BNS score increases with the increase of /i iA B . 2) Given the same /i iA B , when iA  increases, i iA B  also increase." ></td>
	<td class="line x" title="89:174	It seems that the BNS score does not show a clear relationship with iA . In summary, the BNS FS method is biased towards the terms with the high category ratio but cannot be said to be sensitive to document frequency." ></td>
	<td class="line x" title="90:174	3.6 Weighted Log Likelihood Ratio (WLLR) WLLR method (Nigam et al., 2000) is defined as ( | )( , ) ( | )log ( | ) i i i i P t cWLLR t c P t c P t c= And it is estimated as ( )logi i all i i i i A A N NWLLR N B N  =  The formula shows WLLR is proportional to the frequency measurement and the logarithm of the ratio measurement." ></td>
	<td class="line x" title="91:174	Clearly, WLLR is biased towards the terms with both high category ratio and high document frequency and the frequency measurement seems to take a more important place than the ratio measurement." ></td>
	<td class="line x" title="92:174	3.7 Weighed Frequency and Odds (WFO) So far in this section, we have shown that the two basic measurements constitute the six FS methods." ></td>
	<td class="line x" title="93:174	The class prior probabilities, ( ), 1,2,,iP c i m= , are also related to the selection methods except for the two basic measurements." ></td>
	<td class="line x" title="94:174	Since they are often estimated according to the distribution of the documents in the training data and are identical for all the terms in a class, we ignore the discussion of their influence on the selection measurements." ></td>
	<td class="line x" title="95:174	In the experiment, we consider the case when training data have equal class prior probabilities." ></td>
	<td class="line x" title="96:174	When training data are unbalanced, we need to change the forms of the two basic measurements to /i iA N  and ( ) / ( )i all i i iA N N B N   . Because some methods are expressed in complex forms, it is difficult to explain their relationship with the two basic measurements, for example, which one prefers the category ratio most." ></td>
	<td class="line x" title="97:174	Instead, we will give the preference analysis in the experiment by analyzing the features in real applications." ></td>
	<td class="line x" title="98:174	But the following two conclusions are drawn without doubt according to the theoretical analysis given above." ></td>
	<td class="line x" title="99:174	1) Good features are features with high document frequency; 2) Good features are features with high category ratio." ></td>
	<td class="line x" title="100:174	These two conclusions are consistent with the original intuition." ></td>
	<td class="line x" title="101:174	However, using any single one does not provide competence in selecting the best set of features." ></td>
	<td class="line x" title="102:174	For example, stop words, such as a, the and as, have very high document frequency but are useless for the classification." ></td>
	<td class="line x" title="103:174	In real applications, we need to mix these two measurements to select good features." ></td>
	<td class="line x" title="104:174	Because of different distribution of features in different domains, the importance of each measurement may differ a lot in different applications." ></td>
	<td class="line x" title="105:174	Moreover, even in a given domain, when different numbers of features are to be selected, different combinations of the two measurements are required to provide the best performance." ></td>
	<td class="line x" title="106:174	Although a great number of FS methods is available, none of them can appropriately change the preference of the two measurements." ></td>
	<td class="line x" title="107:174	A better way is to tune the importance according to the application rather than to use a predetermined combination." ></td>
	<td class="line x" title="108:174	Therefore, we propose a new FS method called Weighed Frequency and Odds (WFO), which is defined as 696  ( | ) / ( | ) 1i iwhen P t c P t c > 1( | )( , ) ( | ) [log ] ( | ) i i i i P t cWFO t c P t c P t c  =                  ( , ) 0i else WFO t c = And it is estimated as 1( )( ) (log )i i all i i i i A A N NWFO N B N   =  where   is the parameter for tuning the weight between frequency and odds." ></td>
	<td class="line x" title="109:174	The value of  varies from 0 to 1." ></td>
	<td class="line x" title="110:174	By assigning different value to   we can adjust the preference of each measurement." ></td>
	<td class="line x" title="111:174	Specially, when 0 = , the algorithm prefers the category ratio that is equivalent to the MI method; when 1 = , the algorithm is similar to DF; when 0.5 = , the algorithm is exactly the WLLR method." ></td>
	<td class="line x" title="112:174	In real applications, a suitable parameter   needs to be learned by using training data." ></td>
	<td class="line x" title="113:174	4 Experimental Studies 4.1 Experimental Setup Data Set:  The experiments are carried out on both topic-based and sentiment text classification datasets." ></td>
	<td class="line x" title="114:174	In topic-based text classification, we use two popular data sets: one subset of Reuters-21578 referred to as R2 and the 20 Newsgroup dataset referred to as 20NG." ></td>
	<td class="line x" title="115:174	In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578." ></td>
	<td class="line x" title="116:174	And 20NG is a collection of approximately 20,000 20-category documents 1 . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset2 (Pang and Lee, 2004) and one dataset from product reviews of domain DVD3 (Blitzer et al., 2007)." ></td>
	<td class="line x" title="117:174	Both of them are 2-category tasks and each consists of 2,000 reviews." ></td>
	<td class="line x" title="118:174	In our experiments, the document numbers of all data sets are (nearly) equally distributed cross all categories." ></td>
	<td class="line x" title="119:174	Classification Algorithm: Many classification algorithms are available for text classification, such as Nave Bayes, Maximum Entropy, k-NN, and SVM." ></td>
	<td class="line oc" title="120:174	Among these methods, SVM is shown to perform better than other methods (Yang and Pedersen, 1997; Pang et al.,  1 http://people.csail.mit.edu/~jrennie/20Newsgroups/ 2 http://www.cs.cornell.edu/People/pabo/movie-review-data/ 3 http://www.seas.upenn.edu/~mdredze/datasets/sentiment/  2002)." ></td>
	<td class="line x" title="121:174	Hence we apply SVM algorithm with the help of the LIBSVM 4  tool." ></td>
	<td class="line x" title="122:174	Almost all parameters are set to their default values except the kernel function which is changed from a polynomial kernel function to a linear one because the linear one usually performs better for text classification tasks." ></td>
	<td class="line x" title="123:174	Experiment Implementation: In the experiments, each dataset is randomly and evenly split into two subsets: 90% documents as the training data and the remaining 10% as testing data." ></td>
	<td class="line x" title="124:174	The training data are used for training SVM classifiers, learning parameters in WFO method and selecting 'good' features for each FS method." ></td>
	<td class="line x" title="125:174	The features are single words with a bool weight (0 or 1), representing the presence or absence of a feature." ></td>
	<td class="line x" title="126:174	In addition to the principled FS methods, terms occurring in less than three documents ( 3DF  ) in the training set are removed." ></td>
	<td class="line x" title="127:174	4.2 Relationship between FS Methods and the Two Basic Measurements To help understand the relationship between FS methods and the two basic measurements, the empirical study is presented as follows." ></td>
	<td class="line x" title="128:174	Since the methods of DF and MI only utilize the document frequency and category information respectively, we use the DF scores and MI scores to represent the information of the two basic measurements." ></td>
	<td class="line x" title="129:174	Thus we would select the top-2% terms with each method and then investigate the distribution of their DF and MI scores." ></td>
	<td class="line x" title="130:174	First of all, for clear comparison, we normalize the scores coming from all the methods using Min-Max normalization method which is designed to map a score s  to 's  in the range [0, 1] by computing ' s Mins Max Min=  where Min  and Max  denote the minimum and maximum values respectively in all terms scores using one FS method." ></td>
	<td class="line x" title="131:174	Table 1 shows the mean values of all top-2% terms MI scores and DF scores of all the six FS methods in each domain." ></td>
	<td class="line x" title="132:174	From this table, we can apparently see the relationship between each method and the two basic measurements." ></td>
	<td class="line x" title="133:174	For instance, BNS most distinctly prefers the terms with high MI scores and low DF scores." ></td>
	<td class="line x" title="134:174	According to the degree of this preference, we  4 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 697 FS Methods Domain 20NG R2 Movie DVD DF score MI score DF score MI score DF score MI score DF score MI score MI 0.004 0.870 0.047 0.959 0.003 0.888 0.004 0.881 BNS 0.005 0.864 0.117 0.922 0.008 0.881 0.006 0.880 CHI 0.015 0.814 0.211 0.748 0.092 0.572 0.055 0.676 IG 0.087 0.525 0.209 0.792 0.095 0.559 0.066 0.669 WLLR 0.026 0.764 0.206 0.805 0.168 0.414 0.127 0.481 DF 0.122 0.252 0.268 0.562 0.419 0.09 0.321 0.111  Table 1." ></td>
	<td class="line x" title="135:174	The mean values of all top-2% terms MI and DF scores using six FS methods in each domain  can rank these six methods as MI, BNS IG, CHI, WLLR DFf f , where x yf means method x  prefers the terms with higher MI scores (higher category information) and lower DF scores (lower document frequency) than method y. This empirical discovery is in agreement with the finding that WLLR is biased towards the high frequency terms and also with the finding that BNS is biased towards high category information (cf.Section 3 theoretical analysis)." ></td>
	<td class="line x" title="137:174	Also, we can find that CHI and IG share a similar preference of these two measurements in 2-category domains, i.e., R2, movie, and DVD." ></td>
	<td class="line x" title="138:174	This gives a good explanation that CHI and IG are two similar-performed methods for 2-category tasks, which have been found by Forman (2003) in their experimental studies." ></td>
	<td class="line x" title="139:174	According to the preference, we roughly cluster FS methods into three groups." ></td>
	<td class="line x" title="140:174	The first group includes the methods which dramatically prefer the category information, e.g., MI and BNS; the second one includes those which prefer both kinds of information, e.g., CHI, IG, and WLLR; and the third one includes those which strongly prefer frequency information, e.g., DF." ></td>
	<td class="line x" title="141:174	4.3 Performances of Different FS Methods It is worth noting that learning parameters in WFO is very important for its good performance." ></td>
	<td class="line x" title="142:174	We use 9-fold cross validation to help learning the parameter   so as to avoid over-fitting." ></td>
	<td class="line x" title="143:174	Specifically, we run nine times by using every 8 fold documents as a new training data set and the remaining one fold documents as a development data set." ></td>
	<td class="line x" title="144:174	In each running with one fixed feature number m, we get the best ,i m best  (i=1,, 9) value through varying ,i m  from 0 to 1 with the step of 0.1 to get the best performance in the development data set." ></td>
	<td class="line x" title="145:174	The average value m best  , i.e., 9 ,1( ) / 9m best i m besti  ==  is used for further testing." ></td>
	<td class="line x" title="146:174	Figure 2 shows the experimental results when using all FS methods with different selected feature numbers." ></td>
	<td class="line x" title="147:174	The red line with star tags represents the results of WFO." ></td>
	<td class="line x" title="148:174	At the first glance, in R2 domain, the differences of performances across all are very noisy when the feature number is larger than 1,000, which makes the comparison meaningless." ></td>
	<td class="line x" title="149:174	We think that this is because the performances themselves in this task are very high (nearly 98%) and the differences between two FS methods cannot be very large (less than one percent)." ></td>
	<td class="line x" title="150:174	Even this, WFO method do never get the worst performance and can also achieve the top performance in about half times, e.g., when feature numbers are 20, 50, 100, 500, 3000." ></td>
	<td class="line x" title="151:174	Let us pay more attention to the other three domains and discuss the results in the following two cases." ></td>
	<td class="line x" title="152:174	In the first case when the feature number is low (about less than 1,000), the FS methods in the second group including IG, CHI, WLLR, always perform better than those in the other two groups." ></td>
	<td class="line x" title="153:174	WFO can also perform well because its parameters m best   are successfully learned to be around 0.5, which makes it consistently belong to the second group." ></td>
	<td class="line x" title="154:174	Take 500 feature number for instance, the parameters 500 best   are 0.42, 0.50, and 0.34 in these three domains respectively." ></td>
	<td class="line x" title="155:174	In the second case when the feature number is large, among the six traditional methods, MI and BNS take the leads in the domains of 20NG and Movie while IG and CHI seem to be better and more stable than others in the domain of DVD." ></td>
	<td class="line x" title="156:174	As for WFO, its performances are excellent cross all these three domains and different feature numbers." ></td>
	<td class="line x" title="157:174	In each domain, it performs similarly as or better than the top methods due to its well-learned parameters." ></td>
	<td class="line x" title="158:174	For example, in 20NG, the parameters m best   are 0.28, 0.20, 0.08, and 0.01 when feature numbers are 10,000, 15,000, 20,000, and 30,000." ></td>
	<td class="line x" title="159:174	These values are close to 0 698 (WFO equals MI when 0 = ) while MI is the top one in this domain." ></td>
	<td class="line x" title="160:174	10 20 50 100 200 500 1000 2000 3000 4227 0.88 0.9 0.92 0.94 0.96 0.98 1 feature number ac cu rac y Topic R2   DF MI IG BNS CHI WLLR WFO  200 500 1000 2000 5000 10000 15000 20000 30000 320910.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 feature number ac cu rac y Topic 20NG   DF MI IG BNS CHI WLLR WFO  50 200 500 1000 4000 7000 10000 13000 151760.55 0.6 0.65 0.7 0.75 0.8 0.85 feature number ac cu rac y Sentiment Movie   DF MI IG BNS CHI WLLR WFO  20 50 100 500 1000 1500 2000 3000 4000 58240.5 0.55 0.6 0.65 0.7 0.75 0.8 feature number ac cu rac y Sentiment DVD   DF MI IG BNS CHI WLLR WFO  Figure 2." ></td>
	<td class="line x" title="161:174	The classification accuracies of the four domains using seven different FS methods while increasing the number of selected features." ></td>
	<td class="line x" title="162:174	From Figure 2, we can also find that FS does help sentiment classification." ></td>
	<td class="line x" title="163:174	At least, it can dramatically decrease the feature numbers without losing classification accuracies (see Movie domain, using only 500-4000 features is as good as using all 15176 features)." ></td>
	<td class="line x" title="164:174	5 Conclusion and Future Work In this paper, we propose a framework with two basic measurements and use it to theoretically analyze six FS methods." ></td>
	<td class="line x" title="165:174	The differences among them mainly lie in how they use these two measurements." ></td>
	<td class="line x" title="166:174	Moreover, with the guidance of the analysis, a novel method called WFO is proposed, which combine these two measurements with trained weights." ></td>
	<td class="line x" title="167:174	The experimental results show that our framework helps us to better understand and compare different FS methods." ></td>
	<td class="line x" title="168:174	Furthermore, the novel method WFO generated from the framework, can perform robustly across different domains and feature numbers." ></td>
	<td class="line x" title="169:174	In our study, we use four data sets to test our new method." ></td>
	<td class="line x" title="170:174	There are much more data sets on text categorization which can be used." ></td>
	<td class="line x" title="171:174	In additional, we only focus on using balanced samples in each category to do the experiments." ></td>
	<td class="line x" title="172:174	It is also necessary to compare the FS methods on some unbalanced data sets, which are common in real-life applications (Forman, 2003; Mladeni and Marko, 1999)." ></td>
	<td class="line x" title="173:174	These matters will be dealt with in the future work." ></td>
	<td class="line x" title="174:174	Acknowledgments The research work described in this paper has been partially supported by Start-up Grant for Newly Appointed Professors, No. 1-BBZM in the Hong Kong Polytechnic University." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1079
Mine the Easy, Classify the Hard: A Semi-Supervised Approach to Automatic Sentiment Classification
Dasgupta, Sajib;Ng, Vincent;"></td>
	<td class="line x" title="1:254	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 701709, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:254	c2009 ACL and AFNLP Mine the Easy, Classify the Hard: A Semi-Supervised Approach to Automatic Sentiment Classification Sajib Dasgupta and Vincent Ng Human Language Technology Research Institute University of Texas at Dallas Richardson, TX 75083-0688 {sajib,vince}@hlt.utdallas.edu Abstract Supervised polarity classification systems are typically domain-specific." ></td>
	<td class="line x" title="3:254	Building these systems involves the expensive process of annotating a large amount of data for each domain." ></td>
	<td class="line x" title="4:254	A potential solution to this corpus annotation bottleneck is to build unsupervised polarity classification systems." ></td>
	<td class="line x" title="5:254	However, unsupervised learning of polarity is difficult, owing in part to the prevalence of sentimentally ambiguous reviews, where reviewers discuss both the positive and negative aspects of a product." ></td>
	<td class="line x" title="6:254	To address this problem, we propose a semi-supervised approach to sentiment classification where we first mine the unambiguous reviews using spectral techniques and then exploit them to classify the ambiguous reviews via a novel combination of active learning, transductive learning, and ensemble learning." ></td>
	<td class="line x" title="7:254	1 Introduction Sentiment analysis has recently received a lot of attention in the Natural Language Processing (NLP) community." ></td>
	<td class="line x" title="8:254	Polarity classification, whose goal is to determine whether the sentiment expressed in a document is thumbs up or thumbs down, is arguably one of the most popular tasks in document-level sentiment analysis." ></td>
	<td class="line x" title="9:254	Unlike topic-based text classification, where a high accuracy can be achieved even for datasets with a large number of classes (e.g., 20 Newsgroups), polarity classification appears to be a more difficult task." ></td>
	<td class="line x" title="10:254	One reason topic-based text classification is easier than polarity classification is that topic clusters are typically well-separated from each other, resulting from the fact that word usage differs considerably between two topically-different documents." ></td>
	<td class="line x" title="11:254	On the other hand, many reviews are sentimentally ambiguous for a variety of reasons." ></td>
	<td class="line x" title="12:254	For instance, an author of a movie review may have negative opinions of the actors but at the same time talk enthusiastically about how much she enjoyed the plot." ></td>
	<td class="line x" title="13:254	Here, the review is ambiguous because she discussed both the positive and negative aspects of the movie, which is not uncommon in reviews." ></td>
	<td class="line x" title="14:254	As another example, a large portion of a movie review may be devoted exclusively to the plot, with the author only briefly expressing her sentiment at the end of the review." ></td>
	<td class="line x" title="15:254	In this case, the review is ambiguous because the objective material in the review, which bears no sentiment orientation, significantly outnumbers its subjective counterpart." ></td>
	<td class="line x" title="16:254	Realizing the challenges posed by ambiguous reviews, researchers have explored a number of techniques to improve supervised polarity classifiers." ></td>
	<td class="line x" title="17:254	For instance, Pang and Lee (2004) train an independent subjectivity classifier to identify and remove objective sentences from a review prior to polarity classification." ></td>
	<td class="line x" title="18:254	Koppel and Schler (2006) use neutral reviews to help improve the classification of positive and negative reviews." ></td>
	<td class="line x" title="19:254	More recently, McDonald et al.(2007) have investigated a model for jointly performing sentenceand document-level sentiment analysis, allowing the relationship between the two tasks to be captured and exploited." ></td>
	<td class="line x" title="21:254	However, the increased sophistication of supervised polarity classifiers has also resulted in their increased dependence on annotated data." ></td>
	<td class="line x" title="22:254	For instance, Koppel and Schler needed to manually identify neutral reviews to train their polarity classifier, and McDonald et al.s joint model requires that each sentence in a review be labeled with polarity information." ></td>
	<td class="line x" title="23:254	Given the difficulties of supervised polarity classification, it is conceivable that unsupervised polarity classification is a very challenging task." ></td>
	<td class="line x" title="24:254	Nevertheless, a solution to unsupervised polarity classification is of practical significance." ></td>
	<td class="line x" title="25:254	One reason is that the vast majority of supervised polarity 701 classification systems are domain-specific." ></td>
	<td class="line x" title="26:254	Hence, when given a new domain, a large amount of annotated data from the domain typically needs to be collected in order to train a high-performance polarity classification system." ></td>
	<td class="line x" title="27:254	As Blitzer et al.(2007) point out, this data collection process can be prohibitively expensive, especially since product features can change over time." ></td>
	<td class="line x" title="29:254	Unfortunately, to our knowledge, unsupervised polarity classification is largely an under-investigated task in NLP." ></td>
	<td class="line x" title="30:254	Turneys (2002) work is perhaps one of the most notable examples of unsupervised polarity classification." ></td>
	<td class="line x" title="31:254	However, while his system learns the semantic orientation of phrases in a review in an unsupervised manner, such information is used to heuristically predict the polarity of a review." ></td>
	<td class="line x" title="32:254	At first glance, it may seem plausible to apply an unsupervised clustering algorithm such as kmeans to cluster the reviews according to their polarity." ></td>
	<td class="line x" title="33:254	However, there is reason to believe that such a clustering approach is doomed to fail: in the absence of annotated data, an unsupervised learner is unable to identify which features are relevant for polarity classification." ></td>
	<td class="line x" title="34:254	The situation is further complicated by the prevalence of ambiguous reviews, which may contain a large amount of irrelevant and/or contradictory information." ></td>
	<td class="line x" title="35:254	In light of the difficulties posed by ambiguous reviews, we differentiate between ambiguous and unambiguous reviews in our classification process by addressing the task of semi-supervised polarity classification via a mine the easy, classify the hard approach." ></td>
	<td class="line x" title="36:254	Specifically, we propose a novel system architecture where we first automatically identify and label the unambiguous (i.e., easy) reviews, then handle the ambiguous (i.e., hard) reviews using a discriminative learner to bootstrap from the automatically labeled unambiguous reviews and a small number of manually labeled reviews that are identified by an active learner." ></td>
	<td class="line x" title="37:254	It is worth noting that our system differs from existing work on unsupervised/active learning in two aspects." ></td>
	<td class="line x" title="38:254	First, while existing unsupervised approaches typically rely on clustering or learning via a generative model, our approach distinguishes between easy and hard instances and exploits the strengths of discriminative models to classify the hard instances." ></td>
	<td class="line x" title="39:254	Second, while existing active learners typically start with manually labeled seeds, our active learner relies only on seeds that are automatically extracted from the data." ></td>
	<td class="line x" title="40:254	Experimental results on five sentiment classification datasets demonstrate that our system can generate high-quality labeled data from unambiguous reviews, which, together with a small number of manually labeled reviews selected by the active learner, can be used to effectively classify ambiguous reviews in a discriminative fashion." ></td>
	<td class="line x" title="41:254	The rest of the paper is organized as follows." ></td>
	<td class="line x" title="42:254	Section 2 gives an overview of spectral clustering, which will facilitate the presentation of our approach to unsupervised sentiment classification in Section 3." ></td>
	<td class="line x" title="43:254	We evaluate our approach in Section 4 and present our conclusions in Section 5." ></td>
	<td class="line x" title="44:254	2 Spectral Clustering In this section, we give an overview of spectral clustering, which is at the core of our algorithm for identifying ambiguous reviews." ></td>
	<td class="line x" title="45:254	2.1 Motivation When given a clustering task, an important question to ask is: which clustering algorithm should be used?" ></td>
	<td class="line x" title="46:254	A popular choice is k-means." ></td>
	<td class="line x" title="47:254	Nevertheless, it is well-known that k-means has the major drawback of not being able to separate data points that are not linearly separable in the given feature space (e.g, see Dhillon et al.(2004))." ></td>
	<td class="line x" title="49:254	Spectral clustering algorithms were developed in response to this problem with k-means clustering." ></td>
	<td class="line x" title="50:254	The central idea behind spectral clustering is to (1) construct a low-dimensional space from the original (typically high-dimensional) space while retaining as much information about the original space as possible, and (2) cluster the data points in this lowdimensional space." ></td>
	<td class="line x" title="51:254	2.2 Algorithm Although there are several well-known spectral clustering algorithms in the literature (e.g., Weiss (1999), Meila and Shi (2001), Kannan et al.(2004)), we adopt the one proposed by Ng et al.(2002), as it is arguably the most widely used." ></td>
	<td class="line x" title="54:254	The algorithm takes as input a similarity matrix S created by applying a user-defined similarity function to each pair of data points." ></td>
	<td class="line x" title="55:254	Below are the main steps of the algorithm: 1." ></td>
	<td class="line x" title="56:254	Create the diagonal matrix G whose (i,i)th entry is the sum of the i-th row of S, and then construct the Laplacian matrix L = G1/2SG1/2." ></td>
	<td class="line x" title="57:254	2. Find the eigenvalues and eigenvectors of L. 702 3." ></td>
	<td class="line x" title="58:254	Create a new matrix from the m eigenvectors that correspond to the m largest eigenvalues.1 4." ></td>
	<td class="line x" title="59:254	Each data point is now rank-reduced to a point in the m-dimensional space." ></td>
	<td class="line x" title="60:254	Normalize each point to unit length (while retaining the sign of each value)." ></td>
	<td class="line x" title="61:254	5." ></td>
	<td class="line x" title="62:254	Cluster the resulting data points using kmeans." ></td>
	<td class="line x" title="63:254	In essence, each dimension in the reduced space is defined by exactly one eigenvector." ></td>
	<td class="line x" title="64:254	The reason why eigenvectors with large eigenvalues are retained is that they capture the largest variance in the data." ></td>
	<td class="line x" title="65:254	Therefore, each of them can be thought of as revealing an important dimension of the data." ></td>
	<td class="line x" title="66:254	3 Our Approach While spectral clustering addresses a major drawback of k-means clustering, it still cannot be expected to accurately partition the reviews due to the presence of ambiguous reviews." ></td>
	<td class="line x" title="67:254	Motivated by this observation, rather than attempting to cluster all the reviews at the same time, we handle them in different stages." ></td>
	<td class="line x" title="68:254	As mentioned in the introduction, we employ a mine the easy, classify the hard approach to polarity classification, where we (1) identify and classify the easy (i.e., unambiguous) reviews with the help of a spectral clustering algorithm; (2) manually label a small number of hard (i.e., ambiguous) reviews selected by an active learner; and (3) using the reviews labeled thus far, apply a transductive learner to label the remaining (ambiguous) reviews." ></td>
	<td class="line x" title="69:254	In this section, we discuss each of these steps in detail." ></td>
	<td class="line x" title="70:254	3.1 Identifying Unambiguous Reviews We begin by preprocessing the reviews to be classified." ></td>
	<td class="line x" title="71:254	Specifically, we tokenize and downcase each review and represent it as a vector of unigrams, using frequency as presence." ></td>
	<td class="line x" title="72:254	In addition, we remove from the vector punctuation, numbers, words of length one, and words that occur in a single review only." ></td>
	<td class="line x" title="73:254	Finally, following the common practice in the information retrieval community, we remove words with high document frequency, many of which are stopwords or domainspecific general-purpose words (e.g., movies in the movie domain)." ></td>
	<td class="line x" title="74:254	A preliminary examination of our evaluation datasets reveals that these words 1For brevity, we will refer to the eigenvector with the n-th largest eigenvalue simply as the n-th eigenvector." ></td>
	<td class="line x" title="75:254	typically comprise 12% of a vocabulary." ></td>
	<td class="line x" title="76:254	The decision of exactly how many terms to remove from each dataset is subjective: a large corpus typically requires more removals than a small corpus." ></td>
	<td class="line x" title="77:254	To be consistent, we simply sort the vocabulary by document frequency and remove the top 1.5%." ></td>
	<td class="line x" title="78:254	Recall that in this step we use spectral clustering to identify unambiguous reviews." ></td>
	<td class="line x" title="79:254	To make use of spectral clustering, we first create a similarity matrix, defining the similarity between two reviews as the dot product of their feature vectors, but following Ng et al.(2002), we set its diagonal entries to 0." ></td>
	<td class="line x" title="81:254	We then perform an eigen-decomposition of this matrix, as described in Section 2.2." ></td>
	<td class="line x" title="82:254	Finally, using the resulting eigenvectors, we partition the length-normalized reviews into two sets." ></td>
	<td class="line x" title="83:254	As Ng et al. point out, different authors still disagree on which eigenvectors to use, and how to derive clusters from them." ></td>
	<td class="line x" title="84:254	To create two clusters, the most common way is to use only the second eigenvector, as Shi and Malik (2000) proved that this eigenvector induces an intuitively ideal partition of the data  the partition induced by the minimum normalized cut of the similarity graph2, where the nodes are the data points and the edge weights are the pairwise similarity values of the points." ></td>
	<td class="line x" title="85:254	Clustering in a one-dimensional space is trivial: since we have a linearization of the points, all we need to do is to determine a threshold for partitioning the points." ></td>
	<td class="line x" title="86:254	A common approach is to set the threshold to zero." ></td>
	<td class="line x" title="87:254	In other words, all points whose value in the second eigenvector is positive are classified as positive, and the remaining points are classified as negative." ></td>
	<td class="line x" title="88:254	However, we found that the second eigenvector does not always induce a partition of the nodes that corresponds to the minimum normalized cut." ></td>
	<td class="line x" title="89:254	One possible reason is that Shi and Maliks proof assumes the use of a Laplacian matrix that is different from the one used by Ng et al. To address this problem, we use the first five eigenvectors: for each eigenvector, we (1) use each of its n elements as a threshold to independently generate n partitions, (2) compute the normalized cut value for each partition, and (3) find the minimum of the n cut values." ></td>
	<td class="line x" title="90:254	We then select the eigenvector that corresponds to the smallest of the five minimum cut values." ></td>
	<td class="line x" title="91:254	Next, we identify the ambiguous reviews from 2Using the normalized cut (as opposed to the usual cut) ensures that the size of the two clusters are relatively balanced, avoiding trivial cuts where one cluster is empty and the other is full." ></td>
	<td class="line x" title="92:254	See Shi and Malik (2000) for details." ></td>
	<td class="line x" title="93:254	703 the resulting partition." ></td>
	<td class="line x" title="94:254	To see how this is done, consider the example in Figure 1, where the goal is to produce two clusters from five data points." ></td>
	<td class="line x" title="95:254	parenleftBigg1 1 1 0 0 1 1 1 0 00 0 1 1 0 0 0 0 1 10 0 0 1 1 parenrightBigg parenleftBigg0.6983 0.7158 0.6983 0.7158 0.9869 0.1616 0.6224 0.7827 0.6224 0.7827 parenrightBigg Figure 1: Sample data and the top two eigenvectors of its Laplacian In the matrix on the left, each row is the feature vector generated for Di, the i-th data point." ></td>
	<td class="line x" title="96:254	By inspection, one can identify two clusters, {D1,D2} and {D4,D5}." ></td>
	<td class="line x" title="97:254	D3 is ambiguous, as it bears resemblance to the points in both clusters and therefore can be assigned to any of them." ></td>
	<td class="line x" title="98:254	In the matrix on the right, the two columns correspond to the top two eigenvectors obtained via an eigendecomposition of the Laplacian matrix formed from the five data points." ></td>
	<td class="line x" title="99:254	As we can see, the second eigenvector gives us a natural cluster assignment: all the points whose corresponding values in the second eigenvector are strongly positive will be in one cluster, and the strongly negative points will be in another cluster." ></td>
	<td class="line x" title="100:254	Being ambiguous, D3 is weakly negative and will be assigned to the negative cluster." ></td>
	<td class="line x" title="101:254	Before describing our algorithm for identifying ambiguous data points, we make two additional observations regarding D3." ></td>
	<td class="line x" title="102:254	First, if we removed D3, we could easily cluster the remaining (unambiguous) points, since the similarity graph becomes more disconnected as we remove more ambiguous data points." ></td>
	<td class="line x" title="103:254	The question then is: why is it important to produce a good clustering of the unambiguous points?" ></td>
	<td class="line x" title="104:254	Recall that the goal of this step is not only to identify the unambiguous reviews, but also to annotate them as POSITIVE or NEGATIVE, so that they can serve as seeds for semi-supervised learning in a later step." ></td>
	<td class="line x" title="105:254	If we have a good 2-way clustering of the seeds, we can simply annotate each cluster (by sampling a handful of its reviews) rather than each seed." ></td>
	<td class="line x" title="106:254	To reiterate, removing the ambiguous data points can help produce a good clustering of their unambiguous counterparts." ></td>
	<td class="line x" title="107:254	Second, as an ambiguous data point, D3 can in principle be assigned to any of the two clusters." ></td>
	<td class="line x" title="108:254	According to the second eigenvector, it should be assigned to the negative cluster; but if feature #4 were irrelevant, it should be assigned to the positive cluster." ></td>
	<td class="line x" title="109:254	In other words, the ability to determine the relevance of each feature is crucial to the accurate clustering of the ambiguous data points." ></td>
	<td class="line x" title="110:254	However, in the absence of labeled data, it is not easy to assess feature relevance." ></td>
	<td class="line x" title="111:254	Even if labeled data were present, the ambiguous points might be better handled by a discriminative learning system than a clustering algorithm, as discriminative learners are more sophisticated, and can handle ambiguous feature space more effectively." ></td>
	<td class="line x" title="112:254	Taking into account these two observations, we aim to (1) remove the ambiguous data points while clustering their unambiguous counterparts, and then (2) employ a discriminative learner to label the ambiguous points in a later step." ></td>
	<td class="line x" title="113:254	The question is: how can we identify the ambiguous data points?" ></td>
	<td class="line x" title="114:254	To do this, we exploit an important observation regarding eigendecomposition." ></td>
	<td class="line x" title="115:254	In the computation of eigenvalues, each data point factors out the orthogonal projections of each of the other data points with which they have an affinity." ></td>
	<td class="line x" title="116:254	Ambiguous data points receive the orthogonal projections from both the positive and negative data points, and hence they have near-zero values in the pivot eigenvectors." ></td>
	<td class="line x" title="117:254	Given this observation, our algorithm uses the eight steps below to remove the ambiguous points in an iterative fashion and produce a clustering of the unambiguous points." ></td>
	<td class="line x" title="118:254	1." ></td>
	<td class="line x" title="119:254	Create a similarity matrix S from the data points D. 2." ></td>
	<td class="line x" title="120:254	Form the Laplacian matrix L from S. 3." ></td>
	<td class="line x" title="121:254	Find the top five eigenvectors of L. 4." ></td>
	<td class="line x" title="122:254	Row-normalize the five eigenvectors." ></td>
	<td class="line x" title="123:254	5." ></td>
	<td class="line x" title="124:254	Pick the eigenvector e for which we get the minimum normalized cut." ></td>
	<td class="line x" title="125:254	6." ></td>
	<td class="line x" title="126:254	Sort D according to e and remove  points in the middle of D (i.e., the points indexed from |D| 2   2 +1 to |D| 2 +  2 ).7." ></td>
	<td class="line x" title="127:254	If |D| = , goto Step 8; else goto Step 1." ></td>
	<td class="line x" title="128:254	8. Run 2-means on e to cluster the points in D. This algorithm can be thought of as the opposite of self-training." ></td>
	<td class="line x" title="129:254	In self-training, we iteratively train a classifier on the data labeled so far, use it to classify the unlabeled instances, and augment the labeled data with the most confidently labeled instances." ></td>
	<td class="line x" title="130:254	In our algorithm, we start with an initial clustering of all of the data points, and then iteratively remove the  most ambiguous points from the dataset and cluster the remaining points." ></td>
	<td class="line x" title="131:254	Given this analogy, it should not be difficult to see the advantage of removing the data points in an iterative fashion (as opposed to removing them in a 704 single iteration): the clusters produced in a given iteration are supposed to be better than those in the previous iterations, as subsequent clusterings are generated from less ambiguous points." ></td>
	<td class="line x" title="132:254	In our experiments, we set  to 50 and  to 500.3 Finally, we label the two clusters." ></td>
	<td class="line x" title="133:254	To do this, we first randomly sample 10 reviews from each cluster and manually label each of them as POSITIVE or NEGATIVE." ></td>
	<td class="line x" title="134:254	Then, we label a cluster as POSITIVE if more than half of the 10 reviews from the cluster are POSITIVE; otherwise, it is labeled as NEGATIVE." ></td>
	<td class="line x" title="135:254	For each of our evaluation datasets, this labeling scheme always produces one POSITIVE cluster and one NEGATIVE cluster." ></td>
	<td class="line x" title="136:254	In the rest of the paper, we will refer to these 500 automatically labeled reviews as seeds." ></td>
	<td class="line x" title="137:254	A natural question is: can this algorithm produce high-quality seeds?" ></td>
	<td class="line x" title="138:254	To answer this question, we show in the middle column of Table 1 the labeling accuracy of the 500 reviews produced by our iterative algorithm for our five evaluation datasets (see Section 4.1 for details on these datasets)." ></td>
	<td class="line x" title="139:254	To better understand whether it is indeed beneficial to remove the ambiguous points in an iterative fashion, we also show the results of a version of this algorithm in which we remove all but the 500 least ambiguous points in just one iteration (see the rightmost column)." ></td>
	<td class="line x" title="140:254	As we can see, for three datasets (Movie, Kitchen, and Electronics), the accuracy is above 80%." ></td>
	<td class="line x" title="141:254	For the remaining two (Book and DVD), the accuracy is not particularly good." ></td>
	<td class="line x" title="142:254	One plausible reason is that the ambiguous reviews in Book and DVD are relatively tougher to identify." ></td>
	<td class="line x" title="143:254	Another reason can be attributed to the failure of the chosen eigenvector to capture the sentiment dimension." ></td>
	<td class="line x" title="144:254	Recall that each eigenvector captures an important dimension of the data, and if the eigenvector that corresponds to the minimum normalized cut (i.e., the eigenvector that we chose) does not reveal the sentiment dimension, the resulting clustering (and hence the seed accuracy) will be poor." ></td>
	<td class="line x" title="145:254	However, even with imperfectly labeled seeds, we will show in the next section how we exploit these seeds to learn a better classifier." ></td>
	<td class="line x" title="146:254	3.2 Incorporating Active Learning Spectral clustering allows us to focus on a small number of dimensions that are relevant as far as creating well-separated clusters is concerned, but 3Additional experiments indicate that the accuracy of our approach is not sensitive to small changes to these values." ></td>
	<td class="line x" title="147:254	Dataset Iterative Single Step Movie 89.3 86.5 Kitchen 87.9 87.1 Electronics 80.4 77.6 Book 68.5 70.3 DVD 66.3 65.4 Table 1: Seed accuracies on five datasets." ></td>
	<td class="line x" title="148:254	they are not necessarily relevant for creating polarity clusters." ></td>
	<td class="line x" title="149:254	In fact, owing to the absence of labeled data, unsupervised clustering algorithms are unable to distinguish between useful and irrelevant features for polarity classification." ></td>
	<td class="line x" title="150:254	Nevertheless, being able to distinguish between relevant and irrelevant information is important for polarity classification, as discussed before." ></td>
	<td class="line x" title="151:254	Now that we have a small, high-quality seed set, we can potentially make better use of the available features by training a discriminative classifier on the seed set and having it identify the relevant and irrelevant features for polarity classification." ></td>
	<td class="line x" title="152:254	Despite the high quality of the seed set, the resulting classifier may not perform well when applied to the remaining (unlabeled) points, as there is no reason to believe that a classifier trained solely on unambiguous reviews can achieve a high accuracy when classifying ambiguous reviews." ></td>
	<td class="line x" title="153:254	We hypothesize that a high accuracy can be achieved only if the classifier is trained on both ambiguous and unambiguous reviews." ></td>
	<td class="line x" title="154:254	As a result, we apply active learning (Cohn et al., 1994) to identify the ambiguous reviews." ></td>
	<td class="line x" title="155:254	Specifically, we train a discriminative classifier using the support vector machine (SVM) learning algorithm (Joachims, 1999) on the set of unambiguous reviews, and then apply the resulting classifier to all the reviews in the training folds4 that are not seeds." ></td>
	<td class="line x" title="156:254	Since this classifier is trained solely on the unambiguous reviews, it is reasonable to assume that the reviews whose labels the classifier is most uncertain about (and therefore are most informative to the classifier) are those that are ambiguous." ></td>
	<td class="line x" title="157:254	Following previous work on active learning for SVMs (e.g., Campbell et al.(2000), Schohn and Cohn (2000), Tong and Koller (2002)), we define the uncertainty of a data point as its distance from the separating hyperplane." ></td>
	<td class="line x" title="159:254	In other words, 4Following Dredze and Crammer (2008), we perform cross-validation experiments on the 2000 labeled reviews in each evaluation dataset, choosing the active learning points from the training folds." ></td>
	<td class="line x" title="160:254	Note that the seeds obtained in the previous step were also acquired using the training folds only." ></td>
	<td class="line x" title="161:254	705 points that are closer to the hyperplane are more uncertain than those that are farther away." ></td>
	<td class="line x" title="162:254	We perform active learning for five iterations." ></td>
	<td class="line x" title="163:254	In each iteration, we select the 10 most uncertain points from each side of the hyperplane for human annotation, and then re-train a classifier on all of the points annotated so far." ></td>
	<td class="line x" title="164:254	This yields a total of 100 manually labeled reviews." ></td>
	<td class="line x" title="165:254	3.3 Applying Transductive Learning Given that we now have a labeled set (composed of 100 manually labeled points selected by active learning and 500 unambiguous points) as well as a larger set of points that are yet to be labeled (i.e., the remaining unlabeled points in the training folds and those in the test fold), we aim to train a better classifier by using a weakly supervised learner to learn from both the labeled and unlabeled data." ></td>
	<td class="line x" title="166:254	As our weakly supervised learner, we employ a transductive SVM." ></td>
	<td class="line x" title="167:254	To begin with, note that the automatically acquired 500 unambiguous data points are not perfectly labeled (see Section 3.1)." ></td>
	<td class="line x" title="168:254	Since these unambiguous points significantly outnumber the manually labeled points, they could undesirably dominate the acquisition of the hyperplane and diminish the benefits that we could have obtained from the more informative and perfectly labeled active learning points otherwise." ></td>
	<td class="line x" title="169:254	We desire a system that can use the active learning points effectively and at the same time is noise-tolerant to the imperfectly labeled unambiguous data points." ></td>
	<td class="line x" title="170:254	Hence, instead of training just one SVM classifier, we aim to reduce classification errors by training an ensemble of five classifiers, each of which uses all 100 manually labeled reviews and a different subset of the 500 automatically labeled reviews." ></td>
	<td class="line x" title="171:254	Specifically, we partition the 500 automatically labeled reviews into five equal-sized sets as follows." ></td>
	<td class="line x" title="172:254	First, we sort the 500 reviews in ascending order of their corresponding values in the eigenvector selected in the last iteration of our algorithm for removing ambiguous points (see Section 3.1)." ></td>
	<td class="line x" title="173:254	We then put point i into set Li mod 5." ></td>
	<td class="line x" title="174:254	This ensures that each set consists of not only an equal number of positive and negative points, but also a mix of very confidently labeled points and comparatively less confidently labeled points." ></td>
	<td class="line x" title="175:254	Each classifier Ci will then be trained transductively, using the 100 manually labeled points and the points in Li as labeled data, and the remaining points (including all points in Lj, where i negationslash= j) as unlabeled data." ></td>
	<td class="line x" title="176:254	After training the ensemble, we classify each unlabeled point as follows: we sum the (signed) confidence values assigned to it by the five ensemble classifiers, labeling it as POSITIVE if the sum is greater than zero (and NEGATIVE otherwise)." ></td>
	<td class="line x" title="177:254	Since the points in the test fold are included in the unlabeled data, they are all classified in this step." ></td>
	<td class="line pc" title="178:254	4 Evaluation 4.1 Experimental Setup For evaluation, we use five sentiment classification datasets, including the widely-used movie review dataset [MOV] (Pang et al., 2002) as well as four datasets that contain reviews of four different types of product from Amazon [books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT)] (Blitzer et al., 2007)." ></td>
	<td class="line o" title="179:254	Each dataset has 2000 labeled reviews (1000 positives and 1000 negatives)." ></td>
	<td class="line x" title="180:254	We divide the 2000 reviews into 10 equal-sized folds for cross-validation purposes, maintaining balanced class distributions in each fold." ></td>
	<td class="line x" title="181:254	It is important to note that while the test fold is accessible to the transductive learner (Step 3), only the reviews in training folds (but not their labels) are used for the acquisition of seeds (Step 1) and the selection of active learning points (Step 2)." ></td>
	<td class="line x" title="182:254	We report averaged 10-fold cross-validation results in terms of accuracy." ></td>
	<td class="line x" title="183:254	Following Kamvar et al.(2003), we also evaluate the clusters produced by our approach against the gold-standard clusters using Adjusted Rand Index (ARI)." ></td>
	<td class="line x" title="185:254	ARI ranges from 1 to 1; better clusterings have higher ARI values." ></td>
	<td class="line x" title="186:254	4.2 Baseline Systems Recall that our approach uses 100 hand-labeled reviews chosen by active learning." ></td>
	<td class="line x" title="187:254	To ensure a fair comparison, each of our three baselines has access to 100 labeled points chosen from the training folds." ></td>
	<td class="line x" title="188:254	Owing to the randomness involved in the choice of labeled data, all baseline results are averaged over ten independent runs for each fold." ></td>
	<td class="line x" title="189:254	Semi-supervised spectral clustering." ></td>
	<td class="line x" title="190:254	We implemented Kamvar et al.s (2003) semi-supervised spectral clustering algorithm, which incorporates labeled data into the clustering framework in the form of must-link and cannot-link constraints." ></td>
	<td class="line x" title="191:254	Instead of computing the similarity between each pair of points, the algorithm computes the similarity between a point and its k most similar points only." ></td>
	<td class="line x" title="192:254	Since its performance is highly sensitive to 706 Accuracy Adjusted Rand Index System Variation MOV KIT ELE BOO DVD MOV KIT ELE BOO DVD 1 Semi-supervised spectral learning 67.3 63.7 57.7 55.8 56.2 0.12 0.08 0.01 0.02 0.02 2 Transductive SVM 68.7 65.5 62.9 58.7 57.3 0.14 0.09 0.07 0.03 0.02 3 Active learning 68.9 68.1 63.3 58.6 58.0 0.14 0.14 0.08 0.03 0.03 4 Our approach (after 1st step) 69.8 70.8 65.7 58.6 55.8 0.15 0.17 0.10 0.03 0.01 5 Our approach (after 2nd step) 73.5 73.0 69.9 60.6 59.8 0.22 0.21 0.16 0.04 0.04 6 Our approach (after 3rd step) 76.2 74.1 70.6 62.1 62.7 0.27 0.23 0.17 0.06 0.06 Table 2: Results in terms of accuracy and Adjusted Rand Index for the five datasets." ></td>
	<td class="line x" title="193:254	k, we tested values of 10, 15, , 50 for k and reported in row 1 of Table 2 the best results." ></td>
	<td class="line x" title="194:254	As we can see, accuracy ranges from 56.2% to 67.3%, whereas ARI ranges from 0.02 to 0.12." ></td>
	<td class="line x" title="195:254	Transductive SVM." ></td>
	<td class="line x" title="196:254	We employ as our second baseline a transductive SVM5 trained using 100 points randomly sampled from the training folds as labeled data and the remaining 1900 points as unlabeled data." ></td>
	<td class="line x" title="197:254	Results of this baseline are shown in row 2 of Table 3." ></td>
	<td class="line x" title="198:254	As we can see, accuracy ranges from 57.3% to 68.7% and ARI ranges from 0.02 to 0.14, which are significantly better than those of semi-supervised spectral learning." ></td>
	<td class="line x" title="199:254	Active learning." ></td>
	<td class="line x" title="200:254	Our last baseline implements the active learning procedure as described in Tong and Koller (2002)." ></td>
	<td class="line x" title="201:254	Specifically, we begin by training an inductive SVM on one labeled example from each class, iteratively labeling the most uncertain unlabeled point on each side of the hyperplane and re-training the SVM until 100 points are labeled." ></td>
	<td class="line x" title="202:254	Finally, we train a transductive SVM on the 100 labeled points and the remaining 1900 unlabeled points, obtaining the results in row 3 of Table 1." ></td>
	<td class="line x" title="203:254	As we can see, accuracy ranges from 58% to 68.9%, whereas ARI ranges from 0.03 to 0.14." ></td>
	<td class="line x" title="204:254	Active learning is the best of the three baselines, presumably because it has the ability to choose the labeled data more intelligently than the other two." ></td>
	<td class="line x" title="205:254	4.3 Our Approach Results of our approach are shown in rows 46 of Table 2." ></td>
	<td class="line x" title="206:254	Specifically, rows 4 and 5 show the results of the SVM classifier when it is trained on the labeled data obtained after the first step (unsupervised extraction of unambiguous reviews) and the second step (active learning), respectively." ></td>
	<td class="line x" title="207:254	After the first step, our approach can already achieve 5All the SVM classifiers in this paper are trained using the SVMlight package (Joachims, 1999)." ></td>
	<td class="line x" title="208:254	All SVM-related learning parameters are set to their default values, except in transductive learning, where we set p (the fraction of unlabeled examples to be classified as positive) to 0.5 so that the system does not have any bias towards any class." ></td>
	<td class="line x" title="209:254	comparable results to the best baseline." ></td>
	<td class="line x" title="210:254	Performance increases substantially after the second step, indicating the benefits of active learning." ></td>
	<td class="line x" title="211:254	Row 6 shows the results of transductive learning with ensemble." ></td>
	<td class="line x" title="212:254	Comparing rows 5 and 6, we see that performance rises by 0.7%-2.9% for all five datasets after ensembled transduction." ></td>
	<td class="line x" title="213:254	This could be attributed to (1) the unlabeled data, which may have provided the transductive learner with useful information that are not accessible to the other learners, and (2) the ensemble, which is more noise-tolerant to the imperfect seeds." ></td>
	<td class="line x" title="214:254	4.4 Additional Experiments To gain insight into how the design decisions we made in our approach impact performance, we conducted the following additional experiments." ></td>
	<td class="line x" title="215:254	Importance of seeds." ></td>
	<td class="line x" title="216:254	Table 1 showed that for all but one dataset, the seeds obtained through multiple iterations are more accurate than those obtained in a single iteration." ></td>
	<td class="line x" title="217:254	To envisage the importance of seeds, we conducted an experiment where we repeated our approach using the seeds learned in a single iteration." ></td>
	<td class="line x" title="218:254	Results are shown in the first row of Table 3." ></td>
	<td class="line x" title="219:254	In comparison to row 6 of Table 2, we can see that results are indeed better when we bootstrap from higher-quality seeds." ></td>
	<td class="line x" title="220:254	To further understand the role of seeds, we experimented with a version of our approach that bootstraps from no seeds." ></td>
	<td class="line x" title="221:254	Specifically, we used the 500 seeds to guide the selection of active learning points, but trained a transductive SVM using only the active learning points as labeled data (and the rest as unlabeled data)." ></td>
	<td class="line x" title="222:254	As can be seen in row 2 of Table 3, the results are poor, suggesting that our approach yields better performance than the baselines not only because of the way the active learning points were chosen, but also because of contributions from the imperfectly labeled seeds." ></td>
	<td class="line x" title="223:254	We also experimented with training a transductive SVM using only the 100 least ambiguous seeds (i.e., the points with the largest unsigned 707 Accuracy Adjusted Rand Index System Variation MOV KIT ELE BOO DVD MOV KIT ELE BOO DVD 1 Single-step cluster purification 74.9 72.7 70.1 66.9 60.7 0.25 0.21 0.16 0.11 0.05 2 Using no seeds 58.3 55.6 59.7 54.0 56.1 0.04 0.04 0.02 0.01 0.01 3 Using the least ambiguous seeds 74.6 69.7 69.1 60.9 63.3 0.24 0.16 0.14 0.05 0.07 4 No Ensemble 74.1 72.7 68.8 61.5 59.9 0.23 0.21 0.14 0.05 0.04 5 Passive learning 74.1 72.4 68.0 63.7 58.6 0.23 0.20 0.13 0.07 0.03 6 Using 500 active learning points 82.5 78.4 77.5 73.5 73.4 0.42 0.32 0.30 0.22 0.22 7 Fully supervised results 86.1 81.7 79.3 77.6 80.6 0.53 0.41 0.34 0.30 0.38 Table 3: Additional results in terms of accuracy and Adjusted Rand Index for the five datasets." ></td>
	<td class="line x" title="224:254	second eigenvector values) in combination with the active learning points as labeled data (and the rest as unlabeled data)." ></td>
	<td class="line x" title="225:254	Note that the accuracy of these 100 least ambiguous seeds is 45% higher than that of the 500 least ambiguous seeds shown in Table 1." ></td>
	<td class="line x" title="226:254	Results are shown in row 3 of Table 3." ></td>
	<td class="line x" title="227:254	As we can see, using only 100 seeds turns out to be less beneficial than using all of them via an ensemble." ></td>
	<td class="line x" title="228:254	One reason is that since these 100 seeds are the most unambiguous, they may also be the least informative as far as learning is concerned." ></td>
	<td class="line x" title="229:254	Remember that SVM uses only the support vectors to acquire the hyperplane, and since an unambiguous seed is likely to be far away from the hyperplane, it is less likely to be a support vector." ></td>
	<td class="line x" title="230:254	Role of ensemble learning To get a better idea of the role of the ensemble in the transductive learning step, we used all 500 seeds in combination with the 100 active learning points to train a single transductive SVM." ></td>
	<td class="line x" title="231:254	Results of this experiment (shown in row 4 of Table 3) are worse than those in row 6 of Table 2, meaning that the ensemble has contributed positively to performance." ></td>
	<td class="line x" title="232:254	This should not be surprising: as noted before, since the seeds are not perfectly labeled, using all of them without an ensemble might overwhelm the more informative active learning points." ></td>
	<td class="line x" title="233:254	Passive learning." ></td>
	<td class="line x" title="234:254	To better understand the role of active learning in our approach, we replaced it with passive learning, where we randomly picked 100 data points from the training folds and used them as labeled data." ></td>
	<td class="line x" title="235:254	Results, shown in row 5 of Table 3, are averaged over ten independent runs for each fold." ></td>
	<td class="line x" title="236:254	In comparison to row 6 of Table 2, we see that employing points chosen by an active learner yields significantly better results than employing randomly chosen points, which suggests that the way the points are chosen is important." ></td>
	<td class="line x" title="237:254	Using more active learning points." ></td>
	<td class="line x" title="238:254	An interesting question is: how much improvement can we obtain if we employ more active learning points?" ></td>
	<td class="line x" title="239:254	In row 6 of Table 3, we show the results when the experiment in row 6 of Table 2 was repeated using 500 active learning points." ></td>
	<td class="line x" title="240:254	Perhaps not surprisingly, the 400 additional labeled points yield a 4 11% increase in accuracy." ></td>
	<td class="line x" title="241:254	For further comparison, we trained a fully supervised SVM classifier using all of the training data." ></td>
	<td class="line x" title="242:254	Results are shown in row 7 of Table 3." ></td>
	<td class="line x" title="243:254	As we can see, employing only 500 active learning points enables us to almost reach fully-supervised performance for three datasets." ></td>
	<td class="line x" title="244:254	5 Conclusions We have proposed a novel semi-supervised approach to polarity classification." ></td>
	<td class="line x" title="245:254	Our key idea is to distinguish between unambiguous, easy-tomine reviews and ambiguous, hard-to-classify reviews." ></td>
	<td class="line x" title="246:254	Specifically, given a set of reviews, we applied (1) an unsupervised algorithm to identify and classify those that are unambiguous, (2) an active learner that is trained solely on automatically labeled unambiguous reviews to identify a small number of prototypical ambiguous reviews for manual labeling, and (3) an ensembled transductive learner to train a sophisticated classifier on the reviews labeled so far to handle the ambiguous reviews." ></td>
	<td class="line x" title="247:254	Experimental results on five sentiment datasets demonstrate that our mine the easy, classify the hard approach, which only requires manual labeling of a small number of ambiguous reviews, can be employed to train a highperformance polarity classification system." ></td>
	<td class="line x" title="248:254	We plan to extend our approach by exploring two of its appealing features." ></td>
	<td class="line x" title="249:254	First, none of the steps in our approach is designed specifically for sentiment classification." ></td>
	<td class="line x" title="250:254	This makes it applicable to other text classification tasks." ></td>
	<td class="line x" title="251:254	Second, our approach is easily extensible." ></td>
	<td class="line x" title="252:254	Since the semisupervised learner is discriminative, our approach can adopt a richer representation that makes use of more sophisticated features such as bigrams or manually labeled sentiment-oriented words." ></td>
	<td class="line x" title="253:254	708 Acknowledgments We thank the three anonymous reviewers for their invaluable comments on an earlier draft of the paper." ></td>
	<td class="line x" title="254:254	This work was supported in part by NSF Grant IIS-0812261." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1095
Semi-Supervised Cause Identification from Aviation Safety Reports
Persing, Isaac;Ng, Vincent;"></td>
	<td class="line x" title="1:221	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 843851, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:221	c2009 ACL and AFNLP Semi-Supervised Cause Identification from Aviation Safety Reports Isaac Persing and Vincent Ng Human Language Technology Research Institute University of Texas at Dallas Richardson, TX 75083-0688 {persingq,vince}@hlt.utdallas.edu Abstract We introduce cause identification, a new problem involving classification of incident reports in the aviation domain." ></td>
	<td class="line x" title="3:221	Specifically, given a set of pre-defined causes, a cause identification system seeks to identify all and only those causes that can explain why the aviation incident described in a given report occurred." ></td>
	<td class="line x" title="4:221	The difficulty of cause identification stems in part from the fact that it is a multi-class, multilabel categorization task, and in part from the skewness of the class distributions and the scarcity of annotated reports." ></td>
	<td class="line x" title="5:221	To improve the performance of a cause identification system for the minority classes, we present a bootstrapping algorithm that automatically augments a training set by learning from a small amount of labeled data and a large amount of unlabeled data." ></td>
	<td class="line x" title="6:221	Experimental results show that our algorithm yields a relative error reduction of 6.3% in F-measure for the minority classes in comparison to a baseline that learns solely from the labeled data." ></td>
	<td class="line x" title="7:221	1 Introduction Automatic text classification is one of the most important applications in natural language processing (NLP)." ></td>
	<td class="line x" title="8:221	The difficulty of a text classification task depends on various factors, but typically, the task can be difficult if (1) the amount of labeled data available for learning the task is small; (2) it involves multiple classes; (3) it involves multilabel categorization, where more than one label can be assigned to each document; (4) the class distributions are skewed, with some categories significantly outnumbering the others; and (5) the documents belong to the same domain (e.g., movie review classification)." ></td>
	<td class="line x" title="9:221	In particular, when the documents to be classified are from the same domain, they tend to be more similar to each other with respect to word usage, thus making the classes less easily separable." ></td>
	<td class="line x" title="10:221	This is one of the reasons why topic-based classification, even with multiple classes as in the 20 Newsgroups dataset1, tends to be easier than review classification, where reviews from the same domain are to be classified according to the sentiment expressed2." ></td>
	<td class="line x" title="11:221	In this paper, we introduce a new text classification problem involving the Aviation Safety Reporting System (ASRS) that can be viewed as a difficult task along each of the five dimensions discussed above." ></td>
	<td class="line x" title="12:221	Established in 1967, ASRS collects voluntarily submitted reports about aviation safety incidents written by flight crews, attendants, controllers, and other related parties." ></td>
	<td class="line x" title="13:221	These incident reports are made publicly available to researchers for automatic analysis, with the ultimate goal of improving the aviation safety situation." ></td>
	<td class="line x" title="14:221	One central task in the automatic analysis of these reports is cause identification, or the identification of why an incident happened." ></td>
	<td class="line x" title="15:221	Aviation safety experts at NASA have identified 14 causes (or shaping factors in NASA terminology) that could explain why an incident occurred." ></td>
	<td class="line x" title="16:221	Hence, cause identification can be naturally recast as a text classification task: given an incident report, determine which of a set of 14 shapers contributed to the occurrence of the incident described in the report." ></td>
	<td class="line x" title="17:221	As mentioned above, cause identification is considered challenging along each of the five aforementioned dimensions." ></td>
	<td class="line x" title="18:221	First, there is a scarcity of incident reports labeled with the shapers." ></td>
	<td class="line x" title="19:221	This can be attributed to the fact that there has been very little work on this task." ></td>
	<td class="line pc" title="20:221	While the NASA researchers have applied a heuristic method for labeling a report with shapers (Posse 1http://kdd.ics.uci.edu/databases/20newsgroups/ 2Of course, the fact that sentiment classification requires a deeper understanding of a text also makes it more difficult than topic-based text classification (Pang et al., 2002)." ></td>
	<td class="line x" title="21:221	843 et al., 2005), the method was evaluated on only 20 manually labeled reports, which are not made publicly available." ></td>
	<td class="line x" title="22:221	Second, the fact that this is a 14-class classification problem makes it more challenging than a binary classification problem." ></td>
	<td class="line x" title="23:221	Third, a report can be labeled with more than one category, as several shapers can contribute to the occurrence of an aviation incident." ></td>
	<td class="line x" title="24:221	Fourth, the class distribution is very skewed: based on an analysis of our 1,333 annotated reports, 10 of the 14 categories can be considered minority classes, which account for only 26% of the total number of labels associated with the reports." ></td>
	<td class="line x" title="25:221	Finally, our cause identification task is domain-specific, involving the classification of documents that all belong to the aviation domain." ></td>
	<td class="line x" title="26:221	This paper focuses on improving the accuracy of minority class prediction for cause identification." ></td>
	<td class="line x" title="27:221	Not surprisingly, when trained on a dataset with a skewed class distribution, most supervised machine learning algorithms will exhibit good performance on the majority classes, but relatively poor performance on the minority classes." ></td>
	<td class="line x" title="28:221	Unfortunately, achieving good accuracies on the minority classes is very important in our task of identifying shapers from aviation safety reports, where 10 out of the 14 shapers are minority classes, as mentioned above." ></td>
	<td class="line x" title="29:221	Minority class prediction has been tackled extensively in the machine learning literature, using methods that typically involve sampling and re-weighting of training instances, with the goal of creating a less skewed class distribution (e.g., Pazzani et al.(1994), Fawcett (1996), Kubat and Matwin (1997))." ></td>
	<td class="line x" title="31:221	Such methods, however, are unlikely to perform equally well for our cause identification task given our small labeled set, as the minority class prediction problem is complicated by the scarcity of labeled data." ></td>
	<td class="line x" title="32:221	More specifically, given the scarcity of labeled data, many words that are potentially correlated with a shaper (especially a minority shaper) may not appear in the training set, and the lack of such useful indicators could hamper the acquisition of an accurate classifier via supervised learning techniques." ></td>
	<td class="line x" title="33:221	We propose to address the problem of minority class prediction in the presence of a small training set by means of a bootstrapping approach, where we introduce an iterative algorithm to (1) use a small set of labeled reports and a large set of unlabeled reports to automatically identify words that are most relevant to the minority shaper under consideration, and (2) augment the labeled data by using the resulting words to annotate those unlabeled reports that can be confidently labeled." ></td>
	<td class="line x" title="34:221	We evaluate our approach using cross-validation on 1,333 manually annotated reports." ></td>
	<td class="line x" title="35:221	In comparison to a supervised baseline approach where a classifier is acquired solely based on the training set, our bootstrapping approach yields a relative error reduction of 6.3% in F-measure for the minority classes." ></td>
	<td class="line x" title="36:221	In sum, the contributions of our work are threefold." ></td>
	<td class="line x" title="37:221	First, we introduce a new, challenging text classification problem, cause identification from aviation safety reports, to the NLP community." ></td>
	<td class="line x" title="38:221	Second, we created an annotated dataset for cause identification that is made publicly available for stimulating further research on this problem3." ></td>
	<td class="line x" title="39:221	Third, we introduce a bootstrapping algorithm for improving the prediction of minority classes in the presence of a small training set." ></td>
	<td class="line x" title="40:221	The rest of the paper is organized as follows." ></td>
	<td class="line x" title="41:221	In Section 2, we present the 14 shapers." ></td>
	<td class="line x" title="42:221	Section 3 explains how we preprocess and annotate the reports." ></td>
	<td class="line x" title="43:221	Sections 4 and 5 describe the baseline approaches and our bootstrapping algorithm, respectively." ></td>
	<td class="line x" title="44:221	We present results in Section 6, discuss related work in Section 7, and conclude in Section 8." ></td>
	<td class="line x" title="45:221	2 Shaping Factors As mentioned in the introduction, the task of cause identification involves labeling an incident report with all the shaping factors that contributed to the occurrence of the incident." ></td>
	<td class="line x" title="46:221	Table 1 lists the 14 shaping factors, as well as a description of each shaper taken verbatim from Posse et al.(2005)." ></td>
	<td class="line x" title="48:221	As we can see, the 14 classes are not mutually exclusive." ></td>
	<td class="line x" title="49:221	For instance, a lack of familiarity with equipment often implies a deficit in proficiency in its use, so the two shapers frequently co-occur." ></td>
	<td class="line x" title="50:221	In addition, while some classes cover a specific and well-defined set of issues (e.g., Illusion), some encompass a relatively large range of situations." ></td>
	<td class="line x" title="51:221	For instance, resource deficiency can include problems with equipment, charts, or even aviation personnel." ></td>
	<td class="line x" title="52:221	Furthermore, ten shaping factors can be considered minority classes, as each of them account for less than 10% of the labels." ></td>
	<td class="line x" title="53:221	Accurately predicting minority classes is important in this domain because, for example, the physical factors minority shaper is frequently associated with incidents involving near-misses between aircraft." ></td>
	<td class="line x" title="54:221	3http://www.hlt.utdallas.edu/persingq/ASRSdataset.html 844 Id Shaping Factor Description % 1 Attitude Any indication of unprofessional or antagonistic attitude by a controller or flight crew member, e.g., complacency or get-homeitis (in a hurry to get home)." ></td>
	<td class="line x" title="55:221	2.4 2 Communication Environment Interferences with communications in the cockpit such as noise, auditory interference, radio frequency congestion, or language barrier." ></td>
	<td class="line x" title="56:221	5.5 3 Duty Cycle A strong indication of an unusual working period, e.g., a long day, flying very late at night, exceeding duty time regulations, having short and inadequate rest periods." ></td>
	<td class="line x" title="57:221	1.8 4 Familiarity A lack of factual knowledge, such as new to or unfamiliar with company, airport, or aircraft." ></td>
	<td class="line x" title="58:221	3.2 5 Illusion Bright lights that cause something to blend in, black hole, white out, sloping terrain, etc. 0.1 6 Other Anything else that could be a shaper, such as shift change, passenger discomfort, or disorientation." ></td>
	<td class="line x" title="59:221	13.3 7 Physical Environment Unusual physical conditions that could impair flying or make things difficult." ></td>
	<td class="line x" title="60:221	16.0 8 Physical Factors Pilot ailment that could impair flying or make things more difficult, such as being tired, drugged, incapacitated, suffering from vertigo, illness, dizziness, hypoxia, nausea, loss of sight or hearing." ></td>
	<td class="line x" title="61:221	2.2 9 Preoccupation A preoccupation, distraction, or division of attention that creates a deficit in performance, such as being preoccupied, busy (doing something else), or distracted." ></td>
	<td class="line x" title="62:221	6.7 10 Pressure Psychological pressure, such as feeling intimidated, pressured, or being low on fuel." ></td>
	<td class="line x" title="63:221	1.8 11 Proficiency A general deficit in capabilities, such as inexperience, lack of training, not qualified, or not current." ></td>
	<td class="line x" title="64:221	14.4 12 Resource Deficiency Absence, insufficient number, or poor quality of a resource, such as overworked or unavailable controller, insufficient or out-of-date chart, malfunctioning or inoperative or missing equipment." ></td>
	<td class="line x" title="65:221	30.0 13 Taskload Indicators of a heavy workload or many tasks at once, such as short-handed crew." ></td>
	<td class="line x" title="66:221	1.9 14 Unexpected Something sudden and surprising that is not expected." ></td>
	<td class="line x" title="67:221	0.6 Table 1: Descriptions of shaping factor classes." ></td>
	<td class="line x" title="68:221	The % column shows the percent of labels the shapers account for." ></td>
	<td class="line x" title="69:221	3 Dataset We downloaded our corpus from the ASRS website4." ></td>
	<td class="line x" title="70:221	The corpus consists of 140,599 incident reports collected during the period from January 1998 to December 2007." ></td>
	<td class="line x" title="71:221	Each report is a free text narrative that describes not only why an incident happened, but also what happened, where it happened, how the reporter felt about the incident, the reporters opinions of other people involved in the incident, and any other comments the reporter cared to include." ></td>
	<td class="line x" title="72:221	In other words, a lot of information in the report is irrelevant to (and thus complicates) the task of cause identification." ></td>
	<td class="line x" title="73:221	3.1 Preprocessing Unlike newswire articles, at which many topicbased text classification tasks are targeted, the ASRS reports are informally written using various domain-specific abbreviations and acronyms, tend to contain poor grammar, and have capitalization information removed, as illustrated in the following sentence taken from one of the reports." ></td>
	<td class="line x" title="74:221	HAD BEEN CLRED FOR APCH BY ZOA AND HAD BEEN HANDED OFF TO SANTA ROSA TWR." ></td>
	<td class="line x" title="75:221	4http://asrs.arc.nasa.gov/ This sentence is grammatically incorrect (due to the lack of a subject), and contains abbreviations such as CLRED, APCH, and TWR." ></td>
	<td class="line x" title="76:221	This makes it difficult for a non-aviation expert to understand." ></td>
	<td class="line x" title="77:221	To improve readability (and hence facilitate the annotation process), we preprocess each report as follows." ></td>
	<td class="line x" title="78:221	First, we expand the abbreviations/acronyms with the help of an official list of acronyms/abbreviations and their expanded forms5." ></td>
	<td class="line x" title="79:221	Second, though not as crucial as the first step, we heuristically restore the case of the words by relying on an English lexicon: if a word appears in the lexicon, we assume that it is not a proper name, and therefore convert it into lowercase." ></td>
	<td class="line x" title="80:221	After preprocessing, the example sentence appears as had been cleared for approach by ZOA and had been handed off to santa rosa tower." ></td>
	<td class="line x" title="81:221	Finally, to facilitate automatic analysis, we stem each word in the narratives." ></td>
	<td class="line x" title="82:221	3.2 Human Annotation Next, we randomly picked 1,333 preprocessed reports and had two graduate students not affiliated 5See http://akama.arc.nasa.gov/ASRSDBOnline/pdf/ ASRS Decode.pdf." ></td>
	<td class="line x" title="83:221	In the very infrequently-occurring case where the same abbreviation or acronym may have more than expansion, we arbitrarily chose one of the possibilities." ></td>
	<td class="line x" title="84:221	845 Id Total (%) F1 F2 F3 F4 F5 1 52 (3.9) 11 7 7 17 10 2 119 (8.9) 29 29 22 16 23 3 38 (2.9) 10 5 6 9 8 4 70 (5.3) 11 12 9 14 24 5 3 (0.2) 0 0 0 1 2 6 289 (21.7) 76 44 60 42 67 7 348 (26.1) 73 63 82 59 71 8 48 (3.6) 11 14 8 11 4 9 145 (10.9) 29 25 38 28 25 10 38 (2.9) 12 10 4 7 5 11 313 (23.5) 65 50 74 46 78 12 652 (48.9) 149 144 125 123 111 13 42 (3.2) 7 8 8 6 13 14 14 (1.1) 3 3 3 3 2 Table 2: Number of occurrences of each shaping factor in the dataset." ></td>
	<td class="line x" title="85:221	The Total column shows the number of narratives labeled with each shaper and the percentage of narratives tagged with each shaper in the 1,333 labeled narrative set." ></td>
	<td class="line x" title="86:221	The F columns show the number narratives associated with each shaper in folds F1  F5." ></td>
	<td class="line x" title="87:221	x (# Shapers) 1 2 3 4 5 6 Percentage 53.6 33.2 10.3 2.7 0.2 0.1 Table 3: Percentage of documents with x labels." ></td>
	<td class="line x" title="88:221	with this research independently annotate them with shaping factors, based solely on the definitions presented in Table 1." ></td>
	<td class="line x" title="89:221	To measure interannotator agreement, we compute Cohens Kappa (Carletta, 1996) from the two sets of annotations, obtaining a Kappa value of only 0.43." ></td>
	<td class="line x" title="90:221	This not only suggests the difficulty of the cause identification task, but also reveals the vagueness inherent in the definition of the 14 shapers." ></td>
	<td class="line x" title="91:221	As a result, we had the two annotators re-examine each report for which there was a disagreement and reach an agreement on its final set of labels." ></td>
	<td class="line x" title="92:221	Statistics of the annotated dataset can be found in Table 2, where the Total column shows the size of each of the 14 classes, expressed both as the number of reports that are labeled with a particular shaper and as a percent (in parenthesis)." ></td>
	<td class="line x" title="93:221	Since we will perform 5-fold cross validation in our experiments, we also show the number of reports labeled with each shaper under the F columns for each fold." ></td>
	<td class="line x" title="94:221	To get a better idea of how many reports have multiple labels, we categorize the reports according to the number of labels they contain in Table 3." ></td>
	<td class="line x" title="95:221	4 Baseline Approaches In this section, we describe two baseline approaches to cause identification." ></td>
	<td class="line x" title="96:221	Since our ultimate goal is to evaluate the effectiveness of our bootstrapping algorithm, the baseline approaches only make use of small amounts of labeled data for acquiring classifiers." ></td>
	<td class="line x" title="97:221	More specifically, both baselines recast the cause identification problem as a set of 14 binary classification problems, one for predicting each shaper." ></td>
	<td class="line x" title="98:221	In the binary classification problem for predicting shaper si, we create one training instance from each document in the training set, labeling the instance as positive if the document has si as one of its labels, and negative otherwise." ></td>
	<td class="line x" title="99:221	After creating training instances, we train a binary classifier, ci, for predicting si, employing as features the top 50 unigrams that are selected according to information gain computed over the training data (see Yang and Pedersen (1997))." ></td>
	<td class="line x" title="100:221	The SVM learning algorithm as implemented in the LIBSVM software package (Chang and Lin, 2001) is used for classifier training, owing to its robust performance on many text classification tasks." ></td>
	<td class="line x" title="101:221	In our first baseline, we set all the learning parameters to their default values." ></td>
	<td class="line x" title="102:221	As noted before, we divide the 1,333 annotated reports into five folds of roughly equal size, training the classifiers on four folds and applying them separately to the remaining fold." ></td>
	<td class="line x" title="103:221	Results are reported in terms of precision (P), recall (R), and F-measure (F), which are computed by aggregating over the 14 shapers as follows." ></td>
	<td class="line x" title="104:221	Let tpi be the number of test reports correctly labeled as positive by ci; pi be the total number of test reports labeled as positive by ci; and ni be the total number of test reports that belong to si according to the gold standard." ></td>
	<td class="line x" title="105:221	Then, P = summationtext i tpisummationtext i pi ,R = summationtext i tpisummationtext i ni ,and F = 2PRP +R. Our second baseline is similar to the first, except that we tune the classification threshold (CT) to optimize F-measure." ></td>
	<td class="line x" title="106:221	More specifically, recall that LIBSVM trains a classifier that by default employs a CT of 0.5, thus classifying an instance as positive if and only if the probability that it belongs to the positive class is at least 0.5." ></td>
	<td class="line x" title="107:221	However, this may not be the optimal threshold to use as far as performance is concerned, especially for the minority classes, where the class distribution is skewed." ></td>
	<td class="line x" title="108:221	This is the motivation behind tuning the CT of each classifier." ></td>
	<td class="line x" title="109:221	To ensure a fair comparison with the first baseline, we do not employ additional labeled data for parameter tuning; rather, we reserve 25% of the available training data for tuning, and use the remaining 75% for classifier 846 acquisition." ></td>
	<td class="line x" title="110:221	This amounts to using three folds for training and one fold for development in each cross validation experiment." ></td>
	<td class="line x" title="111:221	Using the development data, we tune the 14 CTs jointly to optimize overall F-measure." ></td>
	<td class="line x" title="112:221	However, an exact solution to this optimization problem is computationally expensive." ></td>
	<td class="line x" title="113:221	Consequently, we find a local maximum by employing a local search algorithm, which alters one parameter at a time to optimize F-measure by holding the remaining parameters fixed." ></td>
	<td class="line x" title="114:221	5 Our Bootstrapping Algorithm One of the potential weaknesses of the two baselines described in the previous section is that the classifiers are trained on only a small amount of labeled data." ></td>
	<td class="line x" title="115:221	This could have an adverse effect on the accuracy of the resulting classifiers, especially those for the minority classes." ></td>
	<td class="line x" title="116:221	The situation is somewhat aggravated by the fact that we are adopting a one-versus-all scheme for generating training instances for a particular shaper, which, together with the small amount of labeled data, implies that only a couple of positive instances may be available for training the classifier for a minority class." ></td>
	<td class="line x" title="117:221	To alleviate the data scarcity problem and improve the accuracy of the classifiers, we propose in this section a bootstrapping algorithm that automatically augments a training set by exploiting a large amount of unlabeled data." ></td>
	<td class="line x" title="118:221	The basic idea behind the algorithm is to iteratively identify words that are high-quality indicators of the positive or negative examples, and then automatically label unlabeled documents that contain a sufficient number of such indicators." ></td>
	<td class="line x" title="119:221	Our bootstrapping algorithm, shown in Figure 1, aims to augment the set of positive and negative training instances for a given shaper." ></td>
	<td class="line x" title="120:221	The main function, Train, takes as input four arguments." ></td>
	<td class="line x" title="121:221	The first two arguments, P and N, are the positive and negative instances, respectively, generated by the one-versus-one scheme from the initial training set, as described in the previous section." ></td>
	<td class="line x" title="122:221	The third argument, U, is the unlabeled set of documents, which consists of all but the documents in the training set." ></td>
	<td class="line x" title="123:221	In particular, U contains the documents in the development and test sets." ></td>
	<td class="line x" title="124:221	Hence, we are essentially assuming access to the test documents (but not their labels) during the training process, as in a transductive learning setting." ></td>
	<td class="line x" title="125:221	The last argument, k, is the number of bootstrapping iterations." ></td>
	<td class="line x" title="126:221	In addition, the algoTrain(P,N,U,k) Inputs: P: positively labeled training examples of shaper x N: negatively labeled training examples of shaper x U: set of unlabeled narratives in corpus k: number of bootstrapping iterations PW  NW  for i = 0 to k1 do if|P|>|N|then [P,PW]ExpandTrainingSet(P,N,U,PW) else [N,NW]ExpandTrainingSet(N,P,U,NW) end if end for ExpandTrainingSet(A,B,U,W) Inputs: A,B,U: narrative sets W: unigram feature set for j = 1 to 4 do targ maxt/W parenleftBig log( C(t,A)C(t,B)+1) parenrightBig // C(t,X): number of narratives in X containing t W W{t} end for return [AS(W,U),W] // S(W,U): narratives in U containing3 words in W Figure 1: Our bootstrapping algorithm." ></td>
	<td class="line x" title="127:221	rithm uses two variables, PW and NW, to store the sets of high-quality indicators for the positive instances and the negative instances, respectively, that are found during the bootstrapping process." ></td>
	<td class="line x" title="128:221	Next, we begin our k bootstrapping iterations." ></td>
	<td class="line x" title="129:221	In each iteration, we expand either P or N, depending on their relative sizes." ></td>
	<td class="line x" title="130:221	In order to keep the two sets as close in size as possible, we choose to expand the smaller of the two sets.6 After that, we execute the function ExpandTrainingSet to expand the selected set." ></td>
	<td class="line x" title="131:221	Without loss of generality, assume that P is chosen for expansion." ></td>
	<td class="line x" title="132:221	To do this, ExpandTrainingSet selects four words that seem much more likely to appear in P than in N from the set of candidate words7." ></td>
	<td class="line x" title="133:221	To select these words, we calculate the log likelihood ratio log( C(t,P)C(t,N)+1) for each candidate word t, where C(t,P) is the number of narratives in P that contain t, and C(t,N) similarly is the number of narratives in N that contain t. If this ratio is large, 6It may seem from the way P and N are constructed that N is almost always larger than P and therefore is unlikely to be selected for expansion." ></td>
	<td class="line x" title="134:221	However, the ample size of the unlabeled set means that the algorithm still adds large numbers of narratives to the training data." ></td>
	<td class="line x" title="135:221	Hence, even for minority classes, P often grows larger than N by iteration 3." ></td>
	<td class="line x" title="136:221	7A candidate word is a word that appears in the training set (PN) at least four times." ></td>
	<td class="line x" title="137:221	847 we posit that t is a good indicator of P. Note that incrementing the count in the denominator by one has a smoothing effect: it avoids selecting words that appears infrequently in P and not at all in N. There is a reason for selecting multiple words (rather than just one word) in each bootstrapping iteration: we want to prevent the algorithm from selecting words that are too specific to one subcategory of a shaping factor." ></td>
	<td class="line x" title="138:221	For example, shaping factor 7 (Physical Environment) is composed largely of incidents influenced by weather phenomena." ></td>
	<td class="line x" title="139:221	In one experiment, we tried selecting only one word per bootstrapping iteration." ></td>
	<td class="line x" title="140:221	For shaper 7, the first word added to PW was snow." ></td>
	<td class="line x" title="141:221	Upon the next iteration, the algorithm added plow to PW." ></td>
	<td class="line x" title="142:221	While plow may itself be indicative of shaper 7, we believe its selection was due to the recent addition to P of a large number of narratives containing snow." ></td>
	<td class="line x" title="143:221	Hence, by selecting four words per iteration, we are forcing the algorithm to branch out among these subcategories." ></td>
	<td class="line x" title="144:221	After adding the selected words to PW, we augment P with all the unlabeled documents containing at least three words from PW." ></td>
	<td class="line x" title="145:221	The reason we impose the at least three requirement is precision: we want to ensure, with a reasonable level of confidence, that the unlabeled documents chosen to augment P should indeed be labeled with the shaper under consideration, as incorrectly labeled documents would contaminate the labeled data, thus accelerating the deterioration of the quality of the automatically labeled data in subsequent bootstrapping iterations and adversely affecting the accuracy of the classifier trained on it (Pierce and Cardie, 2001)." ></td>
	<td class="line x" title="146:221	The above procedure is repeated in each bootstrapping iteration." ></td>
	<td class="line x" title="147:221	As mentioned above, if N is smaller in size than P, we will expand N instead, adding to NW the four words that are the strongest indicators of a narrative being a negative example of the shaper under consideration, and augmenting N with those unlabeled narratives that contain at least three words from NW." ></td>
	<td class="line x" title="148:221	The number of bootstrapping iterations is controlled by the input parameter k. As we will see in the next section, we run the bootstrapping algorithm for up to five iterations only, as the quality of the bootstrapped data deteriorates fairly rapidly." ></td>
	<td class="line x" title="149:221	The exact value of k will be determined automatically using development data, as discussed below." ></td>
	<td class="line x" title="150:221	After bootstrapping, the augmented training data can be used in combination with any of the two baseline approaches to acquire a classifier for identifying a particular shaper." ></td>
	<td class="line x" title="151:221	Whichever baseline is used, we need to reserve one of the five folds to tune the parameter k in our cross validation experiments." ></td>
	<td class="line x" title="152:221	In particular, if the second baseline is used, we will tune CT and k jointly on the development data using the local search algorithm described previously, where we adjust the values of both CT and k for one of the 14 classifiers in each step of the search process to optimize the overall F-measure score." ></td>
	<td class="line x" title="153:221	6 Evaluation 6.1 Baseline Systems Since our evaluation centers on the question of how effective our bootstrapping algorithm is in exploiting unlabeled documents to improve classifier performance, our two baselines only employ the available labeled documents to train the classifiers." ></td>
	<td class="line x" title="154:221	Recall that our first baseline, which we call B0.5 (due to its being a baseline with a CT of 0.5), employs default values for all of the learning parameters." ></td>
	<td class="line x" title="155:221	Micro-averaged 5-fold cross validation results of this baseline for all 14 shapers and for just 10 minority classes (due to our focus on improving minority class prediction) are expressed as percentages in terms of precision (P), recall (R), and F-measure (F) in the first row of Table 4." ></td>
	<td class="line x" title="156:221	As we can see, the baseline achieves an F-measure of 45.4 (14 shapers) and 35.4 (10 shapers)." ></td>
	<td class="line x" title="157:221	Comparing these two results, the higher F-measure achieved using all 14 shapers can be attributed primarily to improvements in recall." ></td>
	<td class="line x" title="158:221	This should not be surprising: as mentioned above, the number of positive instances of a minority class may be small, thus causing the resulting classifier to be biased towards classifying a document as negative." ></td>
	<td class="line x" title="159:221	Instead of employing a CT value of 0.5, our second baseline, Bct, tunes CT using one of the training folds and simply trains a classifier on the remaining three folds." ></td>
	<td class="line x" title="160:221	For parameter tuning, we tested CTs of 0.0, 0.05, , 1.0." ></td>
	<td class="line x" title="161:221	Results of this baseline are shown in row 2 of Table 4." ></td>
	<td class="line x" title="162:221	In comparison to the first baseline, we see that F-measure improves considerably by 7.4% and 4.5% for 14 shapers and 10 shapers respectively8, which illus8It is important to note that the parameters are optimized separately for each pair of 14-shaper and 10-shaper experiments in this paper, and that the 10-shaper results are not 848 All 14 Classes 10 Minority Classes System P R F P R F B0.5 67.0 34.4 45.4 68.3 23.9 35.4 Bct 47.4 59.2 52.7 47.8 34.3 39.9 E0.5 60.9 40.4 48.6 53.2 35.3 42.4 Ect 50.5 54.9 52.6 49.1 39.4 43.7 Table 4: 5-fold cross validation results." ></td>
	<td class="line x" title="163:221	trates the importance of employing the right CT for the cause identification task." ></td>
	<td class="line x" title="164:221	6.2 Our Approach Next, we evaluate the effectiveness of our bootstrapping algorithm in improving classifier performance." ></td>
	<td class="line x" title="165:221	More specifically, we apply the two baselines separately to the augmented training set produced by our bootstrapping algorithm." ></td>
	<td class="line x" title="166:221	When combining our bootstrapping algorithm with the first baseline, we produce a system that we call E0.5 (due to its being trained on the expanded training set with a CT of 0.5)." ></td>
	<td class="line x" title="167:221	E0.5 has only one tunable parameter, k (i.e., the number of bootstrapping iterations), whose allowable values are 0, 1, , 5." ></td>
	<td class="line x" title="168:221	When our algorithm is used in combination with the second baseline, we produce another system, Ect, which has both k and the CT as its parameters." ></td>
	<td class="line x" title="169:221	The allowable values of these parameters, which are to be tuned jointly, are the same as those employed by Bct and E0.5." ></td>
	<td class="line x" title="170:221	Results of E0.5 are shown in row 3 of Table 4." ></td>
	<td class="line x" title="171:221	In comparison to B0.5, we see that F-measure increases by 3.2% and 7.0% for 14 shapers and 10 shapers, respectively." ></td>
	<td class="line x" title="172:221	Such increases can be attributed to less imbalanced recall and precision values, as a result of a large gain in recall accompanied by a roughly equal drop in precision." ></td>
	<td class="line x" title="173:221	These results are consistent with our intuition: recall can be improved with a larger training set, but precision can be hampered when learning from noisily labeled data." ></td>
	<td class="line x" title="174:221	Overall, these results suggest that learning from the augmented training set is useful, especially for the minority classes." ></td>
	<td class="line x" title="175:221	Results of Ect are shown in row 4 of Table 4." ></td>
	<td class="line x" title="176:221	In comparison to Bct, we see mixed results: Fmeasure increases by 3.8% for 10 shapers (which represents a relative error reduction of 6.3%, but drops by 0.1% for 14 shapers." ></td>
	<td class="line x" title="177:221	Overall, these results suggest that when the CT is tunable, training set expansion helps the minority classes but hurts the remaining classes." ></td>
	<td class="line x" title="178:221	A closer look at the results reveals that the 0.1% F-measure drop is due simply extracted from the 14-shaper experiments." ></td>
	<td class="line x" title="179:221	to a large drop in recall accompanied by a smaller gain in precision." ></td>
	<td class="line x" title="180:221	In other words, for the four non-minority classes, the benefits obtained from using the bootstrapped documents can also be obtained by simply adjusting the CT. This could be attributed to the fact that a decent classifier can be trained using only the hand-labeled training examples for these four shapers, and as a result, the automatically labeled examples either provide very little new knowledge or are too noisy to be useful." ></td>
	<td class="line x" title="181:221	On the other hand, for the 10 minority classes, the 3.8% gain in F-measure can be attributed to a simultaneous rise in recall and precision." ></td>
	<td class="line x" title="182:221	Note that such gain cannot possibly be obtained by simply adjusting the CT, since adjusting the CT always results in higher recall and lower precision or vice versa." ></td>
	<td class="line x" title="183:221	Overall, the simultaneous rise in recall and precision implies that the bootstrapped documents have provided useful knowledge, particularly in the form of positive examples, for the classifiers." ></td>
	<td class="line x" title="184:221	Even though the bootstrapped documents are noisily labeled, they can still be used to improve the classifiers, as the set of initially labeled positive examples for the minority classes is too small." ></td>
	<td class="line x" title="185:221	6.3 Additional Analyses Quality of the bootstrapped data." ></td>
	<td class="line x" title="186:221	Since the bootstrapped documents are noisily labeled, a natural question is: How noisy are they?" ></td>
	<td class="line x" title="187:221	To get a sense of the accuracy of the bootstrapped documents without further manual labeling, recall that our experimental setup resembles a transductive setting where the test documents are part of the unlabeled data, and consequently, some of them may have been automatically labeled by the bootstrapping algorithm." ></td>
	<td class="line x" title="188:221	In fact, 137 documents in the five test folds were automatically labeled in the 14-shaper Ect experiments, and 69 automatically labeled documents were similarity obtained from the 10-shaper Ect experiments." ></td>
	<td class="line x" title="189:221	For 14 shapers, the accuracies of the positively and negatively labeled documents are 74.6% and 97.1%, respectively, and the corresponding numbers for 10 shapers are 43.2% and 81.3%." ></td>
	<td class="line x" title="190:221	These numbers suggest that negative examples can be acquired with high accuracies, but the same is not true for positive examples." ></td>
	<td class="line x" title="191:221	Nevertheless, learning the 10 shapers from the not-so-accurately-labeled positive examples still allows us to outperform the corresponding baseline." ></td>
	<td class="line x" title="192:221	849 Shaping Factor Positive Expanders Negative Expanders Familiarity unfamiliar, layout, unfamilarity, rely Physical Environment cloud, snow, ice, wind Physical Factors fatigue, tire, night, rest, hotel, awake, sleep, sick declare, emergency, advisory, separation Preoccupation distract, preoccupied, awareness, situational, task, interrupt, focus, eye, configure, sleep declare, ice snow, crash, fire, rescue, anti, smoke Pressure bad, decision, extend, fuel, calculate, reserve, diversion, alternate Table 5: Example positive and negative expansion words collected by Ect for selected shaping factors." ></td>
	<td class="line x" title="193:221	Analysis of the expanders." ></td>
	<td class="line x" title="194:221	To get an idea of whether the words acquired during the bootstrapping process (henceforth expanders) make intuitive sense, we show in Table 5 example positive and negative expanders obtained for five shaping factors from the Ect experiments." ></td>
	<td class="line x" title="195:221	As we can see, many of the positive expanders are intuitively obvious." ></td>
	<td class="line x" title="196:221	We might, however, wonder about the connection between, for example, the shaper Familiarity and the word rely, or between the shaper Pressure and the word extend." ></td>
	<td class="line x" title="197:221	We suspect that the bootstrapping algorithm is likely to make poor word selections particularly in the cases of the minority classes, where the positively labeled training data used to select expansion words is more sparse." ></td>
	<td class="line x" title="198:221	As suggested earlier, poor word choice early in the algorithm is likely to cause even poorer word choice later on." ></td>
	<td class="line x" title="199:221	On the other hand, while none of the negative expanders seem directly meaningful in relation to the shaper for which they were selected, some of them do appear to be related to other phenomena that may be negatively correlated with the shaper." ></td>
	<td class="line x" title="200:221	For instance, the words snow and ice were selected as negative expanders for Preoccupation and also as positive expanders for Physical Environment." ></td>
	<td class="line x" title="201:221	While these two shapers are only slightly negatively correlated, it is possible that Preoccupation may be strongly negatively correlated with the subset of Physical Environment incidents involving cold weather." ></td>
	<td class="line x" title="202:221	7 Related Work Since we recast cause identification as a text classification task and proposed a bootstrapping approach that targets at improving minority class prediction, the work most related to ours involves one or both of these topics." ></td>
	<td class="line x" title="203:221	Guzman-Cabrera et al.(2007) address the problem of class skewness in text classification." ></td>
	<td class="line x" title="205:221	Specifically, they first under-sample the majority classes, and then bootstrap the classifier trained on the under-sampled data using unlabeled documents collected from the Web." ></td>
	<td class="line x" title="206:221	Minority classes can be expanded without the availability of unlabeled data as well." ></td>
	<td class="line x" title="207:221	For example, Chawla et al.(2002) describe a method by which synthetic training examples of minority classes can be generated from other labeled training examples to address the problem of imbalanced data in a variety of domains." ></td>
	<td class="line x" title="209:221	Nigam et al.(2000) propose an iterative semisupervised method that employs the EM algorithm in combination with the naive Bayes generative model to combine a small set of labeled documents and a large set of unlabeled documents." ></td>
	<td class="line x" title="211:221	McCallum and Nigam (1999) suggest that the initial labeled examples can be obtained using a list of keywords rather than through annotated data, yielding an unsupervised algorithm." ></td>
	<td class="line x" title="212:221	Similar bootstrapping methods are applicable outside text classification as well." ></td>
	<td class="line x" title="213:221	One of the most notable examples is Yarowskys (1995) bootstrapping algorithm for word sense disambiguation." ></td>
	<td class="line x" title="214:221	Beginning with a list of unlabeled contexts surrounding a word to be disambiguated and a list of seed words for each possible sense, the algorithm iteratively uses the seeds to label a training set from the unlabeled contexts, and then uses the training set to identify more seed words." ></td>
	<td class="line x" title="215:221	8 Conclusions We have introduced a new problem, cause identification from aviation safety reports, to the NLP community." ></td>
	<td class="line x" title="216:221	We recast it as a multi-class, multilabel text classification task, and presented a bootstrapping algorithm for improving the prediction of minority classes in the presence of a small training set." ></td>
	<td class="line x" title="217:221	Experimental results show that our algorithm yields a relative error reduction of 6.3% in F-measure over a purely supervised baseline when applied to the minority classes." ></td>
	<td class="line x" title="218:221	By making our annotated dataset publicly available, we hope to stimulate research in this challenging problem." ></td>
	<td class="line x" title="219:221	850 Acknowledgments We thank the three anonymous reviewers for their invaluable comments on an earlier draft of the paper." ></td>
	<td class="line x" title="220:221	We are indebted to Muhammad Arshad Ul Abedin, who provided us with a preprocessed version of the ASRS corpus and, together with Marzia Murshed, annotated the 1,333 documents." ></td>
	<td class="line x" title="221:221	This work was supported in part by NASA Grant NNX08AC35A and NSF Grant IIS-0812261." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-2043
Toward finer-grained sentiment identification in product reviews through linguistic and ontological analyses
Min, Hye-Jin;Park, Jong C.;"></td>
	<td class="line x" title="1:107	Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 169172, Suntec, Singapore, 4 August 2009." ></td>
	<td class="line x" title="2:107	c 2009 ACL and AFNLP Toward finer-grained sentiment identification in product reviews through linguistic and ontological analyses   Hye-Jin Min Computer Science Department KAIST, Daejeon, KOREA hjmin@nlp.kiast.ac.kr Jong C. Park Computer Science Department KAIST, Daejeon, KOREA park@nlp.kaist.ac.kr    Abstract  We propose categories of finer-grained polarity for a more effective aspect-based sentiment summary, and describe linguistic and ontological clues that may affect such fine-grained polarity." ></td>
	<td class="line x" title="3:107	We argue that relevance for satisfaction, contrastive weight clues, and certain adverbials work to affect the polarity, as evidenced by the statistical analysis." ></td>
	<td class="line oc" title="4:107	1 Introduction Sentiment analysis have been widely conducted in several domains such as movie reviews, product reviews, news and blog reviews (Pang et al., 2002; Turney, 2002)." ></td>
	<td class="line x" title="5:107	The unit of the sentiment varies from a document level to a sentence level to a phrase-level, where a more fine-grained approach has been receiving more attention for its accuracy." ></td>
	<td class="line x" title="6:107	Sentiment analysis on product reviews identifies or summarizes sentiment from reviews by extracting relevant opinions about certain attributes of products such as their parts, or properties (Hu and Liu, 2004; Popescu and Etzioni, 2005)." ></td>
	<td class="line x" title="7:107	Aspect-based sentiment analysis summarizes sentiments with diverse attributes, so that customers may have to look more closely into analyzed sentiments (Titov and McDonald, 2008)." ></td>
	<td class="line x" title="8:107	However, there are additional problems." ></td>
	<td class="line x" title="9:107	First, it is rather hard to choose the right level of detail." ></td>
	<td class="line x" title="10:107	If concepts corresponding to attributes are too general, the level of detail may not be so much finer than the ones on a document level." ></td>
	<td class="line x" title="11:107	On the other hand, if concepts are too specific, there may be some attributes that are hardly mentioned in the reviews, resulting in the data sparseness problem." ></td>
	<td class="line x" title="12:107	Second, there are cases when some crucial information is lost." ></td>
	<td class="line x" title="13:107	For example, suppose that two product attributes are mentioned in a sentence with a coordinated or subordinated structure." ></td>
	<td class="line x" title="14:107	In this case, the information about their relation may not be shown in the summary if they are classified into different upper-level attributes." ></td>
	<td class="line x" title="15:107	Consider (1)." ></td>
	<td class="line x" title="16:107	(1) a.    /   ,     . osun macciman, sayksangi nemwu etwuweyo." ></td>
	<td class="line x" title="17:107	It fits me okay, but the color is too dark. (size: barely positive, color: negative) b.      ,            ." ></td>
	<td class="line x" title="18:107	sayngkakpota com yalpciman, aney patchye ipnun kenikka nalum kwaynchanhunke kathayo." ></td>
	<td class="line x" title="19:107	Its a bit thinner than I thought, but it is good enough for layering. (thickness: negative but acceptable, overall: positive)  Example (1) shows sample customer reviews about clothes, each first in Korean, followed by a Yale Romanized form, and an English translation." ></td>
	<td class="line x" title="20:107	Note that the weight of the polarity in the sentiment about size e.g. in (1a) is overcome by the one about color." ></td>
	<td class="line x" title="21:107	However, if the overall sentiment is computed by considering only the number of semantically identical phrases in the reviews, it misses the big picture." ></td>
	<td class="line x" title="22:107	In particular, when opinions regarding attributes are described with respect to expressions whose polarities are dependent on the specific contexts such as the weather or user preference, an overestimated or underestimated weight of the sentiment for each attribute may be assigned." ></td>
	<td class="line x" title="23:107	In our example, / yalpta/thin has an ambiguous polarity, i.e., either positive or negative, whose real value depends on the expected utility of the clothes." ></td>
	<td class="line x" title="24:107	In this case, the negative polarity is the intended one, as shown in (1b)." ></td>
	<td class="line x" title="25:107	In order to reflect this possibility, we need to adjust the weight of each polarity accordingly." ></td>
	<td class="line x" title="26:107	In this paper, we propose to look into the kind of linguistic and ontological clues that may in169 fluence the use of polarities, or the relevance for satisfaction of purchase inspired by Kanos theory of quality element classification (Huiskonen and Pirttila, 1998), the conceptual granularities, and such syntactic and lexical clues as conjunction items and adverbs." ></td>
	<td class="line x" title="27:107	They may play significant roles in putting together the identified polarity information, so as to assess correctly what the customers consider most important." ></td>
	<td class="line x" title="28:107	We conducted several one-way Analysis of Variance (ANOVA) tests to identify the effects of each clue on deriving categories of polarity and quantification method 2 to see whether these clues can distinguish fine-grained polarities correctly." ></td>
	<td class="line x" title="29:107	Section 2 introduces categories of polarity." ></td>
	<td class="line x" title="30:107	Section 3 analyzes ontological and linguistic clues for identifying the proper category." ></td>
	<td class="line x" title="31:107	Section 4 describes our method to extract such clues for a statistical analysis." ></td>
	<td class="line x" title="32:107	Section 5 discusses the results of the analysis and implications of the results." ></td>
	<td class="line x" title="33:107	Section 6 concludes the paper." ></td>
	<td class="line x" title="34:107	2 Categories of polarity We suggest two more fine-grained categories of polarity, or barely positive (BP) and acceptably negative (AN), in addition to positive (P), negative (N) and neutral (NEU)." ></td>
	<td class="line x" title="35:107	We distinguish barely positive from normal positive and distinguish acceptably negative from normal negative in order to derive finer-grained sentiments." ></td>
	<td class="line x" title="36:107	Wilson and colleagues (2006) identified the strength of news articles in the MPQA corpus, where they separated intensity (low, medium, high) from categories (private states)." ></td>
	<td class="line x" title="37:107	For the purpose of identifying each attributes contribution to the satisfaction after purchase, we believe that it is not necessary to have so many degrees of intensity." ></td>
	<td class="line x" title="38:107	We argue that the polarity of barely positive may hold attributes that must be satisfied and that acceptably negative may hold those that are somewhat optional." ></td>
	<td class="line x" title="39:107	3 Linguistic and Ontological Analyses In this section, we discuss linguistic and ontological clues that influence the process of identifying finer-grained polarity." ></td>
	<td class="line x" title="40:107	For the purpose of exposition, we build hierarchical and aspect-based review structure as shown in Figure 1." ></td>
	<td class="line x" title="41:107	Major aspects include Price, Delivery, Service, and Product." ></td>
	<td class="line x" title="42:107	If we go down another level, Product is divided into Quality and Comfortableness." ></td>
	<td class="line x" title="43:107	In defining relevant attributes, we consider all the lower-level concepts of major aspects, which contain the characteristics of the product with a description of the associated sentiment." ></td>
	<td class="line x" title="44:107	Figure 1." ></td>
	<td class="line x" title="45:107	Review structure  Relevance for Satisfaction: We consider relevant attributes that affect the quality and satisfaction of the products as one of the important clues." ></td>
	<td class="line x" title="46:107	Quality elements classified by Kano as shown in Table 1 can be base indicators of relevant attributes for satisfaction in real review text." ></td>
	<td class="line x" title="47:107	For example, while completeness of the product may become crucial if the product has a defect, it is usually not the case that it would contribute much to the overall satisfaction of the customer." ></td>
	<td class="line x" title="48:107	Quality Elements  Example features Must-be Quality (MQ) Durability, Completeness 1-dimension Quality (1DQ) Design, Color, Material Attractive Quality (AQ) Luxurious look Table 1." ></td>
	<td class="line x" title="49:107	Kano's Quality Elements  Conceptual Granularity: The concepts corresponding to attributes have a different level of detail." ></td>
	<td class="line x" title="50:107	If the customer wants to comment on some attributes in detail, she could use a finegrained concept (e.g., the width of the thigh part of the pants) rather than a coarse-grained one (e.g., just the size of the pants)." ></td>
	<td class="line x" title="51:107	To deal properly with the changing granularity of such concepts, we constructed a domain specific semihierarchical network for clothes of the ClothingType structure, in addition to the Review structure, by utilizing hierarchical category information in online shopping malls." ></td>
	<td class="line x" title="52:107	Figure 2 shows an example for pants." ></td>
	<td class="line x" title="53:107	ClothingType Bottom Pants Sub_f Sub_p Thigh CalfWaistHip Length+ Material+ Design: Line+ Design: Pattern* Design: Style* Color Size Design: Detail*  Figure 2." ></td>
	<td class="line x" title="54:107	ClothingType structure for pants  Syntactic and Lexical Clues: Descriptions of each attribute in the reviews are often expressed 170 in a phrase or clause, so that conjunctions, or endings of a word with a conjunctive marker in Korean, play a significant role in connecting one attribute to another." ></td>
	<td class="line x" title="55:107	They also convey a subtle meaning of the sentiment about relations between two or more connected attributes." ></td>
	<td class="line x" title="56:107	We classified such syntactic clues into 4 groups of likeness (L), contrary (C), cause-effect (CE), and contrary with contrastive markers (CC)." ></td>
	<td class="line x" title="57:107	Wilson and colleagues (2006) selected some syntactic clues as features for intensity classification." ></td>
	<td class="line x" title="58:107	The selected features are shown to improve the accuracy, but the set of clues may vary to the nature of the given corpus, so that some otherwise useful clues that reflect a particular focused structure may not be selected." ></td>
	<td class="line x" title="59:107	We argue that some syntactic clues such as the use of certain conjunctions can be identified manually to make up for the limitation of feature selection." ></td>
	<td class="line x" title="60:107	Adverbs modifying adjectives or verbs such as too, and very also strengthen the polarity of a given sentiment, so such clues work to differentiate normal positive or negative from barely positive and acceptably negative." ></td>
	<td class="line x" title="61:107	Table 2 summarizes linguistic clues in the present analysis." ></td>
	<td class="line x" title="62:107	Clues  Examples CONJ/ END L -  -ko and C -  -ciman but,   kulena however CE -  -ese so,  kulayse therefore CC -    -kin -ciman  Its , but, though ADV Strong   maywu very,  nemwu too Mild   com a little Table 2." ></td>
	<td class="line x" title="63:107	Syntactic and Lexical Clues  All these three types of clue that appear in the review text may interact with one another." ></td>
	<td class="line x" title="64:107	For example, attributes with barely positive tend to be described with a concept on a coarse level, and may belong to Must-be Quality (e,g.,  size in (1a))." ></td>
	<td class="line x" title="65:107	However, if such attributes are negative, customers may explain them with a very finegrained concept (e.g., the width of thigh is okay, but the calf part is too wide; interaction between relevance for satisfaction and conceptual granularity)." ></td>
	<td class="line x" title="66:107	They may also use adverbs such as too to emphasize such unexpected polarity information." ></td>
	<td class="line x" title="67:107	For emphasis, a contrastive structure can be used to indicate which attribute has a more weight (e.g., A but B; interaction between syntactic clues and relevance for satisfaction)." ></td>
	<td class="line x" title="68:107	In addition, an unfocused attribute A may be the attribute with acceptably negative if the polarity of the attribute B is positive." ></td>
	<td class="line x" title="69:107	We believe that the interaction between lexical and syntactic clues and relevance for satisfaction are the most important and that this correlation information may be utilized with such fine-grained polarity as barely positive or acceptably negative." ></td>
	<td class="line x" title="70:107	4 Clue Acquisition We acquired data semi-automatically for each clue from the extracted attributes and their descriptions from 500 product reviews of several types of pants and annotated polarities manually." ></td>
	<td class="line x" title="71:107	We obtained raw text reviews from one of the major online shopping malls in Korea 1  and performed a morphology analysis and POS-tagging." ></td>
	<td class="line x" title="72:107	After POS-tagging, we collected all the noun phrases as candidates of attributes." ></td>
	<td class="line x" title="73:107	We regarded some of them as attributes with the following guidelines and filtered out the rest: 1) NP with frequent adjectives 2) NP with frequent nonfunctional and intransitive verbs." ></td>
	<td class="line x" title="74:107	In the case of subject omission, we converted adjectives or verbs into their corresponding nouns, such as thin into thickness." ></td>
	<td class="line x" title="75:107	Hu and Liu (2004) identified attributes of IT products based on frequent noun phrases and Popescu and Etzioni (2005) utilized PMI values between product class (hotels and scanners) and some phrases including product." ></td>
	<td class="line x" title="76:107	In our case, we used attributes that belong only to the Product concept in the Review structure, because most attributes we consider are sub-types or sub-attribute of Product." ></td>
	<td class="line x" title="77:107	The total number of <attribute, polarity> pairs is 474." ></td>
	<td class="line x" title="78:107	For relevance for satisfaction, we converted extracted attributes into one of the types of Kanos quality elements by the mapping table we built." ></td>
	<td class="line x" title="79:107	For conceptual granularity we regarded all the attributes with a depth less than 2 as coarse and those more than 2 as fine." ></td>
	<td class="line x" title="80:107	Syntactic and lexical clues are identified from the context information around extracted adjective or verbs by the patterns based on POS information." ></td>
	<td class="line x" title="81:107	5 Statistical Analysis and Discussion We conducted one-way Analysis of Variance (ANOVA) tests using relevance for satisfaction (ReV), conceptual granularity (Granul), and two linguistic clues, ADV and CONJ/END, in order to assess the effects of each clue on identifying categories of polarity." ></td>
	<td class="line x" title="82:107	The ANOVA suggests  1  http://www.11st.co.kr 171 reliable effects of ReV (F(2,474) = 22.2; p = .000), ADV (F(2, 474) = 41.3; p = .000), and CONJ/END (F(3, 474) = 6.1; p = .000)." ></td>
	<td class="line x" title="83:107	We also performed post-hoc tests to test significant differences." ></td>
	<td class="line x" title="84:107	For ReV, there are significant differences between MQ and 1DQ (p=.000), and between MQ and AQ (p =.032)." ></td>
	<td class="line x" title="85:107	AQ is related to positive and MQ to acceptably negative by the result." ></td>
	<td class="line x" title="86:107	For ADV, there are significant differences between all pairs (p <.05)." ></td>
	<td class="line x" title="87:107	For CONJ/END, there are significant differences between likeness and contrary (p = .015), and between likeness and contrary with contrastive markers (p = .025)." ></td>
	<td class="line x" title="88:107	The contrary and contrary with contrastive markers types of conjunctions are related to acceptably negative." ></td>
	<td class="line x" title="89:107	We also conducted Quantification method 2 to see if these clues can discriminate between BP and P and discriminate between AN and N. The regression equation for distinguishing AN from N is statistically significant at the 5% level (F(7,177) = 12,2; R 2 =0.335; Std." ></td>
	<td class="line x" title="90:107	error of the estimate =  0.821; error rate for discriminant = 0.21)." ></td>
	<td class="line x" title="91:107	The coefficients for mild (t 2 =30.8), contrary (t 2 =17.8) and contrary with contrastive markers (t 2 =14.1) are significant." ></td>
	<td class="line x" title="92:107	The results lead us to conclude that we can identify acceptably negative from the clothes reviews by extracting the particular lexical clue, adverbs of mild category and syntactic clue, such as conjunctions of contrary, and contrary with contrastive markers, or contrastive weight." ></td>
	<td class="line x" title="93:107	This clue may convey the customers argumentative intention toward the product, or argumentative orientation, for instance, A and B in A but B. C have different influence on the following discourse C (Elhadad and McKeown, 1990)." ></td>
	<td class="line x" title="94:107	Although contrary with contrastive markers plays an important role in identifying acceptably negative, it could also be used to identify another type of positive as shown in  example (2)." ></td>
	<td class="line x" title="95:107	(2)    ." ></td>
	<td class="line x" title="96:107	   . com twukkeptanun sayngkaki tupnita." ></td>
	<td class="line x" title="97:107	kulayto ttattushakin haneyyo." ></td>
	<td class="line x" title="98:107	It is a bit thick, but it keeps me warm.  It is a positive feature, but neither fully positive nor barely positive." ></td>
	<td class="line x" title="99:107	It seems to be somewhere inbetween." ></td>
	<td class="line x" title="100:107	The order of appearance in reviews may also affect the strength of polarity." ></td>
	<td class="line x" title="101:107	In addition, particular cue phrases such as ~ / kesman ppayko/except that  can also convey acceptably negative, too." ></td>
	<td class="line x" title="102:107	In the future, we need to assess the importance of each proposed clue relative to others and to the existing ones." ></td>
	<td class="line x" title="103:107	We also need to investigate the nature of interactions among linguistic, ontological and relevance for satisfaction clues, which may influence the actual performance for identifying finer-grained polarity." ></td>
	<td class="line x" title="104:107	6 Conclusion and Future Work We proposed further categories of polarity in order to make aspect-based sentiment summary more effective." ></td>
	<td class="line x" title="105:107	Our linguistic and ontological analyses suggest that there are clues, such as relevance for satisfaction, contrastive weight and certain adverbials, that work to affect polarity in a more subtle but crucial manner, as evidenced also by the statistical analysis." ></td>
	<td class="line x" title="106:107	We plan to find out product attributes that contribute most to modeling the interaction among the proposed clues in effective sentiment summarization." ></td>
	<td class="line x" title="107:107	Acknowledgments This work was funded in part by the Intelligent Robotics Development Program, a 21 st  Century Frontier R&D Program by the Ministry of Knowledge Economy in Korea, and in part by the 2 nd  stage of the Brain Korea 21 project." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-2080
Graph Ranking for Sentiment Transfer
Wu, Qiong;Tan, Songbo;Cheng, Xue-Qi;"></td>
	<td class="line x" title="1:102	Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 317320, Suntec, Singapore, 4 August 2009." ></td>
	<td class="line x" title="2:102	c 2009 ACL and AFNLP Graph Ranking for Sentiment Transfer  Qiong Wu 1,2 , Songbo Tan 1  and Xueqi Cheng 1  1 Institute of Computing Technology, Chinese Academy of Sciences, China 2  Graduate University of Chinese Academy of Sciences, China {wuqiong,tansongbo}@software.ict.ac.cn, cxq@ict.ac.cn  Abstract With the aim to deal with sentiment-transfer problem, we proposed a novel approach, which integrates the sentiment orientations of documents into the graph-ranking algorithm." ></td>
	<td class="line x" title="3:102	We apply the graph-ranking algorithm using the accurate labels of old-domain documents as well as the pseudo labels of new-domain documents." ></td>
	<td class="line x" title="4:102	Experimental results show that proposed algorithm could improve the performance of baseline methods dramatically for sentiment transfer." ></td>
	<td class="line x" title="5:102	1 Introduction With the rapid growth of reviewing pages, sentiment classification is drawing more and more attention (Bai et al., 2005; Pang and Lee, 2008)." ></td>
	<td class="line x" title="6:102	Generally speaking, sentiment classification can be considered as a special kind of traditional text classification (Tan et al., 2005; Tan, 2006)." ></td>
	<td class="line pc" title="7:102	In most cases, supervised learning methods can perform well (Pang et al., 2002)." ></td>
	<td class="line n" title="8:102	But when training data and test data are drawn from different domains, supervised learning methods always produce disappointing results." ></td>
	<td class="line x" title="9:102	This is so-called cross-domain sentiment classification problem (or sentiment-transfer problem)." ></td>
	<td class="line x" title="10:102	Sentiment transfer is a new study field." ></td>
	<td class="line x" title="11:102	In recent years, only a few works are conducted on this field." ></td>
	<td class="line x" title="12:102	They are generally divided into two categories." ></td>
	<td class="line x" title="13:102	The first one needs a small amount of labeled training data for the new domain (Aue and Gamon, 2005)." ></td>
	<td class="line x" title="14:102	The second one needs no labeled data for the new domain (Blitzer et al., 2007; Tan et al., 2007; Andreevskaia and Bergler, 2008; Tan et al., 2008; Tan et al., 2009)." ></td>
	<td class="line x" title="15:102	In this paper, we concentrate on the second category which proves to be used more widely." ></td>
	<td class="line x" title="16:102	Graph-ranking algorithm has been successfully used in many fields (Wan et al., 2006; Esuli and Sebastiani, 2007), whose idea is to give a node high score if it is strongly linked with other high-score nodes." ></td>
	<td class="line x" title="17:102	In this work, we extend the graph-ranking algorithm for sentiment transfer by integrating the sentiment orientations of the documents, which could be considered as a sentiment-transfer version of the graph-ranking algorithm." ></td>
	<td class="line x" title="18:102	In this algorithm, we assign a score for every unlabelled document to denote its extent to negative or positive, then we iteratively calculate the score by making use of the accurate labels of old-domain data as well as the pseudo labels of new-domain data, and the final score for sentiment classification is achieved when the algorithm converges, so we can label the newdomain data based on these scores." ></td>
	<td class="line x" title="19:102	2 The Proposed Approach 2.1 Overview In this paper, we have two document sets: the test data D U  = {d 1 ,,d n } where d i  is the term vector of the i th  text document and each d i  D U (i = 1,,n) is unlabeled; the training data D L  = {d n+1 ,d n+m } where d j  represents the term vector of the j th  text document and each d j  D L (j = n+1,,n+m) should have a label from a category set C = {negative, positive}." ></td>
	<td class="line x" title="20:102	We assume the training dataset D L  is from the related but different domain with the test dataset D U . Our objective is to maximize the accuracy of assigning a label in C to d i  D U  (i = 1,,n) utilizing the training data D L  in another domain." ></td>
	<td class="line x" title="21:102	The proposed algorithm is based on the following presumptions:    (1) Let W L  denote the word space of old domain, W U  denote the word space of new domain." ></td>
	<td class="line x" title="22:102	W L  W U  ." ></td>
	<td class="line x" title="23:102	(2) The labels of documents appear both in the training data and the test data should be the same." ></td>
	<td class="line x" title="24:102	Based on graph-ranking algorithm, it is thought that if a document is strongly linked with positive (negative) documents, it is probably positive (negative)." ></td>
	<td class="line x" title="25:102	And this is the basic idea of learning from a documents neighbors." ></td>
	<td class="line x" title="26:102	Our algorithm integrates the sentiment orientations of the documents into the graph-ranking algorithm." ></td>
	<td class="line x" title="27:102	In our algorithm, we build a graph 317 whose nodes denote documents and edges denote the content similarities between documents." ></td>
	<td class="line x" title="28:102	We initialize every document a score (1 denotes positive, and -1 denotes negative) to represent its degree of sentiment orientation, and we call it sentiment score." ></td>
	<td class="line x" title="29:102	The proposed algorithm calculates the sentiment score of every unlabelled document by learning from its neighbors in both old domain and new domain, and then iteratively calculates the scores with a unified formula." ></td>
	<td class="line x" title="30:102	Finally, the algorithm converges and each document gets its sentiment score." ></td>
	<td class="line x" title="31:102	When its sentiment score falls in the range [0, 1] (or [-1, 0]], the document should be classified as positive (or negative)." ></td>
	<td class="line x" title="32:102	The closer its sentiment score is near 1 (or -1), the higher the positive (or negative) degree is. 2.2 Score Documents Score Documents Using Old-domain Information We build a graph whose nodes denote documents in both D L  and D U  and edges denote the content similarities between documents." ></td>
	<td class="line x" title="33:102	If the content similarity between two documents is 0, there is no edge between the two nodes." ></td>
	<td class="line x" title="34:102	Otherwise, there is an edge between the two nodes whose weight is the content similarity." ></td>
	<td class="line x" title="35:102	The content similarity between two documents is computed with the cosine measure." ></td>
	<td class="line x" title="36:102	We use an adjacency matrix U to denote the similarity matrix between D U  and D L . U=[U ij ] nxm  is defined as follows: mnnjni dd dd U ji ji ij ++==   = ,,1,,,1,    (1) The weight associated with term t is computed with tf t idf t where tf t  is the frequency of term t in the document and idf t is the inverse document frequency of term t, i.e. 1+log(N/n t ), where N is the total number of documents and n t  is the number of documents containing term t in a data set." ></td>
	<td class="line x" title="37:102	In consideration of convergence, we normalize U to  U by making the sum of each row equal to 1: 11 ,0  0, mm ij ij ij jj ij UUifU U otherwise ==    =             (2) In order to find the neighbors (in another word, the nearest documents) of a document, we sort every row of  U  to U % in descending order." ></td>
	<td class="line x" title="38:102	That is: U % ij  U % ik  (i = 1,n; j,k = 1,m; kj)." ></td>
	<td class="line x" title="39:102	Then for d i D U  (i = 1,,n), U % ij  (j = 1,,K ) corresponds to K neighbors in D L . So we can get its K neighbors." ></td>
	<td class="line x" title="40:102	We use a matrix [] ij n K NN  = to denote the neighbors of D U  in old domain, with N ij  corresponding to the j th  nearest neighbor of d i . At last, we can calculate sentiment score s i  (i = 1,,n) using the scores of the d i s neighbors as follows: nisUs i j Nj k ij k i ,,1,)  ( )1()( ==            (3) where i means the i th  row of a matrix and )(k i s denotes the i s at the k th  iteration." ></td>
	<td class="line x" title="41:102	Score Documents Using New-domain Information Similarly, a graph is built, in which each node corresponds to a document in D U  and the weight of the edge between any different documents is computed by the cosine measure." ></td>
	<td class="line x" title="42:102	We use an adjacency matrix V=[V ij ] n x n  to describe the similarity matrix." ></td>
	<td class="line x" title="43:102	And V is similarly normalized to  V to make the sum of each row equal to 1." ></td>
	<td class="line x" title="44:102	Then we sort every row of  V  to V % in descending order, thus we can get K neighbors of d i D U  (i = 1,,n) from V % ij  (j = 1,K), and we use a matrix [] ij n K MM  =  to denote the neighbors of D U  in the new domain." ></td>
	<td class="line x" title="45:102	Finally, we can calculate s i  using the sentiment scores of the d i s neighbors as follows:     == i ji Mj k ij k nisVs ,,1),  ( )1()(          (4) 2.3 Sentiment Transfer Algorithm Initialization Firstly, we classify the test data D U  to get their initial labels using a traditional classifier." ></td>
	<td class="line x" title="46:102	For simplicity, we use prototype classification algorithm (Tan et al., 2005) in this work." ></td>
	<td class="line x" title="47:102	Then, we give -1 to s i (0)  if d i s label is negative, and 1 if positive." ></td>
	<td class="line x" title="48:102	So we obtain the initial sentiment score vector S (0)  for both domain data." ></td>
	<td class="line x" title="49:102	At last, s i (0)  (i = 1,,n) is normalized as follows to make the sum of positive scores of D U  equal to 1, and the sum of negative scores of D U  equal to -1: ni sifss sifss s i Dj ji i Dj ji U pos U neg i ,,1 0, 0,)( )0()0()0( )0()0()0( )0( =        > < =       (5) 318 where U neg D and U pos D denote the negative and positive document set of D U respectively." ></td>
	<td class="line x" title="50:102	The same as (5), s j  (0)  (j =n+1,,n+m) is normalized." ></td>
	<td class="line x" title="51:102	Algorithm Introduction In our algorithm, we label D U  by making use of information of both old domain and new domain." ></td>
	<td class="line x" title="52:102	We fuse equations (3) and (4), and get the iterative equation as follows: nisVsUs i h i ji Mh k ih Nj k ij k ,,1,)  ()  ( )1()1()( =+=        (6) where 1 +=, and  and show the relative importance of old domain and new domain to the final sentiment scores." ></td>
	<td class="line x" title="53:102	In consideration of the convergence, S (k)  (S at the k th  iteration) is normalized after each iteration." ></td>
	<td class="line x" title="54:102	Here is the complete algorithm: 1." ></td>
	<td class="line x" title="55:102	Classify D U  with a traditional classifier." ></td>
	<td class="line x" title="56:102	Initialize the sentiment score s i  of d i D U D L  (i = 1,n+m) and normalize it." ></td>
	<td class="line x" title="57:102	2." ></td>
	<td class="line x" title="58:102	Iteratively calculate the S (k)  of D U  and normalize it until it achieves the convergence: nisVsUs i h i ji Mh k ih Nj k ij k ,,1,)  ()  ( )1()1()( =+=        ni sifss sifss s k i Dj k j k i k i Dj k j k i k i U pos U neg ,,1 0, 0,)( )()()( )()()( )( =        > < =      3." ></td>
	<td class="line x" title="59:102	According to s i S (i = 1,,n), assign each d i D U (i = 1,n) a label." ></td>
	<td class="line x" title="60:102	If s i  is between -1 and 0, assign d i  the label negative; if s i  is between 0 and 1, assign d i  the label positive." ></td>
	<td class="line x" title="61:102	3 EXPERIMENTS 3.1 Data Preparation We prepare three Chinese domain-specific data sets from on-line reviews, which are: Electronics Reviews (Elec, from http://detail.zol.com.cn/), Stock Reviews (Stock, from http://blog.sohu.com /stock/) and Hotel Reviews (Hotel, from http://www.ctrip.com/)." ></td>
	<td class="line x" title="62:102	And then we manually label the reviews as negative or positive." ></td>
	<td class="line x" title="63:102	The detailed composition of the data sets are shown in Table 1, which shows the name of the data set (DataSet), the number of negative reviews (Neg), the number of positive reviews (Pos), the average length of reviews (Length), the number of different words (Vocabulary) in this data set." ></td>
	<td class="line x" title="64:102	DataSet Neg Pos Length Vocabulary Elec 554 1,054 121 6,200 Stock 683 364 460 13,012 Hotel 2,000 2,000 181 11,336 Table 1." ></td>
	<td class="line x" title="65:102	Data sets composition We make some preprocessing on the datasets." ></td>
	<td class="line x" title="66:102	First, we use ICTCLAS (http://ictclas.org/), a Chinese text POS tool, to segment these Chinese reviews." ></td>
	<td class="line x" title="67:102	Second, the documents are represented by vector space model." ></td>
	<td class="line x" title="68:102	3.2 Evaluation Setup In our experiment, we use prototype classification algorithm (Tan et al., 2005) and Support Vector Machine experimenting on the three data sets as our baselines separately." ></td>
	<td class="line x" title="69:102	The Support Vector Machine is a state-of-the-art supervised learning algorithm." ></td>
	<td class="line x" title="70:102	In our experiment, we use LibSVM (www.csie.ntu.edu.tw/~cjlin/libsvm/) with a linear kernel and set all options by default." ></td>
	<td class="line x" title="71:102	We also compare our algorithm to Structural Correspondence Learning (SCL) (Blitzer et al., 2007)." ></td>
	<td class="line x" title="72:102	SCL is a state-of-the-art sentimenttransfer algorithm which automatically induces correspondences among features from different domains." ></td>
	<td class="line x" title="73:102	It identifies correspondences among features from different domains by modeling their correlations with pivot features, which are features that behave in the same way for discriminative learning in both domains." ></td>
	<td class="line x" title="74:102	In our experiment, we use 100 pivot features." ></td>
	<td class="line x" title="75:102	3.3 Overall Performance In this section, we conduct two groups of experiments where we separately initialize the sentiment scores in our algorithm by prototype classifier and Support Vector Machine." ></td>
	<td class="line x" title="76:102	There are two parameters in our algorithm, K and  ( can be calculated by 1- )." ></td>
	<td class="line x" title="77:102	We set the parameters K and with 150 and 0.7 respectively, which indicates we use 150 neighbors and the contribution from old domain is a little more important than that from new domain." ></td>
	<td class="line x" title="78:102	It is thought that the algorithm achieves the convergence when the changing between the sentiment score s i  computed at two successive iterations for any d i  D U (i = 1,n) falls below a given threshold, and we set the threshold 0.00001 in this work." ></td>
	<td class="line x" title="79:102	Table 2 shows the accuracy of Prototype, LibSVM, SCL and our algorithm when training data and test data belong to different domains." ></td>
	<td class="line x" title="80:102	319 Our algorithm is separately initialized by Prototype and LibSVM." ></td>
	<td class="line x" title="81:102	Baseline Proposed Algorithm  Prototype LibSVM SCL Prototype+ OurApproach LibSVM+ OurApproach Elec->Stock 0.6652 0.6478 0.7507 0.7326 0.7304 Elec->Hotel 0.7304 0.7522 0.7750 0.7543 0.7543 Stock->Hotel 0.6848 0.6957 0.7683 0.7435 0.7457 Stock->Elec 0.7043 0.6696 0.8340 0.8457 0.8435 Hotel->Stock 0.6196 0.5978 0.6571 0.7848 0.7848 Hotel->Elec 0.6674 0.6413 0.7270 0.8609 0.8609 Average 0.6786 0.6674 0.7520 0.7870 0.7866 Table 2." ></td>
	<td class="line x" title="82:102	Accuracy comparison of different methods As we can observe from Table 2, our algorithm can dramatically increase the accuracy of sentiment-transfer." ></td>
	<td class="line x" title="83:102	Seen from the 2 nd  column and the 5 th  column, every accuracy of the proposed algorithm is increased comparing to Prototype." ></td>
	<td class="line x" title="84:102	The average increase of accuracy over all the 6 problems is 10.8%." ></td>
	<td class="line x" title="85:102	Similarly, the accuracy of our algorithm is higher than LibSVM in every problem and the average increase of accuracy is 11.9%." ></td>
	<td class="line x" title="86:102	The great improvement comparing with the baselines indicates that the proposed algorithm performs very effectively and robustly." ></td>
	<td class="line x" title="87:102	Seen from Table 2, our result about SCL is in accord with that in (Blitzer et al., 2007) on the whole." ></td>
	<td class="line x" title="88:102	The average accuracy of SCL is higher than both baselines, which convinces that SCL is effective for sentiment-transfer." ></td>
	<td class="line x" title="89:102	However, our approach outperforms SCL: the average accuracy of our algorithm is about 3.5 % higher than SCL." ></td>
	<td class="line x" title="90:102	This is caused by two reasons." ></td>
	<td class="line x" title="91:102	First, SCL is essentially based on co-occurrence of words (the window size is the whole document), so it is easily affected by low frequency words and the size of data set." ></td>
	<td class="line x" title="92:102	Second, the pivot features of SCL are totally dependent on experts in the field, so the quality of pivot features will seriously affect the performance of SCL." ></td>
	<td class="line x" title="93:102	This improvement convinces us of the effectiveness of our algorithm." ></td>
	<td class="line x" title="94:102	4 Conclusion and Future Work In this paper, we propose a novel sentimenttransfer algorithm." ></td>
	<td class="line x" title="95:102	It integrates the sentiment orientations of the documents into the graphranking based method for sentiment-transfer problem." ></td>
	<td class="line x" title="96:102	The algorithm assigns a score for every document being predicted, and it iteratively calculates the score making use of the accurate labels of old-domain data, as well as the pseudo labels of new-domain data, finally it labels the new-domain data as negative or positive basing on this score." ></td>
	<td class="line x" title="97:102	The experiment results show that the proposed approach can dramatically improve the accuracy when transferred to a new domain." ></td>
	<td class="line x" title="98:102	In this study, we find the neighbors of a given document using cosine similarity." ></td>
	<td class="line x" title="99:102	This is too general, and perhaps not so proper for sentiment classification." ></td>
	<td class="line x" title="100:102	In the next step, we will try other methods to calculate the similarity." ></td>
	<td class="line x" title="101:102	Also, our approach can be applied to multi-task learning." ></td>
	<td class="line x" title="102:102	5 Acknowledgments This work was mainly supported by two funds, i.e., 0704021000 and 60803085, and one another project, i.e., 2004CB318109." ></td>
</tr></table>
</div
</body></html>
