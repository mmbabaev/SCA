<html><body><head><link rel="stylesheet" type="text/css" href="style.css" /><script src="map.js"></script><script src="jquery-1.7.1.min.js"></script></head>
<div class="dstPaperData">
P07-1033 <div class="dstPaperTitle">Frustratingly Easy Domain Adaptation</div><div class="dstPaperAuthors">Daum√© III, Hal;</div>
</div>
<table cellspacing="0" cellpadding="0"><tr>
	<td class="srcData" >Source Paper</td>
	<td class="pp legend" ><input type="checkbox" id="cbIPositive" checked="true"/><label for="cbIPositive">Informal +<label></td>
	<td class="nn legend" ><input type="checkbox" id="cbINegative" checked="true"/><label for="cbINegative">Informal -<label></td>
	<td class="oo legend" ><input type="checkbox" id="cbIObjective" checked="true"/><label for="cbIObjective">Informal Neutral<label></td>
	<td class="ppc legend" ><input type="checkbox" id="cbEPositive" checked="true"/><label for="cbEPositive">Formal +</label></td>
	<td class="nnc legend" ><input type="checkbox" id="cbENegative" checked="true"/><label for="cbENegative">Formal -</label></td>
	<td class="ooc legend" ><input type="checkbox" id="cbEObjective" checked="true"/><label for="cbEObjective">Formal Neutral</label></td>
	<td class="lb"><input type="checkbox" id="cbSentenceBoundary"/><label for="cbSentenceBoundary">Sentence Boundary</label></td>
</tr></table>
<div class="dstPaper">
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1003
On Robustness and Domain Adaptation using SVD for Word Sense Disambiguation
Agirre, Eneko;Lopez de Lacalle, Oier;"></td>
	<td class="line x" title="1:210	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1724 Manchester, August 2008 On Robustness and Domain Adaptation using SVD for Word Sense Disambiguation Eneko Agirre and Oier Lopez de Lacalle Informatika Fakultatea, University of the Basque Country 20018, Donostia, Basque Country {e.agirre,oier.lopezdelacalle}@ehu.es Abstract In this paper we explore robustness and domain adaptation issues for Word Sense Disambiguation (WSD) using Singular Value Decomposition (SVD) and unlabeled data." ></td>
	<td class="line x" title="2:210	We focus on the semi-supervised domain adaptation scenario, where we train on the source corpus and test on the target corpus, and try to improve results using unlabeled data." ></td>
	<td class="line x" title="3:210	Our method yields up to 16.3% error reduction compared to state-of-the-art systems, being the first to report successful semi-supervised domain adaptation." ></td>
	<td class="line x" title="4:210	Surprisingly the improvement comes from the use of unlabeled data from the source corpus, and not from the target corpora, meaning that we get robustness rather than domain adaptation." ></td>
	<td class="line x" title="5:210	In addition, we study the behavior of our system on the target domain." ></td>
	<td class="line x" title="6:210	1 Introduction In many Natural Language Processing (NLP) tasks we find that a large collection of manuallyannotated text is used to train and test supervised machine learning models." ></td>
	<td class="line x" title="7:210	While these models have been shown to perform very well when tested on the text collection related to the training data (what we call the source domain), the performance drops considerably when testing on text from other domains (called target domains)." ></td>
	<td class="line oc" title="8:210	In order to build models that perform well in new (target) domains we usually find two settings (Daume III, 2007): In the semi-supervised setting the goal is to improve the system trained on the source domain using unlabeled data from the target domain, and the baseline is that of the system c2008." ></td>
	<td class="line x" title="9:210	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="10:210	Some rights reserved." ></td>
	<td class="line x" title="11:210	trained on the source domain." ></td>
	<td class="line x" title="12:210	In the supervised setting, training data from both source and target domains are used, and the baseline is provided by the system trained on the target domain." ></td>
	<td class="line x" title="13:210	The semi-supervised setting is the most attractive, as it would save developers the need to hand-annotate target corpora every time a new domain is to be processed." ></td>
	<td class="line x" title="14:210	The main goal of this paper is to use unlabeled data in order to get better domain-adaptation results for Word Sense Disambiguation (WSD) in the semi-supervised setting." ></td>
	<td class="line x" title="15:210	Singular Value Decomposition (SVD) has been shown to find correlations between terms which are helpful to overcome the scarcity of training data in WSD (Gliozzo et al., 2005)." ></td>
	<td class="line x" title="16:210	This paper explores how this ability of SVD can be applied to the domain-adaptation of WSD systems, and we show that SVD and unlabeled data improve the results of two state-of-theart WSD systems (k-NN and SVM)." ></td>
	<td class="line x" title="17:210	For the sake of this paper we call this set of experiments the domain adaptation scenario." ></td>
	<td class="line x" title="18:210	In addition, we also perform some related experiments on just the target domain." ></td>
	<td class="line x" title="19:210	We use unlabeled data in order to improve the results of a system trained and tested in the target domain." ></td>
	<td class="line x" title="20:210	These results are complementary to the domain adaptation experiments, and also provide an upperbound for semi-supervised domain adaptation." ></td>
	<td class="line x" title="21:210	We call these experiments the target domain scenario." ></td>
	<td class="line x" title="22:210	Note that both scenarios are semi-supervised, in that our focus is on the use of unlabeled data in addition to the available labeled data." ></td>
	<td class="line x" title="23:210	The experiments were performed on a publicly available corpus which was designed to study the effect of domain in WSD (Koeling et al., 2005)." ></td>
	<td class="line x" title="24:210	It comprises 41 nouns closely related to the SPORTS and FINANCES domains with 300 examples for each." ></td>
	<td class="line x" title="25:210	The 300 examples were drawn from the British National Corpus (Leech, 1992) (BNC), the SPORTS section of the Reuters corpus (Leech, 17 1992), and the FINANCES section of Reuters in equal number." ></td>
	<td class="line x" title="26:210	The paper is structured as follows." ></td>
	<td class="line x" title="27:210	Section 2 reviews prior work in the area." ></td>
	<td class="line x" title="28:210	Section 3 presents the datasets used, and Section 4 the learning methods, including the application of SVD." ></td>
	<td class="line x" title="29:210	The experimental results are presented in Section 5, for the semisupervised domain adaptation scenario, and Section 6, for the target scenario." ></td>
	<td class="line x" title="30:210	Section 7 presents the discussion and Section 8 the conclusions and future work." ></td>
	<td class="line x" title="31:210	2 Prior Work Domain adaptation is a subject attracting more and more attention." ></td>
	<td class="line x" title="32:210	In the semi-supervised setting, Blitzer et al.(2006) use Structural Correspondence Learning and unlabeled data to adapt a Part-of-Speech tagger." ></td>
	<td class="line x" title="34:210	They carefully select socalled pivot features to learn linear predictors, perform SVD on the weights learned by the predictor, and thus learn correspondences among features in both source and target domains." ></td>
	<td class="line x" title="35:210	Our technique also uses SVD, but we directly apply it to all features, and thus avoid the need to define pivot features." ></td>
	<td class="line x" title="36:210	In preliminary work we unsuccessfully tried to carry along the idea of pivot features to WSD." ></td>
	<td class="line x" title="37:210	Zelikovitz and Hirsh (2001) use unlabeled data (so-called background knowledge) with Latent Semantic Indexing (also based on SVD) on a Text Classification task with positive results." ></td>
	<td class="line x" title="38:210	They use related unlabeled text and include it in the term-by-document matrix to expand it and capture better the interesting properties of the data." ></td>
	<td class="line x" title="39:210	Their approach is similar to our SMA method in Section 4.2)." ></td>
	<td class="line pc" title="40:210	In the supervised setting, a recent paper by Daume III (2007) shows that, using a very simple feature augmentation method coupled with Support Vector Machines, he is able to effectively use both labeled target and source data to provide the best results in a number of NLP tasks." ></td>
	<td class="line x" title="41:210	His method improves or equals over previously explored more sophisticated methods (Daume III and Marcu, 2006; Chelba and Acero, 2004)." ></td>
	<td class="line x" title="42:210	Regarding WSD, some initial works made basic analysis of the particular issues." ></td>
	<td class="line x" title="43:210	Escudero et al.(2000) tested the supervised adaptation setting on the DSO corpus, which had examples from the Brown corpus and Wall Street Journal corpus." ></td>
	<td class="line x" title="45:210	They found that the source corpus did not help when tagging the target corpus, showing that tagged corpora from each domain would suffice, and concluding that hand tagging a large general corpus would not guarantee robust broad-coverage WSD." ></td>
	<td class="line x" title="46:210	Agirre and Martnez (2000) also used the DSO corpus in the supervised setting to show that training on a subset of the source corpora that is topically related to the target corpus does allow for some domain adaptation." ></td>
	<td class="line x" title="47:210	Their work used the fact that the genre tags of Brown allowed to detect which parts of the corpus were related to the target corpus." ></td>
	<td class="line x" title="48:210	More recently, Koeling et al.(2005) presented an unsupervised system to learn the predominant senses of particular domains." ></td>
	<td class="line x" title="50:210	Their system was based on the use of a similarity thesaurus induced from the domain corpus and WordNet." ></td>
	<td class="line x" title="51:210	They used the same dataset as in this paper for evaluation." ></td>
	<td class="line x" title="52:210	Chan and Ng (2007) performed supervised domain adaptation on a manually selected subset of 21 nouns from the DSO corpus." ></td>
	<td class="line x" title="53:210	They used active learning, count-merging, and predominant sense estimation in order to save target annotation effort." ></td>
	<td class="line x" title="54:210	They showed that adding just 30% of the target data to the source examples the same precision as the full combination of target and source data could be achieved." ></td>
	<td class="line x" title="55:210	They also showed that using the source corpus allowed to significantly improve results when only 10%-30% of the target corpus was used for training." ></td>
	<td class="line x" title="56:210	No data was given about the use of both tagged corpora." ></td>
	<td class="line x" title="57:210	Though not addressing domain adaptation, other works on WSD also used SVD and are closely related to the present paper." ></td>
	<td class="line x" title="58:210	Gliozzo et al.(2005) used SVD to reduce the space of the term-todocument matrix, and then computed the similarity between train and test instances using a mapping to the reduced space (similar to our SMA method in Section 4.2)." ></td>
	<td class="line x" title="60:210	They combined other knowledge sources into a complex kernel using SVM." ></td>
	<td class="line x" title="61:210	They report improved performance on a number of languages in the Senseval-3 lexical sample dataset." ></td>
	<td class="line x" title="62:210	Our present paper differs from theirs in that we propose an additional method to use SVD (the OMT method, Section 4.2), and that we evaluate the contribution of unlabeled data and SVD in isolation, leaving combination for future work." ></td>
	<td class="line x" title="63:210	Ando (2006) used Alternative Structured Optimization, which is closely related to Structural Learning (cited above)." ></td>
	<td class="line x" title="64:210	He first trained one linear predictor for each target word, and then performed SVD on 7 carefully selected submatrices 18 of the feature-to-predictor matrix of weights." ></td>
	<td class="line x" title="65:210	The system attained small but consistent improvements (no significance data was given) on the Senseval3 lexical sample datasets using SVD and unlabeled data." ></td>
	<td class="line x" title="66:210	We have previously shown (Agirre et al., 2005; Agirre and Lopez de Lacalle, 2007) that performing SVD on the feature-to-documents matrix is a simple technique that allows to improve performance with and without unlabeled data." ></td>
	<td class="line x" title="67:210	The use of several k-NN classifiers trained on a number of reduced and original spaces was shown to rank first in the Senseval-3 dataset and second in the SemEval 2007 competition." ></td>
	<td class="line x" title="68:210	The present work extends our own in that we present a comprehensive study on a domain adaptation dataset, producing additional insight on our method and the relation between SVD, features and unlabeled data." ></td>
	<td class="line x" title="69:210	3 Data sets The dataset we use was designed for domainrelated WSD experiments by Koeling et al.(2005), and is publicly available." ></td>
	<td class="line x" title="71:210	The examples come from the BNC (Leech, 1992) and the SPORTS and FINANCES sections of the Reuters corpus (Rose et al., 2002), comprising around 300 examples (roughly 100 from each of those corpora) for each of the 41 nouns." ></td>
	<td class="line x" title="72:210	The nouns were selected because they were salient in either the SPORTS or FINANCES domains, or because they had senses linked to those domains." ></td>
	<td class="line x" title="73:210	The occurrences were hand-tagged with the senses from WordNet (WN) version 1.7.1 (Fellbaum, 1998)." ></td>
	<td class="line x" title="74:210	Compared to the DSO corpus used in prior work (cf.Section 2) this corpus has been explicitly created for domain adaptation studies." ></td>
	<td class="line x" title="76:210	DSO contains texts coming from the Brown corpus and the Wall Street Journal, but the texts are not classified according to specific domains (e.g. Sports, Finances), which make DSO less suitable to study domain adaptation." ></td>
	<td class="line x" title="77:210	In addition to the labeled data, we also use unlabeled data coming from the three sources used in the labeled corpus: the written part of the BNC (89.7M words), the FINANCES part of Reuters (117,734 documents, 32.5M words), and the SPORTS part (35,317 documents, 9.1M words)." ></td>
	<td class="line x" title="78:210	4 Learning features and methods In this section, we review the learning features, the two methods to apply SVD, and the two learning algorithms used in the experiments." ></td>
	<td class="line x" title="79:210	4.1 Learning features We relied on the usual features used in previous WSD work, grouped in three main sets." ></td>
	<td class="line x" title="80:210	Local collocations comprise the bigrams and trigrams formed around the target word (using either lemmas, word-forms, and PoS tags1), those formed with the previous/posterior lemma/word-form in the sentence, and the content words in a4-word window around the target." ></td>
	<td class="line x" title="81:210	Syntactic dependencies2 use the object, subject, noun-modifier, preposition, and sibling lemmas, when available." ></td>
	<td class="line x" title="82:210	Finally, Bag-of-words features are the lemmas of the content words in the whole context, plus the salient bigrams in the context (Pedersen, 2001)." ></td>
	<td class="line x" title="83:210	4.2 Features from the reduced space Apart from the original space of features, we have the so called SVD features, obtained from the projection of the feature vectors into the reduced space (Deerwester et al., 1990)." ></td>
	<td class="line x" title="84:210	Basically, we set a term-by-document or feature-by-example matrix M from the corpus (see section below for more details)." ></td>
	<td class="line x" title="85:210	SVD decomposes it into three matrices, M = UV T . If the desired number of dimensions in the reduced space is p, we select p rows from  and V , yielding p and Vp respectively." ></td>
	<td class="line x" title="86:210	We can map any feature vectorvectort (which represents either a train or test example) into the p-dimensional space as follows: vectortp = vectortTVp1p . Those mapped vectors have p dimensions, and each of the dimensions is what we call a SVD feature." ></td>
	<td class="line x" title="87:210	We can now use the mapped vectors (vectortp) to train and test any learning method, as usual." ></td>
	<td class="line x" title="88:210	We have explored two different variants in order to build the reduced matrix and obtain the SVD features, as follows." ></td>
	<td class="line x" title="89:210	Single Matrix for All target words (SVDSMA)." ></td>
	<td class="line x" title="90:210	The method comprises the following steps: (i) extract bag-of-word features (terms in this case) from unlabeled corpora, (ii) build the term-bydocument matrix, (iii) decompose it with SVD, and (iv) project the labeled data (train/test)." ></td>
	<td class="line x" title="91:210	This technique is very similar to previous work on SVD (Gliozzo et al., 2005; Zelikovitz and Hirsh, 2001)." ></td>
	<td class="line x" title="92:210	The dimensionality reduction is performed once, over the whole unlabeled corpus, and it is then applied to the labeled data of each word." ></td>
	<td class="line x" title="93:210	The reduced 1The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001) 2This software was kindly provided by David Yarowskys group, from Johns Hopkins University." ></td>
	<td class="line x" title="94:210	19 space is constructed only with terms, which correspond to bag-of-words features, and thus discards the rest of the features." ></td>
	<td class="line x" title="95:210	Given that the WSD literature has shown that all features, including local and syntactic features, are necessary for optimal performance (Pradhan et al., 2007), we propose the following alternative to construct the matrix." ></td>
	<td class="line x" title="96:210	One Matrix per Target word (SVD-OMT)." ></td>
	<td class="line x" title="97:210	For each word: (i) construct a corpus with its occurrences in the labeled and, if desired, unlabeled corpora, (ii) extract all features, (iii) build the featureby-example matrix, (iv) decompose it with SVD, and (v) project all the labeled training and test data for the word." ></td>
	<td class="line x" title="98:210	Note that this variant performs one SVD process for each target word separately, hence its name." ></td>
	<td class="line x" title="99:210	We proposed this technique in (Agirre et al., 2005)." ></td>
	<td class="line x" title="100:210	An important parameter when doing SVD is the number of dimensions in the reduced space (p)." ></td>
	<td class="line x" title="101:210	We tried two different values for p (25 and 200) in the BNC domain, and the results were consistent in that 25 performed better for SVD-OMT and 200 better for SVD-SMA." ></td>
	<td class="line x" title="102:210	Those values were chosen for testing in the SPORTS and FINANCES domains, i.e. 25 for SVD-OMT and 200 for SVD-SMA." ></td>
	<td class="line x" title="103:210	4.3 Building Matrices The methods in the previous section can be applied to the following matrices M:  TRAIN: The matrix comprises features from labeled train examples alone." ></td>
	<td class="line x" title="104:210	This matrix can only be used to obtain OMT features." ></td>
	<td class="line x" title="105:210	 TRAIN  BNC: In addition to TRAIN, we matrix also includes unlabeled examples from the source corpus (BNC)." ></td>
	<td class="line x" title="106:210	Both OMT and SMA features can be obtained." ></td>
	<td class="line x" title="107:210	 TRAIN  {SPORTS,FINANCES}: Like the previous, but using unlabeled examples from one of the target corpora (FINANCES or SPORTS) instead." ></td>
	<td class="line x" title="108:210	Both OMT and SMA feature can be obtained." ></td>
	<td class="line x" title="109:210	Based on previous work (Agirre et al., 2005), we used 50% of the respective unlabeled corpora for OMT features, and the whole corpora for SMA." ></td>
	<td class="line x" title="110:210	4.4 Learning methods We used two well known classifiers, Support Vector Machines (SVM) and k-Nearest Neighbors (kNN)." ></td>
	<td class="line x" title="111:210	Regarding SVM we used linear kernels implemented in SVM-Light (Joachims, 1999)." ></td>
	<td class="line x" title="112:210	We estimated the soft margin (C) for each feature space and each word using a greedy process in a preliminary experiment on the source training data using cross-validation." ></td>
	<td class="line x" title="113:210	The same C value was used in the rest of the settings." ></td>
	<td class="line x" title="114:210	k-NN is a memory based learning method, where the neighbors are the k most similar labeled examples to the test example." ></td>
	<td class="line x" title="115:210	The similarity among instances is measured by the cosine of their vectors." ></td>
	<td class="line x" title="116:210	The test instance is labeled with the sense obtaining the maximum the sum of the weighted vote of the k most similar contexts." ></td>
	<td class="line x" title="117:210	We set k to 5 based on previous results (Agirre and Lopez de Lacalle, 2007)." ></td>
	<td class="line x" title="118:210	5 Domain adaptation scenario In this scenario we try to adapt a general purpose supervised WSD system trained on the source corpus (BNC) to a target corpus (either SPORTS or FINANCES) using unlabeled corpora only." ></td>
	<td class="line x" title="119:210	5.1 Experimental results Table 1 shows the precision results for this scenario." ></td>
	<td class="line x" title="120:210	Note that all methods have full coverage, i.e. they return a sense for all test examples, and therefore precision suffices to compare among systems." ></td>
	<td class="line x" title="121:210	We have computed significance ranges for all results in this paper using bootstrap resampling (Noreen, 1989)." ></td>
	<td class="line x" title="122:210	F1 scores outside of these intervals are assumed to be significantly different from the related F1 score (p < 0.05)." ></td>
	<td class="line x" title="123:210	The table has two main parts, each regarding to one of the target domains, SPORTS and FINANCES." ></td>
	<td class="line x" title="124:210	The use of two target domains allows to test whether the methods behave similarly in both domains." ></td>
	<td class="line x" title="125:210	The columns denote the classifier and SVD method used: the MFS column corresponds to the most frequent sense, k-NN-ORIG (SVMORIG) corresponds to performing k-NN (SVM) on the original feature space, k-NN-OMT (SVM-OMT) corresponds to k-NN (SVM) on the reduced dimensions of the OMT strategy, and k-NN-SMA (SVMSMA) corresponds to k-NN (SVM) on the reduced dimensions of the SMA strategy (cf.Section 4.2)." ></td>
	<td class="line x" title="127:210	The rows correspond to the matrix used for SVD (cf.Section 4.3)." ></td>
	<td class="line x" title="129:210	Note that some of the cells have no result, because that combination is not applicable, e.g. using the TRAIN  BNC in the original space." ></td>
	<td class="line x" title="130:210	In the first row (TRAIN) of Table 1 we can see that in both domains SVM on the original space outperforms k-NN with statistical signifi20 BNCSPORTS matrix configuration MFS k-NN-ORIG k-NN-OMT k-NN-SMA SVM-ORIG SVM-OMT SVM-SMA TRAIN 39.01.3 51.71.3 53.01.6 53.91.3 47.41.5 TRAINSPORTS 47.81.5 49.71.5 51.81.5 53.81.5 TRAINBNC 61.41.4 57.11.5 57.11.6 57.21.5 BNCFINANCES matrix configuration MFS k-NN-ORIG k-NN-OMT k-NN-SMA SVM-ORIG SVM-OMT SVM-SMA TRAIN 51.21.6 60.41.6 62.51.4 62.91.6 59.41.5 TRAINFINANCES 57.41.9 60.61.5 60.41.4 62.71.4 TRAINBNC 65.91.5 68.31.4 67.01.3 66.81.5 Table 1: Precision for the domain adaptation scenario: training on labeled source corpus, plus unlabeled corpora." ></td>
	<td class="line x" title="131:210	cance." ></td>
	<td class="line x" title="132:210	Those are the baseline systems." ></td>
	<td class="line x" title="133:210	On the same row, working on the reduced space of the TRAIN matrix with OMT allows to improve the results of k-NN, but not for SVM." ></td>
	<td class="line x" title="134:210	Contrary to our expectations, adding target unlabeled corpora (TRAINSPORTS and TRAINFINANCES rows respectively) does not improve the results over the baseline." ></td>
	<td class="line x" title="135:210	But using the source unlabeled data (TRAINBNC), we find that for both domains and in all four columns the results are significantly better than for the best baseline in both SPORTS and FINANCES corpora." ></td>
	<td class="line x" title="136:210	The best results on the TRAIN  BNC row depend on the domain corpus." ></td>
	<td class="line x" title="137:210	While k-NN-OMT obtains the best results for SPORTS, in FINANCES k-NN-SMA is best." ></td>
	<td class="line x" title="138:210	k-NN, in principle a weaker method that SVM, is able to attain the same or superior performance than SVM on the reduced spaces." ></td>
	<td class="line x" title="139:210	Table 3 summarizes the main results, and also shows the error reduction figures, which range between 6.9% and 16.3%." ></td>
	<td class="line x" title="140:210	As the most important conclusion, we want to stress that, in this scenario, we are able to build a very robust system just adding unlabeled source material, and that we fail to adapt to the domain using the target corpus." ></td>
	<td class="line x" title="141:210	These results are relevant to improve a generic WSD system to be more robust when ported to new domains." ></td>
	<td class="line x" title="142:210	5.2 Controlling size In the original experiments reported in the previous sections, the size of the unlabeled corpora was not balanced." ></td>
	<td class="line x" title="143:210	Due to the importance of the amount of unlabeled data, we performed two control experiments for the OMT and SMA matrices on the domain adaptation scenario, focusing on the k-NN method." ></td>
	<td class="line x" title="144:210	Regarding OMT, we used the minimum number of instances per word between BNC and each of the target domains." ></td>
	<td class="line x" title="145:210	The system obtained 60.0 of precision using unlabeled data from BNC and 49.5 for SPORTS data (compared to 61.4 and 47.8 in table 1, respectively)." ></td>
	<td class="line x" title="146:210	We did the same in the FINANCES domain, and we obtained 65.6 of precision for BNC and 54.4 for FINANCES (compared to 65.7 and 57.4 in table 1, respectively)." ></td>
	<td class="line x" title="147:210	Although the contribution of BNC unlabeled data is slightly lower in this experiment, due to the smaller amount of data, it still outperforms the target unlabeled data by a large margin." ></td>
	<td class="line x" title="148:210	In the case of the SMA matrix, we used 25% of the BNC, which is comparable to the SPORTS and FINANCES sizes." ></td>
	<td class="line x" title="149:210	The results, 56.9 of precision in SPORTS domain and 68.1 in FINANCES (compared to 57.1 and 68.3 in table 1, respectively), confirm that the size is not an important factor for SMA either." ></td>
	<td class="line x" title="150:210	6 Target scenario In this second scenario we focus on the target domain." ></td>
	<td class="line x" title="151:210	We train and test on the target domain, and use unlabeled data in order to improve the result." ></td>
	<td class="line x" title="152:210	The goal of these experiments is to check the behavior of our method when applied to the target domain, in order to better understand the results on the domain adaptation scenario." ></td>
	<td class="line x" title="153:210	They also provide an upperbound for semi-supervised domain adaptation." ></td>
	<td class="line x" title="154:210	6.1 Experimental results The results are presented in table 2." ></td>
	<td class="line x" title="155:210	All experiments in this section have been performed using 3-fold cross-validation." ></td>
	<td class="line x" title="156:210	Again, we have full coverage in all cases, and the significance ranges correspond to the 95% confidence level." ></td>
	<td class="line x" title="157:210	The table has two main parts, each regarding to one of the target domains, SPORTS and FINANCES." ></td>
	<td class="line x" title="158:210	As in Table 1, the columns specify the classifier and SVD method used, and the rows correspond to the matrices used 21 SPORTSSPORTS (xval) matrix configuration MFS k-NN-ORIG k-NN-OMT k-NN-SMA SVM-ORIG SVM-OMT SVM-SMA TRAIN 77.81.2 84.51.0 85.01.1 85.11.0 81.01.5 TRAINSPORTS 86.10.9 82.71.1 85.11.1 80.31.5 TRAINBNC 84.41.0 80.41.5 84.30.9 79.81.2 FINANCESFINANCES (xval) matrix configuration MFS k-NN-ORIG k-NN-OMT k-NN-SMA SVM-ORIG SVM-OMT SVM-SMA TRAIN 82.31.3 87.11.0 87.41.0 87.01.0 85.51.1 TRAINSPORTS 87.80.8 84.31.4 86.40.9 82.91.1 TRAINBNC 87.41.2 83.51.2 85.70.9 84.31.1 Table 2: Precision for the target scenario: training on labeled target corpora, plus unlabeled corpora." ></td>
	<td class="line x" title="159:210	to obtain the features." ></td>
	<td class="line x" title="160:210	Table 2 shows that k-NN-OMT using the target corpus (SPORTS and FINANCES, respectively) slightly improves over the k-NN-ORIG and SVMORIG classifiers, with significant difference in the SPORTS domain." ></td>
	<td class="line x" title="161:210	Contrary to the results on the previous section, the source unlabeled corpus degrades performance, but the target corpus does allow for small improvements." ></td>
	<td class="line x" title="162:210	Note that, in this scenario, both SVM and k-NN perform similarly in the original space, but only k-NN is able to profit from the reduced space." ></td>
	<td class="line x" title="163:210	Table 3 summarizes the best result, alongside the error reduction." ></td>
	<td class="line x" title="164:210	The results of these experiments allow to contrast both scenarios, and to get deeper insight about the relation between the labeled and unlabeled data when performing SVD, as we will examine in the next section." ></td>
	<td class="line x" title="165:210	7 Discussion The main contribution of this paper is to show that we obtain robustness when faced with domain shifts using a semi-supervised strategy." ></td>
	<td class="line x" title="166:210	We show that we can obtain it using a large, general, unlabeled corpus." ></td>
	<td class="line x" title="167:210	Note that our semi-supervised method to attain robustness for domain shifts is very cost-effective, as it does not require costly hand-tagged material nor even large numbers of unlabeled data from each target domain." ></td>
	<td class="line x" title="168:210	These results are more valuable given the lack of substantial positive results on the literature on semisupervised or supervised domain adaptation for WSD (Escudero et al., 2000; Martnez and Agirre, 2000; Chan and Ng, 2007)." ></td>
	<td class="line x" title="169:210	Compared to other settings, our semi-supervised results improve over the completely unsupervised system in (Koeling et al., 2005), which had 43.7% and 49.9% precision for the SPORTS and FINANCES domains respectively, but lag well behind the target domain scenario, showing that there is still room for improvement in the semi-supervised setting." ></td>
	<td class="line x" title="170:210	While these results are based on a lexical sample, and thus not directly generalizable to an allwords corpus, we think that they reflect the main trends for nouns, as the 41 nouns where selected among those exhibiting domain dependence (Koeling et al., 2005)." ></td>
	<td class="line x" title="171:210	We can assume, though it would be needed to be explored empirically, that other nouns exhibiting domain independence would degrade less when moving to other domains, and thus corroborate the robustness effect we have discovered." ></td>
	<td class="line x" title="172:210	The fact that we attain robustness rather than domain adaptation proper deserves some analysis." ></td>
	<td class="line x" title="173:210	In the domain adaptation scenario only source unlabeled data helped, but the results on the target scenario show that it is the target unlabeled data which is helping, and not the source one." ></td>
	<td class="line x" title="174:210	Given that SVD basically finds correlations among features, it seems that constructing the term-by-document (or feature-by-example) matrix with the training data and the unlabeled corpus related to the training data is the key factor in play here." ></td>
	<td class="line x" title="175:210	The reasons for this can be traced back as follows." ></td>
	<td class="line x" title="176:210	Our source corpus is the BNC, which is a balanced corpus containing a variety of genres and domains." ></td>
	<td class="line x" title="177:210	The 100 examples for each word that have been hand-tagged were gathered at random, and thus cover several domains." ></td>
	<td class="line x" title="178:210	For instance, the OMT strategy for building the matrix extracts hundreds of other examples from the BNC, and when SVD collapses the features into a reduced space, it effectively captures the most important correlations in the feature-by-example matrix." ></td>
	<td class="line x" title="179:210	When faced with examples from a new domain, the reduced matrix is able to map some of the features found in the test example to those in the train example." ></td>
	<td class="line x" title="180:210	Such overlap is more difficult if only 100 examples from the source domain are available." ></td>
	<td class="line x" title="181:210	22 SPORTS FINANCES sign." ></td>
	<td class="line x" title="182:210	E.R (%) method 53.91.3 62.91.6 labeled source (SVM-ORIG: baseline ) 57.11.5 68.31.4 ++ 6.9/14.5 labeled source + SVD on unlabeled source (k-NN-SMA) 61.41.4 65.91.5 ++ 16.3/8.1 labeled source + SVD on unlabeled source (k-NN-OMT) 85.11.0 87.01.0 labeled target (SVM-ORIG: baseline) 86.10.9 87.80.8 + 6.7/6.1 labeled target + SVD on unlabeled target (k-NN-OMT) Table 3: Summary with the most important results for the two scenarios (best results for each in bold)." ></td>
	<td class="line x" title="183:210	The significance column shows significance over baselines: ++ (significant in both target domains), + (significant in a single domain)." ></td>
	<td class="line x" title="184:210	The E.R column shows the error reduction in percentages over the baseline methods." ></td>
	<td class="line x" title="185:210	The unlabeled data and SVD process allow to capture correlations among the features occurring in the test data and those in the training data." ></td>
	<td class="line x" title="186:210	On the other hand, we are discarding all original features, as we focus on the features from the reduced space alone." ></td>
	<td class="line x" title="187:210	The newly found correlations come at the price of possibly ignoring effective original features, causing information loss." ></td>
	<td class="line x" title="188:210	Only when the correlations found in the reduced space outweigh this information loss do we get better performance on the reduced space than in the original space." ></td>
	<td class="line x" title="189:210	The experiment in Section 6 is important in that it shows that the improvement is much smaller and only significant in the target domain scenario, which is in accordance with the hypothesis above." ></td>
	<td class="line x" title="190:210	This information loss is a motivation for the combination of the features from the reduced space with the original features, which will be the focus of our future work." ></td>
	<td class="line x" title="191:210	Regarding the learning method and the two strategies to apply SVD, the results show that kNN profits from the reduced spaces more than SVM, even if its baseline performance is lower than SVM." ></td>
	<td class="line x" title="192:210	Regarding the matrix building system, in the domain adaptation scenario, k-NN-OMT obtains the best results (with statistical significance) in the SPORTS corpus, and k-NN-SMA yields the best results (with statistical significance) in the FINANCES domain." ></td>
	<td class="line x" title="193:210	Averaging over both domains, k-NN-OMT is best." ></td>
	<td class="line x" title="194:210	The target scenario results confirm this trend, as k-NN-OMT is superior to k-NNSMA in both domains." ></td>
	<td class="line x" title="195:210	These results are in accordance with our previous experience on WSD (Agirre et al., 2005), where our OMT method got better results than SMA and those of (Gliozzo et al., 2005) (who also use a method similar to SMA) on the Senseval-3 lexical sample." ></td>
	<td class="line x" title="196:210	While OMT reduces the feature-by-example matrix of each target word, SMA reduces a single term-by-document matrix." ></td>
	<td class="line x" title="197:210	SMA is able to find important correlations among similar terms in the corpus, but it misses the rich feature set used by WSD systems, as it focuses on bag-of-words alone." ></td>
	<td class="line x" title="198:210	OMT on the other hand is able to find correlations between all features which are relevant to the target word only." ></td>
	<td class="line x" title="199:210	8 Conclusions and Future Work In this paper we explore robustness and domain adaptation issues for Word Sense Disambiguation using SVD and unlabeled data." ></td>
	<td class="line x" title="200:210	We focus on the semi-supervised scenario, where we train on the source corpus (BNC), test on two target corpora (SPORTS and FINANCES sections of Reuters), and improve the results using unlabeled data." ></td>
	<td class="line x" title="201:210	Our method yields up to 16.3% error reduction compared to SVM and k-NN on the labeled data alone, showing the first positive results on domain adaptation for WSD." ></td>
	<td class="line x" title="202:210	In fact, we show that our results are due to the use of a large, general, unlabeled corpus, and rather than domain-adaptation proper we show robustness in face of a domain shift." ></td>
	<td class="line x" title="203:210	This kind of robustness is even more costeffective than semi-supervised domain adaptation, as it does not require large unlabeled corpora and repeating the computations for each new target domain." ></td>
	<td class="line x" title="204:210	This paper shows that the OMT technique to apply SVD that we proposed in (Agirre et al., 2005) compares favorably to SMA, which has been previously used in (Gliozzo et al., 2005), and that k-NN excels SVM on the features from the reduced space." ></td>
	<td class="line x" title="205:210	We also show that the unlabeled data needs to be related to the training data, and that the benefits of our method are larger when faced with a domain shift (compared to test data coming from the same domain as the training data)." ></td>
	<td class="line x" title="206:210	In the future, we plan to combine the features from the reduced space with the rest of features, either using a combination of k-NN classifiers (Agirre et al., 2005; Agirre and Lopez de Lacalle, 2007) or a complex kernel (Gliozzo et al., 2005)." ></td>
	<td class="line x" title="207:210	23 A natural extension of our work would be to apply our techniques to the supervised domain adaptation scenario." ></td>
	<td class="line x" title="208:210	Acknowledgments We wish to thank Diana McCarthy and Rob Koeling for kindly providing us the Reuters tagged corpora, David Martnez for helping us with the learning features, and Walter Daelemans for his advice on domain adaptation." ></td>
	<td class="line x" title="209:210	Oier Lopez de Lacalle has a PhD grant from the Basque Government." ></td>
	<td class="line x" title="210:210	This work is partially funded by the Education Ministry (KNOW TIN2006-15049, OpenMT TIN2006-15307-C03-02) and the Basque Country University (IT-397-07)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1015
Learning Reliable Information for Dependency Parsing Adaptation
Chen, Wenliang;Wu, Youzheng;Isahara, Hitoshi;"></td>
	<td class="line x" title="1:255	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 113120 Manchester, August 2008 Learning Reliable Information for Dependency Parsing Adaptation Wenliang Chen, Youzheng Wu, Hitoshi Isahara Language Infrastructure Group Spoken Language Communication Group, ATR Machine Translation Group National Institute of Information and Communications Technology 3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289 {chenwl, youzheng.wu, isahara}@nict.go.jp Abstract In this paper, we focus on the adaptation problem that has a large labeled data in the source domain and a large but unlabeled data in the target domain." ></td>
	<td class="line x" title="2:255	Our aim is to learn reliable information from unlabeled target domain data for dependency parsingadaptation." ></td>
	<td class="line x" title="3:255	Currentstate-of-the-artstatistical parsers perform much better for shorter dependencies than for longer ones." ></td>
	<td class="line x" title="4:255	Thus we propose an adaptation approach by learning reliable information on shorter dependencies in an unlabeled target data to help parse longer distance words." ></td>
	<td class="line x" title="5:255	The unlabeled data is parsed by a dependency parser trained on labeled source domain data." ></td>
	<td class="line x" title="6:255	The experimental results indicate that our proposed approach outperforms the baseline system, and is better than current state-of-the-art adaptation techniques." ></td>
	<td class="line x" title="7:255	1 Introduction Dependency parsing aims to build the dependency relations between words in a sentence." ></td>
	<td class="line x" title="8:255	There are many supervised learning methods for training high-performance dependency parsers(Nivre et al., 2007), if given sufficient labeled data." ></td>
	<td class="line x" title="9:255	However, theperformanceofparsersdeclineswhenwearein the situation that a parser is trained in one source domain but is to parse the sentences in a second target domain." ></td>
	<td class="line oc" title="10:255	There are two tasks(Daume III, 2007) for the domain adaptation problem." ></td>
	<td class="line x" title="11:255	The first one is that we have a large labeled data in the source domain and a small labeled data in target c2008." ></td>
	<td class="line x" title="12:255	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="13:255	Some rights reserved." ></td>
	<td class="line x" title="14:255	domain." ></td>
	<td class="line x" title="15:255	The second is similar, but instead of having a small labeled target data, we have a large but unlabeled target data." ></td>
	<td class="line x" title="16:255	In this paper, we focus on the latter one." ></td>
	<td class="line x" title="17:255	Current statistical dependency parsers perform worse while the distance of two words is becoming longer for domain adaptation." ></td>
	<td class="line x" title="18:255	An important characteristic of parsing adaptation is that the parsers perform much better for shorter dependencies than forlongerones(thescoreatlength l ismuchhigher than the scores at length> l )." ></td>
	<td class="line x" title="19:255	In this paper, we propose an approach by using the information on shorter dependencies in autoparsed target data to help parse longer distance words for adapting a parser." ></td>
	<td class="line x" title="20:255	Compared with the adaptation methods of Sagae and Tsujii (2007) and Reichart and Rappoport (2007), our approach uses the information on word pairs in auto-parsed data instead of using the whole sentences as newly labeled data for training new parsers." ></td>
	<td class="line x" title="21:255	It is difficult to detect reliable parsed sentences, but we can find relative reliable parsed word pairs according to dependency length." ></td>
	<td class="line x" title="22:255	The experimental results show that our approach significantly outperforms baseline system and current state of the art techniques." ></td>
	<td class="line x" title="23:255	2 Motivation and prior work In dependency parsing, we assign head-dependent relations between words in a sentence." ></td>
	<td class="line x" title="24:255	A simple example is shown in Figure 1, where the arc between a and hat indicates that hat is the head of a. Current statistical dependency parsers perform better if the dependency lengthes are shorter (McDonald and Nivre, 2007)." ></td>
	<td class="line x" title="25:255	Here the length of the dependency from word wi to word wj is simply equal to |i  j|." ></td>
	<td class="line x" title="26:255	Figure 2 shows the results (F1 113 The  boy  saw    a       red       hat    . Figure 1: An example for dependency relations." ></td>
	<td class="line x" title="27:255	20  30  40  50  60  70  80  90  100  0  2  4  6  8  10  12  14  16  18  20 F1 Dependency Length sameDomain diffDomain Figure2: Thescoresrelativetodependencylength." ></td>
	<td class="line x" title="28:255	SameDomain refers to training and testing in the same domain, and diffDomain refers to training and testing in two domains (domain adaptation)." ></td>
	<td class="line x" title="29:255	score)1 on our testing data, provided by a deterministic parser, which is trained on labeled source data." ></td>
	<td class="line x" title="30:255	Comparing two curves at the figure, we find thatthescoresofdiffDomaindecreasesmuchmore sharply than the scores of sameDomain, when dependency length increases." ></td>
	<td class="line x" title="31:255	The score decreases from about 92% at length 1 to 50% at 7." ></td>
	<td class="line x" title="32:255	When lengthes are larger than 7, the scores are below 50%." ></td>
	<td class="line x" title="33:255	Wealsofind thatthescoreat length l ismuch higher (around 10%) than the score at length l + 1 fromlength1to7." ></td>
	<td class="line x" title="34:255	Thereisonlyoneexceptionthat the score at length 4 is a little less than the score at length 5." ></td>
	<td class="line x" title="35:255	But this does not change so much and the scores at length 4 and 5 are much higher than the one at length 6." ></td>
	<td class="line x" title="36:255	Two words (word wi and word wj) having a dependency relation in one sentence can be adjacent words (word distance = 1), neighboring words (word distance = 2), or the words with distance > 2 in other sentences." ></td>
	<td class="line x" title="37:255	Here the distance of word pair (word wi and word wj) is equal to |i j|." ></td>
	<td class="line x" title="38:255	For example, a and hat has dependency relation in the sentence at Figure 1." ></td>
	<td class="line x" title="39:255	They can also be adjacent words in the sentence The boy saw a hat. and the words with distance = 3 in I see a red beautiful hat.." ></td>
	<td class="line x" title="40:255	This makes it possible for the word pairs with different distances to share the information." ></td>
	<td class="line x" title="41:255	Accordingtotheaboveobservations, wepresent 1F1 = 2  precision  recall/(precision + recall) where precision is the percentage of predicted arcs of length d that are correct and recall is the percentage of gold standard arcs of length d that are correctly predicted." ></td>
	<td class="line x" title="42:255	an idea that the information on shorter dependencies in auto-parsed target data is reliable for parsing the words with longer distance for domain adaptation." ></td>
	<td class="line x" title="43:255	Here, shorter is not exactly short." ></td>
	<td class="line x" title="44:255	That is to say, the information on dependency length l in auto-parsed data can be used to help parse the words whose distances are longer than l when testing, where l can be any number." ></td>
	<td class="line x" title="45:255	We do not use the dependencies whose lengthes are too long because the accuracies of long dependencies are very low." ></td>
	<td class="line x" title="46:255	In the following content, we demonstrate our idea with an example." ></td>
	<td class="line x" title="47:255	The example shows how to use the information on length 1 to help parse two words whose distance is longer than 1." ></td>
	<td class="line x" title="48:255	Similarly, the information on length l can also be used to help parse the words whose distance is longer than l. Figure 2 shows that the dependency parser performs best at tagging the relations between adjacent words." ></td>
	<td class="line x" title="49:255	Thus, we expect that dependencies of adjacent words in auto-parsed target data can provide useful information for parsing words whose distances are longer than 1." ></td>
	<td class="line x" title="50:255	We suppose that our task is Chinese dependency parsing adaptation." ></td>
	<td class="line x" title="51:255	Here, we have two words vJJ(large-scale) and Z	NN(exhibition)." ></td>
	<td class="line x" title="52:255	Figure 3 shows the examples in which word distances of these two words are different." ></td>
	<td class="line x" title="53:255	For the sentences in the bottom part, there is a ambiguity of JJ + NN1 + NN2 at vJJ(large-scale)/\NN(art)/Z	NN(exhibition), vJJ(largescale)/NN(culture)/\NN(art)/Z 	NN(exhibition) and vJJ(large-scale)/ SNR(China)/NN(culture)/\NN(art)/Z 	NN(exhibition)." ></td>
	<td class="line x" title="54:255	Both NN1 and NN2 could be the head of JJ." ></td>
	<td class="line x" title="55:255	In the examples in the upper part, vJJ(large-scale) and Z	NN(exhibition) are adjacent words, for which current parsers can work well." ></td>
	<td class="line x" title="56:255	We use a parser to parse the sentences in the upper part." ></td>
	<td class="line x" title="57:255	Z	(exhibition) is assigned as the head of v(large-scale)." ></td>
	<td class="line x" title="58:255	Then we expect the information from the upper part can help parse the sentences in the bottom part." ></td>
	<td class="line x" title="59:255	Now, we consider what a learning model could do to assign the appropriate relation between v (large-scale) and Z	(exhibition) in the bottom part." ></td>
	<td class="line x" title="60:255	We provide additional information that Z	(exhibition) is the possible head of v (large-scale) in the auto-parsed data (the upper part)." ></td>
	<td class="line x" title="61:255	In this way, the learning model may use this information to make correct decision." ></td>
	<td class="line x" title="62:255	114 A1)g1039g2162g7053g17836g16786g16757g2372g2058g1114g457g10988g13911g12946g12082/g3835g3423/g4649g16284/g458g4471g1268g6252g20041 A2)g8504g8437/g3835g3423/g4649g16284/g11013g9005g8755g11477g4008g4471g1268g18108 B1)g1328g2709g1849g17885g3281g1881g3818/g3835g3423/g14414g7427/g4649g16284/g712g4661g14731g3882g20045g17829g1128g2325g20045 B3)g3247g5041g2488g1207g7003g2282g14414g7427g451g17220g2533g11439g2788g12573/g3835g3423/g1025g3281/g7003g2282/g14414g7427/g4649g16284/g452 B2)g3324g2283g1152g1042g15904g11352g5064g16211/g3835g3423/g7003g2282/g14414g7427/g4649g16284/g452 Figure 3: Examples for v(large-scale) and Z	(exhibition)." ></td>
	<td class="line x" title="63:255	The upper part (A) refers to the sentences from unlabeled data and the bottom part (B) refers to the sentences waiting for parsing." ></td>
	<td class="line x" title="64:255	Up to now, we demonstrate how to use the information on length 1." ></td>
	<td class="line x" title="65:255	Similarly, we can use the information on length 2, 3,  By this way, we propose an approach by exploiting the information from a large-scale unlabeled target data for dependency parsing adaptation." ></td>
	<td class="line x" title="66:255	In this paper, our approach is to use unlabeled data for parsing adaptation." ></td>
	<td class="line x" title="67:255	There are several studies relevant to ours as described below." ></td>
	<td class="line x" title="68:255	CoNLL 2007(Nivre et al., 2007) organized a shared task for domain adaptation without annotated data in new domain." ></td>
	<td class="line x" title="69:255	The labeled data was from the Wall Street Journal, the development data wasfrom biomedical abstracts, and the testing data was from chemical abstracts and parent-child dialogues." ></td>
	<td class="line x" title="70:255	Additionally, a large unlabeled corpus was provided." ></td>
	<td class="line x" title="71:255	The systems by Sagae and Tsujii (2007), Attardi et al.(2007), and Dredze et al.(2007) performed top three in the shared task." ></td>
	<td class="line x" title="74:255	Sagae and Tsujii (2007) presented a procedure similar to a single iteration of co-training." ></td>
	<td class="line x" title="75:255	Firstly, they trained two parsers on labeled source data." ></td>
	<td class="line x" title="76:255	Then the two parsers were used to parse the sentences in unlabeled data." ></td>
	<td class="line x" title="77:255	They selected only identical parsing results produced by the two parsers." ></td>
	<td class="line x" title="78:255	Finally, they retrained a parser on newly parsed sentences and the original labeled data." ></td>
	<td class="line x" title="79:255	They performed the highest scores for this track." ></td>
	<td class="line x" title="80:255	Attardi et al.(2007) presented a procedure with correcting errors by a revision techniques." ></td>
	<td class="line x" title="82:255	Dredze et al.(2007) submitted parsing results without adaptation." ></td>
	<td class="line x" title="84:255	They declared that it was difficult to significantly improve performance on any test domain beyond that of a state-of-the-art parser." ></td>
	<td class="line x" title="85:255	Their error analysis suggested that the primary cause of loss from adaptation is because of differences in the annotation guidelines." ></td>
	<td class="line x" title="86:255	Without specific knowledge of the target domains annotation standards, significant improvement can not be made." ></td>
	<td class="line x" title="87:255	Reichart and Rappoport (2007) studied selftraining method for domain adaptation (The WSJ data and the Brown data) of phrase-based parsers." ></td>
	<td class="line x" title="88:255	McClosky et al.(2006) presented a successful instance of parsing with self-training by using a reranker." ></td>
	<td class="line x" title="90:255	Both of them used the whole sentences as newly labeled data for adapting the parsers, while our approach uses the information on word pairs." ></td>
	<td class="line x" title="91:255	Chen et al.(2008) presented an approach by using the information of adjacent words for indomain parsing." ></td>
	<td class="line x" title="93:255	As Figure 2 shows, the score curves of sameDomain (in-domain) parsing and diffDomain (out-domain) parsing are quite different." ></td>
	<td class="line x" title="94:255	Our work focuses on parsing adaptation and is basedonthefactthatcurrentparsersperformmuch better for shorter dependencies than for longer ones." ></td>
	<td class="line x" title="95:255	This causes that our work differs in that we use the information on shorter dependencies in auto-parsed target data to help parse the words with longer distance for parsing adaptation." ></td>
	<td class="line x" title="96:255	In this paper, shorter and longer are relative." ></td>
	<td class="line x" title="97:255	Length l is relatively shorter than length l + 1, where l can be any number." ></td>
	<td class="line x" title="98:255	3 The parsing approach In this paper, we choose the model described by Nivre (2003) as our parsing model." ></td>
	<td class="line x" title="99:255	It is a deterministic parser and works quite well in the sharedtask of CoNLL2006(Nivre et al., 2006)." ></td>
	<td class="line x" title="100:255	3.1 The parsing model The Nivre (2003) model is a shift-reduce type algorithm, which uses a stack to store processed tokens and a queue to store remaining input tokens." ></td>
	<td class="line x" title="101:255	It can perform dependency parsing in O(n) time." ></td>
	<td class="line x" title="102:255	The dependency parsing tree is built from atomic actions in a left-to-right pass over the input." ></td>
	<td class="line x" title="103:255	The parsing actions are defined by four operations: Shift, Reduce, Left-Arc, and Right-Arc, for the stack and the queue." ></td>
	<td class="line x" title="104:255	TOP is the token on top of the stack and NEXT is next token in the queue." ></td>
	<td class="line x" title="105:255	The Left-Arc and Right-Arc operations mean that there is a dependency relation between TOP and NEXT." ></td>
	<td class="line x" title="106:255	The model uses a classifier to produce a sequence of actions for a sentence." ></td>
	<td class="line x" title="107:255	In this paper, we use the SVM model." ></td>
	<td class="line x" title="108:255	And LIBSVM(Chang and Lin, 2001) is used in our experiments." ></td>
	<td class="line x" title="109:255	Note that the approach (see section 4)we present in this paper can also be applied to 115 other parsers, such as the parser by Yamada and Matsumoto (2003), or the one by McDonald et al.(2006)." ></td>
	<td class="line x" title="111:255	3.2 Parsing with basic features The parser is a history-based parsing model, which relies on features of the parsed tokens to predict next parsing action." ></td>
	<td class="line x" title="112:255	We represent basic features based on words and part-of-speech (POS) tags." ></td>
	<td class="line x" title="113:255	The basic features are listed as follows:  Lexical Features on TOP: the word of TOP, the word of the head of TOP, and the words of leftmost and rightmost dependent of TOP." ></td>
	<td class="line x" title="114:255	 Lexical Features on NEXT: the word of NEXT and the word of the token immediately after NEXT in the original input string." ></td>
	<td class="line x" title="115:255	 POS features on TOP: the POS of TOP, the POS of the token immediately below TOP, and the POS of leftmost and rightmost dependent of TOP." ></td>
	<td class="line x" title="116:255	 POS features on NEXT: the POS of NEXT, the POS of next three tokens after NEXT, and the POS of the token immediately before NEXT in original input string." ></td>
	<td class="line x" title="117:255	Basedontheaboveparsingmodelandbasicfeatures, we train a basic parser on annotated source data." ></td>
	<td class="line x" title="118:255	In the following content, we call this parser Basic Parser." ></td>
	<td class="line x" title="119:255	4 Domain adaptation with shorter dependency This section presents our adaptation approach by using the information based on relative shorter dependencies in auto-parsed data to help parse the words whose distances are longer." ></td>
	<td class="line x" title="120:255	Firstly, we use the Basic Parser to parse all the sentences in unlabeled target data." ></td>
	<td class="line x" title="121:255	Then we explore reliable information based on dependency relations in autoparsed data." ></td>
	<td class="line x" title="122:255	Finally, we incorporate the features based on reliable information into the parser to improve performance." ></td>
	<td class="line x" title="123:255	4.1 Extracting word pairs from auto-parsed data In this section, we collect word pairs from the auto-parsed data." ></td>
	<td class="line x" title="124:255	At first, we collect the word pairs with length 1." ></td>
	<td class="line x" title="125:255	In a parsed sentence, if two words have dependency relation and their word distance is 1, we will add this word pair into the list Ldep and count its frequency." ></td>
	<td class="line x" title="126:255	We also consider the direction, LA for left arc and RA for right arc. For example, v(large-scale) and Z	(exhibition) are adjacent words in the sentence  (We)/	(held)/v(large-scale)/Z 	(exhibition)/b and have a left dependency arc assigned by the Basic Parser." ></td>
	<td class="line x" title="127:255	The word pair v(large-scale)-Z	(exhibition) with LA is added into Ldep." ></td>
	<td class="line x" title="128:255	Similarly, we collect the pairs whose word distances are longer than 1." ></td>
	<td class="line x" title="129:255	In Ldep, with length l and direction dr(LA or RA), the pair pu has freql(pu : dr)." ></td>
	<td class="line x" title="130:255	For example, freq2(pu : LA) = 3 refers to the word pair pu with left arc(LA) occurs 3 times in the auto-parsed data when two words distance is 2." ></td>
	<td class="line x" title="131:255	Because figure 2 shows that the accuracies of long dependencies are low, we only collect the pairs whose distances are not larger than a predefined length lmax." ></td>
	<td class="line x" title="132:255	4.2 The adaptation approach The word pair pt is the pair < wi,wj >." ></td>
	<td class="line x" title="133:255	4.2.1 The information on shorter distances If the distance of pt is d, we will use the pairs whose lengthes are less than d. It results in the words with different distances using different set of word pairs in Ldep." ></td>
	<td class="line x" title="134:255	For example, if d is 5, we can use the pairs with dependency lengthes from 1 to 4 in Ldep." ></td>
	<td class="line x" title="135:255	The information is represented by the equation as follows: Id(pt : dr) =    0 pt nelement Ldep freq1(pt : dr) d = 1summationtext d1 l=1 freql(pt : dr) d > 1 (1) 4.2.2 Classifying into buckets According to Id(pt : dr), word pairs are grouped into different buckets as follows: Bucketd(pt : dr) =      B0 Id(pt : dr) = 0 B1 0 < Id(pt : dr)  f1  Bn fn1 < Id(pt : dr)  fn Ba fn < Id(pt : dr) (2) where, f1, f2,, fn are the thresholds." ></td>
	<td class="line x" title="136:255	For example, I3(v-Z	:LA) is 20, f3 = 15 and f4 = 25." ></td>
	<td class="line x" title="137:255	Then it is grouped into the bucket B4." ></td>
	<td class="line x" title="138:255	We set f1 = 2, f2 = 8, and f3 = 15 in the experiments." ></td>
	<td class="line x" title="139:255	4.2.3 Parsing with the adapting Features Basedonthebucketsofwordpairs,werepresent new features on labeled source data for the parser." ></td>
	<td class="line x" title="140:255	We call these new features adapting features." ></td>
	<td class="line x" title="141:255	According to different word distances between TOP and NEXT, the features are listed at Table 1." ></td>
	<td class="line x" title="142:255	So we have 8 types of the adapting features, including 2 types for distance=1, 3 types for distance=2, and 3 types for distance3." ></td>
	<td class="line x" title="143:255	Each feature is formatted as DistanceType:FeatureType:Bucket, where DistanceType is D1, D2, or D3 corresponding to 116 three distances, FeatureType is FB0, FB1, or FB 1 corresponding to three positions." ></td>
	<td class="line x" title="144:255	Here, if a word pairhastwodependencydirectionsin Ldep,wewill choose the direction having higher frequency." ></td>
	<td class="line x" title="145:255	Then using the parsing model of Nivre (2003), we train a new parser based on the adapting features and basic features." ></td>
	<td class="line x" title="146:255	distance FB 1 FB0 FB1 =1 + + =2 + + + 3 + + + Table 1: Adapting features." ></td>
	<td class="line x" title="147:255	FB0 refers to the bucket of the word pair of TOP and NEXT, FB1 refers to the bucket of the word pair of TOP and next token after NEXT, and FB 1 refers to the bucket of the word pair of TOP and the token immediately before NEXT." ></td>
	<td class="line x" title="148:255	+ refers to this item having this type of feature." ></td>
	<td class="line x" title="149:255	4.2.4 An example We show an example for representing the adapting features." ></td>
	<td class="line x" title="150:255	For example, we have the string vJJ(large-scale)/NN(culture)/\ NN(art)/Z	NN(exhibition)/b." ></td>
	<td class="line x" title="151:255	And v (large-scale) is TOP and Z	(exhibition) is NEXT." ></td>
	<td class="line x" title="152:255	Because the distance of TOP and NEXT is 3, we have three features." ></td>
	<td class="line x" title="153:255	We suppose that (FB0) the bucket of the word pair (v-Z	) of TOP and NEXT is bucket B4, (FB1) the bucket of the word pair (v-b) of TOP and next token after NEXT is bucket B0, and (FB 1) the bucket of the word pair (v-\)of TOP and the token immediately before NEXT is bucket B1." ></td>
	<td class="line x" title="154:255	Then, wehavethefeatures: D3:FB0:B4, D3:FB1:B0, and D3:FB 1:B1." ></td>
	<td class="line x" title="155:255	4.3 Adaptation for unknown word2 The unknown word problem is an important issue for domain adaptation(Dredze et al., 2007)." ></td>
	<td class="line x" title="156:255	Our approach can work for improving performance of parsing unknown word pairs in which there is at least one unknown word." ></td>
	<td class="line x" title="157:255	We collect word pairs including unknown word pairs at Section 4.1." ></td>
	<td class="line x" title="158:255	Then unknown word pairs in testing data are also mapped into one of the buckets via Equation (2)." ></td>
	<td class="line x" title="159:255	So known word pairs can share the features with unknown word pairs." ></td>
	<td class="line x" title="160:255	2An unknown word is a word that is not included in training data." ></td>
	<td class="line x" title="161:255	5 Experimental setup CoNLL 2007(Nivre et al., 2007) organized the domain adaptation task and provided a data set in English." ></td>
	<td class="line x" title="162:255	However, the data set had differences between the annotation guidelines in source and target domains." ></td>
	<td class="line x" title="163:255	Without specific knowledge of the target domains annotation standards, significant improvement can not be made(Dredze et al., 2007)." ></td>
	<td class="line x" title="164:255	Inthispaper, wediscussedthesituationthat the data of source and target domains were annotated under the same annotation guideline." ></td>
	<td class="line x" title="165:255	So we used a data set converted from Penn Chinese Treebank (CTB)3." ></td>
	<td class="line x" title="166:255	Labeled data: the CTB(V5.0) was used in our experiments." ></td>
	<td class="line x" title="167:255	The data set was converted by the same rules for conversion as Chen et al.(2008) did." ></td>
	<td class="line x" title="169:255	We used files 1-270, 400-554, and 600-931 as source domain training data (STrain), files 271300 as source domain testing data (STest) and files 590-596 as target domain testing data (TTest)." ></td>
	<td class="line x" title="170:255	We used the gold standard segmentation and POS tags intheCTB.ThetargetdomaindatawasfromSinorama magazine, Taiwan and the source domain data was mainly from Xinhua newswire, mainland of China." ></td>
	<td class="line x" title="171:255	The genres of these two parts were quite different." ></td>
	<td class="line x" title="172:255	Table 2 shows the statistical information of the data sets." ></td>
	<td class="line x" title="173:255	Given the words of the STrain data, TTest included 30.79% unknown words." ></td>
	<td class="line x" title="174:255	We also checked the distribution of POS tags." ></td>
	<td class="line x" title="175:255	The difference was large, too." ></td>
	<td class="line x" title="176:255	Unlabeled data: three data sets were used in our experiments, including the PFR data (5.44M words), the CKIP data (5.44M words), and the SINO data (25K words)." ></td>
	<td class="line x" title="177:255	The PFR corpus4 included the documents from Peoples Daily at 1998 and we used about 1/3 of all sentences." ></td>
	<td class="line x" title="178:255	The CKIP5 corpus was used for SIGHAN word segmentation bakeoff 2005." ></td>
	<td class="line x" title="179:255	To simplify, we used their segmentation." ></td>
	<td class="line x" title="180:255	The SINO data was the files 1001-1151 of CTB, also from Sinorama magazine, the same as our testing target data." ></td>
	<td class="line x" title="181:255	We removed the annotation tags from the SINO data." ></td>
	<td class="line x" title="182:255	Among the three unlabeled data, the SINO data was closest to testing target data because they came from the same resource." ></td>
	<td class="line x" title="183:255	Table 2 lists the information of data sets." ></td>
	<td class="line x" title="184:255	From the table, we found that the PFR data was 3More detailed information can be found at http://www.cis.upenn.edu/chinese/." ></td>
	<td class="line x" title="185:255	4More detailed information can be found at http://www.icl.pku.edu." ></td>
	<td class="line x" title="186:255	5More detailed information can be found at http://rocling.iis.sinica.edu.tw/CKIP/index.htm 117 Num Of Words Unknown Word Rate STrain 17983 STest 1829 9.73 TTest 1783 30.79 CKIP 140k STest 1829 11.42 TTest 1783 8.63 PFR 123k STest 1829 8.58 TTest 1783 15.64 Table 2: The information of the data sets closer to source domain and the CKIP data was closer to target domain." ></td>
	<td class="line x" title="187:255	To assign POS tags for the unlabeled data, we used the package TNT (Brants, 2000) to train a POS tagger on training data." ></td>
	<td class="line x" title="188:255	Because the PFR data and the CTB used different POS standards, we did not use the POS tags in the PFR data." ></td>
	<td class="line x" title="189:255	We measured the quality of the parser by the unlabeled attachment score (UAS), i.e., the percentage of tokens with correct head." ></td>
	<td class="line x" title="190:255	We also reported the accuracy of ROOT." ></td>
	<td class="line x" title="191:255	6 Results In the following content, OURS refers to our proposed approach." ></td>
	<td class="line x" title="192:255	The baseline system refers to the Basic Parser." ></td>
	<td class="line x" title="193:255	6.1 Basic experiments In this section, we examined the performance of baseline systems and our proposed approach with different unlabeled data sets." ></td>
	<td class="line x" title="194:255	Table 3 shows the experimental results, where OURS with SINO(GOLD) refers to the parser using gold standard POS tags, and OURS with SINO(AUTO) refers to the parser using autoassigned POS tags." ></td>
	<td class="line x" title="195:255	From the two results of baseline, we found that the parser performed very differently in two domains by 8.24%." ></td>
	<td class="line x" title="196:255	With the help of SINO(AUTO), OURS provided 1.11% improvement for UAS and 6.16% for ROOT." ></td>
	<td class="line x" title="197:255	If we used gold standard POS tags, the score was 78.40% for UAS (1.34% improvement), and 65.40% for ROOT (6.64% improvement)." ></td>
	<td class="line x" title="198:255	By using the SINO data, our approach achieved significant improvements over baseline system." ></td>
	<td class="line x" title="199:255	It was surprised that OURS with CKIP achieved 78.30% score, just a little lower than the one with SINO(GOLD)." ></td>
	<td class="line x" title="200:255	The reason may be that the size of the CKIP data was much bigger than the SINO data." ></td>
	<td class="line x" title="201:255	So we can obtain more word pairs from the CKIP data." ></td>
	<td class="line x" title="202:255	The parser achieved 0.30% Data UAS ROOT baseline(STest) 85.30 88.21 baseline(TTest) 77.06 58.76 OURS with SINO(GOLD) 78.40(+1.34) 65.40 OURS with SINO(AUTO) 78.17(+1.11) 64.92 OURS with CKIP 78.30(+1.24) 65.87 OURS with PFR 77.36(+0.30) 63.03 Table 3: Basic results lmax SINO(GOLD) SINO(AUTO) 1 77.84 77.80 3 78.03 77.95 5 78.22 78.17 7 78.40 78.11 9 78.38 78.13  78.35 78.09 Table 4: The effect of different lmax improvement with PFR." ></td>
	<td class="line x" title="203:255	Even though the size of the SINO data was smaller, the parser performed well with its help." ></td>
	<td class="line x" title="204:255	These results indicated that we should collect the unlabeled data that is closer to target domain or larger." ></td>
	<td class="line x" title="205:255	The improvements of OURS with CKIP and OURS with SINO were significant in one-tail paired t-test (p < 105)." ></td>
	<td class="line x" title="206:255	6.2 The effect of different lmax Table 4 shows the experimental results, where lmax is described at Section 4.1." ></td>
	<td class="line x" title="207:255	With SINO(GOLD), our parser performed best at lmax = 7." ></td>
	<td class="line x" title="208:255	And with SINO(AUTO), it performed best at lmax = 5." ></td>
	<td class="line x" title="209:255	These indicated that our approach can incorporate pairs with different lengthes to improve performance." ></td>
	<td class="line x" title="210:255	We also found that the long dependencies were not reliable, as the curve (diffDomain) of Figure 2 showed that the scores were less than 50% when lengthes were larger than 8." ></td>
	<td class="line x" title="211:255	6.3 Comparison of other systems In this section, we turned to compare our approach with other methods." ></td>
	<td class="line x" title="212:255	We implemented two systems: SelfTrain and CoTrain." ></td>
	<td class="line x" title="213:255	The SelfTrain system was following to the method described by Reichart and Rappoport (2007) and randomly selected new auto-parsed sentences." ></td>
	<td class="line x" title="214:255	The CoTrain system was similar to the learning scheme described by Sagae and Tsujii (2007)." ></td>
	<td class="line x" title="215:255	However, we did not use the same parsing algorithms as the ones used by Sagae and Tsujii (2007)." ></td>
	<td class="line x" title="216:255	Firstly, we 118 Method UAS ROOT baseline 77.06 58.76 SelfTrain 77.44 60.18 CoTrain 77.57 60.81 OURS 78.30 65.87 Table 5: The results of several adaptation methods with CKIP trained a forward parser (same as our baseline system) and a backward parser." ></td>
	<td class="line x" title="217:255	Then the identical parsed sentences by the two parsers were selected as newly labeled data." ></td>
	<td class="line x" title="218:255	Finally, we retrained a forward parser with new training data." ></td>
	<td class="line x" title="219:255	We selected the sentences having about 200k words from the CKIP data as newly labeled data for the SelfTrain and CoTrain systems." ></td>
	<td class="line x" title="220:255	Table 5 shows the experimental results." ></td>
	<td class="line x" title="221:255	Both systems provided about 0.4%-0.5% improvement over baseline system." ></td>
	<td class="line x" title="222:255	Our approach performed best among all systems." ></td>
	<td class="line x" title="223:255	Another problem was thatthe time for training the SelfTrainand CoTrain systems became much longer because they almost used double size of training data." ></td>
	<td class="line x" title="224:255	7 Analysis In this section, we try to understand the benefit in our proposed adaptation methods." ></td>
	<td class="line x" title="225:255	Here, we compare OURSs results with baselines. 7.1 Improvement relative to dependency length We presented an idea that using the information on shorter dependencies in auto-parsed target data to help parse the words with longer distance for domain adaptation." ></td>
	<td class="line x" title="226:255	In this section, we investigated how our approach performed for parsing longer distance words." ></td>
	<td class="line x" title="227:255	Figure 4 shows the improvement relative to dependency length." ></td>
	<td class="line x" title="228:255	From the figure, we found that our approach always performed better than baseline when dependency lengthes were 1-7." ></td>
	<td class="line x" title="229:255	Especially, our approach achieved improvements by 2.58% at length 3, 5.38% at 6, and 3.67% at 7." ></td>
	<td class="line x" title="230:255	For longer ones, the improvement was not stable." ></td>
	<td class="line x" title="231:255	One reason may be that the numbers of longer ones were small." ></td>
	<td class="line x" title="232:255	Another reason was that parsing long distance words was very difficult." ></td>
	<td class="line x" title="233:255	However, we still found that our approach did improve the performance for longer ones, by performing better at 8 points and worse at 5 points when length was not less than 8." ></td>
	<td class="line x" title="234:255	10  20  30  40  50  60  70  80  90  100  0  2  4  6  8  10  12  14  16  18  20 F1 Dependency Length baseline OURS Figure4: Performanceasafunctionofdependency length 7.2 Improvement relative to unknown words The unknown word problem is an important issue for adaptation." ></td>
	<td class="line x" title="235:255	Our approach can partially release the unknown word problem." ></td>
	<td class="line x" title="236:255	We listed the data of the numbers of unknown words from 0 to 8 because the number of sentences was very small for others." ></td>
	<td class="line x" title="237:255	We grouped each sentence into one of three classes: (Better) those where our approachs score increased relative to the baselines score, (NoChange) those where the score remained the same, and (Worse) those where the score had a relative decrease." ></td>
	<td class="line x" title="238:255	We added another class (NoWorse) by merging Better and NoChange." ></td>
	<td class="line x" title="239:255	Figure 5 shows the experimental results, where x axis refers to the number of unknown words in one sentence and y axis refers to how many percent the class has." ></td>
	<td class="line x" title="240:255	For example, for the sentences having5unknownwords, about45.45%improved, 22.73% became worse, 31.82% kept unchanged, and 77.27% did not become worse." ></td>
	<td class="line x" title="241:255	The NoWorse curve showed that regardless of the number of unknown words in a sentence, there was more than 60% chance that our approach did not harm the result." ></td>
	<td class="line x" title="242:255	The Better curve and Worse curve showed that our approach always provided better results." ></td>
	<td class="line x" title="243:255	Our approach achieved most improvement for the middle ones." ></td>
	<td class="line x" title="244:255	The reason was that parsing the sentence having too many unknown words was very difficult." ></td>
	<td class="line x" title="245:255	7.3 Improvement relative to POS pairs In this section, we listed the improvements relative to POS tags of paired words having a dependency relation." ></td>
	<td class="line x" title="246:255	Table 6 shows the accuracies of baseline and OURS on TOP 20 POS pairs (ordered by the frequencies of their occurrences in testing data), where A1 refers to the accuracy of baseline, A2 refers to the accuracy of OURS, and Pairs is the POS pairs of dependent-head." ></td>
	<td class="line x" title="247:255	119  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  0  1  2  3  4  5  6  7  8 Percentage Number of unknown words Better NoChange Worse NoWorse Figure 5: Performance as a function of number of unknown words Pairs A1 A2(A2-A1) Pairs A1 A2(A2-A1) NN-VV 79.61 81.90(+2.29) DEG-NN 94.74 94.74(=) VV-VV 50.00 50.40(+0.40) CD-M 96.77 97.85(+1.08) NN-NN 86.08 87.26(+1.18) NN-P 76.92 76.92(=) AD-VV 91.01 91.01(=) JJ-NN 92.11 94.74(+2.63) P-VV 68.60 70.25(+1.65) AD-VA 98.55 98.55(=) DEC-NN 97.48 98.32(+0.84) NN-VA 78.95 84.21(+5.26) NR-VV 81.98 81.98(=) NN-DEG 96.43 94.64(-1.79) VV-DEC 74.07 73.15(-0.92) VV-VC 40.82 46.94(+6.12) NR-NN 87.85 87.85(=) AD-VC 95.92 93.88(-2.04) NN-VC 90.91 91.92(+1.01) VA-VV 60.87 67.39(+6.52) Table 6: Improvement relative to POS pairs For example, NN-VV means that NN is the POS of the dependent and VV is the POS of the head." ></td>
	<td class="line x" title="248:255	And baseline yielded 79.61% accuracy and OURS yielded 81.90% (2.29% higher) on NNVV." ></td>
	<td class="line x" title="249:255	From the table, we found that our approach worked well for most POS pairs (better for eleven pairs, no change for six, and worse for three)." ></td>
	<td class="line x" title="250:255	8 Conclusion Thispaperpresentsasimplebuteffectiveapproach to adapt dependency parser by using unlabeled target data." ></td>
	<td class="line x" title="251:255	We extract the information on shorter dependencies in an unlabeled data parsed by a basic parser to help parse longer distance words." ></td>
	<td class="line x" title="252:255	The experimental results show that our approach significantly outperforms baseline system and current state of the art adaptation techniques." ></td>
	<td class="line x" title="253:255	There are a lot of ways in which this research could be continued." ></td>
	<td class="line x" title="254:255	First, we can apply our approach to other languages because our approach is independent on language." ></td>
	<td class="line x" title="255:255	Second, we can enlarge the unlabeled data set to obtain more word pairs to provide more information for the parsers." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1059
Stopping Criteria for Active Learning of Named Entity Recognition
Laws, Florian;Sch√ºtze, Hinrich;"></td>
	<td class="line x" title="1:229	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 465472 Manchester, August 2008 Stopping Criteria for Active Learning of Named Entity Recognition Florian Laws Institute for NLP Universitat Stuttgart fl@ifnlp.org Hinrich Schutze Institute for NLP Universitat Stuttgart hs999@ifnlp.org Abstract Active learning is a proven method for reducing the cost of creating the training sets thatarenecessaryforstatisticalNLP.However, there has been little work on stopping criteria for active learning." ></td>
	<td class="line x" title="2:229	An operational stopping criterion is necessary to be able to use active learning in NLP applications." ></td>
	<td class="line x" title="3:229	We investigate three different stopping criteria for active learning of named entity recognition (NER) and show that one of them, gradient-based stopping, (i) reliably stops active learning, (ii) achieves nearoptimal NER performance, (iii) and needs only about 20% as much training data as exhaustive labeling." ></td>
	<td class="line x" title="4:229	1 Introduction Supervised statistical learning methods are important and widely successful tools for natural language processing." ></td>
	<td class="line x" title="5:229	These methods learn by estimating a statistical model on labeled training data." ></td>
	<td class="line x" title="6:229	Often, these models require a large amount of training data that needs to be hand-annotated by human experts." ></td>
	<td class="line x" title="7:229	This is time-consuming and expensive." ></td>
	<td class="line x" title="8:229	Active learning (AL) reduces this annotation effort by selecting unlabeled examples that are maximally informative for the statistical learning method and handing them to a human annotator for labeling." ></td>
	<td class="line x" title="9:229	The statistical model is then updated with the newly gathered information." ></td>
	<td class="line x" title="10:229	In this paper, we adopt the uncertainty sampling approach to AL (Lewis and Gale, 1994)." ></td>
	<td class="line x" title="11:229	Uncertainty sampling selects those examples in the pool as most inc2008." ></td>
	<td class="line x" title="12:229	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="13:229	Some rights reserved." ></td>
	<td class="line x" title="14:229	formative for which the statistical classifier is least certain in its classification decision." ></td>
	<td class="line x" title="15:229	While AL is an active area of research in NLP, the issue of determining when to stop the AL process has only recently come into focus (Zhu and Hovy, 2007; Vlachos, 2008)." ></td>
	<td class="line x" title="16:229	This is somewhat surprising because the main purpose of active learning is to save on annotation effort; deciding on the point when enough data is annotated is crucial to fulfilling this goal." ></td>
	<td class="line x" title="17:229	We investigate three different stopping criteria in this paper." ></td>
	<td class="line x" title="18:229	First, a user of a classification system may want to set a minimum absolute performance forthesystemtobedeployed." ></td>
	<td class="line x" title="19:229	Thestandardwayof assessing classifier performance uses a held-out labeled test set." ></td>
	<td class="line x" title="20:229	However, labeling a test set of sufficient size is contrary to the goal of minimizing annotation effort and impractical in most real-world settings." ></td>
	<td class="line x" title="21:229	We will show that the classifier can estimate its own performance using only an unlabeled reference set and propose to stop active learning if estimated performance reaches the threshold set by the user." ></td>
	<td class="line x" title="22:229	The estimation is somewhat inaccurate, however, and we investigate possible reasons for estimation error." ></td>
	<td class="line x" title="23:229	An alternative criterion is based on maximum possible performance." ></td>
	<td class="line x" title="24:229	We will show that our performance estimation method supports stopping AL at a point where performance is almost optimal." ></td>
	<td class="line x" title="25:229	The third and last criterion is convergence." ></td>
	<td class="line x" title="26:229	The basicideahereistostopactivelearningwhenmore examples from the pool do not contribute more information, indicated either by the fact that the classifierhasreachedmaximumperformanceorby the fact that the uncertainty of the classifier cannot be decreased further." ></td>
	<td class="line x" title="27:229	We determine the point where the pool has become uninformative by computing the gradient of either performance or uncer465 tainty." ></td>
	<td class="line x" title="28:229	This paper is organized as follows." ></td>
	<td class="line x" title="29:229	Section 2 shows that three uncertainty measures achieve near-optimal performance for NER at a fraction of the labeling cost of exhaustive labeling of the training set." ></td>
	<td class="line x" title="30:229	In Section 3, we introduce a new method for estimating the performance of an actively learned classifier in support of stopping active learning when a certain level of performance has been reached." ></td>
	<td class="line x" title="31:229	Section 4 shows that the stopping criterion of reaching peak confidence is not applicable to NER with multiclass logistic regression." ></td>
	<td class="line x" title="32:229	Section 5 presents stopping criteria based on convergence." ></td>
	<td class="line x" title="33:229	Sections 6 and 7 discuss related work and present our conclusions." ></td>
	<td class="line x" title="34:229	2 Selection Functions For measuring the uncertainty of a classification decisioninuncertaintysamplingthereexistdiverse measures appropriate for different basic classifiers (e.g. margin-based measures for SVMs, and measures based on class probability for classification)." ></td>
	<td class="line x" title="35:229	Choosing such an uncertainty measure is relatively straightforwardforabinaryclassificationproblem, but for multiclass problems we need different measures, and it is not obvious which will perform best." ></td>
	<td class="line x" title="36:229	Following Schein (2005), but in the context of NER, we compare several measures of uncertainty for multiclass logistic regression." ></td>
	<td class="line x" title="37:229	For a given measureMi,X,weselectineachiterationtheunlabeled example(s) in the pool that have the smallest value for Mi,X (corresponding to the maximum uncertainty)." ></td>
	<td class="line x" title="38:229	1-Entropy." ></td>
	<td class="line x" title="39:229	Mi,1-Entropy = 1 H(p(.|xi)) = 1 + summationdisplay j p(cj|xi)log p(cj|xi) where p(cj|xi) is the current estimate of the probability of class cj given the example xi.1 1-Entropy favors examples where the classifier assigns similar probabilities to all classes." ></td>
	<td class="line x" title="40:229	Margin." ></td>
	<td class="line x" title="41:229	If c and cprime are the two most likely classes, the margin is defined as follows: Mi,Margin = |p(c|xi)  p(cprime|xi)| Margin picks examples where the distinction between two likely classes is hard." ></td>
	<td class="line x" title="42:229	1We use 1-Entropy instead of entropy, so all three measures will have lower values for less certain instances." ></td>
	<td class="line x" title="43:229	MinMax." ></td>
	<td class="line x" title="44:229	Mi,MinMax = maxj (p(cj|xi)) The rationale here is that a low probability of the selected class indicates uncertainty." ></td>
	<td class="line x" title="45:229	We propose MinMax as a measure that is more directly based on the classifiers decision for a particular example." ></td>
	<td class="line x" title="46:229	The other two measures also take into account the classifiers assessment of classes that were not chosen for the unlabeled example." ></td>
	<td class="line x" title="47:229	2.1 Experiments We used the newswire section of the ACE 2005 Multilingual Training Corpus (128 documents, 66,015 tokens) for our experiments." ></td>
	<td class="line x" title="48:229	A subset of thedocumentswasrandomlysampledintoanevaluation set that consists of 6301 tokens." ></td>
	<td class="line x" title="49:229	We used 30.000 of the remaining tokens as the uncertainty sampling pool." ></td>
	<td class="line x" title="50:229	The rest was left aside for future experiments." ></td>
	<td class="line x" title="51:229	We use the BBR package (Genkin et al., 2007) for binary logistic regression as our base classifier, with default values for all of BBRs parameters." ></td>
	<td class="line x" title="52:229	As our main focus is on AL, we only use basic features like capitalization, puctuation as well as word identity, prefixes and suffixes, each for the classified word itself and for left and right contexts." ></td>
	<td class="line x" title="53:229	We train separate classifiers for each named entity(NE)classandanotheronefortheclassnotan NE (0)." ></td>
	<td class="line x" title="54:229	For each token we normalize the output probability of the individual classifiers so they sum to 1 and then select for each token the class with thehighestprobability." ></td>
	<td class="line x" title="55:229	Evaluationisperformedby comparing individual tokens to the gold standard.2 Usingalllabeledtrainingdataasourfullysupervised baseline results in a performance of 78.7% F1 (henceforth: F) and 96.6% accuracy." ></td>
	<td class="line oc" title="56:229	This is comparable to the accuracy of 96.29% reported by (Daume III, 2007) on the newswire domain." ></td>
	<td class="line o" title="57:229	Daumes work is the only study known to us that uses the ACE dataset, but not the proprietary ACE value score." ></td>
	<td class="line x" title="58:229	In the rest of this paper, we report F scores, because we believe that F is a more informative measure for NER than accuracy." ></td>
	<td class="line x" title="59:229	We use AL based on uncertainty sampling." ></td>
	<td class="line x" title="60:229	We start with a seed set of ten consecutive tokens randomly selected from the training pool and label it." ></td>
	<td class="line x" title="61:229	In each round of AL we select the ten tokens with the smallest value of Mi,X (where 2Chunk-based NER results are not directly comparable with this token-based evaluation." ></td>
	<td class="line x" title="62:229	466 Selection Baseline Peak perf." ></td>
	<td class="line x" title="63:229	1-Entropy 78.7 2139 (7.1%) 80.8 3460 (11.5%) MinMax 78.7 2108 (7.0%) 80.8 3650 (12.1%) Margin 78.7 2019 (6.7%) 81.2 3694 (12.3%) Table 1: Percentage of data needed by AL to reach baseline or peak performance." ></td>
	<td class="line x" title="64:229	X  {1-Entropy,Margin,MinMax}) from the remaining pool, including tokens with the label 0." ></td>
	<td class="line x" title="65:229	We then label these tokens and add them to the labeledtrainingset." ></td>
	<td class="line x" title="66:229	Theclassifiersareretrainedwith the new training set and the AL loop repeats." ></td>
	<td class="line x" title="67:229	We performed 20 runs of the experiments, each with the same sampling pool, but a different seed set, randomly selected as described above." ></td>
	<td class="line x" title="68:229	Table 1 shows that AL is quite successful for NER." ></td>
	<td class="line x" title="69:229	Only 7% of the training data is needed to achieve the same performance as the supervised baseline." ></td>
	<td class="line x" title="70:229	Furthermore we find that after the baseline performance is reached the increase in performance quickly levels off to a point where using more training data does not yield performance improvements anymore." ></td>
	<td class="line x" title="71:229	In fact, our experiments show that there is a peak in performance reached at about 12% of the training data and performance decreases again after this point (see Figure 1)." ></td>
	<td class="line x" title="72:229	The peak is more prominent if the pool is large." ></td>
	<td class="line x" title="73:229	On a pool of 30,000 tokens, peak performance is about 2.5% F-Score better than the baseline; on a 6000 token pool, thedifference is only about 1.7%." ></td>
	<td class="line x" title="74:229	Therefore, once the peak is reached, the AL process should stop, even if the annotation budget is not yet used up." ></td>
	<td class="line x" title="75:229	0 2000 4000 6000 8000 10000 0.60 0.65 0.70 0.75 0.80 Training examples FScore Margin 1Entropy MinMax Figure 1: Performance as a function of number of labeled training examples used Comparing the different selection functions, we found little difference between their performance." ></td>
	<td class="line x" title="76:229	Margin performs significantly better (Students ttest,  = 0.05), but the difference is small (< 1% F-Score)." ></td>
	<td class="line x" title="77:229	If we compare two AL processes (say Margin and 1-Entropy) that were started with the same pool and seed set and stop both processes when they each reach their respective peak performances, Margin has a better peak performance of 0.3% F-Score on average (significant at  = 0.05)." ></td>
	<td class="line x" title="78:229	Thedifferencesbetween1-EntropyandMinMax are not statistically significant, except for a short start-up phase (see Figure 1)." ></td>
	<td class="line x" title="79:229	3 Performance Estimation In practical applications, classifiers can only be reliably deployed when they attain a predefined minimum absolute performance level." ></td>
	<td class="line x" title="80:229	Thus, we would like to determine if this level has been reached and then stop the annotation process." ></td>
	<td class="line x" title="81:229	However, this is not a simple task, because in these settings there is no labeled test set available to evaluate performance." ></td>
	<td class="line x" title="82:229	Creating this test set would mean a substantial annotation effort, which is what we want to avoid by using AL in the first place." ></td>
	<td class="line x" title="83:229	Therefore, we will try to estimate the classifiers performance on unlabeled data." ></td>
	<td class="line x" title="84:229	Following Lewis (1995), we estimate the FScore based on the current estimates of the class probabilities." ></td>
	<td class="line x" title="85:229	Based on the F measures definition as the harmonic mean of precision (P) and recall (R), we can write F as a function of true positives (TP), false positives (TP) and false negatives (FN): F = 2 P RP +R = 2TP2TP + FP + FN Similar to Lewis, we estimate TP, FP, FN, but we need to extend their work from binary classification to 1-vs-all multiclass classification: TP = nsummationdisplay i Esummationdisplay j p(cj|xi)di,j (1) FP = nsummationdisplay i Esummationdisplay j (1  p(cj|xi))di,j (2) FN = nsummationdisplay i Esummationdisplay j p(cj|xi)(1 di,j) (3) where n is the number of examples, E is the number of named entity classes, excluding the not 467 an NE class." ></td>
	<td class="line x" title="86:229	p(cj|xi) is the estimated probability that example xi has class cj." ></td>
	<td class="line x" title="87:229	The flag di,j indicates is winning class: di,j = 1 if j = argmaxj p(cj|xi) and di,j = 0 else." ></td>
	<td class="line x" title="88:229	Like standard NER evaluation schemes, e.g.(Tjong Kim Sang and De Meulder, 2003), we consider only those decisions to be TPs where (i) the reference class matches the selected class and (ii) this class is not not an NE." ></td>
	<td class="line x" title="90:229	When estimating TP, we assume that the probability of a match equals the probability of the selected class (which is p(cj|xi)  di,j)." ></td>
	<td class="line x" title="91:229	The probability of making an FP error is just the remaining probability mass. For FN, we can calculate the estimated probability by summing up the class probabilities of the non-selected named entity classes." ></td>
	<td class="line x" title="92:229	3.1 Evaluation of Performance Estimation To evaluate the performance estimation method, we ran it on an unlabeled reference set." ></td>
	<td class="line x" title="93:229	The reference set is a set of unlabeled data distinct from the sampling pool." ></td>
	<td class="line x" title="94:229	In our experiments, we use the tokens in the test set from 2.1, but with the labels stripped off." ></td>
	<td class="line x" title="95:229	We compare the true performance on the test set (reported as True in Table 2) with the estimate (reported as Lewis)." ></td>
	<td class="line x" title="96:229	The  columns report the differenceofthenamedmethodtoTrue." ></td>
	<td class="line x" title="97:229	Wealso tested leave-one-out (LOO) estimation of F, P and R using the data of the selected training set." ></td>
	<td class="line x" title="98:229	True Lewis  Lewis LOO  LOO F 79 92 +13 85 +6 P 81 92 +11 86 +5 R 77 92 +15 84 +7 Table 2: Performance estimation." ></td>
	<td class="line x" title="99:229	LOO and Lewis overestimate true F by 6% and 13%, respectively." ></td>
	<td class="line x" title="100:229	We find that both methods overestimate precision and recall by a large margin." ></td>
	<td class="line x" title="101:229	We also note that the peak in performance at about 4200 training examples that we found when evaluating on held-out data (see Figure 1) does not occur when evaluating performance using the Lewis method." ></td>
	<td class="line x" title="102:229	Instead, the estimate of F grows monotonically." ></td>
	<td class="line x" title="103:229	This means that we cannot use a peak of estimated F as a criterion for stopping." ></td>
	<td class="line x" title="104:229	When setting an absolute threshold of F = 80% for stopping, active learning stops at about 1000 iterations, yielding a true performance of only F = 73% (selection by Margin, 20 trials)." ></td>
	<td class="line x" title="105:229	This indicates that we cannot directly use Lewis estimates for stopping." ></td>
	<td class="line x" title="106:229	3.2 Error Analysis The reason for the overestimation is that the logistic regression classifier is too confident in its own decision." ></td>
	<td class="line x" title="107:229	For positive decisions, the class probability very often is close to 1, for negative decisions, it is close to 0." ></td>
	<td class="line x" title="108:229	As a result, the estimator gives very little score for FN (Equation 3) or FP (Equation 2) in most instances, which leads to the high overestimation of performance." ></td>
	<td class="line x" title="109:229	To verify this, we grouped the empirical probability of a selected class being the correct class in bins according to the estimated probability of the logistic classifier." ></td>
	<td class="line x" title="110:229	Table 3 shows this empirical probability given a class and its estimate." ></td>
	<td class="line x" title="111:229	The table is split into two halves, such that the empirical probabilities for positive decisions (the class got chosen as the best class) and negative decisions are shown separately." ></td>
	<td class="line x" title="112:229	The top value in each cell(emp)showstheempiricalprobabilityasopposed to the estimated probability, which is the value below (est)." ></td>
	<td class="line x" title="113:229	The product of the difference of these two probabilities and the number of instances that were counted into this bin (cnt), gives an estimate of how much the probability estimates in the bin contribute to the error (absolute value) of the performance estimation." ></td>
	<td class="line x" title="114:229	The table shows that class probabilities are in fact estimated too optimistically." ></td>
	<td class="line x" title="115:229	For many of the entries in the positives table, the estimated probabilities are greater than the empirical probabilities." ></td>
	<td class="line x" title="116:229	In the negatives table, the estimated probabilities are smaller." ></td>
	<td class="line x" title="117:229	In both cases, the estimates are closer to the respective extreme values 1 or 0, which means they are overconfident." ></td>
	<td class="line x" title="118:229	Note that for positive decisions, the estimation error of the valuesinasinglebincontributestotheoverallestimation error in two ways: overestimating TPs and underestimating FPs." ></td>
	<td class="line x" title="119:229	For example, the estimation error for the cell in bold is 29.2, contributing 29.2 for FP (underestimation) and +29.2 for TP (overestimation)." ></td>
	<td class="line x" title="120:229	Also note that due to the high number of non-NE tokens in the text, there is a large number of negative decisions for each entity-class classifier; thus, small differences in the probabilities make large contributions to error." ></td>
	<td class="line x" title="121:229	We ran a separate experiment in which we trained a classifier on the entire labeled pool." ></td>
	<td class="line x" title="122:229	The Lewis estimator overestimated F by 12% in this case." ></td>
	<td class="line x" title="123:229	This indicates that the estimation error does not primarily come from the biased selection of training examples inherent in the selective sam468 negative decisions positive decisions 0-.2 .2-.4 .4-.6 .2-.4 .4-.6 .6-.8 .8-1 O emp 0.0643 0.269 0.25 0.0 0.25 0.233 0.991 est 0.00825 0.295 0.438 0.394 0.537 0.714 0.999 cnt 607 26 12 1 16 30 5609 err 34 -0.67 -2.25 0.394(tn) 4.6 (tn) 14.4 (tn) 45.4 (tn) GPE emp 0.00384 0.391 0.5 0.0 0.333 0.571 0.875 est 0.000812 0.296 0.435 0.357 0.535 0.687 0.989 cnt 5985 23 6 1 9 21 256 err 18.1 (fn) 2.19 (fn) 0.388 (fn) 0.357 (fp) 1.82 (fp) 2.42 (fp) 29.2 (fp) ORG emp 0.00853 0.393 0.667 0.5 0.615 0.828 est 0.000847 0.283 0.441 0.545 0.71 0.968 cnt 6093 28 12 14 26 128 err 46.8 (fn) 3.06 (fn) 2.7 (fn) 0.631 (fp) 2.46 (fp) 17.9 (fp) PER emp 0.0041 0.455 0.5 0.273 0.5 0.93 est 0.000748 0.283 0.48 0.563 0.718 0.98 cnt 6102 22 6 11 18 142 err 20.4 (fn) 3.78 (fn) 0.121 (fn) 3.2 (fp) 3.93 (fp) 7.19 (fp) Table 3: Empirical probabilities and contribution to estimation errors." ></td>
	<td class="line x" title="124:229	(We omit small classes and empty columns.)" ></td>
	<td class="line x" title="125:229	Example (cell in bold): 256 tokens were estimated to be a GPE by the classifier with estimated probabilities between 0.8 and 1.0." ></td>
	<td class="line x" title="126:229	The average estimate was 0.989." ></td>
	<td class="line x" title="127:229	In reality, only 224 of these tokens (87.5%) were GPEs." ></td>
	<td class="line x" title="128:229	The contribution of this cell to the overall FP count is (0.9890.875)256  29.2." ></td>
	<td class="line x" title="129:229	pling method, but from bias inherent in either the whole pool of training data or the base classifier." ></td>
	<td class="line x" title="130:229	3.3 Towards a Better Estimate Over-optimistic estimates for precision and recall stem from the classifiers over-optimistic probability estimates." ></td>
	<td class="line x" title="131:229	We try to correct the estimates by replacing the predicted class probabilities with the appropriate value in an empirical probability table like the one shown in Table 3." ></td>
	<td class="line x" title="132:229	However, since in practice we do not have labels for the test set, we cannot compute the empirical probabilities directly." ></td>
	<td class="line x" title="133:229	Instead, we use leave-one-out estimation to bootstrap the adjustment table from the selected training data." ></td>
	<td class="line x" title="134:229	The adjusted estimation shows a marked increase in the estimates for FP and FN, leading to a quite accurate estimate for precision (+5 absolute error), but the now pessimistic estimate for recall (16) leads to underestimation of F-Score overall (8) (see Table 4)." ></td>
	<td class="line x" title="135:229	True Lewis adj." ></td>
	<td class="line x" title="136:229	Lewis  adj." ></td>
	<td class="line x" title="137:229	Lewis F 78 91 70 -8 P 81 93 86 +5 R 76 89 60 -16 TP 520 596 555 +35 FP 125 48 90 -35 FN 163 70 379 +216 Table 4: Lewis estimation with adjusted probabilities As we see, the adjustment overshoots for recall, indicating that the new estimated probabilities are still off." ></td>
	<td class="line x" title="138:229	There could be several reasons for this." ></td>
	<td class="line x" title="139:229	Thefirstreasonisthatthebinwidthisquitecoarse, as there are only five bins for the entire probability interval, each bin covering a range of 0.2." ></td>
	<td class="line x" title="140:229	However, using finer bin widths can lead to data sparsity problems." ></td>
	<td class="line x" title="141:229	Another reason might be the estimation errors within individual bins that compound to a quite large overall error especially in the negative case." ></td>
	<td class="line x" title="142:229	Finally, differences in the distributions of training set and reference set could cause unreliable estimates." ></td>
	<td class="line x" title="143:229	The empirical probabilities for the adjustment table are estimated with leave-one-out on the training set." ></td>
	<td class="line x" title="144:229	However, since the training set is created by selective sampling, it will be biased." ></td>
	<td class="line x" title="145:229	4 Confidence-based Stopping We have found that performance estimation is not yet reliable enough to stop when a desired performance level is reached." ></td>
	<td class="line x" title="146:229	However, since there is a maximum performance that can be reached on any given sampling pool, the annotation process still should stop at this point regardless of whether a target performance level has been reached or not." ></td>
	<td class="line x" title="147:229	We therefore seek a stopping criterion that finds the maximum possible performance when the classifier is iteratively trained on a given sampling pool." ></td>
	<td class="line x" title="148:229	Again, in practice we do not have a labeled test set to evaluate against, so we have to try to find the stopping point from either the remaining pool, or the separate unlabeled reference set." ></td>
	<td class="line x" title="149:229	Vlachos (2008) proposes to calculate the confidence of the classifier by using the average uncer469 tainty on the unlabeled reference set." ></td>
	<td class="line x" title="150:229	For multiclass problems, he uses SVM classifiers with the SVM margin size as the uncertainty measure." ></td>
	<td class="line x" title="151:229	Using this measure, Vlachos reports finding, albeit distortedbyfluctuations, apeakpatterninthisconfidence measure that coincides with reaching maximal performance in his experiments." ></td>
	<td class="line x" title="152:229	He then suggests to use this peak confidence as the stopping criterion." ></td>
	<td class="line x" title="153:229	However, in our experiments with multiclass logistic regression, we could not find this peak pattern when calculating the confidence using the three uncertainty measures introduced above: 1Entropy, Margin and MinMax." ></td>
	<td class="line x" title="154:229	0 2000 4000 6000 8000 10000 0.80 0.85 0.90 0.95 1.00 Iterations Confidence on unlabeled reference data 1Entropy Minmax Margin Figure 2: Confidence on unlabeled reference set (selection: 1-Entropy)." ></td>
	<td class="line x" title="155:229	The vertical lines indicate when baseline and optimal performance are reached." ></td>
	<td class="line x" title="156:229	There is no peak pattern in the curves, so reaching peak confidence cannot be used as a stopping criterion." ></td>
	<td class="line x" title="157:229	In Figure 2, we show the three measures, averaged over 20 trials as described in section 2.1." ></td>
	<td class="line x" title="158:229	Due to instability of AL during start-up, there are some fluctuations in the first 100 iterations." ></td>
	<td class="line x" title="159:229	After 500 iterations the confidence curves stabilize and at about 4000 iterations approach asymptotes, without exhibiting peak patterns." ></td>
	<td class="line x" title="160:229	Thus, the proposed criterion of peak confidence based on average reference uncertainty does not seem applicable for controlling AL with multiclass logistic regression." ></td>
	<td class="line x" title="161:229	5 Gradient-based Stopping Since we cannot use peaks for stopping, we propose to stop when a base measurement character0 2000 4000 6000 8000 10000 0.0 0.2 0.4 0.6 0.8 1.0 Iterations Margin Figure 3: Margin uncertainty of selected instance (single run)." ></td>
	<td class="line x" title="162:229	The graph demonstrates that without smoothing this criterion is too noisy." ></td>
	<td class="line x" title="163:229	izingtheprogressofactivelearninghasconverged." ></td>
	<td class="line x" title="164:229	Weidentifythepointofconvergencebycomputing gradients." ></td>
	<td class="line x" title="165:229	We find that the rise of the performance estimation slows to an almost horizontal slope at about the time when the true performance reaches its peak." ></td>
	<td class="line x" title="166:229	We therefore propose the following new stopping criterion: Estimate the gradient of the curve and stop when it approaches 0." ></td>
	<td class="line x" title="167:229	Since we do not need an accurate estimation of absolute performance here, we can use the unadjusted Lewis estimate for this method." ></td>
	<td class="line x" title="168:229	We call this stopping criterion (estimated) performance convergence." ></td>
	<td class="line x" title="169:229	In a similar way, we can use the gradient of the uncertainty of the last selected instance." ></td>
	<td class="line x" title="170:229	The instance that was selected last is always the one with maximum uncertainty, and thus the most informative for training." ></td>
	<td class="line x" title="171:229	When the uncertainty measure comes close to the extreme value of 1, we decide that there are no informative examples left in the pool and we stop the AL process." ></td>
	<td class="line x" title="172:229	(Unfortunately, 1 is minimum uncertainty and 0 is maximum uncertainty according to our definitions of the three measures.)" ></td>
	<td class="line x" title="173:229	The gradient of the uncertainty measure approaches 0 at this point (see Figure 3), so we can again use a gradient criterion for implementing this idea." ></td>
	<td class="line x" title="174:229	We call this stopping criterion uncertainty convergence." ></td>
	<td class="line x" title="175:229	In Figure 3, which shows a graph of the Margin uncertainty of the selected instance, we can also see that it is quite noisy." ></td>
	<td class="line x" title="176:229	The value drops sharply when some examples are encountered but quickly returns to the previous level after a few iterations." ></td>
	<td class="line x" title="177:229	The performance estimation measure is 470 slightly noisy as well, so we need a robust way of computing the gradient." ></td>
	<td class="line x" title="178:229	We achieve this with a moving median approach." ></td>
	<td class="line x" title="179:229	At each step, we compute the median of w2 = {ank,,an} (the last n values) and of w1 = {ank1,,an1} (the previous last n values)." ></td>
	<td class="line x" title="180:229	Each value ai is the performance at iteration i (for the performance gradient) or the uncertainty of the instance selected in iteration i (for the uncertainty gradient)." ></td>
	<td class="line x" title="181:229	We then estimate the gradient using the medians of the two windows: g = (median(w2)  median(w1))/1 (4) For the performance estimate, which is less noisy, we can also use the arithmetic mean instead of the median." ></td>
	<td class="line x" title="182:229	In this case, we simply replace median with mean in Equation 4." ></td>
	<td class="line x" title="183:229	We found that a window of size k = 100 yields good results in mitigating the noise while still reacting fast enough to the changes in the gradient." ></td>
	<td class="line x" title="184:229	We combine this criterion with a maximum criterionandonlystopifthelastvaluean isanewmaximum." ></td>
	<td class="line x" title="185:229	WestoptheALprocesswhen(i)thecurrent certainty or estimated performance is a new maximum and (ii) the newly calculated gradient g is positive and (iii) g falls below a predefined level epsilon1." ></td>
	<td class="line x" title="186:229	5.1 Evaluation We show the results of gradient stopping applied to each of the three uncertainty measures and the Lewis estimate." ></td>
	<td class="line x" title="187:229	For comparison, we also include results with a threshold-based criterion, where AL stops when the uncertainty measure of the selected instancereachesathresholdof1epsilon1." ></td>
	<td class="line x" title="188:229	Thisissimilar to (Zhu and Hovy, 2007), but extended by us to all three uncertainty measures." ></td>
	<td class="line x" title="189:229	Table 5 shows results for each criterion." ></td>
	<td class="line x" title="190:229	The Stop value indicates number of tokens at which thestoppingcriterionstoppedAL.Blindicates the difference between baseline performance and performance at the stopping point, Pk the difference to peak performance." ></td>
	<td class="line x" title="191:229	The sd columns show the respective standard deviations." ></td>
	<td class="line x" title="192:229	We find that all stopping criteria stop before 20% of the pool is used, providing a large reduction in annotation effort." ></td>
	<td class="line x" title="193:229	While the point of peak performance can not be precisely found by the criteria, all criteria reliably stop at a performance level that surpasses the fully supervised baseline." ></td>
	<td class="line x" title="194:229	The threshold criteria seem to be a bit better in finding a stopping point closer to optimal performance." ></td>
	<td class="line x" title="195:229	Not unsurprisingly, the stopping function that matches the selection function performs best." ></td>
	<td class="line x" title="196:229	Thegradientmethods,however,seemtobeproviding better-than-baseline performance more consistently (less variation) and might require less tuning ofthethresholdparameterwhenotherfactors(e.g., the batch size) change." ></td>
	<td class="line x" title="197:229	If lower noise allows it, as for the Lewis estimate, moving averages should be used in place of moving medians." ></td>
	<td class="line x" title="198:229	6 Related Work Schutze et al.(2006) studied a Lewis-based performance estimation method in a binary text classification setting." ></td>
	<td class="line x" title="200:229	They attribute difficulties in estimating recall to a missed cluster effect, meaning that the active sampling procedure is failing to select some clusters of relevant training examples in the pool that are too dissimilar to the relevant examples already known." ></td>
	<td class="line x" title="201:229	Diversity measures as proposed by (Shen et al., 2004) might help in mitigating this effect, but our experiments show that there are fundamental differences between text classification and NER." ></td>
	<td class="line x" title="202:229	Since missed clusters of relevant examples in the training data would eventually be used as we exhaustively label the entire pool, we shouldseeimprovementsinrecallwhenthemissed clusters get used." ></td>
	<td class="line x" title="203:229	Instead, we observed in section 2.1, that there are no further performance gains afteracertainportionof thepoolislabeled." ></td>
	<td class="line x" title="204:229	Thus, all examples that the classifier can make use of must have been taken into account, and there appear to be no missed clusters." ></td>
	<td class="line x" title="205:229	Tomanek et al.(2007) present a stopping criterion for query-by-committee-based AL that is based on the rate of disagreement of the classifiers in the committee." ></td>
	<td class="line x" title="207:229	While our uncertainty convergence criterion can only be applied to uncertainty sampling, the performance convergence criterion can be used in a committee-based setting." ></td>
	<td class="line x" title="208:229	Li and Sethi (2006) estimate the conditional errorasameasureofuncertaintyinselection(instead of using it for stopping as we do), using a variablebin histogram for improving the error estimates." ></td>
	<td class="line x" title="209:229	They do not evaluate the quality of the probability estimates." ></td>
	<td class="line x" title="210:229	As with our stopping criterion, we expect this selection criterion to be the more effective the more accurate the probability estimates are." ></td>
	<td class="line x" title="211:229	We therefore believe that our method of improving probability estimates based on LOO bins could improve their selection criterion." ></td>
	<td class="line x" title="212:229	471 Stop crit." ></td>
	<td class="line x" title="213:229	epsilon1 Peak Stop  Bl sd  Pk sd 1-Entropy threshold 0.01 80.8 3645 12.0% 1.44 0.7 0.68 0.4 MinMax threshold 0.01 80.8 3133 10.3% 0.11 1.0 2.0 0.8 Margin threshold 0.01 80.8 3158 10.4% 1.1 0.8 1.0 0.8 1-Entropy gradient 0.00005 80.8 4572 15.0% 0.97 0.4 1.1 0.5 MinMax gradient 0.00005 80.8 4397 14.5% 1.02 0.4 1.1 0.5 Margin gradient 0.00005 80.8 5292 17.5% 0.81 0.3 1.32 0.4 Lewis grd." ></td>
	<td class="line x" title="214:229	(Median) 0.00005 80.8 2791 9.2% 0.8 1.4 1.3 1.4 Lewis grd." ></td>
	<td class="line x" title="215:229	(Mean) 0.00005 80.8 3999 13.1% 1.1 0.8 0.95 0.6 Table 5: Performance at stopping points (baseline perf." ></td>
	<td class="line x" title="216:229	78.7, Selection: 1-Entropy) 7 Conclusion and Future Work In this paper, we presented several criteria to stop the AL process." ></td>
	<td class="line x" title="217:229	For stopping the training at a user-defined performance level, we proposed a method for estimating classifier performance in a multiclass classification setting." ></td>
	<td class="line x" title="218:229	While we could achieve acceptable accuracy in estimation of precision, we find that recall estimation is hard." ></td>
	<td class="line x" title="219:229	Estimation is not accurate enough to assist in making a reliable decision if the performance of the classifierisacceptableforpracticaluse." ></td>
	<td class="line x" title="220:229	Inthefuture, we plantoimproveonperformanceestimationquality, e.g., by using the variable-bin approach suggested by Li and Sethi (2006)." ></td>
	<td class="line x" title="221:229	Nevertheless, we showed that the gradient of the performance estimate can successfully be used as a stopping criterion relative to the optimal performance that is attainable on a given pool." ></td>
	<td class="line x" title="222:229	We also describe stopping criteria based on the gradient of the uncertainty measure of the instances selected for training." ></td>
	<td class="line x" title="223:229	The criteria reliably determine stopping points that result in a performance that is better than the supervised baseline and close to the optimal performance." ></td>
	<td class="line x" title="224:229	We believe that these criteria can be applied to any AL setting based on uncertainty sampling, not just NER." ></td>
	<td class="line x" title="225:229	If it turns out that the maximum possible performance does not meet a users expectations, the user needs to acquire fresh data and refill the pool." ></td>
	<td class="line x" title="226:229	This might lead to an approach to reduce the computational cost of AL we want to evaluate in future work: Subdivide a large sampling pool into smaller sub-pools, run AL sequentially on the subpools." ></td>
	<td class="line x" title="227:229	When the stopping criterion is reached, switch to the next sub-pool." ></td>
	<td class="line x" title="228:229	We also found that uncertainty curves of the selected examples are quite noisy." ></td>
	<td class="line x" title="229:229	We would like to investigate which properties of the training examples cause these drops in the uncertainty curve." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D08-1105
Word Sense Disambiguation Using OntoNotes: An Empirical Study
Zhong, Zhi;Ng, Hwee Tou;Chan, Yee Seng;"></td>
	<td class="line x" title="1:155	Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 10021010, Honolulu, October 2008." ></td>
	<td class="line x" title="2:155	c2008 Association for Computational Linguistics Word Sense Disambiguation Using OntoNotes: An Empirical Study Zhi Zhong and Hwee Tou Ng and Yee Seng Chan Department of Computer Science National University of Singapore Law Link, Singapore 117590 {zhongzhi, nght, chanys}@comp.nus.edu.sg Abstract The accuracy of current word sense disambiguation (WSD) systems is affected by the fine-grained sense inventory of WordNet as well as a lack of training examples." ></td>
	<td class="line x" title="3:155	Using the WSD examples provided through OntoNotes, we conduct the first large-scale WSD evaluation involving hundreds of word types and tens of thousands of sense-tagged examples, while adopting a coarse-grained sense inventory." ></td>
	<td class="line x" title="4:155	We show that though WSD systems trained with a large number of examples can obtain a high level of accuracy, they nevertheless suffer a substantial drop in accuracy when applied to a different domain." ></td>
	<td class="line x" title="5:155	To address this issue, we propose combining a domain adaptation technique using feature augmentation with active learning." ></td>
	<td class="line x" title="6:155	Our results show that this approach is effective in reducing the annotation effort required to adapt a WSD system to a new domain." ></td>
	<td class="line x" title="7:155	Finally, we propose that one can maximize the dual benefits of reducing the annotation effort while ensuring an increase in WSD accuracy, by only performing active learning on the set of most frequently occurring word types." ></td>
	<td class="line x" title="8:155	1 Introduction In language, many words have multiple meanings." ></td>
	<td class="line x" title="9:155	The process of identifying the correct meaning, or sense of a word in context, is known as word sense disambiguation (WSD)." ></td>
	<td class="line x" title="10:155	WSD is one of the fundamental problems in natural language processing and is important for applications such as machine translation (MT) (Chan et al., 2007a; Carpuat and Wu, 2007), information retrieval (IR), etc. WSD is typically viewed as a classification problem where each ambiguous word is assigned a sense label (from a pre-defined sense inventory) during the disambiguation process." ></td>
	<td class="line x" title="11:155	In current WSD research, WordNet (Miller, 1990) is usually used as the sense inventory." ></td>
	<td class="line x" title="12:155	WordNet, however, adopts a very fine level of sense granularity, thus restricting the accuracy of WSD systems." ></td>
	<td class="line x" title="13:155	Also, current state-of-the-art WSD systems are based on supervised learning and face a general lack of training data." ></td>
	<td class="line x" title="14:155	To provide a standardized test-bed for evaluation of WSD systems, a series of evaluation exercises called SENSEVAL were held." ></td>
	<td class="line x" title="15:155	In the English all-words task of SENSEVAL-2 and SENSEVAL3 (Palmer et al., 2001; Snyder and Palmer, 2004), no training data was provided and systems must tag all the content words (noun, verb, adjective, and adverb) in running English texts with their correct WordNet senses." ></td>
	<td class="line x" title="16:155	In SENSEVAL-2, the best performing system (Mihalcea and Moldovan, 2001) in the English all-words task achieved an accuracy of 69.0%, while in SENSEVAL-3, the best performing system (Decadt et al., 2004) achieved an accuracy of 65.2%." ></td>
	<td class="line x" title="17:155	In SemEval-2007, which was the most recent SENSEVAL evaluation, a similar English all-words task was held, where systems had to provide the correct WordNet sense tag for all the verbs and head words of their arguments in running English texts." ></td>
	<td class="line x" title="18:155	For this task, the best performing system (Tratz et al., 2007) achieved an accuracy of 59.1%." ></td>
	<td class="line x" title="19:155	Results of these evaluations showed that state-of-the-art English all-words WSD systems performed with an accuracy of 60%70%, using the fine-grained sense inventory of WordNet." ></td>
	<td class="line x" title="20:155	The low level of performance by these state-ofthe-art WSD systems is a cause for concern, since WSD is supposed to be an enabling technology to be incorporated as a module into applications 1002 such as MT and IR." ></td>
	<td class="line x" title="21:155	As mentioned earlier, one of the major reasons for the low performance is that these evaluation exercises adopted WordNet as the reference sense inventory, which is often too finegrained." ></td>
	<td class="line x" title="22:155	As an indication of this, inter-annotator agreement (ITA) reported for manual sense-tagging on these SENSEVAL English all-words datasets is typically in the mid-70s." ></td>
	<td class="line x" title="23:155	To address this issue, a coarse-grained English all-words task (Navigli et al., 2007) was conducted during SemEval-2007." ></td>
	<td class="line x" title="24:155	This task used a coarse-grained version of WordNet and reported an ITA of around 90%." ></td>
	<td class="line x" title="25:155	We note that the best performing system (Chan et al., 2007b) of this task achieved a relatively high accuracy of 82.5%, highlighting the importance of having an appropriate level of sense granularity." ></td>
	<td class="line x" title="26:155	Another issue faced by current WSD systems is the lack of training data." ></td>
	<td class="line x" title="27:155	We note that the top performing systems mentioned in the previous paragraphs are all based on supervised learning." ></td>
	<td class="line x" title="28:155	With this approach, however, one would need to obtain a corpus where each ambiguous word occurrence is manually annotated with the correct sense, to serve as training data." ></td>
	<td class="line x" title="29:155	Since it is time consuming to perform sense annotation of word occurrences, only a handful of sense-tagged corpora are publicly available." ></td>
	<td class="line x" title="30:155	Among the existing sense-tagged corpora, the SEMCOR corpus (Miller et al., 1994) is one of the most widely used." ></td>
	<td class="line x" title="31:155	In SEMCOR, content words have been manually tagged with WordNet senses." ></td>
	<td class="line x" title="32:155	Current supervised WSD systems (which include all the top-performing systems in the English all-words task) usually rely on this relatively small manually annotated corpus for training examples, and this has inevitably affected the accuracy and scalability of current WSD systems." ></td>
	<td class="line x" title="33:155	Related to the problem of a lack of training data for WSD, there is also a lack of test data." ></td>
	<td class="line x" title="34:155	Having a large amount of test data for evaluation is important to ensure the robustness and scalability of WSD systems." ></td>
	<td class="line x" title="35:155	Due to the expensive process of manual sense-tagging, the SENSEVAL English all-words task evaluations were conducted on relatively small sets of evaluation data." ></td>
	<td class="line x" title="36:155	For instance, the evaluation data of SENSEVAL-2 and SENSEVAL-3 English all-words task consists of 2,473 and 2,041 test examples respectively." ></td>
	<td class="line x" title="37:155	In SemEval-2007, the fine-grained English all-words task consists of only 465 test examples, while the SemEval-2007 coarse-grained English all-words task consists of 2,269 test examples." ></td>
	<td class="line x" title="38:155	Hence, it is necessary to address the issues of sense granularity, and the lack of both training and test data." ></td>
	<td class="line x" title="39:155	To this end, a recent large-scale annotation effort called the OntoNotes project (Hovy et al., 2006) was started." ></td>
	<td class="line x" title="40:155	Building on the annotations from the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1993), the project added several new layers of semantic annotations, such as coreference information, word senses, etc. In its first release (LDC2007T21) through the Linguistic Data Consortium (LDC), the project manually sense-tagged more than 40,000 examples belonging to hundreds of noun and verb types with an ITA of 90%, based on a coarse-grained sense inventory, where each word has an average of only 3.2 senses." ></td>
	<td class="line x" title="41:155	Thus, besides providing WSD examples that were sense-tagged with a high ITA, the project also addressed the previously discussed issues of a lack of training and test data." ></td>
	<td class="line x" title="42:155	In this paper, we use the sense-tagged data provided by the OntoNotes project to investigate the accuracy achievable by current WSD systems when adopting a coarse-grained sense inventory." ></td>
	<td class="line x" title="43:155	Through our experiments, we then highlight that domain adaptation for WSD is an important issue as it substantially affects the performance of a state-of-theart WSD system which is trained on SEMCOR but evaluated on sense-tagged examples in OntoNotes." ></td>
	<td class="line x" title="44:155	To address this issue, we then show that by combining a domain adaptation technique using feature augmentation with active learning, one only needs to annotate a small amount of in-domain examples to obtain a substantial improvement in the accuracy of the WSD system which is previously trained on out-of-domain examples." ></td>
	<td class="line x" title="45:155	The contributions of this paper are as follows." ></td>
	<td class="line x" title="46:155	To our knowledge, this is the first large-scale WSD evaluation conducted that involves hundreds of word types and tens of thousands of sense-tagged examples, and that is based on a coarse-grained sense inventory." ></td>
	<td class="line x" title="47:155	The present study also highlights the practical significance of domain adaptation in word sense disambiguation in the context of a large-scale empirical evaluation, and proposes an effective method to address the domain adaptation problem." ></td>
	<td class="line x" title="48:155	In the next section, we give a brief description of 1003 our WSD system." ></td>
	<td class="line x" title="49:155	In Section 3, we describe experiments where we conduct both training and evaluation using data from OntoNotes." ></td>
	<td class="line x" title="50:155	In Section 4, we investigate the WSD performance when we train our system on examples that are gathered from a different domain as compared to the OntoNotes evaluation data." ></td>
	<td class="line x" title="51:155	In Section 5, we perform domain adaptation experiments using a recently introduced feature augmentation technique." ></td>
	<td class="line x" title="52:155	In Section 6, we investigate the use of active learning to reduce the annotation effort required to adapt our WSD system to the domain of the OntoNotes data, before concluding in Section 7." ></td>
	<td class="line x" title="53:155	2 The WSD System For the experiments reported in this paper, we follow the supervised learning approach of (Lee and Ng, 2002), by training an individual classifier for each word using the knowledge sources of local collocations, parts-of-speech (POS), and surrounding words." ></td>
	<td class="line x" title="54:155	For local collocations, we use 11 features: C1,1, C1,1, C2,2, C2,2, C2,1, C1,1, C1,2, C3,1, C2,1, C1,2, and C1,3, where Ci,j refers to the ordered sequence of tokens in the local context of an ambiguous word w. Offsets i and j denote the starting and ending position (relative to w) of the sequence, where a negative (positive) offset refers to a token to its left (right)." ></td>
	<td class="line x" title="55:155	For parts-of-speech, we use 7 features: P3, P2, P1, P0, P1, P2, P3, where P0 is the POS of w, and Pi (Pi) is the POS of the ith token to the left (right) of w. For surrounding words, we consider all unigrams (single words) in the surrounding context of w. These words can be in a different sentence from w. For our experiments reported in this paper, we use support vector machines (SVM) as our learning algorithm, which was shown to achieve good WSD performance in (Lee and Ng, 2002; Chan et al., 2007b)." ></td>
	<td class="line x" title="56:155	3 Training and Evaluating on OntoNotes The annotated data of OntoNotes is drawn from the Wall Street Journal (WSJ) portion of the Penn Treebank corpus, divided into sections 00-24." ></td>
	<td class="line x" title="57:155	These WSJ documents have been widely used in various NLP tasks such as syntactic parsing (Collins, 1999) and semantic role labeling (SRL) (Carreras and MarSection No." ></td>
	<td class="line x" title="58:155	of No." ></td>
	<td class="line x" title="59:155	of word tokens word types Individual Cumulative 02 248 425 425 03 79 107 532 04 186 389 921 05 287 625 1546 06 224 446 1992 07 270 549 2541 08 177 301 2842 09 308 677 3519 10 648 3048 6567 11 724 4071 10638 12 740 4296 14934 13 749 4577 19511 14 710 3900 23411 15 748 4768 28179 16 306 576 28755 17 219 398 29153 18 266 566 29719 19 219 389 30108 20 288 536 30644 21 262 470 31114 23 685 3755 Table 1: Size of the sense-tagged data in the various WSJ sections." ></td>
	<td class="line x" title="60:155	quez, 2005)." ></td>
	<td class="line x" title="61:155	In these tasks, the practice is to use documents from WSJ sections 02-21 as training data and WSJ section 23 as test data." ></td>
	<td class="line x" title="62:155	Hence for our experiments reported in this paper, we follow this convention and use the annotated instances from WSJ sections 02-21 as our training data, and instances in WSJ section 23 as our test data." ></td>
	<td class="line x" title="63:155	As mentioned in Section 1, the OntoNotes data provided WSD examples for a large number of nouns and verbs, which are sense-tagged according to a coarse-grained sense inventory." ></td>
	<td class="line x" title="64:155	In Table 1, we show the amount of sense-tagged data available from OntoNotes, across the various WSJ sections.1 In the table, for each WSJ section, we list the number of word types, the number of sense-tagged examples, and the cumulative count on the number of 1We removed erroneous examples which were simply tagged with XXX as sense-tag, or tagged with senses that were not found in the sense-inventory provided." ></td>
	<td class="line x" title="65:155	Also, since we will be comparing against training on SEMCOR later (which was tagged using WordNet senses), we removed examples tagged with OntoNotes senses which were not mapped to WordNet senses." ></td>
	<td class="line x" title="66:155	On the whole, about 7% of the original OntoNotes examples were removed as a result." ></td>
	<td class="line x" title="67:155	1004 sense-tagged examples." ></td>
	<td class="line x" title="68:155	From the table, we see that sections 02-21, which will be used as training data in our experiments, contain a total of slightly over 31,000 sense-tagged examples." ></td>
	<td class="line x" title="69:155	Using examples from sections 02-21 as training data, we trained our WSD system and evaluated on the examples from section 23." ></td>
	<td class="line x" title="70:155	In our experiments, if a word type in section 23 has no training examples from sections 02-21, we randomly select an OntoNotes sense as the answer." ></td>
	<td class="line x" title="71:155	Using these experimental settings, our WSD system achieved an accuracy of 89.1%." ></td>
	<td class="line x" title="72:155	We note that this accuracy is much higher than the 60%70% accuracies achieved by state-of-the-art English all-words WSD systems which are trained using the fine-grained sense inventory of WordNet." ></td>
	<td class="line x" title="73:155	Hence, this highlights the importance of having an appropriate level of sense granularity." ></td>
	<td class="line x" title="74:155	Besides training on the entire set of examples from sections 02-21, we also investigated the performance achievable from training on various subsections of the data and show these results as ON in Figure 1." ></td>
	<td class="line x" title="75:155	From the figure, we see that WSD accuracy increases as we add more training examples." ></td>
	<td class="line x" title="76:155	The fact that current state-of-the-art WSD systems are able to achieve a high level of performance is important, as this means that WSD systems will potentially be more usable for inclusion in endapplications." ></td>
	<td class="line x" title="77:155	For instance, the high level of performance by syntactic parsers allows it to be used as an enabling technology in various NLP tasks." ></td>
	<td class="line x" title="78:155	Here, we note that the 89.1% WSD accuracy we obtained is comparable to state-of-the-art syntactic parsing accuracies, such as the 91.0% performance by the statistical parser of Charniak and Johnson (2005)." ></td>
	<td class="line x" title="79:155	4 Building WSD Systems with Out-of-Domain Data Although our WSD system had achieved a high accuracy of 89.1%, this was achieved by training on a large amount (about 31,000) of manually sense annotated examples from sections 02-21 of the OntoNotes data." ></td>
	<td class="line x" title="80:155	Further, all these training data and test data are gathered from the same domain of WSJ." ></td>
	<td class="line x" title="81:155	In reality, however, since manual sense annotation is time consuming, it is not feasible to collect such a large amount of manually sense-tagged data for every domain of interest." ></td>
	<td class="line x" title="82:155	Hence, in this section, we investigate the performance of our WSD system when it is trained on out-of-domain data." ></td>
	<td class="line x" title="83:155	In the English all-words task of the previous SENSEVAL evaluations (SENSEVAL-2, SENSEVAL3, SemEval-2007), the best performing English all-words task systems with the highest WSD accuracy were trained on SEMCOR (Mihalcea and Moldovan, 2001; Decadt et al., 2004; Chan et al., 2007b)." ></td>
	<td class="line x" title="84:155	Hence, we similarly trained our WSD system on SEMCOR and evaluated on section 23 of the OntoNotes corpus." ></td>
	<td class="line x" title="85:155	For those word types in section 23 which do not have training examples from SEMCOR, we randomly chose an OntoNotes sense as the answer." ></td>
	<td class="line x" title="86:155	In training on SEMCOR, we have also ensured that there is a domain difference between our training and test data." ></td>
	<td class="line x" title="87:155	This is because while the OntoNotes data was gathered from WSJ, which contains mainly business related news, the SEMCOR corpus is the sense-tagged portion of the Brown Corpus (BC), which is a mixture of several genres such as scientific texts, fictions, etc. Evaluating on the section 23 test data, our WSD system achieved only 76.2% accuracy." ></td>
	<td class="line x" title="88:155	Compared to the 89.1% accuracy achievable when we had trained on examples from sections 02-21, this is a substantially lower and disappointing drop of performance and motivates the need for domain adaptation." ></td>
	<td class="line x" title="89:155	The need for domain adaptation is a general and important issue for many NLP tasks (Daume III and Marcu, 2006)." ></td>
	<td class="line x" title="90:155	For instance, SRL systems are usually trained and evaluated on data drawn from the WSJ." ></td>
	<td class="line x" title="91:155	In the CoNLL-2005 shared task on SRL (Carreras and Marquez, 2005), however, a task of training and evaluating systems on different domains was included." ></td>
	<td class="line x" title="92:155	For that task, systems that were trained on the PropBank corpus (Palmer et al., 2005) (which was gathered from the WSJ), suffered a 10% drop in accuracy when evaluated on test data drawn from BC, as compared to the performance achievable when evaluated on data drawn from WSJ." ></td>
	<td class="line x" title="93:155	More recently, CoNLL-2007 included a shared task on dependency parsing (Nivre et al., 2007)." ></td>
	<td class="line x" title="94:155	In this task, systems that were trained on Penn Treebank (drawn from WSJ), but evaluated on data drawn from a different domain (such as chemical abstracts and parent-child dialogues) showed a similar drop in performance." ></td>
	<td class="line o" title="95:155	For research involving training and eval1005  55  60  65  70  75  80  85  90  95  100 02 02-03 02-04 02-05 02-06 02-07 02-08 02-09 02-10 02-12 02-14 02-21 WSD accuracy (%) Section number WSD Accuracies on Section 23 59.2 76.8 77.5 60.5 77.1 77.5 64.4 77.1 77.6 73.3 78.9 80.3 76.8 79.3 80.9 80.2 79.9 82.1 80.5 80.5 82.6 81.6 80.8 83.1 85.8 83.3 85.6 87.5 86.1 87.6 88.3 87.2 88.7 89.1 87.9 88.9 ON SC+ON SC+ON Augment Figure 1: WSD accuracies evaluated on section 23, using SEMCOR and different OntoNotes sections as training data." ></td>
	<td class="line x" title="96:155	ON: only OntoNotes as training data." ></td>
	<td class="line o" title="97:155	SC+ON: SEMCOR and OntoNotes as training data, SC+ON Augment: Combining SEMCOR and OntoNotes via the Augment domain adaptation technique." ></td>
	<td class="line x" title="98:155	uating WSD systems on data drawn from different domains, several prior research efforts (Escudero et al., 2000; Martinez and Agirre, 2000) observed a similar drop in performance of about 10% when a WSD system that was trained on the BC part of the DSO corpus was evaluated on the WSJ part of the corpus, and vice versa." ></td>
	<td class="line x" title="99:155	In the rest of this paper, we perform domain adaptation experiments for WSD, focusing on domain adaptation methods that use in-domain annotated data." ></td>
	<td class="line oc" title="100:155	In particular, we use a feature augmentation technique recently introduced by Daume III (2007), and active learning (Lewis and Gale, 1994) to perform domain adaptation of WSD systems." ></td>
	<td class="line oc" title="101:155	5 Combining In-Domain and Out-of-Domain Data for Training In this section, we will first introduce the AUGMENT technique of Daume III (2007), before showing the performance of our WSD system with and without using this technique." ></td>
	<td class="line pc" title="102:155	5.1 The AUGMENT technique for Domain Adaptation The AUGMENT technique introduced by Daume III (2007) is a simple yet very effective approach to performing domain adaptation." ></td>
	<td class="line o" title="103:155	This technique is applicable when one has access to training data from the source domain and a small amount of training data from the target domain." ></td>
	<td class="line o" title="104:155	The technique essentially augments the feature space of an instance." ></td>
	<td class="line x" title="105:155	Assuming x is an instance and its original feature vector is (x), the augmented feature vector for instance x is (x) = braceleftBigg < (x),(x),0 > if x  Ds < (x),0,(x) > if x  Dt , where 0 is a zero vector of size |(x)|, Ds and Dt are the sets of instances from the source and target domains respectively." ></td>
	<td class="line o" title="106:155	We see that the technique essentially treats the first part of the augmented feature space as holding general features that are not meant to be differentiated between different 1006 domains." ></td>
	<td class="line x" title="107:155	Then, different parts of the augmented feature space are reserved for holding source domain specific, or target domain specific features." ></td>
	<td class="line p" title="108:155	Despite its relative simplicity, this AUGMENT technique has been shown to outperform other domain adaptation techniques on various tasks such as named entity recognition, part-of-speech tagging, etc. 5.2 Experimental Results As mentioned in Section 4, training our WSD system on SEMCOR examples gave a relatively low accuracy of 76.2%, as compared to the 89.1% accuracy obtained from training on the OntoNotes section 0221 examples." ></td>
	<td class="line x" title="109:155	Assuming we have access to some indomain training data, then a simple method to potentially obtain better accuracies is to train on both the out-of-domain and in-domain examples." ></td>
	<td class="line x" title="110:155	To investigate this, we combined the SEMCOR examples with various amounts of OntoNotes examples to train our WSD system and show the resulting SC+ON accuracies obtained in Figure 1." ></td>
	<td class="line o" title="111:155	We also performed another set of experiments, where instead of simply combining the SEMCOR and OntoNotes examples, we applied the AUGMENT technique when combining these examples, treating SEMCOR examples as out-of-domain (source domain) data and OntoNotes examples as in-domain (target domain) data." ></td>
	<td class="line o" title="112:155	We similarly show the resulting accuracies as SC+ON Augment in Figure 1." ></td>
	<td class="line o" title="113:155	Comparing the SC+ON and SC+ON Augment accuracies in Figure 1, we see that the AUGMENT technique always helps to improve the accuracy of our WSD system." ></td>
	<td class="line o" title="114:155	Further, notice from the first few sets of results in the figure that when we have access to limited in-domain training examples from OntoNotes, incorporating additional outof-domain training data from SEMCOR (either using the strategies SC+ON or SC+ON Augment) achieves better accuracies than ON." ></td>
	<td class="line x" title="115:155	Significance tests using one-tailed paired t-test reveal that these accuracy improvements are statistically significant at the level of significance 0.01 (all significance tests in the rest of this paper use the same level of significance 0.01)." ></td>
	<td class="line x" title="116:155	These results validate the contribution of the SemCor examples." ></td>
	<td class="line x" title="117:155	This trend continues till the result for sections 02-06." ></td>
	<td class="line o" title="118:155	The right half of Figure 1 shows the accuracy trend of the various strategies, in the unlikely event DSthe set of SEMCOR training examples DAthe set of OntoNotes sections 02-21 examples DT empty while DAnegationslash=  pmin WSD system trained on DS and DT using AUGMENT technique for each dDA do bsword sense prediction for d using  pconfidence of prediction bs if p < pmin then pminp, dmind end end DADA{dmin} provide correct sense s for dmin and add dmin to DT end Figure 2: The active learning algorithm." ></td>
	<td class="line x" title="119:155	that we have access to a large amount of in-domain training examples." ></td>
	<td class="line p" title="120:155	Although we observe that in this scenario, ON performs better than SC+ON, SC+ON Augment continues to perform better than ON (where the improvement is statistically significant) till the result for sections 02-09." ></td>
	<td class="line o" title="121:155	Beyond that, as we add more OntoNotes examples, significance testing reveals that the SC+ON Augment and ON strategies give comparable performance." ></td>
	<td class="line p" title="122:155	This means that the SC+ON Augment strategy, besides giving good performance when one has few in-domain examples, does continue to perform well even when one has a large number of in-domain examples." ></td>
	<td class="line p" title="123:155	6 Active Learning with AUGMENT Technique So far in this paper, we have seen that when we have access to some in-domain examples, a good strategy is to combine the out-of-domain and in-domain examples via the AUGMENT technique." ></td>
	<td class="line x" title="124:155	This suggests that when one wishes to apply a WSD system to a new domain of interest, it is worth the effort to annotate a small number of examples gathered from the new domain." ></td>
	<td class="line x" title="125:155	However, instead of randomly selecting in-domain examples to annotate, we could use active learning (Lewis and Gale, 1994) to help select in-domain examples to annotate." ></td>
	<td class="line x" title="126:155	By doing so, we could minimize the manual annotation effort needed." ></td>
	<td class="line o" title="127:155	1007 WSD Accuracies on Section 23 76 78 80 82 84 86 88 90 SemCor 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 Iteration Number W SD  A cc ur ac y ( %) 50 100 150 200 300 400 500 all Figure 3: Results of applying active learning with the AUGMENT technique on different number of word types." ></td>
	<td class="line x" title="128:155	Each curve represents the adaptation process of applying active learning on a certain number of most frequently occurring word types." ></td>
	<td class="line x" title="129:155	In WSD, several prior research efforts have successfully used active learning to reduce the annotation effort required (Zhu and Hovy, 2007; Chan and Ng, 2007; Chen et al., 2006; Fujii et al., 1998)." ></td>
	<td class="line x" title="130:155	With the exception of (Chan and Ng, 2007) which tried to adapt a WSD system trained on the BC part of the DSO corpus to the WSJ part of the DSO corpus, the other researchers simply applied active learning to reduce the annotation effort required and did not deal with the issue of adapting a WSD system to a new domain." ></td>
	<td class="line x" title="131:155	Also, these prior research efforts only experimented with a few word types." ></td>
	<td class="line x" title="132:155	In contrast, we perform active learning experiments on the hundreds of word types in the OntoNotes data, with the aim of adapting our WSD system trained on SEMCOR to the WSJ domain represented by the OntoNotes data." ></td>
	<td class="line x" title="133:155	For our active learning experiments, we use the uncertainty sampling strategy (Lewis and Gale, 1994), as shown in Figure 2." ></td>
	<td class="line x" title="134:155	For our experiments, the SEMCOR examples will be our initial set of training examples, while the OntoNotes examples from sections 02-21 will be used as our pool of adaptation examples, from which we will select examples to annotate via active learning." ></td>
	<td class="line p" title="135:155	Also, since we have found that the AUGMENT technique is useful in increasing WSD accuracy, we will apply the AUGMENT technique during each iteration of active learning to combine the SEMCOR examples and the selected adaptation examples." ></td>
	<td class="line x" title="136:155	As shown in Figure 2, we train an initial WSD system using only the set DS of SEMCOR examples." ></td>
	<td class="line x" title="137:155	We then apply our WSD system on the set DA of OntoNotes adaptation examples." ></td>
	<td class="line x" title="138:155	The example in DA which is predicted with the lowest confidence will be removed from DA and added to the set DT of in-domain examples that have been selected via active learning thus far." ></td>
	<td class="line o" title="139:155	We then use the AUGMENT technique to combine the set of examples in DS and DT to train a new WSD system, which is then applied again on the set DA of remaining adaptation examples, and this active learning process continues until we have used up all the adaptation examples." ></td>
	<td class="line x" title="140:155	Note that because we are using OntoNotes sections 02-21 (which have already been sense-tagged beforehand) as our adaptation data, the annotation of the selected example during each active learning iteration is simply simulated by referring to its tagged sense." ></td>
	<td class="line x" title="141:155	6.1 Experimental Results As mentioned earlier, we use the examples in OntoNotes sections 02-21 as our adaptation exam1008 ples during active learning." ></td>
	<td class="line x" title="142:155	Hence, we perform active learning experiments on all the word types that have sense-tagged examples from OntoNotes sections 02-21, and show the evaluation results on OntoNotes section 23 as the topmost all curve in Figure 3." ></td>
	<td class="line x" title="143:155	Since our aim is to reduce the human annotation effort required in adapting a WSD system to a new domain, we may not want to perform active learning on all the word types in practice." ></td>
	<td class="line x" title="144:155	Instead, we can maximize the benefits by performing active learning only on the more frequently occurring word types." ></td>
	<td class="line x" title="145:155	Hence, in Figure 3, we also show via various curves the results of applying active learning only to various sets of word types, according to their frequency, or number of sense-tagged examples in OntoNotes sections 02-21." ></td>
	<td class="line x" title="146:155	Note that the various accuracy curves in Figure 3 are plotted in terms of evaluation accuracies over all the test examples in OntoNotes section 23, hence they are directly comparable to the results reported thus far in this paper." ></td>
	<td class="line x" title="147:155	Also, since the accuracies for the various curves stabilize after 35 active learning iterations, we only show the results of the first 35 iterations." ></td>
	<td class="line x" title="148:155	From Figure 3, we note that by performing active learning on the set of 150 most frequently occurring word types, we are able to achieve a WSD accuracy of 82.6% after 10 active learning iterations." ></td>
	<td class="line x" title="149:155	Note that in Section 4, we mentioned that training only on the out-of-domain SEMCOR examples gave an accuracy of 76.2%." ></td>
	<td class="line x" title="150:155	Hence, we have gained an accuracy improvement of 6.4% (82.6%  76.2%) by just using 1,500 in-domain OntoNotes examples." ></td>
	<td class="line x" title="151:155	Compared with the 12.9% (89.1%  76.2%) improvement in accuracy achieved by using all 31,114 OntoNotes sections 02-21 examples, we have obtained half of this maximum increase in accuracy, by requiring only about 5% (1,500/31,114) of the total number of sense-tagged examples." ></td>
	<td class="line p" title="152:155	Based on these results, we propose that when there is a need to apply a previously trained WSD system to a different domain, one can apply the AUGMENT technique with active learning on the most frequent word types, to greatly reduce the annotation effort required while obtaining a substantial improvement in accuracy." ></td>
	<td class="line x" title="153:155	7 Conclusion Using the WSD examples made available through OntoNotes, which are sense-tagged according to a coarse-grained sense inventory, we show that our WSD system is able to achieve a high accuracy of 89.1% when we train and evaluate on these examples." ></td>
	<td class="line x" title="154:155	However, when we apply a WSD system that is trained on SEMCOR, we suffer a substantial drop in accuracy, highlighting the need to perform domain adaptation." ></td>
	<td class="line p" title="155:155	We show that by combining the AUGMENT domain adaptation technique with active learning, we are able to effectively reduce the amount of annotation effort required for domain adaptation." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="I08-2097
Learning Reliability of Parses for Domain Adaptation of Dependency Parsing
Kawahara, Daisuke;Uchimoto, Kiyotaka;"></td>
	<td class="line x" title="1:160	Learning Reliability of Parses for Domain Adaptation of Dependency Parsing Daisuke Kawahara and Kiyotaka Uchimoto National Institute of Information and Communications Technology, 3-5 Hikaridai Seika-cho Soraku-gun, Kyoto, 619-0289, Japan {dk, uchimoto}@nict.go.jp Abstract The accuracy of parsing has exceeded 90% recently, but this is not high enough to use parsing results practically in natural languageprocessing(NLP)applicationssuchas paraphrase acquisition and relation extraction." ></td>
	<td class="line x" title="2:160	We present a method for detecting reliable parses out of the outputs of a single dependency parser." ></td>
	<td class="line x" title="3:160	This technique is also applied to domain adaptation of dependency parsing." ></td>
	<td class="line x" title="4:160	Our goal was to improve the performance of a state-of-the-art dependency parser on the data set of the domain adaptation track of the CoNLL 2007 shared task, a formidable challenge." ></td>
	<td class="line x" title="5:160	1 Introduction Dependency parsing has been utilized in a variety of natural language processing (NLP) applications, such as paraphrase acquisition, relation extraction and machine translation." ></td>
	<td class="line x" title="6:160	For newspaper articles, the accuracy of dependency parsers exceeds 90% (for English), but it is still not sufcient for practical use in these NLP applications." ></td>
	<td class="line x" title="7:160	Moreover, the accuracy declinessignicantlyforout-of-domaintext, suchas weblogsandwebpages,whichhavecommonlybeen used as corpora." ></td>
	<td class="line x" title="8:160	From this point of view, it is important to consider the following points to use a parser practically in applications:  to select reliable parses, especially for knowledge acquisition,  and to adapt the parser to new domains." ></td>
	<td class="line x" title="9:160	This paper proposes a method for selecting reliable parses from parses output by a single dependency parser." ></td>
	<td class="line x" title="10:160	We do not use an ensemble method based on multiple parsers, but use only a single parser, because speed and efciency are important when processing a massive volume of text." ></td>
	<td class="line x" title="11:160	The resulting highly reliable parses would be useful to automatically construct dictionaries and knowledge bases, such as case frames (Kawahara and Kurohashi, 2006)." ></td>
	<td class="line x" title="12:160	Furthermore, we incorporate the reliable parses we obtained into the dependency parser to achieve domain adaptation." ></td>
	<td class="line x" title="13:160	The CoNLL 2007 shared task tackled domain adaptation of dependency parsers for the rst time (Nivre et al., 2007)." ></td>
	<td class="line x" title="14:160	Sagae and Tsujii applied an ensemble method to the domain adaptation track and achieved the highest score (Sagae and Tsujii, 2007)." ></td>
	<td class="line x" title="15:160	They rst parsed in-domain unlabeled sentences using two parsers trained on out-of-domain labeled data." ></td>
	<td class="line x" title="16:160	Then, they extracted identical parses that were produced by the two parsers and added them to the original (out-of-domain) training set to train a domain-adapted model." ></td>
	<td class="line x" title="17:160	Dredze et al. yielded the second highest score1 in the domain adaptation track (Dredze et al., 2007)." ></td>
	<td class="line x" title="18:160	However, their results were obtained without adaptation." ></td>
	<td class="line x" title="19:160	They concluded that it is very difcult to substantially improve the target domain performance over that of a state-of-the-art parser." ></td>
	<td class="line x" title="20:160	To conrm this, we parsed the test set (CHEM) of the domain adaptationtrackbyusingoneofthebestdependency parsers, second-order MSTParser (McDonald et al., 1Dredze et al. achieved the second highest score on the CHEM test set for unlabeled dependency accuracy." ></td>
	<td class="line x" title="21:160	709 2006)2." ></td>
	<td class="line x" title="22:160	Though this parser was trained on the provided out-of-domain (Penn Treebank) labeled data, surprisingly, its accuracy slightly outperformed the highest score achieved by Sagae and Tsujii (unlabeled dependency accuracy: 83.58 > 83.42 (Sagae and Tsujii, 2007))." ></td>
	<td class="line x" title="23:160	Our goal is to improve a stateof-the-art parser on this domain adaptation track." ></td>
	<td class="line x" title="24:160	Dredze et al. also indicated that unlabeled dependency parsing is not robust to domain adaptation (Dredze et al., 2007)." ></td>
	<td class="line x" title="25:160	This paper therefore focuses on unlabeled dependency parsing." ></td>
	<td class="line x" title="26:160	2 Related Work We have already described the domain adaptation track of the CoNLL 2007 shared task." ></td>
	<td class="line x" title="27:160	For the multilingual dependency parsing track, which was the other track of the shared task, Nilsson et al. achieved the best performance using an ensemble method (Hall et al., 2007)." ></td>
	<td class="line x" title="28:160	They used a method of combining several parsers outputs in the framework of MST parsing (Sagae and Lavie, 2006)." ></td>
	<td class="line x" title="29:160	This method does not select parses, but considers all the output parses with weights to decide a nal parse of a given sentence." ></td>
	<td class="line x" title="30:160	Reichart and Rappoport also proposed an ensemble method to select high-quality parses from the outputs of constituency parsers (Reichart and Rappoport, 2007a)." ></td>
	<td class="line x" title="31:160	They regarded parses as being of high quality if 20 different parsers agreed." ></td>
	<td class="line x" title="32:160	They did notapplytheirmethodtodomainadaptationorother applications." ></td>
	<td class="line x" title="33:160	Reranking methods for parsing have a relation to parse selection." ></td>
	<td class="line x" title="34:160	They rerank the n-best parses that are output by a generative parser using a lot of lexical and syntactic features (Collins and Koo, 2005; Charniak and Johnson, 2005)." ></td>
	<td class="line x" title="35:160	There are several related methods for 1-best outputs, such as revision learning (Nakagawa et al., 2002) and transformation-based learning (Brill, 1995) for partof-speech tagging." ></td>
	<td class="line x" title="36:160	Attardi and Ciaramita proposed a method of tree revision learning for dependency parsing (Attardi and Ciaramita, 2007)." ></td>
	<td class="line x" title="37:160	As for the use of unlabeled data, self-training methods have been successful in recent years." ></td>
	<td class="line x" title="38:160	McClosky et al. improved a state-of-the-art constituency parser by 1.1% using self-training (Mc2http://sourceforge.net/projects/mstparser/ Table 1: Labeled and unlabeled data provided for the shared task." ></td>
	<td class="line x" title="39:160	The labeled PTB data is used for training, and the labeled BIO data is used for development." ></td>
	<td class="line x" title="40:160	ThelabeledCHEMdataisusedforthenal test." ></td>
	<td class="line x" title="41:160	name source labeled unlabeled PTB Penn Treebank 18,577 1,625,606 BIO Penn BioIE 200 369,439 CHEM Penn BioIE 200 396,128 Closkyetal., 2006a)." ></td>
	<td class="line x" title="42:160	Theyalsoappliedself-training to domain adaptation of a constituency parser (McClosky et al., 2006b)." ></td>
	<td class="line x" title="43:160	Their method simply adds parsed unlabeled data without selecting it to the training set." ></td>
	<td class="line x" title="44:160	Reichart and Rappoport applied selftraining to domain adaptation using a small set of in-domain training data (Reichart and Rappoport, 2007b)." ></td>
	<td class="line x" title="45:160	Van Noord extracted bilexical preferences from a Dutch parsed corpus of 500M words without selection(vanNoord, 2007)." ></td>
	<td class="line x" title="46:160	Headdedsomefeaturesinto an HPSG (head-driven phrase structure grammar) parser to consider the bilexical preferences, and obtained an improvement of 0.5% against a baseline." ></td>
	<td class="line x" title="47:160	Kawahara and Kurohashi extracted reliable dependencies from automatic parses of Japanese sentences on the web to construct large-scale case frames (Kawahara and Kurohashi, 2006)." ></td>
	<td class="line x" title="48:160	Then they incorporated the constructed case frames into a probabilistic dependency parser, and outperformed their baseline parser by 0.7%." ></td>
	<td class="line x" title="49:160	3 The Data Set This paper uses the data set that was used in the CoNLL 2007 shared task (Nivre et al., 2007)." ></td>
	<td class="line x" title="50:160	Table 1 lists the data set provided for the domain adaptation track." ></td>
	<td class="line x" title="51:160	We pre-processed all the unlabeled sentences using a conditional random elds (CRFs)-based partof-speech tagger." ></td>
	<td class="line x" title="52:160	This tagger is trained on the PTB training set that consists of 18,577 sentences." ></td>
	<td class="line x" title="53:160	The features are the same as those in (Ratnaparkhi, 1996)." ></td>
	<td class="line x" title="54:160	As an implementation of CRFs, we used CRF++3." ></td>
	<td class="line x" title="55:160	If a method of domain adaptation is applied to the tagger, the accuracy of parsing unlabeled sentences will improve (Yoshida et al., 2007)." ></td>
	<td class="line x" title="56:160	This 3http://crfpp.sourceforge.net/ 710 paper, however, does not deal with domain adaptation of a tagger but focuses on that of a parser." ></td>
	<td class="line x" title="57:160	4 Learning Reliability of Parses Our approach assesses automatic parses of a single parser in order to select only reliable parses from them." ></td>
	<td class="line x" title="58:160	We compare automatic parses and their goldstandard ones, and regard accurate parses as positive examples and the remainder as negative examples." ></td>
	<td class="line x" title="59:160	Based on these examples, we build a binary classier that classies each sentence as reliable or not." ></td>
	<td class="line x" title="60:160	To precisely detect reliable parses, we make use of several linguistic features inspired by the notion of controlled language (Mitamura et al., 1991)." ></td>
	<td class="line x" title="61:160	That is to say, the reliability of parses is judged based on the degree of sentence difculty." ></td>
	<td class="line x" title="62:160	Beforedescribingourbasedependencyparserand the algorithm for detecting reliable parses, we rst explain the data sets used for them." ></td>
	<td class="line x" title="63:160	We prepared the following three labeled data sets to train the base dependency parser and the reliability detector." ></td>
	<td class="line x" title="64:160	PTB base train: training set for the base parser: 14,862 sentences PTB rel train: training set for reliability detector: 2,500 sentences4 BIO rel dev: development set for reliability detector: 200 sentences (= labeled BIO data) PTB base train is used to train the base dependency parser, and PTB rel train is used to train our reliability detector." ></td>
	<td class="line x" title="65:160	BIO rel dev is used for tuning the parameters of the reliability detector." ></td>
	<td class="line x" title="66:160	4.1 Base Dependency Parser We used the MSTParser (McDonald et al., 2006), which achieved top results in the CoNLL 2006 (CoNLL-X) shared task, as a base dependency parser." ></td>
	<td class="line x" title="67:160	To enable second-order features, the parameter order was set to 2." ></td>
	<td class="line x" title="68:160	The other parameters were set to default." ></td>
	<td class="line x" title="69:160	We used PTB base train (14,862 sentences) to train this parser." ></td>
	<td class="line x" title="70:160	4.2 Algorithm to Detect Reliable Parses Webuiltabinaryclassierfordetectingreliablesentences from a set of automatic parses produced by 41,215 labeled PTB sentences are left as another development set for the reliability detector, but they are not used in this paper." ></td>
	<td class="line x" title="71:160	the base dependency parser." ></td>
	<td class="line x" title="72:160	We used support vector machines (SVMs) as a binary classier with a third-degree polynomial kernel." ></td>
	<td class="line x" title="73:160	We parsed PTB rel train (2,500 sentences) usingthebaseparser, andevaluatedeachsentencewith the metric of unlabeled dependency accuracy." ></td>
	<td class="line x" title="74:160	We regarded the sentences whose accuracy is better than a threshold, , as positive examples, and the others as negative ones." ></td>
	<td class="line x" title="75:160	In this experiment, we set the accuracy threshold  at 100%." ></td>
	<td class="line x" title="76:160	As a result, 736 out of 2,500 examples (sentences) were judged to be positive." ></td>
	<td class="line x" title="77:160	To evaluate the reliability of parses, we take advantage of the following features that can be related to the difculty of sentences." ></td>
	<td class="line x" title="78:160	sentence length: The longer the sentence is, the poorer the parser performs (McDonald and Nivre, 2007)." ></td>
	<td class="line x" title="79:160	We determine sentence length by the number of words." ></td>
	<td class="line x" title="80:160	dependency lengths: Long-distance dependencies exhibit bad performance (McDonald and Nivre, 2007)." ></td>
	<td class="line x" title="81:160	We calculate the average of the dependency length of each word." ></td>
	<td class="line x" title="82:160	difculty of vocabulary: It is hard for supervisedparserstolearndependenciesthatincludelowfrequency words." ></td>
	<td class="line x" title="83:160	We count word frequencies in the training data and make a word list in descending order of frequency." ></td>
	<td class="line x" title="84:160	For a given sentence, we calculate the average frequency rank of each word." ></td>
	<td class="line x" title="85:160	number of unknown words: Similarly, dependency accuracy for unknown words is notoriously poor." ></td>
	<td class="line x" title="86:160	We count the number of unknown words in a given sentence." ></td>
	<td class="line x" title="87:160	number of commas: Sentences with multiple commas are difcult to parse." ></td>
	<td class="line x" title="88:160	We count the number of commas in a given sentence." ></td>
	<td class="line x" title="89:160	number of conjunctions (and/or): Sentences with coordinate structures are also difcult to parse (Kurohashi and Nagao, 1994)." ></td>
	<td class="line x" title="90:160	We count the number of coordinate conjunctions (and/or) in a given sentence." ></td>
	<td class="line x" title="91:160	To apply these features to SVMs in practice, the numbers are binned at a certain interval for each feature." ></td>
	<td class="line x" title="92:160	For instance, the number of conjunctions is split into four bins: 0, 1, 2 and more than 2." ></td>
	<td class="line x" title="93:160	711 Table 2: Example BIO sentences judged as reliable." ></td>
	<td class="line x" title="94:160	The underlined words have incorrect modifying heads." ></td>
	<td class="line x" title="95:160	dep." ></td>
	<td class="line x" title="96:160	accuracy sentences judged as reliable 12/12 (100%) No mutations resulting in truncation of the APC protein were found . 12/13 (92%) Conventional imaging techniques did not show two in 10 of these patients . 6/6 (100%) Pancreatic juice was sampled endoscopically . 11/12 (92%) The specicity of p53 mutation for pancreatic cancer is very high . 9/10 (90%) K-ras mutations are early genetic changes in colon cancer .  0  10  20  30  40  50  60  70  80  90  100  80  82  84  86  88  90  92  94  96  98  100 Sentence coverage (%) Dependency accuracy (%) Figure 1: Accuracy-coverage curve on BIO rel dev." ></td>
	<td class="line x" title="97:160	4.3 Experiments on Detecting Reliable Parses We conducted an experiment on detecting the reliability of parses." ></td>
	<td class="line x" title="98:160	Our detector was applied to the automatic parses of BIO rel dev, and only reliable parses were selected from them." ></td>
	<td class="line x" title="99:160	When parsing this set, the POS tags contained in the set were substituted with automatic POS tags because it is preferable to have the same environment as when applying the parser to unlabeled data." ></td>
	<td class="line x" title="100:160	We evaluated unlabeled dependency accuracy of the extracted parses." ></td>
	<td class="line x" title="101:160	The accuracy-coverage curve shown in Figure 1 was obtained by changing the soft margin parameter C 5 of SVMs from 0.0001 to 10." ></td>
	<td class="line x" title="102:160	In this gure, the coverage is the ratio of selected sentences out of all the sentences (200 sentences), and the accuracy is unlabeled dependency accuracy." ></td>
	<td class="line x" title="103:160	A coverage of 100% indicates that the accuracy of 200 sentences without any selection was 80.85%." ></td>
	<td class="line x" title="104:160	If the soft margin parameter C is set to 0.001, we can obtain 19 sentences out of 200 at a dependency accuracy of 93.85% (183/195)." ></td>
	<td class="line x" title="105:160	The average sentence length was 10.3 words." ></td>
	<td class="line x" title="106:160	Out of obtained 19 sentences, 14 sentences achieved a dependency accuracy of 100%, and thus the precision of the reliability detector itself was 73.7% (14/19)." ></td>
	<td class="line x" title="107:160	Out of 200 sentences, 36 sentences were correctly parsed by the 5A higher soft margin value allows more classication errors, and thus leads to the increase of recall and the decrease of precision." ></td>
	<td class="line x" title="108:160	base parser, and thus the recall is 38.9% (14/36)." ></td>
	<td class="line x" title="109:160	Table 2 shows some sentences that were evaluated as reliable using the above setting (C = 0.001)." ></td>
	<td class="line x" title="110:160	Major errors were caused by prepositional phrase (PP)attachment." ></td>
	<td class="line x" title="111:160	To improve the accuracy of detecting reliable parses, it would be necessary to consider the numberofPP-attachmentambiguitiesinagivensentence as a feature." ></td>
	<td class="line x" title="112:160	5 Domain Adaptation of Dependency Parsing For domain adaptation, we adopt a self-training method." ></td>
	<td class="line x" title="113:160	We combine in-domain unlabeled (automatically labeled) data with out-of-domain labeled data to make a training set." ></td>
	<td class="line oc" title="114:160	There are many possible methods for combining unlabeled and labeled data (Daume III, 2007), but we simply concatenate unlabeled data with labeled data to see the effectiveness of the selected reliable parses." ></td>
	<td class="line x" title="115:160	The in-domain unlabeled data to be added are selected by the reliability detector." ></td>
	<td class="line x" title="116:160	We set the soft margin parameter at 0.001 to extract highly reliable parses." ></td>
	<td class="line x" title="117:160	As mentioned in the previous section, the accuracy of selected parses was approximately 94%." ></td>
	<td class="line x" title="118:160	We parsed the unlabeled sentences of BIO and CHEM(approximately400Ksentencesforeach)using the base dependency parser that is trained on the entire PTB labeled data." ></td>
	<td class="line x" title="119:160	Then, we applied the reliability detector to these parsed sentences to obtain 31,266 sentences for BIO and 31,470 sentences for CHEM." ></td>
	<td class="line x" title="120:160	We call the two sets of obtained sentences  BIO pool and  CHEM pool . For each training set of the experiments described below, a certain number of sentences are randomly selected from the pool and combined with the entire out-of-domain (PTB) labeled data." ></td>
	<td class="line x" title="121:160	5.1 Experiment on BIO Development Data We rst conducted an experiment of domain adaptation using the BIO development set." ></td>
	<td class="line x" title="122:160	712  83  83.5  84  84.5  85  0  5000  10000  15000  20000  25000 Accuracy (%) Number of Unlabeled Sentences reliable parsesrandomly selected parses without addition Figure 2: Dependency accuracies on BIO when the number of added unlabeled data is changed." ></td>
	<td class="line x" title="123:160	Figure 2 shows how the accuracy changes when the number of added reliable parses is changed." ></td>
	<td class="line x" title="124:160	The solid line represents our proposed method, and the dotted line with points represents a baseline method." ></td>
	<td class="line x" title="125:160	This baseline is a self-training method that simply adds unlabeled data without selection to the PTB labeled data." ></td>
	<td class="line x" title="126:160	Each experimental result is the average of ve trials done to randomly select a certain number of parses from the BIO pool." ></td>
	<td class="line x" title="127:160	The horizontal dotted line (84.07%) represents the accuracy of the parser without adding unlabeled data (trained only on the PTB labeled data)." ></td>
	<td class="line x" title="128:160	From this gure, we can see that the proposed method always outperforms the baseline by approximately 0.4%." ></td>
	<td class="line x" title="129:160	The best accuracy was achieved when 18,000 unlabeled parses were added." ></td>
	<td class="line x" title="130:160	However, if more than 18,000 sentences are added, the accuracy declines." ></td>
	<td class="line x" title="131:160	This can be attributed to the balance of the number of labeled data and unlabeled data." ></td>
	<td class="line x" title="132:160	Since the number of added unlabeled data is more than the number of labeled data, the entire training set might be unreliable, though the accuracy of added unlabeled data is relatively high." ></td>
	<td class="line x" title="133:160	To address this problem, it is necessary to weigh labeled data or to change the way information from acquired unlabeled data is handled." ></td>
	<td class="line x" title="134:160	5.2 Experiment on CHEM Test Data The addition of 18,000 sentences showed the highest accuracy for the BIO development data." ></td>
	<td class="line x" title="135:160	To adapt the parser to the CHEM test set, we used 18,000 reliable unlabeled sentences from the CHEM pool with the PTB labeled sentences to train the parser." ></td>
	<td class="line x" title="136:160	Table 3 lists the experimental results." ></td>
	<td class="line x" title="137:160	In this table, the Table 3: Experimental results on CHEM test data." ></td>
	<td class="line x" title="138:160	system accuracy PTB+unlabel (18,000 sents.)" ></td>
	<td class="line x" title="139:160	84.12 only PTB (baseline) 83.58 1st (Sagae and Tsujii, 2007) 83.42 2nd (Dredze et al., 2007) 83.38 3rd (Attardi et al., 2007) 83.08 third row lists the three highest scores of the domain adaptation track of the CoNLL 2007 shared task." ></td>
	<td class="line x" title="140:160	The baseline parser was trained only on the PTB labeled data (as described in Section 1)." ></td>
	<td class="line x" title="141:160	The proposed method (PTB+unlabel (18,000 sents.)) outperformed the baseline by approximately 0.5%, and also beat all the systems submitted to the domain adaptation track." ></td>
	<td class="line x" title="142:160	These systems include an ensemble method (Sagae and Tsujii, 2007) and an approach of tree revision learning with a selection method of only using short training sentences (shorter than 30 words) (Attardi et al., 2007)." ></td>
	<td class="line x" title="143:160	6 Discussion and Conclusion This paper described a method for detecting reliable parses out of the outputs of a single dependency parser." ></td>
	<td class="line x" title="144:160	This technique was also applied to domain adaptation of dependency parsing." ></td>
	<td class="line x" title="145:160	To extract reliable parses, we did not adopt an ensemble method, but used a single-parser approach because speed and efciency are important in processing a gigantic volume of text to benet knowledge acquisition." ></td>
	<td class="line x" title="146:160	In this paper, we employed the MSTParser, which can process 3.9 sentences/s on a XEON 3.0GHz machine in spite of the time complexity of O(n3)." ></td>
	<td class="line x" title="147:160	If greater efciency is required, it is possible to apply a pre-lter that removes long sentences (e.g., longer than 30 words), which are seldom selected by the reliability detector." ></td>
	<td class="line x" title="148:160	In addition, our method does not depend on a particular parser, and can be applied to other state-of-theart parsers, such as Malt Parser (Nivre et al., 2006), which is a feature-rich linear-time parser." ></td>
	<td class="line x" title="149:160	In general, it is very difcult to improve the accuracy of the best performing systems by using unlabeled data." ></td>
	<td class="line x" title="150:160	There are only a few successful studies, such as (Ando and Zhang, 2005) for chunking and (McClosky et al., 2006a; McClosky et al., 2006b) on constituency parsing." ></td>
	<td class="line x" title="151:160	We succeeded in boosting the accuracy of the second-order MST parser, which is 713 a state-of-the-art dependency parser, in the CoNLL 2007 domain adaptation task." ></td>
	<td class="line x" title="152:160	This was a difcult challenge as many participants in the task failed to obtain any meaningful gains from unlabeled data (Dredze et al., 2007)." ></td>
	<td class="line x" title="153:160	The key factor in our success was the extraction of only reliable information from unlabeled data." ></td>
	<td class="line x" title="154:160	However, that improvement was not satisfactory." ></td>
	<td class="line x" title="155:160	In order to achieve more gains, it is necessary to exploit a much larger number of unlabeled data." ></td>
	<td class="line x" title="156:160	In this paper, we adopted a simple method to combine unlabeled data with labeled data." ></td>
	<td class="line x" title="157:160	To use this method more effectively, we need to balance the labeled and unlabeled data very carefully." ></td>
	<td class="line x" title="158:160	However, this method is not scalable because the training time increases signicantlyasthesizeofatrainingsetexpands." ></td>
	<td class="line x" title="159:160	We can consider the information from more unlabeled data as features of machine learning techniques." ></td>
	<td class="line x" title="160:160	Another approach is to formalize a probabilistic model based on unlabeled data." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-1029
Exploiting Feature Hierarchy for Transfer Learning in Named Entity Recognition
Arnold, Andrew;Nallapati, Ramesh;Cohen, William W.;"></td>
	<td class="line x" title="1:161	Proceedings of ACL-08: HLT, pages 245253, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:161	c2008 Association for Computational Linguistics Exploiting Feature Hierarchy for Transfer Learning in Named Entity Recognition Andrew Arnold, Ramesh Nallapati and William W. Cohen Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA {aarnold, nmramesh, wcohen}@cs.cmu.edu Abstract We present a novel hierarchical prior structure for supervised transfer learning in named entity recognition, motivated by the common structure of feature spaces for this task across natural language data sets." ></td>
	<td class="line x" title="3:161	The problem of transfer learning, where information gained in one learning task is used to improve performance in another related task, is an important new area of research." ></td>
	<td class="line x" title="4:161	In the subproblem of domain adaptation, a model trained over a source domain is generalized to perform well on a related target domain, where the two domains data are distributed similarly, but not identically." ></td>
	<td class="line x" title="5:161	We introduce the concept of groups of closely-related domains, called genres, and show how inter-genre adaptation is related to domain adaptation." ></td>
	<td class="line x" title="6:161	We also examine multitask learning, where two domains may be related, but where the concept to be learned in each case is distinct." ></td>
	<td class="line x" title="7:161	We show that our prior conveys useful information across domains, genres and tasks, while remaining robust to spurious signals not related to the target domain and concept." ></td>
	<td class="line x" title="8:161	We further show that our model generalizes a class of similar hierarchical priors, smoothed to varying degrees, and lay the groundwork for future exploration in this area." ></td>
	<td class="line x" title="9:161	1 Introduction 1.1 Problem definition Consider the task of named entity recognition (NER)." ></td>
	<td class="line x" title="10:161	Specifically, you are given a corpus of news articles in which all tokens have been labeled as either belonging to personal name mentions or not." ></td>
	<td class="line x" title="11:161	The standard supervised machine learning problem is to learn a classifier over this training data that will successfully label unseen test data drawn from the same distribution as the training data, where same distribution could mean anything from having the train and test articles written by the same author to having them written in the same language." ></td>
	<td class="line x" title="12:161	Having successfully trained a named entity classifier on this news data, now consider the problem of learning to classify tokens as names in e-mail data." ></td>
	<td class="line x" title="13:161	An intuitive solution might be to simply retrain the classifier, de novo, on the e-mail data." ></td>
	<td class="line x" title="14:161	Practically, however, large, labeled datasets are often expensive to build and this solution would not scale across a large number of different datasets." ></td>
	<td class="line x" title="15:161	Clearly the problems of identifying names in news articles and e-mails are closely related, and learning to do well on one should help your performance on the other." ></td>
	<td class="line x" title="16:161	At the same time, however, there are serious differences between the two problems that need to be addressed." ></td>
	<td class="line x" title="17:161	For instance, capitalization, which will certainly be a useful feature in the news problem, may prove less informative in the e-mail data since the rules of capitalization are followed less strictly in that domain." ></td>
	<td class="line x" title="18:161	These are the problems we address in this paper." ></td>
	<td class="line x" title="19:161	In particular, we develop a novel prior for named entity recognition that exploits the hierarchical feature space often found in natural language domains (1.2) and allows for the transfer of information from labeled datasets in other domains (1.3)." ></td>
	<td class="line x" title="20:161	2 introduces the maximum entropy (maxent) and conditional random field (CRF) learning techniques employed, along with specifications for the design and training of our hierarchical prior." ></td>
	<td class="line x" title="21:161	Finally, in 3 we present an empirical investigation of our priors performance against a number of baselines, demonstrating both its effectiveness and robustness." ></td>
	<td class="line x" title="22:161	1.2 Hierarchical feature trees In many NER problems, features are often constructed as a series of transformations of the input training data, performed in sequence." ></td>
	<td class="line x" title="23:161	Thus, if our task is to identify tokens as either being (O)utside or (I)nside person names, and we are given the labeled 245 sample training sentence: O O O O O I Give the book to Professor Caldwell (1) one such useful feature might be: Is the token one slot to the left of the current token Professor?" ></td>
	<td class="line x" title="24:161	We can represent this symbolically as L.1.Professor where we describe the whole space of useful features of this form as: {direction = (L)eft, (C)urrent, (R)ight}.{distance = 1, 2, 3, }.{value = Professor, book, }." ></td>
	<td class="line x" title="25:161	We can conceptualize this structure as a tree, where each slot in the symbolic name of a feature is a branch and each period between slots represents another level, going from root to leaf as read left to right." ></td>
	<td class="line x" title="26:161	Thus a subsection of the entire feature tree for the token Caldwell could be drawn as in Figure 1 (zoomed in on the section of the tree where the L.1.Professor feature resides)." ></td>
	<td class="line x" title="27:161	direction L C R distance 1 2    value Professor book   true false  Figure 1: Graphical representation of a hierarchical feature tree for token Caldwell in example Sentence 1." ></td>
	<td class="line x" title="28:161	Representing feature spaces with this kind of tree, besides often coinciding with the explicit language used by common natural language toolkits (Cohen, 2004), has the added benefit of allowing a model to easily back-off, or smooth, to decreasing levels of specificity." ></td>
	<td class="line x" title="29:161	For example, the leaf level of the feature tree for our sample Sentence 1 tells us that the word Professor is important, with respect to labeling person names, when located one slot to the left of the current word being classified." ></td>
	<td class="line x" title="30:161	This may be useful in the context of an academic corpus, but might be less useful in a medical domain where the word Professor occurs less often." ></td>
	<td class="line x" title="31:161	Instead, we might want to learn the related feature L.1.Dr. In fact, it might be useful to generalize across multiple domains the fact that the word immediately preceding the current word is often important with respect LeftToken.* LeftToken.IsWord.* LeftToken.IsWord.IsTitle.* LeftToken.IsWord.IsTitle.equals.* LeftToken.IsWord.IsTitle.equals.mr Table 1: A few examples of the feature hierarchy to the named entity status of the current word." ></td>
	<td class="line x" title="32:161	This is easily accomplished by backing up one level from a leaf in the tree structure to its parent, to represent a class of features such as L.1.*." ></td>
	<td class="line x" title="33:161	It has been shown empirically that, while the significance of particular features might vary between domains and tasks, certain generalized classes of features retain their importance across domains (Minkov et al., 2005)." ></td>
	<td class="line x" title="34:161	By backing-off in this way, we can use the feature hierarchy as a prior for transferring beliefs about the significance of entire classes of features across domains and tasks." ></td>
	<td class="line x" title="35:161	Some examples illustrating this idea are shown in table 1." ></td>
	<td class="line x" title="36:161	1.3 Transfer learning When only the type of data being examined is allowed to vary (from news articles to e-mails, for example), the problem is called domain adaptation (Daume III and Marcu, 2006)." ></td>
	<td class="line x" title="37:161	When the task being learned varies (say, from identifying person names to identifying protein names), the problem is called multi-task learning (Caruana, 1997)." ></td>
	<td class="line x" title="38:161	Both of these are considered specific types of the overarching transfer learning problem, and both seem to require a way of altering the classifier learned on the first problem (called the source domain, or source task) to fit the specifics of the second problem (called the target domain, or target task)." ></td>
	<td class="line x" title="39:161	More formally, given an example x and a class label y, the standard statistical classification task is to assign a probability, p(y|x), to x of belonging to class y. In the binary classification case the labels are Y  {0,1}." ></td>
	<td class="line x" title="40:161	In the case we examine, each example xi is represented as a vector of binary features (f1(xi),,fF(xi)) where F is the number of features." ></td>
	<td class="line x" title="41:161	The data consists of two disjoint subsets: the training set (Xtrain,Ytrain) = {(x1,y1),(xN,yN)}, available to the model for its training and the test set Xtest = (x1,,xM), upon which we want to use our trained classifier to make predictions." ></td>
	<td class="line x" title="42:161	246 In the paradigm of inductive learning, (Xtrain,Ytrain) are known, while both Xtest and Ytest are completely hidden during training time." ></td>
	<td class="line x" title="43:161	In this cases Xtest and Xtrain are both assumed to have been drawn from the same distribution, D. In the setting of transfer learning, however, we would like to apply our trained classifier to examples drawn from a distribution different from the one upon which it was trained." ></td>
	<td class="line x" title="44:161	We therefore assume there are two different distributions,Dsource andDtarget, from which data may be drawn." ></td>
	<td class="line x" title="45:161	Given this notation we can then precisely state the transfer learning problem as trying to assign labels Y targettest to test data Xtargettest drawn from Dtarget, given training data (Xsourcetrain ,Y sourcetrain ) drawn fromDsource." ></td>
	<td class="line x" title="46:161	In this paper we focus on two subproblems of transfer learning:  domain adaptation, where we assume Y (the set of possible labels) is the same for both Dsource and Dtarget, while Dsource and Dtarget themselves are allowed to vary between domains." ></td>
	<td class="line x" title="47:161	 multi-task learning (Ando and Zhang, 2005; Caruana, 1997; Sutton and McCallum, 2005; Zhang et al., 2005) in which the task (and label set) is allowed to vary from source to target." ></td>
	<td class="line x" title="48:161	Domain adaptation can be further distinguished by the degree of relatedness between the source and target domains." ></td>
	<td class="line x" title="49:161	For example, in this work we group data collected in the same medium (e.g., all annotated e-mails or all annotated news articles) as belonging to the same genre." ></td>
	<td class="line x" title="50:161	Although the specific boundary between domain and genre for a particular set of data is often subjective, it is nevertheless a useful distinction to draw." ></td>
	<td class="line x" title="51:161	One common way of addressing the transfer learning problem is to use a prior which, in conjunction with a probabilistic model, allows one to specify a priori beliefs about a distribution, thus biasing the results a learning algorithm would have produced had it only been allowed to see the training data (Raina et al., 2006)." ></td>
	<td class="line x" title="52:161	In the example from1.1, our belief that capitalization is less strict in e-mails than in news articles could be encoded in a prior that biased the importance of the capitalization feature to be lower for e-mails than news articles." ></td>
	<td class="line x" title="53:161	In the next section we address the problem of how to come up with a suitable prior for transfer learning across named entity recognition problems." ></td>
	<td class="line x" title="54:161	2 Models considered 2.1 Basic Conditional Random Fields In this work, we will base our work on Conditional Random Fields (CRFs) (Lafferty et al., 2001), which are now one of the most preferred sequential models for many natural language processing tasks." ></td>
	<td class="line x" title="55:161	The parametric form of the CRF for a sentence of length n is given as follows: p(Y = y|x) = 1Z(x) exp( nsummationdisplay i=1 Fsummationdisplay j=1 fj(x,yi)j) (2) where Z(x) is the normalization term." ></td>
	<td class="line x" title="56:161	CRF learns a model consisting of a set of weights  ={1F} over the features so as to maximize the conditional likelihood of the training data, p(Ytrain|Xtrain), given the model p." ></td>
	<td class="line x" title="57:161	2.2 CRF with Gaussian priors To avoid overfitting the training data, these s are often further constrained by the use of a Gaussian prior (Chen and Rosenfeld, 1999) with diagonal covariance,N(,2), which tries to maximize: argmax  Nsummationdisplay k=1 parenleftbigg log p(yk|xk) parenrightbigg  Fsummationdisplay j (jj)2 22j where  > 0 is a parameter controlling the amount of regularization, and N is the number of sentences in the training set." ></td>
	<td class="line x" title="58:161	2.3 Source trained priors One recently proposed method (Chelba and Acero, 2004) for transfer learning in Maximum Entropy models 1 involves modifying the s of this Gaussian prior." ></td>
	<td class="line x" title="59:161	First a model of the source domain, source, is learned by training on{Xsourcetrain ,Y sourcetrain }." ></td>
	<td class="line x" title="60:161	Then a model of the target domain is trained over a limited set of labeled target data braceleftBig Xtargettrain ,Y targettrain bracerightBig , but instead of regularizing this target to be near zero (i.e. setting  = 0), target is instead regularized towards the previously learned source values source (by setting  = source, while 2 remains 1) and thus minimizing (targetsource)2." ></td>
	<td class="line x" title="61:161	1Maximum Entropy models are special cases of CRFs that use the I.I.D. assumption." ></td>
	<td class="line x" title="62:161	The method under discussion can also be extended to CRF directly." ></td>
	<td class="line x" title="63:161	247 Note that, since this model requires Y targettrain in order to learn target, it, in effect, requires two distinct labeled training datasets: one on which to train the prior, and another on which to learn the models final weights (which we call tuning), using the previously trained prior for regularization." ></td>
	<td class="line x" title="64:161	If we are unable to find a match between features in the training and tuning datasets (for instance, if a word appears in the tuning corpus but not the training), we backoff to a standardN(0,1) prior for that feature." ></td>
	<td class="line x" title="65:161	3 y x i i (1) (1) (1)M w (1)1 y x i i ( M y x i i ( M (2) 2) (2) (3) 3) (3) w w (1) w (1) w1 w w w1 w(1)2 3 4 (2) (2) (2)2 3 (3) (3)2 z z z 1 2 Figure 2: Graphical representation of the hierarchical transfer model." ></td>
	<td class="line x" title="66:161	2.4 New model: Hierarchical prior model In this section, we will present a new model that learns simultaneously from multiple domains, by taking advantage of our feature hierarchy." ></td>
	<td class="line x" title="67:161	We will assume that there are D domains on which we are learning simultaneously." ></td>
	<td class="line x" title="68:161	Let there be Md training data in each domain d. For our experiments with non-identically distributed, independent data, we use conditional random fields (cf.2.1)." ></td>
	<td class="line x" title="70:161	However, this model can be extended to any discriminative probabilistic model such as the MaxEnt model." ></td>
	<td class="line x" title="71:161	Let (d) = ((d)1 ,,(d)Fd ) be the parameters of the discriminative model in the domain d where Fd represents the number of features in the domain d. Further, we will also assume that the features of different domains share a common hierarchy represented by a treeT, whose leaf nodes are the features themselves (cf.Figure 1)." ></td>
	<td class="line x" title="73:161	The model parameters (d), then, form the parameters of the leaves of this hierarchy." ></td>
	<td class="line x" title="74:161	Each non-leaf node n  non-leaf(T) of the tree is also associated with a hyper-parameter zn." ></td>
	<td class="line x" title="75:161	Note that since the hierarchy is a tree, each node n has only one parent, represented by pa(n)." ></td>
	<td class="line x" title="76:161	Similarly, we represent the set of children nodes of a node n as ch(n)." ></td>
	<td class="line x" title="77:161	The entire graphical model for an example consisting of three domains is shown in Figure 2." ></td>
	<td class="line x" title="78:161	The conditional likelihood of the entire training data (y,x) ={(y(d)1 ,x(d)1 ),,(y(d)Md,x(d)Md)}Dd=1 is given by: P(y|x,w,z) = braceleftBigg Dproductdisplay d=1 Mdproductdisplay k=1 P(y(d)k |x(d)k ,(d)) bracerightBigg     Dproductdisplay d=1 Fdproductdisplay f=1 N((d)f |zpa(f(d)),1)        productdisplay nTnonleaf N(zn|zpa(n),1)    (3) where the terms in the first line of eq." ></td>
	<td class="line x" title="79:161	(3) represent the likelihood of data in each domain given their corresponding model parameters, the second line represents the likelihood of each model parameter in each domain given the hyper-parameter of its parent in the tree hierarchy of features and the last term goes over the entire treeT except the leaf nodes." ></td>
	<td class="line x" title="80:161	Note that in the last term, the hyper-parameters are shared across the domains, so there is no product over d. We perform a MAP estimation for each model parameter as well as the hyper-parameters." ></td>
	<td class="line x" title="81:161	Accordingly, the estimates are given as follows: (d)f = Mdsummationdisplay i=1  (d)f parenleftBig logP(ydi|x(d)i ,(d)) parenrightBig + zpa(f(d)) zn = zpa(n) +summationtextich(n)(|z)i 1 +|ch(n)| (4) where we used the notation (|z)i because node i, the child node of n, could be a parameter node or a hyper-parameter node depending on the position of the node n in the hierarchy." ></td>
	<td class="line x" title="82:161	Essentially, in this model, the weights of the leaf nodes (model parameters) depend on the log-likelihood as well as the prior weight of its parent." ></td>
	<td class="line x" title="83:161	Additionally, the weight 248 of each hyper-parameter node in the tree is computed as the average of all its children nodes and its parent, resulting in a smoothing effect, both up and down the tree." ></td>
	<td class="line x" title="84:161	2.5 An approximate Hierarchical prior model The Hierarchical prior model is a theoretically well founded model for transfer learning through feature heirarchy." ></td>
	<td class="line x" title="85:161	However, our preliminary experiments indicated that its performance on real-life data sets is not as good as expected." ></td>
	<td class="line x" title="86:161	Although a more thorough investigation needs to be carried out, our analysis indicates that the main reason for this phenomenon is over-smoothing." ></td>
	<td class="line x" title="87:161	In other words, by letting the information propagate from the leaf nodes in the hierarchy all the way to the root node, the model loses its ability to discriminate between its features." ></td>
	<td class="line x" title="88:161	As a solution to this problem, we propose an approximate version of this model that weds ideas from the exact heirarchical prior model and the Chelba model." ></td>
	<td class="line x" title="89:161	As with the Chelba prior method in2.3, this approximate hierarchical method also requires two distinct data sets, one for training the prior and another for tuning the final weights." ></td>
	<td class="line x" title="90:161	Unlike Chelba, we smooth the weights of the priors using the featuretree hierarchy presented in1.1, like the hierarchical prior model." ></td>
	<td class="line x" title="91:161	For smoothing of each feature weight, we chose to back-off in the tree as little as possible until we had a large enough sample of prior data (measured as M, the number of subtrees below the current node) on which to form a reliable estimate of the mean and variance of each feature or class of features." ></td>
	<td class="line x" title="92:161	For example, if the tuning data set is as in Sentence 1, but the prior contains no instances of the word Professor, then we would back-off and compute the prior mean and variance on the next higher level in the tree." ></td>
	<td class="line x" title="93:161	Thus the prior for L.1.Professor would be N(mean(L.1.*), variance(L.1.*)), where mean() and variance() of L.1.* are the sample mean and variance of all the features in the prior dataset that match the pattern L.1.*  or, put another way, all the siblings of L.1.Professor in the feature tree." ></td>
	<td class="line x" title="94:161	If fewer than M such siblings exist, we continue backing-off, up the tree, until an ancestor with sufficient descendants is found." ></td>
	<td class="line x" title="95:161	A detailed description of the approximate hierarchical algorithm is shown in table 2." ></td>
	<td class="line x" title="96:161	Input:Dsource = (Xsourcetrain ,Y sourcetrain ) Dtarget = (Xtargettrain ,Y targettrain ); Feature setsFsource,Ftarget; Feature HierarchiesHsource,Htarget Minimum membership size M Train CRF usingDsource to obtain feature weights source For each feature f Ftarget Initialize: node n = f While (n /Hsource or|Leaves(Hsource(n))|M) and nnegationslash= root(Htarget) nPa(Htarget(n)) Compute f and f using the sample {sourcei |iLeaves(Hsource(n))} Train Gaussian prior CRF usingDtarget as data and{f}and{f}as Gaussian prior parameters." ></td>
	<td class="line x" title="97:161	Output:Parameters of the new CRF target." ></td>
	<td class="line x" title="98:161	Table 2: Algorithm for approximate hierarchical prior: Pa(Hsource(n)) is the parent of node n in feature hierarchy Hsource; |Leaves(Hsource(n))| indicates the number of leaf nodes (basic features) under a node n in the hierarchyHsource." ></td>
	<td class="line x" title="99:161	It is important to note that this smoothed tree is an approximation of the exact model presented in 2.4 and thus an important parameter of this method in practice is the degree to which one chooses to smooth up or down the tree." ></td>
	<td class="line x" title="100:161	One of the benefits of this model is that the semantics of the hierarchy (how to define a feature, a parent, how and when to back-off and up the tree, etc.) can be specified by the user, in reference to the specific datasets and tasks under consideration." ></td>
	<td class="line x" title="101:161	For our experiments, the semantics of the tree are as presented in1.1." ></td>
	<td class="line x" title="102:161	The Chelba method can be thought of as a hierarchical prior in which no smoothing is performed on the tree at all." ></td>
	<td class="line x" title="103:161	Only the leaf nodes of the priors feature tree are considered, and, if no match can be found between the tuning and priors training datasets features, a N(0,1) prior is used instead." ></td>
	<td class="line x" title="104:161	However, in the new approximate hierarchical model, even if a certain feature in the tuning dataset does not have an analog in the training dataset, we can always back-off until an appropriate match is found, even to the level of the root." ></td>
	<td class="line x" title="105:161	Henceforth, we will use only the approximate hierarchical model in our experiments and discussion." ></td>
	<td class="line x" title="106:161	249 Table 3: Summary of data used in experiments Corpus Genre Task UTexas Bio Protein Yapex Bio Protein MUC6 News Person MUC7 News Person CSPACE E-mail Person 3 Investigation 3.1 Data, domains and tasks For our experiments, we have chosen five different corpora (summarized in Table 3)." ></td>
	<td class="line x" title="107:161	Although each corpus can be considered its own domain (due to variations in annotation standards, specific task, date of collection, etc), they can also be roughly grouped into three different genres." ></td>
	<td class="line x" title="108:161	These are: abstracts from biological journals [UT (Bunescu et al., 2004), Yapex (Franzen et al., 2002)]; news articles [MUC6 (Fisher et al., 1995), MUC7 (Borthwick et al., 1998)]; and personal e-mails [CSPACE (Kraut et al., 2004)]." ></td>
	<td class="line x" title="109:161	Each corpus, depending on its genre, is labeled with one of two name-finding tasks:  protein names in biological abstracts  person names in news articles and e-mails We chose this array of corpora so that we could evaluate our hierarchical priors ability to generalize across and incorporate information from a variety of domains, genres and tasks." ></td>
	<td class="line x" title="110:161	In each case, each item (abstract, article or e-mail) was tokenized and each token was hand-labeled as either being part of a name (protein or person) or not, respectively." ></td>
	<td class="line x" title="111:161	We used a standard natural language toolkit (Cohen, 2004) to compute tens of thousands of binary features on each of these tokens, encoding such information as capitalization patterns and contextual information from surrounding words." ></td>
	<td class="line x" title="112:161	This toolkit produces features of the type described in1.2 and thus was amenable to our hierarchical prior model." ></td>
	<td class="line x" title="113:161	In particular, we chose to use the simplest default, out-of-the-box feature generator and purposefully did not use specifically engineered features, dictionaries, or other techniques commonly employed to boost performance on such tasks." ></td>
	<td class="line x" title="114:161	The goal of our experiments was to see to what degree named entity recognition problems naturally conformed to hierarchical methods, and not just to achieve the highest performance possible." ></td>
	<td class="line x" title="115:161	0.1 0.2 0.3 0.4 0.5 0.6 0.7 0 20 40 60 80 100 F1 Percent of target-domain data used for tuning Intra-genre transfer performance evaluated on MUC6 (a) GAUSS: tuned on MUC6 (b) CAT: tuned on MUC6+7 (c) HIER: MUC6+7 prior, tuned on MUC6 (d) CHELBA: MUC6+7 prior, tuned on MUC6 Figure 3: Adding a relevant HIER prior helps compared to the GAUSS baseline ((c) > (a)), while simply CATing or using CHELBA can hurt ((d)(b) < (a), except with very little data), and never beats HIER ((c) > (b)(d))." ></td>
	<td class="line x" title="116:161	3.2 Experiments & results We evaluated the performance of various transfer learning methods on the data and tasks described in3.1." ></td>
	<td class="line x" title="117:161	Specifically, we compared our approximate hierarchical prior model (HIER), implemented as a CRF, against three baselines:  GAUSS: CRF model tuned on a single domains data, using a standardN(0,1) prior  CAT: CRF model tuned on a concatenation of multiple domains data, using aN(0,1) prior  CHELBA: CRF model tuned on one domains data, using a prior trained on a different, related domains data (cf.2.3) We use token-level F1 as our main evaluation measure, combining precision and recall into one metric." ></td>
	<td class="line x" title="118:161	3.2.1 Intra-genre, same-task transfer learning Figure 3 shows the results of an experiment in learning to recognize person names in MUC6 news articles." ></td>
	<td class="line x" title="119:161	In this experiment we examined the effect of adding extra data from a different, but related domain from the same genre, namely, MUC7." ></td>
	<td class="line x" title="120:161	Line a shows the F1 performance of a CRF model tuned only on the target MUC6 domain (GAUSS) across a range of tuning data sizes." ></td>
	<td class="line x" title="121:161	Line b shows the same experiment, but this time the CRF model has been tuned on a dataset comprised of a simple concatenation of the training MUC6 data from (a), along with a different training set from MUC7 (CAT)." ></td>
	<td class="line x" title="122:161	We can see that adding extra data in this way, though 250 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 20 40 60 80 100 F1 Percent of target-domain data used for tuning Inter-genre transfer performance evaluated on MUC6 (e) HIER: MUC6+7 prior, tuned on MUC6 (f) CAT: tuned on all domains (g) HIER: all domains prior, tuned on MUC6 (h) CHELBA: all domains prior, tuned on MUC6 Figure 4: Transfer aware priors CHELBA and HIER effectively filter irrelevant data." ></td>
	<td class="line x" title="123:161	Adding more irrelevant data to the priors doesnt hurt ((e)  (g)  (h)), while simply CATing it, in this case, is disastrous ((f) << (e)." ></td>
	<td class="line x" title="124:161	the data is closely related both in domain and task, has actually hurt the performance of our recognizer for training sizes of moderate to large size." ></td>
	<td class="line x" title="125:161	This is most likely because, although the MUC6 and MUC7 datasets are closely related, they are still drawn from different distributions and thus cannot be intermingled indiscriminately." ></td>
	<td class="line x" title="126:161	Line c shows the same combination of MUC6 and MUC7, only this time the datasets have been combined using the HIER prior." ></td>
	<td class="line x" title="127:161	In this case, the performance actually does improve, both with respect to the single-dataset trained baseline (a) and the naively trained double-dataset (b)." ></td>
	<td class="line x" title="128:161	Finally, line d shows the results of the CHELBA prior." ></td>
	<td class="line x" title="129:161	Curiously, though the domains are closely related, it does more poorly than even the non-transfer GAUSS." ></td>
	<td class="line x" title="130:161	One possible explanation is that, although much of the vocabulary is shared across domains, the interpretation of the features of these words may differ." ></td>
	<td class="line x" title="131:161	Since CHELBA doesnt model the hierarchy among features like HIER, it is unable to smooth away these discrepancies." ></td>
	<td class="line x" title="132:161	In contrast, we see that our HIER prior is able to successfully combine the relevant parts of data across domains while filtering the irrelevant, and possibly detrimental, ones." ></td>
	<td class="line x" title="133:161	This experiment was repeated for other sets of intra-genre tasks, and the results are summarized in3.2.3." ></td>
	<td class="line x" title="134:161	3.2.2 Inter-genre, multi-task transfer learning In Figure 4 we see that the properties of the hierarchical prior hold even when transferring across tasks." ></td>
	<td class="line x" title="135:161	Here again we are trying to learn to recognize person names in MUC6 e-mails, but this time, instead of adding only other datasets similarly labeled with person names, we are additionally adding biological corpora (UT & YAPEX), labeled not with person names but with protein names instead, along with the CSPACE e-mail and MUC7 news article corpora." ></td>
	<td class="line x" title="136:161	The robustness of our prior prevents a model trained on all five domains (g) from degrading away from the intra-genre, same-task baseline (e), unlike the model trained on concatenated data (f )." ></td>
	<td class="line x" title="137:161	CHELBA (h) performs similarly well in this case, perhaps because the domains are so different that almost none of the features match between prior and tuning data, and thus CHELBA backs-off to a standardN(0,1) prior." ></td>
	<td class="line x" title="138:161	This robustness in the face of less similarly related data is very important since these types of transfer methods are most useful when one possesses only very little target domain data." ></td>
	<td class="line x" title="139:161	In this situation, it is often difficult to accurately estimate performance and so one would like assurance than any transfer method being applied will not have negative effects." ></td>
	<td class="line x" title="140:161	3.2.3 Comparison of HIER prior to baselines Each scatter plot in Figure 5 shows the relative performance of a baseline method against HIER." ></td>
	<td class="line x" title="141:161	Each point represents the results of two experiments: the y-coordinate is the F1 score of the baseline method (shown on the y-axis), while the xcoordinate represents the score of the HIER method in the same experiment." ></td>
	<td class="line x" title="142:161	Thus, points lying below the y = x line represent experiments for which HIER received a higher F1 value than did the baseline." ></td>
	<td class="line x" title="143:161	While all three plots show HIER outperforming each of the three baselines, not surprisingly, the non-transfer GAUSS method suffers the worst, followed by the naive concatenation (CAT) baseline." ></td>
	<td class="line x" title="144:161	Both methods fail to make any explicit distinction between the source and target domains and thus suffer when the domains differ even slightly from each other." ></td>
	<td class="line x" title="145:161	Although the differences are more subtle, the right-most plot of Figure 5 suggests HIER is likewise able to outperform the nonhierarchical CHELBA prior in certain transfer scenarios." ></td>
	<td class="line x" title="146:161	CHELBA is able to avoid suffering as much as the other baselines when faced with large difference between domains, but is still unable to capture 251 0 .2 .4 .6 .8 1 0 .2 .4 .6 .8 1 GA US S( F1 ) HIER (F1) 0 .2 .4 .6 .8 1 0 .2 .4 .6 .8 1 CA T( F1 ) HIER (F1) .4 .6 .8 .4 .6 .8 CH EL BA (F1 ) HIER (F1)  y = x MUC6@3% MUC6@6% MUC6@13% MUC6@25% MUC6@50% MUC6@100% CSPACE@3% CSPACE@6% CSPACE@13% CSPACE@25% CSPACE@50% CSPACE@100% Figure 5: Comparative performance of baseline methods (GAUSS, CAT, CHELBA) vs. HIER prior, as trained on nine prior datasets (both pure and concatenated) of various sample sizes, evaluated on MUC6 and CSPACE datasets." ></td>
	<td class="line x" title="147:161	Points below the y = x line indicate HIER outperforming baselines." ></td>
	<td class="line x" title="148:161	as many dependencies between domains as HIER." ></td>
	<td class="line x" title="149:161	4 Conclusions, related & future work In this work we have introduced hierarchical feature tree priors for use in transfer learning on named entity extraction tasks." ></td>
	<td class="line x" title="150:161	We have provided evidence that motivates these models on intuitive, theoretical and empirical grounds, and have gone on to demonstrate their effectiveness in relation to other, competitive transfer methods." ></td>
	<td class="line x" title="151:161	Specifically, we have shown that hierarchical priors allow the user enough flexibility to customize their semantics to a specific problem, while providing enough structure to resist unintended negative effects when used inappropriately." ></td>
	<td class="line x" title="152:161	Thus hierarchical priors seem a natural, effective and robust choice for transferring learning across NER datasets and tasks." ></td>
	<td class="line x" title="153:161	Some of the first formulations of the transfer learning problem were presented over 10 years ago (Thrun, 1996; Baxter, 1997)." ></td>
	<td class="line oc" title="154:161	Other techniques have tried to quantify the generalizability of certain features across domains (Daume III and Marcu, 2006; Jiang and Zhai, 2006), or tried to exploit the common structure of related problems (Ben-David et al., 2007; Scholkopf et al., 2005)." ></td>
	<td class="line x" title="155:161	Most of this prior work deals with supervised transfer learning, and thus requires labeled source domain data, though there are examples of unsupervised (Arnold et al., 2007), semi-supervised (Grandvalet and Bengio, 2005; Blitzer et al., 2006), and transductive approaches (Taskar et al., 2003)." ></td>
	<td class="line x" title="156:161	Recent work using so-called meta-level priors to transfer information across tasks (Lee et al., 2007), while related, does not take into explicit account the hierarchical structure of these meta-level features often found in NLP tasks." ></td>
	<td class="line nc" title="157:161	Daume allows an extra degree of freedom among the features of his domains, implicitly creating a two-level feature hierarchy with one branch for general features, and another for domain specific ones, but does not extend his hierarchy further (Daume III, 2007))." ></td>
	<td class="line x" title="158:161	Similarly, work on hierarchical penalization (Szafranski et al., 2007) in two-level trees tries to produce models that rely only on a relatively small number of groups of variable, as structured by the tree, as opposed to transferring knowledge between branches themselves." ></td>
	<td class="line x" title="159:161	Our future work is focused on designing an algorithm to optimally choose a smoothing regime for the learned feature trees so as to better exploit the similarities between domains while neutralizing their differences." ></td>
	<td class="line x" title="160:161	Along these lines, we are working on methods to reduce the amount of labeled target domain data needed to tune the prior-based models, looking forward to semi-supervised and unsupervised transfer methods." ></td>
	<td class="line x" title="161:161	252" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1086
Parser Adaptation and Projection with Quasi-Synchronous Grammar Features
Smith, David A.;Eisner, Jason M.;"></td>
	<td class="line x" title="1:269	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 822831, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:269	c 2009 ACL and AFNLP Parser Adaptation and Projection with Quasi-Synchronous Grammar Features David A. Smith Department of Computer Science University of Massachusetts Amherst Amherst, MA 01003, USA dasmith@cs.umass.edu Jason Eisner Department of Computer Science Johns Hopkins University Baltimore, MD 21218, USA jason@cs.jhu.edu Abstract We connect two scenarios in structured learning: adapting a parser trained on onecorpustoanotherannotationstyle, and projecting syntactic annotations from one language to another." ></td>
	<td class="line x" title="3:269	We propose quasisynchronous grammar (QG) features for these structured learning tasks." ></td>
	<td class="line x" title="4:269	That is, we score a aligned pair of source and target trees based on local features of the trees and the alignment." ></td>
	<td class="line x" title="5:269	Our quasi-synchronous model assigns positive probability to any alignmentofanytrees, incontrasttoasynchronous grammar, which would insist on some form of structural parallelism." ></td>
	<td class="line x" title="6:269	In monolingual dependency parser adaptation, we achieve high accuracy in translating among multiple annotation styles for the same sentence." ></td>
	<td class="line x" title="7:269	On the more difficult problem of cross-lingual parser projection, we learn a dependency parser for a target language by using bilingual text, an English parser, and automatic word alignments." ></td>
	<td class="line x" title="8:269	Our experiments show that unsupervised QG projection improves on parses trained using only highprecision projected annotations and far outperforms, by more than 35% absolute dependency accuracy, learning an unsupervised parser from raw target-language text alone." ></td>
	<td class="line x" title="9:269	When a few target-language parse trees are available, projection gives a boost equivalent to doubling the number of target-language trees." ></td>
	<td class="line x" title="10:269	The first author would like to thank the Center for Intelligent Information Retrieval at UMass Amherst." ></td>
	<td class="line x" title="11:269	We would also like to thank Noah Smith and Rebecca Hwa for helpful discussions and the anonymous reviewers for their suggestions for improving the paper." ></td>
	<td class="line x" title="12:269	1 Introduction 1.1 Parser Adaptation Consider the problem of learning a dependency parser, which must produce a directed tree whose vertices are the words of a given sentence." ></td>
	<td class="line x" title="13:269	There are many differing conventions for representing syntactic relations in dependency trees." ></td>
	<td class="line x" title="14:269	Say that we wish to output parses in the Prague style and so have annotated a small target corpuse.g., 100 sentenceswith those conventions." ></td>
	<td class="line x" title="15:269	A parser trained on those hundred sentences will achieve mediocre dependency accuracy (the proportion of words that attach to their correct parent)." ></td>
	<td class="line x" title="16:269	But what if we also had a large number of trees in the CoNLL style (the source corpus)?" ></td>
	<td class="line x" title="17:269	Ideally they should help train our parser." ></td>
	<td class="line x" title="18:269	But unfortunately, a parser that learned to produce perfect CoNLL-style trees would, for example, get both links wrong when its coordination constructions were evaluated against a Prague-style gold standard (Figure 1)." ></td>
	<td class="line x" title="19:269	If it were just a matter of this one construction, the obvious solution would be to write a few rules by hand to transform the large source training corpus into the target style." ></td>
	<td class="line x" title="20:269	Suppose, however, that there were many more ways that our corpora differed." ></td>
	<td class="line x" title="21:269	Then we would like to learn a statistical model to transform one style of tree into another." ></td>
	<td class="line x" title="22:269	We may not possess hand-annotated training data for this tree-to-tree transformation task." ></td>
	<td class="line x" title="23:269	That would require the two corpora to annotate some of the same sentences in different styles." ></td>
	<td class="line x" title="24:269	But fortunately, we can automatically obtain a noisy form of the necessary paired-tree training data." ></td>
	<td class="line x" title="25:269	A parser trained on the source corpus can parse the sentences in our target corpus, yielding trees (or more generally, probability distributions over trees) in the source style." ></td>
	<td class="line x" title="26:269	We will then learn a tree transformation model relating these noisy source trees to our known trees in the target style." ></td>
	<td class="line x" title="27:269	822 now od15d15 r d15d15never now d15d15or d15d15never Prague Melcuk no d15d15w d15d15or never now or ned15d15 verd15d15 CoNLL MALT Figure 1: Four of the five logically possible schemes for annotating coordination show up in human-produced dependency treebanks." ></td>
	<td class="line x" title="28:269	(The other possibility is a reverse Melcuk scheme.)" ></td>
	<td class="line x" title="29:269	These treebanks also differ on other conventions." ></td>
	<td class="line x" title="30:269	This model should enable us to convert the original large source corpus to target style, giving us additional training data in the target style." ></td>
	<td class="line x" title="31:269	1.2 Parser Projection For many target languages, however, we do not have the luxury of a large parsed source corpus in the language, even one in a different style or domain as above." ></td>
	<td class="line x" title="32:269	Thus, we may seek other forms of data to augment our small target corpus." ></td>
	<td class="line x" title="33:269	One option would be to leverage unannotated text (McClosky et al., 2006; Smith and Eisner, 2007)." ></td>
	<td class="line x" title="34:269	But we can also try to transfer syntactic information from a parsed source corpus in another language." ></td>
	<td class="line x" title="35:269	This is an extreme case of out-of-domain data." ></td>
	<td class="line x" title="36:269	This leads to the second task of this paper: learning a statistical model to transform a syntactic analysis of a sentence in one language into an analysis of its translation." ></td>
	<td class="line x" title="37:269	Tree transformations are often modeled with synchronous grammars." ></td>
	<td class="line x" title="38:269	Suppose we are given a sentence wprime in the source language and its translation w into the target language." ></td>
	<td class="line x" title="39:269	Their syntactic parses tprime and t are presumably not independent, but will tend to have some parallel or at least correlated structure." ></td>
	<td class="line x" title="40:269	So we could jointly model the parses tprime,t and the alignment a between them, with a model of the form p(t,a,tprime | w,wprime)." ></td>
	<td class="line x" title="41:269	Such a joint model captures how t,a,tprime mutually constrain each other, so that even partial knowledge of some of these three variables can help us to recover the others when training or decoding on bilingual text." ></td>
	<td class="line x" title="42:269	This idea underlies a number of recent papers on syntax-based alignment (using t and tprime to better recover a), grammar induction from bitext (using a to better recover t and tprime), parser projection (using tprime and a to better Figure 2: With the English tree and alignment provided by a parser and aligner at test time, the Chinese parser finds the correct dependencies (see 6)." ></td>
	<td class="line x" title="43:269	A monolingual parsers incorrect edges are shown with dashed lines." ></td>
	<td class="line x" title="44:269	recover t), as well as full joint parsing (Smith and Smith, 2004; Burkett and Klein, 2008)." ></td>
	<td class="line x" title="45:269	In this paper, we condition on the 1-best source tree tprime." ></td>
	<td class="line x" title="46:269	As for the alignment a, our models either condition on the 1-best alignment or integrate the alignment out." ></td>
	<td class="line x" title="47:269	Our models are thus of the form p(t | w,wprime,tprime,a) or, in the generative case, p(w,t,a | wprime,tprime)." ></td>
	<td class="line x" title="48:269	We intend to consider other formulations in future work." ></td>
	<td class="line x" title="49:269	So far, this is very similar to the monolingual parser adaptation scenario, but there are a few key differences." ></td>
	<td class="line x" title="50:269	Since the source and target sentences in the bitext are in different languages, there is no longer a trivial alignment between the words of the source and target trees." ></td>
	<td class="line x" title="51:269	Given word alignments, we could simply try to project dependency links in the source tree onto the target text." ></td>
	<td class="line x" title="52:269	A link-by-link projection, however, could result in invalid trees on the target side, with cycles or disconnected words." ></td>
	<td class="line x" title="53:269	Instead, our models learn the necessarytransformationsthatalignandtransform a source tree into a target tree by means of quasisynchronous grammar (QG) features." ></td>
	<td class="line x" title="54:269	Figure 2 shows an example of bitext helping disambiguation when a parser is trained with only a small number of Chinese trees." ></td>
	<td class="line x" title="55:269	With the help of the English tree and alignment, the parser is able to recover the correct Chinese dependencies using QG features." ></td>
	<td class="line x" title="56:269	Incorrect edges from the monolingual parser are shown with dashed lines." ></td>
	<td class="line x" title="57:269	(The bilingual parser corrects additional errors in the second half of this sentence, which has been removed to improve legibility.)" ></td>
	<td class="line x" title="58:269	The parser is able to recover the long-distance dependency from the first Chinese word (China) to the last (begun), while skipping over the intervening noun 823 phrase that confused the undertrained monolingual parser." ></td>
	<td class="line x" title="59:269	Although, due to the auxiliary verb, China and begun are siblings in English and not in direct dependency, the QG features still leverage this indirect projection." ></td>
	<td class="line x" title="60:269	1.3 Plan of the Paper We start by describing the features we use to augment conditional and generative parsers when scoring pairs of trees (2)." ></td>
	<td class="line x" title="61:269	Then we discuss in turn monolingual (3) and cross-lingual (4) parser adaptation." ></td>
	<td class="line x" title="62:269	Finally, we present experiments on cross-lingual parser projection in conditions when no target language trees are available for training (5) and when some trees are available (6)." ></td>
	<td class="line x" title="63:269	2 Form of the Model What should our model of source and target trees look like?" ></td>
	<td class="line x" title="64:269	In our view, traditional approaches based on synchronous grammar are problematic both computationally and linguistically." ></td>
	<td class="line x" title="65:269	Full inference takes O(n6) time or worse (depending on the grammar formalism)." ></td>
	<td class="line x" title="66:269	Yet synchronous models only consider a limited hypothesis space: e.g., parsesmustbeprojective,andalignmentsmustdecompose according to the recursive parse structure." ></td>
	<td class="line x" title="67:269	(For example, two nodes can be aligned only if their respective parents are also aligned.)" ></td>
	<td class="line x" title="68:269	The synchronous models probability mass function is also restricted to decompose in this way, so it makes certain conditional independence assumptions; put another way, it can evaluate only certain properties of the triple (t,a,tprime)." ></td>
	<td class="line x" title="69:269	Weinsteadmodel(t,a,tprime)asanarbitrarygraph that includes dependency links among the words of each sentence as well as arbitrary alignment links between the words of the two sentences." ></td>
	<td class="line x" title="70:269	This permits non-synchronous and many-to-many alignments." ></td>
	<td class="line x" title="71:269	The only hard constraint we impose is that the dependency links within each sentence must constitute a valid monolingual parsea directed projective spanning tree.1 Given the two sentences w,wprime, our probability distribution over possible graphs considers local features of the parses, the alignment, and both jointly." ></td>
	<td class="line x" title="72:269	Thus, we learn what local syntactic configurations tend to occur in each language and how they correspond across languages." ></td>
	<td class="line x" title="73:269	As a result, we might learn that parses are mostly synchronous, but that there are some systematic cross-linguistic 1Non-projective parsing would also be possible." ></td>
	<td class="line x" title="74:269	divergences and some instances of sloppy (nonparallelorinexact)translation." ></td>
	<td class="line x" title="75:269	Ourmodelisthusa form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a)." ></td>
	<td class="line x" title="76:269	In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009)." ></td>
	<td class="line x" title="77:269	All the models in this paper are conditioned on the source tree tprime." ></td>
	<td class="line x" title="78:269	Conditionally-trained models of adaptation and projection also condition on the target string w and its alignment a to wprime and thus have the form p(t | w,wprime,tprime,a); the unsupervised, generative projection models in 5 have the form p(w,t,a | wprime,tprime)." ></td>
	<td class="line x" title="79:269	The score s of a given tuple of trees, words, and alignment can thus be written as a dot product of weights w with features f and g: s(t,tprime,a,w,wprime) = summationdisplay i wifi(t,w) + summationdisplay j wjgj(t,tprime,a,w,wprime) The features f look only at target words and dependencies." ></td>
	<td class="line x" title="80:269	In the conditional models of 3 and 6, these features are those of an edge-factored dependency parser (McDonald et al., 2005)." ></td>
	<td class="line x" title="81:269	In the generative models of 5, f has the form of a dependency model with valence (Klein and Manning, 2004)." ></td>
	<td class="line x" title="82:269	All models, for instance, have a feature template that considers the parts of speech of a potential parent-child relation." ></td>
	<td class="line x" title="83:269	In order to benefit from the source language, we also need to include bilingual features g. When scoring a candidate target dependency link from word x  y, these features consider the relationship of their corresponding source words xprime and yprime." ></td>
	<td class="line x" title="84:269	(The correspondences are determined by the alignment a.)" ></td>
	<td class="line x" title="85:269	For instance, the source tree tprime may contain the link xprime  yprime, which would cause a feature for monotonic projection to fire for the x  y edge." ></td>
	<td class="line x" title="86:269	If, on the other hand, yprime  xprime  tprime, a head-swapping feature fires." ></td>
	<td class="line x" title="87:269	If xprime = yprime, i.e. x and y align to the same word, the same-word feature fires." ></td>
	<td class="line x" title="88:269	Similar features fire when xprime and yprime are ingrandparent-grandchild, sibling, c-command, or none-of-the above relationships, or when y aligns to NULL." ></td>
	<td class="line x" title="89:269	These alignment classes are called configurations (Smith and Eisner, 2006a, and following)." ></td>
	<td class="line x" title="90:269	When training is conditioned on the target words (see 3 and 6 below), we conjoin these 824 configuration features with the part of speech and coarse part of speech of one or both of the source andtargetwords, i.e. thefeaturetemplatehasfrom one to four tags." ></td>
	<td class="line x" title="91:269	In conditional training, the exponentiated scores s are normalized by a constant: Z =summationtext t exp[s(t,t prime,a,w,wprime)]." ></td>
	<td class="line x" title="92:269	For the generative model, the locally normalized generative process is explained in 5.3.4." ></td>
	<td class="line x" title="93:269	Previous researchers have written fix-up rules to massage the projected links after the fact and learned a parser from the resulting trees (Hwa et al., 2005)." ></td>
	<td class="line x" title="94:269	Instead, our models learn the necessary transformations that align and transform a source tree into a target tree." ></td>
	<td class="line x" title="95:269	Other researchers have tackled the interesting task of learning parsers from unparsed bitext alone (Kuhn, 2004; Snyder et al., 2009); our methods take advantage of investments in high-resource languages such as English." ></td>
	<td class="line x" title="96:269	In workmostcloselyrelatedtothispaper,Ganchevet al.(2009) constrain the posterior distribution over target-language dependencies to align to source dependencies some reasonable proportion of the time ( 70%, cf.Table 2 in this paper)." ></td>
	<td class="line x" title="99:269	This approach performs well but cannot directly learn regular cross-language non-isomorphisms; for instance, some fixup rules for auxiliary verbs need to be introduced." ></td>
	<td class="line x" title="100:269	Finally, Huang et al.(2009) use features, somewhat like QG configurations, on the shift-reduce actions in a monolingual, targetlanguage parser." ></td>
	<td class="line x" title="102:269	3 Adaptation As discussed in 1, the adaptation scenario is a special case of parser projection where the word alignments are one-to-one and observed." ></td>
	<td class="line x" title="103:269	To test our handling of QG features, we performed experiments in which training saw the correct parse trees in both source and target domains, and the mapping between them was simple and regular." ></td>
	<td class="line x" title="104:269	We also performed experiments where the source treeswerereplacedbythenoisyoutputofatrained parser, making the mapping more complex and harder to learn." ></td>
	<td class="line x" title="105:269	We used the subset of the Penn Treebank from the CoNLL 2007 shared task and converted it to dependency representation while varying two parameters: (1) CoNLL vs. Prague coordination style (Figure 1), and (2) preposition the head vs. the child of its nominal object." ></td>
	<td class="line x" title="106:269	We trained an edge-factored dependency parser (McDonald et al., 2005) on source domain data that followed one set of dependency conventions." ></td>
	<td class="line x" title="107:269	We then trained an edge-factored parser with QG features on a small amount of target domain data." ></td>
	<td class="line x" title="108:269	The source parser outputs were produced for all target data, both training and test, so that features for the target parser could refer to them." ></td>
	<td class="line x" title="109:269	In this task, we know what the gold-standard source language parses are for any given text, since we can produce them from the original Penn Treebank." ></td>
	<td class="line x" title="110:269	We can thus measure the contribution of adaptation loss alone, and the combined loss of imperfect source-domain parsing with adaptation (Table 1)." ></td>
	<td class="line x" title="111:269	When no target domain trees are available, we simply have the performance of the source domain parser on this out-of-domain data." ></td>
	<td class="line x" title="112:269	Training a target-domain parser on as few as 10 sentences shows substantial improvements in accuracy." ></td>
	<td class="line x" title="113:269	In the gold conditions, where the target parser starts with perfect source trees, accuracy approaches 100%; in the realistic parse conditions, where the target-domain parser gets noisy source-domain parses, the improvements are quite significant but approach a lower ceiling imposed by the performance of the source parser.2 The adaptation problem in this section is a simpleproofofconceptoftheQGapproach; however, more complex and realistic adaptation problems exist." ></td>
	<td class="line x" title="114:269	Monolingual adaptation is perhaps most obviously useful when the source parser is a blackbox or rule-based system or is trained on unavailable data." ></td>
	<td class="line x" title="115:269	One might still want to use such a parser in some new context, which might require new data or a new annotation standard." ></td>
	<td class="line x" title="116:269	We are also interested in scenarios where we want to avoid expensive retraining on large reannotated treebanks." ></td>
	<td class="line x" title="117:269	We would like a linguist to be able to annotate a few trees according to a hypothesized theory and then quickly use QG adaptation to get a parser for that theory." ></td>
	<td class="line x" title="118:269	One example would be adapting a constituency parser to produce dependency parses." ></td>
	<td class="line x" title="119:269	We have concentrated here on adapting between two dependency parse styles, in order to line up with the cross-lingual tasks to which we now turn." ></td>
	<td class="line x" title="120:269	2In the diagonal cells, source and target styles match, so training the QG parser amounts to a stacking technique (Martins et al., 2008)." ></td>
	<td class="line x" title="121:269	The small training size and overregularization of the QG parser mildly hurts in-domain parsing performance." ></td>
	<td class="line x" title="122:269	825 % Dependency Accuracy on Target CoNLL-PrepHead CoNLL-PrepChild Prague-PrepHead Prague-PrepChild Source 0 10 100 0 10 100 0 10 100 0 10 100 Gold CoNLL-PrepHead 100 99.6 99.6 79.5 96.9 97.8 90.5 95.0 98.1 71.0 92.7 95.4 Parse CoNLL-PrepHead 89.5 88.9 89.0 71.4 85.9 87.9 82.5 84.3 87.8 65.2 82.2 86.1 Gold CoNLL-PrepChild 79.5 96.6 97.3 100 99.6 99.6 71.0 91.3 95.5 89.9 94.5 97.9 Parse CoNLL-PrepChild 71.0 84.2 86.8 88.1 87.5 88.0 64.9 80.7 84.9 80.9 83.5 86.1 Gold Prague-PrepHead 90.5 95.5 96.7 71.0 92.0 94.2 100 99.6 99.6 79.6 97.4 98.1 Parse Prague-PrepHead 83.0 87.1 87.4 65.6 84.2 85.9 88.5 88.3 88.0 70.7 86.4 86.8 Gold Prague-PrepChild 71.0 91.6 93.8 89.9 95.6 96.4 79.6 96.0 97.1 100 99.6 99.6 Parse Prague-PrepChild 65.3 81.7 84.6 81.2 84.5 86.1 70.4 83.2 85.3 86.9 86.1 86.8 Table 1: Adapting a parser to a new annotation style." ></td>
	<td class="line x" title="123:269	We learn to parse in a target style (wide column label) given some number (narrow column label) of supervised target-style training sentences." ></td>
	<td class="line x" title="124:269	As a font of additional features, all training and test sentences have already been augmented with parses in some source style (row label): either gold-standard parses (an oracle experiment) or else the output of a parser trained on 18k source trees (more realistic)." ></td>
	<td class="line x" title="125:269	If we have 0 training sentences, we simply output the source-style parse." ></td>
	<td class="line x" title="126:269	But with 10 or 100 target-style training sentences, each off-diagonal block learns to adapt, mostly closing the gap with the diagonal block in the same column." ></td>
	<td class="line x" title="127:269	In the diagonal blocks, source and target styles match, and the QG parser degrades performance when acting as a stacked parser." ></td>
	<td class="line x" title="128:269	4 Cross-Lingual Projection: Background As in the adaptation scenario above, many syntactic structures can be transferred from one language to another." ></td>
	<td class="line x" title="129:269	In this section, we evaluate the extent of this direct projection on a small handannotated corpus." ></td>
	<td class="line x" title="130:269	In 5, we will use a QG generative model to learn dependency parsers from bitext when there are no annotations in the target language." ></td>
	<td class="line x" title="131:269	Finally, in 6,we show how QG features can augment a target-language parser trained on a small set of labeled trees." ></td>
	<td class="line x" title="132:269	For syntactic annotation projection to work at all, we must hypothesize, or observe, that at least some syntactic structures are preserved in translation." ></td>
	<td class="line x" title="133:269	Hwa et al.(2005) have called this intuition the Direct Correspondence Assumption (DCA, with slight notational changes): Given a pair of sentences w and wprime that are translations of each other with syntactic structure t and tprime, if nodes xprime and yprime of tprime are aligned with nodes x and y of t, respectively, and if syntactic relationship R(xprime,yprime) holds in tprime, then R(x,y) holds in t. The validity of this assumption clearly depends on the node-to-node alignment of the two trees." ></td>
	<td class="line x" title="135:269	We again work in a dependency framework, where syntactic nodes are simply lexical items." ></td>
	<td class="line x" title="136:269	This allows us to use existing work on word alignment." ></td>
	<td class="line x" title="137:269	Hwa et al.(2005) tested the DCA under idealized conditions by obtaining hand-corrected dependency parse trees of a few hundred sentences of Spanish-English and Chinese-English bitext." ></td>
	<td class="line x" title="139:269	They also used human-produced word alignments." ></td>
	<td class="line x" title="140:269	Corpus Prec.[%] Rec.[%] Spanish 64.3 28.4 (no punc.)" ></td>
	<td class="line x" title="141:269	72.0 30.8 Chinese 65.1 11.1 (no punc.)" ></td>
	<td class="line x" title="142:269	68.2 11.5 Table 2: Precision and recall of direct dependency projection via one-to-one links alone." ></td>
	<td class="line x" title="143:269	Since their word alignments could be many-tomany, they gave a heuristic Direct Projection Algorithm (DPA) for resolving them into component dependency relations." ></td>
	<td class="line x" title="144:269	It should be noted that this process introduced empty words into the projected target language tree and left words that are unaligned to English detached from the tree; as a result, they measured performance in dependency Fscore rather than accuracy." ></td>
	<td class="line x" title="145:269	With manual English parses and word alignments, this DPA achieved 36.8% F-score in Spanish and 38.1% in Chinese." ></td>
	<td class="line x" title="146:269	With Collins-model English parses and GIZA++ word alignments, F-score was 33.9% for Spanish and 26.3% for Chinese." ></td>
	<td class="line x" title="147:269	Compare this to the Spanish attach-left baseline of 31.0% and the Chinese attach-right baselines of 35.9%." ></td>
	<td class="line x" title="148:269	These discouragingly low numbers led them to write languagespecifictransformationrulestofixuptheprojected trees." ></td>
	<td class="line x" title="149:269	After these rules were applied to the projections of automatic English parses, F-score was 65.7% for English and 52.4% for Chinese." ></td>
	<td class="line x" title="150:269	While these F-scores were low, it is useful to look at a subset of the alignment: dependencies projected across one-to-one alignments before the heuristic fix-ups had a much higher precision, if lower recall, than Hwa et al.s final results." ></td>
	<td class="line x" title="151:269	Us826 ing Hwa et al.s data, we calculated that the precisionofprojectiontoSpanishandChineseviathese one-to-one links was  65% (Table 2)." ></td>
	<td class="line x" title="152:269	There is clearly more information in these direct links than one would think from the F-scores." ></td>
	<td class="line x" title="153:269	To exploit this information, however, we need to overcome the problems of (1) learning from partial trees, when not all target words are attached, and (2) learning inthepresenceofthestillconsiderablenoiseinthe projected one-to-one dependenciese.g., at least 28% error for Spanish non-punctuation dependencies." ></td>
	<td class="line x" title="154:269	What does this noise consist of?" ></td>
	<td class="line x" title="155:269	Some errors reflect fairly arbitrary annotation conventions in treebanks, e.g. should the auxiliary verb govern the main verb or vice versa." ></td>
	<td class="line x" title="156:269	(Examples like this suggest that the projection problem contains the adaptation problem above.)" ></td>
	<td class="line x" title="157:269	Other errors arise from divergences in the complements required of certain head words." ></td>
	<td class="line x" title="158:269	In the German-English translation pair, with co-indexed words aligned, [an [den Libanon1]] denken2  remember2 Lebanon1 we would prefer that the preposition an attach to denken, even though the prepositions object Libanon aligns to a direct child of remember." ></td>
	<td class="line x" title="159:269	In other words, we would like the grandparentparent-child chain of denken  an  Libanon to align to the parent-child pair of remember  Lebanon." ></td>
	<td class="line x" title="160:269	Finally, naturally occurring bitexts contain some number of free or erroneous translations." ></td>
	<td class="line x" title="161:269	Machine translation researchers often seek to strike these examples from their training corpora; free translations are not usually welcome from an MT system." ></td>
	<td class="line x" title="162:269	5 Unsupervised Cross-Lingual Projection First, we consider the problem of parser projection when there are zero target-language trees available." ></td>
	<td class="line x" title="163:269	As in much other work on unsupervised parsing, we try to learn a generative model that can predict target-language sentences." ></td>
	<td class="line x" title="164:269	Our novel contribution is to condition the probabilities of the generative actions on the dependency parse of a source-language translation." ></td>
	<td class="line x" title="165:269	Thus, our generative model is a quasi-synchronous grammar, exactly as in (Smith and Eisner, 2006a).3 When training on target sentences w, therefore, we tune the model parameters to maximize notsummationtextt p(t,w) as in ordinary EM, but rather 3Our task here is new; they used it for alignment." ></td>
	<td class="line x" title="166:269	summationtext t p(t,w,a | t prime,wprime)." ></td>
	<td class="line x" title="167:269	We hope that this conditional EMtrainingwilldrivethemodeltopositappropriate syntactic relationships in the latent variable t, becausethanks to the structure of the QG modelthat is the easiest way for it to exploit the extra information in tprime,wprime to help predict w.4 At test time, tprime,wprime are not made available, so we just use the trained model to find argmaxt p(t | w), backing off from the conditioning on tprime,wprime and summing over a. Below, we present the specific generative model (5.1) and some details of training (5.2)." ></td>
	<td class="line x" title="168:269	We will then compare three approaches (5.3): 5.3.2 a straight EM baseline (which does not condition on tprime,wprime at all) 5.3.3 ahardprojectionbaseline(whichnaively projects tprime,wprime to derive direct supervision in the target language) 5.3.4 ourconditionalEMapproachabove(which makes tprime,wprime available to the learner for soft indirect supervision via QG) 5.1 Generative Models Our base models of target-language syntax are generative dependency models that have achieved state-of-theartresultsinunsuperviseddependency structure induction." ></td>
	<td class="line x" title="169:269	The simplest version, called Dependency Model with Valence (DMV), has been used in isolation and in combination with other models (Klein and Manning, 2004; Smith and Eisner, 2006b)." ></td>
	<td class="line x" title="170:269	The DMV generates the right children, and then independently the left children, for each node in the dependency tree." ></td>
	<td class="line x" title="171:269	Nodes correspond to words, which are represented by their part-of-speech tags." ></td>
	<td class="line x" title="172:269	At each step of generation, the DMV stochastically chooses whether to stop generating, conditioned on the currently generating head; whether it is generating to the right or left; and whether it has yet generated any children on that side." ></td>
	<td class="line x" title="173:269	If it chooses to continue, it then 4The contrastive estimation of Smith and Eisner (2005) also used a form of conditional EM, with similar motivation." ></td>
	<td class="line x" title="174:269	They suggested that EM grammar induction, which learnstopredictw, unfortunatelylearnsmostlytopredictlexical topic or other properties of the training sentences that do not strongly require syntactic latent variables." ></td>
	<td class="line x" title="175:269	To focus EM on modeling the syntactic relationships, they conditioned the prediction of w on almost complete knowledge of the lexical items." ></td>
	<td class="line x" title="176:269	Similarly, we condition on a source translation of w. Furthermore, our QG model structure makes it easy for EM to learn to exploit the (explicitly represented) syntactic properties of that translation when predicting w. 827 stochastically generates the tag of a new child, conditioned on the head." ></td>
	<td class="line x" title="177:269	The parameters of the model are thus of the form p(stop | head,dir,adj) (1) p(child | head,dir) (2) where head and child are part-of-speech tags, dir  {left,right}, and adj,stop  {true,false}." ></td>
	<td class="line x" title="178:269	ROOT is stipulated to generate a single right child." ></td>
	<td class="line x" title="179:269	Bilingual configurations that condition on tprime,wprime (2) are incorporated into the generative process as in Smith and Eisner (2006a)." ></td>
	<td class="line x" title="180:269	When the model is generating a new child for word x, aligned to xprime, it first chooses a configuration and then chooses a source word yprime in that configuration." ></td>
	<td class="line x" title="181:269	The child y is then generated, conditioned on its parent x, most recent sibling a, and its source analogue yprime." ></td>
	<td class="line x" title="182:269	5.2 Details of EM Training As in previous work on grammar induction, we learn the DMV from part-of-speech-tagged targetlanguage text." ></td>
	<td class="line x" title="183:269	We use expectation maximization (EM)tomaximizethelikelihoodofthedata." ></td>
	<td class="line x" title="184:269	Since the likelihood function is nonconvex in the unsupervised case, our choice of initial parameters can have a significant effect on the outcome." ></td>
	<td class="line x" title="185:269	Although we could also try many random starting points, the initializer in Klein and Manning (2004) performs quite well." ></td>
	<td class="line x" title="186:269	The base dependency parser generates the right dependents of a head separately from the left dependents, which allows O(n3) dynamic programming for an n-word target sentence." ></td>
	<td class="line x" title="187:269	Since the QG annotates nonterminals of the grammar with single nodes of tprime, and we consider two nodes of tprime when evaluating the above dependency configurations, QGparsingrunsinO(n3m2) foranm-word source sentence." ></td>
	<td class="line x" title="188:269	If, however, we restrict candidate senses for a target child c to come from links in an IBM Model 4 Viterbi alignment, we achieve O(n3k2), where k is the maximum number of possible words aligned to a given target language word." ></td>
	<td class="line x" title="189:269	In practice, k lessmuch m, and parsing is not appreciably slower than in the monolingual setting." ></td>
	<td class="line x" title="190:269	If all configurations were equiprobable, the source sentence would provide no information to the target." ></td>
	<td class="line x" title="191:269	In our QG experiments, therefore, we started with a bias towards direct parentchild links and a very small probability for breakages of locality." ></td>
	<td class="line x" title="192:269	The values of other configuration parameters seem, experimentally, less important for insuring accurate learning." ></td>
	<td class="line x" title="193:269	5.3 Experiments Our experiments compare learning on target language text to learning on parallel text." ></td>
	<td class="line x" title="194:269	In the latter case, we compare learning from high-precision one-to-one alignments alone, to learning from all alignments using a QG." ></td>
	<td class="line x" title="195:269	5.3.1 Corpora Our development and test data were drawn from the German TIGER and Spanish Cast3LB treebanks as converted to projective dependencies for the CoNLL 2007 Shared Task (Brants et al., 2002; Civit Torruella and Mart Antonn, 2002).5 Our training data were subsets of the 2006 Statistical Machine Translation Workshop Shared Task, in particular from the German-English and Spanish-English Europarl parallel corpora (Koehn, 2002)." ></td>
	<td class="line x" title="196:269	The Shared Task provided prebuilt automatic GIZA++ word alignments, which we used to facilitate replicability." ></td>
	<td class="line x" title="197:269	Since these word alignments do not contain posterior probabilities or null links, nor do they distinguish which linksareintheIBMModelintersection,wetreated all links as equally likely when learning the QG." ></td>
	<td class="line x" title="198:269	Target language words unaligned to any source language words were the only nodes allowed to align to NULL in QG derivations." ></td>
	<td class="line x" title="199:269	We parsed the English side of the bitext with the projective dependency parser described by McDonald et al.(2005) trained on the Penn Treebank 220." ></td>
	<td class="line x" title="201:269	Much previous work on unsupervised grammar induction has used gold-standard partof-speech tags (Smith and Eisner, 2006b; Klein and Manning, 2004; Klein and Manning, 2002)." ></td>
	<td class="line x" title="202:269	While there are no gold-standard tags for the Europarl bitext, we did train a conditional Markov 5We made one change to the annotation conventions in German: in the dependencies provided, words in a noun phrase governed by a preposition were all attached to that preposition." ></td>
	<td class="line x" title="203:269	This meant that in the phrase das Kind (the child) in, say, subject position, das was the child of Kind; but, in fur das Kind (for the child), das was the child of fur." ></td>
	<td class="line x" title="204:269	This seems to be a strange choice in converting from the TIGER constituency format, which does in fact annotate NPs inside PPs; we have standardized prepositions to govern only the head of the noun phrase." ></td>
	<td class="line x" title="205:269	We did not change any other annotation conventions to make them more like English." ></td>
	<td class="line x" title="206:269	In the Spanish treebank, for instance, control verbs are the children of their verbal complements: in quiero decir (I want to say=I mean), quiero is the child of decir." ></td>
	<td class="line x" title="207:269	In German coordinations, the coordinands all attach to the first, but in English, they all attach to the last." ></td>
	<td class="line x" title="208:269	These particular divergences in annotation style hurt all of our models equally (since none of them have access to labeled trees)." ></td>
	<td class="line x" title="209:269	These annotation divergences are one motivation for experiments below that include some target trees." ></td>
	<td class="line x" title="210:269	828 Dependency accuracy [%] Baselines German Spanish Modify prev." ></td>
	<td class="line x" title="211:269	18.2 28.5 Modify next 27.5 21.4 EM 30.2 25.6 Hard proj." ></td>
	<td class="line x" title="212:269	66.2 59.1 Hard proj." ></td>
	<td class="line x" title="213:269	w/EM 58.6 53.0 QG w/EM 68.5 64.8 Table 3: Test accuracy with unsupervised training methods model tagger on a few thousand tagged sentences." ></td>
	<td class="line x" title="214:269	This is the only supervised data we used in the target." ></td>
	<td class="line x" title="215:269	We created versions of each training corpus with the first thousand, ten thousand, and hundred thousand sentence pairs, each a prefix of the next." ></td>
	<td class="line x" title="216:269	Since the target-language-only baseline converged much more slowly, we used a version of the corpora with sentences 15 target words or fewer." ></td>
	<td class="line x" title="217:269	5.3.2 Fully Unsupervised EM Using the target side of the bitext as training data, we initialized our model parameters as described in 5.2 and ran EM." ></td>
	<td class="line x" title="218:269	We checked convergence on a development set and measured unlabeled dependency accuracy on held-out test data." ></td>
	<td class="line x" title="219:269	We compare performance to simple attach-right and attach left baselines (Table 3)." ></td>
	<td class="line x" title="220:269	For mostly headfinal German, the modify next baseline is better; for mostly head-initial Spanish, modify previous wins." ></td>
	<td class="line x" title="221:269	Even after several hundred iterations, performancewasslightly, butnotsignificantlybetter than the baseline for German." ></td>
	<td class="line x" title="222:269	EM training did not beat the baseline for Spanish.6 5.3.3 Hard Projection, Semi-Supervised EM The simplest approach to using the high-precision one-to-one word alignments is labeled hard projection in the table." ></td>
	<td class="line x" title="223:269	We filtered the training corpus to find sentences where enough links were projected to completely determine a target language tree." ></td>
	<td class="line x" title="224:269	Of course, we needed to filter more than 1000 sentences of bitext to output 1000 training sentences in this way." ></td>
	<td class="line x" title="225:269	We simply perform supervised training with this subset, which is still quite noisy (4), and performance quickly 6While these results are worse than those obtained previously for this model, the experiments in Klein and Manning (2004) and only used sentences of 10 words or fewer, without punctuation, and with gold-standard tags." ></td>
	<td class="line x" title="226:269	Punctuation in particular seems to trip up the initializer: since a sentence-final periods appear in most sentences, EM often decides to make it the head." ></td>
	<td class="line x" title="227:269	plateaus." ></td>
	<td class="line x" title="228:269	Still, this method substantially improves over the baselines and unsupervised EM." ></td>
	<td class="line x" title="229:269	Restricting ourselves to fully projected trees seems a waste of information." ></td>
	<td class="line x" title="230:269	We can also simply take all one-to-one projected links, impute expectedcountsfortheremainingdependencieswith EM, and update our models." ></td>
	<td class="line x" title="231:269	This approach (hard projection with EM), however, performed worse than using only the fully projected trees." ></td>
	<td class="line x" title="232:269	In fact, only the first iteration of EM with this method made any improvement; afterwards, EM degraded accuracy further from the numbers in Table 3." ></td>
	<td class="line x" title="233:269	5.3.4 Soft Projection: QG & Conditional EM The quasi-synchronous model used all of the alignmentsinre-estimatingitsparametersandperformed significantly better than hard projection." ></td>
	<td class="line x" title="234:269	Unlike EM on the target language alone, the QGs performance does not depend on a clever initializer for initial model weightsall parameters of the generative model except for the QG configuration features were initialized to zero." ></td>
	<td class="line x" title="235:269	Setting the prior to prefer direct correspondence provides the necessary bias to initialize learning." ></td>
	<td class="line x" title="236:269	Error analysis showed that certain types of dependencies eluded the QGs ability to learn from bitext." ></td>
	<td class="line x" title="237:269	The Spanish treebank treats some verbal complements as the heads of main verbs and auxiliary verbs as the children of participles; the QG, following the English, learned the opposite dependency direction." ></td>
	<td class="line x" title="238:269	Spanish treebank conventions for punctuation were also a common source of errors." ></td>
	<td class="line x" title="239:269	In both German and Spanish, coordinations (a common bugbear for dependency grammars) were often mishandled: both treebanks attach the later coordinands and any conjunctions to the first coordinand; the reverse is true in English." ></td>
	<td class="line x" title="240:269	Finally, in both German and Spanish, preposition attachments often led to errors, which is not surprising given the unlexicalized target-language grammars." ></td>
	<td class="line x" title="241:269	Rather than trying to adjudicate which dependencies are mere annotation conventions, it would be useful to test learned dependency models on some extrinsic task such as relation extraction or machine translation." ></td>
	<td class="line x" title="242:269	6 Supervised Cross-Lingual Projection Finally, we consider the problem of parser projectionwhensometargetlanguagetreesareavailable." ></td>
	<td class="line x" title="243:269	As in the adaptation case (3), we train a conditional model (not a generative DMV) of the target 829 tree given the target sentence, using the monolingual and bilingual QG features, including configurations conjoined with tags, outlined above (2)." ></td>
	<td class="line x" title="244:269	For these experiments, we used the LDCs English-Chinese Parallel Treebank (ECTB)." ></td>
	<td class="line x" title="245:269	Since manual word alignments also exist for a part of this corpus, we were able to measure the loss in accuracy (if any) from the use of an automatic English parser and word aligner." ></td>
	<td class="line x" title="246:269	The sourcelanguage English dependency parser was trained on the Wall Street Journal, where it achieved 91% dependency accuracy on development data." ></td>
	<td class="line x" title="247:269	However, it was only 80.3% accurate when applied to our task, the English side of the ECTB.7 After parsing the source side of the bitext, we train a parser on the annotated target side, using QG features described above (2)." ></td>
	<td class="line x" title="248:269	Both the monolingual target-language parser and the projected parsers are trained to optimize conditional likelihood of the target trees tprime with ten iterations of stochastic gradient ascent." ></td>
	<td class="line x" title="249:269	In Figure 3, we plot the performance of the target-language parser on held-out bitext." ></td>
	<td class="line x" title="250:269	Althoughprojectionperformanceis,notsurprisingly, better if we know the true source trees at training and test time, even with the 1-best output of the source parser, QG features help produce a parser as accurate asq one trained on twice the amount of monolingual data." ></td>
	<td class="line x" title="251:269	In ablation experiments, we included bilingual features only for directly projected links, with no features for head-swapping, grandparents, etc. When using 1-best English parses, parsers trained only with direct-projection and monolingual features performed worse; when using gold English parses, parsers with directprojection-only features performed better when trained with more Chinese trees." ></td>
	<td class="line x" title="252:269	7 Discussion The two related problems of parser adaptation and projection are often approached in different ways." ></td>
	<td class="line o" title="253:269	Many adaptation methods operate by simple augmentations of the target feature space, as we have donehere(DaumeIII,2007)." ></td>
	<td class="line x" title="254:269	Parserprojection, on the other hand, often uses a multi-stage pipeline 7It would be useful to explore whether the techniques of 3 above could be used to improve English accuracy by domain adaptation." ></td>
	<td class="line x" title="255:269	In theory a model with QG features trained to perform well on Chinese should not suffer from an inaccurate, but consistent, English parser, but the results in Figure 3 indicate a significant benefit to be had from better English parsing or from joint Chinese-English inference." ></td>
	<td class="line x" title="256:269	10 20 50 100 200 50010002000 0.60 0.65 0.70 0.75 0.80 0.85 Training examples Unlabeled accuracy Target only +Gold alignments +Source text +Gold parses, alignments +Gold parses Figure 3: Parser projection with target trees." ></td>
	<td class="line x" title="257:269	Using the true or 1-best parse trees in the source language is equivalent to having twice as much data in the target language." ></td>
	<td class="line x" title="258:269	Note that the penalty for using automatic alignments instead of gold alignments is negligible; in fact, using Source text alone is often higher than +Gold alignments." ></td>
	<td class="line x" title="259:269	Using gold source trees, however, significantly outperforms using 1-best source trees." ></td>
	<td class="line x" title="260:269	(Hwa et al., 2005)." ></td>
	<td class="line x" title="261:269	The methods presented here move parser projection much closer in efficiency and simplicity to monolingual parsing." ></td>
	<td class="line x" title="262:269	We showed that augmenting a target parser with quasi-synchronous features can lead to significant improvementsfirst in experiments with adapting to different dependency representations in English, and then in cross-language parser projection." ></td>
	<td class="line x" title="263:269	As with many domain adaptation problems, it is quite helpful to have some annotated target data, especially when annotation styles vary (Dredze et al., 2007)." ></td>
	<td class="line x" title="264:269	Our experiments show that unsupervised QG projection improves on parsers trained using only high-precision projected annotations and far outperforms, by more than 35% absolute dependency accuracy, unsupervised EM." ></td>
	<td class="line x" title="265:269	When a small number of target-language parse trees is available, projection gives a boost equivalent to doubling the number of target trees." ></td>
	<td class="line x" title="266:269	The loss in performance from conditioning only on noisy 1-best source parses points to some natural avenues for improvement." ></td>
	<td class="line x" title="267:269	We are exploring methods that incorporate a packed parse forest on the source side and similar representations of uncertainty about alignments." ></td>
	<td class="line x" title="268:269	Building on our recent belief propagation work (Smith and Eisner, 2008), we can jointly infer two dependency trees and their alignment, under a joint distribution p(t,a,tprime | w,wprime) that evaluates the full graph of dependency and alignment edges." ></td>
	<td class="line x" title="269:269	830" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1158
Domain adaptive bootstrapping for named entity recognition
Wu, Dan;Lee, Wee Sun;Ye, Nan;Chieu, Hai Leong;"></td>
	<td class="line x" title="1:234	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 15231532, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:234	c 2009 ACL and AFNLP Domain adaptive bootstrapping for named entity recognition Dan Wu1, Wee Sun Lee2, Nan Ye2 1Singapore MIT Alliance 2Department of Computer Science National University of Singapore {dwu@,leews@comp,g0701171@}nus.edu.sg Hai Leong Chieu DSO National Laboratories chaileon@dso.org.sg Abstract Bootstrapping is the process of improving the performance of a trained classifier by iteratively adding data that is labeled by the classifier itself to the training set, and retraining the classifier." ></td>
	<td class="line x" title="3:234	It is often used in situations where labeled training data is scarce but unlabeled data is abundant." ></td>
	<td class="line x" title="4:234	In this paper, we consider the problem of domain adaptation: the situation where training data may not be scarce, but belongs to a different domain from the target application domain." ></td>
	<td class="line x" title="5:234	As the distribution of unlabeled data is different from the training data, standard bootstrapping often has difficulty selecting informative data to add to the training set." ></td>
	<td class="line x" title="6:234	We propose an effective domain adaptive bootstrapping algorithm that selects unlabeled target domain data that are informative about the target domain and easy to automatically label correctly." ></td>
	<td class="line x" title="7:234	We call these instances bridges, as they are used to bridge the source domain to the target domain." ></td>
	<td class="line x" title="8:234	We show that the method outperforms supervised, transductive and bootstrapping algorithms on the named entity recognition task." ></td>
	<td class="line x" title="9:234	1 Introduction Most recent researches on natural language processing (NLP) problems are based on machine learning algorithms." ></td>
	<td class="line x" title="10:234	High performance can often be achieved if the system is trained and tested on data from the same domain." ></td>
	<td class="line x" title="11:234	However, the performance of NLP systems often degrades badly when the test data is drawn from a source that is different from the labeled data used to train the system." ></td>
	<td class="line x" title="12:234	For named entity recognition (NER), for example, Ciaramita and Altun (2005) reported that a system trained on a labeled Reuters corpus achieved an F-measure of 91% on a Reuters test set, but only 64% on a Wall Street Journal test set." ></td>
	<td class="line x" title="13:234	The task of adapting a system trained on one domain (called the source domain) to a new domain (called the target domain) is called domain adaptation." ></td>
	<td class="line x" title="14:234	In domain adaptation, it is generally assumed that we have labeled data in the source domain while labeled data may or may not be available in the target domain." ></td>
	<td class="line oc" title="15:234	Previous work in domain adaptation can be classified into two categories: [S+T+], where a small, labeled target domain data is available, e.g.(Blitzer et al., 2006; Jiang and Zhai, 2007; Daume III, 2007; Finkel and Manning, 2009), or [S+T-], where no labeled target domain data is available, e.g.(Blitzer et al., 2006; Jiang and Zhai, 2007)." ></td>
	<td class="line o" title="18:234	In both cases, and especially for [S+T-], domain adaptation can leverage on large amounts of unlabeled data in the target domain." ></td>
	<td class="line x" title="19:234	In practice, it is often unreasonable to expect labeled data for every new domain that we come across, such as blogs, emails, a different newspaper agency, or simply articles from a different topic or period in time." ></td>
	<td class="line x" title="20:234	Thus although [S+T+] is easier to handle, [S+T-] is of higher practical importance." ></td>
	<td class="line x" title="21:234	In this paper, we propose a domain adaptive bootstrapping (DAB) approach to tackle the domain adaptation problem under the setting [S+T-]." ></td>
	<td class="line x" title="22:234	Bootstrapping is an iterative process that uses a trained classifier to label and select unlabeled instances to add to the training set for retraining the classifier." ></td>
	<td class="line x" title="23:234	It is often used when labeled training data is scarce but unlabeled data is abundant." ></td>
	<td class="line x" title="24:234	In contrast, for domain adaptation problems, we may have a lot of training data but the target application domain has a different data distribution." ></td>
	<td class="line x" title="25:234	Standard bootstrapping usually selects instances that are most confidently labeled from the unlabeled data." ></td>
	<td class="line x" title="26:234	In domain adaptation situations, usually the most confidently labeled instances are the ones that are most similar to the source domain in1523 stances these instances tend to contain very little information about the target domain." ></td>
	<td class="line x" title="27:234	For domain adaptive bootstrapping, we propose a selection criterion that selects instances that are informative and easy to automatically label correctly." ></td>
	<td class="line x" title="28:234	In addition, we propose a criterion for stopping the process of bootstrapping before it adds uninformative and incorrectly labeled instances that can reduce performance." ></td>
	<td class="line x" title="29:234	Our approach leverages on instances in the target domain called bridges." ></td>
	<td class="line x" title="30:234	These instances contain domain-independent features, as well as features specific to the target domain." ></td>
	<td class="line x" title="31:234	As they contain domain-independent features, they can be classified correctly by classifiers trained on the source domain labeled data." ></td>
	<td class="line x" title="32:234	We argue that these instances act as a bridge between the source and the target domain." ></td>
	<td class="line x" title="33:234	We show that, on the NER task, DAB outperforms supervised, transductive and standard bootstrapping algorithms, as well as a bootstrapping variant, called balanced bootstrapping (Jiang and Zhai, 2007), that has recently been proposed for domain adaptation." ></td>
	<td class="line x" title="34:234	2 Related work One general class of approaches to domain adaptation is to consider that the instances from the source and the target domain are drawn from different distributions." ></td>
	<td class="line x" title="35:234	Bickel et al.(Bickel et al., 2007) discriminatively learns a scaling factor for source domain training data, so as to adapt the source domain data distribution to resemble the target domain data distribution, under the [S+T-] setting." ></td>
	<td class="line x" title="37:234	Daume III and Marcu (Daume III and Marcu, 2006) considers that the data distribution is a mixture distribution over general, source domain and target domain data." ></td>
	<td class="line x" title="38:234	They learn the underlying mixture distribution using the conditional expectation maximization algorithm, under the [S+T+] setting." ></td>
	<td class="line x" title="39:234	Jiang and Zhai (2007) proposed an instance re-weighting framework that handles both the [S+T+] and [S+T-] settings." ></td>
	<td class="line x" title="40:234	For [S+T-], the resulting algorithm is a balanced bootstrapping algorithm, which was shown to outperform the standard bootstrapping algorithm." ></td>
	<td class="line x" title="41:234	In this paper, we assume the [S+T-] settings, and we show that the approach proposed in this paper, domain adaptive bootstrapping (DAB), outperforms the balanced bootstrapping algorithm on NER." ></td>
	<td class="line x" title="42:234	Another class of approaches to domain adaptation is feature-based." ></td>
	<td class="line oc" title="43:234	Daume III (Daume III, 2007) divided features into three classes: domainindependent features, source-domain features and target-domain features." ></td>
	<td class="line o" title="44:234	He assumed the existence of training data in the target-domain (under the setting [S+T+]), so that the three classes of features can be jointly trained using source and target domain labeled data." ></td>
	<td class="line x" title="45:234	This cannot be done in the setting [S+T-], where no training data is available in the target domain." ></td>
	<td class="line x" title="46:234	Using a different approach, Blitzer et al.(2006) induces correspondences between feature spaces in different domains, by detecting pivot features." ></td>
	<td class="line x" title="48:234	Pivot features are features that occur frequently and behave similarly in different domains." ></td>
	<td class="line x" title="49:234	Pivot features are used to put domain-specific features in correspondence." ></td>
	<td class="line x" title="50:234	In this paper, instead of pivot features, we attempt to leverage on pivot instances that we call bridges, which are instances that bridge the source and target domain." ></td>
	<td class="line x" title="51:234	This will be illustrated in Section 3." ></td>
	<td class="line x" title="52:234	It is generally recognized that adding informative and correctly labeled instances is more useful for learning." ></td>
	<td class="line x" title="53:234	Active learning queries the user for labels of most informative or relevant instances." ></td>
	<td class="line x" title="54:234	Active learning, which has been applied to the problem of NER in (Shen et al., 2004), is used in situations where a large amount of unlabeled data exists and data labeling is expensive." ></td>
	<td class="line x" title="55:234	It has also been applied to the problem of domain adaptation for word sense disambiguation in (Chan and Ng, 2007)." ></td>
	<td class="line x" title="56:234	However, active learning requires human intervention." ></td>
	<td class="line x" title="57:234	Here, we want to achieve the same goal without human intervention." ></td>
	<td class="line x" title="58:234	3 Bootstrapping for domain adaptation We first define the notations used for domain adaptation in the [S+T-] setting." ></td>
	<td class="line x" title="59:234	A set of training data DS = {xi,yi}1i|DS| is given in the source domain, where the notation |X| denotes the size of a set X. Each instance xi in DS has been manually annotated with a label, yi, from a given set of labels Y . The objective of domain adaptation is to label a set of unlabeled data, DT = {xi}1i|DT| with labels from Y . A machine learning algorithm will take a labeled data set (for e.g. DS) and outputs a classifier, which can then be used to classify unlabeled data, i.e. assign labels to unlabeled instances." ></td>
	<td class="line x" title="60:234	A special class of machine learning algorithms, called transductive learning algorithms, is able to take the unlabeled data DT into account during the learning process (see e.g.(Joachims, 1999))." ></td>
	<td class="line x" title="62:234	1524 However, such algorithms do not take into account the shift in domain of the test data." ></td>
	<td class="line x" title="63:234	Jiang and Zhai (2007) recently proposed an instance re-weighting framework to take domain shift into account." ></td>
	<td class="line x" title="64:234	For [S+T-], the resulting algorithm is a balanced bootstrapping algorithm, which we describe below." ></td>
	<td class="line x" title="65:234	3.1 Standard and balanced bootstrapping We define a general bootstrapping algorithm in Algorithm 1." ></td>
	<td class="line x" title="66:234	The algorithm can be applied to any machine learning algorithm that allows training instances to be weighted, and that gives confidence scores for the labels when used to classify test data." ></td>
	<td class="line x" title="67:234	The bootstrapping procedure iteratively improves the performance of a classifier SCt over a number of iterations." ></td>
	<td class="line x" title="68:234	In Algorithm 1, we have left a number of parameters unspecified." ></td>
	<td class="line x" title="69:234	These parameters are (1) the selection-criterion for instances to be added to the training data, (2) the terminationcriterion for the bootstrapping process, and (3) the weights (wS,wT) given to the labeled and bootstrapped training sets." ></td>
	<td class="line x" title="70:234	Standard bootstrapping: (Jiang and Zhai, 2007) the selection-criterion is based on selecting the top k most-confidently labeled instances in Rt." ></td>
	<td class="line x" title="71:234	The weight wSt is equal to wTt . The value of k is a parameter for the bootstrapping algorithm." ></td>
	<td class="line x" title="72:234	Balanced bootstrapping: (Jiang and Zhai, 2007) the selection-criterion is still based on selecting the top k most-confidently labeled instances in Rt." ></td>
	<td class="line x" title="73:234	Balanced bootstrapping was formulated for domain adaptation, and hence they set the weights to satisfy the ratio wStwT t = |Tt||DS|." ></td>
	<td class="line x" title="74:234	This allows the small amount of target data added, Tt, to have an equal weight to the large source domain training set DS." ></td>
	<td class="line x" title="75:234	In this paper, we formulate a selection-criterion and a termination-criterion which are better than those used in standard and balanced bootstrapping." ></td>
	<td class="line x" title="76:234	Regarding the selection-criterion, standard and balanced bootstrapping both select instances which are confidently labeled by SCt to be used for training SCt+1, in the hope of avoiding using wrongly labeled data in bootstrapping." ></td>
	<td class="line x" title="77:234	However, instances that are already confidently labeled by SCt may not contain sufficient information which is not in DS, and using them to train SCt+1 may result in SCt+1 performing similarly to SCt." ></td>
	<td class="line x" title="78:234	This motivates us to select samples which are both informative and easy to automatically label correctly." ></td>
	<td class="line x" title="79:234	Regarding the termination-criterion, which Algorithm 1 Bootstrapping algorithm Input: labeled data DS, test data DT and a machine learning algorithm." ></td>
	<td class="line x" title="80:234	Output: the predicted labels of the set DT . Set T0 = , R0 = DT , and t = 0 Repeat 1." ></td>
	<td class="line x" title="81:234	learn a classifier SCt with (DS,Tt) with weights (wSt ,wTt ) 2." ></td>
	<td class="line x" title="82:234	label the set Rt with SCt 3." ></td>
	<td class="line x" title="83:234	select St  Rt based on selection-criterion 4." ></td>
	<td class="line x" title="84:234	Tt+1 = Tt St, and Rt+1 = Rt \St. Until termination-criterion Output the predicted labels of DT by SCt." ></td>
	<td class="line x" title="85:234	is not mentioned in the paper (Jiang and Zhai, 2007), we assume that bootstrapping is simply run for either a single iteration, or a small and fixed number of iterations." ></td>
	<td class="line x" title="86:234	However, it is known that such simple criterion may result in stopping too early or too late, leading to sub-optimal performance." ></td>
	<td class="line x" title="87:234	We propose a more effective terminationcriterion here." ></td>
	<td class="line x" title="88:234	3.2 Domain adaptive bootstrapping (DAB) Our selection-criterion relies on the observation that in domain adaptation, instances (from the source or the target domain) can be divided into three types according to their information content: generalists are instances that contain only domainindependent information and are present in all domains; specialists are instances containing only domain-specific information and are present only in their respective domains; bridges are instances containing both domain-independent and domainspecific information, also present only in their respective domains but are useful as a bridge between the source and the target domains." ></td>
	<td class="line x" title="89:234	The implication of the above observation is that when choosing unlabeled target domain data for bootstrapping, we should exploit the bridges, because the generalists are not likely to contain much information not in DS due to their domainindependence, and the specialists are difficult to be labeled correctly due to their domain-specificity." ></td>
	<td class="line x" title="90:234	In contrast, the bridges are informative and easier to label correctly." ></td>
	<td class="line x" title="91:234	Choosing confidently classified instances for bootstrapping, as in standard bootstrapping and balanced bootstrapping, is simple, but results in choosing mostly generalists, and is too conservative." ></td>
	<td class="line x" title="92:234	We design a scoring function 1525 on instances, which has high value when the instance is informative and sufficiently likely to be correctly labeled in order to identify correctly labeled bridges." ></td>
	<td class="line x" title="93:234	Intuitively, informativeness of an instance can be measured by the prediction results of the ideal classifier IS for the source domain and the ideal classifier IT for the target domain." ></td>
	<td class="line x" title="94:234	If IS and IT are both probabilistic classifiers, IS should return a noninformative distribution while IT should return an informative one." ></td>
	<td class="line x" title="95:234	The ideal classifier for the source domain is approximated with a source classifier SC trained on DS, while the ideal classifier for the target domain is approximated by training a classifier, TC, on target domain instances labeled by the source classifier." ></td>
	<td class="line x" title="96:234	We also try to ensure that instances that are selected are correctly classified." ></td>
	<td class="line x" title="97:234	As the label used is provided by the target classifier, we estimate the precision of the target classification." ></td>
	<td class="line x" title="98:234	The final ranking function is constructed by combining this estimate with the informativeness of the instance." ></td>
	<td class="line x" title="99:234	We show the algorithm for the instance selection in Algorithm 2." ></td>
	<td class="line x" title="100:234	The notations used follow those used in Algorithm 1." ></td>
	<td class="line x" title="101:234	For simplicity, we assume that wSt = wTt = 1 for all t. We expect TC to be a reasonable classifier on DT due to the presence of generalists and bridges." ></td>
	<td class="line x" title="102:234	Note that the target classifier is constructed by randomly splitting DT into two partitions, training a classifier on each partition and using the prediction of the trained classifier on the partition it is not trained on." ></td>
	<td class="line x" title="103:234	This is because classifiers tend to fit the data that they have been trained on too well making the probability estimates on their training data unreliable." ></td>
	<td class="line x" title="104:234	Also, a random partition is used to ensure that the data in each partition is representative of Du." ></td>
	<td class="line x" title="105:234	3.3 The scoring function: score(p(s),p(t)) The scoring function score(p(s),p(t)) in Algorithm 2 is simply implemented as the product of two components: a measure of the informativeness and the probability that SCs label is correct." ></td>
	<td class="line x" title="106:234	We show how the intuitive ideas (described above) behind these two components are formalized." ></td>
	<td class="line x" title="107:234	Informativeness of a distribution p on a set of discrete labels Y is measured by its entropy h(p) defined by h(p) = summationdisplay yY p(y)logp(y)." ></td>
	<td class="line x" title="108:234	Algorithm 2 Algorithm for selecting instances for bootstrapping at iteration t Input: Labeled source domain data DS, target domain training data Tt, remaining data Rt, the classifier SCt trained on DS Tt, and a scoring function score(p(s),p(t)) Output: k instances for bootstrapping." ></td>
	<td class="line x" title="109:234	1." ></td>
	<td class="line x" title="110:234	Label Rt with SCt, and to each instance xi  Rt, SCt outputs a distribution p(s)i (yi) over its labels." ></td>
	<td class="line x" title="111:234	2." ></td>
	<td class="line x" title="112:234	Randomly split Rt into two partitions, R0t and R1t with their labels assigned by SCt." ></td>
	<td class="line x" title="113:234	3." ></td>
	<td class="line x" title="114:234	Train each target classifier, TCxt with the data Rxt , for x = {0,1}." ></td>
	<td class="line x" title="115:234	4." ></td>
	<td class="line x" title="116:234	Label R(1x)t with the classifier TCxt , which to each instance xi  Rt, outputs a distribution p(t)i (yi) over its labels." ></td>
	<td class="line x" title="117:234	5." ></td>
	<td class="line x" title="118:234	Score each instance from xi  Rt with the function score(p(s)i ,p(t)i )." ></td>
	<td class="line x" title="119:234	6." ></td>
	<td class="line x" title="120:234	Select top k instances from Rt with the highest scores." ></td>
	<td class="line x" title="121:234	h(p) is nonnegative; h(p) = 0 if and only if p has probability 1 on one of the labels; h(p) attains its maximum value when the distribution p is uniform over all labels." ></td>
	<td class="line x" title="122:234	Hence, an instance is classified with high confidence when the distribution over its labels has low entropy." ></td>
	<td class="line x" title="123:234	We measure the informativeness of an instance using h(p(s))h(p(t)), where p(s) and p(t) are as in Algorithm 2." ></td>
	<td class="line x" title="124:234	We argue that a larger value of this expression implies that the instance is more likely to be a bridge instance." ></td>
	<td class="line x" title="125:234	This expression has a high value when the source classifier is uncertain, and the target classifier is certain." ></td>
	<td class="line x" title="126:234	Uncertain classification by the source classifier indicates that the instance is unlikely to be a generalist." ></td>
	<td class="line x" title="127:234	Moreover, if the target classifier is certain on xi, it means that instances similar to the instance xi are consistently labeled with the same label by the source classifier SCt, indicating that it is likely to be a bridge instance." ></td>
	<td class="line x" title="128:234	The probability that TCs label is correct cannot be estimated directly because we do not have labeled target domain data." ></td>
	<td class="line x" title="129:234	Instead, we use the source domain to give an estimate." ></td>
	<td class="line x" title="130:234	We do this with a simple pre-processing step: we split the data DS into two partitions of equal size, train a classifier on each partition, and test each classifier on the 1526 other partition." ></td>
	<td class="line x" title="131:234	We then measure the resulting accuracy given each label: (y) = # correctly labeled instances of label y# total instances of label y . Summarizing the above discussion, the scoring function is as shown below." ></td>
	<td class="line x" title="132:234	score(p(s),p(t)) = (y) bracketleftBig h(p(s))h(p(t)) bracketrightBig , where y = argmaxyY p(s)(y) The scoring function has a high value when the information content of the example is high and the label has high precision." ></td>
	<td class="line x" title="133:234	3.4 The termination criterion Intuitively, our algorithm terminates when there are not enough informative instances." ></td>
	<td class="line x" title="134:234	Formally, we define the termination criterion as follows: we terminate the bootstrapping process when, there exists an instance xi in the top k instances satisfying the following condition: 1." ></td>
	<td class="line x" title="135:234	h(p(s)i ) < h(p(t)i ), or 2." ></td>
	<td class="line x" title="136:234	maxyY p(s)i (y) > maxyY p(t)i (y) The second case is used to check for instances where the classifier SCt is more confident than the target classifiers TCxt , on their respective predicted labels." ></td>
	<td class="line x" title="137:234	This shows that the instance xi is more of a generalist than a bridge." ></td>
	<td class="line x" title="138:234	4 NER task and implementation The algorithm described in Section 3 is not specific to any particular application." ></td>
	<td class="line x" title="139:234	In this paper, we apply it to the problem of named entity recognition (NER)." ></td>
	<td class="line x" title="140:234	In this section, we describe the NER classifier and the features used in our experiments." ></td>
	<td class="line x" title="141:234	4.1 NER features We used the features generated by the CRF package (Finkel et al., 2005)." ></td>
	<td class="line x" title="142:234	These features include the word string feature, the case feature for the current word, the context words for the current word and their cases, the presence in dictionaries for the current word, the position of the current word in the sentence, prefix and suffix of the current word as well as the case information of the multiple occurrences of the current word." ></td>
	<td class="line x" title="143:234	We use the same set of features for all classifiers used in the bootstrapping process, and for all baselines used in the experimental section." ></td>
	<td class="line x" title="144:234	4.2 Machine learning algorithms A base machine learning algorithm is required in bootstrapping approaches." ></td>
	<td class="line x" title="145:234	We describe the two machine learning algorithms used in this paper." ></td>
	<td class="line x" title="146:234	We chose these algorithms for their good performance on the NER task." ></td>
	<td class="line x" title="147:234	Maximum entropy classification (MaxEnt): The MaxEnt approach, or logistic regression, is one of the most competitive methods for named entity recognition (Tjong and Meulder, 2003)." ></td>
	<td class="line x" title="148:234	MaxEnt is a discriminative method that learns a distribution, p(yi|xi), over the labels, yi, given the vector of features, xi." ></td>
	<td class="line x" title="149:234	We used the implementation of MaxEnt classifier described in (Manning and Klein, 2003)." ></td>
	<td class="line x" title="150:234	For NER, each instance represents a single word token within a sentence, with the feature vector xi derived from the sentence as described in the previous section." ></td>
	<td class="line x" title="151:234	MaxEnt is not designed for sequence classification." ></td>
	<td class="line x" title="152:234	To deal with sequences, each name-class (e.g. PERSON) is divided into sub-classes: first token (e.g. PERSON-begin), unique token (e.g. PERSONunique), or subsequent tokens (e.g. PERSONcontinue) in the name-class." ></td>
	<td class="line x" title="153:234	To ensure that the results returned by MaxEnt is coherent, we define deterministic transition probabilities that disallow transitions such as one from PERSON-begin to LOCATION-continue." ></td>
	<td class="line x" title="154:234	A Viterbi parse is used to find the valid sequence of name-classes with the highest probability." ></td>
	<td class="line x" title="155:234	Support vector machines (SVM): The basic idea behind SVM for binary classification problems is to consider the data points in their feature space, and to separate the two classes with a hyper-plane, by maximizing the shortest distance between the data points and the hyper-plane." ></td>
	<td class="line x" title="156:234	If there exists no hyperplane that can split the two labels, the soft margin version of SVM will choose a hyperplane that splits the examples as cleanly as possible, while still maximizing the distance to the nearest cleanly split examples (Joachims, 2002)." ></td>
	<td class="line x" title="157:234	We used the SVMlight package for our experiments (Joachims, 2002)." ></td>
	<td class="line x" title="158:234	For the multi-label NER classification with N classes, we learn N SVM classifiers, and use a softmax function to obtain the distribution." ></td>
	<td class="line x" title="159:234	Formally, denoting by s(y) the confidence returned by the classifier for each label y  Y , the probability of the label yi is given by p(yi|xi) = exp(s(yi))summationtext yY exp(s(y)) 1527 Similarly to MaxEnt, we subdivide name-classes into begin, continue, and unique sub-classes, and use a Viterbi parse for the sequence of highest probability." ></td>
	<td class="line x" title="160:234	The SVMlight package also implements a transductive version of the SVM algorithm." ></td>
	<td class="line x" title="161:234	We also compare our approach with the transductive SVM (Joachims, 1999) in our experimental results." ></td>
	<td class="line x" title="162:234	5 Experimental results In this paper, we use the annotated data provided by the Automatic Content Extraction (ACE) program." ></td>
	<td class="line x" title="163:234	The ACE data set is annotated for an Entity Detection task, and the annotation consists of the labeling of entity names (e.g. Powell) and mentions for each entity (e.g. pronouns such as he)." ></td>
	<td class="line x" title="164:234	In this paper, we are interested in the problem of recognition of the proper names (the named entity recognition task), and hence use only entities labeled with the type NAM (LDC, 2005)." ></td>
	<td class="line x" title="165:234	Entities are classified into seven types: Person entities are humans mentioned in a document; Organization entities are limited to established associations of people; Geo-political entities are geographical areas defined by political and/or social groups; Location entities are geographical items like landmasses and bodies of water; Facility entities refer to buildings and real estate improvements; Vehicle entities are devices used for transportation; and Weapon entities are devices used for harming or destruction." ></td>
	<td class="line x" title="166:234	We compare performances of a few algorithms: MaxEnt classifier (MaxEnt); MaxEnt classifier with standard bootstrapping (MaxEnt-SB); balanced bootstrapping based on MaxEnt classifier (MaxEnt-BB); MaxEnt with DAB (MaxEntDAB); SVM classifier (SVM); transductive SVM classifier (SVM-Trans); and DAB based on SVM classifier (SVM-DAB)." ></td>
	<td class="line x" title="167:234	No regularization is used for MaxEnt classifiers." ></td>
	<td class="line x" title="168:234	SVM classifiers use a value of 10 for parameter C (trade-off between training error and margin)." ></td>
	<td class="line x" title="169:234	Bootstrapping based algorithms are run for 30 iterations and 100 instances are selected in every iteration." ></td>
	<td class="line x" title="170:234	The evaluation measure used is the F-measure." ></td>
	<td class="line x" title="171:234	F-measure is the harmonic mean of precision and recall, and is commonly used to evaluate NER systems." ></td>
	<td class="line x" title="172:234	We use the scorer for CONLL 2003 shared task (Tjong and Meulder, 2003) where the F-measure is computed by averaging F-measures for name-classes, weighted by the number of ocCode Source Num docs NW Newswire 81 BC Broadcast conversation 52 WL Weblog 114 CTS Conversational Telephone Speech 34 Table 1: The sources, and the number of documents in each source, in the ACE 2005 data set." ></td>
	<td class="line x" title="173:234	currences." ></td>
	<td class="line x" title="174:234	5.1 Cross-source transfer The ACE 2005 data set consists of articles drawn from a variety of sources." ></td>
	<td class="line x" title="175:234	We use the four categories shown in Table 1." ></td>
	<td class="line x" title="176:234	Each category is considered to be a domain, and we consider each pair of categories as the source and the target domain in turn." ></td>
	<td class="line x" title="177:234	Figure 1 compares the performance of MaxEntSB, MaxEnt-BB and MaxEnt-DAB over multiple iterations." ></td>
	<td class="line x" title="178:234	Figure 2 compares the performance of SVM, SVM-Trans and SVM-DAB." ></td>
	<td class="line x" title="179:234	Each line in the figures represents the average F-measure across all the domains over many iterations." ></td>
	<td class="line x" title="180:234	When the termination condition is met for one domain, its F-measure remains at the value of the final iteration." ></td>
	<td class="line x" title="181:234	Despite a large number of iterations, both standard and balanced bootstrapping fail to improve performance." ></td>
	<td class="line x" title="182:234	Supervised learning performance on each domain is shown in Table 3 (by 2-fold crossvalidation with random ordering) as a reference." ></td>
	<td class="line x" title="183:234	In Table 5, we compare the F-measures obtained by different algorithms at the last iteration they were run." ></td>
	<td class="line x" title="184:234	We will discuss more on this in Section 5.3." ></td>
	<td class="line x" title="185:234	5.2 Cross-topic transfer This data set is constructed from 175 articles from the ACE 2005 corpus." ></td>
	<td class="line x" title="186:234	The data set is used to evaluate transfer across topics." ></td>
	<td class="line x" title="187:234	We manually classify the articles into 4 categories: military operations (MO), political relationship or politicians (POL), terrorism-related (TER), and those which are not in the above categories (OTH)." ></td>
	<td class="line x" title="188:234	A detailed breakdown of the number of documents in the each topic is given in Table 2." ></td>
	<td class="line x" title="189:234	Supervised learning performance on each domain is shown in Table 4 (by 2-fold crossvalidation with random ordering) as a reference." ></td>
	<td class="line x" title="190:234	Experimental results on cross-topic evaluation are shown in Table 6." ></td>
	<td class="line x" title="191:234	Figure 3 compares the performance of MaxEnt-SB, MaxEnt-BB and MaxEnt1528 57.558.0 58.559.0 59.560.0 60.561.0 61.562.0  0  5  10 15 20 25 30 35 F-measure Number of iterations MaxEnt-DABMaxEnt-SBMaxEnt-BB Figure 1: Average performance on the crosssource transfer using MaxEnt classifier." ></td>
	<td class="line x" title="192:234	35.040.0 45.050.0 55.060.0 65.070.0  1  2  3  4  5  6 F-measure Number of iterations SVM-DABSVMSVM-Trans Figure 2: Average performance on the crosssource transfer using SVM classifier." ></td>
	<td class="line x" title="193:234	66.266.4 66.666.8 67.067.2 67.467.6 67.868.0 68.2  0  5  10 15 20 25 30 35 F-measure Number of iterations MaxEnt-DABMaxEnt-SBMaxEnt-BB Figure 3: Average performance on the cross-topic transfer using MaxEnt classifier." ></td>
	<td class="line x" title="194:234	56.058.0 60.062.0 64.066.0 68.070.0 72.0  1  2  3  4 F-measure Number of iterations SVM-DABSVMSVM-Trans Figure 4: Average performance on the cross-topic transfer using SVM classifier." ></td>
	<td class="line x" title="195:234	Topic Topic description # docs MO Military operations 92 POL Political relationships 40 TER Terrorist-related 28 OTH None of the above 15 Table 2: The topics, their descriptions, and the number of training and test documents in each topic." ></td>
	<td class="line x" title="196:234	Domain MaxEnt SVM NW 82.47 82.32 BC 78.21 77.91 WL 71.41 71.84 CTS 93.90 94.01 Table 3: F-measure of supervised learning on the cross-source target domains." ></td>
	<td class="line x" title="197:234	DAB over multiple iterations." ></td>
	<td class="line x" title="198:234	Figure 4 compares the performance of SVM, SVM-Trans and SVMDAB." ></td>
	<td class="line x" title="199:234	Similar to cross-source transfer, standard and balanced bootstrapping perform badly." ></td>
	<td class="line x" title="200:234	This will be discussed in Section 5.3." ></td>
	<td class="line x" title="201:234	Domain MaxEnt SVM MO 80.52 80.6 POL 77.99 79.05 TER 81.74 82.12 OTH 71.33 72.08 Table 4: F-measure of supervised learning on the cross-topic target domains." ></td>
	<td class="line x" title="202:234	5.3 Discussion We show in our experiments that DAB outperforms standard and balanced bootstrapping, as well as the transductive SVM." ></td>
	<td class="line x" title="203:234	We have also shown DAB to be robust across two state-of-the-art classifiers, MaxEnt and SVM." ></td>
	<td class="line x" title="204:234	Balanced bootstrapping has been shown to be more effective for domain adaptation than standard bootstrapping (Jiang and Zhai, 2007) for named entity classification on a subset of the dataset used here." ></td>
	<td class="line x" title="205:234	In contrast, we found that both methods perform poorly on domain adaptation for NER." ></td>
	<td class="line x" title="206:234	In named entity classification, the names have already been segmented out and only need to be classified with the appropriate class." ></td>
	<td class="line x" title="207:234	However, for NER, the names also 1529 Train Test MaxEnt MaxEnt-SB MaxEnt-BB MaxEnt-DAB SVM SVM-Trans SVM-DAB BC CTS 74.26 74.19 74.16 81.03 72.47 43.27 75.43 BC NW 64.81 64.76 64.80 66.20 64.08 43.01 64.39 BC WL 47.81 47.80 47.76 49.52 47.98 36.58 47.93 CTS BC 46.19 46.12 46.40 54.62 46.02 40.44 49.64 CTS NW 54.25 54.15 54.26 53.07 55.63 23.61 58.99 CTS WL 40.42 40.43 40.72 41.27 39.96 29.05 42.04 NW BC 59.90 59.83 59.80 60.55 59.89 45.71 58.42 NW CTS 66.64 66.48 66.59 66.73 68.28 28.80 73.47 NW WL 52.52 52.53 52.47 53.44 52.19 36.39 52.30 WL BC 58.58 58.79 58.65 56.00 58.43 52.64 58.64 WL CTS 64.63 63.89 64.50 80.45 65.96 45.04 81.04 WL NW 67.79 67.72 67.92 68.46 68.38 43.40 69.33 Average 58.15 58.06 58.17 60.95 58.27 39.00 60.97 Table 5: F-measure of the cross-source transfer." ></td>
	<td class="line x" title="208:234	Train Test MaxEnt MaxEnt-SB MaxEnt-BB MaxEnt-DAB SVM SVM-Trans SVM-DAB MO OTH 81.70 81.48 81.57 81.95 81.78 75.68 81.94 MO POL 73.21 73.11 73.28 74.97 72.56 58.13 72.66 MO TER 68.13 68.07 68.24 69.89 69.40 65.02 69.38 OTH MO 63.30 63.80 63.94 63.91 64.18 61.03 65.45 OTH POL 67.96 68.05 67.86 69.13 68.29 56.50 70.67 OTH TER 45.34 44.82 45.30 51.06 45.71 48.77 52.87 POL MO 62.14 62.12 61.95 61.94 61.98 51.67 62.32 POL OTH 77.91 77.72 77.79 76.58 78.11 65.71 78.13 POL TER 66.55 66.38 66.08 66.38 66.44 51.29 67.24 TER MO 58.35 58.62 58.02 57.29 58.30 49.80 58.14 TER OTH 66.83 67.61 66.83 68.97 66.28 58.25 68.12 TER POL 67.34 66.94 67.16 72.00 67.54 50.55 70.65 Average 66.56 66.56 66.50 67.84 66.71 57.70 68.13 Table 6: F-measure of the cross-topic transfer." ></td>
	<td class="line x" title="209:234	need to be separated from not-a-name instances." ></td>
	<td class="line x" title="210:234	We find that the addition of not-a-name instances changes the problem the not-a-names form most of the instances classified with high confidence." ></td>
	<td class="line x" title="211:234	As a result, we find that both standard and balanced bootstrapping fail to improve performance: the selection of the most confident instances no longer provide sufficient new information to improve performance." ></td>
	<td class="line x" title="212:234	We also find that transductive SVM performs poorly on this task." ></td>
	<td class="line x" title="213:234	This is because it assumes that the unlabeled data comes from the same distribution as the labeled data." ></td>
	<td class="line x" title="214:234	In general, applying semi-supervised learning methods directly to [S+T-] type domain adaptation problems do not work and appropriate modifications need to be made to the methods." ></td>
	<td class="line x" title="215:234	The ACE 2005 data set also contains a set of ariticles from the broadcast news (BN) source which is written entirely in lower case." ></td>
	<td class="line x" title="216:234	This makes NER much more difficult." ></td>
	<td class="line x" title="217:234	However, when BN is the source domain, the capitalization information can be discovered by DAB." ></td>
	<td class="line x" title="218:234	Figures 5 and 6 show the average performance when BN is used as the source domain and all other domains in Table 1 as the target domains." ></td>
	<td class="line x" title="219:234	The source domain classifier tends to have high precision and low recall, DAB results in an increase in recall, with a small decrease in precision." ></td>
	<td class="line x" title="220:234	Testing the significance of the F-measure is not trivial because the named entities wrongly labeled by two classifiers are not directly comparable." ></td>
	<td class="line x" title="221:234	We tested the labeling disagreements instead, using a McNemar paired test." ></td>
	<td class="line x" title="222:234	The significance test is performed on the improvement of MaxEnt-DAB over MaxEnt and SVM-DAB over SVM." ></td>
	<td class="line x" title="223:234	In most of the domains for the cross-source transfer, the improvements are significant at a significance level of 0.05, using MaxEnt classifier." ></td>
	<td class="line x" title="224:234	The exceptional train-test pairs are NW-WL and WL-BC." ></td>
	<td class="line x" title="225:234	In the case of WL-BC, this means the slight decrement in performance is not statistically significant." ></td>
	<td class="line x" title="226:234	Similar result is achieved for the cross-source transfer using SVM classifier." ></td>
	<td class="line x" title="227:234	In the cross-topic transfer, the source domain and the target domain are not very different." ></td>
	<td class="line x" title="228:234	When we have a large amount of training data and little testing data, the gain of DAB can be not statistically significant, as in the case when we train with MO and POL domains." ></td>
	<td class="line x" title="229:234	1530 20.025.0 30.035.0 40.045.0 50.0  1  2  3  4  5  6  7  8  9 F-measure Number of iterations MaxEnt-DABMaxEnt-SBMaxEnt-BB Figure 5: Performance on recovering capitalization using MaxEnt classifier." ></td>
	<td class="line x" title="230:234	28.030.0 32.034.0 36.038.0 40.042.0 44.046.0  1  2  3  4 F-measure Number of iterations SVM-DABSVMSVM-Trans Figure 6: Performance on recovering capitalization using SVM classifier." ></td>
	<td class="line x" title="231:234	6 Conclusion We proposed a bootstrapping approach for domain adaptation, and we applied it to the named entity recognition task." ></td>
	<td class="line x" title="232:234	Our approach leverages on instances that serve as bridges between the source and target domain." ></td>
	<td class="line x" title="233:234	Empirically, our method outperforms baseline approaches including supervised, transductive and standard bootstrapping approaches." ></td>
	<td class="line x" title="234:234	It also outperforms balanced bootstrapping, an approach designed for domain adaptation (Jiang and Zhai, 2007)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E09-1006
Supervised Domain Adaption for WSD
Agirre, Eneko;Lopez de Lacalle, Oier;"></td>
	<td class="line x" title="1:229	Proceedings of the 12th Conference of the European Chapter of the ACL, pages 4250, Athens, Greece, 30 March  3 April 2009." ></td>
	<td class="line x" title="2:229	c2009 Association for Computational Linguistics Supervised Domain Adaption for WSD Eneko Agirre and Oier Lopez de Lacalle IXA NLP Group University of the Basque Country Donostia, Basque Contry {e.agirre,oier.lopezdelacalle}@ehu.es Abstract The lack of positive results on supervised domain adaptation for WSD have cast some doubts on the utility of handtagging general corpora and thus developing generic supervised WSD systems." ></td>
	<td class="line x" title="3:229	In this paper we show for the first time that our WSD system trained on a general sourcecorpus(BNC)andthetargetcorpus, obtains up to 22% error reduction when compared to a system trained on the target corpus alone." ></td>
	<td class="line x" title="4:229	In addition, we show that as little as 40% of the target corpus (when supplemented with the source corpus) is sufficient to obtain the same results as training on the full target data." ></td>
	<td class="line x" title="5:229	The key for success is the use of unlabeled data with SVD, a combination of kernels and SVM." ></td>
	<td class="line x" title="6:229	1 Introduction In many Natural Language Processing (NLP) tasks we find that a large collection of manuallyannotated text is used to train and test supervised machine learning models." ></td>
	<td class="line x" title="7:229	While these models have been shown to perform very well when tested on the text collection related to the training data (what we call the source domain), the performance drops considerably when testing on text from other domains (called target domains)." ></td>
	<td class="line oc" title="8:229	In order to build models that perform well in new (target) domains we usually find two settings (Daume III, 2007)." ></td>
	<td class="line x" title="9:229	In the semi-supervised setting, the training hand-annotated text from the source domain is supplemented with unlabeled data from the target domain." ></td>
	<td class="line x" title="10:229	In the supervised setting, we use training data from both the source and target domains to test on the target domain." ></td>
	<td class="line x" title="11:229	In (Agirre and Lopez de Lacalle, 2008) we studied semi-supervised Word Sense Disambiguation (WSD) adaptation, and in this paper we focus on supervised WSD adaptation." ></td>
	<td class="line x" title="12:229	We compare the performance of similar supervised WSD systems on three different scenarios." ></td>
	<td class="line x" title="13:229	In the source to target scenario the WSD system is trained on the source domain and tested on the target domain." ></td>
	<td class="line x" title="14:229	In the target scenario the WSD system is trained and tested on the target domain (using cross-validation)." ></td>
	<td class="line x" title="15:229	In the adaptation scenario the WSD system is trained on both source and target domain and tested in the target domain (also using cross-validation over the target data)." ></td>
	<td class="line x" title="16:229	The source to target scenario represents a weak baseline for domain adaptation, as it does not use any examples from the target domain." ></td>
	<td class="line x" title="17:229	The target scenario represents the hard baseline, and in fact, if the domain adaptation scenario does not yield better results, the adaptation would have failed, as it would meanthatthesourceexamplesarenotusefulwhen we do have hand-labeled target examples." ></td>
	<td class="line x" title="18:229	Previous work shows that current state-of-theart WSD systems are not able to obtain better results on the adaptation scenario compared to the target scenario (Escudero et al., 2000; Agirre and Martnez, 2004; Chan and Ng, 2007)." ></td>
	<td class="line x" title="19:229	This would mean that if a user of a generic WSD system (i.e. based on hand-annotated examples from a generic corpus) would need to adapt it to a specific domain, he would be better off throwing away the generic examples and hand-tagging domain examples directly." ></td>
	<td class="line x" title="20:229	This paper will show that domain adaptation is feasible, even for difficult domainrelated words, in the sense that generic corpora can be reused when deploying WSD systems in specific domains." ></td>
	<td class="line x" title="21:229	We will also show that, given the source corpus, our technique can save up to 60% of effort when tagging domain-related occurrences." ></td>
	<td class="line x" title="22:229	We performed on a publicly available corpus which was designed to study the effect of domains in WSD (Koeling et al., 2005)." ></td>
	<td class="line x" title="23:229	It comprises 41 42 nouns which are highly relevant in the SPORTS and FINANCES domains, with 300 examples for each." ></td>
	<td class="line x" title="24:229	The use of two target domains strengthens the conclusions of this paper." ></td>
	<td class="line x" title="25:229	Our system uses Singular Value Decomposition (SVD) in order to find correlations between terms, which are helpful to overcome the scarcity of training data in WSD (Gliozzo et al., 2005)." ></td>
	<td class="line x" title="26:229	This work explores how this ability of SVD and a combination of the resulting feature spaces improves domain adaptation." ></td>
	<td class="line x" title="27:229	We present two ways to combine the reduced spaces: kernel combination with Support Vector Machines (SVM), and k Nearest-Neighbors (k-NN) combination." ></td>
	<td class="line x" title="28:229	The paper is structured as follows." ></td>
	<td class="line x" title="29:229	Section 2 reviews prior work in the area." ></td>
	<td class="line x" title="30:229	Section 3 presents the data sets used." ></td>
	<td class="line x" title="31:229	In Section 4 we describe the learning features, including the application of SVD, and in Section 5 the learning methods and the combination." ></td>
	<td class="line x" title="32:229	The experimental results are presented in Section 6." ></td>
	<td class="line x" title="33:229	Section 7 presents the discussion and some analysis of this paper and finally Section 8 draws the conclusions." ></td>
	<td class="line x" title="34:229	2 Prior work Domain adaptation is a practical problem attracting more and more attention." ></td>
	<td class="line pc" title="35:229	In the supervised setting, a recent paper by Daume III (2007) shows that a simple feature augmentation method for SVM is able to effectively use both labeled target and source data to provide the best domainadaptation results in a number of NLP tasks." ></td>
	<td class="line p" title="36:229	His method improves or equals over previously explored more sophisticated methods (Daume III and Marcu, 2006; Chelba and Acero, 2004)." ></td>
	<td class="line x" title="37:229	The feature augmentation consists in making three version of the original features: a general, a sourcespecific and a target-specific versions." ></td>
	<td class="line x" title="38:229	That way the augmented source contains the general and source-specific version and the augmented target data general and specific versions." ></td>
	<td class="line x" title="39:229	The idea behind this is that target domain data has twice the influence as the source when making predictions about test target data." ></td>
	<td class="line x" title="40:229	We reimplemented this method and show that our results are better." ></td>
	<td class="line x" title="41:229	Regarding WSD, some initial works made a basic analysis of domain adaptation issues." ></td>
	<td class="line x" title="42:229	Escudero et al.(2000) tested the supervised adaptation scenario on the DSO corpus, which had examples from the Brown corpus and Wall Street Journal corpus." ></td>
	<td class="line x" title="44:229	They found that the source corpus did not help when tagging the target corpus, showing that tagged corpora from each domain would suffice, and concluding that hand tagging a large general corpus would not guarantee robust broadcoverage WSD." ></td>
	<td class="line x" title="45:229	Agirre and Martnez (2000) used theDSOcorpusinthesupervisedscenariotoshow that training on a subset of the source corpora that is topically related to the target corpus does allow for some domain adaptation." ></td>
	<td class="line x" title="46:229	More recently, Chan and Ng (2007) performed supervised domain adaptation on a manually selected subset of 21 nouns from the DSO corpus." ></td>
	<td class="line x" title="47:229	They used active learning, count-merging, and predominant sense estimation in order to save target annotation effort." ></td>
	<td class="line x" title="48:229	They showed that adding just 30% of the target data to the source examples the same precision as the full combination of target and source data could be achieved." ></td>
	<td class="line x" title="49:229	They also showed that using the source corpus allowed to significantly improve results when only 10%30% of the target corpus was used for training." ></td>
	<td class="line x" title="50:229	Unfortunately, no data was given about the target corpus results, thus failing to show that domainadaptation succeeded." ></td>
	<td class="line x" title="51:229	In followup work (Zhong et al., 2008), the feature augmentation approach was combined with active learning and tested on the OntoNotes corpus, on a large domain-adaptation experiment." ></td>
	<td class="line x" title="52:229	They reduced significantly the effort of hand-tagging, but only obtained domainadaptation for smaller fractions of the source and target corpus." ></td>
	<td class="line x" title="53:229	Similarly to these works we show that we can save annotation effort on the target corpus, but, in contrast, we do get domain adaptation when using the full dataset." ></td>
	<td class="line x" title="54:229	In a way our approach is complementary, and we could also apply active learning to further reduce the number of target examples to be tagged." ></td>
	<td class="line x" title="55:229	Though not addressing domain adaptation, other works on WSD also used SVD and are closely related to the present paper." ></td>
	<td class="line x" title="56:229	Ando (2006) used Alternative Structured Optimization." ></td>
	<td class="line x" title="57:229	She first trained one linear predictor for each target word, and then performed SVD on 7 carefully selected submatrices of the feature-to-predictor matrix of weights." ></td>
	<td class="line x" title="58:229	The system attained small but consistentimprovements(nosignificancedatawas given) on the Senseval-3 lexical sample datasets using SVD and unlabeled data." ></td>
	<td class="line x" title="59:229	Gliozzo et al.(2005) used SVD to reduce the space of the term-to-document matrix, and then computed the similarity between train and test 43 instances using a mapping to the reduced space (similar to our SMA method in Section 4.2)." ></td>
	<td class="line x" title="61:229	They combinedotherknowledgesourcesintoacomplex kernel using SVM." ></td>
	<td class="line x" title="62:229	They report improved performance on a number of languages in the Senseval3 lexical sample dataset." ></td>
	<td class="line x" title="63:229	Our present paper differs from theirs in that we propose an additional method to use SVD (the OMT method), and that we focus on domain adaptation." ></td>
	<td class="line x" title="64:229	In the semi-supervised setting, Blitzer et al.(2006) used Structural Correspondence Learning and unlabeled data to adapt a Part-of-Speech tagger." ></td>
	<td class="line x" title="66:229	They carefully select so-called pivot features to learn linear predictors, perform SVD on theweightslearnedbythepredictor, andthuslearn correspondences among features in both source and target domains." ></td>
	<td class="line x" title="67:229	Our technique also uses SVD, but we directly apply it to all features, and thus avoid the need to define pivot features." ></td>
	<td class="line x" title="68:229	In preliminary work we unsuccessfully tried to carry along the idea of pivot features to WSD." ></td>
	<td class="line x" title="69:229	On the contrary, in (Agirre and Lopez de Lacalle, 2008) we show that methods closely related to those presented in this paper produce positive semi-supervised domain adaptation results for WSD." ></td>
	<td class="line x" title="70:229	The methods used in this paper originated in (Agirre et al., 2005; Agirre and Lopez de Lacalle, 2007), where SVD over a feature-to-documents matrix improved WSD performance with and without unlabeled data." ></td>
	<td class="line x" title="71:229	The use of several kNN classifiers trained on a number of reduced and original spaces was shown to get the best results in the Senseval-3 dataset and ranked second in the SemEval 2007 competition." ></td>
	<td class="line x" title="72:229	The present paper extends this work and applies it to domain adaptation." ></td>
	<td class="line x" title="73:229	3 Data sets The dataset we use was designed for domainrelatedWSDexperimentsbyKoelingetal.(2005), and is publicly available." ></td>
	<td class="line x" title="74:229	The examples come from the BNC (Leech, 1992) and the SPORTS and FINANCES sections of the Reuters corpus (Rose et al., 2002), comprising around 300 examples (roughly 100 from each of those corpora) for each of the 41 nouns." ></td>
	<td class="line x" title="75:229	The nouns were selected because they were salient in either the SPORTS or FINANCES domains, or because they had senses linked to those domains." ></td>
	<td class="line x" title="76:229	The occurrences were hand-tagged with the senses from WordNet (WN) version 1.7.1 (Fellbaum, 1998)." ></td>
	<td class="line x" title="77:229	In our experiments the BNC examples play the role of general source corpora, and the FINANCES and SPORTS examples the role of two specific domain target corpora." ></td>
	<td class="line x" title="78:229	ComparedtotheDSOcorpususedinpriorwork (cf.Section 2) this corpus has been explicitly created for domain adaptation studies." ></td>
	<td class="line x" title="80:229	DSO contains texts coming from the Brown corpus and the Wall Street Journal, but the texts are not classified according to specific domains (e.g. Sports, Finances), which make DSO less suitable to study domain adaptation." ></td>
	<td class="line x" title="81:229	The fact that the selected nouns are related to the target domain makes the (Koeling et al., 2005) corpus more demanding than the DSO corpus, because one would expect the performance of a generic WSD system to drop when moving to the domain corpus for domainrelated words (cf.Table 1), while the performance would be similar for generic words." ></td>
	<td class="line x" title="83:229	In addition to the labeled data, we also use unlabeled data coming from the three sources used in the labeled corpus: the written part of the BNC (89.7M words), the FINANCES part of Reuters (32.5M words), and the SPORTS part (9.1M words)." ></td>
	<td class="line x" title="84:229	4 Original and SVD features In this section, we review the features and two methods to apply SVD over the features." ></td>
	<td class="line x" title="85:229	4.1 Features We relied on the usual features used in previous WSD work, grouped in three main sets." ></td>
	<td class="line x" title="86:229	Local collocations comprise the bigrams and trigrams formed around the target word (using either lemmas, word-forms, or PoS tags) , those formed with the previous/posterior lemma/word-form in the sentence, and the content words in a 4-word window around the target." ></td>
	<td class="line x" title="87:229	Syntactic dependencies use the object, subject, noun-modifier, preposition, and sibling lemmas, when available." ></td>
	<td class="line x" title="88:229	Finally, Bag-of-words features are the lemmas of the content words in the whole context, plus the salient bigrams in the context (Pedersen, 2001)." ></td>
	<td class="line x" title="89:229	We refer to these features as original features." ></td>
	<td class="line x" title="90:229	4.2 SVD features Apart from the original space of features, we have used the so called SVD features, obtained from the projection of the feature vectors into the reduced space (Deerwester et al., 1990)." ></td>
	<td class="line x" title="91:229	Basically, 44 we set a term-by-document or feature-by-example matrix M from the corpus (see section below for more details)." ></td>
	<td class="line x" title="92:229	SVD decomposes M into three matrices, M = UV T. If the desired number of dimensions in the reduced space is p, we select p rows from  and V , yielding p and Vp respectively." ></td>
	<td class="line x" title="93:229	We can map any feature vector vectort (which represents either a train or test example) into the p-dimensional space as follows: vectortp = vectortTVp1p . Those mapped vectors have p dimensions, and each of the dimensions is what we call a SVD feature." ></td>
	<td class="line x" title="94:229	We have explored two different variants in order to build the reduced matrix and obtain the SVD features, as follows." ></td>
	<td class="line x" title="95:229	Single Matrix for All target words (SVDSMA)." ></td>
	<td class="line x" title="96:229	The method comprises the following steps: (i)extractbag-of-wordfeatures(termsinthiscase) from unlabeled corpora, (ii) build the term-bydocumentmatrix, (iii)decomposeitwith SVD, and (iv) map the labeled data (train/test)." ></td>
	<td class="line x" title="97:229	This technique is very similar to previous work on SVD (Gliozzo et al., 2005; Zelikovitz and Hirsh, 2001)." ></td>
	<td class="line x" title="98:229	The dimensionality reduction is performed once, over the whole unlabeled corpus, and it is then applied to the labeled data of each word." ></td>
	<td class="line x" title="99:229	The reduced space is constructed only with terms, which correspond to bag-of-words features, and thus discards the rest of the features." ></td>
	<td class="line x" title="100:229	Given that the WSD literature shows that all features are necessary for optimal performance (Pradhan et al., 2007), we propose the following alternative to construct the matrix." ></td>
	<td class="line x" title="101:229	OneMatrixperTargetword(SVD-OMT)." ></td>
	<td class="line x" title="102:229	For each word: (i) construct a corpus with its occurrencesinthelabeledand, ifdesired, unlabeledcorpora, (ii) extract all features, (iii) build the featureby-example matrix, (iv) decompose it with SVD, and (v) map all the labeled training and test data for the word." ></td>
	<td class="line x" title="103:229	Note that this variant performs one SVD processforeachtargetwordseparately, hence its name." ></td>
	<td class="line x" title="104:229	When building the SVD-OMT matrices we can use only the training data (TRAIN) or both the train and unlabeled data (+UNLAB)." ></td>
	<td class="line x" title="105:229	When building the SVD-SMA matrices, given the small size of the individual word matrices, we always use both the trainandunlabeleddata(+UNLAB)." ></td>
	<td class="line x" title="106:229	Regardingthe amount of data, based also on previous work, we used 50% of the available data for OMT, and the whole corpora for SMA." ></td>
	<td class="line x" title="107:229	An important parameter when doing SVD is the number of dimensions in the reduced space (p)." ></td>
	<td class="line x" title="108:229	We tried two different values for p (25 and 200) in the BNC domain, and set a dimension for each classifier/matrix combination." ></td>
	<td class="line x" title="109:229	4.3 Motivation The motivationbehind our method isthat although the train and test feature vectors overlap sufficiently in the usual WSD task, the domain difference makes such overlap more scarce." ></td>
	<td class="line x" title="110:229	SVD implicitly finds correlations among features, as it maps related features into nearby regions in the reduced space." ></td>
	<td class="line x" title="111:229	In the case of SMA, SVD is applied over the joint term-by-document matrix of labeled (and possibly unlabeled corpora), and it thus can find correlations among closely related words (e.g. cat and dog)." ></td>
	<td class="line x" title="112:229	These correlations can help reduce the gap among bag-of-words features from the source and target examples." ></td>
	<td class="line x" title="113:229	In the case of OMT, SVD over the joint feature-by-example matrix of labeled and unlabeled examples of a word allows to find correlations among features that show similar occurrence patterns in the source and target corpora for the target word." ></td>
	<td class="line x" title="114:229	5 Learning methods k-NN is a memory based learning method, where the neighbors are thek most similar labeled examples to the test example." ></td>
	<td class="line x" title="115:229	The similarity among instances is measured by the cosine of their vectors." ></td>
	<td class="line x" title="116:229	The test instance is labeled with the sense obtaining the maximum sum of the weighted vote of the k most similar contexts." ></td>
	<td class="line x" title="117:229	We set k to 5 based on previous results published in (Agirre and Lopez de Lacalle, 2007)." ></td>
	<td class="line x" title="118:229	Regarding SVM, we used linear kernels, but also purpose-built kernels for the reduced spaces and the combinations (cf.Section 5.2)." ></td>
	<td class="line x" title="120:229	We used the default soft margin (C=0)." ></td>
	<td class="line x" title="121:229	In previous experiments we learnt that C is very dependent on the feature set and training data used." ></td>
	<td class="line x" title="122:229	As we will experiment with different features and training datasets, it did not make sense to optimize it across all settings." ></td>
	<td class="line x" title="123:229	We will now detail how we combined the original and SVD features in each of the machine learning methods." ></td>
	<td class="line x" title="124:229	5.1 k-NN combinations Our k-NN combination method (Agirre et al., 2005; Agirre and Lopez de Lacalle, 2007) takes 45 advantage of the properties ofk-NN classifiers and exploit the fact that a classifier can be seen as k points (number of nearest neighbor) each casting one vote." ></td>
	<td class="line x" title="125:229	This makes easy to combine several classifiers, one for each feature space." ></td>
	<td class="line x" title="126:229	For instance, taking two k-NN classifiers of k = 5, C1 andC2, we can combine them into a singlek = 10 classifier, where five votes come from C1 and five from C2." ></td>
	<td class="line x" title="127:229	This allows to smoothly combine classifiers from different feature spaces." ></td>
	<td class="line x" title="128:229	In this work we built three single k-NN classifiers trained on OMT, SMA and the original features, respectively." ></td>
	<td class="line x" title="129:229	In order to combine them we weight each vote by the inverse ratio of its position in the rank of the single classifier, (k ri +1)/k, where ri is the rank." ></td>
	<td class="line x" title="130:229	5.2 Kernel combination The basic idea of kernel methods is to find a suitable mapping function () in order to get a better generalization." ></td>
	<td class="line x" title="131:229	Instead of doing this mapping explicitly, kernels give the chance to do it inside the algorithm." ></td>
	<td class="line x" title="132:229	We will formalize it as follows." ></td>
	<td class="line x" title="133:229	First, wedefinethemappingfunction : X  F. Once the function is defined, we can use it in the kernel function in order to become an implicit function K(x,z) = (x)(z), where  denotes a inner product between vectors in the feature space." ></td>
	<td class="line x" title="134:229	This way, we can very easily define mappings representing different information sources and use this mappings in several machine learning algorithm." ></td>
	<td class="line x" title="135:229	In our work we use SVM." ></td>
	<td class="line x" title="136:229	We defined three individual kernels (OMT, SMA and original features) and the combined kernel." ></td>
	<td class="line x" title="137:229	Theoriginalfeaturekernel(KOrig)isgivenby the identity function over the features : X  X, defining the following kernel: KOrig(xi,xj) = xi xjradicalbigx i xixj xj where the denominator is used to normalize and avoid any kind of bias in the combination." ></td>
	<td class="line x" title="138:229	The OMT kernel (KOmt) and SMA kernel (KSma) are defined using OMT and SMA projectionmatrices, respectively (cf.Section4.2)." ></td>
	<td class="line x" title="140:229	Given the OMT function mapping omt : Rm  Rp, where m is the number of the original features and p the reduced dimensionality, then we define KOmt(xi,xj) as follows (KSma is defined similarly): omt(xi)omt(xj)radicalbig omt(xi)omt(xi)omt(xj)omt(xj) BNC  X SPORTS FINANCES MFS 39.0 51.2 k-NN 51.7 60.4 SVM 53.9 62.9 Table 1: Source to target results: Train on BNC, test on SPORTS and FINANCES." ></td>
	<td class="line x" title="141:229	Finally, we define the kernel combination: KComb(xi,xj) = nsummationdisplay l=1 Kl(xi,xj)radicalbig Kl(xi,xi)Kl(xj,xj) where n is the number of single kernels explained above, and l the index for the kernel type." ></td>
	<td class="line x" title="142:229	6 Domain adaptation experiments Inthissectionwepresenttheresultsinourtworeference scenarios (source to target, target) and our reference scenario (domain adaptation)." ></td>
	<td class="line x" title="143:229	Note that all methods presented here have full coverage, i.e. they return a sense for all test examples, and therefore precision equals recall, and suffices to compare among systems." ></td>
	<td class="line x" title="144:229	6.1 Source to target scenario: BNC  X In this scenario our supervised WSD systems are trained on the general source corpus (BNC) and tested on the specific target domains separately (SPORTS and FINANCES)." ></td>
	<td class="line x" title="145:229	We do not perform any kind of adaptation, and therefore the results are those expected for a generic WSD system when applied to domain-specific texts." ></td>
	<td class="line x" title="146:229	Table 1 shows the results for k-NN and SVM trained with the original features on the BNC." ></td>
	<td class="line x" title="147:229	In addition, we also show the results for the Most Frequent Sense baseline (MFS) taken from the BNC." ></td>
	<td class="line x" title="148:229	The second column denotes the accuracies obtained when testing on SPORTS, and the third column the accuracies for FINANCES." ></td>
	<td class="line x" title="149:229	The low accuracy obtained with MFS, e.g. 39.0 of precision in SPORTS, shows the difficulty of this task." ></td>
	<td class="line x" title="150:229	Both classifiers improve over MFS." ></td>
	<td class="line x" title="151:229	These classifiers are weak baselines for the domain adaptation system." ></td>
	<td class="line x" title="152:229	6.2 Target scenario X  X In this scenario we lay the harder baseline which the domain adaptation experiments should improve on (cf.next section)." ></td>
	<td class="line x" title="154:229	The WSD systems are trained and tested on each of the target corpora (SPORTS and FINANCES) using 3-fold crossvalidation." ></td>
	<td class="line x" title="155:229	46 SPORTS FINANCES X X TRAIN +UNLAB TRAIN +UNLAB MFS 77.8 82.3 k-NN 84.5 87.1 SVM 85.1 87.0 k-NN-OMT 85.0 86.1 87.3 87.6 SVM-OMT 82.9 85.1 85.3 86.4 k-NN-SMA 81.1 83.2 SVM-SMA 81.3 84.1 k-NN-COMB 86." ></td>
	<td class="line x" title="156:229	0 86.7 87.9 88.6 SVM-COMB 86.5 88.5 Table 2: Target results: train and test on SPORTS, train and test on FINANCES, using 3-fold crossvalidation." ></td>
	<td class="line x" title="157:229	Table 2 summarizes the results for this scenario." ></td>
	<td class="line x" title="158:229	TRAIN denotes that only tagged data was used to train, +UNLAB denotes that we added unlabeled data related to the source corpus when computing SVD." ></td>
	<td class="line x" title="159:229	Therowsdenotetheclassifierandthefeature spaces used, which are organized in four sections." ></td>
	<td class="line x" title="160:229	On the top rows we show the three baseline classifiers on the original features." ></td>
	<td class="line x" title="161:229	The two sections below show the results of those classifiers on the reduced dimensions, OMT and SMA (cf.Section 4.2)." ></td>
	<td class="line x" title="163:229	Finally, the last rows show the results of the combination strategies (cf.Sections 5.1 and 5.2)." ></td>
	<td class="line x" title="165:229	Note that some of the cells have no result, because that combination is not applicable (e.g. using the train and unlabeled data in the original space)." ></td>
	<td class="line x" title="166:229	First of all note that the results for the baselines (MFS, SVM, k-NN) are much larger than those in Table 1, showing that this dataset is specially demanding for supervised WSD, and particularly difficult for domain adaptation experiments." ></td>
	<td class="line x" title="167:229	These results seem to indicate that the examples from the source general corpus could be of little use when tagging the target corpora." ></td>
	<td class="line x" title="168:229	Note specially the difference in MFS performance." ></td>
	<td class="line x" title="169:229	The priors of the senses are very different in the source and target corpora, which is a well-known shortcoming for supervised systems." ></td>
	<td class="line x" title="170:229	Note the high results of the baseline classifiers, which leave small room for improvement." ></td>
	<td class="line x" title="171:229	The results for the more sophisticated methods show that SVD and unlabeled data helps slightly, except for k-NN-OMT on SPORTS." ></td>
	<td class="line x" title="172:229	SMA decreases the performance compared to the classifiers trained on original features." ></td>
	<td class="line x" title="173:229	The best improvements come when the three strategies are combined in one, as both the kernel and k-NN combinations obtain improvements over the respective single classifiers." ></td>
	<td class="line x" title="174:229	Note that both thek-NN BNC + X SPORTS FINANCES X TRAIN + UNLAB TRAIN + UNLAB BNC X 53.9 62.9 X X 86.0 86.7 87.9 88.5 MFS 68.2 73.1 k-NN 81.3 86.0 SVM 84.7 87.5 k-NN-OMT 84.0 84.7 87.5 86.0 SVM-OMT 85.1 84.7 84.2 85.5 k-NN-SMA 77.1 81.6 SVM-SMA 78.1 80.7 k-NN-COMB 84.5 87.2 88.1 88.7 SVM-COMB 88.4 89.7 SVM-AUG 85.9 88.1 Table 3: Domain adaptation results: Train on BNC and SPORTS, test on SPORTS (same for FINANCES)." ></td>
	<td class="line x" title="175:229	and SVM combinations perform similarly." ></td>
	<td class="line x" title="176:229	In the combination strategy we show that unlabeled data helps slightly, because instead of only combining OMT and original features we have the opportunity to introduce SMA." ></td>
	<td class="line x" title="177:229	Note that it was not our aim to improve the results of the basic classifiers on this scenario, but given the fact that we are going to apply all these techniques in the domain adaptation scenario, we need to show these results asbaselines." ></td>
	<td class="line x" title="178:229	Thatis, inthenextsectionwewilltry to obtain results which improve significantly over the best results in this section." ></td>
	<td class="line x" title="179:229	6.3 Domain adaptation scenario BNC + X  X In this last scenario we try to show that our WSD system trained on both source (BNC) and target (SPORTS and FINANCES) data performs better than the one trained on the target data alone." ></td>
	<td class="line x" title="180:229	We also use 3-fold cross-validation for the target data, but the entire source data is used in each turn." ></td>
	<td class="line x" title="181:229	The unlabeled data here refers to the combination of unlabeled source and target data." ></td>
	<td class="line x" title="182:229	The results are presented in table 3." ></td>
	<td class="line x" title="183:229	Again, the columns denote if unlabeled data has been used in the learning process." ></td>
	<td class="line x" title="184:229	The rows correspond to classifiers and the feature spaces involved." ></td>
	<td class="line x" title="185:229	The first rows report the best results in the previous scenarios: BNC  X for the source to target scenario, and X  X for the target scenario." ></td>
	<td class="line x" title="186:229	The rest of the table corresponds to the domain adaptation scenario." ></td>
	<td class="line x" title="187:229	The rows below correspond to MFS and the baseline classifiers, followed by the OMT and SMA results, and the combination results." ></td>
	<td class="line oc" title="188:229	The last row shows the results for the feature augmentation algorithm (Daume III, 2007)." ></td>
	<td class="line x" title="189:229	47 SPORTS FINANCES BNC X MFS 39.0 51.2 SVM 53.9 62.9 X X MFS 77.8 82.3 SVM 85.1 87.0 k-NN-COMB (+UNLAB) 86.7 88.6 BNC +X X MFS 68.2 73.1 SVM 84.7 87.5 SVM-AUG 85.9 88.1 SVM-COMB (+UNLAB) 88.4 89.7 Table 4: The most important results in each scenario." ></td>
	<td class="line x" title="190:229	Focusing on the results, the table shows that MFS decreases with respect to the target scenario (cf.Table 2) when the source data is added, probably caused by the different sense distributions in BNC and the target corpora." ></td>
	<td class="line x" title="192:229	The baseline classifiers (k-NN and SVM) are not able to improve over the baseline classifiers on the target data alone, which is coherent with past research, and shows that straightforward domain adaptation does not work." ></td>
	<td class="line x" title="193:229	The following rows show that our reduction methods on themselves (OMT, SMA used by kNN and SVM) also fail to perform better than in the target scenario, but the combinations using unlabeled data (k-NN-COMB and specially SVMCOMB) do manage to improve the best results for the target scenario, showing that we were able to attain domain adaptation." ></td>
	<td class="line x" title="194:229	The feature augmentation approach (SVM-AUG) does improve slightly over SVM in the target scenario, but not over the best results in the target scenario, showing the difficulty of domain adaptation for WSD, at least on this dataset." ></td>
	<td class="line x" title="195:229	7 Discussion and analysis Table 4 summarizes the most important results." ></td>
	<td class="line x" title="196:229	The kernel combination method with unlabeled data on the adaptation scenario reduces the error on 22.1% and 17.6% over the baseline SVM on the target scenario (SPORTS and FINANCES respectively), and 12.7% and 9.0% over the k-NN combination method on the target scenario." ></td>
	<td class="line x" title="197:229	These gains are remarkable given the already high baseline, specially taking into consideration that the 41 nouns are closely related to the domains." ></td>
	<td class="line x" title="198:229	The differences, including SVM-AUG, are statistically significant according to the Wilcoxon test with %25 %32 %50 %62 %75 %82 %10 sports (%) 80 82 84 86 8 accuracy (%) SVM-COMB (+UNLAB, BNC + SPORTS -> SPORTS) SVM-AUG (BNC + SPORTS -> SPORTS) SVM-ORIG (SPORTS -> SPORTS) y=85.1 Figure 1: Learning curves for SPORTS." ></td>
	<td class="line x" title="199:229	The X axis denotes the amount of SPORTS data and the Y axis corresponds to accuracy." ></td>
	<td class="line x" title="200:229	%25 %32 %50 %62 %75 %82 %10 finances (%) 84 86 8 90 accuracy (%) SVM-COMB (+UNLAB, BNC + FIN." ></td>
	<td class="line x" title="201:229	-> FIN.)" ></td>
	<td class="line x" title="202:229	SVM-AUG (BNC + FIN." ></td>
	<td class="line x" title="203:229	-> FIN.)" ></td>
	<td class="line x" title="204:229	SVM-ORIG (FI." ></td>
	<td class="line x" title="205:229	-> FIN.)" ></td>
	<td class="line x" title="206:229	y=87.0 Figure 2: Learning curves for FINANCES." ></td>
	<td class="line x" title="207:229	The X axis denotes the amount of FINANCES data and Y axis corresponds to the accuracy." ></td>
	<td class="line x" title="208:229	p < 0.01." ></td>
	<td class="line x" title="209:229	In addition, we carried extra experiments to examine the learning curves, and to check, given the source examples, how many additional examples from the target corpus are needed to obtain the same results as in the target scenario using all available examples." ></td>
	<td class="line x" title="210:229	We fixed the source data and used increasing amounts of target data." ></td>
	<td class="line x" title="211:229	We show the original SVM on the target scenario, and SVM-COMB (+UNLAB) and SVM-AUG as the domain adaptation approaches." ></td>
	<td class="line x" title="212:229	The results are shown in figure 1 for SPORTS and figure 2 for FINANCES." ></td>
	<td class="line x" title="213:229	The horizontal line corresponds to the performance of SVM on the target domain." ></td>
	<td class="line x" title="214:229	The point where the learning curves cross the horizontal line show that our domain adaptation method needs only around 40% of the target data in order to get the same performance as the baseline SVM on the target data." ></td>
	<td class="line x" title="215:229	The learning curves also shows 48 that the domain adaptation kernel combination approach, no matter the amount of target data, is always above the rest of the classifiers, showing the robustness of our approach." ></td>
	<td class="line x" title="216:229	8 Conclusion and future work In this paper we explore supervised domain adaptation for WSD with positive results, that is, whether hand-labeling general domain (source) text is worth the effort when training WSD systemsthataretobeappliedtospecificdomains(targets)." ></td>
	<td class="line x" title="217:229	We performed several experiments in three scenarios." ></td>
	<td class="line x" title="218:229	In the first scenario (source to target scenario), the classifiers were trained on source domain data (the BNC) and tested on the target domains, composed by the SPORTS and FINANCES sections of Reuters." ></td>
	<td class="line x" title="219:229	In the second scenario (target scenario) we set the main baseline for our domain adaptation experiment, training and testing ourclassifiersonthetargetdomaindata." ></td>
	<td class="line x" title="220:229	Inthelast scenario (domain adaptation scenario), we combine both source and target data for training, and test on the target data." ></td>
	<td class="line x" title="221:229	We report results in each scenario for k-NN and SVM classifiers, for reduced features obtained using SVD over the training data, for the use of unlabeled data, and for k-NN and SVM combinations of all." ></td>
	<td class="line x" title="222:229	Our results show that our best domain adaptation strategy (using kernel combination of SVD features and unlabeled data related to the training data) yields statistically significant improvements: up to 22% error reduction compared to SVM on the target domain data alone." ></td>
	<td class="line x" title="223:229	We also show that our domain adaptation method only needs 40% of the target data (in addition to the source data) in order to get the same results as SVM on the target alone." ></td>
	<td class="line x" title="224:229	We obtain coherent results in two target scenarios, and consistent improvement at all levels of the learning curves, showing the robustness or our findings." ></td>
	<td class="line x" title="225:229	We think that our dataset, which comprises examples for 41 nouns that are closely related to the target domains, is specially demanding, as one would expect the performance of a generic WSD system to drop when moving to the domain corpus, specially on domain-related words, while we could expect the performance to be similar for generic or unrelated words." ></td>
	<td class="line x" title="226:229	In the future we would like to evaluate our method on other datasets (e.g. DSO or OntoNotes), to test whether the positive results are confirmed." ></td>
	<td class="line x" title="227:229	We would also like to study word-byword behaviour, in order to assess whether target examples are really necessary for words which are less related to the domain." ></td>
	<td class="line x" title="228:229	Acknowledgments This work has been partially funded by the EU Commission (project KYOTO ICT-2007-211423) and Spanish Research Department (project KNOW TIN2006-15049-C03-01)." ></td>
	<td class="line x" title="229:229	Oier Lopez de Lacalle has a PhD grant from the Basque Government." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E09-3005
Structural Correspondence Learning for Parse Disambiguation
Plank, Barbara;"></td>
	<td class="line x" title="1:229	Proceedings of the EACL 2009 Student Research Workshop, pages 3745, Athens, Greece, 2 April 2009." ></td>
	<td class="line x" title="2:229	c2009 Association for Computational Linguistics Structural Correspondence Learning for Parse Disambiguation Barbara Plank Alfa-informatica University of Groningen, The Netherlands b.plank@rug.nl Abstract The paper presents an application of Structural Correspondence Learning (SCL) (Blitzer et al., 2006) for domain adaptation of a stochastic attribute-value grammar (SAVG)." ></td>
	<td class="line x" title="3:229	So far, SCL has been applied successfully in NLP for Part-of-Speech tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007)." ></td>
	<td class="line x" title="4:229	An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007), however, without any clear conclusions." ></td>
	<td class="line x" title="5:229	We report on our exploration of applying SCL to adapt a syntactic disambiguation model and show promising initial results on Wikipedia domains." ></td>
	<td class="line x" title="6:229	1 Introduction Many current, effective natural language processing systems are based on supervised Machine Learning techniques." ></td>
	<td class="line x" title="7:229	The parameters of such systems are estimated to best reflect the characteristics of the training data, at the cost of portability: a system will be successful only as long as the training material resembles the input that the model gets." ></td>
	<td class="line oc" title="8:229	Therefore, whenever we have access to a large amount of labeled data from some source (out-of-domain), but we would like a model that performs well on some new target domain (Gildea, 2001; Daume III, 2007), we face the problem of domain adaptation." ></td>
	<td class="line x" title="9:229	The need for domain adaptation arises in many NLP tasks: Part-of-Speech tagging, Sentiment Analysis, Semantic Role Labeling or Statistical Parsing, to name but a few." ></td>
	<td class="line x" title="10:229	For example, the performance of a statistical parsing system drops in an appalling way when a model trained on the Wall Street Journal is applied to the more varied Brown corpus (Gildea, 2001)." ></td>
	<td class="line oc" title="11:229	The problem itself has started to get attention only recently (Roark and Bacchiani, 2003; Hara et al., 2005; Daume III and Marcu, 2006; Daume III, 2007; Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007)." ></td>
	<td class="line oc" title="12:229	We distinguish two main approaches to domain adaptation that have been addressed in the literature (Daume III, 2007): supervised and semi-supervised." ></td>
	<td class="line oc" title="13:229	In supervised domain adaptation (Gildea, 2001; Roark and Bacchiani, 2003; Hara et al., 2005; Daume III, 2007), besides the labeled source data, we have access to a comparably small, but labeled amount of target data." ></td>
	<td class="line x" title="14:229	In contrast, semi-supervised domain adaptation (Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007) is the scenario in which, in addition to the labeled source data, we only have unlabeled and no labeled target domain data." ></td>
	<td class="line x" title="15:229	Semi-supervised adaptation is a much more realistic situation, while at the same time also considerably more difficult." ></td>
	<td class="line pc" title="16:229	Studies on the supervised task have shown that straightforward baselines (e.g. models based on source only, target only, or the union of the data) achieve a relatively high performance level and are surprisingly difficult to beat (Daume III, 2007)." ></td>
	<td class="line pc" title="17:229	Thus, one conclusion from that line of work is that as soon as there is a reasonable (often even small) amount of labeled target data, it is often more fruitful to either just use that, or to apply simple adaptation techniques (Daume III, 2007; Plank and van Noord, 2008)." ></td>
	<td class="line x" title="18:229	2 Motivation and Prior Work While several authors have looked at the supervised adaptation case, there are less (and especially less successful) studies on semi-supervised domain adaptation (McClosky et al., 2006; Blitzer et al., 2006; Dredze et al., 2007)." ></td>
	<td class="line x" title="19:229	Of these, McClosky et al.(2006) deal specifically with selftraining for data-driven statistical parsing." ></td>
	<td class="line x" title="21:229	They show that together with a re-ranker, improvements 37 are obtained." ></td>
	<td class="line x" title="22:229	Similarly, Structural Correspondence Learning (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008) has proven to be successful for the two tasks examined, PoS tagging and Sentiment Classification." ></td>
	<td class="line x" title="23:229	In contrast, Dredze et al.(2007) report on frustrating results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. no team was able to improve target domain performance substantially over a state of the art baseline." ></td>
	<td class="line x" title="25:229	In the same shared task, an attempt was made to apply SCL to domain adaptation for data-driven dependency parsing (Shimizu and Nakagawa, 2007)." ></td>
	<td class="line x" title="26:229	The system just ended up at rank 7 out of 8 teams." ></td>
	<td class="line x" title="27:229	However, based on annotation differences in the datasets (Dredze et al., 2007) and a bug in their system (Shimizu and Nakagawa, 2007), their results are inconclusive.1 Thus, the effectiveness of SCL is rather unexplored for parsing." ></td>
	<td class="line x" title="28:229	So far, most previous work on domain adaptation for parsing has focused on data-driven systems (Gildea, 2001; Roark and Bacchiani, 2003; McClosky et al., 2006; Shimizu and Nakagawa, 2007), i.e. systems employing (constituent or dependency based) treebank grammars (Charniak, 1996)." ></td>
	<td class="line x" title="29:229	Parse selection constitutes an important part of many parsing systems (Johnson et al., 1999; Hara et al., 2005; van Noord and Malouf, 2005; McClosky et al., 2006)." ></td>
	<td class="line x" title="30:229	Yet, the adaptation of parse selection models to novel domains is a far less studied area." ></td>
	<td class="line x" title="31:229	This may be motivated by the fact that potential gains for this task are inherently bounded by the underlying grammar." ></td>
	<td class="line x" title="32:229	The few studies on adapting disambiguation models (Hara et al., 2005; Plank and van Noord, 2008) have focused exclusively on the supervised scenario." ></td>
	<td class="line x" title="33:229	Therefore, the direction we explore in this study is semi-supervised domain adaptation for parse disambiguation." ></td>
	<td class="line x" title="34:229	We examine the effectiveness of Structural Correspondence Learning (SCL) (Blitzer et al., 2006) for this task, a recently proposed adaptation technique shown to be effective for PoS tagging and Sentiment Analysis." ></td>
	<td class="line x" title="35:229	The system used in this study is Alpino, a wide-coverage Stochastic Attribute Value Grammar (SAVG) for Dutch (van Noord and Malouf, 2005; van Noord, 2006)." ></td>
	<td class="line x" title="36:229	For our empirical eval1As shown in Dredze et al.(2007), the biggest problem for the shared task was that the provided datasets were annotated with different annotation guidelines, thus the general conclusion was that the task was ill-defined (Nobuyuki Shimizu, personal communication)." ></td>
	<td class="line x" title="38:229	uation we explore Wikipedia as primary test and training collection." ></td>
	<td class="line x" title="39:229	In the sequel, we first introduce the parsing system." ></td>
	<td class="line x" title="40:229	Section 4 reviews Structural Correspondence Learning and shows our application of SCL to parse selection, including all our design choices." ></td>
	<td class="line x" title="41:229	In Section 5 we present the datasets, introduce the process of constructing target domain data from Wikipedia, and discuss interesting initial empirical results of this ongoing study." ></td>
	<td class="line x" title="42:229	3 Background: Alpino parser Alpino (van Noord and Malouf, 2005; van Noord, 2006) is a robust computational analyzer for Dutch that implements the conceptual two-stage parsing approach." ></td>
	<td class="line x" title="43:229	The system consists of approximately 800 grammar rules in the tradition of HPSG, and a large hand-crafted lexicon, that together with a left-corner parser constitutes the generation component." ></td>
	<td class="line x" title="44:229	For parse selection, Alpino employs a discriminative approach based on Maximum Entropy (MaxEnt)." ></td>
	<td class="line x" title="45:229	The output of the parser is dependency structure based on the guidelines of CGN (Oostdijk, 2000)." ></td>
	<td class="line x" title="46:229	The Maximum Entropy model (Berger et al., 1996; Ratnaparkhi, 1997; Abney, 1997) is a conditional model that assigns a probability to every possible parse  for a given sentence s. The model consists of a set of m feature functions fj() that describe properties of parses, together with their associated weights j. The denominator is a normalization term where Y (s) is the set of parses with yield s: p(|s;) = exp( summationtextm j=1 jfj())summationtext yY (s) exp( summationtextm j=1 jfj(y))) (1) The parameters (weights) j can be estimated efficiently by maximizing the regularized conditional likelihood of a training corpus (Johnson et al., 1999; van Noord and Malouf, 2005):  = argmax  logL()  summationtextm j=1  2j 22 (2) where L() is the likelihood of the training data." ></td>
	<td class="line x" title="47:229	The second term is a regularization term (Gaussian prior on the feature weights with mean zero and variance )." ></td>
	<td class="line x" title="48:229	The estimated weights determine the contribution of each feature." ></td>
	<td class="line x" title="49:229	Features appearing in correct parses are given increasing (positive) weight, while features in incorrect parses are 38 given decreasing (negative) weight." ></td>
	<td class="line x" title="50:229	Once a model is trained, it can be applied to choose the parse with the highest sum of feature weights." ></td>
	<td class="line x" title="51:229	The MaxEnt model consists of a large set of features, corresponding to instantiations of feature templates that model various properties of parses." ></td>
	<td class="line x" title="52:229	For instance, Part-of-Speech tags, dependency relations, grammar rule applications, etc. The current standard model uses about 11,000 features." ></td>
	<td class="line x" title="53:229	We will refer to this set of features as original features." ></td>
	<td class="line x" title="54:229	They are used to train the baseline model on the given labeled source data." ></td>
	<td class="line x" title="55:229	4 Structural Correspondence Learning SCL (Structural Correspondence Learning) (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008) is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains." ></td>
	<td class="line x" title="56:229	Before describing the algorithm in detail, let us illustrate the intuition behind SCL with an example, borrowed from Blitzer et al.(2007)." ></td>
	<td class="line x" title="58:229	Suppose we have a Sentiment Analysis system trained on book reviews (domain A), and we would like to adapt it to kitchen appliances (domain B)." ></td>
	<td class="line x" title="59:229	Features such as boring and repetitive are common ways to express negative sentiment in A, while not working or defective are specific to B. If there are features across the domains, e.g. dont buy, with which the domain specific features are highly correlated with, then we might tentatively align those features." ></td>
	<td class="line x" title="60:229	Therefore, the key idea of SCL is to identify automatically correspondences among features from different domains by modeling their correlations with pivot features." ></td>
	<td class="line x" title="61:229	Pivots are features occurring frequently and behaving similarly in both domains (Blitzer et al., 2006)." ></td>
	<td class="line x" title="62:229	They are inspired by auxiliary problems from Ando and Zhang (2005)." ></td>
	<td class="line x" title="63:229	Non-pivot features that correspond with many of the same pivot-features are assumed to correspond." ></td>
	<td class="line x" title="64:229	Intuitively, if we are able to find good correspondences among features, then the augmented labeled source domain data should transfer better to a target domain (where no labeled data is available) (Blitzer et al., 2006)." ></td>
	<td class="line x" title="65:229	The outline of the algorithm is given in Figure 1." ></td>
	<td class="line x" title="66:229	The first step is to identify m pivot features occurring frequently in the unlabeled data of both Input: labeled source data {(xs,ys)Nss=1} unlabeled data from both source and target domain xul = xs,xt 1." ></td>
	<td class="line x" title="67:229	Select m pivot features 2." ></td>
	<td class="line x" title="68:229	Train m binary classifiers (pivot predictors) 3." ></td>
	<td class="line x" title="69:229	Create matrix Wnm of binary predictor weight vectors W = [w1,,wm], where n is the number of nonpivot features in xul 4." ></td>
	<td class="line x" title="70:229	Apply SVD to W: Wnm = UnnDnmV Tmm where  = UT[1:h,:] are the h top left singular vectors of W. 5." ></td>
	<td class="line x" title="71:229	Apply projection xs and train a predictor on the original and new features obtained through the projection." ></td>
	<td class="line x" title="72:229	Figure 1: SCL algorithm (Blitzer et al., 2006)." ></td>
	<td class="line x" title="73:229	domains." ></td>
	<td class="line x" title="74:229	Then, a binary classifier is trained for each pivot feature (pivot predictor) of the form: Does pivot feature l occur in this instance?." ></td>
	<td class="line x" title="75:229	The pivots are masked in the unlabeled data and the aim is to predict them using non-pivot features." ></td>
	<td class="line x" title="76:229	In this way, we obtain a weight vector w for each pivot predictor." ></td>
	<td class="line x" title="77:229	Positive entries in the weight vector indicate that a non-pivot is highly correlated with the respective pivot feature." ></td>
	<td class="line x" title="78:229	Step 3 is to arrange the m weight vectors in a matrix W, where a column corresponds to a pivot predictor weight vector." ></td>
	<td class="line x" title="79:229	Applying the projection WTx (where x is a training instance) would give us m new features, however, for both computational and statistical reasons (Blitzer et al., 2006; Ando and Zhang, 2005) a low-dimensional approximation of the original feature space is computed by applying Singular Value Decomposition (SVD) on W (step 4)." ></td>
	<td class="line x" title="80:229	Let  = UThn be the top h left singular vectors of W (with h a dimension parameter and n the number of non-pivot features)." ></td>
	<td class="line x" title="81:229	The resulting  is a projection onto a lower dimensional space Rh, parameterized by h. The final step of SCL is to train a linear predictor on the augmented labeled source data x,x." ></td>
	<td class="line x" title="82:229	In more detail, the original feature space x is augmented with h new features obtained by applying the projection x. In this way, we can learn weights for domain-specific features, which otherwise would not have been observed." ></td>
	<td class="line x" title="83:229	If  contains meaningful correspondences, then the pre39 dictor trained on the augmented data should transfer well to the new domain." ></td>
	<td class="line x" title="84:229	4.1 SCL for Parse Disambiguation A property of the pivot predictors is that they can be trained from unlabeled data, as they represent properties of the input." ></td>
	<td class="line x" title="85:229	So far, pivot features on the word level were used (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008), e.g. Does the bigram not buy occur in this document? (Blitzer, 2008)." ></td>
	<td class="line x" title="86:229	Pivot features are the key ingredient for SCL, and they should align well with the NLP task." ></td>
	<td class="line x" title="87:229	For PoS tagging and Sentiment Analysis, features on the word level are intuitively well-related to the problem at hand." ></td>
	<td class="line x" title="88:229	For the task of parse disambiguation based on a conditional model this is not the case." ></td>
	<td class="line x" title="89:229	Hence, we actually introduce an additional and new layer of abstraction, which, we hypothesize, aligns well with the task of parse disambiguation: we first parse the unlabeled data." ></td>
	<td class="line x" title="90:229	In this way we obtain full parses for given sentences as produced by the grammar, allowing access to more abstract representations of the underlying pivot predictor training data (for reasons of efficiency, we here use only the first generated parse as training data for the pivot predictors, rather than n-best)." ></td>
	<td class="line x" title="91:229	Thus, instead of using word-level features, our features correspond to properties of the generated parses: application of grammar rules (r1,r2 features), dependency relations (dep), PoS tags (f1,f2), syntactic features (s1), precedence (mf ), bilexical preferences (z), apposition (appos) and further features for unknown words, temporal phrases, coordination (h,in year and p1, respectively)." ></td>
	<td class="line x" title="92:229	This allows us to get a possibly noisy, but more abstract representation of the underlying data." ></td>
	<td class="line x" title="93:229	The set of features used in Alpino is further described in van Noord and Malouf (2005)." ></td>
	<td class="line x" title="94:229	Selection of pivot features As pivot features should be common across domains, here we restrict our pivots to be of the type r1,p1,s1 (the most frequently occurring feature types)." ></td>
	<td class="line x" title="95:229	In more detail, r1 indicates which grammar rule applied, p1 whether coordination conjuncts are parallel, and s1 whether topicalization or long-distance dependencies occurred." ></td>
	<td class="line x" title="96:229	We count how often each feature appears in the parsed source and target domain data, and select those r1,p1,s1 features as pivot features, whose count is > t, where t is a specified threshold." ></td>
	<td class="line x" title="97:229	In all our experiments, we set t = 5000." ></td>
	<td class="line x" title="98:229	In this way we obtained on average 360 pivot features, on the datasets described in Section 5." ></td>
	<td class="line x" title="99:229	Predictive features As pointed out by Blitzer et al.(2006), each instance will actually contain features which are totally predictive of the pivot features (i.e. the pivot itself)." ></td>
	<td class="line x" title="101:229	In our case, we additionally have to pay attention to more specific features, e.g. r2 is a feature that extends r1, in the sense that it incorporates more information than its parent (i.e. which grammar rules applied in the construction of daughter nodes)." ></td>
	<td class="line x" title="102:229	It is crucial to remove these predictive features when creating the training data for the pivot predictors." ></td>
	<td class="line x" title="103:229	Matrix and SVD Following Blitzer et al.(2006) (which follow Ando and Zhang (2005)), we only use positive entries in the pivot predictors weight vectors to compute the SVD." ></td>
	<td class="line x" title="105:229	Thus, when constructing the matrix W, we disregard all negative entries in W and compute the SVD (W = UDV T ) on the resulting non-negative sparse matrix." ></td>
	<td class="line x" title="106:229	This sparse representation saves both time and space." ></td>
	<td class="line x" title="107:229	4.2 Further practical issues of SCL In practice, there are more free parameters and model choices (Ando and Zhang, 2005; Ando, 2006; Blitzer et al., 2006; Blitzer, 2008) besides the ones discussed above." ></td>
	<td class="line x" title="108:229	Feature normalization and feature scaling." ></td>
	<td class="line x" title="109:229	Blitzer et al.(2006) found it necessary to normalize and scale the new features obtained by the projection , in order to allow them to receive more weight from a regularized discriminative learner." ></td>
	<td class="line x" title="111:229	For each of the features, they centered them by subtracting out the mean and normalized them to unit variance (i.e. x  mean/sd)." ></td>
	<td class="line x" title="112:229	They then rescaled the features by a factor  found on heldout data: x. Restricted Regularization." ></td>
	<td class="line x" title="113:229	When training the supervised model on the augmented feature space x,x, Blitzer et al.(2006) only regularize the weight vector of the original features, but not the one for the new low-dimensional features." ></td>
	<td class="line x" title="115:229	This was done to encourage the model to use the new low-dimensional representation rather than the higher-dimensional original representation (Blitzer, 2008)." ></td>
	<td class="line x" title="116:229	Dimensionality reduction by feature type." ></td>
	<td class="line x" title="117:229	An extension suggested in Ando and Zhang (2005) is 40 to compute separate SVDs for blocks of the matrix W corresponding to feature types (as illustrated in Figure 2), and then to apply separate projection for every type." ></td>
	<td class="line x" title="118:229	Due to the positive results in Ando (2006), Blitzer et al.(2006) include this in their standard setting of SCL and report results using block SVDs only." ></td>
	<td class="line x" title="120:229	Figure 2: Illustration of dimensionality reduction by feature type (Ando and Zhang, 2005)." ></td>
	<td class="line x" title="121:229	The grey area corresponds to a feature type (submatrix of W) on which the SVD is computed (block SVD); the white area is regarded as fixed to zero matrices." ></td>
	<td class="line x" title="122:229	5 Experiments and Results 5.1 Experimental design The base (source domain) disambiguation model is trained on the Alpino Treebank (van Noord, 2006) (newspaper text), which consists of approximately 7,000 sentences and 145,000 tokens." ></td>
	<td class="line x" title="123:229	For parameter estimation of the disambiguation model, in all reported experiments we use the TADM2 toolkit (toolkit for advanced discriminative training), with a Gaussian prior (2=1000) and the (default) limited memory variable metric estimation technique (Malouf, 2002)." ></td>
	<td class="line x" title="124:229	For training the binary pivot predictors, we use the MegaM3 Optimization Package with the socalled bernoulli implicit input format." ></td>
	<td class="line x" title="125:229	To compute the SVD, we use SVDLIBC.4 The output of the parser is dependency structure." ></td>
	<td class="line x" title="126:229	A standard evaluation metric is to measure the amount of generated dependencies that are identical to the stored dependencies (correct labeled dependencies), expressed as f-score." ></td>
	<td class="line x" title="127:229	An alternative measure is concept accuracy (CA), which is similar to f-score, but allows possible discrepancy between the number of returned dependencies (van Noord, 2006; Plank and van Noord, 2http://tadm.sourceforge.net/ 3http://www.cs.utah.edu/hal/megam/ 4http://tedlab.mit.edu/dr/svdlibc/ 2008)." ></td>
	<td class="line x" title="128:229	CA is usually slightly lower than f-score." ></td>
	<td class="line x" title="129:229	Let Dip be the number of dependencies produced by the parser for sentence i. Dig is the number of dependencies in the treebank parse, and Dio is the number of correct dependencies produced by the parser." ></td>
	<td class="line x" title="130:229	Then, CA = Dosummationtext i max(Dig,Dip) If we want to compare the performance of disambiguation models, we can employ the  measure (van Noord and Malouf, 2005; van Noord, 2007)." ></td>
	<td class="line x" title="131:229	Intuitively, it tells us how much of the disambiguation problem has been solved." ></td>
	<td class="line x" title="132:229	 = CAbaseoracle  base  100 In more detail, the  measure incorporates an upper and lower bound: base measures the accuracy of a model that simply selects the first parse for each sentence; oracle represents the accuracy achieved by a model that always selects the best parse from the set of potential parses (within the coverage of the parser)." ></td>
	<td class="line x" title="133:229	In addition, we also report relative error reduction (rel.er), which is the relative difference in  scores for two models." ></td>
	<td class="line x" title="134:229	As target domain, we consider the Dutch part of Wikipedia as data collection, described in the following." ></td>
	<td class="line x" title="135:229	5.2 Wikipedia as resource In our experiments, we exploit Wikipedia both as testset and as unlabeled data source." ></td>
	<td class="line x" title="136:229	We assume that in order to parse data from a very specific domain, say about the artist Prince, then data related to that domain, like information about the New Power Generation, the Purple rain movie, or other American singers and artists, should be of help." ></td>
	<td class="line x" title="137:229	Thus, we exploit Wikipedia and its category system to gather domain-specific target data." ></td>
	<td class="line x" title="138:229	Construction of target domain data In more detail, we use the Dutch part of Wikipedia provided by WikiXML,5 a collection of Wikipedia articles converted to XML format." ></td>
	<td class="line x" title="139:229	As the corpus is encoded in XML, we can exploit general purpose XML Query Languages, such as XQuery, Xslt and XPath, to extract relevant information from the Wikipedia corpus." ></td>
	<td class="line x" title="140:229	Given a wikipage p, with c  categories(p), we can identify pages related to p of various 5http://ilps.science.uva.nl/WikiXML/ 41 types of relatedness: directly related pages (those that share a category, i.e. all p where c  categories(p) such that c = c), or alternatively, pages that share a subor supercategory of p, i.e. p where c  categories(p) and c  sub categories(p) or c  super categories(p)." ></td>
	<td class="line x" title="141:229	For example, Figure 3 shows the categories extracted for the Wikipedia article about pope Johannes Paulus II." ></td>
	<td class="line x" title="142:229	<wikipage id='6677'> <cat t='direct' n='Categorie:Paus'/> <cat t='direct' n='Categorie:Pools_theoloog'/> <cat t='super' n='Categorie:Religieus leider'/> <cat t='super' n='Categorie:Rooms-katholiek persoon'/> <cat t='super' n='Categorie:Vaticaanstad'/> <cat t='super' n='Categorie:Bisschop'/> <cat t='super' n='Categorie:Kerkgeschiedenis'/> <cat t='sub' n='Categorie:Tegenpaus'/> <cat t='super' n='Categorie:Pools persoon'/> </wikipage> Figure 3: Example of extracted Wikipedia categories for a given article (direct, supand subcats)." ></td>
	<td class="line x" title="143:229	To create the set of related pages for a given article p, we proceed as follows: 1." ></td>
	<td class="line x" title="144:229	Find suband supercategories of p 2." ></td>
	<td class="line x" title="145:229	Extract all pages that are related to p (through sharing a direct, sub or super category) 3." ></td>
	<td class="line x" title="146:229	Optionally, filter out certain pages In our empirical setup, we followed Blitzer et al.(2006) and tried to balance the size of source and target data." ></td>
	<td class="line x" title="148:229	Thus, depending on the size of the resulting target domain dataset, and the broadness of the categories involved in creating it, we might wish to filter out certain pages." ></td>
	<td class="line x" title="149:229	We implemented a filter mechanism that excludes pages of a certain category (e.g. a supercategory that is hypothesized to be too broad)." ></td>
	<td class="line x" title="150:229	Alternatively, we might have used a filter mechanism that excludes certain pages directly." ></td>
	<td class="line x" title="151:229	In our experiments, we always included pages that are directly related to a page of interest, and those that shared a subcategory." ></td>
	<td class="line x" title="152:229	Of course, the page itself is not included in that dataset." ></td>
	<td class="line x" title="153:229	With regard to supercategories, we usually included all pages having a category c  super categories(p), unless stated otherwise." ></td>
	<td class="line x" title="154:229	Test collection Our testset consists of a selection of Wikipedia articles that have been manually corrected in the course of the D-Coi/LASSY project.6 6Ongoing project, see http://www.let.rug.nl/ vannoord/Lassy/ An overview of the testset including size indications is given in Table 1." ></td>
	<td class="line x" title="155:229	Table 2 provides information on the target domain datasets constructed from Wikipedia." ></td>
	<td class="line x" title="156:229	Wiki/DCOI ID Title Sents 6677/026563 Prince (musician) 358 6729/036834 Paus Johannes Paulus II 232 182654/041235 Augustus De Morgan 259 Table 1: Size of test datasets." ></td>
	<td class="line x" title="157:229	Related to Articles Sents Tokens Relationship Prince 290 9,772 145,504 filtered super Paus 445 8,832 134,451 all De Morgan 394 8,466 132,948 all Table 2: Size of related unlabeled data; relationship indicates whether all related pages are used or some are filtered out (see section 5.2)." ></td>
	<td class="line x" title="158:229	5.3 Empirical Results For all reported results, we randomly select n = 200 maximum number of parses per sentence for evaluation." ></td>
	<td class="line x" title="159:229	Baseline accuracies Table 3 shows the baseline performance (of the standard Alpino model) on the various Wikipedia testsets (CA, f-score)." ></td>
	<td class="line x" title="160:229	The third and fourth column indicate the upperand lower bound measures (defined in section 5.1)." ></td>
	<td class="line x" title="161:229	Title CA f-score base oracle Prince (musician) 85.03 85.38 71.95 88.70 Paus Johannes Paulus II 85.72 86.32 74.30 89.09 Augustus De Morgan 80.09 80.61 70.08 83.52 Table 3: Baseline results." ></td>
	<td class="line x" title="162:229	While the parser normally operates on an accuracy level of roughly 88-89% (van Noord, 2007) on its own domain (newspaper text), the accuracy on these subdomains drops to around 85%." ></td>
	<td class="line x" title="163:229	The biggest performance decrease (to 80%) was on the article about the British logician and mathematician De Morgan." ></td>
	<td class="line x" title="164:229	This confirms the intuition that this specific subdomain is the hardest, given that mathematical expressions might emerge in the data (e.g. Wet der distributiviteit : a(b+c) = ab+ac distributivity law)." ></td>
	<td class="line x" title="165:229	SCL results Table 4 shows the results of our instantiation of SCL for parse disambiguation, with varying h parameter (dimensionality parameter; 42 h = 25 means that applying the projection x resulted in adding 25 new features to every source domain instance)." ></td>
	<td class="line x" title="166:229	CA f-score  rel.er." ></td>
	<td class="line x" title="167:229	baseline Prince 85.03 85.38 78.06 0.00 SCL[+/-], h = 25 85.12 85.46 78.64 2.64 SCL[+/-], h = 50 85.29 85.63 79.66 7.29 SCL[+/-], h = 100 85.19 85.53 79.04 4.47 SCL[+/-], h = 200 85.21 85.54 79.18 5.10 baseline Paus 85.72 86.32 77.23 0.00 SCL[+/-], h = 25 85.87 86.48 78.26 4.52 SCL[+/-], h = 50 85.82 86.43 77.87 2.81 SCL[+/-], h = 100 85.87 86.49 78.26 4.52 SCL[+/-], h = 200 85.87 86.48 78.26 4.52 baseline DeMorgan 80.09 80.61 74.44 0.00 SCL[+/-], h = 25 80.15 80.67 74.92 1.88 SCL[+/-], h = 50 80.12 80.64 74.68 0.94 SCL[+/-], h = 100 80.12 80.64 74.68 0.94 SCL[+/-], h = 200 80.15 80.67 74.91 1.88 Table 4: Results of our instantiation of SCL (with varying h parameter and no feature normalization)." ></td>
	<td class="line x" title="168:229	The results show a (sometimes) small but consistent increase in absolute performance on all testsets over the baseline system (up to +0.26 absolute CA score), as well as an increase in  measure (absolute error reduction)." ></td>
	<td class="line x" title="169:229	This corresponds to a relative error reduction of up to 7.29%." ></td>
	<td class="line x" title="170:229	Thus, our first instantiation of SCL for parse disambiguation indeed shows promising results." ></td>
	<td class="line x" title="171:229	We can confirm that changing the dimensionality parameter h has rather little effect (Table 4), which is in line with previous findings (Ando and Zhang, 2005; Blitzer et al., 2006)." ></td>
	<td class="line x" title="172:229	Thus we might fix the parameter and prefer smaller dimensionalities, which saves space and time." ></td>
	<td class="line x" title="173:229	Note that these results were obtained without any of the additional normalization, rescaling, feature-specific regularization, or block SVD issues, etc.(discussed in section 4.2)." ></td>
	<td class="line x" title="175:229	We used the same Gaussian regularization term (2=1000) for all features (original and new features), and did not perform any feature normalization or rescaling." ></td>
	<td class="line x" title="176:229	This means our current instantiation of SCL is an actually simplified version of the original SCL algorithm, applied to parse disambiguation." ></td>
	<td class="line x" title="177:229	Of course, our results are preliminary and, rather than warranting many definite conclusions, encourage further exploration of SCL and related semi-supervised adaptation techniques." ></td>
	<td class="line x" title="178:229	5.4 Additional Empirical Results In the following, we describe additional results obtained by extensions and/or refinements of our current SCL instantiation." ></td>
	<td class="line x" title="179:229	Feature normalization." ></td>
	<td class="line x" title="180:229	We also tested feature normalization (as described in Section 4.2)." ></td>
	<td class="line x" title="181:229	While Blitzer et al.(2006) found it necessary to normalize (and scale) the projection features, we did not observe any improvement by normalizing them (actually, it slightly degraded performance in our case)." ></td>
	<td class="line x" title="183:229	Thus, we found this step unnecessary, and currently did not look at this issue any further." ></td>
	<td class="line x" title="184:229	A look at  To gain some insight of which kind of correspondences SCL learned in our case, we started to examine the rows of ." ></td>
	<td class="line x" title="185:229	Recall that applying a row of the projection matrix i to a training instance x gives us a new real-valued feature." ></td>
	<td class="line x" title="186:229	If features from different domains have similar entries (scores) in the projection row, they are assumed to correspond (Blitzer, 2008)." ></td>
	<td class="line x" title="187:229	Figure 4 shows example of correspondences that SCL found in the Prince dataset." ></td>
	<td class="line x" title="188:229	The first column represents the score of a feature." ></td>
	<td class="line x" title="189:229	The labels wiki and alp indicate the domain of the features, respectively." ></td>
	<td class="line x" title="190:229	For readability, we here grouped the features obtaining similar scores." ></td>
	<td class="line x" title="191:229	0.00010248|dep35(Chaka Khan,name(PER),hd/su,verb,ben)|wiki 0.00010248|dep35(de,det,hd/det,adj,Afro-Amerikaanse)|wiki 0.00010248|dep35(Yvette Marie Stevens,name(PER),hd/app, noun,zangeres)|wiki 0.000102772|dep34(leraar,noun,hd/su,verb)|alp 0.000161095|dep34(commissie,noun,hd/obj1,prep)|16|alp 0.00016113|dep34(Confessions Tour,name,hd/obj1,prep)|2|wiki 0.000161241|dep34(orgel,noun,hd/obj1,prep)|1|wiki 0.000217698|dep34(tournee,noun,hd/su,verb)|1|wiki 0.000223301|dep34(regisseur,noun,hd/su,verb)|15|wiki 0.000224517|dep34(voorsprong,noun,hd/su,verb)|2|alp 0.000224684|dep34(wetenschap,noun,hd/su,verb)|2|alp 0.000226617|dep34(pop_rock,noun,hd/su,verb)|1|wiki 0.000228918|dep34(plan,noun,hd/su,verb)|9|alp Figure 4: Example projection from  (row 2)." ></td>
	<td class="line x" title="192:229	SCL clustered information about Chaka Khan, an Afro-Amerikaanse zangeres (afro-american singer) whose real name is Yvette Marie Stevens." ></td>
	<td class="line x" title="193:229	She had close connections to Prince, who even wrote one of her singles." ></td>
	<td class="line x" title="194:229	These features got aligned to the Alpino feature leraar (teacher)." ></td>
	<td class="line x" title="195:229	Moreover, SCL finds that tournee, regisseur and pop rock in the Prince domain behave like voorsprong (advance), wetenschap (research) and plan as possible heads in a subject relation in the newspaper domain." ></td>
	<td class="line x" title="196:229	Similarly, correspon43 dences between the direct object features Confessions Tour and orgel (pipe organ) to commissie (commission) are discovered." ></td>
	<td class="line x" title="197:229	More unlabeled data In the experiments so far, we balanced the amount of source and target data." ></td>
	<td class="line x" title="198:229	We started to examine the effect of more unlabeled target domain data." ></td>
	<td class="line x" title="199:229	For the Prince dataset, we included all supercategories in constructing the related target domain data." ></td>
	<td class="line x" title="200:229	The so obtained dataset contains: 859 articles, 29,186 sentences and 385,289 tokens; hence, the size approximately tripled (w.r.t. Table 2)." ></td>
	<td class="line x" title="201:229	Table 5 shows the effect of using this larger dataset for SCL with h = 25." ></td>
	<td class="line x" title="202:229	The accuracy increases (from 85.12 to 85.25)." ></td>
	<td class="line x" title="203:229	Thus, there seems to be a positive effect (to be investigated further)." ></td>
	<td class="line x" title="204:229	CA f-score  rel.er." ></td>
	<td class="line x" title="205:229	baseline Prince 85.03 85.38 78.06 0.00 SCL[+/-], h = 25, all 85.25 85.58 79.42 6.20 Table 5: First result on increasing unlabeled data." ></td>
	<td class="line x" title="206:229	Dimensionality reduction by feature type We have started to implement the extension discussed in section 4.2, i.e. perform separate dimensionality reductions based on blocks of nonpivot features." ></td>
	<td class="line x" title="207:229	We clustered nonpivots (see section 4.1 for a description) into 9 types (ordered in terms of decreasing cluster size): dep, f1/f2 (pos), r1/r2 (rules), appos person, mf, z, h1, in year, dist. For each type, a separate SVD was computed on submatrix Wt (illustrated in Figure 2)." ></td>
	<td class="line x" title="208:229	Then, separate projections were applied to every training instance." ></td>
	<td class="line x" title="209:229	The results of these experiments on the Prince dataset are shown in Figure 5." ></td>
	<td class="line x" title="210:229	Applying SCL with dimensionality reduction by feature type (SCL block) results in a model that performs better (CA 85.27,  79.52, rel.er." ></td>
	<td class="line x" title="211:229	6.65%) than the model with no feature split (no block SVDs), thus obtaining a relative error reduction of 6.65% over the baseline." ></td>
	<td class="line x" title="212:229	The same figure also shows what happens if we remove a specific feature type at a time; the apposition features contribute the most on this Prince domain." ></td>
	<td class="line x" title="213:229	As a fact, one third of the sentences in the Prince testset contain constructions with appositions (e.g. about film-, albumand song titles)." ></td>
	<td class="line x" title="214:229	6 Conclusions and Future Work The paper presents an application of Structural Correspondence Learning (SCL) to parse disamFigure 5: Results of dimensionality reduction by feature type, h = 25; block SVD included all 9 feature types; the right part shows the accuracy when one feature type was removed." ></td>
	<td class="line x" title="215:229	biguation." ></td>
	<td class="line x" title="216:229	While SCL has been successfully applied to PoS tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007), its effectiveness for parsing was rather unexplored." ></td>
	<td class="line x" title="217:229	The empirical results show that our instantiation of SCL to parse disambiguation gives promising initial results, even without the many additional extensions on the feature level as done in Blitzer et al.(2006)." ></td>
	<td class="line x" title="219:229	We exploited Wikipedia as primary resource, both for collecting unlabeled target domain data, as well as test suite for empirical evaluation." ></td>
	<td class="line x" title="220:229	On the three examined datasets, SCL slightly but constantly outperformed the baseline." ></td>
	<td class="line x" title="221:229	Applying SCL involves many design choices and practical issues, which we tried to depict here in detail." ></td>
	<td class="line x" title="222:229	A novelty in our application is that we first actually parse the unlabeled data from both domains." ></td>
	<td class="line x" title="223:229	This allows us to get a possibly noisy, but more abstract representation of the underlying data on which the pivot predictors are trained." ></td>
	<td class="line x" title="224:229	In the near future, we plan to extend the work on semi-supervised domain adaptation for parse disambiguation, viz." ></td>
	<td class="line x" title="225:229	(1) further explore/refine SCL (block SVDs, varying amount of target domain data, other testsets, etc.), and (2) examine selftraining." ></td>
	<td class="line x" title="226:229	Studies on the latter have focused mainly on generative, constituent based, i.e. data-driven parsing systems." ></td>
	<td class="line x" title="227:229	Furthermore, from a machine learning point of view, it would be interesting to know a measure of corpus similarity to estimate the success of porting an NLP system from one domain to another." ></td>
	<td class="line x" title="228:229	This relates to the general question of what is meant by domain." ></td>
	<td class="line x" title="229:229	44" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-1032
Domain Adaptation with Latent Semantic Association for Named Entity Recognition
Guo, Hong Lei;Zhu, Huijia;Guo, Zhili;Zhang, Xiaoxun;Wu, Xian;Su, Zhong;"></td>
	<td class="line x" title="1:250	Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 281289, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:250	c 2009 Association for Computational Linguistics Domain Adaptation with Latent Semantic Association for Named Entity Recognition Honglei Guo Huijia Zhu Zhili Guo Xiaoxun Zhang Xian Wu and Zhong Su IBM China Research Laboratory Beijing, P. R. China {guohl, zhuhuiji, guozhili, zhangxx, wuxian, suzhong}@cn.ibm.com Abstract Domain adaptation is an important problem in named entity recognition (NER)." ></td>
	<td class="line x" title="3:250	NER classifiers usually lose accuracy in the domain transfer due to the different data distribution between the source and the target domains." ></td>
	<td class="line x" title="4:250	The major reason for performance degrading is that each entity type often has lots of domainspecific term representations in the different domains." ></td>
	<td class="line x" title="5:250	The existing approaches usually need an amount of labeled target domain data for tuning the original model." ></td>
	<td class="line x" title="6:250	However, it is a labor-intensive and time-consuming task to build annotated training data set for every target domain." ></td>
	<td class="line x" title="7:250	We present a domain adaptation method with latent semantic association (LaSA)." ></td>
	<td class="line x" title="8:250	This method effectively overcomes the data distribution difference without leveraging any labeled target domain data." ></td>
	<td class="line x" title="9:250	LaSA model is constructed to capture latent semantic association among words from the unlabeled corpus." ></td>
	<td class="line x" title="10:250	It groups words into a set of concepts according to the related context snippets." ></td>
	<td class="line x" title="11:250	In the domain transfer, the original term spaces of both domains are projected to a concept space using LaSA model at first, then the original NER model is tuned based on the semantic association features." ></td>
	<td class="line x" title="12:250	Experimental results on English and Chinese corpus show that LaSA-based domain adaptation significantly enhances the performance of NER." ></td>
	<td class="line x" title="13:250	1 Introduction Named entities (NE) are phrases that contain names of persons, organizations, locations, etc. NER is an important task in information extraction and natural language processing (NLP) applications." ></td>
	<td class="line x" title="14:250	Supervised learning methods can effectively solve NER problem by learning a model from manually labeled data (Borthwick, 1999; Sang and Meulder, 2003; Gao et al., 2005; Florian et al., 2003)." ></td>
	<td class="line x" title="15:250	However, empirical study shows that NE types have different distribution across domains (Guo et al., 2006)." ></td>
	<td class="line x" title="16:250	Trained NER classifiers in the source domain usually lose accuracy in a new target domain when the data distribution is different between both domains." ></td>
	<td class="line x" title="17:250	Domain adaptation is a challenge for NER and other NLP applications." ></td>
	<td class="line x" title="18:250	In the domain transfer, the reason for accuracy loss is that each NE type often has various specific term representations and context clues in the different domains." ></td>
	<td class="line x" title="19:250	For example, {economist, singer, dancer, athlete, player, philosopher, } are used as context clues for NER." ></td>
	<td class="line x" title="20:250	However, the distribution of these representations are varied with domains." ></td>
	<td class="line x" title="21:250	We expect to do better domain adaptation for NER by exploiting latent semantic association among words from different domains." ></td>
	<td class="line x" title="22:250	Some approaches have been proposed to group words into topics to capture important relationships between words, such as Latent Semantic Indexing (LSI) (Deerwester et al., 1990), probabilistic Latent Semantic Indexing (pLSI) (Hofmann, 1999), Latent Dirichlet Allocation (LDA) (Blei et al., 2003)." ></td>
	<td class="line x" title="23:250	These models have been successfully employed in topic modeling, dimensionality reduction for text categorization (Blei et al., 2003), ad hoc IR (Wei and Croft., 2006), and so on." ></td>
	<td class="line x" title="24:250	In this paper, we present a domain adaptation method with latent semantic association." ></td>
	<td class="line x" title="25:250	We focus 281 on capturing the hidden semantic association among words in the domain adaptation." ></td>
	<td class="line x" title="26:250	We introduce the LaSA model to overcome the distribution difference between the source domain and the target domain." ></td>
	<td class="line x" title="27:250	LaSA model is constructed from the unlabeled corpus at first." ></td>
	<td class="line x" title="28:250	It learns latent semantic association among words from their related context snippets." ></td>
	<td class="line x" title="29:250	In the domain transfer, words in the corpus are associated with a low-dimension concept space using LaSA model, then the original NER model is tuned using these generated semantic association features." ></td>
	<td class="line x" title="30:250	The intuition behind our method is that words in one concept set will have similar semantic features or latent semantic association, and share syntactic and semantic context in the corpus." ></td>
	<td class="line x" title="31:250	They can be considered as behaving in the same way for discriminative learning in the source and target domains." ></td>
	<td class="line x" title="32:250	The proposed method associates words from different domains on a semantic level rather than by lexical occurrence." ></td>
	<td class="line x" title="33:250	It can better bridge the domain distribution gap without any labeled target domain samples." ></td>
	<td class="line x" title="34:250	Experimental results on English and Chinese corpus show that LaSA-based adaptation significantly enhances NER performance across domains." ></td>
	<td class="line x" title="35:250	The rest of this paper is organized as follows." ></td>
	<td class="line x" title="36:250	Section 2 briefly describes the related works." ></td>
	<td class="line x" title="37:250	Section 3 presents a domain adaptation method based on latent semantic association." ></td>
	<td class="line x" title="38:250	Section 4 illustrates how to learn LaSA model from the unlabeled corpus." ></td>
	<td class="line x" title="39:250	Section 5 shows experimental results on large-scale English and Chinese corpus across domains, respectively." ></td>
	<td class="line x" title="40:250	The conclusion is given in Section 6." ></td>
	<td class="line x" title="41:250	2 Related Works Some domain adaptation techniques have been employed in NLP in recent years." ></td>
	<td class="line x" title="42:250	Some of them focus on quantifying the generalizability of certain features across domains." ></td>
	<td class="line x" title="43:250	Roark and Bacchiani (2003) use maximum a posteriori (MAP) estimation to combine training data from the source and target domains." ></td>
	<td class="line x" title="44:250	Chelba and Acero (2004) use the parameters of the source domain maximum entropy classifier as the means of a Gaussian prior when training a new model on the target data." ></td>
	<td class="line x" title="45:250	Daume III and Marcu (2006) use an empirical Bayes model to estimate a latent variable model grouping instances into domain-specific or common across both domains." ></td>
	<td class="line oc" title="46:250	Daume III (2007) further augments the feature space on the instances of both domains." ></td>
	<td class="line x" title="47:250	Jiang and Zhai (2006) exploit the domain structure contained in the training examples to avoid over-fitting the training domains." ></td>
	<td class="line x" title="48:250	Arnold et al.(2008) exploit feature hierarchy for transfer learning in NER." ></td>
	<td class="line x" title="50:250	Instance weighting (Jiang and Zhai, 2007) and active learning (Chan and Ng, 2007) are also employed in domain adaptation." ></td>
	<td class="line x" title="51:250	Most of these approaches need the labeled target domain samples for the model estimation in the domain transfer." ></td>
	<td class="line x" title="52:250	Obviously, they require much efforts for labeling the target domain samples." ></td>
	<td class="line x" title="53:250	Some approaches exploit the common structure of related problems." ></td>
	<td class="line x" title="54:250	Ando et al.(2005) learn predicative structures from multiple tasks and unlabeled data." ></td>
	<td class="line x" title="56:250	Blitzer et al.(2006, 2007) employ structural corresponding learning (SCL) to infer a good feature representation from unlabeled source and target data sets in the domain transfer." ></td>
	<td class="line x" title="58:250	We present LaSA model to overcome the data gap across domains by capturing latent semantic association among words from unlabeled source and target data." ></td>
	<td class="line x" title="59:250	In addition, Miller et al.(2004) and Freitag (2004) employ distributional and hierarchical clustering methods to improve the performance of NER within a single domain." ></td>
	<td class="line x" title="61:250	Li and McCallum (2005) present a semi-supervised sequence modeling with syntactic topic models." ></td>
	<td class="line x" title="62:250	In this paper, we focus on capturing hidden semantic association among words in the domain adaptation." ></td>
	<td class="line x" title="63:250	3 Domain Adaptation Based on Latent Semantic Association The challenge in domain adaptation is how to capture latent semantic association from the source and target domain data." ></td>
	<td class="line x" title="64:250	We present a LaSA-based domain adaptation method in this section." ></td>
	<td class="line x" title="65:250	NER can be considered as a classification problem." ></td>
	<td class="line x" title="66:250	Let X be a feature space to represent the observed word instances, and let Y be the set of class labels." ></td>
	<td class="line x" title="67:250	Let ps(x,y) and pt(x,y) be the true underlying distributions for the source and the target domains, respectively." ></td>
	<td class="line x" title="68:250	In order to minimize the efforts required in the domain transfer, we often expect to use ps(x,y) to approximate pt(x,y)." ></td>
	<td class="line x" title="69:250	However, data distribution are often varied with the domains." ></td>
	<td class="line x" title="70:250	For example, in the economics-to282 entertainment domain transfer, although many NE triggers (e.g. company and Mr.) are used in both domains, some are totally new, like dancer, singer." ></td>
	<td class="line x" title="71:250	Moreover, many useful words (e.g. economist) in the economics NER are useless in the entertainment domain." ></td>
	<td class="line x" title="72:250	The above examples show that features could change behavior across domains." ></td>
	<td class="line x" title="73:250	Some useful predictive features from one domain are not predictive or do not appear in another domain." ></td>
	<td class="line x" title="74:250	Although some triggers (e.g. singer, economist) are completely distinct for each domain, they often appear in the similar syntactic and semantic context." ></td>
	<td class="line x" title="75:250	For example, triggers of person entity often appear as the subject of visited, said, etc, or are modified by excellent, popular, famous etc. Such latent semantic association among words provides useful hints for overcoming the data distribution gap of both domains." ></td>
	<td class="line x" title="76:250	Hence, we present a LaSA model s,t to capture latent semantic association among words in the domain adaptation." ></td>
	<td class="line x" title="77:250	s,t is learned from the unlabeled source and target domain data." ></td>
	<td class="line x" title="78:250	Each instance is characterized by its co-occurred context distribution in the learning." ></td>
	<td class="line x" title="79:250	Semantic association feature in s,t is a hidden random variable that is inferred from data." ></td>
	<td class="line x" title="80:250	In the domain adaptation, we transfer the problem of semantic association mapping to a posterior inference task using LaSA model." ></td>
	<td class="line x" title="81:250	Latent semantic concept association set of a word instance x (denoted by SA(x)) is generated by s,t. Instances in the same concept set are considered as behaving in the same way for discriminative learning in both domains." ></td>
	<td class="line x" title="82:250	Even though word instances do not appear in a training corpus (or appear rarely) but are in similar context, they still might have relatively high probability in the same semantic concept set." ></td>
	<td class="line x" title="83:250	Obviously, SA(x) can better bridge the gap between the two distributions ps(y|x) and pt(y|x)." ></td>
	<td class="line x" title="84:250	Hence, LaSA model can enhance the estimate of the source domain distribution ps(y|x;s,t) to better approximate the target domain distribution pt(y|x;s,t)." ></td>
	<td class="line x" title="85:250	4 Learning LaSA Model from Virtual Context Documents In the domain adaptation, LaSA model is employed to find the latent semantic association structures of words in a text corpus." ></td>
	<td class="line x" title="86:250	We will illustrate how to build LaSA model from words and their context snippets in this section." ></td>
	<td class="line x" title="87:250	LaSA model actually can be considered as a general probabilistic topic model." ></td>
	<td class="line x" title="88:250	It can be learned on the unlabeled corpus using the popular hidden topic models such as LDA or pLSI." ></td>
	<td class="line x" title="89:250	4.1 Virtual Context Document The distribution of content words (e.g. nouns, adjectives) is usually varied with domains." ></td>
	<td class="line x" title="90:250	Hence, in the domain adaptation, we focus on capturing the latent semantic association among content words." ></td>
	<td class="line x" title="91:250	In order to learn latent relationships among words from the unlabeled corpus, each content word is characterized by a virtual context document as follows." ></td>
	<td class="line x" title="92:250	Given a content word xi, the virtual context document of xi (denoted by vdxi) consists of all the context units around xi in the corpus." ></td>
	<td class="line x" title="93:250	Let n be the total number of the sentences which contain xi in the corpus." ></td>
	<td class="line x" title="94:250	vdxi is constructed as follows." ></td>
	<td class="line x" title="95:250	vdxi = {F(xs1i ),,F(xski ),,F(xsni )} where, F(xski ) denotes the context feature set of xi in the sentence sk, 1  k  n. Given the context window size {-t, t} (i.e. previous t words and next t words around xi in sk)." ></td>
	<td class="line x" title="96:250	F(xski ) usually consists of the following features." ></td>
	<td class="line x" title="97:250	1." ></td>
	<td class="line x" title="98:250	Anchor unit AxiC : the current focused word unit xi." ></td>
	<td class="line x" title="99:250	2." ></td>
	<td class="line x" title="100:250	Left adjacent unit AxiL : The nearest left adjacent unit xi1 around xi, denoted by AL(xi1)." ></td>
	<td class="line x" title="101:250	3." ></td>
	<td class="line x" title="102:250	Right adjacent unit AxiR : The nearest right adjacent unit xi+1 around xi, denoted by AR(xi+1)." ></td>
	<td class="line x" title="103:250	4." ></td>
	<td class="line x" title="104:250	Left context set CxiL : the other left adjacent units {xit, , xij, , xi2} (2 j t) around xi, denoted by {CL(xit), , CL(xij), , CL(xi2)}." ></td>
	<td class="line x" title="105:250	5." ></td>
	<td class="line x" title="106:250	Right context set CxiR : the other right adjacent units {xi+2, , xi+j, , xi+t} (2 j  t ) around xi, denoted by {CR(xi+2), , CR(xi+j), , CR(xi+t)}." ></td>
	<td class="line x" title="107:250	For example, given xi=singer, sk=This popular new singer attended the new year party." ></td>
	<td class="line x" title="108:250	Let the context window size be {-3,3}." ></td>
	<td class="line x" title="109:250	F(singer) = {singer, AL(new), AR(attend(ed)), CL(this), CL(popular), CR(the), CR(new) }." ></td>
	<td class="line x" title="110:250	vdxi actually describes the semantic and syntactic feature distribution of xi in the domains." ></td>
	<td class="line x" title="111:250	We construct the feature vector of xi with all the observed context features in vdxi." ></td>
	<td class="line x" title="112:250	Given vdxi = 283 {f1,,fj,,fm}, fj denotes jth context feature around xi, 1  j  m, m denotes the total number of features in vdxi." ></td>
	<td class="line x" title="113:250	The value of fj is calculated by Mutual Information (Church and Hanks, 1990) between xi and fj." ></td>
	<td class="line x" title="114:250	Weight(fj,xi) = log2 P(fj,xi)P(f j)P(xi) (1) where, P(fj,xi) is the joint probability of xi and fj co-occurred in the corpus, P(fj) is the probability of fj occurred in the corpus." ></td>
	<td class="line x" title="115:250	P(xi) is the probability of xi occurred in the corpus." ></td>
	<td class="line x" title="116:250	4.2 Learning LaSA Model Topic models are statistical models of text that posit a hidden space of topics in which the corpus is embedded (Blei et al., 2003)." ></td>
	<td class="line x" title="117:250	LDA (Blei et al., 2003) is a probabilistic model that can be used to model and discover underlying topic structures of documents." ></td>
	<td class="line x" title="118:250	LDA assumes that there are K topics, multinomial distributions over words, which describes a collection." ></td>
	<td class="line x" title="119:250	Each document exhibits multiple topics, and each word in each document is associated with one of them." ></td>
	<td class="line x" title="120:250	LDA imposes a Dirichlet distribution on the topic mixture weights corresponding to the documents in the corpus." ></td>
	<td class="line x" title="121:250	The topics derived by LDA seem to possess semantic coherence." ></td>
	<td class="line x" title="122:250	Those words with similar semantics are likely to occur in the same topic." ></td>
	<td class="line x" title="123:250	Since the number of LDA model parameters depends only on the number of topic mixtures and vocabulary size, LDA is less prone to over-fitting and is capable of estimating the probability of unobserved test documents." ></td>
	<td class="line x" title="124:250	LDA is already successfully applied to enhance document representations in text classification (Blei et al., 2003), information retrieval (Wei and Croft., 2006)." ></td>
	<td class="line x" title="125:250	In the following, we illustrate how to construct LDA-style LaSA model s,t on the virtual context documents." ></td>
	<td class="line x" title="126:250	Algorithm 1 describes LaSA model training method in detail, where, Function AddTo(data,Set) denotes that data is added to Set." ></td>
	<td class="line x" title="127:250	Given a large-scale unlabeled data set Du which consists of the source and target domain data, virtual context document for each candidate content word is extracted from Du at first, then the value of each feature in a virtual context document is calculated using its Mutual Information ( see Equation 1 in Section 4.1) instead of the counts when running Algorithm 1: LaSA Model Training Inputs:1  Unlabeled data set: Du;2 Outputs:3 LaSA model: s,t;4 Initialization:5  Virtual context document set: VDs,t = ;6  Candidate content word set: Xs,t = ;7 Steps:8 begin9 foreach content word xi Du do10 if Frequency(xi) the predefined threshold then11 AddTo(xi,Xs,t);12 foreach xk Xs,t do13 foreach sentence Si Du do14 if xk Si then15 F(xSik ) 16 {xk,AxkL ,AxkR ,CxkL ,CxkR }; AddTo(F(xSik ),vdxk); AddTo(vdxk,VDs,t);17  Generate LaSA model s,t with Dirichlet distribution on VDs,t.18 end19 LDA." ></td>
	<td class="line x" title="128:250	LaSA model s,t with Dirichlet distribution is generated on the virtual context document set VDs,t using the algorithm presented by Blei et al (2003)." ></td>
	<td class="line x" title="129:250	1 2 3 4 5 customer theater company Beijing music president showplace government Hongkong film singer courtyard university China arts manager center community Japan concert economist city team Singapore party policeman gymnasium enterprise New York Ballet reporter airport bank Vienna dance director square market America song consumer park organization Korea band dancer building agency international opera Table 1: Top 10 nouns from 5 randomly selected topics computed on the economics and entertainment domains LaSA model learns the posterior distribution to decompose words and their corresponding virtual context documents into topics." ></td>
	<td class="line x" title="130:250	Table 1 lists top 10 nouns from a random selection of 5 topics computed on the unlabeled economics and entertainment domain data." ></td>
	<td class="line x" title="131:250	As shown, words in the same topic are representative nouns." ></td>
	<td class="line x" title="132:250	They actually are grouped into broad concept sets." ></td>
	<td class="line x" title="133:250	For example, set 1, 3 and 4 correspond to nominal person, nominal organization and location, respectively." ></td>
	<td class="line x" title="134:250	With a large-scale unlabeled corpus, we will have enough words assigned to each topic concept to better approximate the underlying semantic association distribution." ></td>
	<td class="line x" title="135:250	In LDA-style LaSA model, the topic mixture is drawn from a conjugate Dirichlet prior that remains the same for all the virtual context docu284 ments." ></td>
	<td class="line x" title="136:250	Hence, given a word xi in the corpus, we may perform posterior inference to determine the conditional distribution of the hidden topic feature variables associated with xi." ></td>
	<td class="line x" title="137:250	Latent semantic association set of xi (denoted by SA(xi)) is generated using Algorithm 2." ></td>
	<td class="line x" title="138:250	Here, Multinomial(s,t(vdxi)) refers to sample from the posterior distribution over topics given a virtual document vdxi." ></td>
	<td class="line x" title="139:250	In the domain adaptation, we do semantic association inference on the source domain training data using LaSA model at first, then the original source domain NER model is tuned on the source domain training data set by incorporating these generated semantic association features." ></td>
	<td class="line x" title="140:250	Algorithm 2: Generate Latent Semantic Association Set of Word xi Using K-topic LaSA Model Inputs:1 s,t: LaSA model with multinomial distribution;2 Dirichlet(): Dirichlet distribution with parameter ;3 xi: Content word;4 Outputs:5 SA(xi): Latent semantic association set of xi;6 Steps:7 begin8  Extract vdxi from the corpus.9  Draw topic weights s,t(vdxi) from Dirichlet();10  foreach fj in vdxi do11 draw a topic zj{ 1,,K} from Multinomial(s,t(vdxi));12 AddTo(zj,Topics(vdxi));13  Rank all the topics in Topics(vdxi);14 SA(xi) top n topics in Topics(vdxi);15 end16 LaSA model better models latent semantic association distribution in the source and the target domains." ></td>
	<td class="line x" title="141:250	By grouping words into concepts, we effectively overcome the data distribution difference of both domains." ></td>
	<td class="line x" title="142:250	Thus, we may reduce the number of parameters required to model the target domain data, and improve the quality of the estimated parameters in the domain transfer." ></td>
	<td class="line x" title="143:250	LaSA model extends the traditional bag-of-words topic models to context-dependence concept association model." ></td>
	<td class="line x" title="144:250	It has potential use for concept grouping." ></td>
	<td class="line x" title="145:250	5 Experiments We evaluate LaSA-based domain adaptation method on both English and Chinese corpus in this section." ></td>
	<td class="line x" title="146:250	In the experiments, we focus on recognizing person (PER), location (LOC) and organization (ORG) in the given four domains, including economics (Eco), entertainment (Ent), politics (Pol) and sports (Spo)." ></td>
	<td class="line x" title="147:250	5.1 Experimental setting In the NER domain adaptation, nouns and adjectives make a significant impact on the performance." ></td>
	<td class="line x" title="148:250	Thus, we focus on capturing latent semantic association for high-frequency nouns and adjectives (i.e. occurrence count  50 ) in the unlabeled corpus." ></td>
	<td class="line x" title="149:250	LaSA models for nouns and adjectives are learned from the unlabeled corpus using Algorithm 1 (see section 4.2), respectively." ></td>
	<td class="line x" title="150:250	Our empirical study shows that better adaptation is obtained with a 50-topic LaSA model." ></td>
	<td class="line x" title="151:250	Therefore, we set the number of topics N as 50, and define the context view window size as {3,3} (i.e. previous 3 words and next 3 words) in the LaSA model learning." ></td>
	<td class="line x" title="152:250	LaSA features for other irrespective words (e.g. token unit the) are assigned with a default topic value N+1." ></td>
	<td class="line x" title="153:250	All the basic NER models are trained on the domain-specific training data using RRM classifier (Guo et al., 2005)." ></td>
	<td class="line x" title="154:250	RRM is a generalization Winnow learning algorithm (Zhang et al., 2002)." ></td>
	<td class="line x" title="155:250	We set the context view window size as {-2,2} in NER." ></td>
	<td class="line x" title="156:250	Given a word instance x, we employ local linguistic features (e.g. word unit, part of speech) of x and its context units ( i.e. previous 2 words and next 2 words ) in NER." ></td>
	<td class="line x" title="157:250	All Chinese texts in the experiments are automatically segmented into words using HMM." ></td>
	<td class="line x" title="158:250	In LaSA-based domain adaptation, the semantic association features of each unit in the observation window{-2,2}are generated by LaSA model at first, then the basic source domain NER model is tuned on the original source domain training data set by incorporating the semantic association features." ></td>
	<td class="line x" title="159:250	For example, given the sentence This popular new singer attended the new year party, Figure 1 illustrates various features and views at the current word wi= singer in LaSA-based adaptation." ></td>
	<td class="line x" title="160:250	 Tagging  Position wi2 wi1 wi wi+1 wi+2 Word popular new singer attend the POS adj adj noun verb article SA SA(popular) SA(new) SA(singer) SA(attend) SA(the)  Tag ti2 ti1 ti Figure 1: Feature window in LaSA-based adaptation In the viewing window at the word singer (see Figure 1), each word unit around singer is codified with a set of primitive features (e.g. POS, SA, Tag), together with its relative position to singer." ></td>
	<td class="line x" title="161:250	285 Here, SA denotes semantic association feature set which is generated by LaSA model." ></td>
	<td class="line x" title="162:250	Tag denotes NE tags labeled in the data set." ></td>
	<td class="line x" title="163:250	Given the input vector constructed with the above features, RRM method is then applied to train linear weight vectors, one for each possible class-label." ></td>
	<td class="line x" title="164:250	In the decoding stage, the class with the maximum confidence is then selected for each token unit." ></td>
	<td class="line x" title="165:250	In our evaluation, only NEs with correct boundaries and correct class labels are considered as the correct recognition." ></td>
	<td class="line x" title="166:250	We use the standard Precision (P), Recall (R), and F-measure (F = 2PRP+R) to measure the performance of NER models." ></td>
	<td class="line x" title="167:250	5.2 Data We built large-scale English and Chinese annotated corpus." ></td>
	<td class="line x" title="168:250	English corpus are generated from wikipedia while Chinese corpus are selected from Chinese newspapers." ></td>
	<td class="line x" title="169:250	Moreover, test data do not overlap with training data and unlabeled data." ></td>
	<td class="line x" title="170:250	5.2.1 Generate English Annotated Corpus from Wikipedia Wikipedia provides a variety of data resources for NER and other NLP research (Richman and Schone, 2008)." ></td>
	<td class="line x" title="171:250	We generate all the annotated English corpus from wikipedia." ></td>
	<td class="line x" title="172:250	With the limitation of efforts, only PER NEs in the corpus are automatically tagged using an English person gazetteer." ></td>
	<td class="line x" title="173:250	We automatically extract an English Person gazetteer from wikipedia at first." ></td>
	<td class="line x" title="174:250	Then we select the articles from wikipedia and tag them using this gazetteer." ></td>
	<td class="line x" title="175:250	In order to build the English Person gazetteer from wikipdedia, we manually selected several key phrases, including births, deaths, surname, given names and human names at first." ></td>
	<td class="line x" title="176:250	For each article title of interest, we extracted the categories to which that entry was assigned." ></td>
	<td class="line x" title="177:250	The entry is considered as a person name if its related explicit category links contain any one of the key phrases, such as Category: human names." ></td>
	<td class="line x" title="178:250	We totally extracted 25,219 person name candidates from 204,882 wikipedia articles." ></td>
	<td class="line x" title="179:250	And we expanded this gazetteer by adding the other available common person names." ></td>
	<td class="line x" title="180:250	Finally, we obtained a large-scale gazetteer of 51,253 person names." ></td>
	<td class="line x" title="181:250	All the articles selected from wikipedia are further tagged using the above large-scale gazetteer." ></td>
	<td class="line x" title="182:250	Since human annotated set were not available, we held out more than 100,000 words of text from the automatically tagged corpus to as a test set in each domain." ></td>
	<td class="line x" title="183:250	Table 2 shows the data distribution of the training and test data sets." ></td>
	<td class="line x" title="184:250	Domains Training Data Set Test Data Set Size PERs Size PERs Pol 0.45M 9,383 0.23M 6,067 Eco 1.06M 21,023 0.34M 6,951 Spo 0.47M 17,727 0.20M 6,075 Ent 0.36M 12,821 0.15M 5,395 Table 2: English training and test data sets We also randomly select 17M unlabeled English data (see Table 3) from Wikipedia." ></td>
	<td class="line x" title="185:250	These unlabeled data are used to build the English LaSA model." ></td>
	<td class="line x" title="186:250	All Domain Pol Eco Spo Ent Data Size(M) 17.06 7.36 2.59 3.65 3.46 Table 3: Domain distribution in the unlabeled English data set 5.2.2 Chinese Data We built a large-scale high-quality Chinese NE annotated corpus." ></td>
	<td class="line x" title="187:250	All the data are news articles from several Chinese newspapers in 2001 and 2002." ></td>
	<td class="line x" title="188:250	All the NEs (i.e. PER, LOC and ORG ) in the corpus are manually tagged." ></td>
	<td class="line x" title="189:250	Cross-validation checking is employed to ensure the quality of the annotated corpus." ></td>
	<td class="line x" title="190:250	Domain Size NEs in the training data set (M) PER ORG LOC Total Pol 0.90 11,388 6,618 14,350 32,356 Eco 1.40 6,821 18,827 14,332 39,980 Spo 0.60 11,647 8,105 7,468 27,220 Ent 0.60 12,954 2,823 4,665 20,442 Domain Size NEs in the test data set (M) PER ORG LOC Total Pol 0.20 2,470 1,528 2,540 6,538 Eco 0.26 1,098 2,971 2,362 6,431 Spo 0.10 1,802 1,323 1,246 4,371 Ent 0.10 2,458 526 738 3,722 Table 4: Chinese training and test data sets All the domain-specific training and test data are selected from this annotated corpus according to the domain categories (see Table 4)." ></td>
	<td class="line x" title="191:250	8.46M unlabeled Chinese data (see Table 5) are randomly selected from this corpus to build the Chinese LaSA model." ></td>
	<td class="line x" title="192:250	5.3 Experimental Results All the experiments are conducted on the above large-scale English and Chinese corpus." ></td>
	<td class="line x" title="193:250	The overall performance enhancement of NER by LaSA-based 286 All Domain Pol Eco Spo Ent Data Size(M) 8.46 2.34 1.99 2.08 2.05 Table 5: Domain distribution in the unlabeled Chinese data set domain adaptation is evaluated at first." ></td>
	<td class="line x" title="194:250	Since the distribution of each NE type is different across domains, we also analyze the performance enhancement on each entity type by LaSA-based adaptation." ></td>
	<td class="line x" title="195:250	5.3.1 Performance Enhancement of NER by LaSA-based Domain Adaptation Table 6 and 7 show the experimental results for all pairs of domain adaptation on both English and Chinese corpus, respectively." ></td>
	<td class="line x" title="196:250	In the experiment, the basic source domain NER model Ms is learned from the specific domain training data set Ddom (see Table 2 and 4 in Section 5.2)." ></td>
	<td class="line x" title="197:250	Here, dom  {Eco,Ent,Pol,Spo}." ></td>
	<td class="line x" title="198:250	Findom denotes the top-line F-measure of Ms in the source trained domain dom." ></td>
	<td class="line x" title="199:250	When Ms is directly applied in a new target domain, its F-measure in this basic transfer is considered as baseline (denoted by FBase)." ></td>
	<td class="line x" title="200:250	FLaSA denotes F-measure of Ms achieved in the target domain with LaSA-based domain adaptation." ></td>
	<td class="line x" title="201:250	(F) = FLaSAFBase FBase , which denotes the relative F-measureenhancement by LaSA-based domain adaptation." ></td>
	<td class="line x" title="202:250	Source  Performance in the domain transfer Target FBase FLaSA (F) (loss) FTop EcoEnt 57.61% 59.22% +2.79% 17.87% FinEnt=66.62% PolEnt 57.5 % 59.83% +4.05% 25.55% FinEnt=66.62% SpoEnt 58.66% 62.46% +6.48% 47.74% FinEnt=66.62% EntEco 70.56 % 72.46% +2.69% 19.33% FinEco=80.39% PolEco 63.62% 68.1% +7.04% 26.71% FinEco=80.39% SpoEco 70.35% 72.85% +3.55% 24.90% FinEco=80.39% EcoPol 50.59% 52.7% +4.17% 15.81% FinPol=63.94% EntPol 56.12% 59.82% +6.59% 47.31% FinPol=63.94% SpoPol 60.22% 62.6% +3.95% 63.98% FinPol=63.94% EcoSpo 60.28% 61.21% +1.54% 9.93% FinSpo=69.65% EntSpo 60.28% 62.68% +3.98% 25.61% FinSpo=69.65% PolSpo 56.94% 60.48% +6.22% 27.85% FinSpo=69.65% Table 6: Experimental results on English corpus Experimental results on English and Chinese corpus indicate that the performance of Ms significantly degrades in each basic domain transfer without using LaSA model (see Table 6 and 7)." ></td>
	<td class="line x" title="203:250	For example, in the EcoEnt transfer on Chinese corpus (see Table 7), Fineco of Ms is 82.28% while FBase of Ms is 60.45% in the entertainment domain." ></td>
	<td class="line x" title="204:250	Fmeasure of Ms significantly degrades by 21.83 perSource  Performance in the domain transfer Target FBase FLaSA (F) (loss) FTop EcoEnt 60.45% 66.42% +9.88% 26.29% FinEnt=83.16% PolEnt 69.89% 73.07% +4.55% 23.96% FinEnt =83.16% SpoEnt 68.66% 70.89% +3.25% 15.38% FinEnt =83.16% EntEco 58.50% 61.35% + 4.87% 11.98% FinEco=82.28% PolEco 62.89% 64.93% +3.24% 10.52% FinEco=82.28% SpoEco 60.44% 63.20% + 4.57 % 12.64% FinEco=82.28% EcoPol 67.03% 70.90 % +5.77% 27.78% FinPol=80.96% EntPol 66.64 % 68.94 % +3.45% 16.06% FinPol=80.96% SpoPol 65.40% 67.20% +2.75% 11.57% FinPol=80.96% EcoSpo 67.20% 70.77% +5.31% 15.47% FinSpo=90.24% EntSpo 70.05% 72.20% +3.07% 10.64% FinSpo=90.24% PolSpo 70.99% 73.86% +4.04% 14.91% FinSpo=90.24% Table 7: Experimental results on Chinese corpus cent points in this basic transfer." ></td>
	<td class="line x" title="205:250	Significant performance degrading of Ms is observed in all the basic transfer." ></td>
	<td class="line x" title="206:250	It shows that the data distribution of both domains is very different in each possible transfer." ></td>
	<td class="line x" title="207:250	Experimental results on English corpus show that LaSA-based adaptation effectively enhances the performance in each domain transfer (see Table 6)." ></td>
	<td class="line x" title="208:250	For example, in the PolEco transfer, FBase is 63.62% while FLaSA achieves 68.10%." ></td>
	<td class="line x" title="209:250	Compared with FBase, LaSA-based method significantly enhances F-measure by 7.04%." ></td>
	<td class="line x" title="210:250	We perform t-tests on F-measure of all the comparison experiments on English corpus." ></td>
	<td class="line x" title="211:250	The p-value is 2.44E-06, which shows that the improvement is statistically significant." ></td>
	<td class="line x" title="212:250	Table 6 also gives the accuracy loss due to transfer in each domain adaptation on English corpus." ></td>
	<td class="line x" title="213:250	The accuracy loss is defined as loss = 1  FFin dom . And the relative reduction in error is defined as (loss)= |1  lossLaSAlossBase |." ></td>
	<td class="line x" title="214:250	Experimental results indicate that the relative reduction in error is above 9.93% with LaSA-based transfer in each test on English corpus." ></td>
	<td class="line x" title="215:250	LaSA model significantly decreases the accuracy loss by 29.38% in average." ></td>
	<td class="line x" title="216:250	Especially for SpoPol transfer, (loss) achieves 63.98% with LaSA-based adaptation." ></td>
	<td class="line x" title="217:250	All the above results show that LaSA-based adaptation significantly reduces the accuracy loss in the domain transfer for English NER without any labeled target domain samples." ></td>
	<td class="line x" title="218:250	Experimental results on Chinese corpus also show that LaSA-based adaptation effectively increases the accuracy in all the tests (see Table 7)." ></td>
	<td class="line x" title="219:250	For example, in the EcoEnt transfer, compared with FBase, LaSA-based adaptation significantly increases Fmeasure by 9.88%." ></td>
	<td class="line x" title="220:250	We also perform t-tests on F287 measure of 12 comparison experiments on Chinese corpus." ></td>
	<td class="line x" title="221:250	The p-value is 1.99E-06, which shows that the enhancement is statistically significant." ></td>
	<td class="line x" title="222:250	Moreover, the relative reduction in error is above 10% with LaSA-based method in each test." ></td>
	<td class="line x" title="223:250	LaSA model decreases the accuracy loss by 16.43% in average." ></td>
	<td class="line x" title="224:250	Especially for the EcoEnt transfer (see Table 7), (loss) achieves 26.29% with LaSA-based method." ></td>
	<td class="line x" title="225:250	All the above experimental results on English and Chinese corpus show that LaSA-based domain adaptation significantly decreases the accuracy loss in the transfer without any labeled target domain data." ></td>
	<td class="line x" title="226:250	Although automatically tagging introduced some errors in English source training data, the relative reduction in errors in English NER adaptation seems comparable to that one in Chinese NER adaptation." ></td>
	<td class="line x" title="227:250	5.3.2 Accuracy Enhancement for Each NE Type Recognition Our statistic data (Guo et al., 2006) show that the distribution of NE types varies with domains." ></td>
	<td class="line x" title="228:250	Each NE type has different domain features." ></td>
	<td class="line x" title="229:250	Thus, the performance stability of each NE type recognition is very important in the domain transfer." ></td>
	<td class="line x" title="230:250	Figure 2 gives F-measure of each NE type recognition achieved by LaSA-based adaptation on English and Chinese corpus." ></td>
	<td class="line x" title="231:250	Experimental results show that LaSA-based adaptation effectively increases the accuracy of each NE type recognition in the most of the domain transfer tests." ></td>
	<td class="line x" title="232:250	We perform t-tests on F-measure of the comparison experiments on each NE type, respectively." ></td>
	<td class="line x" title="233:250	All the p-value is less than 0.01, which shows that the improvement on each NE type recognition is statistically significant." ></td>
	<td class="line x" title="234:250	Especially, the p-value of English and Chinese PER is 2.44E-06 and 9.43E-05, respectively, which shows that the improvement on PER recognition is very significant." ></td>
	<td class="line x" title="235:250	For example, in the EcoPol transfer on Chinese corpus, compared with FBase, LaSA-based adaptation enhances F-measure of PER recognition by 9.53 percent points." ></td>
	<td class="line x" title="236:250	Performance enhancement for ORG recognition is less than that one for PER and LOC recognition using LaSA model since ORG NEs usually contain much more domainspecific information than PER and LOC." ></td>
	<td class="line x" title="237:250	The major reason for error reduction is that external context and internal units are better semantically associated using LaSA model." ></td>
	<td class="line x" title="238:250	For example, LaSA Figure 2: PER, LOC and ORG recognition in the transfer model better groups various titles from different domains (see Table 1 in Section 4.2)." ></td>
	<td class="line x" title="239:250	Various industry terms in ORG NEs are also grouped into the semantic sets." ></td>
	<td class="line x" title="240:250	These semantic associations provide useful hints for detecting the boundary of NEs in the new target domain." ></td>
	<td class="line x" title="241:250	All the above results show that LaSA model better compensates for the feature distribution difference of each NE type across domains." ></td>
	<td class="line x" title="242:250	6 Conclusion We present a domain adaptation method with LaSA model in this paper." ></td>
	<td class="line x" title="243:250	LaSA model captures latent semantic association among words from the unlabeled corpus." ></td>
	<td class="line x" title="244:250	It better groups words into a set of concepts according to the related context snippets." ></td>
	<td class="line x" title="245:250	LaSAbased domain adaptation method projects words to a low-dimension concept feature space in the transfer." ></td>
	<td class="line x" title="246:250	It effectively overcomes the data distribution gap across domains without using any labeled target domain data." ></td>
	<td class="line x" title="247:250	Experimental results on English and Chinese corpus show that LaSA-based domain adaptation significantly enhances the performance of NER across domains." ></td>
	<td class="line x" title="248:250	Especially, LaSA model effectively increases the accuracy of each NE type recognition in the domain transfer." ></td>
	<td class="line x" title="249:250	Moreover, LaSA-based domain adaptation method works well across languages." ></td>
	<td class="line x" title="250:250	To further reduce the accuracy loss, we will explore informative sampling to capture fine-grained data difference in the domain transfer." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-1068
Hierarchical Bayesian Domain Adaptation
Finkel, Jenny Rose;Manning, Christopher D.;"></td>
	<td class="line x" title="1:220	Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 602610, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:220	c 2009 Association for Computational Linguistics Hierarchical Bayesian Domain Adaptation Jenny Rose Finkel and Christopher D. Manning Computer Science Department Stanford University Stanford, CA 94305 {jrfinkel|manning}@cs.stanford.edu Abstract Multi-task learning is the problem of maximizing the performance of a system across a number of related tasks." ></td>
	<td class="line x" title="3:220	When applied to multiple domains for the same task, it is similar to domain adaptation, but symmetric, rather than limited to improving performance on a target domain." ></td>
	<td class="line x" title="4:220	We present a more principled, better performing model for this problem, based on the use of a hierarchical Bayesian prior." ></td>
	<td class="line x" title="5:220	Each domain has its own domain-specific parameter for each feature but, rather than a constant prior over these parameters, the model instead links them via a hierarchical Bayesian global prior." ></td>
	<td class="line x" title="6:220	This prior encourages the features to have similar weights across domains, unless there is good evidence to the contrary." ></td>
	<td class="line oc" title="7:220	We show that the method of (Daume III, 2007), which was presented as a simple preprocessing step, is actually equivalent, except our representation explicitly separates hyperparameters which were tied in his work." ></td>
	<td class="line nc" title="8:220	We demonstrate that allowing different values for these hyperparameters significantly improves performance over both a strong baseline and (Daume III, 2007) within both a conditional random field sequence model for named entity recognition and a discriminatively trained dependency parser." ></td>
	<td class="line x" title="9:220	1 Introduction The goal of multi-task learning is to improve performance on a set of related tasks, when provided with (potentially varying quantities of) annotated data for each of the tasks." ></td>
	<td class="line x" title="10:220	It is very closely related to domain adaptation, a far more common task in the natural language processing community, but with two primary differences." ></td>
	<td class="line x" title="11:220	Firstly, in domain adaptation the different tasks are actually just different domains." ></td>
	<td class="line x" title="12:220	Secondly, in multi-task learning the focus is on improving performance across all tasks, while in domain adaptation there is a distinction between source data and target data, and the goal is to improve performance on the target data." ></td>
	<td class="line x" title="13:220	In the present work we focus on domain adaptation, but like the multi-task setting, we wish to improve performance across all domains and not a single target domains." ></td>
	<td class="line x" title="14:220	The word domain is used here somewhat loosely: it may refer to a topical domain or to distinctions that linguists might term mode (speech versus writing) or register (formal written prose versus SMS communications)." ></td>
	<td class="line x" title="15:220	For example, one may have a large amount of parsed newswire, and want to use it to augment a much smaller amount of parsed e-mail, to build a higher quality parser for e-mail data." ></td>
	<td class="line x" title="16:220	We also consider the extension to the task where the annotation is not the same, but is consistent, across domains (that is, some domains may be annotated with more information than others)." ></td>
	<td class="line x" title="17:220	This problem is important because it is omnipresent in real life natural language processing tasks." ></td>
	<td class="line x" title="18:220	Annotated data is expensive to produce and limited in quantity." ></td>
	<td class="line x" title="19:220	Typically, one may begin with a considerable amount of annotated newswire data, some annotated speech data, and a little annotated e-mail data." ></td>
	<td class="line x" title="20:220	It would be most desirable if the aggregated training data could be used to improve the performance of a system on each of these domains." ></td>
	<td class="line x" title="21:220	From the baseline of building separate systems for each domain, the obvious first attempt at domain adaptation is to build a system from the union of the training data, and we will refer to this as a second baseline." ></td>
	<td class="line x" title="22:220	In this paper we propose a more principled, formal model of domain adaptation, which not only outperforms previous work, but maintains attractive 602 performance characteristics in terms of training and testing speed." ></td>
	<td class="line oc" title="23:220	We also show that the domain adaptation work of (Daume III, 2007), which is presented as an ad-hoc preprocessing step, is actually equivalent to our formal model." ></td>
	<td class="line oc" title="24:220	However, our representation of the model conceptually separates some of the hyperparameters which are not separated in (Daume III, 2007), and we found that setting these hyperparameters with different values from one another was critical for improving performance." ></td>
	<td class="line x" title="25:220	We apply our model to two tasks, named entity recognition, using a linear chain conditional random field (CRF), and dependency parsing, using a discriminative, chart-based model." ></td>
	<td class="line x" title="26:220	In both cases, we find that our model improves performance over both baselines and prior work." ></td>
	<td class="line x" title="27:220	2 Hierarchical Bayesian Domain Adaptation 2.1 Motivation We call our model hierarchical Bayesian domain adaptation, because it makes use of a hierarchical Bayesian prior." ></td>
	<td class="line x" title="28:220	As an example, take the case of building a logistic classifier to decide if a word is part of a persons name." ></td>
	<td class="line x" title="29:220	There will be a parameter (weight) for each feature, and usually there is a zero-mean Gaussian prior over the parameter values so that they dont get too large.1 In the standard, single-domain, case the log likelihood of the data and prior is calculated, and the optimal parameter values are found." ></td>
	<td class="line x" title="30:220	Now, lets extend this model to the case of two domains, one containing American newswire and the other containing British newswire." ></td>
	<td class="line x" title="31:220	The data distributions will be similar for the two domains, but not identical." ></td>
	<td class="line x" title="32:220	In our model, we have separate parameters for each feature in each domain." ></td>
	<td class="line x" title="33:220	We also have a top level parameter (also to be learned) for each feature." ></td>
	<td class="line x" title="34:220	For each domain, the Gaussian prior over the parameter values is now centered around these top level parameters instead of around zero." ></td>
	<td class="line x" title="35:220	A zero-mean Gaussian prior is then placed over the top level parameters." ></td>
	<td class="line x" title="36:220	In this example, if some feature, say word=Nigel, only appears in the British newswire, the corresponding weight for the American newswire will have a similar value." ></td>
	<td class="line x" title="37:220	This happens because the evidence in the British domain will push the British parameter 1This can be regarded as a Bayesian prior or as weight regularization; we adopt the former perspective here." ></td>
	<td class="line x" title="38:220	to have a high value, and this will in turn influence the top-level parameter to have a high value, which will then influence the American newswire to have a high value, because there will be no evidence in the American data to override the prior." ></td>
	<td class="line x" title="39:220	Conversely, if some feature is highly indicative of isName=true for the British newswire, and of isName=false for the American newswire, then the British parameter will have a high (positive) value while the American parameter will have a low (negative) value, because in both cases the domain-specific evidence will outweigh the effect of the prior." ></td>
	<td class="line x" title="40:220	2.2 Formal Model Our domain adaptation model is based on a hierarchical Bayesian prior, through which the domainspecific parameters are tied." ></td>
	<td class="line x" title="41:220	The model is very general-purpose, and can be applied to any discriminative learning task for which one would typically put a prior with a mean over the parameters." ></td>
	<td class="line x" title="42:220	We will build up to it by first describing a general, singledomain, discriminative learning task, and then we will show how to modify this model to construct our hierarchical Bayesian domain adaptation model." ></td>
	<td class="line x" title="43:220	In a typical discriminative probabilistic model, the learning process consists of optimizing the log conditional likelihood of the data with respect to the parameters, Lorig(D;)." ></td>
	<td class="line x" title="44:220	This likelihood function can take on many forms: logistic regression, a conditional Markov model, a conditional random field, as well as others." ></td>
	<td class="line x" title="45:220	It is common practice to put a zeromean Gaussian prior over the parameters, leading to the following objective, for which we wish to find the optimal parameter values: argmax  parenleftBigg Lorig(D;) i 2i 2 2 parenrightBigg (1) From a graphical models perspective, this looks like Figure 1(a), where  is the mean for the prior (in our case, zero),  2 is the variance for the prior,  are the parameters, or feature weights, and D is the data." ></td>
	<td class="line x" title="46:220	Now we will extend this single-domain model into a multi-domain model (illustrated in Figure 1(b))." ></td>
	<td class="line x" title="47:220	Each feature weight i is replicated once for each domain, as well as for a top-level set of parameters." ></td>
	<td class="line x" title="48:220	We will refer to the parameters for domain d as d, with individual components d,i, the toplevel parameters as , and all parameters collectively as ." ></td>
	<td class="line x" title="49:220	All of the power of our model stems from the relationship between these sets of param603    D N    d d Dd N M    txt txt sp sp d d d d Dd Dd (a) (b) (c) Figure 1: (a) No domain adaptation." ></td>
	<td class="line x" title="50:220	The model parameters, , are normally distributed, with mean  (typically zero) and variance 2." ></td>
	<td class="line x" title="51:220	The likelihood of the data,D, is dependent on the model parameters." ></td>
	<td class="line x" title="52:220	The form of the data distribution depends on the underlying model (e.g., logistic regression, or a CRF)." ></td>
	<td class="line x" title="53:220	(b) Our hierarchical domain adaptation model." ></td>
	<td class="line x" title="54:220	The top-level parameters, , are normally distributed, with mean  (typically zero) and variance 2 . There is a plate for each domain." ></td>
	<td class="line x" title="55:220	Within each plate, the domain-specific parameters, d are normally distributed, with mean  and variance 2d ." ></td>
	<td class="line x" title="56:220	(c) Our hierarchical domain adaptation model, with an extra level of structure." ></td>
	<td class="line x" title="57:220	In this example, the domains are further split into text and speech super-domains, each of which has its own set of parameters (txt and txt for text and sp and sp for speech)." ></td>
	<td class="line x" title="58:220	d is normally distributed with mean txt if domain d is in the text super-domain, and sp if it is in the speech super-domain." ></td>
	<td class="line x" title="59:220	eters." ></td>
	<td class="line x" title="60:220	First, we place a zero-mean Gaussian prior over the top level parameters ." ></td>
	<td class="line x" title="61:220	Then, these top level parameters are used as the mean for a Gaussian prior placed over each of the domain-specific parameters d. These domain-specific parameters are then the parameters used in the original conditional log likelihood functions for each domain." ></td>
	<td class="line x" title="62:220	The domainspecific parameter values jointly influence an appropriate value for the higher-level parameters." ></td>
	<td class="line x" title="63:220	Conversely, the higher-level parameters will largely determine the domain-specific parameters when there is little or no evidence from within a domain, but can be overriden by domain-specific evidence when it clearly goes against the general picture (for instance Leeds is normally a location, but within the sports domain is usually an organization (football team))." ></td>
	<td class="line x" title="64:220	The beauty of this model is that the degree of influence each domain exerts over the others, for each parameter, is based on the amount of evidence each domain has about that parameter." ></td>
	<td class="line x" title="65:220	If a domain has a lot of evidence for a feature weight, then that evidence will outweigh the effect of the prior." ></td>
	<td class="line x" title="66:220	However, when a domain lacks evidence for a parameter the opposite occurs, and the prior (whose value is determined by evidence in the other domains) will have a greater effect on the parameter value." ></td>
	<td class="line x" title="67:220	To achieve this, we modify the objective function." ></td>
	<td class="line x" title="68:220	We now sum over the log likelihood for all domains, including a Gaussian prior for each domain, but which is now centered around , the top-level parameters." ></td>
	<td class="line x" title="69:220	Outside of this summation, we have a Gaussian prior over the top-level parameters which is identical to the prior in the original model: Lhier(D;) = (2)  d parenleftBigg Lorig(Dd;d) i (d,i ,i)2 2 2d parenrightBigg  i (,i)2 2 2 where  2d and  2 are variances on the priors over the parameters for all the domains, as well as the top-level parameters." ></td>
	<td class="line x" title="70:220	The graphical models representation is shown in Figure 1(b)." ></td>
	<td class="line x" title="71:220	One potential source of confusion is with respect to the directed or undirected nature of our domain adaptation model, and the underlying model of the data." ></td>
	<td class="line x" title="72:220	Our hierarchical Bayesian domain adaptation model is directed, as illustrated in Figure 1." ></td>
	<td class="line x" title="73:220	However, somewhat counterintuitively, the underlying (original) model of the data can be either directed or undirected, and for our experiments we use undi604 rected, conditional random field-based models." ></td>
	<td class="line x" title="74:220	The directed domain adaptation model can be viewed as a model of the parameters, and those parameter weights are used by the underlying data model." ></td>
	<td class="line x" title="75:220	In Figure 1, the entire data model is represented by a single node, D, conditioned on the parameters,  or d. The form of that model can then be almost anything, including an undirected model." ></td>
	<td class="line x" title="76:220	From an implementation perspective, the objective function is not much more difficult to implement than the original single-domain model." ></td>
	<td class="line x" title="77:220	For all of our experiments, we optimized the log likelihood using L-BFGS, which requires the function value and partial derivatives of each parameter." ></td>
	<td class="line x" title="78:220	The new partial derivatives for the domain-specific parameters (but not the top-level parameters) utilize the same partial derivatives as in the original model." ></td>
	<td class="line x" title="79:220	The only change in the calculations is with respect to the priors." ></td>
	<td class="line x" title="80:220	The partial derivatives for the domain-specific parameters are: Lhier(D;) d,i = Ld(Dd,d) d,i  d,i ,i  2d (3) and the derivatives for the top level parameters  are: Lhier(D;) ,i = parenleftBigg  d ,i d,i  2d parenrightBigg  ,i 2  (4) This function is convex." ></td>
	<td class="line x" title="81:220	Once the optimal parameters have been learned, the top level parameters can be discarded, since the runtime model for each domain is the same as the original (single-domain) model, parameterized by the parameters learned for that domain in the hierarchical model." ></td>
	<td class="line x" title="82:220	However, it may be useful to retain the top-level parameters for use in adaptation to further domains in the future." ></td>
	<td class="line x" title="83:220	In our model there are d extra hyper-parameters which can be tuned." ></td>
	<td class="line x" title="84:220	These are the variances  2d for each domain." ></td>
	<td class="line x" title="85:220	When this value is large then the prior has little influence, and when set high enough will be equivalent to training each model separately." ></td>
	<td class="line x" title="86:220	When this value is close to zero the prior has a strong influence, and when it is sufficiently close to zero then it will be equivalent to completely tying the parameters, such that d1,i = d2,i for all domains." ></td>
	<td class="line x" title="87:220	Despite having many more parameters, for both of the tasks on which we performed experiments, we found that our model did not take much more time to train that a baseline model trained on all of the data concatenated together." ></td>
	<td class="line x" title="88:220	2.3 Model Generalization The model as presented thus far can be viewed as a two level tree, with the top-level parameters at the root, and the domain-specific ones at the leaves." ></td>
	<td class="line x" title="89:220	However, it is straightforward to generalize the model to any tree structure." ></td>
	<td class="line x" title="90:220	In the generalized version, the domain-specific parameters would still be at the leaves, the top-level parameters at the root, but new mid-level parameters can be added based on beliefs about how similar the various domains are." ></td>
	<td class="line x" title="91:220	For instance, if one had four datasets, two of which contained speech data and two of which contained newswire, then it might be sensible to have two sets of mid-level parameters, one for the speech data and one for the newswire data, as illustrated in Figure 1(c)." ></td>
	<td class="line x" title="92:220	This would allow the speech domains to influence one another more than the newswire domains, and vice versa." ></td>
	<td class="line oc" title="93:220	2.4 Formalization of (Daume III, 2007) As mentioned earlier, our model is equivalent to that presented in (Daume III, 2007), and can be viewed as a formal version of his model.2 In his presentation, the adapation is done through feature augmentation." ></td>
	<td class="line x" title="94:220	Specifically, for each feature in the original version, a new version is created for each domain, as well as a general, domain-independent version of the feature." ></td>
	<td class="line x" title="95:220	For each datum, two versions of each original feature are present: the version for that datums domain, and the domain independent one." ></td>
	<td class="line x" title="96:220	The equivalence between the two models can be shown with simple arithmetic." ></td>
	<td class="line oc" title="97:220	Recall that the log likelihood of our model is:  d parenleftBigg Lorig(Dd;d) i (d,i ,i)2 2 2d parenrightBigg  i (,i)2 2 2 We now introduce a new variable d = d , and plug it into the equation for log likelihood:  d parenleftBigg Lorig(Dd;d +) i (d,i)2 2 2d parenrightBigg  i (,i)2 2 2 The result is the model of (Daume III, 2007), where the d are the domain-specific feature weights, and d are the domain-independent feature weights." ></td>
	<td class="line o" title="98:220	In his formulation, the variances  2d =  2 for all domains d. This separation of the domain-specific and independent variances was critical to our improved performance." ></td>
	<td class="line x" title="99:220	When using a Gaussian prior there are 2Many thanks to David Vickrey for pointing this out to us." ></td>
	<td class="line x" title="100:220	605 two parameters set by the user: the mean,  (usually zero), and the variance,  2." ></td>
	<td class="line x" title="101:220	Technically, each of these parameters is actually a vector, with an entry for each feature, but almost always the vectors are uniform and the same parameter is used for each feature (there are exceptions, e.g.(Lee et al., 2007))." ></td>
	<td class="line oc" title="103:220	Because Daume III (2007) views the adaptation as merely augmenting the feature space, each of his features has the same prior mean and variance, regardless of whether it is domain specific or independent." ></td>
	<td class="line o" title="104:220	He could have set these parameters differently, but he did not.3 In our presentation of the model, we explicitly represent different variances for each domain, as well as the top level parameters." ></td>
	<td class="line x" title="105:220	We found that specifying different values for the domain specific versus domain independent variances significantly improved performance, though we found no gains from using different values for the different domain specific variances." ></td>
	<td class="line x" title="106:220	The values were set based on development data." ></td>
	<td class="line x" title="107:220	3 Named Entity Recognition For our first set of experiments, we used a linearchain, conditional random field (CRF) model, trained for named entity recognition (NER)." ></td>
	<td class="line x" title="108:220	The use of CRFs for sequence modeling has become standard so we will omit the model details; good explanations can be found in a number of places (Lafferty et al., 2001; Sutton and McCallum, 2007)." ></td>
	<td class="line x" title="109:220	Our features were based on those in (Finkel et al., 2005)." ></td>
	<td class="line x" title="110:220	3.1 Data We used three named entity datasets, from the CoNLL 2003, MUC-6 and MUC-7 shared tasks." ></td>
	<td class="line x" title="111:220	CoNLL is British newswire, while MUC-6 and MUC-7 are both American newswire." ></td>
	<td class="line x" title="112:220	Arguably MUC-6 and MUC-7 should not count as separate domains, but because they were annotated separately, for different shared tasks, we chose to treat them as such, and feel that our experimental results justify the distinction." ></td>
	<td class="line x" title="113:220	We used the standard train and test sets for each domain, which for CoNLL corresponds to the (more difficult) testb set." ></td>
	<td class="line x" title="114:220	For details about the number of training and test words in each dataset, please see Table 1." ></td>
	<td class="line x" title="115:220	One interesting challenge in dealing with both CoNLL and MUC data is that the label sets differ." ></td>
	<td class="line x" title="116:220	3Although he alludes to the potential for something similar in the last section of his paper, when discussing the kernelization interpretation of his approach." ></td>
	<td class="line x" title="117:220	# Train # Test Words Words MUC-6 165,082 15,032 MUC-7 89,644 64,490 CoNLL 203,261 46,435 Table 1: Number of words in the training and test sets for each of the named entity recognition datasets." ></td>
	<td class="line x" title="118:220	CoNLL has four classes: person, organization, location, and misc." ></td>
	<td class="line x" title="119:220	MUC data has seven classes: person, organization, location, percent, date, time, and money." ></td>
	<td class="line x" title="120:220	They overlap in the three core classes (person, organization, and location), but CoNLL has one additional class and MUC has four additional classes." ></td>
	<td class="line x" title="121:220	The differences in the label sets led us to perform two sets of experiments for the baseline and hierarchical Bayesian models." ></td>
	<td class="line x" title="122:220	In the first set of experiments, at training time, the model allows any label from the union of the label sets, regardless of whether that label was legal for the domain." ></td>
	<td class="line x" title="123:220	At test time, we would ignore guesses made by the model which were inconsistent with the allowed labels for that domain.4 In the second set of experiments, we restricted the model at training time to only allow legal labels for each domain." ></td>
	<td class="line x" title="124:220	At test time, the domain was specified, and the model was once again restricted so that words would never be tagged with a label outside of that domains label set." ></td>
	<td class="line x" title="125:220	3.2 Experimental Results and Discussion In our experiments, we compared our model to several strong baselines, and the full set of results is in Table 2." ></td>
	<td class="line x" title="126:220	The models we used were: TARGET ONLY." ></td>
	<td class="line x" title="127:220	Trained and tested on only the data for that domain." ></td>
	<td class="line x" title="128:220	ALL DATA." ></td>
	<td class="line x" title="129:220	Trained and tested on data from all domains, concatenated into one large dataset." ></td>
	<td class="line x" title="130:220	ALL DATA*." ></td>
	<td class="line x" title="131:220	Same as ALL DATA, but restricted possible labels for each word based on domain." ></td>
	<td class="line x" title="132:220	DAUME07." ></td>
	<td class="line oc" title="133:220	Trained and tested using the same technique as (Daume III, 2007)." ></td>
	<td class="line x" title="134:220	We note that they present results using per-token label accuracy, while we used the more standard entity precision, recall, and F score (as in the CoNLL 2003 shared task)." ></td>
	<td class="line x" title="135:220	4We treated them identically to the background symbol." ></td>
	<td class="line x" title="136:220	So, for instance, labelling a word a date in the CoNLL data had no effect on the score." ></td>
	<td class="line x" title="137:220	606 Named Entity Recognition Model Precision Recall F1 MUC-6 TARGET ONLY 86.74 80.10 83.29 ALL DATA* 85.04 83.49 84.26 ALL DATA 86.00 82.71 84.32 DAUME07* 87.83 83.41 85.56 DAUME07 87.81 82.23 85.46 HIER BAYES* 88.59 84.97 86.74 HIER BAYES 88.77 85.14 86.92 MUC-7 TARGET ONLY 81.17 70.23 75.30 ALL DATA* 81.66 76.17 78.82 ALL DATA 82.20 70.91 76.14 DAUME07* 83.33 75.42 79.18 DAUME07 83.51 75.63 79.37 HIER BAYES* 82.90 76.95 79.82 HIER BAYES 83.17 77.02 79.98 CoNLL TARGET ONLY 85.55 84.72 85.13 ALL DATA* 86.34 84.45 85.38 ALL DATA 86.58 83.90 85.22 DAUME07* 86.09 85.06 85.57 DAUME07 86.35 85.26 85.80 HIER BAYES* 86.33 85.06 85.69 HIER BAYES 86.51 85.13 85.81 Table 2: Named entity recognition results for each of the models." ></td>
	<td class="line x" title="138:220	With the exception of the TARGET ONLY model, all three datasets were combined when training each of the models." ></td>
	<td class="line x" title="139:220	DAUME07*." ></td>
	<td class="line x" title="140:220	Same as DAUME07, but restricted possible labels for each word based on domain." ></td>
	<td class="line x" title="141:220	HIER BAYES." ></td>
	<td class="line x" title="142:220	Our hierarchical Bayesian domain adaptation model." ></td>
	<td class="line x" title="143:220	HIER BAYES*." ></td>
	<td class="line x" title="144:220	Same as HIER BAYES, but restricted possible labels for each word based on the domain." ></td>
	<td class="line x" title="145:220	For all of the baseline models, and for the top level-parameters in the hierarchical Bayesian model, we used  = 1." ></td>
	<td class="line x" title="146:220	For the domain-specific parameters, we used d = 0.1 for all domains." ></td>
	<td class="line x" title="147:220	The HIER BAYES model outperformed all baselines for both of the MUC datasets, and tied with the DAUME07 for CoNLL." ></td>
	<td class="line x" title="148:220	The largest improvement was on MUC-6, where HIER BAYES outperformed DAUME07*, the second best model, by 1.36%." ></td>
	<td class="line x" title="149:220	This improvement is greater than the improvement made by that model over the ALL DATA* baseline." ></td>
	<td class="line x" title="150:220	To assess significance we used a document-level paired t-test (over all of the data combined), and found that HIER BAYES significantly outperformed all of the baselines (not including HIER BAYES*) with greater than 95% confidence." ></td>
	<td class="line x" title="151:220	For both the HIER BAYES and DAUME07 models, we found that performance was better for the variant which did not restrict possible labels based on the domain, while the ALL DATA model did benefit from the label restriction." ></td>
	<td class="line x" title="152:220	For HIER BAYES and DAUME07, this result may be due to the structure of the models." ></td>
	<td class="line x" title="153:220	Because both models have domainspecific features, the models likely learned that these labels were never actually allowed." ></td>
	<td class="line x" title="154:220	However, when a feature does not occur in the data for a particular domain, then the domain-specific parameter for that feature will have positive weight due to evidence present in the other domains, which at test time can lead to assigning an illegal label to a word." ></td>
	<td class="line x" title="155:220	This information that a word may be of some other (unknown to that domain) entity type may help prevent the model from mislabeling the word." ></td>
	<td class="line x" title="156:220	For example, in CoNLL, nationalities, such as Iraqi and American, are labeled as misc." ></td>
	<td class="line x" title="157:220	If a previously unseen nationality is encountered in the MUC testing data, the MUC model may be tempted to label is as a location, but this evidence from the CoNLL data may prevent that, by causing it to instead be labeled misc, a label which will subsequently be ignored." ></td>
	<td class="line x" title="158:220	In typical domain adaptation work, showing gains is made easier by the fact that the amount of training data in the target domain is comparatively small." ></td>
	<td class="line x" title="159:220	Within the multi-task learning setting, it is more challenging to show gains over the ALL DATA baseline." ></td>
	<td class="line x" title="160:220	Nevertheless, our results show that, so long as the amount of data in each domain is not widely disparate, it is possible to achieve gains on all of the domains simultaneously." ></td>
	<td class="line x" title="161:220	4 Dependency Parsing 4.1 Parsing Model We also tested our model on an untyped dependency parsing task, to see how it performs on a more structurally complex task than sequence modeling." ></td>
	<td class="line x" title="162:220	To our knowledge, the discriminatively trained dependency model we used has not been previously published, but it is very similar to recent work on discriminative constituency parsing (Finkel and Manning, 2008)." ></td>
	<td class="line x" title="163:220	Due to space restrictions, we cannot give a complete treatment of the model, but will give an overview." ></td>
	<td class="line x" title="164:220	607 We built a CRF-based model, optimizing the likelihood of the parse, conditioned on the words and parts of speech of the sentence." ></td>
	<td class="line x" title="165:220	At the heart of our model is the Eisner dependency grammar chartparsing algorithm (Eisner, 1996), which allows for efficient computation of inside and outside scores." ></td>
	<td class="line x" title="166:220	The Eisner algorithm, originally designed for generative parsing, decomposes the probability of a dependency parse into the probabilities of each attachment of a dependent to its parent, and the probabilities of each parent stopping taking dependents." ></td>
	<td class="line x" title="167:220	These probabilities can be conditioned on the child, parent, and direction of the dependency." ></td>
	<td class="line x" title="168:220	We used a slight modification of the algorithm which allows each probability to also be conditioned on whether there is a previous dependent." ></td>
	<td class="line x" title="169:220	While the unmodified version of the algorithm includes stopping probabilities, conditioned on the parent and direction, they have no impact on which parse for a particular sentence is most likely, because all words must eventually stop taking dependents." ></td>
	<td class="line x" title="170:220	However, in the modified version, the stopping probability is also conditioned on whether or not there is a previous dependent, so this probability does make a difference." ></td>
	<td class="line x" title="171:220	While the Eisner algorithm computes locally normalized probabilities for each attachment decision, our model computes unnormalized scores." ></td>
	<td class="line x" title="172:220	From a graphical models perspective, our parsing model is undirected, while the original model is directed.5 The score for a particular tree decomposes the same way in our model as in the original Eisner model, but it is globally normalized instead of locally normalized." ></td>
	<td class="line x" title="173:220	Using the inside and outside scores we can compute partial derivatives for the feature weights, as well as the value of the normalizing constant needed to determine the probability of a particular parse." ></td>
	<td class="line x" title="174:220	This is done in a manner completely analogous to (Finkel and Manning, 2008)." ></td>
	<td class="line x" title="175:220	Partial derivatives and the function value are all that is needed to find the optimal feature weights using L-BFGS.6 Features are computed over each attachment and stopping decision, and can be conditioned on the 5The dependencies themselves are still directed in both cases, it is just the underlying graphical model used to compute the likelihood of a parse which changes from a directed model to an undirected model." ></td>
	<td class="line x" title="176:220	6In (Finkel and Manning, 2008) we used stochastic gradient descent to optimize our weights because our function evaluation was too slow to use L-BFGS." ></td>
	<td class="line x" title="177:220	We did not encounter this problem in this setting." ></td>
	<td class="line x" title="178:220	parent, dependent (or none, if it is a stopping decision), direction of attachment, whether there is a previous dependent in that direction, and the words and parts of speech of the sentence." ></td>
	<td class="line x" title="179:220	We used the same features as (McDonald et al., 2005), augmented with information about whether or not a dependent is the first dependent (information they did not have)." ></td>
	<td class="line x" title="180:220	4.2 Data For our dependency parsing experiments, we used LDC2008T04 OntoNotes Release 2.0 data (Hovy et al., 2006)." ></td>
	<td class="line x" title="181:220	This dataset is still in development, and includes data from seven different domains, labeled for a number of tasks, including PCFG trees." ></td>
	<td class="line x" title="182:220	The domains span both newswire and speech from multiple sources." ></td>
	<td class="line x" title="183:220	We converted the PCFG trees into dependency trees using the Collins head rules (Collins, 2003)." ></td>
	<td class="line x" title="184:220	We also omitted the WSJ portion of the data, because it follows a different annotation scheme from the other domains.7 For each of the remaining six domains, we aimed for an 75/25 data split, but because we divided the data using the provided sections, this split was fairly rough." ></td>
	<td class="line x" title="185:220	The number of training and test sentences for each domain are specified in the Table 3, along with our results." ></td>
	<td class="line x" title="186:220	4.3 Experimental Results and Discussion We compared the same four domain adaptation models for dependency parsing as we did for the named entity experiments, once again setting  = 1.0 and d = 0.1." ></td>
	<td class="line x" title="187:220	Unlike the named entity experiments however, there were no label set discrepencies between the domains, so only one version of each domain adaptation model was necessary, instead of the two versions in that section." ></td>
	<td class="line x" title="188:220	Our full dependency parsing results can be found in Table 3." ></td>
	<td class="line x" title="189:220	Firstly, we found that DAUME07, which had outperformed the ALL DATA baseline for the sequence modeling task, performed worse than the 7Specifically, all the other domains use the new Penn Treebank annotation style, whereas the WSJ data is still in the traditional annotation style, familiar from the past decades work in Penn Treebank parsing." ></td>
	<td class="line x" title="190:220	The major changes are in hyphenation and NP structure." ></td>
	<td class="line x" title="191:220	In the new annotation style, many hyphenated words are separated into multiple tokens, with a new part-of-speech tag given to the hyphens, and leftwardbranching structure inside noun phrases is indicated by use of a new NML phrasal category." ></td>
	<td class="line x" title="192:220	The treatment of hyphenated words, in particular, makes the two annotation styles inconsistent, and so we could not work with all the data together." ></td>
	<td class="line x" title="193:220	608 Dependency Parsing Training Testing TARGET ALL HIER Range # Sent Range # Sent ONLY DATA DAUME07 BAYES ABC 055 1195 5669 199 83.32% 88.97% 87.30% 88.68% CNN 0375 5092 376437 1521 85.53% 87.09% 86.41% 87.26% MNB 017 509 1825 245 77.06% 86.41% 84.70% 86.71% NBC 029 552 3039 149 76.21% 85.82% 85.01% 85.32% PRI 089 1707 90112 394 87.65% 90.28% 89.52% 90.59% VOA 0198 1512 199264 383 89.17% 92.11% 90.67% 92.09% Table 3: Dependency parsing results for each of the domain adaptation models." ></td>
	<td class="line x" title="194:220	Performance is measured as unlabeled attachment accuracy." ></td>
	<td class="line x" title="195:220	baseline here, indicating that the transfer of information between domains in the more structurally complicated task is inherently more difficult." ></td>
	<td class="line x" title="196:220	Our models gains over the ALL DATA baseline are quite small, but we tested their significance using a sentence-level paired t-test (over all of the data combined) and found them to be significant at p < 105." ></td>
	<td class="line x" title="197:220	We are unsure why some domains improved while others did not." ></td>
	<td class="line x" title="198:220	It is not simply a consequence of training set size, but may be due to qualities of the domains themselves." ></td>
	<td class="line oc" title="199:220	5 Related Work We already discussed the relation of our work to (Daume III, 2007) in Section 2.4." ></td>
	<td class="line x" title="200:220	Another piece of similar work is (Chelba and Acero, 2004), who also modify their prior." ></td>
	<td class="line x" title="201:220	Their work is limited to two domains, a source and a target, and their algorithm has a two stage process: First, train a classifier on the source data, and then use the learned weights from that classifier as the mean for a Gaussian prior when training a new model on just the target data." ></td>
	<td class="line x" title="202:220	Daume III and Marcu (2006) also took a Bayesian approach to domain adaptation, but structured their model in a very different way." ></td>
	<td class="line x" title="203:220	In their model, it is assumed that each datum within a domain is either a domain-specific datum, or a general datum, and then domain-specific and general weights were learned." ></td>
	<td class="line x" title="204:220	Whether each datum is domain-specific or general is not known, so they developed an EM based algorithm for determining this information while simultaneously learning the feature weights." ></td>
	<td class="line x" title="205:220	Their model had good performance, but came with a 10 to 15 times slowdown at training time." ></td>
	<td class="line x" title="206:220	Our slowest dependency parser took four days to train, making this model close to infeasible for learning on that data." ></td>
	<td class="line x" title="207:220	Outside of the NLP community there has been much similar work making use of hierarchical Bayesian priors to tie parameters across multiple, similar tasks." ></td>
	<td class="line x" title="208:220	Evgeniou et al.(2005) present a similar model, but based on support vector machines, to predict the exam scores of students." ></td>
	<td class="line x" title="210:220	Elidan et al.(2008) make us of an undirected Bayesian transfer hierarchy to jointly model the shapes of different mammals." ></td>
	<td class="line x" title="212:220	The complete literature on related multi-task learning is too large to fully discuss here, but we direct the reader to (Baxter, 1997; Caruana, 1997; Yu et al., 2005; Xue et al., 2007)." ></td>
	<td class="line x" title="213:220	For a more general discussion of hierarchical priors, we recommend Chapter 5 of (Gelman et al., 2003) and Chapter 12 of (Gelman and Hill, 2006)." ></td>
	<td class="line x" title="214:220	6 Conclusion and Future Work In this paper we presented a new model for domain adaptation, based on a hierarchical Bayesian prior, which allows information to be shared between domains when information is sparse, while still allowing the data from a particular domain to override the information from other domains when there is sufficient evidence." ></td>
	<td class="line x" title="215:220	We outperformed previous work on a sequence modeling task, and showed improvements on dependency parsing, a structurally more complex problem, where previous work failed." ></td>
	<td class="line x" title="216:220	Our model is practically useful and does not require significantly more time to train than a baseline model using the same data (though it does require more memory, proportional to the number of domains)." ></td>
	<td class="line x" title="217:220	In the future we would like to see if the model could be adapted to improve performance on data from a new domain, potentially by using the top-level weights which should be less domain-dependent." ></td>
	<td class="line x" title="218:220	Acknowledgements The first author is supported by a Stanford Graduate Fellowship." ></td>
	<td class="line x" title="219:220	We also thank David Vickrey for his helpful comments and observations." ></td>
	<td class="line x" title="220:220	609" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1056
Distributional Representations for Handling Sparsity in Supervised Sequence-Labeling
Huang, Fei;Yates, Alexander;"></td>
	<td class="line x" title="1:216	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 495503, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:216	c2009 ACL and AFNLP Distributional Representations for Handling Sparsity in Supervised Sequence-Labeling Fei Huang Temple University 1805 N. Broad St. Wachman Hall 324 tub58431@temple.edu Alexander Yates Temple University 1805 N. Broad St. Wachman Hall 324 yates@temple.edu Abstract Supervised sequence-labeling systems in natural language processing often suffer from data sparsity because they use word types as features in their prediction tasks." ></td>
	<td class="line x" title="3:216	Consequently, they have difficulty estimating parameters for types which appear in the test set, but seldom (or never) appear in the training set." ></td>
	<td class="line x" title="4:216	We demonstrate that distributional representations of word types, trained on unannotated text, can be used to improve performance on rare words." ></td>
	<td class="line x" title="5:216	We incorporate aspects of these representations into the feature space of our sequence-labeling systems." ></td>
	<td class="line x" title="6:216	In an experiment on a standard chunking dataset, our best technique improves a chunker from 0.76 F1 to 0.86 F1 on chunks beginning with rare words." ></td>
	<td class="line x" title="7:216	On the same dataset, it improves our part-of-speech tagger from 74% to 80% accuracy on rare words." ></td>
	<td class="line x" title="8:216	Furthermore, our system improves significantly over a baseline system when applied to text from a different domain, and it reduces the sample complexity of sequence labeling." ></td>
	<td class="line x" title="9:216	1 Introduction Data sparsity and high dimensionality are the twin curses of statistical natural language processing (NLP)." ></td>
	<td class="line x" title="10:216	In many traditional supervised NLP systems, the feature space includes dimensions for each word type in the data, or perhaps even combinations of word types." ></td>
	<td class="line x" title="11:216	Since vocabularies can be extremely large, this leads to an explosion in the number of parameters." ></td>
	<td class="line x" title="12:216	To make matters worse, language is Zipf-distributed, so that a large fraction of any training data set will be hapax legomena, very many word types will appear only a few times, and many word types will be left out of the training set altogether." ></td>
	<td class="line x" title="13:216	As a consequence, for many word types supervised NLP systems have very few, or even zero, labeled examples from which to estimate parameters." ></td>
	<td class="line x" title="14:216	The negative effects of data sparsity have been well-documented in the NLP literature." ></td>
	<td class="line x" title="15:216	The performance of state-of-the-art, supervised NLP systems like part-of-speech (POS) taggers degrades significantly on words that do not appear in the training data, or out-of-vocabulary (OOV) words (Lafferty et al., 2001)." ></td>
	<td class="line x" title="16:216	Performance also degrades when the domain of the test set differs from the domain of the training set, in part because the test set includes more OOV words and words that appear only a few times in the training set (henceforth, rare words) (Blitzer et al., 2006; Daume III and Marcu, 2006; Chelba and Acero, 2004)." ></td>
	<td class="line x" title="17:216	We investigate the use of distributional representations, which model the probability distribution of a words context, as techniques for finding smoothed representations of word sequences." ></td>
	<td class="line x" title="18:216	That is, we use the distributional representations to share information across unannotated examples of the same word type." ></td>
	<td class="line x" title="19:216	We then compute features of the distributional representations, and provide them as input to our supervised sequence labelers." ></td>
	<td class="line x" title="20:216	Our technique is particularly well-suited to handling data sparsity because it is possible to improve performance on rare words by supplementing the training data with additional unannotated text containing more examples of the rare words." ></td>
	<td class="line x" title="21:216	We provide empirical evidence that shows how distributional representations improve sequencelabeling in the face of data sparsity." ></td>
	<td class="line x" title="22:216	Specifically, we investigate empirically the effects of our smoothing techniques on two sequence-labeling tasks, POS tagging and chunking, to answer the following: 1." ></td>
	<td class="line x" title="23:216	What is the effect of smoothing on sequencelabeling accuracy for rare word types?" ></td>
	<td class="line x" title="24:216	Our best smoothing technique improves a POS tagger by 11% on OOV words, and a chunker by an impressive 21% on OOV words." ></td>
	<td class="line x" title="25:216	495 2." ></td>
	<td class="line x" title="26:216	Can smoothing improve adaptability to new domains?" ></td>
	<td class="line x" title="27:216	After training our chunker on newswire text, we apply it to biomedical texts." ></td>
	<td class="line x" title="28:216	Remarkably, we find that the smoothed chunker achieves a higher F1 on the new domain than the baseline chunker achieves on a test set from the original newswire domain." ></td>
	<td class="line x" title="29:216	3." ></td>
	<td class="line x" title="30:216	How does our smoothing technique affect sample complexity?" ></td>
	<td class="line x" title="31:216	We show that smoothing drastically reduces sample complexity: our smoothed chunker requires under 100 labeled samples to reach 85% accuracy, whereas the unsmoothed chunker requires 3500 samples to reach the same level of performance." ></td>
	<td class="line x" title="32:216	The remainder of this paper is organized as follows." ></td>
	<td class="line x" title="33:216	Section 2 discusses the smoothing problem for word sequences, and introduces three smoothing techniques." ></td>
	<td class="line x" title="34:216	Section 3 presents our empirical study of the effects of smoothing on two sequencelabeling tasks." ></td>
	<td class="line x" title="35:216	Section 4 describes related work, and Section 5 concludes and suggests items for future work." ></td>
	<td class="line x" title="36:216	2 Smoothing Natural Language Sequences To smooth a dataset is to find an approximation of it that retains the important patterns of the original data while hiding the noise or other complicating factors." ></td>
	<td class="line x" title="37:216	Formally, we define the smoothing task as follows: let D = {(x,z)|x is a word sequence, z is a label sequence} be a labeled dataset of word sequences, and let M be a machine learning algorithm that will learn a function f to predict the correct labels." ></td>
	<td class="line x" title="38:216	The smoothing task is to find a function g such that when M is applied to Dprime = {(g(x),z)|(x,z)  D}, it produces a function fprime that is more accurate than f. For supervised sequence-labeling problems in NLP, the most important complicating factor that we seek to avoid through smoothing is the data sparsity associated with word-based representations." ></td>
	<td class="line x" title="39:216	Thus, the task is to find g such that for every word x, g(x) is much less sparse, but still retains the essential features of x that are useful for predicting its label." ></td>
	<td class="line x" title="40:216	As an example, consider the string Researchers test reformulated gasolines on newer engines. In a common dataset for NP chunking, the word reformulated never appears in the training data, but appears four times in the test set as part of the NP reformulated gasolines. Thus, a learning algorithm supplied with word-level features would have a difficult time determining that reformulated is the start of a NP." ></td>
	<td class="line x" title="41:216	Character-level features are of little help as well, since the -ed suffix is more commonly associated with verb phrases." ></td>
	<td class="line x" title="42:216	Finally, context may be of some help, but test is ambiguous between a noun and verb, and gasolines is only seen once in the training data, so there is no guarantee that context is sufficient to make a correct judgment." ></td>
	<td class="line x" title="43:216	On the other hand, some of the other contexts in which reformulated appears in the test set, such as testing of reformulated gasolines, provide strong evidence that it can start a NP, since of is a highly reliable indicator that a NP is to follow." ></td>
	<td class="line x" title="44:216	This example provides the intuition for our approach to smoothing: we seek to share information about the contexts of a word across multiple instances of the word, in order to provide more information about words that are rarely or never seen in training." ></td>
	<td class="line x" title="45:216	In particular, we seek to represent each word by a distribution over its contexts, and then provide the learning algorithm with features computed from this distribution." ></td>
	<td class="line x" title="46:216	Importantly, we seek distributional representations that will provide features that are common in both training and test data, to avoid data sparsity." ></td>
	<td class="line x" title="47:216	In the next three sections, we develop three techniques for smoothing text using distributional representations." ></td>
	<td class="line x" title="48:216	2.1 Multinomial Representation In its simplest form, the context of a word may be represented as a multinomial distribution over the terms that appear on either side of the word." ></td>
	<td class="line x" title="49:216	If V is the vocabulary, or the set of word types, and X is a sequence of random variables over V, the left and right context of Xi = v may each be represented as a probability distribution over V: P(Xi1|Xi = v) and P(Xi+1|X = v) respectively." ></td>
	<td class="line x" title="50:216	We learn these distributions from unlabeled texts in two different ways." ></td>
	<td class="line x" title="51:216	The first method computes word count vectors for the left and right contexts of each word type in the vocabulary of the training and test texts." ></td>
	<td class="line x" title="52:216	We also use a large collection of additional text to determine the vectors." ></td>
	<td class="line x" title="53:216	We then normalize each vector to form a probability distribution." ></td>
	<td class="line x" title="54:216	The second technique first applies TF-IDF weighting to each vector, where the context words of each word type constitute a document, before applying normalization." ></td>
	<td class="line x" title="55:216	This gives greater weight to words with more idiosyncratic distributions and may improve the informativeness of a distributional representation." ></td>
	<td class="line x" title="56:216	We refer to these techniques as TF and TF-IDF." ></td>
	<td class="line x" title="57:216	496 To supply a sequence-labeling algorithm with information from these distributional representations, we compute real-valued features of the context distributions." ></td>
	<td class="line x" title="58:216	In particular, for every word xi in a sequence, we provide the sequence labeler with a set of features of the left and right contexts indexed by v  V: Fleftv (xi) = P(Xi1 = v|xi) and Frightv (xi) = P(Xi+1 = v|xi)." ></td>
	<td class="line x" title="59:216	For example, the left context for reformulated in our example above would contain a nonzero probability for the word of. Using the features F(xi), a sequence labeler can learn patterns such as, if xi has a high probability of following of, it is a good candidate for the start of a noun phrase." ></td>
	<td class="line x" title="60:216	These features provide smoothing by aggregating information across multiple unannotated examples of the same word." ></td>
	<td class="line x" title="61:216	2.2 LSA Model One drawback of the multinomial representation is that it does not handle sparsity well enough, because the multinomial distributions themselves are so high-dimensional." ></td>
	<td class="line x" title="62:216	For example, the two phrases red lamp and magenta tablecloth share no words in common." ></td>
	<td class="line x" title="63:216	If magenta is never observed in training, the fact that tablecloth appears in its right context is of no help in connecting it with the phrase red lamp. But if we can group similar context words together, putting lamp and tablecloth into a category for household items, say, then these two adjectives will share that category in their context distributions." ></td>
	<td class="line x" title="64:216	Any patterns learned for the more common red lamp will then also apply to the less common magenta tablecloth. Our second distributional representation aggregates information from multiple context words by grouping together the distributions P(xi1 = v|xi = w) and P(xi1 = vprime|xi = w) if v and vprime appear together with many of the same words w. Aggregating counts in this way smooths our representations even further, by supplying better estimates when the data is too sparse to estimate P(xi1|xi) accurately." ></td>
	<td class="line x" title="65:216	Latent Semantic Analysis (LSA) (Deerwester et al., 1990) is a widely-used technique for computing dimensionality-reduced representations from a bag-of-words model." ></td>
	<td class="line x" title="66:216	We apply LSA to the set of right context vectors and the set of left context vectors separately, to find compact versions of each vector, where each dimension represents a combination of several context word types." ></td>
	<td class="line x" title="67:216	We normalize each vector, and then calculate features as above." ></td>
	<td class="line x" title="68:216	After experimenting with different choices for the number of dimensions to reduce our vectors to, we choose a value of 10 dimensions as the one that maximizes the performance of our supervised sequence labelers on held-out data." ></td>
	<td class="line x" title="69:216	2.3 Latent Variable Language Model Representation To take smoothing one step further, we present a technique that aggregates context distributions both for similar context words xi1 = v and vprime, and for similar words xi = w and wprime." ></td>
	<td class="line x" title="70:216	Latent variable language models (LVLMs) can be used to produce just such a distributional representation." ></td>
	<td class="line x" title="71:216	We use Hidden Markov Models (HMMs) as the main example in the discussion and as the LVLMs in our experiments, but the smoothing technique can be generalized to other forms of LVLMs, such as factorial HMMs and latent variable maximum entropy models (Ghahramani and Jordan, 1997; Smith and Eisner, 2005)." ></td>
	<td class="line x" title="72:216	An HMM is a generative probabilistic model that generates each word xi in the corpus conditioned on a latent variable Yi." ></td>
	<td class="line x" title="73:216	Each Yi in the model takes on integral values from 1 to S, and each one is generated by the latent variable for the preceding word, Yi1." ></td>
	<td class="line x" title="74:216	The distribution for a corpus x = (x1,,xN) given a set of state vectors y = (y1,,yN) is given by: P(x|y) = productdisplay i P(xi|yi)P(yi|yi1) Using Expectation-Maximization (Dempster et al., 1977), it is possible to estimate the distributions for P(xi|yi) and P(yi|yi1) from unlabeled data." ></td>
	<td class="line x" title="75:216	We use a trained HMM to determine the optimal sequence of latent states yi using the wellknown Viterbi algorithm (Rabiner, 1989)." ></td>
	<td class="line x" title="76:216	The output of this process is an integer (ranging from 1 to S) for every word xi in the corpus; we include a new boolean feature for each possible value of yi in our sequence labelers." ></td>
	<td class="line x" title="77:216	To compare our models, note that in the multinomial representation we directly model the probability that a word v appears before a word w: P(xi1 = v|xi = w))." ></td>
	<td class="line x" title="78:216	In our LSA model, we find latent categories of context words z, and model the probability that a category appears before the current word w: P(xi1 = z|xi = w)." ></td>
	<td class="line x" title="79:216	The HMM finds (probabilistic) categories Y for both the current word xi and the context word xi1, and models the probability that one category follows the 497 other: P(Yi|Yi1)." ></td>
	<td class="line x" title="80:216	Thus the HMM is our most extreme smoothing model, as it aggregates information over the greatest number of examples: for a given consecutive pair of words xi1,xi in the test set, it aggregates over all pairs of consecutive words xprimei1,xprimei where xprimei1 is similar to xi1 and xprimei is similar to xi." ></td>
	<td class="line x" title="81:216	3 Experiments We tested the following hypotheses in our experiments: 1." ></td>
	<td class="line x" title="82:216	Smoothing can improve the performance of a supervised sequence labeling system on words that are rare or nonexistent in the training data." ></td>
	<td class="line x" title="83:216	2." ></td>
	<td class="line x" title="84:216	A supervised sequence labeler achieves greater accuracy on new domains with smoothing." ></td>
	<td class="line x" title="85:216	3." ></td>
	<td class="line x" title="86:216	A supervised sequence labeler has a better sample complexity with smoothing." ></td>
	<td class="line x" title="87:216	3.1 Experimental Setup We investigate the use of smoothing in two test systems, conditional random field (CRF) models for POS tagging and chunking." ></td>
	<td class="line x" title="88:216	To incorporate smoothing into our models, we follow the following general procedure: first, we collect a set of unannotated text from the same domain as the test data set." ></td>
	<td class="line x" title="89:216	Second, we train a smoothing model on the text of the training data, the test data, and the additional collection." ></td>
	<td class="line x" title="90:216	We then automatically annotate both the training and test data with features calculated from the distributional representation." ></td>
	<td class="line x" title="91:216	Finally, we train the CRF model on the annotated training set and apply it to the test set." ></td>
	<td class="line x" title="92:216	We use an open source CRF software package designed by Sunita Sajarwal and William W. Cohen to implement our CRF models.1 We use a set of boolean features listed in Table 1." ></td>
	<td class="line x" title="93:216	Our baseline CRF system for POS tagging follows the model described by Lafferty et al.(2001)." ></td>
	<td class="line x" title="94:216	We include transition features between pairs of consecutive tag variables, features between tag variables and words, and a set of orthographic features that Lafferty et al. found helpful for performance on OOV words." ></td>
	<td class="line x" title="95:216	Our smoothed models add features computed from the distributional representations, as discussed above." ></td>
	<td class="line x" title="96:216	Our chunker follows the system described by Sha and Pereira (2003)." ></td>
	<td class="line x" title="97:216	In addition to the transition, word-level, and orthographic features, we include features relating automatically-generated POS tags and the chunk labels." ></td>
	<td class="line x" title="98:216	Unlike Sha and 1Available from http://sourceforge.net/projects/crf/ CRF Feature Set Transition zi=z zi=z and zi1=zprime Word xi=w and zi=z POS ti=t and zi=z Orthography for every s  {-ing, -ogy, ed, -s, -ly, -ion, -tion, -ity}, suffix(xi)= s and zi=z xi is capitalized and zi = z xi has a digit and zi = z TF, TF-IDF, and LSA features for every context type v, Fleftv (xi) and Frightv (xi) HMM features yi=y and zi = z Table 1: Features used in our CRF systems." ></td>
	<td class="line x" title="99:216	zi variables represent labels to be predicted, ti represent tags (for the chunker), and xi represent word tokens." ></td>
	<td class="line x" title="100:216	All features are boolean except for the TF, TF-IDF, and LSA features." ></td>
	<td class="line x" title="101:216	Pereira, we exclude features relating consecutive pairs of words and a chunk label, or features relating consecutive tag labels and a chunk label, in order to expedite our experiments." ></td>
	<td class="line x" title="102:216	We found that including such features does improve chunking F1 by approximately 2%, but it also significantly slows down CRF training." ></td>
	<td class="line x" title="103:216	3.2 Rare Word Accuracy For these experiments, we use the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993)." ></td>
	<td class="line x" title="104:216	Following the CoNLL shared task from 2000, we use sections 15-18 of the Penn Treebank for our labeled training data for the supervised sequence labeler in all experiments (Tjong et al., 2000)." ></td>
	<td class="line x" title="105:216	For the tagging experiments, we train and test using the gold standard POS tags contained in the Penn Treebank." ></td>
	<td class="line x" title="106:216	For the chunking experiments, we train and test with POS tags that are automatically generated by a standard tagger (Brill, 1994)." ></td>
	<td class="line x" title="107:216	We tested the accuracy of our models for chunking and POS tagging on section 20 of the Penn Treebank, which corresponds to the test set from the CoNLL 2000 task." ></td>
	<td class="line x" title="108:216	Our distributional representations are trained on sections 2-22 of the Penn Treebank." ></td>
	<td class="line x" title="109:216	Because we include the text from the train and test sets in our training data for the distributional representations, we do not need to worry about smoothing them  when they are decoded on the test set, they 498 Freq: 0 1 2 0-2 all #Samples 438 508 588 1534 46661 Baseline .62 .77 .81 .74 .93 TF .76 .72 .77 .75 .92 TF-IDF .82 .75 .76 .78 .94 LSA .78 .80 .77 .78 .94 HMM .73 .81 .86 .80 .94 Table 2: POS tagging accuracy: our HMM-smoothed tagger outperforms the baseline tagger by 6% on rare words." ></td>
	<td class="line x" title="110:216	Differences between the baseline and the HMM are statistically significant at p < 0.01 for the OOV, 0-2, and all cases using the two-tailed Chi-squared test with 1 degree of freedom." ></td>
	<td class="line x" title="111:216	will not encounter any previously unseen words." ></td>
	<td class="line x" title="112:216	However, to speed up training during our experiments and, in some cases, to avoid running out of memory, we replaced words appearing twice or fewer times in the data with the special symbol *UNKNOWN*." ></td>
	<td class="line x" title="113:216	In addition, all numbers were replaced with another special symbol." ></td>
	<td class="line x" title="114:216	For the LSA model, we had to use a more drastic cutoff to fit the singular value decomposition computation into memory: we replaced words appearing 10 times or fewer with the *UNKNOWN* symbol." ></td>
	<td class="line x" title="115:216	We initialize our HMMs randomly." ></td>
	<td class="line x" title="116:216	We run EM ten times and take the model with the best cross-entropy on a held-out set." ></td>
	<td class="line x" title="117:216	After experimenting with different variations of HMM models, we settled on a model with 80 latent states as a good compromise between accuracy and efficiency." ></td>
	<td class="line x" title="118:216	For our POS tagging experiments, we measured the accuracy of the tagger on rare words, or words that appear at most twice in the training data." ></td>
	<td class="line x" title="119:216	For our chunking experiments, we focus on chunks that begin with rare words, as we found that those were the most difficult for the chunker to identify correctly." ></td>
	<td class="line x" title="120:216	So we define rare chunks as those that begin with words appearing at most twice in training data." ></td>
	<td class="line x" title="121:216	To ensure that our smoothing models have enough training data for our test set, we further narrow our focus to those words that appear rarely in the labeled training data, but appear at least ten times in sections 2-22." ></td>
	<td class="line x" title="122:216	Tables 2 and 3 show the accuracy of our smoothed models and the baseline model on tagging and chunking, respectively." ></td>
	<td class="line x" title="123:216	The line for all in both tables indicates results on the complete test set." ></td>
	<td class="line x" title="124:216	Both our baseline tagger and chunker achieve respectable results on their respective tasks for all words, and the results were good enough for Freq: 0 1 2 0-2 all #Samples 133 199 231 563 21900 Baseline .69 .75 .81 .76 .90 TF .70 .82 .79 .77 .89 TF-IDF .77 .77 .80 .78 .90 LSA .84 .82 .83 .84 .90 HMM .90 .85 .85 .86 .93 Table 3: Chunking F1: our HMM-smoothed chunker outperforms the baseline CRF chunker by 0.21 on chunks that begin with OOV words, and 0.10 on chunks that begin with rare words." ></td>
	<td class="line x" title="125:216	us to be satisfied that performance on rare words closely follows how a state-of-the-art supervised sequence-labeler behaves." ></td>
	<td class="line x" title="126:216	The chunkers accuracy is roughly in the middle of the range of results for the original CoNLL 2000 shared task (Tjong et al., 2000) . While several systems have achieved slightly higher accuracy on supervised POS tagging, they are usually trained on larger training sets." ></td>
	<td class="line x" title="127:216	As expected, the drop-off in the baseline systems performance from all words to rare words is impressive for both tasks." ></td>
	<td class="line x" title="128:216	Comparing performance on all terms and OOV terms, the baseline taggers accuracy drops by 0.31, and the baseline chunkers F1 drops by 0.21." ></td>
	<td class="line x" title="129:216	Comparing performance on all terms and rare terms, the drop is less severe but still dramatic: 0.19 for tagging and 0.15 for chunking." ></td>
	<td class="line x" title="130:216	Our hypothesis that smoothing would improve performance on rare terms is validated by these experiments." ></td>
	<td class="line x" title="131:216	In fact, the more aggregation a smoothing model performs, the better it appears to be at smoothing." ></td>
	<td class="line x" title="132:216	The HMM-smoothed system outperforms all other systems in all categories except tagging on OOV words, where TF-IDF performs best." ></td>
	<td class="line x" title="133:216	And in most cases, the clear trend is for HMM smoothing to outperform LSA, which in turn outperforms TF and TF-IDF." ></td>
	<td class="line x" title="134:216	HMM tagging performance on OOV terms improves by 11%, and chunking performance by 21%." ></td>
	<td class="line x" title="135:216	Tagging performance on all of the rare terms improves by 6%, and chunking by 10%." ></td>
	<td class="line x" title="136:216	In chunking, there is a clear trend toward larger increases in performance as words become rarer in the labeled data set, from a 0.02 improvement on words of frequency 2, to an improvement of 0.21 on OOV words." ></td>
	<td class="line x" title="137:216	Because the test data for this experiment is drawn from the same domain (newswire) as the 499 training data, the rare terms make up a relatively small portion of the overall dataset (approximately 4% of both the tagged words and the chunks)." ></td>
	<td class="line x" title="138:216	Still, the increased performance by the HMMsmoothed model on the rare-word subset contributes in part to an increase in performance on the overall dataset of 1% for tagging and 3% for chunking." ></td>
	<td class="line x" title="139:216	In our next experiment, we consider a common scenario where rare terms make up a much larger fraction of the test data." ></td>
	<td class="line x" title="140:216	3.3 Domain Adaptation For our experiment on domain adaptation, we focus on NP chunking and POS tagging, and we use the labeled training data from the CoNLL 2000 shared task as before." ></td>
	<td class="line x" title="141:216	For NP chunking, we use 198 sentences from the biochemistry domain in the Open American National Corpus (OANC) (Reppen et al., 2005) as or our test set." ></td>
	<td class="line x" title="142:216	We manually tagged the test set with POS tags and NP chunk boundaries." ></td>
	<td class="line x" title="143:216	The test set contains 5330 words and a total of 1258 NP chunks." ></td>
	<td class="line x" title="144:216	We used sections 15-18 of the Penn Treebank as our labeled training set, including the gold standard POS tags." ></td>
	<td class="line x" title="145:216	We use our best-performing smoothing model, the HMM, and train it on sections 13 through 19 of the Penn Treebank, plus the written portion of the OANC that contains journal articles from biochemistry (40,727 sentences)." ></td>
	<td class="line x" title="146:216	We focus on chunks that begin with words appearing 0-2 times in the labeled training data, and appearing at least ten times in the HMMs training data." ></td>
	<td class="line x" title="147:216	Table 4 contains our results." ></td>
	<td class="line x" title="148:216	For our POS tagging experiments, we use 561 MEDLINE sentences (9576 words) from the Penn BioIE project (PennBioIE, 2005), a test set previously used by Blitzer et al.(2006)." ></td>
	<td class="line x" title="149:216	We use the same experimental setup as Blitzer et al.: 40,000 manually tagged sentences from the Penn Treebank for our labeled training data, and all of the unlabeled text from the Penn Treebank plus their MEDLINE corpus of 71,306 sentences to train our HMM." ></td>
	<td class="line x" title="150:216	We report on tagging accuracy for all words and OOV words in Table 5." ></td>
	<td class="line x" title="151:216	This table also includes results for two previous systems as reported by Blitzer et al.(2006): the semi-supervised Alternating Structural Optimization (ASO) technique and the Structural Correspondence Learning (SCL) technique for domain adaptation." ></td>
	<td class="line x" title="153:216	Note that this test set for NP chunking contains a much higher proportion of rare and OOV words: 23% of chunks begin with an OOV word, and 29% begin with a rare word, as compared with Baseline HMM Freq." ></td>
	<td class="line x" title="154:216	# R P F1 R P F1 0 284 .74 .70 .72 .80 .89 .84 1 39 .85 .87 .86 .92 .88 .90 2 39 .79 .86 .83 .92 .90 .91 0-2 362 .75 .73 .74 .82 .89 .85 all 1258 .86 .87 .86 .91 .90 .91 Table 4: On biochemistry journal data from the OANC, our HMM-smoothed NP chunker outperforms the baseline CRF chunker by 0.12 (F1) on chunks that begin with OOV words, and by 0.05 (F1) on all chunks." ></td>
	<td class="line x" title="155:216	Results in bold are statistically significantly different from the baseline results at p < 0.05 using the two-tailed Fishers exact test." ></td>
	<td class="line x" title="156:216	We did not perform significance tests for F1." ></td>
	<td class="line x" title="157:216	All Unknown Model words words Baseline 88.3 67.3 ASO 88.4 70.9 SCL 88.9 72.0 HMM 90.5 75.2 Table 5: On biomedical data from the Penn BioIE project, our HMM-smoothed tagger outperforms the SCL tagger by 3% (accuracy) on OOV words, and by 1.6% (accuracy) on all words." ></td>
	<td class="line x" title="158:216	Differences between the smoothed tagger and the SCL tagger are significant at p < .001 for all words and for OOV words, using the Chi-squared test with 1 degree of freedom." ></td>
	<td class="line x" title="159:216	1% and 4%, respectively, for NP chunks in the test set from the original domain." ></td>
	<td class="line x" title="160:216	The test set for tagging also contains a much higher proportion: 23% OOV words, as compared with 1% in the original domain." ></td>
	<td class="line x" title="161:216	Because of the increase in the number of rare words, the baseline chunkers overall performance drops by 4% compared with performance on WSJ data, and the baseline taggers overall performance drops by 5% in the new domain." ></td>
	<td class="line x" title="162:216	The performance improvements for both the smoothed NP chunker and tagger are again impressive: there is a 12% improvement on OOV words, and a 10% overall improvement on rare words for chunking; the tagger shows an 8% improvement on OOV words compared to out baseline and a 3% improvement on OOV words compared to the SCL model." ></td>
	<td class="line x" title="163:216	The resulting performance of the smoothed NP chunker is almost identical to its performance on the WSJ data." ></td>
	<td class="line x" title="164:216	Through smoothing, the chunker not only improves by 5% 500 in F1 over the baseline system on all words, it in fact outperforms our baseline NP chunker on the WSJ data." ></td>
	<td class="line x" title="165:216	60% of this improvement comes from improved accuracy on rare words." ></td>
	<td class="line x" title="166:216	The performance of our HMM-smoothed chunker caused us to wonder how well the chunker could work without some of its other features." ></td>
	<td class="line x" title="167:216	We removed all tag features and all features for word types that appear fewer than 20 times in training." ></td>
	<td class="line x" title="168:216	This chunker achieves 0.91 F1 on OANC data, and 0.93 F1 on WSJ data, outperforming the baseline system in both cases." ></td>
	<td class="line x" title="169:216	It has only 20% as many features as the baseline chunker, greatly improving its training time." ></td>
	<td class="line x" title="170:216	Thus our smoothing features are more valuable to the chunker than features from POS tags and features for all but the most common words." ></td>
	<td class="line x" title="171:216	Our results point to the exciting possibility that with smoothing, we may be able to train a sequence-labeling system on a small labeled sample, and have it apply generally to other domains." ></td>
	<td class="line x" title="172:216	Exactly what size training set we need is a question that we address next." ></td>
	<td class="line x" title="173:216	3.4 Sample Complexity Our complete system consists of two learned components, a supervised CRF system and an unsupervised smoothing model." ></td>
	<td class="line x" title="174:216	We measure the sample complexity of each component separately." ></td>
	<td class="line x" title="175:216	To measure the sample complexity of the supervised CRF, we use the same experimental setup as in the chunking experiment on WSJ text, but we vary the amount of labeled data available to the CRF." ></td>
	<td class="line x" title="176:216	We take ten random samples of a fixed size from the labeled training set, train a chunking model on each subset, and graph the F1 on the labeled test set, averaged over the ten runs, in Figure 1." ></td>
	<td class="line x" title="177:216	To measure the sample complexity of our HMM with respect to unlabeled text, we use the full labeled training set and vary the amount of unlabeled text available to the HMM." ></td>
	<td class="line x" title="178:216	At minimum, we use the text available in the labeled training and test sets, and then add random subsets of the Penn Treebank, sections 2-22." ></td>
	<td class="line x" title="179:216	For each subset size, we take ten random samples of the unlabeled text, train an HMM and then a chunking model, and graph the F1 on the labeled test set averaged over the ten runs in Figure 2." ></td>
	<td class="line x" title="180:216	The results from our labeled sample complexity experiment indicate that sample complexity is drastically reduced by HMM smoothing." ></td>
	<td class="line x" title="181:216	On rare chunks, the smoothed system reaches 0.78 F1 using only 87 labeled training sentences, a level that the baseline system never reaches, even with 6933 baseline (all) HMM (all) HMM (rare) 0.6 0.7 0.8 0.9 1 F1  (C hu nk ing ) Labeled Sample Complexity baseline (rare) 0.2 0.3 0.4 0.5 1 10 100 1000 10000 F1  (C hu nk ing ) Number of Labeled Sentences (log scale) Figure 1: The smoothed NP chunker requires less than 10% of the samples needed by the baseline chunker to achieve .83 F1, and the same for .88 F1." ></td>
	<td class="line x" title="182:216	Baseline (all) HMM (all) HMM (rare) 0.80 0.85 0.90 0.95 F1  (C hu nk ing ) Unlabeled Sample Complexity Baseline (rare)0.70 0.75 0 10000 20000 30000 40000 F1  (C hu nk ing ) Number of Unannotated Sentences Figure 2: By leveraging plentiful unannotated text, the smoothed chunker soon outperforms the baseline." ></td>
	<td class="line x" title="183:216	labeled sentences." ></td>
	<td class="line x" title="184:216	On the overall data set, the smoothed system reaches 0.83 F1 with 50 labeled sentences, which the baseline does not reach until it has 867 labeled sentences." ></td>
	<td class="line x" title="185:216	With 434 labeled sentences, the smoothed system reaches 0.88 F1, which the baseline system does not reach until it has 5200 labeled samples." ></td>
	<td class="line x" title="186:216	Our unlabeled sample complexity results show that even with access to a small amount of unlabeled text, 6000 sentences more than what appears in the training and test sets, smoothing using the HMM yields 0.78 F1 on rare chunks." ></td>
	<td class="line x" title="187:216	However, the smoothed system requires 25,000 more sentences before it outperforms the baseline system on all chunks." ></td>
	<td class="line x" title="188:216	No peak in performance is reached, so further improvements are possible with more unlabeled data." ></td>
	<td class="line x" title="189:216	Thus smoothing is optimizing performance for the case where unlabeled data is plentiful and labeled data is scarce, as we would hope." ></td>
	<td class="line x" title="190:216	4 Related Work To our knowledge, only one previous system  the REALM system for sparse information extrac501 tion  has used HMMs as a feature representation for other applications." ></td>
	<td class="line x" title="191:216	REALM uses an HMM trained on a large corpus to help determine whether the arguments of a candidate relation are of the appropriate type (Downey et al., 2007)." ></td>
	<td class="line x" title="192:216	We extend and generalize this smoothing technique and apply it to common NLP applications involving supervised sequence-labeling, and we provide an in-depth empirical analysis of its performance." ></td>
	<td class="line x" title="193:216	Several researchers have previously studied methods for using unlabeled data for tagging and chunking, either alone or as a supplement to labeled data." ></td>
	<td class="line x" title="194:216	Ando and Zhang develop a semisupervised chunker that outperforms purely supervised approaches on the CoNLL 2000 dataset (Ando and Zhang, 2005)." ></td>
	<td class="line x" title="195:216	Recent projects in semisupervised (Toutanova and Johnson, 2007) and unsupervised (Biemann et al., 2007; Smith and Eisner, 2005) tagging also show significant progress." ></td>
	<td class="line x" title="196:216	Unlike these systems, our efforts are aimed at using unlabeled data to find distributional representations that work well on rare terms, making the supervised systems more applicable to other domains and decreasing their sample complexity." ></td>
	<td class="line x" title="197:216	HMMs have been used many times for POS tagging and chunking, in supervised, semisupervised, and in unsupervised settings (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Johnson, 2007; Zhou, 2004)." ></td>
	<td class="line x" title="198:216	We take a novel perspective on the use of HMMs by using them to compute features of each token in the data that represent the distribution over that tokens contexts." ></td>
	<td class="line x" title="199:216	Our technique lets the HMM find parameters that maximize cross-entropy, and then uses labeled data to learn the best mapping from the HMM categories to the POS categories." ></td>
	<td class="line x" title="200:216	Smoothing in NLP usually refers to the problem of smoothing n-gram models." ></td>
	<td class="line x" title="201:216	Sophisticated smoothing techniques like modified Kneser-Ney and Katz smoothing (Chen and Goodman, 1996) smooth together the predictions of unigram, bigram, trigram, and potentially higher n-gram sequences to obtain accurate probability estimates in the face of data sparsity." ></td>
	<td class="line x" title="202:216	Our task differs in that we are primarily concerned with the case where even the unigram model (single word) is rarely or never observed in the labeled training data." ></td>
	<td class="line x" title="203:216	Sparsity for low-order contexts has recently spurred interest in using latent variables to represent distributions over contexts in language models." ></td>
	<td class="line x" title="204:216	While n-gram models have traditionally dominated in language modeling, two recent efforts develop latent-variable probabilistic models that rival and even surpass n-gram models in accuracy (Blitzer et al., 2005; Mnih and Hinton, 2007)." ></td>
	<td class="line x" title="205:216	Several authors investigate neural network models that learn not just one latent state, but rather a vector of latent variables, to represent each word in a language model (Bengio et al., 2003; Emami et al., 2003; Morin and Bengio, 2005)." ></td>
	<td class="line x" title="206:216	One of the benefits of our smoothing technique is that it allows for domain adaptation, a topic that has received a great deal of attention from the NLP community recently." ></td>
	<td class="line oc" title="207:216	Unlike our technique, in most cases researchers have focused on the scenario where labeled training data is available in both the source and the target domain (e.g., (Daume III, 2007; Chelba and Acero, 2004; Daume III and Marcu, 2006))." ></td>
	<td class="line x" title="208:216	Our technique uses unlabeled training data from the target domain, and is thus applicable more generally, including in web processing, where the domain and vocabulary is highly variable, and it is extremely difficult to obtain labeled data that is representative of the test distribution." ></td>
	<td class="line x" title="209:216	When labeled target-domain data is available, instance weighting and similar techniques can be used in combination with our smoothing technique to improve our results further, although this has not yet been demonstrated empirically." ></td>
	<td class="line x" title="210:216	HMM-smoothing improves on the most closely related work, the Structural Correspondence Learning technique for domain adaptation (Blitzer et al., 2006), in experiments." ></td>
	<td class="line x" title="211:216	5 Conclusion and Future Work Our study of smoothing techniques demonstrates that by aggregating information across many unannotated examples, it is possible to find accurate distributional representations that can provide highly informative features to supervised sequence labelers." ></td>
	<td class="line x" title="212:216	These features help improve sequence labeling performance on rare word types, on domains that differ from the training set, and on smaller training sets." ></td>
	<td class="line x" title="213:216	Further experiments are of course necessary to investigate distributional representations as smoothing techniques." ></td>
	<td class="line x" title="214:216	One particularly promising area for further study is the combination of smoothing and instance weighting techniques for domain adaptation." ></td>
	<td class="line x" title="215:216	Whether the current techniques are applicable to structured prediction tasks, like parsing and relation extraction, also deserves future attention." ></td>
	<td class="line x" title="216:216	502" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1059
Automatic Adaptation of Annotation Standards: Chinese Word Segmentation and POS Tagging &#8211; A Case Study
Jiang, Wenbin;Huang, Liang;Liu, Qun;"></td>
	<td class="line x" title="1:154	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 522530, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:154	c2009 ACL and AFNLP Automatic Adaptation of Annotation Standards: Chinese Word Segmentation and POS Tagging  A Case Study Wenbin Jiang  Liang Huang  Qun Liu  Key Lab." ></td>
	<td class="line x" title="3:154	of Intelligent Information Processing Google Research Institute of Computing Technology 1350 Charleston Rd. Chinese Academy of Sciences Mountain View, CA 94043, USA P.O. Box 2704, Beijing 100190, China lianghuang@google.com {jiangwenbin, liuqun}@ict.ac.cn liang.huang.sh@gmail.com Abstract Manually annotated corpora are valuable but scarce resources, yet for many annotation tasks such as treebanking and sequence labeling there exist multiple corpora with different and incompatible annotation guidelines or standards." ></td>
	<td class="line x" title="4:154	This seems to be a great waste of human efforts, and it would be nice to automatically adapt one annotation standard to another." ></td>
	<td class="line x" title="5:154	We present a simple yet effective strategy that transfers knowledge from a differently annotated corpus to the corpus with desired annotation." ></td>
	<td class="line x" title="6:154	We test the efficacy of this method in the context of Chinese word segmentation and part-of-speech tagging, where no segmentation and POS tagging standards are widely accepted due to the lack of morphology in Chinese." ></td>
	<td class="line x" title="7:154	Experiments show that adaptation from the much larger Peoples Daily corpus to the smaller but more popular Penn Chinese Treebank results in significant improvements in both segmentation and tagging accuracies (with error reductions of 30.2% and 14%, respectively), which in turn helps improve Chinese parsing accuracy." ></td>
	<td class="line x" title="8:154	1 Introduction Much of statistical NLP research relies on some sort of manually annotated corpora to train their models, but these resources are extremely expensive to build, especially at a large scale, for example in treebanking (Marcus et al., 1993)." ></td>
	<td class="line x" title="9:154	However the linguistic theories underlying these annotation efforts are often heavily debated, and as a result there often exist multiple corpora for the same task with vastly different and incompatible annotation philosophies." ></td>
	<td class="line x" title="10:154	For example just for English treebanking there have been the Chomskian-style cjkC3C01 cjkB8B12 cjkD7DC3 cjkCDB34 cjkB7C35 cjkBBAA6 NR NN VV NR U.S. Vice-President visited China cjkC3C01 cjkB8B12 cjkD7DC3 cjkCDB34 cjkB7C35 cjkBBAA6 ns b n v U.S. Vice President visited-China Figure 1: Incompatible word segmentation and POS tagging standards between CTB (upper) and Peoples Daily (below)." ></td>
	<td class="line x" title="11:154	Penn Treebank (Marcus et al., 1993) the HPSG LinGo Redwoods Treebank (Oepen et al., 2002), and a smaller dependency treebank (Buchholz and Marsi, 2006)." ></td>
	<td class="line x" title="12:154	A second, related problem is that the raw texts are also drawn from different domains, which for the above example range from financial news (PTB/WSJ) to transcribed dialog (LinGo)." ></td>
	<td class="line x" title="13:154	These two problems seem be a great waste in human efforts, and it would be nice if one could automatically adapt from one annotation standard and/or domain to another in order to exploit much larger datasets for better training." ></td>
	<td class="line oc" title="14:154	The second problem, domain adaptation, is very well-studied, e.g. by Blitzer et al.(2006) and Daume III (2007) (and see below for discussions), so in this paper we focus on the less studied, but equally important problem of annotationstyle adaptation." ></td>
	<td class="line x" title="16:154	We present a very simple yet effective strategy that enables us to utilize knowledge from a differently annotated corpora for the training of a model on a corpus with desired annotation." ></td>
	<td class="line x" title="17:154	The basic idea is very simple: we first train on a source corpus, resulting in a source classifier, which is used to label the target corpus and results in a sourcestyle annotation of the target corpus." ></td>
	<td class="line x" title="18:154	We then 522 train a second model on the target corpus with the first classifiers prediction as additional features for guided learning." ></td>
	<td class="line oc" title="19:154	This method is very similar to some ideas in domain adaptation (Daume III and Marcu, 2006; Daume III, 2007), but we argue that the underlying problems are quite different." ></td>
	<td class="line o" title="20:154	Domain adaptation assumes the labeling guidelines are preserved between the two domains, e.g., an adjective is always labeled as JJ regardless of from Wall Street Journal (WSJ) or Biomedical texts, and only the distributions are different, e.g., the word control is most likely a verb in WSJ but often a noun in Biomedical texts (as in control experiment)." ></td>
	<td class="line x" title="21:154	Annotation-style adaptation, however, tackles the problem where the guideline itself is changed, for example, one treebank might distinguish between transitive and intransitive verbs, while merging the different noun types (NN, NNS, etc.), and for example one treebank (PTB) might be much flatter than the other (LinGo), not to mention the fundamental disparities between their underlying linguistic representations (CFG vs. HPSG)." ></td>
	<td class="line x" title="22:154	In this sense, the problem we study in this paper seems much harder and more motivated from a linguistic (rather than statistical) point of view." ></td>
	<td class="line x" title="23:154	More interestingly, our method, without any assumption on the distributions, can be simultaneously applied to both domain and annotation standards adaptation problems, which is very appealing in practice because the latter problem often implies the former, as in our case study." ></td>
	<td class="line x" title="24:154	To test the efficacy of our method we choose Chinese word segmentation and part-of-speech tagging, where the problem of incompatible annotation standards is one of the most evident: so far no segmentation standard is widely accepted due to the lack of a clear definition of Chinese words, and the (almost complete) lack of morphology results in much bigger ambiguities and heavy debates in tagging philosophies for Chinese parts-of-speech." ></td>
	<td class="line x" title="25:154	The two corpora used in this study are the much larger Peoples Daily (PD) (5.86M words) corpus (Yu et al., 2001) and the smaller but more popular Penn Chinese Treebank (CTB) (0.47M words) (Xue et al., 2005)." ></td>
	<td class="line x" title="26:154	They used very different segmentation standards as well as different POS tagsets and tagging guidelines." ></td>
	<td class="line x" title="27:154	For example, in Figure 1, Peoples Daily breaks Vice-President into two words while combines the phrase visited-China as a compound." ></td>
	<td class="line x" title="28:154	Also CTB has four verbal categories (VV for normal verbs, and VC for copulas, etc.) while PD has only one verbal tag (v) (Xia, 2000)." ></td>
	<td class="line x" title="29:154	It is preferable to transfer knowledge from PD to CTB because the latter also annotates tree structures which is very useful for downstream applications like parsing, summarization, and machine translation, yet it is much smaller in size." ></td>
	<td class="line x" title="30:154	Indeed, many recent efforts on Chinese-English translation and Chinese parsing use the CTB as the de facto segmentation and tagging standards, but suffers from the limited size of training data (Chiang, 2007; Bikel and Chiang, 2000)." ></td>
	<td class="line x" title="31:154	We believe this is also a reason why stateof-the-art accuracy for Chinese parsing is much lower than that of English (CTB is only half the size of PTB)." ></td>
	<td class="line x" title="32:154	Our experiments show that adaptation from PD to CTB results in a significant improvement in segmentation and POS tagging, with error reductions of 30.2% and 14%, respectively." ></td>
	<td class="line x" title="33:154	In addition, the improved accuracies from segmentation and tagging also lead to an improved parsing accuracy on CTB, reducing 38% of the error propagation from word segmentation to parsing." ></td>
	<td class="line x" title="34:154	We envision this technique to be general and widely applicable to many other sequence labeling tasks." ></td>
	<td class="line x" title="35:154	In the rest of the paper we first briefly review the popular classification-based method for word segmentation and tagging (Section 2), and then describe our idea of annotation adaptation (Section 3)." ></td>
	<td class="line x" title="36:154	We then discuss other relevant previous work including co-training and classifier combination (Section 4) before presenting our experimental results (Section 5)." ></td>
	<td class="line x" title="37:154	2 Segmentation and Tagging as Character Classification Before describing the adaptation algorithm, we give a brief introduction of the baseline character classification strategy for segmentation, as well as joint segmenation and tagging (henceforth Joint S&T)." ></td>
	<td class="line x" title="38:154	following our previous work (Jiang et al., 2008)." ></td>
	<td class="line x" title="39:154	Given a Chinese sentence as sequence of n characters: C1 C2  Cn where Ci is a character, word segmentation aims to split the sequence into m( n) words: C1:e1 Ce1+1:e2  Cem1+1:em where each subsequence Ci:j indicates a Chinese word spanning from characters Ci to Cj (both in523 Algorithm 1 Perceptron training algorithm." ></td>
	<td class="line x" title="40:154	1: Input: Training examples (xi,yi) 2: vector0 3: for t1  T do 4: for i1  N do 5: ziargmaxzGEN(xi)(xi,z)vector 6: if zinegationslash= yi then 7: vectorvector +(xi,yi)(xi,zi) 8: Output: Parameters vector clusive)." ></td>
	<td class="line x" title="41:154	While in Joint S&T, each word is further annotated with a POS tag: C1:e1/t1 Ce1+1:e2/t2  Cem1+1:em/tm where tk(k = 1m) denotes the POS tag for the word Cek1+1:ek." ></td>
	<td class="line x" title="42:154	2.1 Character Classification Method Xue and Shen (2003) describe for the first time the character classification approach for Chinese word segmentation, where each character is given a boundary tag denoting its relative position in a word." ></td>
	<td class="line x" title="43:154	In Ng and Low (2004), Joint S&T can also be treated as a character classification problem, where a boundary tag is combined with a POS tag in order to give the POS information of the word containing these characters." ></td>
	<td class="line x" title="44:154	In addition, Ng and Low (2004) find that, compared with POS tagging after word segmentation, Joint S&T can achieve higher accuracy on both segmentation and POS tagging." ></td>
	<td class="line x" title="45:154	This paper adopts the tag representation of Ng and Low (2004)." ></td>
	<td class="line x" title="46:154	For word segmentation only, there are four boundary tags:  b: the begin of the word  m: the middle of the word  e: the end of the word  s: a single-character word while for Joint S&T, a POS tag is attached to the tail of a boundary tag, to incorporate the word boundary information and POS information together." ></td>
	<td class="line x" title="47:154	For example, b-NN indicates that the character is the begin of a noun." ></td>
	<td class="line x" title="48:154	After all characters of a sentence are assigned boundary tags (or with POS postfix) by a classifier, the corresponding word sequence (or with POS) can be directly derived." ></td>
	<td class="line x" title="49:154	Take segmentation for example, a character assigned a tag s or a subsequence of words assigned a tag sequence bme indicates a word." ></td>
	<td class="line x" title="50:154	2.2 Training Algorithm and Features Now we will show the training algorithm of the classifier and the features used." ></td>
	<td class="line x" title="51:154	Several classification models can be adopted here, however, we choose the averaged perceptron algorithm (Collins, 2002) because of its simplicity and high accuracy." ></td>
	<td class="line x" title="52:154	It is an online training algorithm and has been successfully used in many NLP tasks, such as POS tagging (Collins, 2002), parsing (Collins and Roark, 2004), Chinese word segmentation (Zhang and Clark, 2007; Jiang et al., 2008), and so on." ></td>
	<td class="line x" title="53:154	Similar to the situation in other sequence labeling problems, the training procedure is to learn a discriminative model mapping from inputs x  X to outputs y  Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results." ></td>
	<td class="line x" title="54:154	Following Collins, we use a function GEN(x) enumerating the candidate results of an input x , a representationmapping each training example (x,y)  X  Y to a feature vector(x,y)  Rd, and a parameter vector vector  Rd corresponding to the feature vector." ></td>
	<td class="line x" title="55:154	For an input character sequence x, we aim to find an output F(x) that satisfies: F(x) = argmax yGEN(x) (x,y) vector (1) where(x,y)vector denotes the inner product of feature vector (x,y) and the parameter vector vector." ></td>
	<td class="line x" title="56:154	Algorithm 1 depicts the pseudo code to tune the parameter vector vector." ></td>
	<td class="line x" title="57:154	In addition, the averaged parameters technology (Collins, 2002) is used to alleviate overfitting and achieve stable performance." ></td>
	<td class="line x" title="58:154	Table 1 lists the feature template and corresponding instances." ></td>
	<td class="line x" title="59:154	Following Ng and Low (2004), the current considering character is denoted as C0, while the ith character to the left of C0 as Ci, and to the right as Ci." ></td>
	<td class="line x" title="60:154	There are additional two functions of which each returns some property of a character." ></td>
	<td class="line x" title="61:154	Pu() is a boolean function that checks whether a character is a punctuation symbol (returns 1 for a punctuation, 0 for not)." ></td>
	<td class="line x" title="62:154	T() is a multi-valued function, it classifies a character into four classifications: number, date, English letter and others (returns 1, 2, 3 and 4, respectively)." ></td>
	<td class="line x" title="63:154	3 Automatic Annotation Adaptation From this section, several shortened forms are adopted for representation inconvenience." ></td>
	<td class="line x" title="64:154	We use source corpus to denote the corpus with the annotation standard that we dont require, which is of 524 Feature Template Instances Ci (i =22) C2 =cjkBEC5, C1 =cjkA996, C0 =cjkC4EA, C1 =cjkB4FA, C2 = R CiCi+1 (i =21) C2C1 =cjkBEC5cjkA996, C1C0 =cjkA996cjkC4EA, C0C1 =cjkC4EAcjkB4FA, C1C2 =cjkB4FAR C1C1 C1C1 =cjkA996cjkB4FA Pu(C0) Pu(C0) = 0 T(C2)T(C1)T(C0)T(C1)T(C2) T(C2)T(C1)T(C0)T(C1)T(C2) = 11243 Table 1: Feature templates and instances from Ng and Low (Ng and Low, 2004)." ></td>
	<td class="line x" title="65:154	Suppose we are considering the third character cjkC4EA in cjkBEC5cjkA996cjkC4EAcjkB4FAR." ></td>
	<td class="line x" title="66:154	course the source of the adaptation, while target corpus denoting the corpus with the desired standard." ></td>
	<td class="line x" title="67:154	And correspondingly, the two annotation standards are naturally denoted as source standard and target standard, while the classifiers following the two annotation standards are respectively named as source classifier and target classifier, if needed." ></td>
	<td class="line x" title="68:154	Considering that word segmentation and Joint S&T can be conducted in the same character classification manner, we can design an unified standard adaptation framework for the two tasks, by taking the source classifiers classification result as the guide information for the target classifiers classification decision." ></td>
	<td class="line x" title="69:154	The following section depicts this adaptation strategy in detail." ></td>
	<td class="line x" title="70:154	3.1 General Adaptation Strategy In detail, in order to adapt knowledge from the source corpus, first, a source classifier is trained on it and therefore captures the knowledge it contains; then, the source classifier is used to classify the characters in the target corpus, although the classification result follows a standard that we dont desire; finally, a target classifier is trained on the target corpus, with the source classifiers classification result as additional guide information." ></td>
	<td class="line x" title="71:154	The training procedure of the target classifier automatically learns the regularity to transfer the source classifiers predication result from source standard to target standard." ></td>
	<td class="line x" title="72:154	This regularity is incorporated together with the knowledge learnt from the target corpus itself, so as to obtain enhanced predication accuracy." ></td>
	<td class="line x" title="73:154	For a given un-classified character sequence, the decoding is analogous to the training." ></td>
	<td class="line x" title="74:154	First, the character sequence is input into the source classifier to obtain an source standard annotated classification result, then it is input into the target classifier with this classification result as additional information to get the final result." ></td>
	<td class="line x" title="75:154	This coincides with the stacking method for combining dependency parsers (Martins et al., 2008; Nivre and McDonsource corpus train with normal features source classifier train with additional features target classifier target corpus source annotationclassification result Figure 2: The pipeline for training." ></td>
	<td class="line x" title="76:154	raw sentence source classifier source annotationclassification result target classifier target annotation classification result Figure 3: The pipeline for decoding." ></td>
	<td class="line oc" title="77:154	ald, 2008), and is also similar to the Pred baseline for domain adaptation in (Daume III and Marcu, 2006; Daume III, 2007)." ></td>
	<td class="line x" title="78:154	Figures 2 and 3 show the flow charts for training and decoding." ></td>
	<td class="line x" title="79:154	The utilization of the source classifiers classification result as additional guide information resorts to the introduction of new features." ></td>
	<td class="line x" title="80:154	For the current considering character waiting for classification, the most intuitive guide features is the source classifiers classification result itself." ></td>
	<td class="line x" title="81:154	However, our effort isnt limited to this, and more special features are introduced: the source classifiers classification result is attached to every feature listed in Table 1 to get combined guide features." ></td>
	<td class="line x" title="82:154	This is similar to feature design in discriminative dependency parsing (McDonald et al., 2005; Mc525 Donald and Pereira, 2006), where the basic features, composed of words and POSs in the context, are also conjoined with link direction and distance in order to obtain more special features." ></td>
	<td class="line x" title="83:154	Table 2 shows an example of guide features and basic features, where  = b  represents that the source classifier classifies the current character as b, the beginning of a word." ></td>
	<td class="line x" title="84:154	Such combination method derives a series of specific features, which helps the target classifier to make more precise classifications." ></td>
	<td class="line x" title="85:154	The parameter tuning procedure of the target classifier will automatically learn the regularity of using the source classifiers classification result to guide its decision making." ></td>
	<td class="line x" title="86:154	For example, if a current considering character shares some basic features in Table 2 and it is classified as b, then the target classifier will probably classify it as m. In addition, the training procedure of the target classifier also learns the relative weights between the guide features and the basic features, so that the knowledge from both the source corpus and the target corpus are automatically integrated together." ></td>
	<td class="line x" title="87:154	In fact, more complicated features can be adopted as guide information." ></td>
	<td class="line x" title="88:154	For error tolerance, guide features can be extracted from n-best results or compacted lattices of the source classifier; while for the best use of the source classifiers output, guide features can also be the classification results of several successive characters." ></td>
	<td class="line x" title="89:154	We leave them as future research." ></td>
	<td class="line x" title="90:154	4 Related Works Co-training (Sarkar, 2001) and classifier combination (Nivre and McDonald, 2008) are two technologies for training improved dependency parsers." ></td>
	<td class="line x" title="91:154	The co-training technology lets two different parsing models learn from each other during parsing an unlabelled corpus: one model selects some unlabelled sentences it can confidently parse, and provide them to the other model as additional training corpus in order to train more powerful parsers." ></td>
	<td class="line x" title="92:154	The classifier combination lets graph-based and transition-based dependency parsers to utilize the features extracted from each others parsing results, to obtain combined, enhanced parsers." ></td>
	<td class="line x" title="93:154	The two technologies aim to let two models learn from each other on the same corpora with the same distribution and annotation standard, while our strategy aims to integrate the knowledge in multiple corpora with different Baseline Features C2 =cjkC3C0 C1 =cjkB8B1 C0 =cjkD7DC C1 =cjkCDB3 C2 =cjkB7C3 C2C1 =cjkC3C0cjkB8B1 C1C0 =cjkB8B1cjkD7DC C0C1 =cjkD7DCcjkCDB3 C1C2 =cjkCDB3cjkB7C3 C1C1 =cjkB8B1cjkCDB3 Pu(C0) = 0 T(C2)T(C1)T(C0)T(C1)T(C2) = 44444 Guide Features  = b C2 =cjkC3C0   = b C1 =cjkB8B1   = b C0 =cjkD7DC   = b C1 =cjkCDB3   = b C2 =cjkB7C3   = b C2C1 =cjkC3C0cjkB8B1   = b C1C0 =cjkB8B1cjkD7DC   = b C0C1 =cjkD7DCcjkCDB3   = b C1C2 =cjkCDB3cjkB7C3   = b C1C1 =cjkB8B1cjkCDB3   = b Pu(C0) = 0   = b T(C2)T(C1)T(C0)T(C1)T(C2) = 44444   = b Table 2: An example of basic features and guide features of standard-adaptation for word segmentation." ></td>
	<td class="line x" title="94:154	Suppose we are considering the third character cjkD7DC in cjkC3C0cjkB8B1cjkD7DCcjkCDB3cjkB7C3cjkBBAA." ></td>
	<td class="line x" title="95:154	annotation-styles." ></td>
	<td class="line x" title="96:154	Gao et al.(2004) described a transformationbased converter to transfer a certain annotationstyle word segmentation result to another style." ></td>
	<td class="line x" title="98:154	They design some class-type transformation templates and use the transformation-based errordriven learning method of Brill (1995) to learn what word delimiters should be modified." ></td>
	<td class="line x" title="99:154	However, this converter need human designed transformation templates, and is hard to be generalized to POS tagging, not to mention other structure labeling tasks." ></td>
	<td class="line x" title="100:154	Moreover, the processing procedure is divided into two isolated steps, conversion after segmentation, which suffers from error propagation and wastes the knowledge in the corpora." ></td>
	<td class="line x" title="101:154	On the contrary, our strategy is automatic, generalizable and effective." ></td>
	<td class="line x" title="102:154	In addition, many efforts have been devoted to manual treebank adaptation, where they adapt PTB to other grammar formalisms, such as such as CCG and LFG (Hockenmaier and Steedman, 2008; Cahill and Mccarthy, 2007)." ></td>
	<td class="line x" title="103:154	However, they are heuristics-based and involve heavy human engineering." ></td>
	<td class="line x" title="104:154	526 5 Experiments Our adaptation experiments are conducted from Peoples Daily (PD) to Penn Chinese Treebank 5.0 (CTB)." ></td>
	<td class="line x" title="105:154	These two corpora are segmented following different segmentation standards and labeled with different POS sets (see for example Figure 1)." ></td>
	<td class="line x" title="106:154	PD is much bigger in size, with about 100K sentences, while CTB is much smaller, with only about 18K sentences." ></td>
	<td class="line x" title="107:154	Thus a classifier trained on CTB usually falls behind that trained on PD, but CTB is preferable because it also annotates tree structures, which is very useful for downstream applications like parsing and translation." ></td>
	<td class="line x" title="108:154	For example, currently, most Chinese constituency and dependency parsers are trained on some version of CTB, using its segmentation and POS tagging as the de facto standards." ></td>
	<td class="line x" title="109:154	Therefore, we expect the knowledge adapted from PD will lead to more precise CTB-style segmenter and POS tagger, which would in turn reduce the error propagation to parsing (and translation)." ></td>
	<td class="line x" title="110:154	Experiments adapting from PD to CTB are conducted for two tasks: word segmentation alone, and joint segmentation and POS tagging (Joint S&T)." ></td>
	<td class="line x" title="111:154	The performance measurement indicators for word segmentation and Joint S&T are balanced F-measure, F = 2PR/(P +R), a function of Precision P and Recall R. For word segmentation, P indicates the percentage of words in segmentation result that are segmented correctly, and R indicates the percentage of correctly segmented words in gold standard words." ></td>
	<td class="line x" title="112:154	For Joint S&T, P and R mean nearly the same except that a word is correctly segmented only if its POS is also correctly labelled." ></td>
	<td class="line x" title="113:154	5.1 Baseline Perceptron Classifier We first report experimental results of the single perceptron classifier on CTB 5.0." ></td>
	<td class="line x" title="114:154	The original corpus is split according to former works: chapters 271300 for testing, chapters 301325 for development, and others for training." ></td>
	<td class="line x" title="115:154	Figure 4 shows the learning curves for segmentation only and Joint S&T, we find all curves tend to moderate after 7 iterations." ></td>
	<td class="line x" title="116:154	The data splitting convention of other two corpora, Peoples Daily doesnt reserve the development sets, so in the following experiments, we simply choose the model after 7 iterations when training on this corpus." ></td>
	<td class="line x" title="117:154	The first 3 rows in each sub-table of Table 3 show the performance of the single perceptron 0.880 0.890 0.900 0.910 0.920 0.930 0.940 0.950 0.960 0.970 0.980  1  2  3  4  5  6  7  8  9  10 F measure number of iterations segmentation only segmentation in Joint S&T Joint S&T Figure 4: Averaged perceptron learning curves for segmentation and Joint S&T. Train on Test on Seg F1% JST F1% Word Segmentation PD PD 97.45  PD CTB 91.71  CTB CTB 97.35  PD  CTB CTB 98.15  Joint S&T PD PD 97.57 94.54 PD CTB 91.68  CTB CTB 97.58 93.06 PD  CTB CTB 98.23 94.03 Table 3: Experimental results for both baseline models and final systems with annotation adaptation." ></td>
	<td class="line x" title="118:154	PD  CTB means annotation adaptation from PD to CTB." ></td>
	<td class="line x" title="119:154	For the upper sub-table, items of JST F1 are undefined since only segmentation is performs." ></td>
	<td class="line x" title="120:154	While in the sub-table below, JST F1 is also undefined since the model trained on PD gives a POS set different from that of CTB." ></td>
	<td class="line x" title="121:154	models." ></td>
	<td class="line x" title="122:154	Comparing row 1 and 3 in the sub-table below with the corresponding rows in the upper sub-table, we validate that when word segmentation and POS tagging are conducted jointly, the performance for segmentation improves since the POS tags provide additional information to word segmentation (Ng and Low, 2004)." ></td>
	<td class="line x" title="123:154	We also see that for both segmentation and Joint S&T, the performance sharply declines when a model trained on PD is tested on CTB (row 2 in each sub-table)." ></td>
	<td class="line x" title="124:154	In each task, only about 92% F1 is achieved." ></td>
	<td class="line x" title="125:154	This obviously fall behind those of the models trained on CTB itself (row 3 in each sub-table), about 97% F1, which are used as the baselines of the following annotation adaptation experiments." ></td>
	<td class="line x" title="126:154	527 POS #Word #BaseErr #AdaErr ErrDec% AD 305 30 19 36.67 AS 76 0 0 BA 4 1 1 CC 135 8 8 CD 356 21 14 33.33 CS 6 0 0 DEC 137 31 23 25.81 DEG 197 32 37  DEV 10 0 0 DT 94 3 1 66.67 ETC 12 0 0 FW 1 1 1 JJ 127 41 44  LB 2 1 1 LC 106 3 2 33.33 M 349 18 4 77.78 MSP 8 2 1 50.00 NN 1715 151 126 16.56 NR 713 59 50 15.25 NT 178 1 2  OD 84 0 0 P 251 10 6 40.00 PN 81 1 1 PU 997 0 1  SB 2 0 0 SP 2 2 2 VA 98 23 21 08.70 VC 61 0 0 VE 25 1 0 100.00 VV 689 64 40 37.50 SUM 6821 213 169 20.66 Table 4: Error analysis for Joint S&T on the developing set of CTB." ></td>
	<td class="line x" title="127:154	#BaseErr and #AdaErr denote the count of words that cant be recalled by the baseline model and adapted model, respectively." ></td>
	<td class="line x" title="128:154	ErrDec denotes the error reduction of Recall." ></td>
	<td class="line x" title="129:154	5.2 Adaptation for Segmentation and Tagging Table 3 also lists the results of annotation adaptation experiments." ></td>
	<td class="line x" title="130:154	For word segmentation, the model after annotation adaptation (row 4 in upper sub-table) achieves an F-measure increment of 0.8 points over the baseline model, corresponding to an error reduction of 30.2%; while for Joint S&T, the F-measure increment of the adapted model (row 4 in sub-table below) is 1 point, which corresponds to an error reduction of 14%." ></td>
	<td class="line x" title="131:154	In addition, the performance of the adapted model for Joint S&T obviously surpass that of (Jiang et al., 2008), which achieves an F1 of 93.41% for Joint S&T, although with more complicated models and features." ></td>
	<td class="line x" title="132:154	Due to the obvious improvement brought by annotation adaptation to both word segmentation and Joint S&T, we can safely conclude that the knowledge can be effectively transferred from on anInput Type Parsing F1% gold-standard segmentation 82.35 baseline segmentation 80.28 adapted segmentation 81.07 Table 5: Chinese parsing results with different word segmentation results as input." ></td>
	<td class="line x" title="133:154	notation standard to another, although using such a simple strategy." ></td>
	<td class="line x" title="134:154	To obtain further information about what kind of errors be alleviated by annotation adaptation, we conduct an initial error analysis for Joint S&T on the developing set of CTB." ></td>
	<td class="line x" title="135:154	It is reasonable to investigate the error reduction of Recall for each word cluster grouped together according to their POS tags." ></td>
	<td class="line x" title="136:154	From Table 4 we find that out of 30 word clusters appeared in the developing set of CTB, 13 clusters benefit from the annotation adaptation strategy, while 4 clusters suffer from it." ></td>
	<td class="line x" title="137:154	However, the compositive error rate of Recall for all word clusters is reduced by 20.66%, such a fact invalidates the effectivity of annotation adaptation." ></td>
	<td class="line x" title="138:154	5.3 Contribution to Chinese Parsing We adopt the Chinese parser of Xiong et al.(2005), and train it on the training set of CTB 5.0 as described before." ></td>
	<td class="line x" title="140:154	To sketch the error propagation to parsing from word segmentation, we redefine the constituent span as a constituent subtree from a start character to a end character, rather than from a start word to a end word." ></td>
	<td class="line x" title="141:154	Note that if we input the gold-standard segmented test set into the parser, the F-measure under the two definitions are the same." ></td>
	<td class="line x" title="142:154	Table 5 shows the parsing accuracies with different word segmentation results as the parsers input." ></td>
	<td class="line x" title="143:154	The parsing F-measure corresponding to the gold-standard segmentation, 82.35, represents the oracle accuracy (i.e., upperbound) of parsing on top of automatic word segmention." ></td>
	<td class="line x" title="144:154	After integrating the knowledge from PD, the enhanced word segmenter gains an F-measure increment of 0.8 points, which indicates that 38% of the error propagation from word segmentation to parsing is reduced by our annotation adaptation strategy." ></td>
	<td class="line x" title="145:154	6 Conclusion and Future Works This paper presents an automatic annotation adaptation strategy, and conducts experiments on a classic problem: word segmentation and Joint 528 S&T. To adapt knowledge from a corpus with an annotation standard that we dont require, a classifier trained on this corpus is used to pre-process the corpus with the desired annotated standard, on which a second classifier is trained with the first classifiers predication results as additional guide information." ></td>
	<td class="line x" title="146:154	Experiments of annotation adaptation from PD to CTB 5.0 for word segmentation and POS tagging show that, this strategy can make effective use of the knowledge from the corpus with different annotations." ></td>
	<td class="line x" title="147:154	It obtains considerable F-measure increment, about 0.8 point for word segmentation and 1 point for Joint S&T, with corresponding error reductions of 30.2% and 14%." ></td>
	<td class="line x" title="148:154	The final result outperforms the latest work on the same corpus which uses more complicated technologies, and achieves the state-of-the-art." ></td>
	<td class="line x" title="149:154	Moreover, such improvement further brings striking Fmeasure increment for Chinese parsing, about 0.8 points, corresponding to an error propagation reduction of 38%." ></td>
	<td class="line x" title="150:154	In the future, we will continue to research on annotation adaptation for other NLP tasks which have different annotation-style corpora." ></td>
	<td class="line x" title="151:154	Especially, we will pay efforts to the annotation standard adaptation between different treebanks, for example, from HPSG LinGo Redwoods Treebank to PTB, or even from a dependency treebank to PTB, in order to obtain more powerful PTB annotation-style parsers." ></td>
	<td class="line x" title="152:154	Acknowledgement This project was supported by National Natural Science Foundation of China, Contracts 60603095 and 60736014, and 863 State Key Project No. 2006AA010108." ></td>
	<td class="line x" title="153:154	We are especially grateful to Fernando Pereira and the anonymous reviewers for pointing us to relevant domain adaption references." ></td>
	<td class="line x" title="154:154	We also thank Yang Liu and Haitao Mi for helpful discussions." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1087
Quadratic-Time Dependency Parsing for Machine Translation
Galley, Michel;Manning, Christopher D.;"></td>
	<td class="line x" title="1:217	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 773781, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:217	c2009 ACL and AFNLP Quadratic-Time Dependency Parsing for Machine Translation Michel Galley Computer Science Department Stanford University Stanford, CA 94305-9020 mgalley@cs.stanford.edu Christopher D. Manning Computer Science Department Stanford University Stanford, CA 94305-9010 manning@cs.stanford.edu Abstract Efficiency is a prime concern in syntactic MT decoding, yet significant developments in statistical parsing with respect to asymptotic efficiency havent yet been explored in MT. Recently, McDonald et al.(2005b) formalized dependency parsing as a maximum spanning tree (MST) problem, which can be solved in quadratic time relative to the length of the sentence." ></td>
	<td class="line x" title="4:217	They show that MST parsing is almost as accurate as cubic-time dependency parsing in the case of English, and that it is more accurate with free word order languages." ></td>
	<td class="line x" title="5:217	This paper applies MST parsing to MT, and describes how it can be integrated into a phrase-based decoder to compute dependency language model scores." ></td>
	<td class="line x" title="6:217	Ourresultsshowthataugmentingastate-ofthe-art phrase-based system with this dependency language model leads to significant improvements in TER (0.92%) and BLEU (0.45%) scores on five NIST Chinese-English evaluation test sets." ></td>
	<td class="line x" title="7:217	1 Introduction Hierarchical approaches to machine translation have proven increasingly successful in recent years (Chiang, 2005; Marcu et al., 2006; Shen et al., 2008), and often outperform phrase-based systems (Och and Ney, 2004; Koehn et al., 2003) on target-language fluency and adequacy." ></td>
	<td class="line x" title="8:217	However, their benefits generally come with high computational costs, particularly when chart parsing, such as CKY, is integrated with language models of high orders (Wu, 1996)." ></td>
	<td class="line x" title="9:217	Indeed, synchronous CFG parsing with m-grams runs in O(n3m) time, where n is the length of the sentence.1 Furthermore, synchronous CFG approaches often only marginally outperform the most com1The algorithmic complexity of (Wu, 1996) is O(n3+4(m1)), though Huang et al.(2005) present a more efficient factorization inspired by (Eisner and Satta, 1999) that yields an overall complexity of O(n3+3(m1)), i.e., O(n3m)." ></td>
	<td class="line x" title="11:217	In comparison, phrase-based decoding can run in linear time if a distortion limit is imposed." ></td>
	<td class="line x" title="12:217	Of course, this comparison holds only for approximate algorithms." ></td>
	<td class="line x" title="13:217	Since exact MT decoding is NP complete (Knight, 1999), there is no exact search algorithm for either phrase-based or syntactic MT that runs in polynomial time (unless P = NP)." ></td>
	<td class="line x" title="14:217	petitive phrase-based systems in large-scale experiments such as NIST evaluations.2 This lack of significant difference may not be completely surprising." ></td>
	<td class="line x" title="15:217	Indeed, researchers have shown that gigantic language models are key to state-ofthe-art performance (Brants et al., 2007), and the ability of phrase-based decoders to handle large-size, high-order language models with no consequence on asymptotic running time during decoding presents a compelling advantage over CKYdecoders,whosetimecomplexitygrowsprohibitively large with higher-order language models." ></td>
	<td class="line x" title="16:217	While context-free decoding algorithms (CKY, Earley, etc.) may sometimes appear too computationally expensive for high-end statistical machine translation, there are many alternative parsing algorithms that have seldom been explored in the machine translation literature." ></td>
	<td class="line x" title="17:217	The parsing literature presents faster alternatives for both phrasestructure and dependency trees, e.g., O(n) shiftreduce parsers and variants ((Ratnaparkhi, 1997; Nivre, 2003), inter alia)." ></td>
	<td class="line x" title="18:217	While deterministic parsers are often deemed inadequate for dealing with ambiguities of natural language, highly accurate O(n2) algorithms exist in the case of dependency parsing." ></td>
	<td class="line x" title="19:217	Building upon the theoretical work of (Chu and Liu, 1965; Edmonds, 1967), McDonald et al.(2005b) present a quadratic-time dependency parsing algorithm that is just 0.7% less accurate than full-fledged chart parsing (which, in thecaseofdependencyparsing,runsintimeO(n3) (Eisner, 1996))." ></td>
	<td class="line x" title="21:217	In this paper, we show how to exploit syntactic dependency structure for better machine translation, under the constraint that the depen2Results of the 2008 NIST Open MT evaluation (http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/ mt08_official_results_v0.html)revealthat, whilemanyof the best systems in the Chinese-English and Arabic-English tasks incorporate synchronous CFG models, score differences with the best phrase-based system were insignificantly small." ></td>
	<td class="line x" title="22:217	773 dency structure is built as a by-product of phrasebased decoding, without reliance on a dynamicprogramming or chart parsing algorithm such as CKY or Earley." ></td>
	<td class="line x" title="23:217	Adapting the approach of McDonald et al.(2005b) for machine translation, we incrementally build dependency structure left-toright in time O(n2) during decoding." ></td>
	<td class="line x" title="25:217	Most interestingly, the time complexity of non-projective dependency parsing remains quadratic as the order of the language model increases." ></td>
	<td class="line x" title="26:217	This provides a compelling advantage over previous dependency language models for MT (Shen et al., 2008),whichusea5-gramLMonlyduringreranking." ></td>
	<td class="line x" title="27:217	In our experiments, we build a competitive baseline (Koehn et al., 2007) incorporating a 5-gram LM trained on a large part of Gigaword and show that our dependency language model provides improvements on five different test sets, with an overall gain of 0.92 in TER and 0.45 in BLEU scores." ></td>
	<td class="line x" title="28:217	These results are found to be statistically very significant (p.01)." ></td>
	<td class="line x" title="29:217	2 Dependency parsing for machine translation In this section, we review dependency parsing formulated as a maximum spanning tree problem (McDonald et al., 2005b), which can be solved in quadratic time, and then present its adaptation and novel application to phrase-based decoding." ></td>
	<td class="line x" title="30:217	Dependency models have recently gained considerable interest in many NLP applications, including machine translation (Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008)." ></td>
	<td class="line x" title="31:217	Dependency structure provides several compelling advantages compared to other syntactic representations." ></td>
	<td class="line x" title="32:217	First, dependency links are close to the semantic relationships, which are more likely to be consistent across languages." ></td>
	<td class="line x" title="33:217	Indeed, Fox (2002) found inter-lingual phrasal cohesion to be greater than for a CFG when using a dependency representation, for which she found only 12.6% of head crossings and 9.2% modifier crossings." ></td>
	<td class="line x" title="34:217	Second, dependency trees contain exactly one node per word, which contributes to cutting down the search space during parsing: indeed, the task of the parser is merely to connect existing nodes rather than hypothesizing new ones." ></td>
	<td class="line x" title="35:217	Finally, dependency models are more flexible and account for (non-projective) head-modifier relations that CFG models fail to represent adequately, which is problematic with certain types of grammatical constructions and with free word order languages, who do you think they hired ? WP VBPRPVBPRPVBD.1 2 3 4 5 6 7 <root> <root> 0 Figure 1: A dependency tree with directed edges going from heads to modifiers." ></td>
	<td class="line x" title="36:217	The edge between who and hired causes this tree to be non-projective." ></td>
	<td class="line x" title="37:217	Such a head-modifier relationship is difficult to represent with a CFG, since all words directly or indirectly headed by hired (i.e., who, think, they, and hired) do not constitute a contiguous sequence of words." ></td>
	<td class="line x" title="38:217	as we will see later in this section." ></td>
	<td class="line x" title="39:217	The most standardly used algorithm for parsing with dependency grammars is presented in (Eisner, 1996; Eisner and Satta, 1999)." ></td>
	<td class="line x" title="40:217	It runs in time O(n3), where n is the length of the sentence." ></td>
	<td class="line x" title="41:217	Their algorithm exploits the special properties of dependency trees to reduce the worst-case complexity of bilexical parsing, which otherwise requires O(n4) for bilexical constituency-based parsing." ></td>
	<td class="line x" title="42:217	While it seems difficult to improve the asymptotic running time of the Eisner algorithm beyond what is presented in (Eisner and Satta, 1999), McDonald et al.(2005b) show O(n2)-time parsing is possible if trees are not required to be projective." ></td>
	<td class="line x" title="44:217	This relaxation entails that dependencies may cross each other rather than being required to be nested, as shown in Fig." ></td>
	<td class="line x" title="45:217	1." ></td>
	<td class="line x" title="46:217	More formally, a non-projective tree is any tree that does not satisfy the following definition of a projective tree: Definition." ></td>
	<td class="line x" title="47:217	Let x = x1xn be an input sentence, and let y be a rooted tree represented as a set in which each element (i, j) y is an ordered pair of word indices of x that defines a dependency relation between a head xi and a modifier xj." ></td>
	<td class="line x" title="48:217	By definition, the tree y is said to be projective if each dependency (i, j) satisfies the following property: each word in xi+1xj1 (if i < j) or in xj+1xi1 (if j < i) is a descendent of head word xi." ></td>
	<td class="line x" title="49:217	This relaxation is key to computational efficiency, since the parser does not need to keep track of whether dependencies assemble into contiguous spans." ></td>
	<td class="line x" title="50:217	It is also linguistically desirable in the case of free word order languages such as Czech, Dutch, and German." ></td>
	<td class="line x" title="51:217	Non-projective dependency structures are sometimes even needed for languages like English, e.g., in the case of the wh-movement shown in Fig." ></td>
	<td class="line x" title="52:217	1." ></td>
	<td class="line x" title="53:217	For languages 774 with relatively rigid word order such as English, there may be some concern that searching the space of non-projective dependency trees, which is considerably larger than the space of projective dependency trees, would yield poor performance." ></td>
	<td class="line x" title="54:217	Thatisnotthecase: dependencyaccuracyfornonprojective parsing is 90.2% for English (McDonald et al., 2005b), only 0.7% lower than a projective parser (McDonald et al., 2005a) that uses the same set of features and learning algorithm." ></td>
	<td class="line x" title="55:217	In the caseofdependencyparsingforCzech,(McDonald et al., 2005b) even outperforms projective parsing, and was one of the top systems in the CoNLL-06 shared task in multilingual dependency parsing." ></td>
	<td class="line x" title="56:217	2.1 O(n2)-time dependency parsing for MT We now formalize weighted non-projective dependency parsing similarly to (McDonald et al., 2005b) and then describe a modified and more efficient version that can be integrated into a phrasebased decoder." ></td>
	<td class="line x" title="57:217	Given the single-head constraint, parsing an input sentence x = (x0,x1,,xn) is reduced to labeling each word xj with an index i identifying its head word xi." ></td>
	<td class="line x" title="58:217	We include the dummy root symbol x0 =rootso that each word can be a modifier." ></td>
	<td class="line x" title="59:217	We score each dependency relation using a standard linear model s(i, j) = f(i, j) (1) whose weight vector  is trained using MIRA (Crammer and Singer, 2003) to optimize dependency parsing accuracy (McDonald et al., 2005a)." ></td>
	<td class="line x" title="60:217	As is commonly the case in statistical parsing, the score of the full tree is decomposed as the sum of the score of all edges: s(x,y) =  (i,j)y f(i, j) (2) When there is no need to ensure projectivity, one can independently select the highest scoring edge (i, j)foreachmodifierxj,yetwegenerallywantto ensurethattheresultingstructureisatree, i.e.,that itdoesnotcontainanycirculardependencies." ></td>
	<td class="line x" title="61:217	This optimization problem is a known instance of the maximum spanning tree (MST) problem." ></td>
	<td class="line x" title="62:217	In our case, the graph is directedindeed, the equality s(i, j) = s(j,i) is generally not true and would be linguisticallyaberrantsotheproblemconstitutes an instance of the less-known MST problem for directed graphs." ></td>
	<td class="line x" title="63:217	This problem is solved with the Chu-Liu-Edmonds(CLE)algorithm(ChuandLiu, 1965; Edmonds, 1967)." ></td>
	<td class="line x" title="64:217	Formally, we represent the graph G = (V,E) with a vertex set V = x ={x0,,xn} and a set of directed edges E = [0,n][1,n], in which each edge (i, j), representing the dependency xixj, is assigned a score s(i, j)." ></td>
	<td class="line x" title="65:217	Finding the spanning tree yE rooted at x0 that maximizes s(x,y) as defined in Equation 2 has a straightforward solution in O(n2log(n)) time for dense graphs such as G, though Tarjan (1977) shows that the problem can be solved in O(n2)." ></td>
	<td class="line x" title="66:217	Hence, non-projective dependency parsing is solved in quadratic time." ></td>
	<td class="line x" title="67:217	The main idea behind the CLE algorithm is to first greedily select for each word xj the incoming edge (i, j) with highest score, then to successively repeat the following two steps: (a) identify a loop in the graph, and if there is none, halt; (b) contract the loop into a single vertex, and update scores for edges coming in and out of the loop." ></td>
	<td class="line x" title="68:217	Onceallloopshavebeeneliminated,thealgorithm maps back the maximum spanning tree of the contracted graph onto the original graph G, and it can beshownthatthisyieldsaspanningtreethatisoptimal with respect to G and s (Georgiadis, 2003)." ></td>
	<td class="line x" title="69:217	The greedy approach of selecting the highest scoring edge (i, j) for each modifier xj can easily be applied left-to-right during phrase-based decoding, which proceeds in the same order." ></td>
	<td class="line x" title="70:217	For each hypothesis expansion, our decoder generates the following information for the new hypothesis h:  a partial translation x;  a coverage set of input words c;  a translation score ." ></td>
	<td class="line x" title="71:217	In the case of non-projective dependency parsing, we need to maintain additional information for each word xj of the partial translation x:  a predicted POS tag tj;  a dependency score sj." ></td>
	<td class="line x" title="72:217	Dependency scores sj are initialized to ." ></td>
	<td class="line x" title="73:217	Each time a new word is added to a partial hypothesis, the decoder executes the routine shown in Table 1." ></td>
	<td class="line x" title="74:217	To avoid cluttering the pseudo-code, we make here the simplifying assumption that each hypothesis expansion adds exactly one word, though the real implementation supports the case of phrases of any length." ></td>
	<td class="line x" title="75:217	Line 3 determines whether the translation hypothesis is complete, in which case it explicitly builds the graph G and 775 Decoding: hypothesis expansion step." ></td>
	<td class="line x" title="76:217	1." ></td>
	<td class="line x" title="77:217	Inferer generates new hypothesis h = (x,c,) 2." ></td>
	<td class="line x" title="78:217	j|x|1 3." ></td>
	<td class="line x" title="79:217	tjtagger(xj3,,xj) 4." ></td>
	<td class="line x" title="80:217	if complete(c) 5." ></td>
	<td class="line x" title="81:217	Chu-Liu-Edmonds(h) 6." ></td>
	<td class="line x" title="82:217	else 7." ></td>
	<td class="line x" title="83:217	for i = 1 to j 8." ></td>
	<td class="line x" title="84:217	sj = max(sj,s(i, j)) 9." ></td>
	<td class="line x" title="85:217	si = max(si,s(j,i)) Table 1: Hypothesis expansion with dependency scoring." ></td>
	<td class="line x" title="86:217	finds the maximum spanning tree." ></td>
	<td class="line x" title="87:217	Note that it is impractical to identify loops each time a new word is added to a translation hypothesis, since this requires explicitly storing the dense graph G, which would require an O(n2) copy operation during each hypothesis expansion; this would of course increase time and space complexity (the max operation in lines 8 and 9 only keeps the current best scoring edges)." ></td>
	<td class="line x" title="88:217	If there is any loop, the dependency score is adjusted in the last hypothesis expansion." ></td>
	<td class="line x" title="89:217	In practice, we delay the computation of dependency scores involvingword xj until tagtj+1 isgenerated, sincedependencyparsingaccuracyis particularly low (0.8%) when the next tag is unknown." ></td>
	<td class="line x" title="90:217	We found that dependency scores with or withoutloopeliminationaregenerallycloseandhighly correlated, and that MT performance without final loop removal was about the same (generally less than 0.2% BLEU)." ></td>
	<td class="line x" title="91:217	While it seems that loopy graphs are undesirable when the goal is to obtain a syntactic analysis, that is not necessarily the case when one just needs a language modeling score." ></td>
	<td class="line x" title="92:217	2.2 Features for dependency parsing In our experiments, we use sets of features that are similar to the ones used in the McDonald parser, though we make a key modification that yields an asymptotic speedup that ensures a genuine O(n2) running time." ></td>
	<td class="line x" title="93:217	The three feature sets that were used in our experiments are shown in Table 2." ></td>
	<td class="line x" title="94:217	We write h-word, h-pos, m-word, m-pos to refer to head and modifier words and POS tags, and append a numerical value to shift the word offset either to the left or to the right (e.g., h-pos+1 is the POS to the right of the head word)." ></td>
	<td class="line x" title="95:217	We use the symbolto represent feature conjunctions." ></td>
	<td class="line x" title="96:217	Each feature in the table has a distinct identifier, so that, e.g., the POS features Unigram features: h-word, h-pos, h-wordh-pos, m-word, m-pos, m-wordm-pos Bigram features: h-wordm-word, h-posm-pos, h-wordh-posm-word, h-wordh-posm-pos, m-wordm-posh-word, m-wordm-posh-pos, h-wordh-posm-wordm-pos Adjacent POS features: h-posh-pos+1m-pos1m-pos, h-posh-pos+1m-posm-pos+1, h-pos1h-posm-pos1m-pos, h-pos1h-posm-posm-pos+1 In-between POS features: if i < j: h-posh-pos+km-pos k[i,min(i+5, j)] h-posm-poskm-pos k[max(i, j5), j] if i > j: m-posm-pos+kh-pos k[ j,min(j+5,i)] m-posh-poskh-pos k[max(j,i5),i] Table 2: Features for dependency parsing." ></td>
	<td class="line x" title="97:217	It is quite similar to the McDonald (2005a) feature set, except that it does not include the set of all POS tags that appear between each candidate head-modifier pair (i, j)." ></td>
	<td class="line x" title="98:217	This modification is essential inordertomakeourparserrunintrueO(n2)time,asopposed to (McDonald et al., 2005b)." ></td>
	<td class="line x" title="99:217	SOURCE IDS GENRE SENTENCES EnglishCTB 050325 newswire 3027 EnglishATB all newswire 13628 OntoNotes all broadcastnews 14056 WSJ 0221 financialnews 39832 Total 70543 Table 3: Characteristics of our training data." ></td>
	<td class="line x" title="100:217	The second column identifies documents and sections selected for training." ></td>
	<td class="line x" title="101:217	h-pos are all distinct from m-pos features.3 The primary difference between our feature sets and the ones of McDonald et al. is that their set of in between POS features includes the set of all tags appearing between each pair of words." ></td>
	<td class="line x" title="102:217	Extracting all these tags takes time O(n) for any arbitrary pair (i, j)." ></td>
	<td class="line x" title="103:217	Since i and j are both free variables, feature computation in (McDonald et al., 2005b) takes time O(n3), even though parsing itself takes O(n2) time." ></td>
	<td class="line x" title="104:217	To make our parser genuinely O(n2), we modified the set of in-between POS features in two ways." ></td>
	<td class="line x" title="105:217	First, we restrict extraction of in-between POS tags to those words that appear within a window of five words relative to either the head or the modifier." ></td>
	<td class="line x" title="106:217	While this change alone ensures that feature extraction is now O(1) for each word pair, this causes a fairly high drop of performance (dependency accuracy 3In addition to these basic features, we follow McDonald in conjoining most features with two extra pieces of information: a boolean variable indicating whether the modifier attaches to the left or to the right, and the binned distance between the two words." ></td>
	<td class="line x" title="107:217	776 ALGORITHM TIME SETUP TRAINING TESTING ACCURACY Projective O(n3) Parsing WSJ(02-21) WSJ(23) 90.60 Chu-Liu-EdmondsO(n3) Parsing WSJ(02-21) WSJ(23) 89.64 Chu-Liu-EdmondsO(n2) Parsing WSJ(02-21) WSJ(23) 89.32 Localclassifier O(n2) Parsing WSJ(02-21) WSJ(23) 89.15 Projective O(n3) MT CTB(050-325) CTB(001-049)86.33 Chu-Liu-EdmondsO(n3) MT CTB(050-325) CTB(001-049)85.68 Chu-Liu-EdmondsO(n2) MT CTB(050-325) CTB(001-049)85.43 Localclassifier O(n2) MT CTB(050-325) CTB(001-049)85.22 Projective O(n3) MT CTB(050-325),WSJ(02-21),ATB,OntoNotesCTB(001-049)87.40(**) Chu-Liu-EdmondsO(n3) MT CTB(050-325),WSJ(02-21),ATB,OntoNotesCTB(001-049)86.79 Chu-Liu-EdmondsO(n2) MT CTB(050-325),WSJ(02-21),ATB,OntoNotesCTB(001-049)86.45(*) Localclassifier O(n2) MT CTB(050-325),WSJ(02-21),ATB,OntoNotesCTB(001-049)86.29 Table 4: Dependency parsing experiments on test sentences of any length." ></td>
	<td class="line x" title="108:217	The projective parsing algorithm is the one implementedasin(McDonaldetal., 2005a), whichisknownasoneofthetopperformingdependencyparsersforEnglish." ></td>
	<td class="line x" title="109:217	The O(n3) non-projective parser of (McDonald et al., 2005b) is slightly more accurate than our version, though ours runs in O(n2) time." ></td>
	<td class="line x" title="110:217	Local classifier refers to non-projective dependency parsing without removing loops as a post-processing step." ></td>
	<td class="line x" title="111:217	The result marked with (*) identifies the parser used for our MT experiments, which is only about 1% less accurate than a state-of-the-art dependency parser (**)." ></td>
	<td class="line x" title="112:217	on our test was down 0.9%)." ></td>
	<td class="line x" title="113:217	To make our genuinely O(n2) parser almost as accurate as the nonprojective parser of McDonald et al., we conjoin each in-between POS with its position relative to (i, j)." ></td>
	<td class="line x" title="114:217	This relatively simple change reduces the drop in accuracy to only 0.34%.4 3 Dependency parsing experiments In this section, we compare the performance of our parsing model to the ones of McDonald et al. SinceourMTtestsetsincludenewswire, web, and audio, we trained our parser on different genres." ></td>
	<td class="line x" title="115:217	Our training data includes newswire from the English translation treebank (LDC2007T02) and the English-Arabic Treebank (LDC2006T10), which are respectively translations of sections of the Chinese treebank (CTB) and Arabic treebank (ATB)." ></td>
	<td class="line x" title="116:217	We also trained the parser on the broadcastnews treebank available in the OntoNotes corpus (LDC2008T04), and added sections 02-21 of the WSJ Penn treebank." ></td>
	<td class="line x" title="117:217	Documents 001-040 of the English CTB data were set aside to constitute a test set for newswire texts." ></td>
	<td class="line x" title="118:217	Our other test set is the standard Section 23 of the Penn treebank." ></td>
	<td class="line x" title="119:217	The splitsandamountsofdatausedfortrainingaredisplayed in Table 3." ></td>
	<td class="line x" title="120:217	Parsing experiments are shown in Table 4." ></td>
	<td class="line x" title="121:217	We 4We need to mention some practical considerations that make feature computation fast enough for MT. Most features are precomputed before actual decoding." ></td>
	<td class="line x" title="122:217	All target-language words to appear during beam search can be determined in advance, and all their unigram feature scores are precomputed." ></td>
	<td class="line x" title="123:217	For features conditioned on both head and modifier, scores are cached whenever possible." ></td>
	<td class="line x" title="124:217	The only features that are not cached are the ones that include contextual POS tags, since their miss rate is relatively high." ></td>
	<td class="line x" title="125:217	distinguish two experimental conditions: Parsing and MT. For Parsing, sentences are cased and tokenization abides to the PTB segmentation as used in the Penn treebank version 3." ></td>
	<td class="line x" title="126:217	For the MT setting, texts are all lower case, and tokenization was changed to improve machine translation (e.g., most hyphenated words were split)." ></td>
	<td class="line x" title="127:217	For this setting, we also had to harmonize the four treebanks." ></td>
	<td class="line x" title="128:217	The most crucial modification was to add NP internal bracketing to the WSJ (Vadas and Curran, 2007), since the three other treebanks contain that information." ></td>
	<td class="line x" title="129:217	Treebanks were also transformed to be consistent with MT tokenization." ></td>
	<td class="line x" title="130:217	We evaluate MT parsing models on CTB rather than on WSJ, since CTB contains newswire and is thus more representative of MT evaluation conditions." ></td>
	<td class="line x" title="131:217	To obtain part-of-speech tags, we use a state-of-the-art maximum-entropy (CMM) tagger (Toutanova et al., 2003)." ></td>
	<td class="line x" title="132:217	In the Parsing setting, we use its best configuration, which reaches a tagging accuracy of 97.25% on standard WSJ test data." ></td>
	<td class="line x" title="133:217	In the MT setting, we need to use a less effective tagger, since we cannot afford to perform Viterbi inference as a by-product of phrase-based decoding." ></td>
	<td class="line x" title="134:217	Hence, we use a simpler tagging model that assigns tag ti to word xi by only using features of words xi3xi, and that does not condition any decision based on any preceding or next tags (ti1, etc.)." ></td>
	<td class="line x" title="135:217	Its performance is 95.02% on the WSJ, and 95.30% on the English CTB." ></td>
	<td class="line x" title="136:217	Additional experiments reveal two main contributing factors to this drop on WSJ: tagging uncased texts reduces tagging accuracy by about 1%, and using only wordbased features further reduces it by 0.6%." ></td>
	<td class="line x" title="137:217	Table 4 shows that the accuracy of our truly 777 O(n2) parser is only .25% to .34% worse than the O(n3) implementation of (McDonald et al., 2005b).5 Compared to the state-of-the-art projective parser as implemented in (McDonald et al., 2005a), performance is 1.28% lower on WSJ, but only 0.95% when training on all our available data and using the MT setting." ></td>
	<td class="line x" title="138:217	Overall, we believe that the drop of performance is a reasonable price to pay considering the computational constraints imposedbyintegratingthedependencyparserintoan MT decoder." ></td>
	<td class="line x" title="139:217	The table also shows a gain of more than 1% in dependency accuracy by adding ATB, OntoNotes, and WSJ to the English CTB training set." ></td>
	<td class="line x" title="140:217	The four sources were assigned non-uniform weights: we set the weight of the CTB data to be 10 times largerthantheothercorpora, whichseemstowork best in our parsing experiments." ></td>
	<td class="line x" title="141:217	While this improvement of 1% may seem relatively small considering that the amount of training data is more than 20 times larger in the latter case, it is quite consistent with previous findings in domain adaptation, which is known to be a difficult task." ></td>
	<td class="line oc" title="142:217	For example, (Daume III, 2007) shows that training a learning algorithm on the weighted union of different data sets (which is basically what we did) performs almost as well as more involved domain adaptation approaches." ></td>
	<td class="line x" title="143:217	4 Machine translation experiments In our experiments, we use a re-implementation of the Moses phrase-based decoder (Koehn et al., 2007)." ></td>
	<td class="line x" title="144:217	We use the standard features implemented almost exactly as in Moses: four translation features (phrase-based translation probabilities and lexically-weighted probabilities), word penalty, phrase penalty, linear distortion, and language model score." ></td>
	<td class="line x" title="145:217	We also incorporated the lexicalized reordering features of Moses, in order to experimentwithabaselinethatisstrongerthanthe default Moses configuration." ></td>
	<td class="line x" title="146:217	The language pair for our experiments is Chinese-to-English." ></td>
	<td class="line x" title="147:217	The training data consists of about 28 million English words and 23.3 million 5Note that our results on WSJ are not exactly the same as those reported in (McDonald et al., 2005b), since we used slightly different head finding rules." ></td>
	<td class="line x" title="148:217	To extract dependencies from treebanks, we used the LTH Penn Converter (http:// nlp.cs.lth.se/pennconverter/), which extracts dependencies that are almost identical to those used for the CoNLL-2008 Shared Task." ></td>
	<td class="line x" title="149:217	We constrain the converter not to use functional tags found in the treebanks, in order to make it possible to use automatically parsed texts (i.e., perform selftraining) in future work." ></td>
	<td class="line x" title="150:217	Chinese words drawn from various news parallel corpora distributed by the Linguistic Data Consortium (LDC)." ></td>
	<td class="line x" title="151:217	In order to provide experiments comparable to previous work, we used the same corpora as (Wang et al., 2007): LDC2002E18, LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E8, and LDC2006G05." ></td>
	<td class="line x" title="152:217	Chinese words were automatically segmented with a conditional random field (CRF) classifier (Chang et al., 2008) that conforms to the Chinese Treebank (CTB) standard." ></td>
	<td class="line x" title="153:217	Inordertotrainacompetitivebaselinegivenour computational resources, we built a large 5-gram language model using the Xinhua and AFP sections of the Gigaword corpus (LDC2007T40) in addition to the target side of the parallel data." ></td>
	<td class="line x" title="154:217	This data represents a total of about 700 million words." ></td>
	<td class="line x" title="155:217	We manually removed documents of Gigaword that were released during periods that overlap with those of our development and test sets." ></td>
	<td class="line x" title="156:217	The language model was smoothed with the modified Kneser-Ney algorithm as implemented in (Stolcke, 2002), and we only kept 4-grams and 5-grams that occurred at least three times in the training data.6 For tuning and testing, we use the official NIST MTevaluationdataforChinesefrom2002to2008 (MT02 to MT08), which all have four English references for each input sentence." ></td>
	<td class="line x" title="157:217	We used the 1082 sentences of MT05 for tuning and all other sets for testing." ></td>
	<td class="line x" title="158:217	Parametertuningwasdonewithminimum error rate training (Och, 2003), which was used to maximize BLEU (Papineni et al., 2001)." ></td>
	<td class="line x" title="159:217	Since MERT is prone to search errors, especially with large numbers of parameters, we ran each tuning experimentthreetimeswith differentinitialconditions." ></td>
	<td class="line x" title="160:217	We used n-best lists of size 200 and a beam size of 200." ></td>
	<td class="line x" title="161:217	In the final evaluations, we report results using both TER (Snover et al., 2006) and the original BLEU metric as described in (Papineni et al., 2001)." ></td>
	<td class="line x" title="162:217	All our evaluations are performed on uncased texts." ></td>
	<td class="line x" title="163:217	The results for our translation experiments are shown in Table 5." ></td>
	<td class="line x" title="164:217	We compared two systems: one with the set of features described earlier in this section." ></td>
	<td class="line x" title="165:217	The second system incorporates one additionalfeature,whichisthedependencylanguage 6We found that sections of Gigaword other than Xinhua and AFP provide almost no improvement in our experiments." ></td>
	<td class="line x" title="166:217	By leaving aside the other sections, we were able to increase the order of the language model to 5-gram and perform relatively little pruning." ></td>
	<td class="line x" title="167:217	This LM required 16GB of RAM during training." ></td>
	<td class="line x" title="168:217	778 BLEU[%] DEP." ></td>
	<td class="line x" title="169:217	LM MT05(tune) MT02 MT03 MT04 MT06 MT08 no 33.42 33.38 33.13 36.21 32.16 24.83 yes 34.19(+.77**) 33.85(+.47) 33.73(+.6*) 36.67(+.46*) 32.84(+.68**) 24.91(+.08) TER[%] DEP." ></td>
	<td class="line x" title="170:217	LM MT05(tune) MT02 MT03 MT04 MT06 MT08 no 57.41 58.07 57.32 56.09 57.24 61.96 yes 56.27(1.14**) 57.15(.92**) 56.09(1.23**) 55.30(.79**) 56.05(1.19**) 61.41(.55*) MT05(tune) MT02 MT03 MT04 MT06 MT08 Sentences 1082 878 919 1788 1664 1357 Table 5: MT experiments with and without a dependency language model." ></td>
	<td class="line x" title="171:217	We use randomization tests (Riezler and Maxwell, 2005) to determine significance: differences marked with a (*) are significant at the p.05 level, and those marked as (**) are significant at the p.01 level." ></td>
	<td class="line x" title="172:217	model score computed with the dependency parsing algorithm described in Section 2." ></td>
	<td class="line x" title="173:217	We used the dependency model trained on the English CTB and ATB treebank, WSJ, and OntoNotes." ></td>
	<td class="line x" title="174:217	We see that the Moses decoder with integrated dependency language model systematically outperforms the Moses baseline." ></td>
	<td class="line x" title="175:217	For BLEU evaluations, differences are significant in four out of six cases, and in the case of TER, all differences are significant." ></td>
	<td class="line x" title="176:217	Regarding the small difference in BLEU scores on MT08, we would like to point out that tuning on MT05 and testing on MT08 had a rather adverse effect with respect to translation length: while the two systems are relatively close in terms of BLEU scores (24.83 and 24.91, respectively), the dependency LM provides a much bigger gain when evaluated with BLEU precision (27.73 vs. 28.79), i.e., by ignoring the brevity penalty." ></td>
	<td class="line x" title="177:217	On the other hand, the difference on MT08 is significant in terms of TER." ></td>
	<td class="line x" title="178:217	Table 6 provides experimental results on the NISTtestdata(excludingthetuningsetMT05)for each of the three genres: newswire, web data, and speech (broadcast news and conversation)." ></td>
	<td class="line x" title="179:217	The last column displays results for all test sets combined." ></td>
	<td class="line x" title="180:217	Results do not suggest any noticeable difference between genres, and the dependency language model provides significant gains on all genres, despite the fact that this model was primarily trained on news data." ></td>
	<td class="line x" title="181:217	We wish to emphasize that our positive results are particularly noteworthy because they are achieved over a baseline incorporating a competitive 5-gram language model." ></td>
	<td class="line x" title="182:217	As is widely acknowledged in the speech community, it can be difficult to outperform high-order n-gram models in large-scale experiments." ></td>
	<td class="line x" title="183:217	Finally, we quantified the effective running time of our phrase-based decoder with and without our dependency language BLEU[%] DEP." ></td>
	<td class="line x" title="184:217	LM newswire web speech all no 32.86 21.75 36.88 32.29 yes 33.19 22.64 37.51 32.74 (+0.33) (+0.89) (+0.63) (+0.45) TER[%] DEP." ></td>
	<td class="line x" title="185:217	LM newswire web speech all no 57.73 62.64 55.16 58.02 yes 56.73 61.97 54.26 57.10 (1) (0.67) (0.9) (0.92) newswire web speech all Sentences 4006 1149 1451 6606 Table 6: Test set performances on MT02-MT04 and MT06MT08, where the data was broken down by genre." ></td>
	<td class="line x" title="186:217	Given the large amount of test data involved in this table, all these results are statistically highly significant (p.01)." ></td>
	<td class="line x" title="187:217	10 20 30 40 50 60 70 80 900 20 40 60 80 100 120 140 160 sentence length seconds   depLM baseline Figure2: Runningtimeofourphrase-baseddecoderwithandwithout quadratic-time dependency LM scoring." ></td>
	<td class="line x" title="188:217	model using MT05 (Fig." ></td>
	<td class="line x" title="189:217	2)." ></td>
	<td class="line x" title="190:217	In both settings, we selected the best tuned model, which yield the performance shown in the first column of Table 5." ></td>
	<td class="line x" title="191:217	Our decoder was run on an AMD Opteron Processor 2216 with 16GB of memory, and without resortingtoanyrescoringmethodsuchascubepruning." ></td>
	<td class="line x" title="192:217	InthecaseofEnglishtranslationsof40words and shorter, the baseline system took 6.5 seconds per sentence, whereas the dependency LM system spent 15.6 seconds per sentence, i.e., 2.4 times the baseline running time." ></td>
	<td class="line x" title="193:217	In the case of translations 779 longerthan40words, averagespeedswererespectively 17.5 and 59.5 seconds per sentence, i.e., the dependency was only 3.4 times slower.7 5 Related work Perhaps due to the high computational cost of synchronous CFG decoding, there have been various attempts to exploit syntactic knowledge and hierarchical structure in other machine translation experiments that do not require chart parsing." ></td>
	<td class="line x" title="194:217	Using a reranking framework, Och et al.(2004) found that various types of syntactic features provided only minor gains in performance, suggesting that phrase-based systems (Och and Ney, 2004) should exploit such information during rather than after decoding." ></td>
	<td class="line x" title="196:217	Wang et al.(2007) sidestep the need to operate large-scale word order changes during decoding (and thus lessening the need for syntactic decoding) by rearranging input words in the training data to match the syntactic structure of the target language." ></td>
	<td class="line x" title="198:217	Finally, Birch et al.(2007) exploit factored phrase-based translation models to associate each word with a supertag, which contains most of the information needed to build a full parse." ></td>
	<td class="line x" title="200:217	When combined with a supertag n-gram languagemodel,ithelpsenforcegrammaticalconstraints on the target side." ></td>
	<td class="line x" title="201:217	There have been various attempts to reduce the computational expense of syntactic decoding, including multi-pass decoding approaches (Zhang and Gildea, 2008; Petrov et al., 2008) and rescoring approaches (Huang and Chiang, 2007)." ></td>
	<td class="line x" title="202:217	In the latter paper, Huang and Chiang introduce rescoring methods named cube pruning and cube growing, which first use a baseline decoder (either synchronous CFG or a phrase-based system) and no LM to generate a hypergraph, and then rescoring this hypergraph with a language model." ></td>
	<td class="line x" title="203:217	Huang and Chiang show significant speed increases with little impact on translation quality." ></td>
	<td class="line x" title="204:217	We believe that their approach is orthogonal (and possibly complementary) to our work, since our paper proposes a new model for fully-integrated decoding that increases MT performance, and does not rely on rescoring." ></td>
	<td class="line x" title="205:217	7We note that our Java-based decoder is research rather thanindustrial-strengthcodeandthatitcouldbesubstantially optimized." ></td>
	<td class="line x" title="206:217	Hence, we think the reader should pay more attention to relative speed differences between the two systems rather than absolute timings." ></td>
	<td class="line x" title="207:217	6 Conclusion and future work In this paper, we presented a non-projective dependency parser whose time-complexity of O(n2) improves upon the cubic time implementation of (McDonald et al., 2005b), and does so with little loss in dependency accuracy (.25% to .34%)." ></td>
	<td class="line x" title="208:217	Since this parser does not need to enforce projectivity constraints, it can easily be integrated into a phrase-based decoder during search (rather than during rescoring)." ></td>
	<td class="line x" title="209:217	We use dependency scores as an extra feature in our MT experiments, and found that our dependency model provides significant gains over a competitive baseline that incorporates a large 5-gram language model (0.92% TER and 0.45% BLEU absolute improvements)." ></td>
	<td class="line x" title="210:217	We plan to pursue other research directions using dependency models discussed in this paper." ></td>
	<td class="line x" title="211:217	While we use a dependency language model to exemplify the use of hierarchical structure within phrase based decoders, we could extend this work toincorporatedependencyfeaturesofbothsourceand target side." ></td>
	<td class="line x" title="212:217	Since parsing of the source is relatively inexpensive compared to the target side, it would be relatively easy to condition headmodifier dependencies not only on the two target words, but also on their corresponding Chinese words and their relative positions in the Chinese tree." ></td>
	<td class="line x" title="213:217	This would enable the decoder to capture syntactic reordering without requiring trees to be isomorphic or even projective." ></td>
	<td class="line x" title="214:217	It would also be interesting to apply these models to target languages that have free word order, which would presumably benefit more from the flexibility of non-projective dependency models." ></td>
	<td class="line x" title="215:217	Acknowledgements The authors wish to thank the anonymous reviewers for their helpful comments on an earlier draft of this paper, and Daniel Cer for his implementation of Phrasal, a phrase-based decoder similar to Moses." ></td>
	<td class="line x" title="216:217	This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM." ></td>
	<td class="line x" title="217:217	The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1114
Multi-Task Transfer Learning for Weakly-Supervised Relation Extraction
Jiang, Jing;"></td>
	<td class="line x" title="1:210	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 10121020, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:210	c2009 ACL and AFNLP Multi-Task Transfer Learning for Weakly-Supervised Relation Extraction Jing Jiang School of Information Systems Singapore Management University 80 Stamford Road, Singapore 178902 jingjiang@smu.edu.sg Abstract Creating labeled training data for relation extraction is expensive." ></td>
	<td class="line x" title="3:210	In this paper, we study relation extraction in a special weakly-supervised setting when we have only a few seed instances of the target relation type we want to extract but we also have a large amount of labeled instances of other relation types." ></td>
	<td class="line x" title="4:210	Observing that different relation types can share certain common structures, we propose to use a multi-task learning method coupled with human guidance to address this weakly-supervised relation extraction problem." ></td>
	<td class="line x" title="5:210	The proposed framework models the commonality among different relation types through a shared weight vector, enables knowledge learned from the auxiliary relation types to be transferred to the target relation type, and allows easy control of the tradeoff between precision and recall." ></td>
	<td class="line x" title="6:210	Empirical evaluation on the ACE 2004 data set shows that the proposed method substantially improves over two baseline methods." ></td>
	<td class="line x" title="7:210	1 Introduction Relation extraction is the task of detecting and characterizing semantic relations between entities from free text." ></td>
	<td class="line x" title="8:210	Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008)." ></td>
	<td class="line x" title="9:210	However, supervised learning heavily relies on a sufficient amount of labeled data for training, which is not always available in practice due to the labor-intensive nature of human annotation." ></td>
	<td class="line x" title="10:210	This problem is especially serious for relation extraction because the types of relations to be extracted are highly dependent on the application domain." ></td>
	<td class="line x" title="11:210	For example, when working in the financial domain we may be interested in the employment relation, but when moving to the terrorism domain we now may be interested in the ethnic and ideology affiliation relation, and thus have to create training data for the new relation type." ></td>
	<td class="line x" title="12:210	However, is the old training data really useless?" ></td>
	<td class="line x" title="13:210	Inspired by recent work on transfer learning and domain adaptation, in this paper, we study how we can leverage labeled data of some old relation types to help the extraction of a new relation type in a weakly-supervised setting, where only a few seed instances of the new relation type are available." ></td>
	<td class="line oc" title="14:210	While transfer learning was proposed more than a decade ago (Thrun, 1996; Caruana, 1997), its application in natural language processing is still a relatively new territory (Blitzer et al., 2006; Daume III, 2007; Jiang and Zhai, 2007a; Arnold et al., 2008; Dredze and Crammer, 2008), and its application in relation extraction is still unexplored." ></td>
	<td class="line x" title="15:210	Our idea of performing transfer learning is motivated by the observation that different relation types share certain common syntactic structures, which can possibly be transferred from the old types to the new type." ></td>
	<td class="line x" title="16:210	We therefore propose to use a general multi-task learning framework in which classification models for a number of related tasks are forced to share a common model component and trained together." ></td>
	<td class="line x" title="17:210	By treating classification of different relation types as related tasks, the learning framework can naturally model the common syntactic structures among different relation types in a principled manner." ></td>
	<td class="line x" title="18:210	It also allows us to introduce human guidance in separating the common model component from the type-specific components." ></td>
	<td class="line x" title="19:210	The framework naturally transfers the knowledge learned from the old relation types to the new relation type and helps improve the recall of the relation extractor." ></td>
	<td class="line x" title="20:210	We also exploit ad1012 ditional human knowledge about the entity type constraints on the relation arguments, which can usually be derived from the definition of a relation type." ></td>
	<td class="line x" title="21:210	Imposing these constraints further improves the precision of the final relation extractor." ></td>
	<td class="line x" title="22:210	Empirical evaluation on the ACE 2004 data set shows that our proposed method largely outperforms two baseline methods, improving the average F1 measure from 0.1532 to 0.4132 when only 10 seed instances of the new relation type are used." ></td>
	<td class="line x" title="23:210	2 Related work Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods." ></td>
	<td class="line x" title="24:210	Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction." ></td>
	<td class="line x" title="26:210	We systematically explored the feature space for relation extraction (Jiang and Zhai, 2007b) . Kernel methods allow a large set of features to be used without being explicitly extracted." ></td>
	<td class="line x" title="27:210	A number of relation extraction kernels have been proposed, including dependency tree kernels (Culotta and Sorensen, 2004), shortest dependency path kernels (Bunescu and Mooney, 2005) and more recently convolution tree kernels (Zhang et al., 2006; Qian et al., 2008)." ></td>
	<td class="line x" title="28:210	However, in both feature-based and kernel-based studies, availability of sufficient labeled training data is always assumed." ></td>
	<td class="line x" title="29:210	Chen et al.(2006) explored semi-supervised learning for relation extraction using label propagation, which makes use of unlabeled data." ></td>
	<td class="line x" title="31:210	Zhou et al.(2008) proposed a hierarchical learning strategy to address the data sparseness problem in relation extraction." ></td>
	<td class="line x" title="33:210	They also considered the commonality among different relation types, but compared with our work, they had a different problem setting and a different way of modeling the commonality." ></td>
	<td class="line x" title="34:210	Banko and Etzioni (2008) studied open domain relation extraction, for which they manually identified several common relation patterns." ></td>
	<td class="line x" title="35:210	In contrast, our method obtains common patterns through statistical learning." ></td>
	<td class="line x" title="36:210	Xu et al.(2008) studied the problem of adapting a rule-based relation extraction system to new domains, but the types of relations to be extracted remain the same." ></td>
	<td class="line x" title="38:210	Transfer learning aims at transferring knowledge learned from one or a number of old tasks to a new task." ></td>
	<td class="line x" title="39:210	Domain adaptation is a special case of transfer learning where the learning task remains the same but the distribution of data changes." ></td>
	<td class="line x" title="40:210	There has been an increasing amount of work on transfer learning and domain adaptation in natural language processing recently." ></td>
	<td class="line x" title="41:210	Blitzer et al.(2006) proposed a structural correspondence learning method for domain adaptation and applied it to part-of-speech tagging." ></td>
	<td class="line oc" title="43:210	Daume III (2007) proposed a simple feature augmentation method to achieve domain adaptation." ></td>
	<td class="line x" title="44:210	Arnold et al.(2008) used a hierarchical prior structure to help transfer learning and domain adaptation for named entity recognition." ></td>
	<td class="line x" title="46:210	Dredze and Crammer (2008) proposed an online method for multi-domain learning and adaptation." ></td>
	<td class="line x" title="47:210	Multi-task learning is another learning paradigm in which multiple related tasks are learned simultaneously in order to achieve better performance for each individual task (Caruana, 1997; Evgeniou and Pontil, 2004)." ></td>
	<td class="line x" title="48:210	Although it was not originally proposed to transfer knowledge to a particular new task, it can be naturally used to achieve this goal because it models the commonality among tasks, which is the knowledge that should be transferred to a new task." ></td>
	<td class="line x" title="49:210	In our work, transfer learning is done through a multi-task learning framework similar to Evgeniou and Pontil (2004)." ></td>
	<td class="line x" title="50:210	3 Task definition Our study is conducted using data from the Automatic Content Extraction (ACE) program1." ></td>
	<td class="line x" title="51:210	We focus on extracting binary relation instances between two relation arguments occurring in the same sentence." ></td>
	<td class="line x" title="52:210	Some example relation instances and their corresponding relation types as defined by ACE can be found in Table 1." ></td>
	<td class="line x" title="53:210	We consider the following weakly-supervised problem setting." ></td>
	<td class="line x" title="54:210	We are interested in extracting instances of a target relation type T , but this relation type is only specified by a small set of seed instances." ></td>
	<td class="line x" title="55:210	We may possibly have some additional knowledge about the target type not in the form of labeled instances." ></td>
	<td class="line x" title="56:210	For example, we may be given the entity type restrictions on the two relation arguments." ></td>
	<td class="line x" title="57:210	In addition to such limited information about the target relation type, we also have a large amount of labeled instances for K auxiliary relation types A1,,AK." ></td>
	<td class="line x" title="58:210	Our goal is to learn a relation extractor for T , leveraging all the data and information we have." ></td>
	<td class="line x" title="59:210	1http://projects.ldc.upenn.edu/ace/ 1013 Syntactic Pattern Relation Instance Relation Type (Subtype) arg-2 arg-1 Arab leaders OTHER-AFF (Ethnic) his father PER-SOC (Family) South Jakarta Prosecution Office GPE-AFF (Based-In) arg-1 of arg-2 leader of a minority government EMP-ORG (Employ-Executive) the youngest son of ex-director Suharto PER-SOC (Family) the Socialist Peoples Party of Montenegro GPE-AFF (Based-In) arg-1 [verb] arg-2 Yemen [sent] planes to Baghdad ART (User-or-Owner) his wife [had] three young children PER-SOC (Family) Jody Scheckter [paced] Ferrari to both victories EMP-ORG (Employ-Staff) Table 1: Examples of similar syntactic structures across different relation types." ></td>
	<td class="line x" title="60:210	The head words of the first and the second arguments are shown in italic and bold, respectively." ></td>
	<td class="line x" title="61:210	Before introducing our transfer learning solution, let us first briefly explain our basic classification approach and the features we use, as well as two baseline solutions." ></td>
	<td class="line x" title="62:210	3.1 Feature configuration We treat relation extraction as a classification problem." ></td>
	<td class="line x" title="63:210	Each pair of entities within a single sentence is considered a candidate relation instance, and the task becomes predicting whether or not each candidate is a true instance of T . We use feature-based logistic regression classifiers." ></td>
	<td class="line x" title="64:210	Following our previous work (Jiang and Zhai, 2007b), we extract features from a sequence representation and a parse tree representation of each relation instance." ></td>
	<td class="line x" title="65:210	Each node in the sequence or the parse tree is augmented by an argument tag that indicates whether the node subsumes arg-1, arg2, both or neither." ></td>
	<td class="line x" title="66:210	Nodes that represent the arguments are also labeled with the entity type, subtype and mention type as defined by ACE." ></td>
	<td class="line x" title="67:210	Based on the findings of Qian et al.(2008), we trim the parse tree of a relation instance so that it contains only the most essential components." ></td>
	<td class="line x" title="69:210	We extract unigram features (consisting of a single node) and bigram features (consisting of two connected nodes) from the graphic representations." ></td>
	<td class="line x" title="70:210	An example of the graphic representation of a relation instance is shown in Figure 1 and some features extracted from this instance are shown in Table 2." ></td>
	<td class="line x" title="71:210	This feature configuration gives state-of-the-art performance (F1 = 0.7223) on the ACE 2004 data set in a standard setting with sufficient data for training." ></td>
	<td class="line x" title="72:210	3.2 Baseline solutions We consider two baseline solutions to the weaklysupervised relation extraction problem." ></td>
	<td class="line x" title="73:210	In the first NP NPB 3 PP1 leaderNN PER ofIN governmentNN ORG NPB 1 0 2 2 2Figure 1: The combined sequence and parse treerepresentation of the relation instance leader of a minority government. The non-essential nodes for a and for minority are removed based on the algorithm from Qian et al.(2008)." ></td>
	<td class="line x" title="75:210	Feature Explanation ORG2 arg-2 is an ORG entity." ></td>
	<td class="line x" title="76:210	of0 government2 arg-2 is government and follows the word of. NP3  PP2 There is a noun phrase containing both arguments, with arg-2 contained in a prepositional phrase inside the noun phrase." ></td>
	<td class="line x" title="77:210	Table 2: Examples of unigram and bigram features extracted from Figure 1." ></td>
	<td class="line x" title="78:210	baseline, we use only the few seed instances of the target relation type together with labeled negative relation instances (i.e. pairs of entities within the same sentence but having no relation) to train a binary classifier." ></td>
	<td class="line x" title="79:210	In the second baseline, we take the union of the positive instances of both the target relation type and the auxiliary relation types as our positive training set, and together with the negative instances we train a binary classifier." ></td>
	<td class="line x" title="80:210	Note that the second baseline method essentially learns 1014 a classifier for any relation type." ></td>
	<td class="line x" title="81:210	Another existing solution to weakly-supervised learning problems is semi-supervised learning, e.g. bootstrapping." ></td>
	<td class="line x" title="82:210	However, because our proposed transfer learning method can be combined with semi-supervised learning, here we do not include semi-supervised learning as a baseline." ></td>
	<td class="line x" title="83:210	4 A multi-task transfer learning solution We now present a multi-task transfer learning solution to the weakly-supervised relation extraction problem, which makes use of the labeled data from the auxiliary relation types." ></td>
	<td class="line x" title="84:210	4.1 Syntactic similarity between relation types To see why the auxiliary relation types may help the identification of the target relation type, let us first look at how different relation types may be related and even similar to each other." ></td>
	<td class="line x" title="85:210	Based on our inspection of a sample of the ACE data, we find that instances of different relation types can share certain common syntactic structures." ></td>
	<td class="line x" title="86:210	For example, the syntactic pattern arg-1 of arg-2 strongly indicates that there exists some relation between the two arguments, although the nature of the relation may be well dependent on the semantic meanings of the two arguments." ></td>
	<td class="line x" title="87:210	More examples are shown in Table 1." ></td>
	<td class="line x" title="88:210	This observation suggests that some of the syntactic patterns learned from the auxiliary relation types may be transferable to the target relation type, making it easier to learn the target relation type and thus alleviating the insufficient training data problem with the target type." ></td>
	<td class="line x" title="89:210	How can we incorporate this desired knowledge transfer process into our learning method?" ></td>
	<td class="line x" title="90:210	While one can make explicit use of these general syntactic patterns in a rule-based relation extraction system, here we restrict our attention to feature-based linear classifiers." ></td>
	<td class="line x" title="91:210	We note that in feature-based linear classifiers, a useful syntactic pattern is translated into large weights for features related to the syntactic pattern." ></td>
	<td class="line x" title="92:210	For example, if arg-1 of arg-2 is a useful pattern, in the learned linear classifier we should have relatively large weights for features such as the word of occurs before arg-2 or a preposition occurs before arg-2, or even more complex features such as there is a prepositional phrase containing arg-2 attached to arg-1. It is the weights of these generally useful features that are transferable from the auxiliary relation types to the target relation type." ></td>
	<td class="line x" title="93:210	4.2 Statistical learning model As we have discussed, we want to force the linear classifiers for different relation types to share their model weights for those features that are related to the common syntactic patterns." ></td>
	<td class="line x" title="94:210	Formally, we consider the following statistical learning model." ></td>
	<td class="line x" title="95:210	Let k denote the weight vector of the linear classifier that separates positive instances of auxiliary type Ak from negative instances, and let T denote a similar weight vector for the target type T . If different relation types are totally unrelated, these weight vectors should also be independent of each other." ></td>
	<td class="line x" title="96:210	But because we observe similar syntactic structures across different relation types, we now assume that these weight vectors are related through a common component : T = T +, k = k + for k = 1,,K. If we assume that only weights of certain general features can be shared between different relation types, we can force certain dimensions of  to be 0." ></td>
	<td class="line x" title="97:210	We express this constraint by introducing a matrix F and setting F = 0." ></td>
	<td class="line x" title="98:210	Here F is a square matrix with all entries set to 0 except that Fi,i = 1 if we want to force i = 0." ></td>
	<td class="line x" title="99:210	Now we can learn these weight vectors in a multi-task learning framework." ></td>
	<td class="line x" title="100:210	Let x represent the feature vector of a candidate relation instance, and y  {+1,1} represent a class label." ></td>
	<td class="line x" title="101:210	Let DT = {(xTi ,yTi )}NTi=1 denote the set of labeled instances for the target type T ." ></td>
	<td class="line x" title="102:210	(Note that the number of positive instances in DT is very small.)" ></td>
	<td class="line x" title="103:210	And let Dk = {(xki,yki )}Nki=1 denote the labeled instances for the auxiliary type Ak." ></td>
	<td class="line x" title="104:210	We learn the optimal weight vectors {k}Kk=1, T and  by optimizing the following objective function: parenleftbigg {k}Kk=1, T, parenrightbigg = argmin {k},T ,,F=0 bracketleftBigg L(DT,T +) + Ksummationdisplay k=1 L(Dk,k +) +TbardblTbardbl2 + Ksummationdisplay k=1 kbardblkbardbl2 +bardblbardbl2 bracketrightBigg ." ></td>
	<td class="line x" title="105:210	(1) 1015 The objective function follows standard empirical risk minimization with regularization." ></td>
	<td class="line x" title="106:210	Here L(D,) is the aggregated loss of labeling x with y for all (x,y) in D, using weight vector ." ></td>
	<td class="line x" title="107:210	In logistic regression models, the loss function is the negative log likelihood, that is, L(D,) =  summationdisplay (x,y)D logp(y|x,), p(y|x,) = exp(y x)summationtext yprime{+1,1}exp(yprime x) . T , k and  are regularization parameters." ></td>
	<td class="line x" title="108:210	By adjusting their values, we can control the degree of weight sharing among the relation types." ></td>
	<td class="line x" title="109:210	The larger the ratio T/ (or k/) is, the more we believe that the model for T (or Ak) should conform to the common model, and the smaller the type-specific weight vector T (or k) will be." ></td>
	<td class="line oc" title="110:210	The model presented above is based on our previous work (Jiang and Zhai, 2007c), which bears the same spirit of some other recent work on multitask learning (Ando and Zhang, 2005; Evgeniou and Pontil, 2004; Daume III, 2007)." ></td>
	<td class="line x" title="111:210	It is general for any transfer learning problem with auxiliary labeled data from similar tasks." ></td>
	<td class="line x" title="112:210	Here we are mostly interested in the models applicability and effectiveness on the relation extraction problem." ></td>
	<td class="line x" title="113:210	4.3 Feature separation Recall that we impose a constraint F = 0 when optimizing the objective function." ></td>
	<td class="line x" title="114:210	This constraint gives us the freedom to force only the weights of a subset of the features to be shared among different relation types." ></td>
	<td class="line x" title="115:210	A remaining question is how to set this matrix F, that is, how to determine the set of general features to use." ></td>
	<td class="line x" title="116:210	We propose two ways of setting this matrix F. Automatically setting F One way is to fix the number of non-zero entries in  to be a pre-defined number H of general features, and allow F to change during the optimization process." ></td>
	<td class="line x" title="117:210	This can be done by repeating the following two steps until F converges: 1." ></td>
	<td class="line x" title="118:210	Fix F, and optimize the objective function as in Equation (1)." ></td>
	<td class="line x" title="119:210	2." ></td>
	<td class="line x" title="120:210	Fix parenleftbigT + parenrightbig and parenleftbigk + parenrightbig, and search for T , {k} and  that minimizes parenleftbigTbardblTbardbl2 +summationtext K k=1  kbardblkbardbl2 + bardblbardbl2parenrightbig, subject to the constraint that at most H entries of  are nonzero." ></td>
	<td class="line x" title="121:210	Human guidance Another way to select the general features is to follow some guidance from human knowledge." ></td>
	<td class="line x" title="122:210	Recall that in Section 4.1 we find that the commonality among different relation types usually lies in the syntactic structures between the two arguments." ></td>
	<td class="line x" title="123:210	This observation gives some intuition about how to separate general features from typespecific features." ></td>
	<td class="line x" title="124:210	In particular, here we consider two hypotheses regarding the generality of different kinds of features." ></td>
	<td class="line x" title="125:210	Argument word features: We hypothesize that the head words of the relation arguments are more likely to be strong indicators of specific relation types rather than any relation type." ></td>
	<td class="line x" title="126:210	For example, if an argument has the head word sister, it strongly indicates a family relation." ></td>
	<td class="line x" title="127:210	We refer to the set of features that contain any head word of an argument as arg-word features." ></td>
	<td class="line x" title="128:210	Entity type features: We hypothesize that the entity types and subtypes of the relation arguments are also more likely to be associated with specific relation types." ></td>
	<td class="line x" title="129:210	For example, arguments that are location entities may be strongly correlated with physical proximity relations." ></td>
	<td class="line x" title="130:210	We refer to the set of features that contain the entity type or subtype of an argument as arg-NE features." ></td>
	<td class="line x" title="131:210	We hypothesize that the arg-word and arg-NE features are type-specific and therefore should be excluded from the set of general features." ></td>
	<td class="line x" title="132:210	We can force the weights of these hypothesized typespecific features to be 0 in the shared weight vector , i.e. we can set the matrix F to achieve this feature separation." ></td>
	<td class="line x" title="133:210	Combined method We can also combine the automatic way of setting F with human guidance." ></td>
	<td class="line x" title="134:210	Specifically, we still follow the first automatic procedure to choose general features, but we then filter out any hypothesized type-specific feature from the set of general features chosen by the automatic procedure." ></td>
	<td class="line x" title="135:210	4.4 Imposing entity type constraints Finally, we consider how we can exploit additional human knowledge about the target relation type T to further improve the classifier." ></td>
	<td class="line x" title="136:210	We note that usually when a relation type is defined, we often have strong preferences or even hard constraints on the types of entities that can possibly be the two relation arguments." ></td>
	<td class="line x" title="137:210	These type constraints can help us 1016 Target Type T BL BL-A TL-auto TL-guide TL-comb TL-NE P 0.0000 0.1692 0.2920 0.2934 0.3325 0.5056 Physical R 0.0000 0.0848 0.1696 0.1722 0.2383 0.2316 F 0.0000 0.1130 0.2146 0.2170 0.2777 0.3176 Personal P 1.0000 0.0804 0.1005 0.3069 0.3214 0.6412 /Social R 0.0386 0.1708 0.1598 0.7245 0.7686 0.7631 F 0.0743 0.1093 0.1234 0.4311 0.4533 0.6969 Employment P 0.9231 0.3561 0.5230 0.5428 0.5973 0.7145 /Membership R 0.0075 0.1850 0.2617 0.2648 0.3632 0.3601 /Subsidiary F 0.0148 0.2435 0.3488 0.3559 0.4518 0.4789 AgentP 0.8750 0.0603 0.1813 0.1825 0.1835 0.1967 Artifact R 0.0343 0.2353 0.6471 0.6225 0.6422 0.6373 F 0.0660 0.0960 0.2833 0.2822 0.2854 0.3006 PER/ORG P 0.8889 0.0838 0.1510 0.1592 0.1667 0.1844 Affiliation R 0.0567 0.4965 0.6950 0.8369 0.8794 0.8723 F 0.1067 0.1434 0.2481 0.2676 0.2802 0.3045 GPE P 1.0000 0.2530 0.3904 0.3604 0.3560 0.5824 Affiliation R 0.0077 0.4509 0.6416 0.5992 0.6166 0.6127 F 0.0153 0.3241 0.4854 0.4501 0.4513 0.5972 P 1.0000 0.0298 0.0503 0.0471 0.1370 0.1370 Discourse R 0.0036 0.0789 0.1075 0.1147 0.3477 0.3477 F 0.0071 0.0433 0.0685 0.0668 0.1966 0.1966 P 0.8124 0.1475 0.2412 0.2703 0.2992 0.4231 Average R 0.0212 0.2432 0.3832 0.4764 0.5509 0.5464 F 0.0406 0.1532 0.2532 0.2958 0.3423 0.4132 Table 3: Comparison of different methods on ACE 2004 data set." ></td>
	<td class="line x" title="138:210	P, R and F stand for precision, recall and F1, respectively." ></td>
	<td class="line x" title="139:210	remove some false positive instances." ></td>
	<td class="line x" title="140:210	We therefore manually identify the entity type constraints for each target relation type based on the definition of the relation type given in the ACE annotation guidelines, and impose these type constraints as a final refinement step on top of the predicted positive instances." ></td>
	<td class="line x" title="141:210	5 Experiments 5.1 Data set and experiment setup We used the ACE 2004 data set to evaluate our proposed methods." ></td>
	<td class="line x" title="142:210	There are seven relation types defined in ACE 2004." ></td>
	<td class="line x" title="143:210	After data cleaning, we obtained 4290 positive instances among 48614 candidate relation instances." ></td>
	<td class="line x" title="144:210	We took each relation type as the target type and used the remaining types as auxiliary types." ></td>
	<td class="line x" title="145:210	This gave us seven sets of experiments." ></td>
	<td class="line x" title="146:210	In each set of experiments for a single target relation type, we randomly divided all the data into five subsets, and used each subset for testing while using the other four subsets for training, i.e. each experiment was repeated five times with different training and test sets." ></td>
	<td class="line x" title="147:210	Each time, we removed most of the positive instances of the target type from the training set except only a small number S of seed instances." ></td>
	<td class="line x" title="148:210	This gave us the weakly-supervised setting." ></td>
	<td class="line x" title="149:210	We kept all the positive instances of the target type in the test set." ></td>
	<td class="line x" title="150:210	In order to concentrate on the classification accuracy for the target relation type, we removed the positive instances of the auxiliary relation types from the test set, although in practice we need to extract these auxiliary relation instances using learned classifiers for these relation types." ></td>
	<td class="line x" title="151:210	5.2 Comparison of different methods We first show the comparison of our proposed multi-task transfer learning methods with the two baseline methods described in Section 3.2." ></td>
	<td class="line x" title="152:210	The performance on each target relation type and the average performance across seven types are shown in Table 3." ></td>
	<td class="line x" title="153:210	BL refers to the first baseline and BLA refers to the second baseline which uses auxil1017 T 100 1000 10000 P 0.6265 0.3162 0.2992 R 0.1170 0.3959 0.5509 F 0.1847 0.2983 0.3423 Table 4: The average performance of TL-comb with different T ." ></td>
	<td class="line x" title="154:210	(k = 104 and  = 1.)" ></td>
	<td class="line x" title="155:210	iary relation instances." ></td>
	<td class="line x" title="156:210	The four TL methods are all based on the multi-task transfer learning framework." ></td>
	<td class="line x" title="157:210	TL-auto sets F automatically within the optimization problem itself." ></td>
	<td class="line x" title="158:210	TL-guide chooses all features except arg-word and arg-NE features as general features and sets F accordingly." ></td>
	<td class="line x" title="159:210	TL-comb combines TL-auto and TL-guide, as described in Section 4.3." ></td>
	<td class="line x" title="160:210	Finally, TL-NE builds on top of TLcomb and uses the entity type constraints to refine the predictions." ></td>
	<td class="line x" title="161:210	In this set of experiments, the number of seed instances for each target relation type was set to 10." ></td>
	<td class="line x" title="162:210	The parameters were set to their optimal values (T = 104, k = 104,  = 1, and H = 500)." ></td>
	<td class="line x" title="163:210	As we can see from the table, first of all, BL generally has high precision but very low recall." ></td>
	<td class="line x" title="164:210	BL-A performs better than BL in terms of F1 because it gives better recall." ></td>
	<td class="line x" title="165:210	However, BL-A still cannot achieve as high recall as the TL methods." ></td>
	<td class="line x" title="166:210	This is probably because the model learned by BLA still focuses more on type-specific features for each relation type rather than on the commonly useful general features, and therefore does not help much in classifying the target relation type." ></td>
	<td class="line x" title="167:210	The four TL methods all outperform the two baseline methods." ></td>
	<td class="line x" title="168:210	TL-comb performs better than both TL-auto and TL-guide, which shows that while we can either choose general features automatically by the learning algorithm or manually with human knowledge, it is more effective to combine human knowledge with the multi-task learning framework." ></td>
	<td class="line x" title="169:210	Not surprisingly, TL-NE improves the precision over TL-comb without hurting the recall much." ></td>
	<td class="line x" title="170:210	Ideally, TL-NE should not decrease recall if the type constraints are strictly observed in the data." ></td>
	<td class="line x" title="171:210	We find that it is not always the case with the ACE data, leading to the small decrease of recall from TL-comb to TL-NE." ></td>
	<td class="line x" title="172:210	5.3 The effect of T Let us now take a look at the effect of using different T . As we can see from Table 4, smaller T gives higher precision while larger T gives  0.1  0.15  0.2  0.25  0.3  0.35  0.4  0.45  0.5  100  1000  10000 avg F1 H TL-comb TL-auto BL-A Figure 2: Performance of TL-comb and TL-auto as H changes." ></td>
	<td class="line x" title="173:210	higher recall." ></td>
	<td class="line x" title="174:210	These results make sense because the larger T is, the more we penalize large weights of T . As a result, the model for the target type is forced to conform to the shared model  and prevented from overfitting the few seed target instances." ></td>
	<td class="line x" title="175:210	T is therefore a useful parameter to help us control the tradeoff between precision and recall for the target type." ></td>
	<td class="line x" title="176:210	While varying k also gives similar effect for typeAk, we found that setting k to smaller values would not helpT because in this case the auxiliary relation instances would be used more for training the type-specific component k rather than the common component ." ></td>
	<td class="line x" title="177:210	5.4 Sensitivity of H Another parameter in the multi-task transfer learning framework is the number of general features H, i.e. the number of non-zero entries in the shared weight vector ." ></td>
	<td class="line x" title="178:210	To see how the performance may vary as H changes, we plot the performance of TL-comb and TL-auto in terms of the average F1 across the seven target relation types, with H ranging from 100 to 50000." ></td>
	<td class="line x" title="179:210	As we can see in Figure 2, the performance is relatively stable, and always above BL-A." ></td>
	<td class="line x" title="180:210	This suggests that the performance of TL-comb and TL-auto is not very sensitive to the value of H. 5.5 Hypothesized type-specific features In Section 4.3, we showed two sets of hypothesized type-specific features, namely, arg-word features and arg-NE features." ></td>
	<td class="line x" title="181:210	We also experimented with each set separately to see whether both sets are useful." ></td>
	<td class="line x" title="182:210	The comparison is shown in Table 5." ></td>
	<td class="line x" title="183:210	As we can see, using either set of typespecific features in either TL-guide or TL-comb can improve the performance over BL-A, but the 1018 arg-word arg-NE union TL-guide 0.2095 0.2983 0.2958 TL-comb 0.2215 0.3331 0.3423 BL-A 0.1532 Table 5: Average F1 using different hypothesized type-specific features." ></td>
	<td class="line x" title="184:210	0  0.1  0.2  0.3  0.4  0.5  0.6  10  100  1000 avg F1 S TL-NE (104) TL-NE (102) BL BL-A Figure 3: Performance of TL-NE, BL and BL-A as the number of seed instances S of the target type increases." ></td>
	<td class="line x" title="185:210	(H = 500." ></td>
	<td class="line x" title="186:210	T was set to 104 and 102)." ></td>
	<td class="line x" title="187:210	arg-NE features are probably more type-specific than arg-word features because they give better performance." ></td>
	<td class="line x" title="188:210	Using the union of the two sets is still the best for TL-comb." ></td>
	<td class="line x" title="189:210	5.6 Changing the number of seed instances Finally, we compare TL-NE with BL and BL-A when the number of seed instances increases." ></td>
	<td class="line x" title="190:210	We set S from 5 up to 1000." ></td>
	<td class="line x" title="191:210	When S is large, the problem becomes more like traditional supervised learning, and our setting of T = 104 is no longer optimal because we are now not afraid of overfitting the large set of seed target instances." ></td>
	<td class="line x" title="192:210	Therefore we also included another TL-NE experiment with T set to 102." ></td>
	<td class="line x" title="193:210	The comparison of the performance is shown in Figure 3." ></td>
	<td class="line x" title="194:210	We see that as S increases, both BL and BL-Acatch up, and BL overtakes BL-A when S is sufficiently large because BL uses positive training examples only from the target type." ></td>
	<td class="line x" title="195:210	Overall, TL-NE still outperforms the two baselines in most of the cases over the wide range of values of S, but the optimal value for T decreases as S increases, as we have suspected." ></td>
	<td class="line x" title="196:210	The results show that if T is set appropriately, our multi-task transfer learning method is robust and advantageous over the baselines under both the weakly-supervised setting and the traditional supervised setting." ></td>
	<td class="line x" title="197:210	6 Conclusions and future work In this paper, we applied multi-task transfer learning to solve a weakly-supervised relation extraction problem, leveraging both labeled instances of auxiliary relation types and human knowledge including hypotheses on feature generality and entity type constraints." ></td>
	<td class="line x" title="198:210	In the multi-task learning framework that we introduced, different relation types are treated as different but related tasks that are learned together, with the common structures among the relation types modeled by a shared weight vector." ></td>
	<td class="line x" title="199:210	The shared weight vector corresponds to the general features across different relation types." ></td>
	<td class="line x" title="200:210	We proposed to choose the general features either automatically inside the learning algorithm or guided by human knowledge." ></td>
	<td class="line x" title="201:210	We also leveraged additional human knowledge about the target relation type in the form of entity type constraints." ></td>
	<td class="line x" title="202:210	Experiment results on the ACE 2004 data show that the multi-task transfer learning method achieves the best performance when we combine human guidance with automatic general feature selection, followed by imposing the entity type constraints." ></td>
	<td class="line x" title="203:210	The final method substantially outperforms two baseline methods, improving the average F1 measure from 0.1532 to 0.4132 when only 10 seed target instances are used." ></td>
	<td class="line x" title="204:210	Our work is the first to explore transfer learning for relation extraction, and we have achieved very promising results." ></td>
	<td class="line x" title="205:210	Because of the practical importance of transfer learning and adaptation for relation extraction due to lack of training data in new domains, we hope our study and findings will lead to further investigation into this problem." ></td>
	<td class="line x" title="206:210	There are still many issues that remain unsolved." ></td>
	<td class="line x" title="207:210	For example, we have not looked at the degrees of relatedness between different pairs of relation types." ></td>
	<td class="line x" title="208:210	Presumably, when adapting to a specific target relation type, we want to choose the most similar auxiliary relation types to use." ></td>
	<td class="line x" title="209:210	Our current study is based on ACE relation types." ></td>
	<td class="line x" title="210:210	It would also be interesting to study similar problems in other domains, for example, the protein-protein interaction extraction problem in biomedical text mining." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-2079
Generalizing Dependency Features for Opinion Mining
Joshi, Mahesh;Ros√©, Carolyn P.;"></td>
	<td class="line x" title="1:87	Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 313316, Suntec, Singapore, 4 August 2009." ></td>
	<td class="line x" title="2:87	c 2009 ACL and AFNLP Generalizing Dependency Features for Opinion Mining Mahesh Joshi 1 and Carolyn Penstein-Rose 1,2 1 Language Technologies Institute 2 Human-Computer Interaction Institute Carnegie Mellon University, Pittsburgh, PA, USA {maheshj,cprose}@cs.cmu.edu Abstract We explore how features based on syntactic dependency relations can be utilized to improve performance on opinion mining." ></td>
	<td class="line x" title="3:87	Using a transformation of dependency relation triples, we convert them into composite back-off features that generalize better than the regular lexicalized dependency relation features." ></td>
	<td class="line x" title="4:87	Experiments comparing our approach with several other approaches that generalize dependency features or ngrams demonstrate the utility of composite back-off features." ></td>
	<td class="line x" title="5:87	1 Introduction Online product reviews are a crucial source of opinions about a product, coming from the people who have experienced it first-hand." ></td>
	<td class="line x" title="6:87	However, the task of a potential buyer is complicated by the sheer number of reviews posted online for a product of his/her interest." ></td>
	<td class="line x" title="7:87	Opinion mining, or sentiment analysis (Pang and Lee, 2008) in product reviews, in part, aims at automatically processing a large number of such product reviews to identify opinionated statements, and to classify them into having either a positive or negative polarity." ></td>
	<td class="line x" title="8:87	One of the most popular techniques used for opinion mining is that of supervised machine learning, for which, many different lexical, syntactic and knowledge-based feature representations have been explored in the literature (Dave et al., 2003; Gamon, 2004; Matsumoto et al., 2005; Ng et al., 2006)." ></td>
	<td class="line x" title="9:87	However, the use of syntactic features for opinion mining has achieved varied results." ></td>
	<td class="line x" title="10:87	In our work, we show that by altering syntactic dependency relation triples in a particular way (namely, backing off only the head word in a dependency relation to its part-of-speech tag), they generalize better and yield a significant improvement on the task of identifying opinions from product reviews." ></td>
	<td class="line x" title="11:87	In effect, this work demonstrates a better way to utilize syntactic dependency relations for opinion mining." ></td>
	<td class="line x" title="12:87	In the remainder of the paper, we first discuss related work." ></td>
	<td class="line x" title="13:87	We then motivate our approach and describe the composite back-off features, followed by experimental results, discussion and future directions for our work." ></td>
	<td class="line x" title="14:87	2 Related Work The use of syntactic or deep linguistic features for opinion mining has yielded mixed results in the literature so far." ></td>
	<td class="line x" title="15:87	On the positive side, Gamon (2004) found that the use of deep linguistic features extracted from phrase structure trees (which include syntactic dependency relations) yield significant improvements on the task of predicting satisfaction ratings in customer feedback data." ></td>
	<td class="line x" title="16:87	Matsumoto et al.(2005) show that when using frequently occurring sub-trees obtained from dependency relation parse trees as features for machine learning, significant improvement in performance is obtained on the task of classifying movie reviews as having positive or negative polarity." ></td>
	<td class="line x" title="18:87	Finally, Wilson et al.(2004) use several different features extracted from dependency parse trees to improve performance on the task of predicting the strength of opinion phrases." ></td>
	<td class="line x" title="20:87	On the flip side, Dave et al.(2003) found that for the task of polarity prediction, adding adjective-noun dependency relationships as features does not provide any benefit over a simple bag-of-words based feature space." ></td>
	<td class="line x" title="22:87	Ng et al.(2006) proposed that rather than focusing on just adjective-noun relationships, the subject-verb and verb-object relationships should also be considered for polarity classification." ></td>
	<td class="line x" title="24:87	However, they observed that the addition of these dependency relationships does not improve performance over a feature space that includes unigrams, bigrams and trigrams." ></td>
	<td class="line x" title="25:87	313 One difference that seems to separate the successes from the failures is that of using the entire set of dependency relations obtained from a dependency parser and allowing the learning algorithm to generalize, rather than picking a small subset of dependency relations manually." ></td>
	<td class="line x" title="26:87	However, in such a situation, one critical issue might be the sparseness of the very specific linguistic features, which may cause the classifier learned from such features to not generalize." ></td>
	<td class="line x" title="27:87	Features based on dependency relations provide a nice way to enable generalization to the right extent through utilization of their structural aspect." ></td>
	<td class="line x" title="28:87	In the next section, we motivate this idea in the context of our task, from a linguistic as well as machine learning perspective." ></td>
	<td class="line x" title="29:87	3 Identifying Opinionated Sentences We focus on the problem of automatically identifying whether a sentence in a product review contains an opinion about the product or one of its features." ></td>
	<td class="line x" title="30:87	We use the definition of this task as formulated by Hu and Liu (2004) on Amazon.com and CNet.com product reviews for five different products." ></td>
	<td class="line x" title="31:87	Their definition of an opinion sentence is reproduced here verbatim: If a sentence contains one or more product features and one or more opinion words, then the sentence is called an opinion sentence. Any other sentence in a review that does not fit the above definition of an opinion sentence is considered as a non-opinion sentence." ></td>
	<td class="line x" title="32:87	In general, these can be expected to be verifiable statements or facts such as product specifications and so on." ></td>
	<td class="line x" title="33:87	Before motivating the use of dependency relations as features for our task, a brief overview about dependency relations follows." ></td>
	<td class="line x" title="34:87	3.1 Dependency Relations The dependency parse for a given sentence is essentially a set of triplets or triples, each of which is composed of a grammatical relation and the pair of words from the sentence among which the grammatical relation holds ({rel i ,w j ,w k }, where rel i is the dependency relation among words w j and w k )." ></td>
	<td class="line x" title="35:87	The set of dependency relations is specific to a given parser  we use the Stanford parser 1 for computing dependency relations." ></td>
	<td class="line x" title="36:87	The word w j is usually referred to as the head word in the depen1 http://nlp.stanford.edu/software/ lex-parser.shtml dency triple, and the word w k is usually referred to as the modifier word." ></td>
	<td class="line x" title="37:87	One straightforward way to use dependency relations as features for machine learning is to generate features of the form RELATION HEAD MODIFIER and use them in a standard bag-of-words type binary or frequencybased representation." ></td>
	<td class="line x" title="38:87	The indices of the head and modifier words are dropped for the obvious reason that one does not expect them to generalize across sentences." ></td>
	<td class="line x" title="39:87	We refer to such features as lexicalized dependency relation features." ></td>
	<td class="line x" title="40:87	3.2 Motivation for our Approach Consider the following examples (these are madeup examples for the purpose of keeping the discussion succinct, but still capture the essence of our approach): (i) This is a great camera!" ></td>
	<td class="line x" title="41:87	(ii) Despite its few negligible flaws, this really great mp3 player won my vote." ></td>
	<td class="line x" title="42:87	Both of these sentences have an adjectival modifier (amod) relationship, the first one having amod camera great)and the second one having amod player great)." ></td>
	<td class="line x" title="43:87	Although both of these features are good indicators of opinion sentences and are closely related, any machine learning algorithm that treats these features independently will not be able to generalize their relationship to the opinion class." ></td>
	<td class="line x" title="44:87	Also, any new test sentence that contains a noun different from either camera or player (for instance in the review of a different electronic product), but is participating in a similar relationship, will not receive any importance in favor of the opinion class  the machine learning algorithm may not have even seen it in the training data." ></td>
	<td class="line x" title="45:87	Now consider the case where we back off the head word in each of the above features to its part-of-speech tag." ></td>
	<td class="line x" title="46:87	This leads to a single feature: amod NN great." ></td>
	<td class="line x" title="47:87	This has two advantages: first, the learning algorithm can now learn a weight for a more general feature that has stronger evidence of association with the opinion class, and second, any new test sentence that contains an unseen noun in a similar relationship with the adjective great will receive some weight in favor of the opinion class." ></td>
	<td class="line x" title="48:87	This back off operation is a generalization of the regular lexicalized dependency relations mentioned above." ></td>
	<td class="line x" title="49:87	In the next section we describe all such generalizations that we experimented with." ></td>
	<td class="line x" title="50:87	314 4 Methodology Composite Back-off Features: The idea behind our composite back-off features is to create more generalizable, but not overly general back-off features by backing off to the part-of-speech (POS) tag of either the head word or the modifier word (but not both at once, as in Gamon (2004) and Wilson et al.(2004))  hence the description composite, as there is a lexical part to the feature, coming from one word, and a POS tag coming from the other word, along with the dependency relation itself." ></td>
	<td class="line x" title="52:87	The two types of composite back-off features that we create from lexicalized dependency triples are as follows: (i) h-bo: Here we use features of the form {rel i ,POS j ,w k } where the head word is replaced by its POS tag, but the modifier word is retained." ></td>
	<td class="line x" title="53:87	(ii) m-bo: Here we use features of the form {rel i ,w j ,POS k }, where the modifier word is replaced by its POS tag, but the head word is retained." ></td>
	<td class="line x" title="54:87	Our hypothesis is that the h-bo features will perform better than purely lexicalized dependency relations for reasons mentioned in Section 3.2 above." ></td>
	<td class="line x" title="55:87	Although m-bo features also generalize the lexicalized dependency features, in a relation such as an adjectival modifier (discussed in Section 3.2 above), the head noun is a better candidate to back-off for enabling generalization across different products, rather than the modifier adjective." ></td>
	<td class="line x" title="56:87	For this reason, we do not expect their performance to be comparable to h-bo features." ></td>
	<td class="line x" title="57:87	We compare our composite back-off features with other similar ways of generalizing dependency relations and lexical ngrams that have been tried in previous work." ></td>
	<td class="line x" title="58:87	We describe these below." ></td>
	<td class="line x" title="59:87	Full Back-off Features: Both Gamon (2004) and Wilson et al.(2004) utilize features based on the following version of dependency relationships: {rel i ,POS j ,POS k }, where they back off both the head word and the modifier word to their respective POS tags (POS j and POS k )." ></td>
	<td class="line x" title="61:87	We refer to this as hm-bo." ></td>
	<td class="line x" title="62:87	NGram Back-off Features: Similar to McDonald et al.(2007), we utilize backed-off versions of lexical bigrams and trigrams, where all possible combinations of the words in the ngram are replaced by their POS tags, creating features such as w j POS k , POS j w k , POS j POS k for each lexical bigram and similarly for trigrams." ></td>
	<td class="line x" title="64:87	We refer to these as bi-bo and tri-bo features respectively." ></td>
	<td class="line x" title="65:87	In addition to these back-off approaches, we also use regular lexical bigrams (bi), lexical trigrams (tri), POS bigrams (POS-bi), POS trigrams (POS-tri) and lexicalized dependency relations (lexdep) as features." ></td>
	<td class="line x" title="66:87	While testing all of our feature sets, we evaluate each of them individually by adding them to the basic set of unigram (uni) features." ></td>
	<td class="line x" title="67:87	5 Experiments and Results Details of our experiments and results follow." ></td>
	<td class="line x" title="68:87	5.1 Dataset We use the extended version of the Amazon.com / CNet.com product reviews dataset released by Hu and Liu (2004), available from their web page 2 . We use a randomly chosen subset consisting of 2,200 review sentences (200 sentences each for 11 different products) 3 . The distribution is 1,053 (47.86%) opinion sentences and 1,147 (52.14%) non-opinion sentences." ></td>
	<td class="line x" title="69:87	5.2 Machine Learning Parameters We have used the Support Vector Machine (SVM) learner (Shawe-Taylor and Cristianini, 2000) from the MinorThird Toolkit (Cohen, 2004), along with the -squared feature selection procedure, where we reject features if their -squared score is not significant at the 0.05 level." ></td>
	<td class="line x" title="70:87	For SVM, we use the default linear kernel with all other parameters also set to defaults." ></td>
	<td class="line x" title="71:87	We perform 11-fold crossvalidation, where each test fold contains all the sentences for one of the 11 products, and the sentences for the remaining ten products are in the corresponding training fold." ></td>
	<td class="line x" title="72:87	Our results are reported in terms of average accuracy and Cohens kappa values across the 11 folds." ></td>
	<td class="line x" title="73:87	5.3 Results Table 1 shows the full set of results from our experiments." ></td>
	<td class="line x" title="74:87	Our results are comparable to those reported by Hu and Liu (2004) on the same task; as well as those by Arora et al.(2009) on a similar task of identifying qualified vs. bald claims in product reviews." ></td>
	<td class="line x" title="76:87	On the accuracy metric, the composite features with the head word backed off 2 http://www.cs.uic.edu/  liub/FBS/ sentiment-analysis.html 3 http://www.cs.cmu.edu/  maheshj/ datasets/acl09short.html 315 Features Accuracy Kappa uni .652 (.048) .295 (.049) uni+bi .657 (.066) .304 (.089) uni+bi-bo .650 (.056) .299 (.079) uni+tri .655 (.062) .306 (.077) uni+tri-bo .647 (.051) .287 (.075) uni+POS-bi .676 (.057) .349 (.083) uni+POS-tri .661 (.050) .317 (.064) uni+lexdep .639 (.055) .268 (.079) uni+hm-bo .670 (.046) .336 (.065) uni+h-bo .679 (.063) .351 (.097) uni+m-bo .657 (.056) .308 (.063) Table 1: Shown are the average accuracy and Cohens kappa across 11 folds." ></td>
	<td class="line x" title="77:87	Bold indicates statistically significant improvements (p<0.05,twotailed pairwise T-test) over the (uni) baseline." ></td>
	<td class="line x" title="78:87	are the only ones that achieve a statistically significant improvement over the uni baseline." ></td>
	<td class="line x" title="79:87	On the kappa metric, using POS bigrams also achieves a statistically significant improvement, as do the composite h-bo features." ></td>
	<td class="line x" title="80:87	None of the other backoff strategies achieve a statistically significant improvement over uni, although numerically hm-bo comes quite close to h-bo." ></td>
	<td class="line x" title="81:87	Evaluation of these two types of features by themselves (without unigrams) shows that h-bo are significantly better than hm-bo at p<0.10 level." ></td>
	<td class="line x" title="82:87	Regular lexicalized dependency relation features perform worse than unigrams alone." ></td>
	<td class="line x" title="83:87	These results thus demonstrate that composite back-off features based on dependency relations, where only the head word is backed off to its POS tag present a useful alternative to encoding dependency relations as features for opinion mining." ></td>
	<td class="line x" title="84:87	6 Conclusions and Future Directions We have shown that for opinion mining in product review data, a feature representation based on a simple transformation (backing off the head word in a dependency relation to its POS tag) of syntactic dependency relations captures more generalizable and useful patterns in data than purely lexicalized dependency relations, yielding a statistically significant improvement." ></td>
	<td class="line x" title="85:87	The next steps that we are currently working on include applying this approach to polarity classification." ></td>
	<td class="line oc" title="86:87	Also, the aspect of generalizing features across different products is closely related to fully supervised domain adaptation (Daume III, 2007), and we plan to combine our approach with the idea from Daume III (2007) to gain insights into whether the composite back-off features exhibit different behavior in domain-general versus domain-specific feature sub-spaces." ></td>
	<td class="line x" title="87:87	Acknowledgments This research is supported by National Science Foundation grant IIS-0803482." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-2420
SemEval-2010 Task 17: All-words Word Sense Disambiguation on a Specific Domain
Agirre, Eneko;Lopez de Lacalle, Oier;Fellbaum, Christiane;Marchetti, Andrea;Toral, Antonio;Vossen, Piek;"></td>
	<td class="line x" title="1:122	Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 123128, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:122	c 2009 Association for Computational Linguistics SemEval-2010 Task 17: All-words Word Sense Disambiguation on a Specific Domain Eneko Agirre IXA NLP group UBC Donostia, Basque Country e.agirre@ehu.es Oier Lopez de Lacalle IXA NLP group UBC Donostia, Basque Country oier.lopezdelacalle@ehu.es Christiane Fellbaum Department of Computer Science Princeton University Princeton, USA fellbaum@princeton.edu Andrea Marchetti IIT CNR Pisa, Italy andrea.marchetti@iit.cnr.it Antonio Toral ILC CNR Pisa, Italy antonio.toral@ilc.cnr.it Piek Vossen Faculteit der Letteren Vrije Universiteit Amsterdam Amsterdam, Netherlands p.vossen@let.vu.nl Abstract Domain portability and adaptation of NLP components and Word Sense Disambiguation systems present new challenges." ></td>
	<td class="line x" title="3:122	The difficulties found by supervised systems to adapt might change the way we assess the strengths and weaknesses of supervised and knowledgebased WSD systems." ></td>
	<td class="line x" title="4:122	Unfortunately, all existing evaluation datasets for specific domains are lexical-sample corpora." ></td>
	<td class="line x" title="5:122	With this paper we want to motivate the creation of an allwords test dataset for WSD on the environment domain in several languages, and present the overall design of this SemEval task." ></td>
	<td class="line o" title="6:122	1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in the last Senseval and Semeval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Pradhan et al., 2007)." ></td>
	<td class="line x" title="7:122	Specific domains pose fresh challenges to WSD systems: the context in which the senses occur might change, distributions and predominant senses vary, some words tend to occur in fewer senses in specific domains, and new senses and terms might be involved." ></td>
	<td class="line x" title="8:122	Both supervised and knowledge-based systems are affected by these issues: while the first suffer from different context and sense priors, the later suffer from lack of coverage of domain-related words and information." ></td>
	<td class="line x" title="9:122	Domain adaptation of supervised techniques is a hot issue in Natural Language Processing, including Word Sense Disambiguation." ></td>
	<td class="line x" title="10:122	Supervised Word Sense Disambiguation systems trained on general corpora are known to perform worse when applied to specific domains (Escudero et al., 2000; Martnez and Agirre, 2000), and domain adaptation techniques have been proposed as a solution to this problem with mixed results." ></td>
	<td class="line x" title="11:122	Current research on applying WSD to specific domains has been evaluated on three available lexicalsample datasets (Ng and Lee, 1996; Weeber et al., 2001; Koeling et al., 2005)." ></td>
	<td class="line x" title="12:122	This kind of dataset contains hand-labeled examples for a handful of selected target words." ></td>
	<td class="line x" title="13:122	As the systems are evaluated on a few words, the actual performance of the systems over complete texts can not be measured." ></td>
	<td class="line x" title="14:122	Differences in behavior of WSD systems when applied to lexical-sample and all-words datasets have been observed on previous Senseval and Semeval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Pradhan et al., 2007): supervised systems attain results on the high 80s and beat the most frequent baseline by a large margin for lexical-sample datasets, but results on the all-words datasets were much more modest, on the low 70s, and a few points above the most frequent baseline." ></td>
	<td class="line x" title="15:122	Thus, the behaviour of WSD systems on domainspecific texts is largely unknown." ></td>
	<td class="line x" title="16:122	While some words could be supposed to behave in similar ways, and thus be amenable to be properly treated by a generic 123 WSD algorithm, other words have senses closely linked to the domain, and might be disambiguated using purpose-built domain adaptation strategies (cf.Section 4)." ></td>
	<td class="line x" title="18:122	While it seems that domain-specific WSD might be a tougher problem than generic WSD, it might well be that domain-related words are easier to disambiguate." ></td>
	<td class="line x" title="19:122	The main goal of this task is to provide a multilingual testbed to evaluate WSD systems when faced with full-texts from a specific domain, that of environment-related texts." ></td>
	<td class="line x" title="20:122	The paper is structured as follows." ></td>
	<td class="line x" title="21:122	The next section presents current lexical sample datasets for domain-specific WSD." ></td>
	<td class="line x" title="22:122	Section 3 presents some possible settings for domain adaptation." ></td>
	<td class="line x" title="23:122	Section 4 reviews the state-of-the art in domain-specific WSD." ></td>
	<td class="line x" title="24:122	Section 5 presents the design of our task, and finally, Section 6 draws some conclusions." ></td>
	<td class="line x" title="25:122	2 Specific domain datasets available We will briefly present the three existing datasets for domain-related studies in WSD, which are all lexical-sample." ></td>
	<td class="line x" title="26:122	The most commonly used dataset is the Defense Science Organization (DSO) corpus (Ng and Lee, 1996), which comprises sentences from two different corpora." ></td>
	<td class="line x" title="27:122	The first is the Wall Street Journal (WSJ), which belongs to the financial domain, and the second is the Brown Corpus (BC) which is a balanced corpora of English usage." ></td>
	<td class="line x" title="28:122	191 polysemous words (nouns and verbs) of high frequency in WSJ and BC were selected and a total of 192,800 occurrences of these words were tagged with WordNet 1.5 senses, more than 1,000 instances per word in average." ></td>
	<td class="line x" title="29:122	The examples from BC comprise 78,080 occurrences of word senses, and examples from WSJ consist on 114,794 occurrences." ></td>
	<td class="line x" title="30:122	In domain adaptation experiments, the Brown Corpus examples play the role of general corpora, and the examples from the WSJ play the role of domain-specific examples." ></td>
	<td class="line x" title="31:122	Koeling et al.(2005) present a corpus were the examples are drawn from the balanced BNC corpus (Leech, 1992) and the SPORTS and FINANCES sections of the newswire Reuters corpus (Rose et al., 2002), comprising around 300 examples (roughly 100 from each of those corpora) for each of the 41 nouns." ></td>
	<td class="line x" title="33:122	The nouns were selected because they were salient in either the SPORTS or FINANCES domains, or because they had senses linked to those domains." ></td>
	<td class="line x" title="34:122	The occurrences were hand-tagged with the senses from WordNet version 1.7.1 (Fellbaum, 1998)." ></td>
	<td class="line x" title="35:122	In domain adaptation experiments the BNC examples play the role of general corpora, and the FINANCES and SPORTS examples the role of two specific domain corpora." ></td>
	<td class="line x" title="36:122	Finally, a dataset for biomedicine was developed by Weeber et al.(2001), and has been used as a benchmark by many independent groups." ></td>
	<td class="line x" title="38:122	The UMLS Metathesaurus was used to provide a set of possible meanings for terms in biomedical text." ></td>
	<td class="line x" title="39:122	50 ambiguous terms which occur frequently in MEDLINE were chosen for inclusion in the test set." ></td>
	<td class="line x" title="40:122	100 instances of each term were selected from citations added to the MEDLINE database in 1998 and manually disambiguated by 11 annotators." ></td>
	<td class="line x" title="41:122	Twelve terms were flagged as problematic due to substantial disagreement between the annotators." ></td>
	<td class="line x" title="42:122	In addition to the meanings defined in UMLS, annotators had the option of assigning a special tag (none) when none of the UMLS meanings seemed appropriate." ></td>
	<td class="line x" title="43:122	Although these three corpora are useful for WSD research, it is difficult to infer which would be the performance of a WSD system on full texts." ></td>
	<td class="line x" title="44:122	The corpus of Koeling et al., for instance, only includes words which where salient for the target domains, but the behavior of WSD systems on other words cannot be explored." ></td>
	<td class="line x" title="45:122	We would also like to note that while the biomedicine corpus tackles scholarly text of a very specific domain, the WSJ part of the DSO includes texts from a financially oriented newspaper, but also includes news of general interest which have no strict relation to the finance domain." ></td>
	<td class="line x" title="46:122	3 Possible settings for domain adaptation When performing supervised WSD on specific domains the first setting is to train on a general domain data set and to test on the specific domain (source setting)." ></td>
	<td class="line x" title="47:122	If performance would be optimal, this would be the ideal solution, as it would show that a generic WSD system is robust enough to tackle texts from new domains, and domain adaptation would not be necessary." ></td>
	<td class="line x" title="48:122	The second setting (target setting) would be to train the WSD systems only using examples from 124 the target domain." ></td>
	<td class="line x" title="49:122	If this would be the optimal setting, it would show that there is no cost-effective method for domain adaptation." ></td>
	<td class="line x" title="50:122	WSD systems would need fresh examples every time they were deployed in new domains, and examples from general domains could be discarded." ></td>
	<td class="line x" title="51:122	In the third setting, the WSD system is trained with examples coming from both the general domain and the specific domain." ></td>
	<td class="line x" title="52:122	Good results in this setting would show that supervised domain adaptation is working, and that generic WSD systems can be supplemented with hand-tagged examples from the target domain." ></td>
	<td class="line x" title="53:122	There is an additional setting, where a generic WSD system is supplemented with untagged examples from the domain." ></td>
	<td class="line x" title="54:122	Good results in this setting would show that semi-supervised domain adaptation works, and that generic WSD systems can be supplemented with untagged examples from the target domain in order to improve their results." ></td>
	<td class="line x" title="55:122	Most of current all-words generic supervised WSD systems take SemCor (Miller et al., 1993) as their source corpus, i.e. they are trained on SemCor examples and then applied to new examples." ></td>
	<td class="line x" title="56:122	SemCor is the largest publicly available annotated corpus." ></td>
	<td class="line x" title="57:122	Its mainly a subset of the Brown Corpus, plus the novel The Red Badge of Courage." ></td>
	<td class="line x" title="58:122	The Brown corpus is balanced, yet not from the general domain, as it comprises 500 documents drawn from different domains, each approximately 2000 words long." ></td>
	<td class="line x" title="59:122	Although the Brown corpus is balanced, SemCor is not, as the documents were not chosen at random." ></td>
	<td class="line x" title="60:122	4 State-of-the-art in WSD for specific domains Initial work on domain adaptation for WSD systems showed that WSD systems were not able to obtain better results on the source or adaptation settings compared to the target settings (Escudero et al., 2000), showing that a generic WSD system (i.e. based on hand-annotated examples from a generic corpus) would not be useful when moved to new domains." ></td>
	<td class="line x" title="61:122	Escudero et al.(2000) tested the supervised adaptation scenario on the DSO corpus, which had examples from the Brown Corpus and Wall Street Journal corpus." ></td>
	<td class="line x" title="63:122	They found that the source corpus did not help when tagging the target corpus, showing that tagged corpora from each domain would suffice, and concluding that hand tagging a large general corpus would not guarantee robust broad-coverage WSD." ></td>
	<td class="line x" title="64:122	Agirre and Martnez (2000) used the same DSO corpus and showed that training on the subset of the source corpus that is topically related to the target corpus does allow for domain adaptation, obtaining better results than training on the target data alone." ></td>
	<td class="line x" title="65:122	In (Agirre and Lopez de Lacalle, 2008), the authors also show that state-of-the-art WSD systems are not able to adapt to the domains in the context of the Koeling et al.(2005) dataset." ></td>
	<td class="line x" title="67:122	While WSD systems trained on the target domain obtained 85.1 and 87.0 of precision on the sports and finances domains, respectively, the same systems trained on the BNC corpus (considered as a general domain corpus) obtained 53.9 and 62.9 of precision on sports and finances, respectively." ></td>
	<td class="line x" title="68:122	Training on both source and target was inferior that using the target examples alone." ></td>
	<td class="line x" title="69:122	Supervised adaptation Supervised adaptation for other NLP tasks has been widely reported." ></td>
	<td class="line pc" title="70:122	For instance, (Daume III, 2007) shows that a simple feature augmentation method for SVM is able to effectively use both labeled target and source data to provide the best domainadaptation results in a number of NLP tasks." ></td>
	<td class="line p" title="71:122	His method improves or equals over previously explored more sophisticated methods (Daume III and Marcu, 2006; Chelba and Acero, 2004)." ></td>
	<td class="line x" title="72:122	In contrast, (Agirre and Lopez de Lacalle, 2009) reimplemented this method and showed that the improvement on WSD in the (Koeling et al., 2005) data was marginal." ></td>
	<td class="line x" title="73:122	Better results have been obtained using purposebuilt adaptation methods." ></td>
	<td class="line x" title="74:122	Chan and Ng (2007) performed supervised domain adaptation on a manually selected subset of 21 nouns from the DSO corpus." ></td>
	<td class="line x" title="75:122	They used active learning, count-merging, and predominant sense estimation in order to save target annotation effort." ></td>
	<td class="line x" title="76:122	They showed that adding just 30% of the target data to the source examples the same precision as the full combination of target and source data could be achieved." ></td>
	<td class="line x" title="77:122	They also showed that using the source corpus significantly improved results when only 10%-30% of the target corpus was used for training." ></td>
	<td class="line x" title="78:122	In followup work (Zhong et 125 Projections for 2100 suggest that temperature in Europe will have risen by between 2 to 6.3 C above 1990 levels." ></td>
	<td class="line x" title="79:122	The sea level is projected to rise, and a greater frequency and intensity of extreme weather events are expected." ></td>
	<td class="line x" title="80:122	Even if emissions of greenhouse gases stop today, these changes would continue for many decades and in the case of sea level for centuries." ></td>
	<td class="line x" title="81:122	This is due to the historical build up of the gases in the atmosphere and time lags in the response of climatic and oceanic systems to changes in the atmospheric concentration of the gases." ></td>
	<td class="line x" title="82:122	Figure 1: Sample text from the environment domain." ></td>
	<td class="line x" title="83:122	al., 2008), the feature augmentation approach was combined with active learning and tested on the OntoNotes corpus, on a large domain-adaptation experiment." ></td>
	<td class="line x" title="84:122	They significantly reduced the effort of hand-tagging, but only obtained positive domainadaptation results for smaller fractions of the target corpus." ></td>
	<td class="line x" title="85:122	In (Agirre and Lopez de Lacalle, 2009) the authors report successful adaptation on the (Koeling et al., 2005) dataset on supervised setting." ></td>
	<td class="line x" title="86:122	Their method is based on the use of unlabeled data, reducing the feature space with SVD, and combination of features using an ensemble of kernel methods." ></td>
	<td class="line x" title="87:122	They report 22% error reduction when using both source and target data compared to a classifier trained on target the target data alone, even when the full dataset is used." ></td>
	<td class="line x" title="88:122	Semi-supervised adaptation There are less works on semi-supervised domain adaptation in NLP tasks, and fewer in WSD task." ></td>
	<td class="line x" title="89:122	Blitzer et al.(2006) used Structural Correspondence Learning and unlabeled data to adapt a Part-ofSpeech tagger." ></td>
	<td class="line x" title="91:122	They carefully select so-called pivot features to learn linear predictors, perform SVD on the weights learned by the predictor, and thus learn correspondences among features in both source and target domains." ></td>
	<td class="line x" title="92:122	Agirre and Lopez de Lacalle (2008) show that methods based on SVD with unlabeled data and combination of distinct feature spaces produce positive semi-supervised domain adaptation results for WSD." ></td>
	<td class="line x" title="93:122	Unsupervised adaptation In this context, we take unsupervised to mean Knowledge-Based methods which do not require hand-tagged corpora." ></td>
	<td class="line x" title="94:122	The predominant sense acquisition method was succesfully applied to specific domains in (Koeling et al., 2005)." ></td>
	<td class="line x" title="95:122	The methos has two steps: In the first, a corpus of untagged text from the target domain is used to construct a thesaurus of similar words." ></td>
	<td class="line x" title="96:122	In the second, each target word is disambiguated using pairwise WordNet-based similarity measures, taking as pairs the target word and each of the most related words according to the thesaurus up to a certain threshold." ></td>
	<td class="line x" title="97:122	This method aims to obtain, for each target word, the sense which is the most predominant for the target corpus." ></td>
	<td class="line x" title="98:122	When a general corpus is used, the most predominant sense in general is obtained, and when a domain-specific corpus is used, the most predominant sense for that corpus is obtained (Koeling et al., 2005)." ></td>
	<td class="line x" title="99:122	The main motivation of the authors is that the most frequent sense is a very powerful baseline, but it is one which requires hand-tagging text, while their method yields similar information automatically." ></td>
	<td class="line x" title="100:122	The results show that they are able to obtain good results." ></td>
	<td class="line x" title="101:122	In related work, (Agirre et al., 2009) report improved results using the same strategy but applying a graph-based WSD method, and highlight the domain-adaptation potential of unsupervised knowledge-based WSD systems compared to supervised WSD." ></td>
	<td class="line x" title="102:122	5 Design of the WSD-domain task This task was designed in the context of Kyoto (Piek Vossen and VanGent, 2008)1, an AsianEuropean project that develops a community platform for modeling knowledge and finding facts across languages and cultures." ></td>
	<td class="line x" title="103:122	The platform operates as a Wiki system with an ontological support that social communities can use to agree on the meaning of terms in specific domains of their interest." ></td>
	<td class="line x" title="104:122	Kyoto will focus on the environmental domain because it poses interesting challenges for information sharing, but the techniques and platforms will be independent of the application domain." ></td>
	<td class="line x" title="105:122	Kyoto 1http://www.kyoto-project.eu/ 126 will make use of semantic technologies based on ontologies and WSD in order to extract and represent relevant information for the domain, and is thus interested on measuring the performance of WSD techniques on this domain." ></td>
	<td class="line x" title="106:122	The WSD-domain task will comprise comparable all-words test corpora on the environment domain." ></td>
	<td class="line x" title="107:122	Texts from the European Center for Nature Conservation2 and Worldwide Wildlife Forum3 will be used in order to build domain specific test corpora." ></td>
	<td class="line x" title="108:122	We will select documents that are written for a general but interested public and that involve specific terms from the domain." ></td>
	<td class="line x" title="109:122	The document content will be comparable across languages." ></td>
	<td class="line x" title="110:122	Figure 1 shows an example in English related to global warming." ></td>
	<td class="line x" title="111:122	The data will be available in a number of languages: English, Dutch, Italian and Chinese." ></td>
	<td class="line x" title="112:122	The sense inventories will be based on wordnets of the respective languages, which will be updated to include new vocabulary and senses." ></td>
	<td class="line x" title="113:122	The test data will comprise three documents of around 2000 words each for each language." ></td>
	<td class="line x" title="114:122	The annotation procedure will involve double-blind annotation plus adjudication, and inter-tagger agreement data will be provided." ></td>
	<td class="line x" title="115:122	The formats and scoring software will follow those of Senseval-34 and SemEval-20075 English all-words tasks." ></td>
	<td class="line x" title="116:122	There will not be training data available, but participants are free to use existing hand-tagged corpora and lexical resources (e.g. SemCor and previous Senseval and SemEval data)." ></td>
	<td class="line x" title="117:122	We plan to make available a corpus of documents from the same domain as the selected documents, as well as wordnets updated to include the terms and senses in the selected documents." ></td>
	<td class="line x" title="118:122	6 Conclusions Domain portability and adaptation of NLP components and Word Sense Disambiguation systems present new challenges." ></td>
	<td class="line x" title="119:122	The difficulties found by supervised systems to adapt might change the way we assess the strengths and weaknesses of supervised and knowledge-based WSD systems." ></td>
	<td class="line x" title="120:122	Unfortunately, all existing evaluation datasets for specific 2http://www.ecnc.org 3http://www.wwf.org 4http://www.senseval.org/senseval3 5http://nlp.cs.swarthmore.edu/semeval/ domains are lexical-sample corpora." ></td>
	<td class="line x" title="121:122	With this paper we have motivated the creation of an all-words test dataset for WSD on the environment domain in several languages, and presented the overall design of this SemEval task." ></td>
	<td class="line x" title="122:122	Further details can be obtained from the Semeval20106 website, our task website7, and in our distribution list8 7 Acknowledgments The organization of the task is partially funded by the European Commission (KYOTO FP7 ICT2007-211423) and the Spanish Research Department (KNOW TIN2006-15049-C03-01)." ></td>
</tr></table>
</div
</body></html>
