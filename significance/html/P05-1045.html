<html><body><head><link rel="stylesheet" type="text/css" href="style.css" /><script src="map.js"></script><script src="jquery-1.7.1.min.js"></script></head>
<div class="dstPaperData">
P05-1045 <div class="dstPaperTitle">Incorporating Non-Local Information Into Information Extraction Systems By Gibbs Sampling</div><div class="dstPaperAuthors">Finkel, Jenny Rose;Grenager, Trond;Manning, Christopher D.;</div>
</div>
<table cellspacing="0" cellpadding="0"><tr>
	<td class="srcData" >Source Paper</td>
	<td class="pp legend" ><input type="checkbox" id="cbIPositive" checked="true"/><label for="cbIPositive">Informal +<label></td>
	<td class="nn legend" ><input type="checkbox" id="cbINegative" checked="true"/><label for="cbINegative">Informal -<label></td>
	<td class="oo legend" ><input type="checkbox" id="cbIObjective" checked="true"/><label for="cbIObjective">Informal Neutral<label></td>
	<td class="ppc legend" ><input type="checkbox" id="cbEPositive" checked="true"/><label for="cbEPositive">Formal +</label></td>
	<td class="nnc legend" ><input type="checkbox" id="cbENegative" checked="true"/><label for="cbENegative">Formal -</label></td>
	<td class="ooc legend" ><input type="checkbox" id="cbEObjective" checked="true"/><label for="cbEObjective">Formal Neutral</label></td>
	<td class="lb"><input type="checkbox" id="cbSentenceBoundary"/><label for="cbSentenceBoundary">Sentence Boundary</label></td>
</tr></table>
<div class="dstPaper">
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N06-1054
A Fast Finite-State Relaxation Method For Enforcing Global Constraints On Sequence Decoding
Tromble, Roy W.;Eisner, Jason M.;"></td>
	<td class="line x" title="1:249	Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 423430, New York, June 2006." ></td>
	<td class="line x" title="2:249	c2006 Association for Computational Linguistics A fast finite-state relaxation method for enforcing global constraints on sequence decoding Roy W. Tromble and Jason Eisner Department of Computer Science and Center for Language and Speech Processing Johns Hopkins University Baltimore, MD 21218 {royt,jason}@cs.jhu.edu Abstract We describe finite-state constraint relaxation, a method for applying global constraints, expressed as automata, to sequence model decoding." ></td>
	<td class="line x" title="3:249	We present algorithms for both hard constraints and binary soft constraints." ></td>
	<td class="line x" title="4:249	On the CoNLL-2004 semantic role labeling task, we report a speedup of at least 16x over a previous method that used integer linear programming." ></td>
	<td class="line x" title="5:249	1 Introduction Many tasks in natural language processing involve sequence labeling." ></td>
	<td class="line x" title="6:249	If one models long-distance or global properties of labeled sequences, it can become intractable to find (decode) the best labeling of an unlabeled sequence." ></td>
	<td class="line p" title="7:249	Nonetheless, such global properties can improve the accuracy of a model, so recent NLP papers have considered practical techniques for decoding with them." ></td>
	<td class="line oc" title="8:249	Such techniques include Gibbs sampling (Finkel et al. , 2005), a general-purpose Monte Carlo method, and integer linear programming (ILP), (Roth and Yih, 2005), a general-purpose exact framework for NP-complete problems." ></td>
	<td class="line x" title="9:249	Under generative models such as hidden Markov models, the probability of a labeled sequence depends only on its local properties." ></td>
	<td class="line x" title="10:249	The situation improves with discriminatively trained models, such as conditional random fields (Lafferty et al. , 2001), which do efficiently allow features that are functions of the entire observation sequence." ></td>
	<td class="line x" title="11:249	However, these features can still only look locally at the label sequence." ></td>
	<td class="line nc" title="12:249	That is a significant shortcoming, because in many domains, hard or soft global constraints on the label sequence are motivated by common sense:  For named entity recognition, a phrase that appears multiple times should tend to get the same label each time (Finkel et al. , 2005)." ></td>
	<td class="line x" title="13:249	 In bibliography entries (Peng and McCallum, 2004), a given field (author, title, etc)." ></td>
	<td class="line x" title="14:249	should be filled by at most one substring of the input, and there are strong preferences on the cooccurrence and order of certain fields." ></td>
	<td class="line x" title="15:249	 In seminar announcements, a given field (speaker, start time, etc)." ></td>
	<td class="line oc" title="16:249	should appear with at most one value in each announcement, although the field and value may be repeated (Finkel et al. , 2005)." ></td>
	<td class="line x" title="17:249	 For semantic role labeling, each argument should be instantiated only once for a given verb." ></td>
	<td class="line x" title="18:249	There are several other constraints that we will describe later (Roth and Yih, 2005)." ></td>
	<td class="line x" title="19:249	A popular approximate technique is to hypothesize a list of possible answers by decoding without any global constraints, and then rerank (or prune) this n-best list using the full model with all constraints." ></td>
	<td class="line x" title="20:249	Reranking relies on the local model being good enough that the globally best answer appears in its n-best list." ></td>
	<td class="line x" title="21:249	Otherwise, reranking cant find it." ></td>
	<td class="line x" title="22:249	In this paper, we propose constraint relaxation, a simple exact alternative to reranking." ></td>
	<td class="line x" title="23:249	As in reranking, we start with a weighted lattice of hypotheses proposed by the local model." ></td>
	<td class="line x" title="24:249	But rather than restrict to the n best of these according to the local model, we aim to directly extract the one best according to the global model." ></td>
	<td class="line x" title="25:249	As in reranking, we hope that the local constraints alone will work well, but if they do not, the penalty is not incorrect decoding, but longer runtime as we gradually fold the global constraints into the lattice." ></td>
	<td class="line x" title="26:249	Constraint relaxation can be used whenever the global constraints can be expressed as regular languages over the label sequence." ></td>
	<td class="line x" title="27:249	In the worst case, our runtime may be exponential in the number of constraints, since we are considering an intractable class of problems." ></td>
	<td class="line x" title="28:249	However, we show that in practice, the method is quite effective at rapid decoding under global hard constraints." ></td>
	<td class="line x" title="29:249	423 0O 1 ? O? Figure 1: An automaton expressing the constraint that the label sequence cannot be O." ></td>
	<td class="line x" title="30:249	Here ? matches any symbol except O. The remainder of the paper is organized as follows: In 2 we describe how finite-state automata can be used to apply global constraints." ></td>
	<td class="line x" title="31:249	We then give a brute-force decoding algorithm (3)." ></td>
	<td class="line x" title="32:249	In 4, we present a more efficient algorithm for the case of hard constraints." ></td>
	<td class="line x" title="33:249	We report results for the semantic role labeling task in 5." ></td>
	<td class="line x" title="34:249	6 treats soft constraints." ></td>
	<td class="line x" title="35:249	2 Finite-state constraints Previous approaches to global sequence labeling Gibbs sampling, ILP, and rerankingseem motivated by the idea that standard sequence methods are incapable of considering global constraints at all." ></td>
	<td class="line x" title="36:249	In fact, finite-state automata (FSAs) are powerful enough to express many long-distance constraints." ></td>
	<td class="line x" title="37:249	Since all finite languages are regular, any constraint over label sequences of bounded length is finitestate." ></td>
	<td class="line x" title="38:249	FSAs are more powerful than n-gram models." ></td>
	<td class="line x" title="39:249	For example, the regular expression XY matches only sequences of labels that contain an X before a Y. Similarly, the regular expression (O) requires at least one non-O label; it compiles into the FSA of Figure 1." ></td>
	<td class="line x" title="40:249	Note that this FSA is in one or the other of its two states according to whether it has encountered a nonO label yet." ></td>
	<td class="line x" title="41:249	In general, the current state of an FSA records properties of the label sequence prefix read so far." ></td>
	<td class="line x" title="42:249	The FSA needs enough states to keep track of whether the label sequence as a whole satisfies the global constraint in question." ></td>
	<td class="line x" title="43:249	FSAs are a flexible approach to constraints because they are closed under logical operations such as disjunction (union) and conjunction (intersection)." ></td>
	<td class="line x" title="44:249	They may be specified by regular expressions (Karttunen et al. , 1996), in a logical language (Vaillette, 2004), or directly as FSAs." ></td>
	<td class="line x" title="45:249	They may also be weighted to express soft constraints." ></td>
	<td class="line x" title="46:249	Formally, we pose the decoding problem in terms of an observation sequence x  X and possible label sequences y  Y." ></td>
	<td class="line x" title="47:249	In many NLP tasks, X is the set of words, and Y the tags." ></td>
	<td class="line x" title="48:249	A lattice L: Y mapsto R maps label sequences to weights, and is encoded as a weighted FSA." ></td>
	<td class="line x" title="49:249	Constraints are formally the same any function C: Y mapsto R is a constraint, including weighted features from a classifier or probabilistic model." ></td>
	<td class="line x" title="50:249	In this paper we will consider only constraints that are weighted in particular ways." ></td>
	<td class="line x" title="51:249	Given a lattice L and constraints C, we seek y def= argmax y parenleftBigg L(y) + summationdisplay CC C(y) parenrightBigg." ></td>
	<td class="line x" title="52:249	(1) We assume the lattice L is generated by a model M: X mapsto (Y mapsto R)." ></td>
	<td class="line x" title="53:249	For a given observation sequence x, we put L = M(x)." ></td>
	<td class="line x" title="54:249	One possible model is a finite-state transducer, where M(x) is an FSA found by composing the transducer with x. Another is a CRF, where M(x) is a lattice with sums of logpotentials for arc weights.1 3 A brute-force finite-state decoder To find the best constrained labeling in a lattice, y, according to (1), we could simply intersect the lattice with all the constraints, then extract the best path." ></td>
	<td class="line x" title="55:249	Weighted FSA intersection is a generalization of ordinary unweighted FSA intersection (Mohri et al. , 1996)." ></td>
	<td class="line x" title="56:249	It is customary in NLP to use the so-called tropical semiring, where weights are represented by their natural logarithms and summed rather than multiplied." ></td>
	<td class="line x" title="57:249	Then the intersected automaton L  C computes (LC)(y) def= L(y) +C(y) (2) To find y, one would extract the best path in L  C1  C2   using the Viterbi algorithm, or Dijkstras algorithm if the lattice is cyclic." ></td>
	<td class="line x" title="58:249	This step is fast if the intersected automaton is small." ></td>
	<td class="line x" title="59:249	The problem is that the multiple intersections in LC1 C2  can quickly lead to an FSA with an intractable number of states." ></td>
	<td class="line x" title="60:249	The intersection of two finite-state automata produces an automaton 1For example, if M is a simple linear-chain CRF, L(y) =P n i=1 f(yi1,yi) + g(xi,yi)." ></td>
	<td class="line x" title="61:249	We build L = M(x) as anacyclic FSA whose state set is Y  {1,2,n}, with transitions (yprime,i1)  (y,i) of weight f(yprime,y) + g(xi,y)." ></td>
	<td class="line x" title="62:249	424 with the cross product state set." ></td>
	<td class="line x" title="63:249	That is, if F has m states and G has n states, then F G has up to mn states (fewer if some of the mn possible states do not lie on any accepting path)." ></td>
	<td class="line x" title="64:249	Intersection of many such constraints, even if they have only a few states each, quickly leads to a combinatorial explosion." ></td>
	<td class="line x" title="65:249	In the worst case, the size, in states, of the resulting lattice is exponential in the number of constraints." ></td>
	<td class="line x" title="66:249	To deal with this, we present a constraint relaxation algorithm." ></td>
	<td class="line x" title="67:249	4 Hard constraints The simplest kind of constraint is the hard constraint." ></td>
	<td class="line x" title="68:249	Hard constraints are necessarily binary either the labeling satisfies the constraint, or it violates it." ></td>
	<td class="line x" title="69:249	Violation is fatalthe labeling produced by decoding must satisfy each hard constraint." ></td>
	<td class="line x" title="70:249	Formally, a hard constraint is a mappingC: Y mapsto {0,}, encoded as an unweighted FSA." ></td>
	<td class="line x" title="71:249	If a string satisfies the constraint, recognition of the string will lead to an accepting state." ></td>
	<td class="line x" title="72:249	If it violates the constraint, recognition will end in a non-accepting state." ></td>
	<td class="line x" title="73:249	Here we give an algorithm for decoding with a set of such constraints." ></td>
	<td class="line x" title="74:249	Later (6), we discuss the case of binary soft constraints." ></td>
	<td class="line x" title="75:249	In what follows, we will assume that there is always at least one path in the lattice that satisfies all of the constraints." ></td>
	<td class="line x" title="76:249	4.1 Decoding by constraint relaxation Our decoding algorithm first relaxes the global constraints and solves a simpler problem." ></td>
	<td class="line x" title="77:249	In particular, we find the best labeling according to the model, y0 def= argmax y L(y) (3) ignoring all the constraints in C. Next, we check whether y0 satisifies the constraints." ></td>
	<td class="line x" title="78:249	If so, then we are doney0 is also y." ></td>
	<td class="line x" title="79:249	If not, then we reintroduce the constraints." ></td>
	<td class="line x" title="80:249	However, rather than include all at once, we introduce them only as they are violated by successive solutions to the relaxed problems: y0, y1, etc. We define y1 def= argmax y (L(y) +C(y)) (4) for some constraint C that y0 violates." ></td>
	<td class="line x" title="81:249	Similarly, y2 satisfies an additional constraint that y1 violates, HARD-CONSTRAIN-LATTICE(L, C): 1." ></td>
	<td class="line x" title="82:249	y := Best-Path(L) 2." ></td>
	<td class="line x" title="83:249	while C  C such that C(y) = : 3." ></td>
	<td class="line x" title="84:249	L := LC 4." ></td>
	<td class="line x" title="85:249	C := C{C} 5." ></td>
	<td class="line x" title="86:249	y := Best-Path(L) 6." ></td>
	<td class="line x" title="87:249	return y Figure 2: Hard constraints decoding algorithm." ></td>
	<td class="line x" title="88:249	and so on." ></td>
	<td class="line x" title="89:249	Eventually, we find some k for which yk satisfies all constraints, and this path is returned." ></td>
	<td class="line x" title="90:249	To determine whether a labeling y satisfies a constraint C, we represent y as a straight-line automaton and intersect with C, checking the result for nonemptiness." ></td>
	<td class="line x" title="91:249	This is equivalent to string recognition." ></td>
	<td class="line x" title="92:249	Our hope is that, although intractable in the worst case, the constraint relaxation algorithm will operate efficiently in practice." ></td>
	<td class="line x" title="93:249	The success of traditional sequence models on NLP tasks suggests that, for natural language, much of the correct analysis can be recovered from local features and constraints alone." ></td>
	<td class="line x" title="94:249	We suspect that, as a result, global constraints will often be easy to satisfy." ></td>
	<td class="line x" title="95:249	Pseudocode for the algorithm appears in Figure 2." ></td>
	<td class="line x" title="96:249	Note that line 2 does not specify how to choose C from among multiple violated constraints." ></td>
	<td class="line x" title="97:249	This is discussed in 7." ></td>
	<td class="line x" title="98:249	Our algorithm resembles the method of Koskenniemi (1990) and later work." ></td>
	<td class="line x" title="99:249	The difference is that there lattices are unweighted and may not contain a path that satisfies all constraints, so that the order of constraint intersection matters." ></td>
	<td class="line x" title="100:249	5 Semantic role labeling The semantic role labeling task (Carreras and M`arques, 2004) involves choosing instantiations of verb arguments from a sentence for a given verb." ></td>
	<td class="line x" title="101:249	The verb and its arguments form a proposition." ></td>
	<td class="line x" title="102:249	We use data from the CoNLL-2004 shared taskthe PropBank (Palmer et al. , 2005) annotations of the Penn Treebank (Marcus et al. , 1993), with sections 1518 as the training set and section 20 as the development set." ></td>
	<td class="line x" title="103:249	Unless otherwise specified, all measurements are made on the development set." ></td>
	<td class="line x" title="104:249	We follow Roth and Yih (2005) exactly, in order to compare system runtimes." ></td>
	<td class="line x" title="105:249	They, in turn, follow Hacioglu et al.(2004) and others in labeling only the heads of syntactic chunks rather than all words." ></td>
	<td class="line x" title="107:249	We label only the core arguments (A0A5), treating 425 (a) 0?" ></td>
	<td class="line x" title="108:249	1 A0 A0 2 ? ?" ></td>
	<td class="line x" title="109:249	(b) 0 1 O A0 A1 A2 A3 A4 A5 2 O (verb position) A1A2A3A4A5 O A0 (c) 0O A0A1A2A3 Figure 4: Automata expressing NO DUPLICATE A0 (?" ></td>
	<td class="line x" title="110:249	matches anything but A0), KNOWN VERB POSITION[2], and DISALLOW ARGUMENTS[A4,A5]." ></td>
	<td class="line x" title="111:249	adjuncts and references as O. Figure 3 shows an example sentence from the shared task." ></td>
	<td class="line x" title="112:249	It is marked with an IOB phrase chunking, the heads of the phrases, and the correct semantic role labeling." ></td>
	<td class="line x" title="113:249	Heads are taken to be the rightmost words of chunks." ></td>
	<td class="line x" title="114:249	On average, there are 18.8 phrases per proposition, vs. 23.5 words per sentence." ></td>
	<td class="line x" title="115:249	Sentences may contain multiple propositions." ></td>
	<td class="line x" title="116:249	There are 4305 propositions in section 20." ></td>
	<td class="line x" title="117:249	5.1 Constraints Roth and Yih use five global constraints on label sequences for the semantic role labeling task." ></td>
	<td class="line x" title="118:249	We express these constraints as FSAs." ></td>
	<td class="line x" title="119:249	The first two are general, and the seven automata encoding them can be constructed offline:  NO DUPLICATE ARGUMENT LABELS (Fig." ></td>
	<td class="line x" title="120:249	4(a)) requires that each verb have at most one argument of each type in a given sentence." ></td>
	<td class="line x" title="121:249	We separate this into six individual constraints, one for each core argument type." ></td>
	<td class="line x" title="122:249	Thus, we have constraints called NO DUPLICATE A0, NO DUPLICATE A1, etc. Each of these is represented as a three-state FSA." ></td>
	<td class="line x" title="123:249	 AT LEAST ONE ARGUMENT (Fig." ></td>
	<td class="line x" title="124:249	1) simply requires that the label sequence is not O." ></td>
	<td class="line x" title="125:249	This is a two-state automaton as described in 2." ></td>
	<td class="line x" title="126:249	The last three constraints require information about the example, and the automata must be constructed on a per-example basis:  ARGUMENT CANDIDATES (Fig." ></td>
	<td class="line x" title="127:249	5) encodes a set of position spans each of which must receive only a single label type." ></td>
	<td class="line x" title="128:249	These spans were proposed using a high-recall heuristic (Xue and Palmer, 2004)." ></td>
	<td class="line x" title="129:249	 KNOWN VERB POSITION (Fig." ></td>
	<td class="line x" title="130:249	4(b)) simply encodes the position of the verb in question, which must be labeled O.  DISALLOW ARGUMENTS (Fig." ></td>
	<td class="line x" title="131:249	4(c)) specifies argument types that are compatible with the verb in question, according to PropBank." ></td>
	<td class="line x" title="132:249	5.2 Experiments We implemented our hard constraint relaxation algorithm, using the FSA toolkit (Kanthak and Ney, 2004) for finite-state operations." ></td>
	<td class="line x" title="133:249	FSA is an opensource C++ library providing a useful set of algorithms on weighted finite-state acceptors and transducers." ></td>
	<td class="line x" title="134:249	For each example we decoded, we chose a random order in which to apply the constraints." ></td>
	<td class="line x" title="135:249	Lattices are generated from what amounts to a unigram modelthe voted perceptron classifier of Roth and Yih." ></td>
	<td class="line x" title="136:249	The features used are a subset of those commonly applied to the task." ></td>
	<td class="line x" title="137:249	Our system produces output identical to that of Roth and Yih." ></td>
	<td class="line x" title="138:249	Table 1 shows F-measure on the core arguments." ></td>
	<td class="line x" title="139:249	Table 2 shows a runtime comparison." ></td>
	<td class="line x" title="140:249	The ILP runtime was provided by the authors (personal communication)." ></td>
	<td class="line x" title="141:249	Because the systems were run under different conditions, the times are not directly comparable." ></td>
	<td class="line x" title="142:249	However, constraint relaxation is more than sixteen times faster than ILP despite running on a slower platform." ></td>
	<td class="line x" title="143:249	5.2.1 Comparison to an ILP solver Roth and Yihs linear program has two kinds of numeric constraints." ></td>
	<td class="line x" title="144:249	Some encode the shortest path problem structure; the others encode the global constraints of 5.1." ></td>
	<td class="line x" title="145:249	The ILP solver works by relaxing to a (real-valued) linear program, which may obtain a fractional solution that represents a path mixture instead of a path." ></td>
	<td class="line x" title="146:249	It then uses branch-and-bound to seek the optimal rounding of this fractional solution to an integer solution (Gueret et al. , 2002) that represents a single path satisfying the global constraints." ></td>
	<td class="line x" title="147:249	Our method avoids fractional solutions: a relaxed solution is always a true single path, which either 426 Mr. Turner said the test will be shipped in 45 days to hospitals and clinical laboratories . B-NP I-NP B-VP B-NP I-NP B-VP I-VP I-VP B-PP B-NP I-NP B-PP B-NP O B-NP I-NP O Turner said test shipped in days to hospitals and laboratories . A0 O A1 A1 A1 A1 A1 A1 A1 A1 O Figure 3: Example sentence, with phrase tags and heads, and core argument labels." ></td>
	<td class="line x" title="148:249	The A1 argument of said is a long clause." ></td>
	<td class="line x" title="149:249	0 1 O A0 A1 A2 A3 A4 A5 2 A2 A3 A4 A5 O A0 A1 4 O 10 A0 16 A1 22 A2 28 A3 34 A4 40 A5 5 O 11 A0 17 A1 23 A2 29 A3 35 A4 41 A5 42 A5 43 A5 44 A5 45 A5 3 A5 46 O A0 A1 A2 A3 A4 A5 O A0A1A2A3A4A5 36 A4 37 A4 38 A4 39 A4 A4 30 A3 31 A3 32 A3 33 A3 A3 24 A2 25 A2 26 A2 27 A2 A2 18 A1 19 A1 20 A1 21 A1 A1 12 A0 13 A0 14 A0 15 A0 A0 6 O 7 O 8 O 9 O O Figure 5: An automaton expressing ARGUMENT CANDIDATES." ></td>
	<td class="line x" title="150:249	Argument Count F-measure A0 2849 79.27 A1 4029 75.59 A2 943 55.68 A3 149 46.41 A4 147 81.82 A5 4 25.00 All 8121 74.51 Table 1: F-measure on core arguments." ></td>
	<td class="line x" title="151:249	satisfies or violates each global constraint." ></td>
	<td class="line x" title="152:249	In effect, we are using two kinds of domain knowledge." ></td>
	<td class="line x" title="153:249	First, we recognize that this is a graph problem, and insist on true paths so we can use Viterbi decoding." ></td>
	<td class="line x" title="154:249	Second, we choose to relax only domain-specific constraints that are likely to be satisfied anyway (in our domain), in contrast to the meta-constraint of integrality relaxed by ILP." ></td>
	<td class="line x" title="155:249	Thus it is cheaper on average for us to repair a relaxed solution." ></td>
	<td class="line x" title="156:249	(Our repair strategyfinite-state intersection in place of branchand-bound searchremains expensive in the worst case, as the problem is NP-hard.)" ></td>
	<td class="line x" title="157:249	5.2.2 Constraint violations The y0s, generated with only local information, satisfy most of the global constraints most of the time." ></td>
	<td class="line x" title="158:249	Table 3 shows the violations by type." ></td>
	<td class="line x" title="159:249	The majority of best labelings according to the local model dont violate any global constraints a fact especially remarkable because there are no label sequence features in Roth and Yihs unigram Constraint Violations Fraction ARGUMENT CANDIDATES 1619 0.376 NO DUPLICATE A1 899 0.209 NO DUPLICATE A0 348 0.081 NO DUPLICATE A2 151 0.035 AT LEAST ONE ARGUMENT 108 0.025 DISALLOW ARGUMENTS 48 0.011 NO DUPLICATE A3 13 0.003 NO DUPLICATE A4 3 0.001 NO DUPLICATE A5 1 0.000 KNOWN VERB POSITION 0 0.000 Table 3: Violations of constraints by y0." ></td>
	<td class="line x" title="160:249	model." ></td>
	<td class="line x" title="161:249	This confirms our intuition that natural language structure is largely apparent locally." ></td>
	<td class="line x" title="162:249	Table 4 shows the breakdown." ></td>
	<td class="line x" title="163:249	The majority of examples are very efficient to decode, because they dont require intersection of the lattice with any constraintsy0 is extracted and is good enough." ></td>
	<td class="line x" title="164:249	Those examples where constraints are violated are still relatively efficient because they only require a small number of intersections." ></td>
	<td class="line x" title="165:249	In total, the average number of intersections needed, even with the naive randomized constraint ordering, was only 0.65." ></td>
	<td class="line x" title="166:249	The order doesnt matter very much, since 75% of examples have one violation or fewer." ></td>
	<td class="line x" title="167:249	5.2.3 Effects on lattice size Figure 6 shows the effect of intersection with violated constraints on the average size of lattices, measured in arcs." ></td>
	<td class="line x" title="168:249	The vertical bars at k = 0, k = 1, . . ." ></td>
	<td class="line x" title="169:249	show the number of examples where con427 Method Total Time Per Example Platform Brute Force Finite-State 37m25.290s 0.522s Pentium III, 1.0 GHz ILP 11m39.220s 0.162s Xeon, 3.x GHz Constraint Relaxation 39.700s 0.009s Pentium III, 1.0 GHz Table 2: A comparison of runtimes for constrained decoding with ILP and FSA." ></td>
	<td class="line x" title="170:249	Violations Labelings Fraction Cumulative 0 2368 0.550 0.550 1 863 0.200 0.750 2 907 0.211 0.961 3 156 0.036 0.997 4 10 0.002 0.999 5 1 0.000 1.000 610 0 0.000 1.000 Table 4: Number of y0 with each violation count." ></td>
	<td class="line x" title="171:249	0 500 1000 1500 2000 2500 0 1 2 3 4 5 VerbsMean Arcs with Relaxation Mean Arcs with Brute Force Figure 6: Mean lattice size (measured in arcs) throughout decoding." ></td>
	<td class="line x" title="172:249	Vertical bars show the number of examples over which each mean is computed." ></td>
	<td class="line x" title="173:249	straint relaxation had to intersect k contraints (i.e. , y  yk)." ></td>
	<td class="line x" title="174:249	The trajectory ending at (for example) k = 3 shows how the average lattice size for that subset of examples evolved over the 3 intersections." ></td>
	<td class="line x" title="175:249	TheXat k = 3 shows the final size of the brute-force lattice on the same subset of examples." ></td>
	<td class="line x" title="176:249	For the most part, our lattices do stay much smaller than those produced by the brute-force algorithm." ></td>
	<td class="line x" title="177:249	(The uppermost curve, k = 5, is an obvious exception; however, that curve describes only the seven hardest examples)." ></td>
	<td class="line x" title="178:249	Note that plotting only the final size of the brute-force lattice obscures the long trajectory of its construction, which involves 10 intersections and, like the trajectories shown, includes larger intermediate automata.2 This explains the far 2The final brute-force lattice is especially shrunk by its inConstraint Violations Fraction ARGUMENT CANDIDATES 90 0.0209 AT LEAST ONE ARGUMENT 27 0.0063 NO DUPLICATE A2 3 0.0007 NO DUPLICATE A0 2 0.0005 NO DUPLICATE A1 2 0.0005 NO DUPLICATE A3 1 0.0002 NO DUPLICATE A4 1 0.0002 Table 5: Violations of constraints by y, measured over the development set." ></td>
	<td class="line x" title="179:249	longer runtime of the brute-force method (Table 2)." ></td>
	<td class="line x" title="180:249	Harder examples (corresponding to longer trajectories) have larger lattices, on average." ></td>
	<td class="line x" title="181:249	This is partly just because it is disproportionately the longer sentences that are hard: they have more opportunities for a relaxed decoding to violate global constraints." ></td>
	<td class="line x" title="182:249	Hard examples are rare." ></td>
	<td class="line x" title="183:249	The left three columns, requiring only 02 intersections, constitute 96% of examples." ></td>
	<td class="line x" title="184:249	The vast majority can be decoded without much more than doubling the local-lattice size." ></td>
	<td class="line x" title="185:249	6 Soft constraints The gold standard labels y occasionally violate the hard global constraints that we are using." ></td>
	<td class="line x" title="186:249	Counts for the development set appear in Table 5." ></td>
	<td class="line x" title="187:249	Counts for violations of NO DUPLICATE A do not include discontinous arguments, of which there are 104 instances, since we ignore them." ></td>
	<td class="line x" title="188:249	Because of the infrequency, the hard constraints still help most of the time." ></td>
	<td class="line x" title="189:249	However, on a small subset of the examples, they preclude us from inferring the correct labeling." ></td>
	<td class="line x" title="190:249	We can apply these constraints with weights, rather than making them inviolable." ></td>
	<td class="line x" title="191:249	This constitutes a transition from hard to soft constraints." ></td>
	<td class="line x" title="192:249	Formally, a soft constraint C: Y mapsto R is a mapping from a label sequence to a non-positive penalty." ></td>
	<td class="line x" title="193:249	Soft constraints present new difficulty for decodclusion of, for example, DISALLOW ARGUMENTS, which can only remove arcs." ></td>
	<td class="line x" title="194:249	That constraint is rarely included in the relaxation lattices because it is rarely violated (see Table 3)." ></td>
	<td class="line x" title="195:249	428 SOFT-CONSTRAIN-LATTICE(L, C): 1." ></td>
	<td class="line x" title="196:249	(y, Score(y)) := (empty,) 2." ></td>
	<td class="line x" title="197:249	branches := [(L,C,0)] 3." ></td>
	<td class="line x" title="198:249	while (L,C,penalty) := Dequeue(branches): 4." ></td>
	<td class="line x" title="199:249	L := Prune(L, Score(y)penalty) 5." ></td>
	<td class="line x" title="200:249	unless Empty(L): 6." ></td>
	<td class="line x" title="201:249	y := Best-Path(L) 7." ></td>
	<td class="line x" title="202:249	for C  C: 8." ></td>
	<td class="line x" title="203:249	if C(y) < 0: (* soC(y) = wC *) 9." ></td>
	<td class="line x" title="204:249	C := C{C} 10." ></td>
	<td class="line x" title="205:249	Enqueue(branches,(LC,C,penalty)) 11." ></td>
	<td class="line x" title="206:249	penalty := penalty + C(y) 12." ></td>
	<td class="line x" title="207:249	if Score(y) < L(y) + penalty: 13." ></td>
	<td class="line x" title="208:249	(y, Score(y)) := (y,L(y) + penalty) 14." ></td>
	<td class="line x" title="209:249	return y Figure 7: Soft constraints decoding algorithm ing, because instead of eliminating paths of L from contention, they just reweight them." ></td>
	<td class="line x" title="210:249	In what follows, we consider only binary soft constraintsthey are either satisfied or violated, and the same penalty is assessed whenever a violation occurs." ></td>
	<td class="line x" title="211:249	That is, C  C,wC < 0 such that y,C(y)  {0,wC}." ></td>
	<td class="line x" title="212:249	6.1 Soft constraint relaxation The decoding algorithm for soft constraints is a generalization of that for hard constraints." ></td>
	<td class="line x" title="213:249	The difference is that, whereas with hard constraints a violation meant disqualification, here violation simply means a penalty." ></td>
	<td class="line x" title="214:249	We therefore must find and compare two labelings: the best that satisfies the constraint, and the best that violates it." ></td>
	<td class="line x" title="215:249	We present a branch-and-bound algorithm (Lawler and Wood, 1966), with pseudocode in Figure 7." ></td>
	<td class="line x" title="216:249	At line 9, we process and eliminate a currently violated constraint C  C by considering two cases." ></td>
	<td class="line x" title="217:249	On the first branch, we insist that C be satisfied, enqueuing LC for later exploration." ></td>
	<td class="line x" title="218:249	On the second branch, we assume C is violated by all paths, and so continue considering L unmodified, but accept a penalty for doing so; we immediately explore the second branch by returning to the start of the for loop.3 Not every branch needs to be completely explored." ></td>
	<td class="line x" title="219:249	Bounding is handled by the PRUNE function at line 4, which shrinks L by removing some 3It is possible that a future best path on the second branch will not actually violate C, in which case we have overpenalized it, but in that case we will also find it with correct penalty on the first branch." ></td>
	<td class="line x" title="220:249	or all paths that cannot score better than Score(y), the score of the best path found on any branch so far." ></td>
	<td class="line x" title="221:249	Our experiments used almost the simplest possible PRUNE: replace L by the empty lattice if the best path falls below the bound, else leave L unchanged.4 A similar bounding would be possible in the implicit branches." ></td>
	<td class="line x" title="222:249	If, during the for loop, we find that the test at line 12 would fail, we can quit the for loop and immediately move to the next branch in the queue at line 3." ></td>
	<td class="line x" title="223:249	There are two factors in this algorithm that contribute to avoiding consideration of all of the exponential number of leaves corresponding to the power set of constraints." ></td>
	<td class="line x" title="224:249	First, bounding stops evaluation of subtrees." ></td>
	<td class="line x" title="225:249	Second, only violated constraints require branching." ></td>
	<td class="line x" title="226:249	If a lattices best path satisifies a constraint, then the best path that violates it can be no better since, by assumption, y,C(y)  0." ></td>
	<td class="line x" title="227:249	6.2 Runtime experiments Using the ten constraints from 5.1, weighted naively by their log odds of violation, the soft constraint relaxation algorithm runs in a time of 58.40 seconds." ></td>
	<td class="line x" title="228:249	It is, as expected, slower than hard constraint relaxation, but only by a factor of about two." ></td>
	<td class="line x" title="229:249	As a side note, softening these particular constraints in this particular way did not improve decoding quality in this case." ></td>
	<td class="line x" title="230:249	It might help to jointly train the relative weights of these constraints and the local modele.g. , using a perceptron algorithm (Freund and Schapire, 1998), which repeatedly extracts the best global path (using our algorithm), compares it to the gold standard, and adjusts the constraint weights." ></td>
	<td class="line x" title="231:249	An obvious alternative is maximumentropy training, but the partition function would have to be computed using the large brute-force lattices, or else approximated by a sampling method." ></td>
	<td class="line x" title="232:249	7 Future work For a given task, we may be able to obtain further speedups by carefully choosing the order in which to test and apply the constraints." ></td>
	<td class="line x" title="233:249	We might treat this as a reinforcement learning problem (Sutton, 1988), 4Partial pruning is also possible: by running the Viterbi version of the forward-backward algorithm, one can discover for each edge the weight of the best path on which it appears." ></td>
	<td class="line x" title="234:249	One can then remove all edges that do not appear on any sufficiently good path." ></td>
	<td class="line x" title="235:249	429 where an agent will obtain rewards by finding y quickly." ></td>
	<td class="line x" title="236:249	In the hard-constraint algorithm, for example, the agents possible moves are to test some constraint for violation by the current best path, or to intersect some constraint with the current lattice." ></td>
	<td class="line x" title="237:249	Several features can help the agent choose the next move." ></td>
	<td class="line x" title="238:249	How large is the current lattice, which constraints does it already incorporate, and which remaining constraints are already known to be satisfied or violated by its best path?" ></td>
	<td class="line x" title="239:249	And what were the answers to those questions at previous stages?" ></td>
	<td class="line x" title="240:249	Our constraint relaxation method should be tested on problems other than semantic role labeling." ></td>
	<td class="line x" title="241:249	For example, information extraction from bibliography entries, as discussed in 1, has about 13 fields to extract, and interesting hard and soft global constraints on co-occurrence, order, and adjacency." ></td>
	<td class="line x" title="242:249	The method should also be evaluated on a task with longer sequences: though the finite-state operations we use do scale up linearly with the sequence length, longer sequences have more chance of violating a global constraint somewhere in the sequence, requiring us to apply that constraint explicitly." ></td>
	<td class="line x" title="243:249	8 Conclusion Roth and Yih (2005) showed that global constraints can improve the output of sequence labeling models for semantic role labeling." ></td>
	<td class="line x" title="244:249	In general, decoding under such constraints is NP-complete." ></td>
	<td class="line x" title="245:249	We exhibited a practical approach, finite-state constraint relaxation, that greatly sped up decoding on this NLP task by using familiar finite-state operationsweighted FSA intersection and best-path extractionrather than integer linear programming." ></td>
	<td class="line x" title="246:249	We have also given a constraint relaxation algorithm for binary soft constraints." ></td>
	<td class="line x" title="247:249	This allows incorporation of constraints akin to reranking features, in addition to inviolable constraints." ></td>
	<td class="line x" title="248:249	Acknowledgments This material is based upon work supported by the National Science Foundation under Grant No. 0347822." ></td>
	<td class="line x" title="249:249	We thank Scott Yih for kindly providing both the voted-perceptron classifier and runtime results for decoding with ILP, and the reviewers for helpful comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-1059
Improving The Scalability Of Semi-Markov Conditional Random Fields For Named Entity Recognition
Okanohara, Daisuke;Miyao, Yusuke;Tsuruoka, Yoshimasa;Tsujii, Jun'ichi;"></td>
	<td class="line x" title="1:177	Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 465472, Sydney, July 2006." ></td>
	<td class="line x" title="2:177	c2006 Association for Computational Linguistics Improving the Scalability of Semi-Markov Conditional Random Fields for Named Entity Recognition Daisuke Okanohara Yusuke Miyao Yoshimasa Tsuruoka  Junichi Tsujii Department of Computer Science, University of Tokyo Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan School of Informatics, University of Manchester POBox 88, Sackville St, MANCHESTER M60 1QD, UK SORST, Solution Oriented Research for Science and Technology Honcho 4-1-8, Kawaguchi-shi, Saitama, Japan {hillbig,yusuke,tsuruoka,tsujii}@is.s.u-tokyo.ac.jp Abstract This paper presents techniques to apply semi-CRFs to Named Entity Recognition tasks with a tractable computational cost." ></td>
	<td class="line x" title="3:177	Our framework can handle an NER task that has long named entities and many labels which increase the computational cost." ></td>
	<td class="line x" title="4:177	To reduce the computational cost, we propose two techniques: the first is the use of feature forests, which enables us to pack feature-equivalent states, and the second is the introduction of a filtering process which significantly reduces the number of candidate states." ></td>
	<td class="line x" title="5:177	This framework allows us to use a rich set of features extracted from the chunk-based representation that can capture informative characteristics of entities." ></td>
	<td class="line x" title="6:177	We also introduce a simple trick to transfer information about distant entities by embedding label information into non-entity labels." ></td>
	<td class="line x" title="7:177	Experimental results show that our model achieves an F-score of 71.48% on the JNLPBA 2004 shared task without using any external resources or post-processing techniques." ></td>
	<td class="line x" title="8:177	1 Introduction The rapid increase of information in the biomedical domain has emphasized the need for automated information extraction techniques." ></td>
	<td class="line x" title="9:177	In this paper we focus on the Named Entity Recognition (NER) task, which is the first step in tackling more complex tasks such as relation extraction and knowledge mining." ></td>
	<td class="line x" title="10:177	Biomedical NER (Bio-NER) tasks are, in general, more difficult than ones in the news domain." ></td>
	<td class="line x" title="11:177	For example, the best F-score in the shared task of Bio-NER in COLING 2004 JNLPBA (Kim et al. , 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC-6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995)." ></td>
	<td class="line o" title="12:177	Many of the previous studies of Bio-NER tasks have been based on machine learning techniques including Hidden Markov Models (HMMs) (Bikel et al. , 1997), the dictionary HMM model (Kou et al. , 2005) and Maximum Entropy Markov Models (MEMMs) (Finkel et al. , 2004)." ></td>
	<td class="line x" title="13:177	Among these methods, conditional random fields (CRFs) (Lafferty et al. , 2001) have achieved good results (Kim et al. , 2005; Settles, 2004), presumably because they are free from the so-called label bias problem by using a global normalization." ></td>
	<td class="line x" title="14:177	Sarawagi and Cohen (2004) have recently introduced semi-Markov conditional random fields (semi-CRFs)." ></td>
	<td class="line x" title="15:177	They are defined on semi-Markov chains and attach labels to the subsequences of a sentence, rather than to the tokens2." ></td>
	<td class="line x" title="16:177	The semiMarkov formulation allows one to easily construct entity-level features." ></td>
	<td class="line x" title="17:177	Since the features can capture all the characteristics of a subsequence, we can use, for example, a dictionary feature which measures the similarity between a candidate segment and the closest element in the dictionary." ></td>
	<td class="line x" title="18:177	Kou et al.(2005) have recently showed that semiCRFs perform better than CRFs in the task of recognition of protein entities." ></td>
	<td class="line x" title="20:177	The main difficulty of applying semi-CRFs to Bio-NER lies in the computational cost at training 1Krauthammer (2004) reported that the inter-annotator agreement rate of human experts was 77.6% for bio-NLP, which suggests that the upper bound of the F-score in a BioNER task may be around 80%." ></td>
	<td class="line x" title="21:177	2Assuming that non-entity words are placed in unit-length segments." ></td>
	<td class="line x" title="22:177	465 Table 1: Length distribution of entities in the training set of the shared task in 2004 JNLPBA Length # entity Ratio 1 21646 42.19 2 15442 30.10 3 7530 14.68 4 3505 6.83 5 1379 2.69 6 732 1.43 7 409 0.80 8 252 0.49 >8 406 0.79 total 51301 100.00 because the number of named entity classes tends to be large, and the training data typically contain many long entities, which makes it difficult to enumerate all the entity candidates in training." ></td>
	<td class="line x" title="23:177	Table 1 shows the length distribution of entities in the training set of the shared task in 2004 JNLPBA." ></td>
	<td class="line x" title="24:177	Formally, the computational cost of training semiCRFs is O(KLN), where L is the upper bound length of entities, N is the length of sentence and K is the size of label set." ></td>
	<td class="line x" title="25:177	And that of training in first order semi-CRFs is O(K2LN)." ></td>
	<td class="line x" title="26:177	The increase of the cost is used to transfer non-adjacent entity information." ></td>
	<td class="line x" title="27:177	To improve the scalability of semi-CRFs, we propose two techniques: the first is to introduce a filtering process that significantly reduces the number of candidate entities by using a lightweight classifier, and the second is to use feature forest (Miyao and Tsujii, 2002), with which we pack the feature equivalent states." ></td>
	<td class="line x" title="28:177	These enable us to construct semi-CRF models for the tasks where entity names may be long and many class-labels exist at the same time." ></td>
	<td class="line x" title="29:177	We also present an extended version of semi-CRFs in which we can make use of information about a preceding named entity in defining features within the framework of first order semi-CRFs." ></td>
	<td class="line x" title="30:177	Since the preceding entity is not necessarily adjacent to the current entity, we achieve this by embedding the information on preceding labels for named entities into the labels for non-named entities." ></td>
	<td class="line x" title="31:177	2 CRFs and Semi-CRFs CRFs are undirected graphical models that encode a conditional probability distribution using a given set of features." ></td>
	<td class="line x" title="32:177	CRFs allow both discriminative training and bi-directional flow of probabilistic information along the sequence." ></td>
	<td class="line x" title="33:177	In NER, we often use linear-chain CRFs, which define the conditional probability of a state sequence y = y1, , yn given the observed sequence x = x1,,xn by: p(y|x,) = 1Z(x) exp(ni=1jjfj(yi1,yi,x,i)), (1) where fj(yi1,yi,x,i) is a feature function and Z(x) is the normalization factor over all the state sequences for the sequence x. The model parameters are a set of real-valued weights  = {j}, each of which represents the weight of a feature." ></td>
	<td class="line x" title="34:177	All the feature functions are real-valued and can use adjacent label information." ></td>
	<td class="line x" title="35:177	Semi-CRFs are actually a restricted version of order-L CRFs in which all the labels in a chunk are the same." ></td>
	<td class="line x" title="36:177	We follow the definitions in (Sarawagi and Cohen, 2004)." ></td>
	<td class="line x" title="37:177	Let s = s1,,sp denote a segmentation of x, where a segment sj = tj, uj, yj consists of a start position tj, an end position uj, and a label yj." ></td>
	<td class="line x" title="38:177	We assume that segments have a positive length bounded above by the pre-defined upper bound L (tj  uj, uj  tj + 1  L) and completely cover the sequence x without overlapping, that is, s satisfies t1 = 1, up = |x|, and tj+1 = uj + 1 for j = 1,,p  1." ></td>
	<td class="line x" title="39:177	Semi-CRFs define a conditional probability of a state sequence y given an observed sequence x by: p(y|x,) = 1Z(x) exp(jiifi(sj)), (2) where fi(sj) := fi(yj1,yj,x,tj,uj) is a feature function and Z(x) is the normalization factor as defined for CRFs." ></td>
	<td class="line x" title="40:177	The inference problem for semi-CRFs can be solved by using a semi-Markov analog of the usual Viterbi algorithm." ></td>
	<td class="line x" title="41:177	The computational cost for semi-CRFs is O(KLN) where L is the upper bound length of entities, N is the length of sentence and K is the number of label set." ></td>
	<td class="line x" title="42:177	If we use previous label information, the cost becomes O(K2LN)." ></td>
	<td class="line x" title="43:177	3 Using Non-Local Information in Semi-CRFs In conventional CRFs and semi-CRFs, one can only use the information on the adjacent previous label when defining the features on a certain state or entity." ></td>
	<td class="line x" title="44:177	In NER tasks, however, information about a distant entity is often more useful than 466 O protein O O DNA O protein O-protein O-protein DNA Figure 1: Modification of O (other labels) to transfer information on a preceding named entity." ></td>
	<td class="line oc" title="45:177	information about the previous state (Finkel et al. , 2005)." ></td>
	<td class="line x" title="46:177	For example, consider the sentence  including Sp1 and CP1. where the correct labels of Sp1 and CP1 are both protein." ></td>
	<td class="line x" title="47:177	It would be useful if the model could utilize the (non-adjacent) information about Sp1 being protein to classify CP1 as protein." ></td>
	<td class="line x" title="48:177	On the other hand, information about adjacent labels does not necessarily provide useful information because, in many cases, the previous label of a named entity is O, which indicates a non-named entity." ></td>
	<td class="line x" title="49:177	For 98.0% of the named entities in the training data of the shared task in the 2004 JNLPBA, the label of the preceding entity was O." ></td>
	<td class="line x" title="50:177	In order to incorporate such non-local information into semi-CRFs, we take a simple approach." ></td>
	<td class="line x" title="51:177	We divide the label of O into O-protein and O so that they convey the information on the preceding named entity." ></td>
	<td class="line x" title="52:177	Figure 1 shows an example of this conversion, in which the two labels for the third and fourth states are converted from O to O-protein." ></td>
	<td class="line x" title="53:177	When we define the features for the fifth state, we can use the information on the preceding entity protein by looking at the fourth state." ></td>
	<td class="line x" title="54:177	Since this modification changes only the label set, we can do this within the framework of semi-CRF models." ></td>
	<td class="line x" title="55:177	This idea is originally proposed in (Peshkin and Pfeffer, 2003)." ></td>
	<td class="line x" title="56:177	However, they used a dynamic Bayesian network (DBNs) rather than a semi-CRF, and semi-CRFs are likely to have significantly better performance than DBNs." ></td>
	<td class="line x" title="57:177	In previous work, such non-local information has usually been employed at a post-processing stage." ></td>
	<td class="line x" title="58:177	This is because the use of long distance dependency violates the locality of the model and prevents us from using dynamic programming techniques in training and inference." ></td>
	<td class="line x" title="59:177	Skip-CRFs (Sutton and McCallum, 2004) are a direct implementation of long distance effects to the model." ></td>
	<td class="line x" title="60:177	However, they need to determine the structure for propagating non-local information in advance." ></td>
	<td class="line nc" title="61:177	In a recent study by Finkel et al. , (2005), nonlocal information is encoded using an independence model, and the inference is performed by Gibbs sampling, which enables us to use a stateof-the-art factored model and carry out training efficiently, but inference still incurs a considerable computational cost." ></td>
	<td class="line x" title="62:177	Since our model handles limited type of non-local information, i.e. the label of the preceding entity, the model can be solved without approximation." ></td>
	<td class="line x" title="63:177	4 Reduction of Training/Inference Cost The straightforward implementation of this modeling in semi-CRFs often results in a prohibitive computational cost." ></td>
	<td class="line x" title="64:177	In biomedical documents, there are quite a few entity names which consist of many words (names of 8 words in length are not rare)." ></td>
	<td class="line x" title="65:177	This makes it difficult for us to use semi-CRFs for biomedical NER, because we have to set L to be eight or larger, where L is the upper bound of the length of possible chunks in semi-CRFs." ></td>
	<td class="line x" title="66:177	Moreover, in order to take into account the dependency between named entities of different classes appearing in a sentence, we need to incorporate multiple labels into a single probabilistic model." ></td>
	<td class="line x" title="67:177	For example, in the shared task in COLING 2004 JNLPBA (Kim et al. , 2004) the number of labels is six (protein, DNA, RNA, cell line, cell type and other)." ></td>
	<td class="line x" title="68:177	This also increases the computational cost of a semi-CRF model." ></td>
	<td class="line x" title="69:177	To reduce the computational cost, we propose two methods (see Figure 2)." ></td>
	<td class="line x" title="70:177	The first is employing a filtering process using a lightweight classifier to remove unnecessary state candidates beforehand (Figure 2 (2)), and the second is the using the feature forest model (Miyao and Tsujii, 2002) (Figure 2 (3)), which employs dynamic programming at training as much as possible." ></td>
	<td class="line x" title="71:177	4.1 Filtering with a naive Bayes classifier We introduce a filtering process to remove low probability candidate states." ></td>
	<td class="line x" title="72:177	This is the first step of our NER system." ></td>
	<td class="line x" title="73:177	After this filtering step, we construct semi-CRFs on the remaining candidate states using a feature forest." ></td>
	<td class="line x" title="74:177	Therefore the aim of this filtering is to reduce the number of candidate states, without removing correct entities." ></td>
	<td class="line x" title="75:177	This idea 467 (1) EnumerateCandidate States(2) Filtering byNave Bayes (3) Construct feature forest Training/Inference : other : entity : other with preceding entity information Figure 2: The framework of our system." ></td>
	<td class="line x" title="76:177	We first enumerate all possible candidate states, and then filter out low probability states by using a light-weight classifier, and represent them by using feature forest." ></td>
	<td class="line x" title="77:177	Table 2: Features used in the naive Bayes Classifier for the entity candidate: ws, ws+1,  , we." ></td>
	<td class="line x" title="78:177	spi is the result of shallow parsing at wi." ></td>
	<td class="line x" title="79:177	Feature Name Example of Features Start/End Word ws, we Inside Word ws, ws+1,  , we Context Word ws1, we+1 Start/End SP sps, spe Inside SP sps, sps+1,  , spe Context SP sps1, spe+1 is similar to the method proposed by Tsuruoka and Tsujii (2005) for chunk parsing, in which implausible phrase candidates are removed beforehand." ></td>
	<td class="line x" title="80:177	We construct a binary naive Bayes classifier using the same training data as those for semi-CRFs." ></td>
	<td class="line x" title="81:177	In training and inference, we enumerate all possible chunks (the max length of a chunk is L as for semi-CRFs) and then classify those into entity or other." ></td>
	<td class="line x" title="82:177	Table 2 lists the features used in the naive Bayes classifier." ></td>
	<td class="line x" title="83:177	This process can be performed independently of semi-CRFs Since the purpose of the filtering is to reduce the computational cost, rather than to achieve a good F-score by itself, we chose the threshold probability of filtering so that the recall of filtering results would be near 100 %." ></td>
	<td class="line x" title="84:177	4.2 Feature Forest In estimating semi-CRFs, we can use an efficient dynamic programming algorithm, which is similar to the forward-backward algorithm (Sarawagi and Cohen, 2004)." ></td>
	<td class="line x" title="85:177	The proposal here is a more general framework for estimating sequential conditional random fields." ></td>
	<td class="line x" title="86:177	This framework is based on the feature forest DNA protein Other DNA protein Other : ornode (disjunctive node) : and node (conjunctive node) pos i i+1  Figure 3: Example of feature forest representation of linear chain CRFs." ></td>
	<td class="line x" title="87:177	Feature functions are assigned to and nodes." ></td>
	<td class="line x" title="88:177	protein O-protein protein uj =8 prev-entity:protein uj = 8prev-entity: protein packed pos 87 9 Figure 4: Example of packed representation of semi-CRFs." ></td>
	<td class="line x" title="89:177	The states that have the same end position and prev-entity label are packed." ></td>
	<td class="line x" title="90:177	model, which was originally proposed for disambiguation models for parsing (Miyao and Tsujii, 2002)." ></td>
	<td class="line x" title="91:177	A feature forest model is a maximum entropy model defined over feature forests, which are abstract representations of an exponential number of sequence/tree structures." ></td>
	<td class="line x" title="92:177	A feature forest is an and/or graph: in Figure 3, circles represent 468 and nodes (conjunctive nodes), while boxes denote or nodes (disjunctive nodes)." ></td>
	<td class="line x" title="93:177	Feature functions are assigned to and nodes." ></td>
	<td class="line x" title="94:177	We can use the information of the previous and node for designing the feature functions through the previous or node." ></td>
	<td class="line x" title="95:177	Each sequence in a feature forest is obtained by choosing a conjunctive node for each disjunctive node." ></td>
	<td class="line x" title="96:177	For example, Figure 3 represents 3  3 = 9 sequences, since each disjunctive node has three candidates." ></td>
	<td class="line x" title="97:177	It should be noted that feature forests can represent an exponential number of sequences with a polynomial number of conjunctive/disjunctive nodes." ></td>
	<td class="line x" title="98:177	One can estimate a maximum entropy model for the whole sequence with dynamic programming by representing the probabilistic events, i.e. sequence of named entity tags, by feature forests (Miyao and Tsujii, 2002)." ></td>
	<td class="line x" title="99:177	In the previous work (Lafferty et al. , 2001; Sarawagi and Cohen, 2004), or nodes are considered implicitly in the dynamic programming framework." ></td>
	<td class="line x" title="100:177	In feature forest models, or nodes are packed when they have same conditions." ></td>
	<td class="line x" title="101:177	For example, or nodes are packed when they have same end positions and same labels in the first order semi-CRFs, In general, we can pack different or nodes that yield equivalent feature functions in the following nodes." ></td>
	<td class="line x" title="102:177	In other words, or nodes are packed when the following states use partial information on the preceding states." ></td>
	<td class="line x" title="103:177	Consider the task of tagging entity and O-entity, where the latter tag is actually O tags that distinguish the preceding named entity tags." ></td>
	<td class="line x" title="104:177	When we simply apply first-order semi-CRFs, we must distinguish states that have different previous states." ></td>
	<td class="line x" title="105:177	However, when we want to distinguish only the preceding named entity tags rather than the immediate previous states, feature forests can represent these events more compactly (Figure 4)." ></td>
	<td class="line x" title="106:177	We can implement this as follows." ></td>
	<td class="line x" title="107:177	In each or node, we generate the following and nodes and their feature functions." ></td>
	<td class="line x" title="108:177	Then we check whether there exist or node which has same conditions by using its information about end position and previous entity." ></td>
	<td class="line x" title="109:177	If so, we connect the and node to the corresponding or node." ></td>
	<td class="line x" title="110:177	If not, we generate a new or node and continue the process." ></td>
	<td class="line x" title="111:177	Since the states with label O-entity and entity are packed, the computational cost of training in our model (First order semi-CRFs) becomes the half of the original one." ></td>
	<td class="line x" title="112:177	5 Experiments 5.1 Experimental Setting Our experiments were performed on the training and evaluation set provided by the shared task in COLING 2004 JNLPBA (Kim et al. , 2004)." ></td>
	<td class="line x" title="113:177	The training data used in this shared task came from the GENIA version 3.02 corpus." ></td>
	<td class="line x" title="114:177	In the task there are five semantic labels: protein, DNA, RNA, cell line and cell type." ></td>
	<td class="line x" title="115:177	The training set consists of 2000 abstracts from MEDLINE, and the evaluation set consists of 404 abstracts." ></td>
	<td class="line x" title="116:177	We divided the original training set into 1800 abstracts and 200 abstracts, and the former was used as the training data and the latter as the development data." ></td>
	<td class="line x" title="117:177	For semi-CRFs, we used amis3 for training the semiCRF with feature-forest." ></td>
	<td class="line x" title="118:177	We used GENIA taggar4 for POS-tagging and shallow parsing." ></td>
	<td class="line x" title="119:177	We set L = 10 for training and evaluation when we do not state L explicitly, where L is the upper bound of the length of possible chunks in semiCRFs." ></td>
	<td class="line x" title="120:177	5.2 Features Table 3 lists the features used in our semi-CRFs." ></td>
	<td class="line x" title="121:177	We describe the chunk-dependent features in detail, which cannot be encoded in token-level features." ></td>
	<td class="line x" title="122:177	Whole chunk is the normalized names attached to a chunk, which performs like the closed dictionary." ></td>
	<td class="line x" title="123:177	Length and Length and EndWord capture the tendency of the length of a named entity." ></td>
	<td class="line x" title="124:177	Count feature captures the tendency for named entities to appear repeatedly in the same sentence." ></td>
	<td class="line x" title="125:177	Preceding Entity and Prev Word are features that capture specifically words for conjunctions such as and or , (comma), e.g., for the phrase OCIM1 and K562, both OCIM1 and K562 are assigned cell line labels." ></td>
	<td class="line x" title="126:177	Even if the model can determine only that OCIM1 is a cell line, this feature helps K562 to be assigned the label cell line." ></td>
	<td class="line x" title="127:177	5.3 Results We first evaluated the filtering performance." ></td>
	<td class="line x" title="128:177	Table 4 shows the result of the filtering on the training 3http://www-tsujii.is.s.u-tokyo.ac.jp/amis/ 4http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/ Note that the evaluation data are not used for training the GENIA tagger." ></td>
	<td class="line x" title="129:177	469 Table 3: Feature templates used for the chunk s := ws ws+1  we where ws and we represent the words at the beginning and ending of the target chunk respectively." ></td>
	<td class="line x" title="130:177	pi is the part of speech tag of wi and sci is the shallow parse result of wi." ></td>
	<td class="line x" title="131:177	Feature Name description of features Non-Chunk Features Word/POS/SC with Position BEGIN + ws, END + we, IN + ws+1,  , IN + we1, BEGIN + ps, Context Uni-gram/Bi-gram ws1, we+1, ws2 + ws1, we+1 + we+2, ws1 + we+1 Prefix/Suffix of Chunk 2/3-gram character prefix of ws, 2/3/4-gram character suffix of we Orthography capitalization and word formation of wswe Chunk Features Whole chunk ws + ws+1 +  + we Word/POS/SC End Bi-grams we1 + we, pe1 + pe, sce1 + sce Length, Length and End Word |s|, |s|+we Count Feature the frequency of wsws+1we in a sentence is greater than one Preceding Entity Features Preceding Entity /and Prev Word PrevState, PrevState + ws1 Table 4: Filtering results using the naive Bayes classifier." ></td>
	<td class="line x" title="132:177	The number of entity candidates for the training set was 4179662, and that of the development set was 418628." ></td>
	<td class="line x" title="133:177	Training set Threshold probability reduction ratio recall 1.0  1012 0.14 0.984 1.0  1015 0.20 0.993 Development set Threshold probability reduction ratio recall 1.0  1012 0.14 0.985 1.0  1015 0.20 0.994 and evaluation data." ></td>
	<td class="line x" title="134:177	The naive Bayes classifiers effectively reduced the number of candidate states with very few falsely removed correct entities." ></td>
	<td class="line x" title="135:177	We then examined the effect of filtering on the final performance." ></td>
	<td class="line x" title="136:177	In this experiment, we could not examine the performance without filtering using all the training data, because training on all the training data without filtering required much larger memory resources (estimated to be about 80G Byte) than was possible for our experimental setup." ></td>
	<td class="line x" title="137:177	We thus compared the result of the recognizers with and without filtering using only 2000 sentences as the training data." ></td>
	<td class="line x" title="138:177	Table 5 shows the result of the total system with different filtering thresholds." ></td>
	<td class="line x" title="139:177	The result indicates that the filtering method achieved very well without decreasing the overall performance." ></td>
	<td class="line x" title="140:177	We next evaluate the effect of filtering, chunk information and non-local information on final performance." ></td>
	<td class="line x" title="141:177	Table 6 shows the performance result for the recognition task." ></td>
	<td class="line x" title="142:177	L means the upper bound of the length of possible chunks in semiCRFs." ></td>
	<td class="line x" title="143:177	We note that we cannot examine the result of L = 10 without filtering because of the intractable computational cost." ></td>
	<td class="line x" title="144:177	The row w/o Chunk Feature shows the result of the system which does not employ Chunk-Features in Table 3 at training and inference." ></td>
	<td class="line x" title="145:177	The row Preceding Entity shows the result of a system which uses Preceding Entity and Preceding Entity and Prev Word features." ></td>
	<td class="line x" title="146:177	The results indicate that the chunk features contributed to the performance, and the filtering process enables us to use full chunk representation (L = 10)." ></td>
	<td class="line x" title="147:177	The results of McNemars test suggest that the system with chunk features is significantly better than the system without it (the p-value is less than 1.0 < 104)." ></td>
	<td class="line x" title="148:177	The result of the preceding entity information improves the performance." ></td>
	<td class="line x" title="149:177	On the other hand, the system with preceding information is not significantly better than the system without it5." ></td>
	<td class="line x" title="150:177	Other non-local information may improve performance with our framework and this is a topic for future work." ></td>
	<td class="line x" title="151:177	Table 7 shows the result of the overall performance in our best setting, which uses the information about the preceding entity and 1.01015 threshold probability for filtering." ></td>
	<td class="line x" title="152:177	We note that the result of our system is similar to those of other sys5The result of the classifier on development data is 74.64 (without preceding information) and 75.14 (with preceding information)." ></td>
	<td class="line x" title="153:177	470 Table 5: Performance with filtering on the development data." ></td>
	<td class="line x" title="154:177	(< 1.0  1012) means the threshold probability of the filtering is 1.0  1012." ></td>
	<td class="line x" title="155:177	Recall Precision F-score Memory Usage (MB) Training Time (s) Small Training Data = 2000 sentences Without filtering 65.77 72.80 69.10 4238 7463 Filtering (< 1.0  10.012) 64.22 70.62 67.27 600 1080 Filtering (< 1.0  10.015) 65.34 72.52 68.74 870 2154 All Training Data = 16713 sentences Without filtering Not available Not available Filtering (< 1.0  10.012) 70.05 76.06 72.93 10444 14661 Filtering (< 1.0  10.015) 72.09 78.47 75.14 15257 31636 Table 6: Overall performance on the evaluation set." ></td>
	<td class="line x" title="156:177	L is the upper bound of the length of possible chunks in semi-CRFs." ></td>
	<td class="line x" title="157:177	Recall Precision F-score L < 5 64.33 65.51 64.92 L = 10 + Filtering (< 1.0  10.012) 70.87 68.33 69.58 L = 10 + Filtering (< 1.0  10.015) 72.59 70.16 71.36 w/o Chunk Feature 70.53 69.92 70.22 + Preceding Entity 72.65 70.35 71.48 tems in several respects, that is, the performance of cell line is not good, and the performance of the right boundary identification (78.91% in F-score) is better than that of the left boundary identification (75.19% in F-score)." ></td>
	<td class="line x" title="158:177	Table 8 shows a comparison between our system and other state-of-the-art systems." ></td>
	<td class="line x" title="159:177	Our system has achieved a comparable performance to these systems and would be still improved by using external resources or conducting pre/post processing." ></td>
	<td class="line x" title="160:177	For example, Zhou et." ></td>
	<td class="line x" title="161:177	al (2004) used post processing, abbreviation resolution and external dictionary, and reported that they improved Fscore by 3.1%, 2.1% and 1.2% respectively." ></td>
	<td class="line x" title="162:177	Kim et." ></td>
	<td class="line x" title="163:177	al (2005) used the original GENIA corpus to employ the information about other semantic classes for identifying term boundaries." ></td>
	<td class="line x" title="164:177	Finkel et." ></td>
	<td class="line x" title="165:177	al (2004) used gazetteers, web-querying, surrounding abstracts, and frequency counts from the BNC corpus." ></td>
	<td class="line x" title="166:177	Settles (2004) used semantic domain knowledge of 17 types of lexicon." ></td>
	<td class="line x" title="167:177	Since our approach and the use of external resources/knowledge do not conflict but are complementary, examining the combination of those techniques should be an interesting research topic." ></td>
	<td class="line x" title="168:177	Table 7: Performance of our system on the evaluation set Class Recall Precision F-score protein 77.74 68.92 73.07 DNA 69.03 70.16 69.59 RNA 69.49 67.21 68.33 cell type 65.33 82.19 72.80 cell line 57.60 53.14 55.28 overall 72.65 70.35 71.48 Table 8: Comparison with other systems System Recall Precision F-score Zhou et." ></td>
	<td class="line x" title="169:177	al (2004) 75.99 69.42 72.55 Our system 72.65 70.35 71.48 Kim et.al (2005) 72.77 69.68 71.19 Finkel et." ></td>
	<td class="line x" title="170:177	al (2004) 68.56 71.62 70.06 Settles (2004) 70.3 69.3 69.8 471 6 Conclusion In this paper, we have proposed a single probabilistic model that can capture important characteristics of biomedical named entities." ></td>
	<td class="line x" title="171:177	To overcome the prohibitive computational cost, we have presented an efficient training framework and a filtering method which enabled us to apply first order semi-CRF models to sentences having many labels and entities with long names." ></td>
	<td class="line x" title="172:177	Our results showed that our filtering method works very well without decreasing the overall performance." ></td>
	<td class="line x" title="173:177	Our system achieved an F-score of 71.48% without the use of gazetteers, post-processing or external resources." ></td>
	<td class="line x" title="174:177	The performance of our system came close to that of the current best performing system which makes extensive use of external resources and rule based post-processing." ></td>
	<td class="line x" title="175:177	The contribution of the non-local information introduced by our method was not significant in the experiments." ></td>
	<td class="line pc" title="176:177	However, other types of nonlocal information have also been shown to be effective (Finkel et al. , 2005) and we will examine the effectiveness of other non-local information which can be embedded into label information." ></td>
	<td class="line x" title="177:177	As the next stage of our research, we hope to apply our method to shallow parsing, in which segments tend to be long and non-local information is important." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-1089
Guessing Parts-Of-Speech Of Unknown Words Using Global Information
Nakagawa, Tetsuji;Matsumoto, Yuji;"></td>
	<td class="line x" title="1:221	Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 705712, Sydney, July 2006." ></td>
	<td class="line x" title="2:221	c2006 Association for Computational Linguistics Guessing Parts-of-Speech of Unknown Words Using Global Information Tetsuji Nakagawa Corporate R&D Center Oki Electric Industry Co. , Ltd. 257 Honmachi, Chuo-ku Osaka 5410053, Japan nakagawa378@oki.com Yuji Matsumoto Graduate School of Information Science Nara Institute of Science and Technology 89165 Takayama, Ikoma Nara 6300101, Japan matsu@is.naist.jp Abstract In this paper, we present a method for guessing POS tags of unknown words using local and global information." ></td>
	<td class="line x" title="3:221	Although many existing methods use only local information (i.e. limited window size or intra-sentential features), global information (extra-sentential features) provides valuable clues for predicting POS tags of unknown words." ></td>
	<td class="line x" title="4:221	We propose a probabilistic model for POS guessing of unknown words using global information as well as local information, and estimate its parameters using Gibbs sampling." ></td>
	<td class="line x" title="5:221	We also attempt to apply the model to semisupervised learning, and conduct experiments on multiple corpora." ></td>
	<td class="line x" title="6:221	1 Introduction Part-of-speech (POS) tagging is a fundamental language analysis task." ></td>
	<td class="line x" title="7:221	In POS tagging, we frequently encounter words that do not exist in training data." ></td>
	<td class="line x" title="8:221	Such words are called unknown words." ></td>
	<td class="line x" title="9:221	They are usually handled by an exceptional process in POS tagging, because the tagging system does not have information about the words." ></td>
	<td class="line x" title="10:221	Guessing the POS tags of such unknown words is a difficult task." ></td>
	<td class="line x" title="11:221	But it is an important issue both for conducting POS tagging accurately and for creating word dictionaries automatically or semiautomatically." ></td>
	<td class="line x" title="12:221	There have been many studies on POS guessing of unknown words (Mori and Nagao, 1996; Mikheev, 1997; Chen et al. , 1997; Nagata, 1999; Orphanos and Christodoulakis, 1999)." ></td>
	<td class="line x" title="13:221	In most of these previous works, POS tags of unknown words were predicted using only local information, such as lexical forms and POS tags of surrounding words or word-internal features (e.g. suffixes and character types) of the unknown words." ></td>
	<td class="line x" title="14:221	However, this approach has limitations in available information." ></td>
	<td class="line x" title="15:221	For example, common nouns and proper nouns are sometimes difficult to distinguish with only the information of a single occurrence because their syntactic functions are almost identical." ></td>
	<td class="line x" title="16:221	In English, proper nouns are capitalized and there is generally little ambiguity between common nouns and proper nouns." ></td>
	<td class="line x" title="17:221	In Chinese and Japanese, no such convention exists and the problem of the ambiguity is serious." ></td>
	<td class="line x" title="18:221	However, if an unknown word with the same lexical form appears in another part with informative local features (e.g. titles of persons), this will give useful clues for guessing the part-of-speech of the ambiguous one, because unknown words with the same lexical form usually have the same part-of-speech." ></td>
	<td class="line x" title="19:221	For another example, there is a part-of-speech named sahen-noun (verbal noun) in Japanese." ></td>
	<td class="line x" title="20:221	Verbal nouns behave as common nouns, except that they are used as verbs when they are followed by a verb suru; e.g., a verbal noun dokusho means reading and dokusho-suru is a verb meaning to read books." ></td>
	<td class="line x" title="21:221	It is difficult to distinguish a verbal noun from a common noun if it is used as a noun." ></td>
	<td class="line x" title="22:221	However, it will be easy if we know that the word is followed by suru in another part in the document." ></td>
	<td class="line x" title="23:221	This issue was mentioned by Asahara (2003) as a problem of possibility-based POS tags." ></td>
	<td class="line x" title="24:221	A possibility-based POS tag is a POS tag that represents all the possible properties of the word (e.g. , a verbal noun is used as a noun or a verb), rather than a property of each instance of the word." ></td>
	<td class="line x" title="25:221	For example, a sahennoun is actually a noun that can be used as a verb when it is followed by suru." ></td>
	<td class="line x" title="26:221	This property cannot be confirmed without observing real usage of the word appearing with suru." ></td>
	<td class="line x" title="27:221	Such POS tags may not be identified with only local information of one instance, because the property that each instance has is only one among all the possible properties." ></td>
	<td class="line x" title="28:221	To cope with these issues, we propose a method that uses global information as well as local information for guessing the parts-of-speech of unknown words." ></td>
	<td class="line x" title="29:221	With this method, all the occurrences of the unknown words in a document1 are taken into consideration at once, rather than that each occurrence of the words is processed separately." ></td>
	<td class="line x" title="30:221	Thus, the method models the whole document and finds a set of parts-of-speech by maximizing its conditional joint probability given the document, rather than independently maximizing the probability of each part-of-speech given each sentence." ></td>
	<td class="line pc" title="31:221	Global information is known to be useful in other NLP tasks, especially in the named entity recognition task, and several studies successfully used global features (Chieu and Ng, 2002; Finkel et al. , 2005)." ></td>
	<td class="line x" title="32:221	One potential advantage of our method is its 1In this paper, we use the word document to denote the whole data consisting of multiple sentences (training corpus or test corpus)." ></td>
	<td class="line x" title="33:221	705 ability to incorporate unlabeled data." ></td>
	<td class="line x" title="34:221	Global features can be increased by simply adding unlabeled data into the test data." ></td>
	<td class="line x" title="35:221	Models in which the whole document is taken into consideration need a lot of computation compared to models with only local features." ></td>
	<td class="line x" title="36:221	They also cannot process input data one-by-one." ></td>
	<td class="line x" title="37:221	Instead, the entire document has to be read before processing." ></td>
	<td class="line x" title="38:221	We adopt Gibbs sampling in order to compute the models efficiently, and these models are suitable for offline use such as creating dictionaries from raw text where real-time processing is not necessary but high-accuracy is needed to reduce human labor required for revising automatically analyzed data." ></td>
	<td class="line x" title="39:221	The rest of this paper is organized as follows: Section 2 describes a method for POS guessing of unknown words which utilizes global information." ></td>
	<td class="line x" title="40:221	Section 3 shows experimental results on multiple corpora." ></td>
	<td class="line x" title="41:221	Section 4 discusses related work, and Section 5 gives conclusions." ></td>
	<td class="line x" title="42:221	2 POS Guessing of Unknown Words with Global Information We handle POS guessing of unknown words as a sub-task of POS tagging, in this paper." ></td>
	<td class="line x" title="43:221	We assume that POS tags of known words are already determined beforehand, and positions in the document where unknown words appear are also identified." ></td>
	<td class="line x" title="44:221	Thus, we focus only on prediction of the POS tags of unknown words." ></td>
	<td class="line x" title="45:221	In the rest of this section, we first present a model for POS guessing of unknown words with global information." ></td>
	<td class="line x" title="46:221	Next, we show how the test data is analyzed and how the parameters of the model are estimated." ></td>
	<td class="line x" title="47:221	A method for incorporating unlabeled data with the model is also discussed." ></td>
	<td class="line x" title="48:221	2.1 Probabilistic Model Using Global Information We attempt to model the probability distribution of the parts-of-speech of all occurrences of the unknown words in a document which have thesame lexical form." ></td>
	<td class="line x" title="49:221	We suppose that such partsof-speech have correlation, and the part-of-speech of each occurrence is also affected by its local context." ></td>
	<td class="line x" title="50:221	Similar situations to this are handled inphysics." ></td>
	<td class="line x" title="51:221	For example, let us consider a case where a number of electrons with spins exist in a system." ></td>
	<td class="line x" title="52:221	The spins interact with each other, and each spin is also affected by the external magnetic field." ></td>
	<td class="line x" title="53:221	In the physical model, if the state of the system is s and the energy of the system is E(s), the probability distribution of s is known to be represented by the following Boltzmann distribution: P(s)= 1Z expE(s)}, (1) where  is inverse temperature and Z is a normalizing constant defined as follows: Z= summationdisplay s expE(s)}." ></td>
	<td class="line x" title="54:221	(2) Takamura et al.(2005) applied this model to an NLP task, semantic orientation extraction, and we apply it to POS guessing of unknown words here." ></td>
	<td class="line x" title="56:221	Suppose that unknown words with the same lexical form appear K times in a document." ></td>
	<td class="line x" title="57:221	Assume that the number of possible POS tags for unknown words is N, and they are represented by integers from 1 to N. Let tk denote the POS tag of the kth occurrence of the unknown words, let wk denotethe local context (e.g. the lexical forms and the POS tags of the surrounding words) of the kth occurrence of the unknown words, and let w and tdenote the sets of w k and tk respectively: w={w1,,wK}, t={t1,,tK}, tk{1,,N}." ></td>
	<td class="line x" title="58:221	i,j is a weight which denotes strength of the interaction between parts-of-speech i and j, and is symmetric (i,j = j,i)." ></td>
	<td class="line x" title="59:221	We define the energy where POS tags of unknown words given w are t as follows: E(t|w)= braceleftBigg 1 2 Ksummationdisplay k=1 Ksummationdisplay kprime=1 kprimenegationslash=k tk,tkprime + Ksummationdisplay k=1 logp0(tk|wk) bracerightBigg, (3) where p0(t|w) is an initial distribution (local model) of the part-of-speech t which is calculated with only the local context w, using arbitrary statistical models such as maximum entropy models." ></td>
	<td class="line x" title="60:221	The right hand side of the above equation consists of two components; one represents global interactions between each pair of parts-of-speech, and the other represents the effects of local information." ></td>
	<td class="line x" title="61:221	In this study, we fix the inverse temperature  = 1." ></td>
	<td class="line x" title="62:221	The distribution of t is then obtained from Equation (1), (2) and (3) as follows: P(t|w)= 1Z(w)p0(t|w)exp braceleftBigg 1 2 Ksummationdisplay k=1 Ksummationdisplay kprime=1 kprimenegationslash=k tk,tkprime bracerightBigg, (4) Z(w)= summationdisplay tT(w) p0(t|w)exp braceleftBigg 1 2 Ksummationdisplay k=1 Ksummationdisplay kprime=1 kprimenegationslash=k tk,tkprime bracerightBigg, (5) p0(t|w) Kproductdisplay k=1 p0(tk|wk), (6) where T(w) is the set of possible configurations of POS tags given w. The size of T(w) is NK, because there are K occurrences of the unknownwords and each unknown word can have one of N POS tags." ></td>
	<td class="line x" title="63:221	The above equations can be rewritten as follows by defining a function fi,j(t): fi,j(t)12 Ksummationdisplay k=1 Ksummationdisplay kprime=1 kprimenegationslash=k (tk,i)(tkprime,j), (7) P(t|w)= 1Z(w)p0(t|w)exp braceleftBigg Nsummationdisplay i=1 Nsummationdisplay j=1 i,jfi,j(t) bracerightBigg, (8) Z(w)= summationdisplay tT(w) p0(t|w)exp braceleftBigg Nsummationdisplay i=1 Nsummationdisplay j=1 i,jfi,j(t) bracerightBigg, (9) 706 where (i,j) is the Kronecker delta: (i,j)= braceleftBig1 (i = j), 0 (i negationslash= j)." ></td>
	<td class="line x" title="64:221	(10) fi,j(t) represents the number of occurrences of the POS tag pair i and j in the whole document (divided by 2), and the model in Equation (8) is essentially a maximum entropy model with the document level features." ></td>
	<td class="line x" title="65:221	As shown above, we consider the conditional joint probability of all the occurrences of the unknown words with the same lexical form in the document given their local contexts, P(t|w), in contrast to conventional approaches which assume independence of the sentences in the document and use the probabilities of all the words only in a sentence." ></td>
	<td class="line x" title="66:221	Note that we assume independence between the unknown words with different lexical forms, and each set of the unknown words with the same lexical form is processed separately from the sets of other unknown words." ></td>
	<td class="line x" title="67:221	2.2 Decoding Let us consider how to find the optimal POS tags t basing on the model, given K local contexts of the unknown words with the same lexical form (test data) w, an initial distribution p0(t|w) and a set of model parameters  = {1,1,,N,N}." ></td>
	<td class="line x" title="68:221	One way to do this is to find a set of POS tags which maximizes P(t|w) among all possible candidates of t. However, the number of all possible candidates of the POS tags is NK and the calculation is generally intractable." ></td>
	<td class="line x" title="69:221	Although HMMs, MEMMs, and CRFs use dynamic programming and some studies with probabilistic models which have specific structures use efficient algorithms (Wang et al. , 2005), such methods cannot be applied here because we are considering interactions (dependencies) between all POS tags, and their joint distribution cannot be decomposed." ></td>
	<td class="line x" title="70:221	Therefore, we use a sampling technique and approximate the solution using samples obtained from the probability distribution." ></td>
	<td class="line x" title="71:221	We can obtain a solution t = {t1,,tK} as follows: tk=argmax t Pk(t|w), (11) where Pk(t|w) is the marginal distribution of the part-of-speech of the kth occurrence of the unknown words given a set of local contexts w, and is calculated as an expected value over the distribution of the unknown words as follows: Pk(t|w)= summationdisplay t1,,tk1,tk+1,,tK tk=t P(t|w), = summationdisplay tT(w) (tk,t)P(t|w)." ></td>
	<td class="line x" title="72:221	(12) Expected values can be approximately calculated using enough number of samples generated from the distribution (MacKay, 2003)." ></td>
	<td class="line x" title="73:221	Suppose that A(x) is a function of a random variable x, P(x) initialize t(1) for m := 2 to M for k := 1 to K t(m)k  P(tk|w,t(m)1,,t(m)k1,t(m1)k+1,,t(m1)K ) Figure 1: Gibbs Sampling is a distribution of x, and {x(1),,x(M)} are M samples generated from P(x)." ></td>
	<td class="line x" title="74:221	Then, the expectation of A(x) over P(x) is approximated by the samples: summationdisplay x A(x)P(x)similarequal 1M Msummationdisplay m=1 A(x(m))." ></td>
	<td class="line x" title="75:221	(13) Thus, if we have M samples {t(1),,t(M)} generated from the conditional joint distribution P(t|w), the marginal distribution of each POS tag is approximated as follows: Pk(t|w)similarequal 1M Msummationdisplay m=1 (t(m)k,t)." ></td>
	<td class="line x" title="76:221	(14) Next, we describe how to generate samples from the distribution." ></td>
	<td class="line x" title="77:221	We use Gibbs sampling for this purpose." ></td>
	<td class="line x" title="78:221	Gibbs sampling is one of the Markov chain Monte Carlo (MCMC) methods, which can generate samples efficiently from highdimensional probability distributions (Andrieu et al. , 2003)." ></td>
	<td class="line x" title="79:221	The algorithm is shown in Figure 1." ></td>
	<td class="line x" title="80:221	The algorithm firstly set the initial state t(1), then one new random variable is sampled at a time from the conditional distribution in which all othervariables are fixed, and new samples are created by repeating the process." ></td>
	<td class="line x" title="81:221	Gibbs sampling is easy to implement and is guaranteed to converge to the true distribution." ></td>
	<td class="line x" title="82:221	The conditional distri-bution P(t k|w,t1,,tk1,tk+1,,tK) in Fig-ure 1 can be calculated simply as follows: P(tk|w,t1,,tk1,tk+1,,tK) = P(t|w)P(t 1,,tk1,tk+1,,tK|w), = 1 Z(w)p0(t|w)exp 1 2 summationtextK kprime=1 summationtextK kprimeprime=1 kprimeprimenegationslash=kprime tkprime,tkprimeprime} summationtextN tk=1 P(t1,,tk1,t  k,tk+1,,tK|w), = p0(tk|wk)expsummationtextKkprime=1 kprimenegationslash=k tkprime,tk} summationtextN tk=1 p0(t  k|wk)exp summationtextK kprime=1 kprimenegationslash=k tkprime,tk}, (15) where the last equation is obtained using the following relation: 1 2 Ksummationdisplay kprime=1 Ksummationdisplay kprimeprime=1 kprimeprimenegationslash=kprime tkprime,tkprimeprime=12 Ksummationdisplay kprime=1 kprimenegationslash=k Ksummationdisplay kprimeprime=1 kprimeprimenegationslash=k,kprimeprimenegationslash=kprime tkprime,tkprimeprime + Ksummationdisplay kprime=1 kprimenegationslash=k tkprime,tk." ></td>
	<td class="line x" title="83:221	In later experiments, the number of samples M is set to 100, and the initial state t(1) is set to the POS tags which maximize p0(t|w)." ></td>
	<td class="line x" title="84:221	The optimal solution obtained by Equation (11) maximizes the probability of each POS tag given w, and this kind of approach is known as the maximum posterior marginal (MPM) estimate (Marroquin, 1985)." ></td>
	<td class="line oc" title="85:221	Finkel et al.(2005) used simulated annealing with Gibbs sampling to find a solution in a similar situation." ></td>
	<td class="line n" title="87:221	Unlike simulated annealing, this approach does not need to define a cooling 707 schedule." ></td>
	<td class="line x" title="88:221	Furthermore, this approach can obtain not only the best solution but also the second best or the other solutions according to Pk(t|w), which are useful when this method is applied to semiautomatic construction of dictionaries because human annotators can check the ranked lists of candidates." ></td>
	<td class="line x" title="89:221	2.3 Parameter Estimation Let us consider how to estimate the parameter  = {1,1,,N,N} in Equation (8) from training data consisting of L examples; {w1,t1,,wL,tL} (i.e. , the training data contains L different lexical forms of unknownwords)." ></td>
	<td class="line x" title="90:221	We define the following objective function L, and find  which maximizes L (the subscript  denotes being parameterized by ): L = log Lproductdisplay l=1 P(tl|wl)+logP(), = log Lproductdisplay l=1 1 Z(wl)p0(t l|wl)exp braceleftBigg Nsummationdisplay i=1 Nsummationdisplay j=1 i,jfi,j(tl) bracerightBigg +logP(), = Lsummationdisplay l=1 bracketleftBigg logZ(wl)+logp0(tl|wl)+ Nsummationdisplay i=1 Nsummationdisplay j=1 i,jfi,j(tl) bracketrightBigg +logP()." ></td>
	<td class="line x" title="91:221	(16) The partial derivatives of the objective functionare: L i,j = Lsummationdisplay l=1 bracketleftBigg fi,j(tl)  i,j logZ(wl) bracketrightBigg +  i,j logP(), = Lsummationdisplay l=1 bracketleftBigg fi,j(tl) summationdisplay tT(wl) fi,j(t)P(t|wl) bracketrightBigg +  i,j logP()." ></td>
	<td class="line x" title="92:221	(17) We use Gaussian priors (Chen and Rosenfeld, 1999) for P(): logP()= Nsummationdisplay i=1 Nsummationdisplay j=1 2i,j 22 +C,  i,j logP() =  i,j 2." ></td>
	<td class="line x" title="93:221	where C is a constant and  is set to 1 in laterexperiments." ></td>
	<td class="line x" title="94:221	The optimal  can be obtained by quasi-Newton methods using the above L and L i,j, and we use L-BFGS (Liu and Nocedal, 1989) for this purpose2." ></td>
	<td class="line x" title="95:221	However, the calculation is intractable because Z(wl) (see Equation (9)) in Equation (16) and a term in Equation (17) contain summations over all the possible POS tags." ></td>
	<td class="line x" title="96:221	To cope with the problem, we use the sampling technique again for the calculation, as suggested by Rosenfeld et al.(2001)." ></td>
	<td class="line x" title="98:221	Z(wl) can be approximated using M samples {t(1),,t(M)} generated from p0(t|wl): Z(wl)= summationdisplay tT(wl) p0(t|wl)exp braceleftBigg Nsummationdisplay i=1 Nsummationdisplay j=1 i,jfi,j(t) bracerightBigg, 2In later experiments, L-BFGS often did not converge completely because we used approximation with Gibbs sampling, and we stopped iteration of L-BFGS in such cases." ></td>
	<td class="line x" title="99:221	similarequal 1M Msummationdisplay m=1 exp braceleftBigg Nsummationdisplay i=1 Nsummationdisplay j=1 i,jfi,j(t(m)) bracerightBigg ." ></td>
	<td class="line x" title="100:221	(18) The term in Equation (17) can also be approximated using M samples {t(1),,t(M)} generated from P(t|wl) with Gibbs sampling: summationdisplay tT(wl) fi,j(t)P(t|wl)similarequal 1M Msummationdisplay m=1 fi,j(t(m))." ></td>
	<td class="line x" title="101:221	(19) In later experiments, the initial state t(1) in Gibbs sampling is set to the gold standard tags in the training data." ></td>
	<td class="line x" title="102:221	2.4 Use of Unlabeled Data In our model, unlabeled data can be easily used by simply concatenating the test data and the unlabeled data, and decoding them in the testing phase." ></td>
	<td class="line x" title="103:221	Intuitively, if we increase the amount of the test data, test examples with informative local features may increase." ></td>
	<td class="line x" title="104:221	The POS tags of such examples can be easily predicted, and they are used as global features in prediction of other examples." ></td>
	<td class="line x" title="105:221	Thus, this method uses unlabeled data in only the testing phase, and the training phase is the same as the case with no unlabeled data." ></td>
	<td class="line x" title="106:221	3 Experiments 3.1 Data and Procedure We use eight corpora for our experiments; the Penn Chinese Treebank corpus 2.0 (CTB), a part of the PFR corpus (PFR), the EDR corpus (EDR), the Kyoto University corpus version 2 (KUC), the RWCP corpus (RWC), the GENIA corpus 3.02p (GEN), the SUSANNE corpus (SUS) and the Penn Treebank WSJ corpus (WSJ), (cf.Table 1)." ></td>
	<td class="line x" title="108:221	All the corpora are POS tagged corpora in Chinese(C), English(E) or Japanese(J), and they are split into three portions; training data, test data and unlabeled data." ></td>
	<td class="line x" title="109:221	The unlabeled data is used in experiments of semi-supervised learning, and POS tags of unknown words in the unlabeled data are eliminated." ></td>
	<td class="line x" title="110:221	Table 1 summarizes detailed information about the corpora we used: the language, the number of POS tags, the number of open class tags (POS tags that unknown words can have, described later), the sizes of training, test and unlabeled data, and the splitting method of them." ></td>
	<td class="line x" title="111:221	For the test data and the unlabeled data, unknown words are defined as words that do not appear in the training data." ></td>
	<td class="line x" title="112:221	The number of unknown words in the test data of each corpus is shown in Table 1, parentheses." ></td>
	<td class="line x" title="113:221	Accuracy of POS guessing of unknown words is calculated based on how many words among them are correctly POS-guessed." ></td>
	<td class="line x" title="114:221	Figure 2 shows the procedure of the experiments." ></td>
	<td class="line x" title="115:221	We split the training data into two parts; the first half as sub-training data 1 and the latter half as sub-training data 2 (Figure 2, *1)." ></td>
	<td class="line x" title="116:221	Then, we check the words that appear in the sub-training 708 Corpus # of POS # of Tokens (# of Unknown Words) [partition in the corpus] (Lang)." ></td>
	<td class="line x" title="117:221	(Open Class) Training Test Unlabeled CTB 34 84,937 7,980 (749) 6,801 (C) (28) [sec." ></td>
	<td class="line x" title="118:221	1270] [sec." ></td>
	<td class="line x" title="119:221	271300] [sec." ></td>
	<td class="line x" title="120:221	301325] PFR 42 304,125 370,627 (27,774) 445,969 (C) (39) [Jan. 1Jan. 9] [Jan. 10Jan. 19] [Jan. 20Jan. 31] EDR 15 2,550,532 1,280,057 (24,178) 1,274,458 (J) (15) [id = 4n+0,id = 4n+1] [id = 4n+2] [id = 4n+3] KUC 40 198,514 31,302 (2,477) 41,227 (J) (36) [Jan. 1Jan. 8] [Jan. 9] [Jan. 10] RWC 66 487,333 190,571 (11,177) 210,096 (J) (55) [110,000th sentences] [10,00114,000th sentences] [14,00118,672th sentences] GEN 47 243,180 123,386 (7,775) 134,380 (E) (36) [110,000th sentences] [10,00115,000th sentences] [15,00120,546th sentences] SUS 125 74,902 37,931 (5,760) 37,593 (E) (90) [sec." ></td>
	<td class="line x" title="121:221	A0108, G0108, [sec." ></td>
	<td class="line x" title="122:221	A0912, G0912, [sec." ></td>
	<td class="line x" title="123:221	A1320, G1322,J0108, N0108] J0917, N0912] J2124, N1318] WSJ 45 912,344 129,654 (4,253) 131,768 (E) (33) [sec." ></td>
	<td class="line x" title="124:221	018] [sec." ></td>
	<td class="line x" title="125:221	2224] [sec." ></td>
	<td class="line x" title="126:221	1921] Table 1: Statistical Information of Corpora Corpus TrainingData TestData UnlabeledData Sub-Training data 1(*1) Sub-Training data 2(*1) Sub-Local Model 1(*3) Sub-Local Model 2(*3) Global Model Local Model(*2) (optional) TestResult Data flow for training Data flow for testing Figure 2: Experimental Procedure data 1 but not in the sub-training data 2, or vice versa." ></td>
	<td class="line x" title="127:221	We handle these words as (pseudo) unknown words in the training data." ></td>
	<td class="line x" title="128:221	Such (two-fold) cross-validation is necessary to make training examples that contain unknown words3." ></td>
	<td class="line x" title="129:221	POS tags that these pseudo unknown words have are defined as open class tags, and only the open class tags are considered as candidate POS tags for unknown words in the test data (i.e. , N is equal to the number of the open class tags)." ></td>
	<td class="line x" title="130:221	In the training phase, we need to estimate two types of parameters; local model (parameters), which is necessary to calculate p0(t|w), and global model (parameters), i.e., i,j. The local model parameters are estimated using all the training data (Figure 2, *2)." ></td>
	<td class="line x" title="131:221	Local 3A major method for generating such pseudo unknown words is to collect the words that appear only once in a corpus (Nagata, 1999)." ></td>
	<td class="line x" title="132:221	These words are called hapax legomena and known to have similar characteristics to real unknown words (Baayen and Sproat, 1996)." ></td>
	<td class="line x" title="133:221	These words are interpreted as being collected by the leave-one-out technique (which is a special case of cross-validation) as follows: One word is picked from the corpus and the rest of the corpus is considered as training data." ></td>
	<td class="line x" title="134:221	The picked word is regarded as an unknown word if it does not exist in the training data." ></td>
	<td class="line x" title="135:221	This procedure is iterated for all the words in the corpus." ></td>
	<td class="line x" title="136:221	However, this approach is not applicable to our experiments because those words that appear only once in the corpus do not have global information and are useless for learning the global model, so we use the two-fold cross validation method." ></td>
	<td class="line x" title="137:221	model parameters and training data are necessary to estimate the global model parameters, but the global model parameters cannot be estimated from the same training data from which the local model parameters are estimated." ></td>
	<td class="line x" title="138:221	In order to estimate the global model parameters, we firstly train sub-local models 1 and 2 from the sub-training data 1 and 2 respectively (Figure 2, *3)." ></td>
	<td class="line x" title="139:221	The sub-local models 1 and 2 are used for calculating p0(t|w) of unknown words in the sub-training data 2 and 1 respectively, when the global model parameters are estimated from the entire training data." ></td>
	<td class="line x" title="140:221	In the testing phase, p0(t|w) of unknown words in the test data are calculated using the local model parameters which are estimated from the entire training data, and test results are obtained using the global model with the local model." ></td>
	<td class="line x" title="141:221	Global information cannot be used for unknown words whose lexical forms appear only once in the training or test data, so we process only nonunique unknown words (unknown words whose lexical forms appear more than once) using the proposed model." ></td>
	<td class="line x" title="142:221	In the testing phase, POS tags of unique unknown words are determined using only the local information, by choosing POS tags which maximize p0(t|w)." ></td>
	<td class="line x" title="143:221	Unlabeled data can be optionally used for semisupervised learning." ></td>
	<td class="line x" title="144:221	In that case, the test data and the unlabeled data are concatenated, and the best POS tags which maximize the probability of the mixed data are searched." ></td>
	<td class="line x" title="145:221	3.2 Initial Distribution In our method, the initial distribution p0(t|w) is used for calculating the probability of t given local context w (Equation (8))." ></td>
	<td class="line x" title="146:221	We use maximum entropy (ME) models for the initial distribution." ></td>
	<td class="line x" title="147:221	p0(t|w) is calculated by ME models as follows (Berger et al. , 1996): p0(t|w)= 1Y(w) exp braceleftBigg Hsummationdisplay h=1 hgh(w,t) bracerightBigg, (20) 709 Language Features English Prefixes of 0 up to four characters, suffixes of 0 up to four characters, 0 contains Arabic numerals, 0 contains uppercase characters, 0 contains hyphens." ></td>
	<td class="line x" title="148:221	Chinese Prefixes of 0 up to two characters, Japanese suffixes of 0 up to two characters, 1, |0|, 1 & |0|,uniontext |0| i=1i} (set of character types)." ></td>
	<td class="line x" title="149:221	(common) |0| (length of 0), 1, +1, 2 & 1, +1 & +2, 1 & +1, 1 & 1, +1 & +1, 2 & 2 & 1 & 1, +1 & +1 & +2 & +2, 1 & 1 & +1 & +1." ></td>
	<td class="line x" title="150:221	Table 2: Features Used for Initial Distribution Y(w)= Nsummationdisplay t=1 exp braceleftBigg Hsummationdisplay h=1 hgh(w,t) bracerightBigg, (21) where gh(w,t) is a binary feature function." ></td>
	<td class="line x" title="151:221	We assume that each local context w contains the following information about the unknown word:  The POS tags of the two words on each side of the unknown word: 2,1,+1,+2.4  The lexical forms of the unknown word itself and the two words on each side of the unknown word: 2,1,0,+1,+2." ></td>
	<td class="line x" title="152:221	 The character types of all the characters composing the unknown word: 1,,|0|." ></td>
	<td class="line x" title="153:221	We use six character types: alphabet, numeral (Arabic and Chinese numerals), symbol, Kanji (Chinese character), Hiragana (Japanese script) and Katakana (Japanese script)." ></td>
	<td class="line x" title="154:221	A feature function gh(w,t) returns 1 if w and t satisfy certain conditions, and otherwise 0; for example: g123(w,t)= braceleftBig1 ( 1 =President and 1 =NNP and t = 5), 0 (otherwise)." ></td>
	<td class="line x" title="155:221	The features we use are shown in Table 2, which are based on the features used by Ratnaparkhi (1996) and Uchimoto et al.(2001)." ></td>
	<td class="line x" title="157:221	The parameters h in Equation (20) are estimated using all the words in the training data whose POS tags are the open class tags." ></td>
	<td class="line x" title="158:221	3.3 Experimental Results The results are shown in Table 3." ></td>
	<td class="line x" title="159:221	In the table, lo-cal, local+global and local+global w/ unlabeled indicate that the results were obtained using only local information, local and global information, and local and global information with the extra unlabeled data, respectively." ></td>
	<td class="line x" title="160:221	The results using only local information were obtained by choosing POS 4In both the training and the testing phases, POS tags of known words are given from the corpora." ></td>
	<td class="line x" title="161:221	When these surrounding words contain unknown words, their POS tags are represented by a special tag Unk." ></td>
	<td class="line x" title="162:221	PFR (Chinese) +162 vn (verbal noun) +150 ns (place name) +86 nz (other proper noun) +85 j (abbreviation) +61 nr (personal name)  26 m (numeral) 100 v (verb) RWC (Japanese) +33 noun-proper noun-person name-family name +32 noun-proper noun-place name +28 noun-proper noun-organization name +17 noun-proper noun-person name-first name +6 noun-proper noun +4 noun-sahen noun  2 noun-proper noun-place name-country name 29 noun SUS (English) +13 NP (proper noun) +6 JJ (adjective) +2 VVD (past tense form of lexical verb) +2 NNL (locative noun) +2 NNJ (organization noun)  3 NN (common noun) 6 NNU (unit-of-measurement noun) Table 4: Ordered List of Increased/Decreased Number of Correctly Tagged Words tags t = {t1,,tK} which maximize the probabilities of the local model: tk=argmax t p0(t|wk)." ></td>
	<td class="line x" title="163:221	(22) The table shows the accuracies, the numbers of errors, the p-values of McNemars test against the results using only local information, and the numbers of non-unique unknown words in the test data." ></td>
	<td class="line x" title="164:221	On an Opteron 250 processor with 8GB of RAM, model parameter estimation and decoding without unlabeled data for the eight corpora took 117 minutes and 39 seconds in total, respectively." ></td>
	<td class="line x" title="165:221	In the CTB, PFR, KUC, RWC and WSJ corpora, the accuracies were improved using global information (statistically significant at p < 0.05), compared to the accuracies obtained using only local information." ></td>
	<td class="line x" title="166:221	The increases of the accuracies on the English corpora (the GEN and SUS corpora) were small." ></td>
	<td class="line x" title="167:221	Table 4 shows the increased/decreased number of correctly tagged words using global information in the PFR, RWC and SUS corpora." ></td>
	<td class="line x" title="168:221	In the PFR (Chinese) and RWC (Japanese) corpora, many proper nouns were correctly tagged using global information." ></td>
	<td class="line x" title="169:221	In Chinese and Japanese, proper nouns are not capitalized, therefore proper nouns are difficult to distinguish from common nouns with only local information." ></td>
	<td class="line x" title="170:221	One reason that only the small increases were obtained with global information in the English corpora seems to be the low ambiguities of proper nouns." ></td>
	<td class="line x" title="171:221	Many verbal nouns in PFR and a few sahen-nouns (Japanese verbal nouns) in RWC, which suffer from the problem of possibility-based POS tags, were also correctly tagged using global information." ></td>
	<td class="line x" title="172:221	When the unlabeled data was used, the number of nonunique words in the test data increased." ></td>
	<td class="line x" title="173:221	Compared with the case without the unlabeled data, the accu710 Corpus Accuracy for Unknown Words (# of Errors) (Lang)." ></td>
	<td class="line x" title="174:221	[p-value] # of Non-unique Unknown Words local local+global local+global w/ unlabeled CTB 0.7423 (193) 0.7717 (171) 0.7704 (172) (C) [0.0000] 344 [0.0001] 361 PFR 0.6499 (9723) 0.6690 (9193) 0.6785 (8930) (C) [0.0000] 16019 [0.0000] 18861 EDR 0.9639 (874) 0.9643 (863) 0.9651 (844) (J) [0.1775] 4903 [0.0034] 7770 KUC 0.7501 (619) 0.7634 (586) 0.7562 (604) (J) [0.0000] 788 [0.0872] 936 RWC 0.7699 (2572) 0.7785 (2476) 0.7787 (2474) (J) [0.0000] 5044 [0.0000] 5878 GEN 0.8836 (905) 0.8837 (904) 0.8863 (884) (E) [1.0000] 4094 [0.0244] 4515 SUS 0.7934 (1190) 0.7957 (1177) 0.7979 (1164) (E) [0.1878] 3210 [0.0116] 3583 WSJ 0.8345 (704) 0.8368 (694) 0.8352 (701) (E) [0.0162] 1412 [0.7103] 1627 Table 3: Results of POS Guessing of Unknown Words Corpus MeanStandard Deviation (Lang)." ></td>
	<td class="line x" title="175:221	Marginal S.A. CTB (C) 0.76960.0021 0.76820.0028 PFR (C) 0.67070.0010 0.67120.0014 EDR (J) 0.96440.0001 0.96450.0001 KUC (J) 0.75950.0031 0.76120.0018 RWC (J) 0.77770.0017 0.77720.0020 GEN (E) 0.88410.0009 0.88400.0007 SUS (E) 0.79970.0038 0.79950.0034 WSJ (E) 0.83660.0013 0.83600.0021 Table 5: Results of Multiple Trials and Comparison to Simulated Annealing racies increased in several corpora but decreased in the CTB, KUC and WSJ corpora." ></td>
	<td class="line x" title="176:221	Since our method uses Gibbs sampling in the training and the testing phases, the results are affected by the sequences of random numbers used in the sampling." ></td>
	<td class="line x" title="177:221	In order to investigate the influence, we conduct 10 trials with different sequences of pseudo random numbers." ></td>
	<td class="line oc" title="178:221	We also conduct experiments using simulated annealing in decoding, as conducted by Finkel et al.(2005) for information extraction." ></td>
	<td class="line x" title="180:221	We increase inverse temperature  in Equation (1) from  = 1 to    with the linear cooling schedule." ></td>
	<td class="line x" title="181:221	The results are shown in Table 5." ></td>
	<td class="line x" title="182:221	The table shows the mean values and the standard deviations of the accuracies for the 10 trials, and Marginal and S.A. mean that decoding is conducted using Equation (11) and simulated annealing respectively." ></td>
	<td class="line x" title="183:221	The variances caused by random numbers and the differences of the accuracies between Marginal and S.A. are relatively small." ></td>
	<td class="line x" title="184:221	4 Related Work Several studies concerning the use of global information have been conducted, especially in named entity recognition, which is a similar task to POS guessing of unknown words." ></td>
	<td class="line x" title="185:221	Chieu and Ng (2002) conducted named entity recognition using global features as well as local features." ></td>
	<td class="line x" title="186:221	In their ME model-based method, some global features were used such as when the word appeared first in a position other than the beginning of sentences, the word was capitalized or not." ></td>
	<td class="line x" title="187:221	These global features are static and can be handled in the same manner as local features, therefore Viterbi decoding was used." ></td>
	<td class="line x" title="188:221	The method is efficient but does not handle interactions between labels." ></td>
	<td class="line oc" title="189:221	Finkel et al.(2005) proposed a method incorporating non-local structure for information extraction." ></td>
	<td class="line o" title="191:221	They attempted to use label consistency of named entities, which is the property that named entities with the same lexical form tend to have the same label." ></td>
	<td class="line o" title="192:221	They defined two probabilistic models; a local model based on conditional random fields and a global model based on loglinear models." ></td>
	<td class="line x" title="193:221	Then the final model was constructed by multiplying these two models, which can be seen as unnormalized log-linear interpolation (Klakow, 1998) of the two models which are weighted equally." ></td>
	<td class="line o" title="194:221	In their method, interactions between labels in the whole document were considered, and they used Gibbs sampling and simulated annealing for decoding." ></td>
	<td class="line o" title="195:221	Our model is largely similar to their model." ></td>
	<td class="line o" title="196:221	However, in their method, parameters of the global model were estimated using relative frequencies of labels or were selected by hand, while in our method, global model parameters are estimated from training data so as to fit to the data according to the objective function." ></td>
	<td class="line x" title="197:221	One approach for incorporating global information in natural language processing is to utilize consistency of labels, and such an approach have been used in other tasks." ></td>
	<td class="line x" title="198:221	Takamura et al.(2005) proposed a method based on the spin models in physics for extracting semantic orientations of words." ></td>
	<td class="line x" title="200:221	In the spin models, each electron has one of two states, up or down, and the models give probability distribution of the states." ></td>
	<td class="line x" title="201:221	The states of electrons interact with each other and neighboring electrons tend to have the same spin." ></td>
	<td class="line x" title="202:221	In their 711 method, semantic orientations (positive or negative) of words are regarded as states of spins, in order to model the property that the semantic orientation of a word tends to have the same orientation as words in its gloss." ></td>
	<td class="line x" title="203:221	The mean field approximation was used for inference in their method." ></td>
	<td class="line x" title="204:221	Yarowsky (1995) studied a method for word sense disambiguation using unlabeled data." ></td>
	<td class="line x" title="205:221	Although no probabilistic models were considered explicitly in the method, they used the property of label consistency named one sense per discourse for unsupervised learning together with local information named one sense per collocation." ></td>
	<td class="line x" title="206:221	There exist other approaches using global information which do not necessarily aim to use label consistency." ></td>
	<td class="line x" title="207:221	Rosenfeld et al.(2001) proposed whole-sentence exponential language models." ></td>
	<td class="line x" title="209:221	The method calculates the probability of a sentence s as follows: P(s)= 1Zp0(s)exp braceleftBiggsummationdisplay i ifi(s) bracerightBigg, where p0(s) is an initial distribution of s and any language models such as trigram models can be used for this." ></td>
	<td class="line x" title="210:221	fi(s) is a feature function and can handle sentence-wide features." ></td>
	<td class="line x" title="211:221	Note that if we regard fi,j(t) in our model (Equation (7)) as a feature function, Equation (8) is essentially the same form as the above model." ></td>
	<td class="line x" title="212:221	Their models can incorporate any sentence-wide features including syntactic features obtained by shallow parsers." ></td>
	<td class="line x" title="213:221	They attempted to use Gibbs sampling and other sampling methods for inference, and model parameters were estimated from training data using the generalized iterative scaling algorithm with the sampling methods." ></td>
	<td class="line x" title="214:221	Although they addressed modeling of whole sentences, the method can be directly applied to modeling of whole documents which allows us to incorporate unlabeled data easily as we have discussed." ></td>
	<td class="line x" title="215:221	This approach, modeling whole wide-scope contexts with log-linear models and using sampling methods for inference, gives us an expressive framework and will be applied to other tasks." ></td>
	<td class="line x" title="216:221	5 Conclusion In this paper, we presented a method for guessing parts-of-speech of unknown words using global information as well as local information." ></td>
	<td class="line x" title="217:221	The method models a whole document by considering interactions between POS tags of unknown words with the same lexical form." ></td>
	<td class="line x" title="218:221	Parameters of the model are estimated from training data using Gibbs sampling." ></td>
	<td class="line x" title="219:221	Experimental results showed that the method improves accuracies of POS guessing of unknown words especially for Chinese and Japanese." ></td>
	<td class="line x" title="220:221	We also applied the method to semisupervised learning, but the results were not consistent and there is some room for improvement." ></td>
	<td class="line x" title="221:221	Acknowledgements This work was supported by a grant from the National Institute of Information and Communications Technology of Japan." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-1141
An Effective Two-Stage Model For Exploiting Non-Local Dependencies In Named Entity Recognition
Krishnan, Vijay;Manning, Christopher D.;"></td>
	<td class="line x" title="1:181	Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 11211128, Sydney, July 2006." ></td>
	<td class="line x" title="2:181	c2006 Association for Computational Linguistics An Effective Two-Stage Model for Exploiting Non-Local Dependencies in Named Entity Recognition Vijay Krishnan Computer Science Department Stanford University Stanford, CA 94305 vijayk@cs.stanford.edu Christopher D. Manning Computer Science Department Stanford University Stanford, CA 94305 manning@cs.stanford.edu Abstract This paper shows that a simple two-stage approach to handle non-local dependencies in Named Entity Recognition (NER) can outperform existing approaches that handle non-local dependencies, while being much more computationally efficient." ></td>
	<td class="line x" title="3:181	NER systems typically use sequence models for tractable inference, but this makes them unable to capture the long distance structure present in text." ></td>
	<td class="line x" title="4:181	We use a Conditional Random Field (CRF) based NER system using local features to make predictions and then train another CRF which uses both local information and features extracted from the output of the first CRF." ></td>
	<td class="line x" title="5:181	Using features capturing non-local dependencies from the same document, our approach yields a 12.6% relative error reduction on the F1 score, over state-of-theart NER systems using local-information alone, when compared to the 9.3% relative error reduction offered by the best systems that exploit non-local information." ></td>
	<td class="line x" title="6:181	Our approach also makes it easy to incorporate non-local information from other documents in the test corpus, and this gives us a 13.3% error reduction over NER systems using local-information alone." ></td>
	<td class="line x" title="7:181	Additionally, our running time for inference is just the inference time of two sequential CRFs, which is much less than that of other more complicated approaches that directly model the dependencies and do approximate inference." ></td>
	<td class="line x" title="8:181	1 Introduction Named entity recognition (NER) seeks to locate and classify atomic elements in unstructured text into predefined entities such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. A particular problem for Named Entity Recognition(NER) systems is to exploit the presence of useful information regarding labels assigned at a long distance from a given entity." ></td>
	<td class="line x" title="9:181	An example is the label-consistency constraint that if our text has two occurrences of New York separated by other tokens, we would want our learner to encourage both these entities to get the same label." ></td>
	<td class="line x" title="10:181	Most statistical models currently used for Named Entity Recognition, use sequence models and thereby capture local structure." ></td>
	<td class="line x" title="11:181	Hidden Markov Models (HMMs) (Leek, 1997; Freitag and McCallum, 1999), Conditional Markov Models (CMMs) (Borthwick, 1999; McCallum et al. , 2000), and Conditional Random Fields (CRFs) (Lafferty et al. , 2001) have been successfully employed in NER and other information extraction tasks." ></td>
	<td class="line x" title="12:181	All these models encode the Markov property i.e. labels directly depend only on the labels assigned to a small window around them." ></td>
	<td class="line x" title="13:181	These models exploit this property for tractable computation as this allows the Forward-Backward, Viterbi and Clique Calibration algorithms to become tractable." ></td>
	<td class="line x" title="14:181	Although this constraint is essential to make exact inference tractable, it makes us unable to exploit the non-local structure present in natural language." ></td>
	<td class="line x" title="15:181	Label consistency is an example of a non-local dependency important in NER." ></td>
	<td class="line x" title="16:181	Apart from label consistency between the same token sequences, we would also like to exploit richer sources of dependencies between similar token sequences." ></td>
	<td class="line x" title="17:181	For example, as shown in Figure 1, we would want it to encourage Einstein to be labeled Person if there is strong evidence that Albert Einstein should be labeled Person." ></td>
	<td class="line x" title="18:181	Sequence models unfortu1121 told that Albert Einstein proved." ></td>
	<td class="line x" title="19:181	on seeing Einstein at the Figure 1: An example of the label consistency problem." ></td>
	<td class="line x" title="20:181	Here we would like our model to encourage entities Albert Einstein and Einstein to get the same label, so as to improve the chance that both are labeled PERSON." ></td>
	<td class="line x" title="21:181	nately cannot model this due to their Markovian assumption." ></td>
	<td class="line x" title="22:181	Recent approaches attempting to capture nonlocal dependencies model the non-local dependencies directly, and use approximate inference algorithms, since exact inference is in general, not tractable for graphs with non-local structure." ></td>
	<td class="line x" title="23:181	Bunescu and Mooney (2004) define a Relational Markov Network (RMN) which explicitly models long-distance dependencies, and use it to represent relations between entities." ></td>
	<td class="line x" title="24:181	Sutton and McCallum (2004) augment a sequential CRF with skip-edges i.e. edges between different occurrences of a token, in a document." ></td>
	<td class="line x" title="25:181	Both these approaches use loopy belief propagation (Pearl, 1988; Yedidia et al. , 2000) for approximate inference." ></td>
	<td class="line oc" title="26:181	Finkel et al.(2005) hand-set penalties for inconsistency in entity labeling at different occurrences in the text, based on some statistics from training data." ></td>
	<td class="line o" title="28:181	They then employ Gibbs sampling (Geman and Geman, 1984) for dealing with their local feature weights and their non-local penalties to do approximate inference." ></td>
	<td class="line x" title="29:181	We present a simple two-stage approach where our second CRF uses features derived from the output of the first CRF." ></td>
	<td class="line x" title="30:181	This gives us the advantage of defining a rich set of features to model non-local dependencies, and also eliminates the need to do approximate inference, since we do not explicitly capture the non-local dependencies in a single model, like the more complex existing approaches." ></td>
	<td class="line nc" title="31:181	This also enables us to do inference efficiently since our inference time is merely the inference time of two sequential CRFs; in contrast Finkel et al.(2005) reported an increase in running time by a factor of 30 over the sequential CRF, with their Gibbs sampling approximate inference." ></td>
	<td class="line x" title="33:181	In all, our approach is simpler, yields higher F1 scores, and is also much more computationally efficient than existing approaches modeling nonlocal dependencies." ></td>
	<td class="line x" title="34:181	2 Conditional Random Fields We use a Conditional Random Field (Lafferty et al. , 2001; Sha and Pereira, 2003) since it represents the state of the art in sequence modeling and has also been very effective at Named Entity Recognition." ></td>
	<td class="line x" title="35:181	It allows us both discriminative training that CMMs offer as well and the bi-directional flow of probabilistic information across the sequence that HMMs allow, thereby giving us the best of both worlds." ></td>
	<td class="line x" title="36:181	Due to the bi-directional flow of information, CRFs guard against the myopic locally attractive decisions that CMMs make." ></td>
	<td class="line x" title="37:181	It is customary to use the Viterbi algorithm, to find the most probably state sequence during inference." ></td>
	<td class="line x" title="38:181	A large number of possibly redundant and correlated features can be supplied without fear of further reducing the accuracy of a high-dimensional distribution." ></td>
	<td class="line x" title="39:181	These are welldocumented benefits (Lafferty et al. , 2001)." ></td>
	<td class="line x" title="40:181	2.1 Our Baseline CRF for Named Entity Recognition Our baseline CRF is a sequence model in which labels for tokens directly depend only on the labels corresponding to the previous and next tokens." ></td>
	<td class="line x" title="41:181	We use features that have been shown to be effective in NER, namely the current, previous and next words, character n-grams of the current word, Part of Speech tag of the current word and surrounding words, the shallow parse chunk of the current word, shape of the current word, the surrounding word shape sequence, the presence of a word in a left window of size 5 around the current word and the presence of a word in a left window of size 5 around the current word." ></td>
	<td class="line x" title="42:181	This gives us a competitive baseline CRF using local information alone, whose performance is close to the best published local CRF models, for Named Entity Recognition 3 Label Consistency The intuition for modeling label consistency is that within a particular document, different occur1122 Document Level Statistics Corpus Level Statistics PER LOC ORG MISC PER LOC ORG MISC PER 3141 4 5 0 33830 113 153 0 LOC 6436 188 3 346966 6749 60 ORG 2975 0 43892 223 MISC 2030 66286 Table 1: Table showing the number of pairs of different occurrences of the same token sequence, where one occurrence is given a certain label and the other occurrence is given a certain label." ></td>
	<td class="line x" title="43:181	We show these counts both within documents, as well as over the whole corpus." ></td>
	<td class="line x" title="44:181	As we would expect, most pairs of the same entity sequence are labeled the same(i.e. the diagonal has most of the density) at both the document and corpus levels." ></td>
	<td class="line x" title="45:181	These statistics are from the CoNLL 2003 English training set." ></td>
	<td class="line x" title="46:181	Document Level Statistics Corpus Level Statistics PER LOC ORG MISC PER LOC ORG MISC PER 1941 5 2 3 9111 401 261 38 LOC 0 167 6 63 68 4560 580 1543 ORG 22 328 819 191 221 19683 5131 4752 MISC 14 224 7 365 50 12713 329 8768 Table 2: Table showing the number of (token sequence, token subsequence) pairs where the token sequence is assigned a certain entity label, and the token subsequence is assigned a certain entity label." ></td>
	<td class="line x" title="47:181	We show these counts both within documents, as well as over the whole corpus." ></td>
	<td class="line x" title="48:181	Rows correspond to sequences, and columns to subsequences." ></td>
	<td class="line x" title="49:181	These statistics are from the CoNLL 2003 English training set." ></td>
	<td class="line x" title="50:181	rences of a particular token sequence (or similar token sequences) are unlikely to have different entity labels." ></td>
	<td class="line x" title="51:181	While this constraint holds strongly at the level of a document, there exists additional value to be derived by enforcing this constraint less strongly across different documents." ></td>
	<td class="line x" title="52:181	We want to model label consistency as a soft and not a hard constraint; while we want to encourage different occurrences of similar token sequences to get labeled as the same entity, we do not want to force this to always hold, since there do exist exceptions, as can be seen from the off-diagonal entries of tables 1 and 2." ></td>
	<td class="line x" title="53:181	A named entity recognition system modeling this structure would encourage all the occurrences of the token sequence to the same entity type, thereby sharing evidence among them." ></td>
	<td class="line x" title="54:181	Thus, if the system has strong evidence about the label of a given token sequence, but is relatively unsure about the label to be assigned to another occurrence of a similar token sequence, the system can gain significantly by using the information about the label assigned to the former occurrence, to label the relatively ambiguous token sequence, leading to accuracy improvements." ></td>
	<td class="line x" title="55:181	The strength of the label consistency constraint, can be seen from statistics extracted from the CoNLL 2003 English training data." ></td>
	<td class="line x" title="56:181	Table 1 shows the counts of entity labels pairs assigned for each pair of identical token sequences both within a document and across the whole corpus." ></td>
	<td class="line x" title="57:181	As we would expect, inconsistent labelings are relatively rare and most pairs of the same entity sequence are labeled the same(i.e. the diagonal has most of the density) at both the document and corpus levels." ></td>
	<td class="line x" title="58:181	A notable exception to this is the labeling of the same text as both organization and location within the same document and across documents." ></td>
	<td class="line x" title="59:181	This is a due to the large amount of sports news in the CoNLL dataset due to which city and country names are often also team names." ></td>
	<td class="line x" title="60:181	We will see that our approach is capable of exploiting this as well, i.e. we can learn a model which would not penalize an Organization-Location inconsistency as strongly as it penalizes other inconsistencies." ></td>
	<td class="line x" title="61:181	In addition, we also want to model subsequence constraints: having seen Albert Einstein earlier in a document as a person is a good indicator that a subsequent occurrence of Einstein should also be labeled as a person." ></td>
	<td class="line x" title="62:181	Here, we would expect that a subsequence would gain much more by knowing the label of a supersequence, than the other way around." ></td>
	<td class="line x" title="63:181	However, as can be seen from table 2, we find that the consistency constraint does not hold nearly so strictly in this case." ></td>
	<td class="line oc" title="64:181	A very common case of this in the CoNLL dataset is that of documents containing references to both The China Daily, a newspaper, and China, the country (Finkel et al. , 2005)." ></td>
	<td class="line x" title="65:181	The first should be labeled as an organization, and second as a location." ></td>
	<td class="line x" title="66:181	The counts of subsequence labelings within a document and across documents listed in Table 2, show that there are many off-diagonal entries: the China Daily case is among the most common, occurring 328 times in the dataset." ></td>
	<td class="line x" title="67:181	Just as we can model off-diagonal pat1123 terns with exact token sequence matches, we can also model off-diagonal patterns for the token subsequence case." ></td>
	<td class="line x" title="68:181	In addition, we could also derive some value by enforcing some label consistency at the level of an individual token." ></td>
	<td class="line x" title="69:181	Obviously, our model would learn much lower weights for these constraints, when compared to label consistency at the level of token sequences." ></td>
	<td class="line x" title="70:181	4 Our Approach to Handling non-local Dependencies To handle the non-local dependencies between same and similar token sequences, we define three sets of feature pairs where one member of the feature pair corresponds to a function of aggregate statistics of the output of the first CRF at the document level, and the other member corresponds to a function of aggregate statistics of the output of the first CRF over the whole test corpus." ></td>
	<td class="line x" title="71:181	Thus this gives us six additional feature types for the second round CRF, namely Document-level Token-majority features, Document-level Entitymajority features, Document-level Superentitymajority features, Corpus-level Token-majority features, Corpus-level Entity-majority features and Corpus-level Superentity-majority features." ></td>
	<td class="line x" title="72:181	These feature types are described in detail below." ></td>
	<td class="line x" title="73:181	All these features are a function of the output labels of the first CRF, where predictions on the test set are obtained by training on all the data, and predictions on the train data are obtained by 10 fold cross-validation (details in the next section)." ></td>
	<td class="line x" title="74:181	Our features fired based on document and corpus level statistics are:  Token-majority features: These refer to the majority label assigned to the particular token in the document/corpus." ></td>
	<td class="line x" title="75:181	Eg: Suppose we have three occurrences of the token Australia, such that two are labeled Location and one is labeled Organization, our tokenmajority feature would take value Location for all three occurrences of the token." ></td>
	<td class="line x" title="76:181	This feature can enable us to capture some dependence between token sequences corresponding to a single entity and having common tokens." ></td>
	<td class="line x" title="77:181	 Entity-majority features: These refer to the majority label assigned to the particular entity in the document/corpus." ></td>
	<td class="line x" title="78:181	Eg: Suppose we have three occurrences of the entity sequence (we define it as a token sequence labeled as a single entity by the first stage CRF) Bank of Australia, such that two are labeled Organization and one is labeled Location, our entitymajority feature would take value Organization for all tokens in all three occurrences of the entity sequence." ></td>
	<td class="line x" title="79:181	This feature enables us to capture the dependence between identical entity sequences." ></td>
	<td class="line x" title="80:181	For token labeled as not a Named Entity by the first CRF, this feature returns the majority label assigned to that token when it occurs as a single token named entity." ></td>
	<td class="line x" title="81:181	 Superentity-majority features: These refer to the majority label assigned to supersequences of the particular entity in the document/corpus." ></td>
	<td class="line x" title="82:181	By entity supersequences, we refer to entity sequences, that strictly contain within their span, another entity sequence." ></td>
	<td class="line x" title="83:181	For example, if we have two occurrences of Bank of Australia labeled Organization and one occurrence of Australia Cup labeled Miscellaneous, then for all occurrences of the entity Australia, the superentity-majority feature would take value Organization." ></td>
	<td class="line x" title="84:181	This feature enables us to take into account labels assigned to supersequences of a particular entity, while labeling it." ></td>
	<td class="line x" title="85:181	For token labeled as not a Named Entity by the first CRF, this feature returns the majority label assigned to all entities containing the token within their span." ></td>
	<td class="line x" title="86:181	The last feature enables entity sequences to benefit from labels assigned to entities which are entity supersequences of it." ></td>
	<td class="line x" title="87:181	We attempted to add subentity-majority features, analogous to the superentity-majority features to model dependence on entity subsequences, but got no benefit from it." ></td>
	<td class="line x" title="88:181	This is intuitive, since the basic sequence model would usually be much more certain about labels assigned to the entity supersequences, since they are longer and have more contextual information." ></td>
	<td class="line x" title="89:181	As a result of this, while there would be several cases in which the basic sequence model would be uncertain about labels of entity subsequences but relatively certain about labels of token supersequences, the converse is very unlikely." ></td>
	<td class="line x" title="90:181	Thus, it is difficult to profit from labels of entity subsequences while labeling entity sequences." ></td>
	<td class="line x" title="91:181	We also attempted using more fine 1124 grained features corresponding to the majority label of supersequences that takes into account the position of the entity sequence in the entity supersequence(whether the entity sequence occurs in the start, middle or end of the supersequence), but could obtain no additional gains from this." ></td>
	<td class="line x" title="92:181	It is to be noted that while deciding if token sequences are equal or hold a subsequencesupersequence relation, we ignore case, which clearly performs better than being sensitive to case." ></td>
	<td class="line x" title="93:181	This is because our dataset contains several entities in allCaps such as AUSTRALIA, especially in news headlines." ></td>
	<td class="line x" title="94:181	Ignoring case enables us to model dependences with other occurrences with a different case such as Australia." ></td>
	<td class="line x" title="95:181	It may appear at first glance, that our framework can only learn to encourage entities to switch to the most popular label assigned to other occurrences of the entity sequence and similar entity sequences." ></td>
	<td class="line x" title="96:181	However this framework is capable of learning interesting off-diagonal patterns as well." ></td>
	<td class="line x" title="97:181	To understand this, let us consider the example of different occurrences of token sequences being labeled Location and Organization." ></td>
	<td class="line x" title="98:181	Suppose, the majority label of the token sequence is Location." ></td>
	<td class="line x" title="99:181	While this majority label would encourage the second CRF to switch the labels of all occurrences of the token sequence to Location, it would not strongly discourage the CRF from labeling these as Organization, since there would be several occurrences of token sequences in the training data labeled Organization, with the majority label of the token sequence being Location." ></td>
	<td class="line x" title="100:181	However it would discourage the other labels strongly." ></td>
	<td class="line x" title="101:181	The reasoning is analogous when the majority label is Organization." ></td>
	<td class="line x" title="102:181	In case of a tie (when computing the majority label), if the label assigned to a particular token sequence is one of the majority labels, we fire the feature corresponding to that particular label being the majority label, instead of breaking ties arbitrarily." ></td>
	<td class="line x" title="103:181	This is done to encourage the second stage CRF to make its decision based on local information, in the absence of compelling non-local information to choose a different label." ></td>
	<td class="line x" title="104:181	5 Advantages of our approach With our two-stage approach, we manage to get improvements on the F1 measure over existing approaches that model non-local dependencies." ></td>
	<td class="line oc" title="105:181	At the same time, the simplicity of our two-stage approach keeps inference time down to just the inference time of two sequential CRFs, when compared to approaches such as those of Finkel et al.(2005) who report that their inference time with Gibbs sampling goes up by a factor of about 30, compared to the Viterbi algorithm for the sequential CRF." ></td>
	<td class="line x" title="107:181	Below, we give some intuition about areas for improvement in existing work and explain how our approach incorporates the improvements." ></td>
	<td class="line oc" title="108:181	 Most existing work to capture labelconsistency, has attempted to create all parenleftbign2parenrightbig pairwise dependencies between the different occurrences of an entity, (Finkel et al. , 2005; Sutton and McCallum, 2004), where n is the number of occurrences of the given entity." ></td>
	<td class="line x" title="109:181	This complicates the dependency graph making inference harder." ></td>
	<td class="line x" title="110:181	It also leads to the penalty for deviation in labeling to grow linearly with n, since each entity would be connected to (n) entities." ></td>
	<td class="line x" title="111:181	When an entity occurs several times, these models would force all occurrences to take the same value." ></td>
	<td class="line x" title="112:181	This is not what we want, since there exist several instances in real-life data where different entities like persons and organizations share the same name." ></td>
	<td class="line x" title="113:181	Thus, our approach makes a certain entitys label depend on certain aggregate information of other labels assigned to the same entity, and does not enforce pairwise dependencies." ></td>
	<td class="line x" title="114:181	 We also exploit the fact that the predictions of a learner that takes non-local dependencies into account would have a good amount of overlap with a sequential CRF, since the sequence model is already quite competitive." ></td>
	<td class="line x" title="115:181	We use this intuition to approximate the aggregate information about labels assigned to other occurrences of the entity by the nonlocal model, with the aggregate information about labels assigned to other occurrences of the entity by the sequence model." ></td>
	<td class="line x" title="116:181	This intuition enables us to learn weights for non-local dependencies in two stages; we first get predictions from a regular sequential CRF and in turn use aggregate information about predictions made by the CRF as extra features to train a second CRF." ></td>
	<td class="line nc" title="117:181	 Most work has looked to model non-local dependencies only within a document (Finkel 1125 et al. , 2005; Chieu and Ng, 2002; Sutton and McCallum, 2004; Bunescu and Mooney, 2004)." ></td>
	<td class="line n" title="118:181	Our model can capture the weaker but still important consistency constraints across the whole document collection, whereas previous work has not, for reasons of tractability." ></td>
	<td class="line x" title="119:181	Capturing label-consistency at the level of the whole test corpus is particularly helpful for token sequences that appear only once in their documents, but occur a few times over the corpus, since they do not have strong nonlocal information from within the document." ></td>
	<td class="line x" title="120:181	 For training our second-stage CRF, we need to get predictions on our train data as well as test data." ></td>
	<td class="line x" title="121:181	Suppose we were to use the same train data to train the first CRF, we would get unrealistically good predictions on our train data, which would not be reflective of its performance on the test data." ></td>
	<td class="line x" title="122:181	One option is to partition the train data." ></td>
	<td class="line x" title="123:181	This however, can lead to a drop in performance, since the second CRF would be trained on less data." ></td>
	<td class="line x" title="124:181	To overcome this problem, we make predictions on our train data by doing a 10-fold cross validation on the train data." ></td>
	<td class="line x" title="125:181	For predictions on the test data, we use all the training data to train the CRF." ></td>
	<td class="line x" title="126:181	Intuitively, we would expect that the quality of predictions with 90% of the train data would be similar to the quality of predictions with all the training data." ></td>
	<td class="line x" title="127:181	It turns out that this is indeed the case, as can be seen from our improved performance." ></td>
	<td class="line x" title="128:181	6 Experiments 6.1 Dataset and Evaluation We test the effectiveness of our technique on the CoNLL 2003 English named entity recognition dataset downloadable from http://cnts.uia.ac.be/conll2003/ner/." ></td>
	<td class="line x" title="129:181	The data comprises Reuters newswire articles annotated with four entity types: person (PER), location (LOC), organization (ORG), and miscellaneous (MISC)." ></td>
	<td class="line x" title="130:181	The data is separated into a training set, a development set (testa), and a test set (testb)." ></td>
	<td class="line x" title="131:181	The training set contains 945 documents, and approximately 203,000 tokens and the test set has 231 documents and approximately 46,000 tokens." ></td>
	<td class="line x" title="132:181	Performance on this task is evaluated by measuring the precision and recall of annotated entities (and not tokens), combined into an F1 score." ></td>
	<td class="line x" title="133:181	There is no partial credit for labeling part of an entity sequence correctly; an incorrect entity boundary is penalized as both a false positive and as a false negative." ></td>
	<td class="line x" title="134:181	6.2 Results and Discussion It can be seen from table 3, that we achieve a 12.6% relative error reduction, by restricting ourselves to features approximating non-local dependency within a document, which is higher than other approaches modeling non-local dependencies within a document." ></td>
	<td class="line x" title="135:181	Additionally, by incorporating non-local dependencies across documents in the test corpus, we manage a 13.3% relative error reduction, over an already competitive baseline." ></td>
	<td class="line x" title="136:181	We can see that all three features approximating non-local dependencies within a document yield reasonable gains." ></td>
	<td class="line x" title="137:181	As we would expect the additional gains from features approximating nonlocal dependencies across the whole test corpus are relatively small." ></td>
	<td class="line x" title="138:181	We use the approximate randomization test (Yeh, 2000) for statistical significance of the difference between the basic sequential CRF and our second round CRF, which has additional features derived from the output of the first CRF." ></td>
	<td class="line x" title="139:181	With a 1000 iterations, our improvements were statistically significant with a p-value of 0.001." ></td>
	<td class="line x" title="140:181	Since this value is less than the cutoff threshold of 0.05, we reject the null hypothesis." ></td>
	<td class="line nc" title="141:181	The simplicity of our approach makes it easy to incorporate dependencies across the whole corpus, which would be relatively much harder to incorporate in approaches like (Bunescu and Mooney, 2004) and (Finkel et al. , 2005)." ></td>
	<td class="line nc" title="142:181	Additionally, our approach makes it possible to do inference in just about twice the inference time with a single sequential CRF; in contrast, approaches like Gibbs Sampling that model the dependencies directly can increase inference time by a factor of 30 (Finkel et al. , 2005)." ></td>
	<td class="line x" title="143:181	An analysis of errors by the first stage CRF revealed that most errors are that of single token entities being mislabeled or missed altogether followed by a much smaller percentage of multiple token entities mislabelled completely." ></td>
	<td class="line x" title="144:181	All our features directly encode information that is useful to reducing these errors." ></td>
	<td class="line x" title="145:181	The widely prevalent boundary detection error is that of missing a single-token entity (i.e. labeling it as Other(O))." ></td>
	<td class="line nc" title="146:181	Our approach helps correct many such errors based on occurrences of the token in other 1126 F1 scores on the CoNLL Dataset Approach LOC ORG MISC PER ALL Relative Error reduction Bunescu and Mooney (2004) (Relational Markov Networks) Only Local Templates 80.09 Global and Local Templates 82.30 11.1% Finkel et al.(2005)(Gibbs Sampling) Local+Viterbi 88.16 80.83 78.51 90.36 85.51 Non Local+Gibbs 88.51 81.72 80.43 92.29 86.86 9.3% Our Approach with the 2-stage CRF Baseline CRF 88.09 80.88 78.26 89.76 85.29 + Document token-majority features 89.17 80.15 78.73 91.60 86.50 + Document entity-majority features 89.50 81.98 79.38 91.74 86.75 + Document superentity-majority features 89.52 82.27 79.76 92.71 87.15 12.6% + Corpus token-majority features 89.48 82.36 79.59 92.65 87.13 + Corpus entity-majority features 89.72 82.40 79.71 92.65 87.23 + Corpus superentity-majority features (All features) 89.80 82.39 79.76 92.57 87.24 13.3% Table 3: Table showing improvements obtained with our additional features, over the baseline CRF." ></td>
	<td class="line nc" title="148:181	We also compare our performance against (Bunescu and Mooney, 2004) and (Finkel et al. , 2005) and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF." ></td>
	<td class="line x" title="149:181	named entities." ></td>
	<td class="line x" title="150:181	Other kinds of boundary detection errors involving multiple tokens are very rare." ></td>
	<td class="line x" title="151:181	Our approach can also handle these errors by encouraging certain tokens to take different labels." ></td>
	<td class="line x" title="152:181	This together with the clique features encoding the markovian dependency among neighbours can correct some multiple-token boundary detection errors." ></td>
	<td class="line oc" title="153:181	7 Related Work Recent work looking to directly model non-local dependencies and do approximate inference are that of Bunescu and Mooney (2004), who use a Relational Markov Network (RMN) (Taskar et al. , 2002) to explicitly model long-distance dependencies, Sutton and McCallum (2004), who introduce skip-chain CRFs, which add additional non-local edges to the underlying CRF sequence model (which Bunescu and Mooney (2004) lack) and Finkel et al.(2005) who hand-set penalties for inconsistency in labels based on the training data and then use Gibbs Sampling for doing approximate inference where the goal is to obtain the label sequence that maximizes the product of the CRF objective function and their penalty." ></td>
	<td class="line x" title="155:181	Unfortunately, in the RMN model, the dependencies must be defined in the model structure before doing any inference, and so the authors use heuristic part-of-speech patterns, and then add dependencies between these text spans using clique templates." ></td>
	<td class="line x" title="156:181	This generates an extremely large number of overlapping candidate entities, which renders necessary additional templates to enforce the constraint that text subsequences cannot both be different entities, something that is more naturally modeled by a CRF." ></td>
	<td class="line x" title="157:181	Another disadvantage of this approach is that it uses loopy belief propagation and a voted perceptron for approximate learning and inference, which are inherently unstable algorithms leading to convergence problems, as noted by the authors." ></td>
	<td class="line x" title="158:181	In the skip-chain CRFs model, the decision of which nodes to connect is also made heuristically, and because the authors focus on named entity recognition, they chose to connect all pairs of identical capitalized words." ></td>
	<td class="line x" title="159:181	They also utilize loopy belief propagation for approximate learning and inference." ></td>
	<td class="line x" title="160:181	It is hard to directly extend their approach to model dependencies richer than those at the token level." ></td>
	<td class="line nc" title="161:181	The approach of Finkel et al.(2005) makes it possible a to model a broader class of longdistance dependencies than Sutton and McCallum (2004), because they do not need to make any initial assumptions about which nodes should be connected and they too model dependencies between whole token sequences representing entities and between entity token sequences and their token supersequences that are entities." ></td>
	<td class="line n" title="163:181	The disadvantage of their approach is the relatively ad-hoc selection of penalties and the high computational cost of running Gibbs sampling." ></td>
	<td class="line x" title="164:181	Early work in discriminative NER employed two stage approaches that are broadly similar to ours, but the effectiveness of this approach appears to have been overlooked in more recent work." ></td>
	<td class="line x" title="165:181	Mikheev et al.(1999) exploit label consistency information within a document using relatively ad hoc multi-stage labeling procedures." ></td>
	<td class="line x" title="167:181	Borth1127 wick (1999) used a two-stage approach similar to ours with CMMs where Reference Resolution features which encoded the frequency of occurrences of other entities similar to the current token sequence, were derived from the output of the first stage." ></td>
	<td class="line x" title="168:181	Malouf (2002) and Curran and Clark (2003) condition the label of a token at a particular position on the label of the most recent previous instance of that same token in a previous sentence of the same document." ></td>
	<td class="line x" title="169:181	This violates the Markov property and therefore instead of finding the maximum likelihood sequence over the entire document (exact inference), they label one sentence at a time, which allows them to condition on the maximum likelihood sequence of previous sentences." ></td>
	<td class="line x" title="170:181	While this approach is quite effective for enforcing label consistency in many NLP tasks, it permits a forward flow of information only, which can result in loss of valuable information." ></td>
	<td class="line x" title="171:181	Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features based on known information, taken from other occurrences of the same token in the document." ></td>
	<td class="line x" title="172:181	This approach has the advantage of allowing the training procedure to automatically learn good weights for these global features relative to the local ones." ></td>
	<td class="line x" title="173:181	However, it is hard to extend this to incorporate other types of non-local structure." ></td>
	<td class="line x" title="174:181	8 Conclusion We presented a two stage approach to model nonlocal dependencies and saw that it outperformed existing approaches to modeling non-local dependencies." ></td>
	<td class="line x" title="175:181	Our approach also made it easy to exploit various dependencies across documents in the test corpus, whereas incorporating this information in most existing approaches would make them intractable due to the complexity of the resultant graphical model." ></td>
	<td class="line x" title="176:181	Our simple approach is also very computationally efficient since the inference time is just twice the inference time of the basic sequential CRF, while for approaches doing approximate inference, the inference time is often well over an order of magnitude over the basic sequential CRF." ></td>
	<td class="line x" title="177:181	The simplicity of our approach makes it easy to understand, implement, and adapt to new applications." ></td>
	<td class="line x" title="178:181	Acknowledgments We wish to Jenny R. Finkel for discussions on NER and her CRF code." ></td>
	<td class="line x" title="179:181	Also, thanks to Trond Grenager for NER discussions and to William Morgan for help with statistical significance tests." ></td>
	<td class="line x" title="180:181	Also, thanks to Vignesh Ganapathy for helpful discussions and Rohini Rajaraman for comments on the writeup." ></td>
	<td class="line x" title="181:181	This work was supported in part by a Scottish Enterprise Edinburgh-Stanford Link grant (R37588), as part of the EASIE project." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-2054
Exploiting Non-Local Features For Spoken Language Understanding
Jeong, Minwoo;Lee, Gary Geunbae;"></td>
	<td class="line x" title="1:262	Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 412419, Sydney, July 2006." ></td>
	<td class="line x" title="2:262	c2006 Association for Computational Linguistics Exploiting Non-local Features for Spoken Language Understanding Minwoo Jeong and Gary Geunbae Lee Department of Computer Science & Engineering Pohang University of Science and Technology, San 31 Hyoja-dong, Nam-gu Pohang 790-784, Korea {stardust,gblee}@postech.ac.kr Abstract In this paper, we exploit non-local features as an estimate of long-distance dependencies to improve performance on the statistical spoken language understanding (SLU) problem." ></td>
	<td class="line x" title="3:262	The statistical natural language parsers trained on text perform unreliably to encode non-local information on spoken language." ></td>
	<td class="line x" title="4:262	An alternative method we propose is to use trigger pairs that are automatically extracted by a feature induction algorithm." ></td>
	<td class="line x" title="5:262	We describe a light version of the inducer in which a simple modification is efficient and successful." ></td>
	<td class="line x" title="6:262	We evaluate our method on an SLU task and show an error reduction of up to 27% over the base local model." ></td>
	<td class="line x" title="7:262	1 Introduction For most sequential labeling problems in natural language processing (NLP), a decision is made based on local information." ></td>
	<td class="line x" title="8:262	However, processing that relies on the Markovian assumption cannot represent higher-order dependencies." ></td>
	<td class="line x" title="9:262	This longdistance dependency problem has been considered at length in computational linguistics." ></td>
	<td class="line x" title="10:262	It is the key limitation in bettering sequential models in various natural language tasks." ></td>
	<td class="line x" title="11:262	Thus, we need new methods to import non-local information into sequential models." ></td>
	<td class="line x" title="12:262	There are two types of method for using nonlocal information." ></td>
	<td class="line x" title="13:262	One is to add edges to structure to allow higher-order dependencies and another is to add features (or observable variables) to encode the non-locality." ></td>
	<td class="line oc" title="14:262	An additional consistent edge of a linear-chain conditional random field (CRF) explicitly models the dependencies between distant occurrences of similar words (Sutton and McCallum, 2004; Finkel et al. , 2005)." ></td>
	<td class="line n" title="15:262	However, this approach requires additional time complexity in inference/learning time and it is only suitable for representing constraints by enforcing label consistency." ></td>
	<td class="line x" title="16:262	We wish to identify ambiguous labels with more general dependency without additional time cost in inference/learning time." ></td>
	<td class="line x" title="17:262	Another approach to modeling non-locality is to use observational features which can capture non-local information." ></td>
	<td class="line x" title="18:262	Traditionally, many systems prefer to use a syntactic parser." ></td>
	<td class="line x" title="19:262	In a language understanding task, the head word dependencies or parse tree path are successfully applied to learn and predict semantic roles, especially those with ambiguous labels (Gildea and Jurafsky, 2002)." ></td>
	<td class="line x" title="20:262	Although the power of syntactic structure is impressive, using the parser-based feature fails to encode correct global information because of the low accuracy of a modern parser." ></td>
	<td class="line x" title="21:262	Furthermore the inaccurate result of parsing is more serious in a spoken language understanding (SLU) task." ></td>
	<td class="line x" title="22:262	In contrast to written language, spoken language loses much information including grammar, structure or morphology and contains some errors in automatically recognized speech." ></td>
	<td class="line x" title="23:262	To solve the above problems, we present one method to exploit non-local information  the trigger feature." ></td>
	<td class="line x" title="24:262	In this paper, we incorporate trigger pairs into a sequential model, a linear-chain CRF." ></td>
	<td class="line x" title="25:262	Then we describe an efficient algorithm to extract the trigger feature from the training data itself." ></td>
	<td class="line x" title="26:262	The framework for inducing trigger features is based on the Kullback-Leibler divergence criterion which measures the improvement of loglikelihood on the current parameters by adding a new feature (Pietra et al. , 1997)." ></td>
	<td class="line x" title="27:262	To reduce the cost of feature selection, we suggest a modified 412 version of an inducing algorithm which is quite efficient." ></td>
	<td class="line x" title="28:262	We evaluate our method on an SLU task, and demonstrate the improvements on both transcripts and recognition outputs." ></td>
	<td class="line x" title="29:262	On a real-world problem, our modified version of a feature selection algorithm is very efficient for both performance and time complexity." ></td>
	<td class="line x" title="30:262	2 Spoken Language Understanding as a Sequential Labeling Problem 2.1 Spoken Language Understanding The goal of SLU is to extract semantic meanings from recognized utterances and to fill the correct values into a semantic frame structure." ></td>
	<td class="line x" title="31:262	A semantic frame (or template) is a well-formed and machine readable structure of extracted information consisting of slot/value pairs." ></td>
	<td class="line x" title="32:262	An example of such a reference frame is as follows." ></td>
	<td class="line x" title="33:262	<s> i wanna go from denver to new york on november eighteenth </s> FROMLOC.CITY NAME = denver TOLOC.CITY NAME = new york MONTH NAME = november DAY NUMBER = eighteenth This example from air travel data (CUCommunicator corpus) was automatically generated by a Phoenix parser and manually corrected (Pellom et al. , 2000; He and Young, 2005)." ></td>
	<td class="line x" title="34:262	In this example, the slot labels are two-level hierarchical; such as FROMLOC.CITY NAME." ></td>
	<td class="line x" title="35:262	This hierarchy differentiates the semantic frame extraction problem from the named entity recognition (NER) problem." ></td>
	<td class="line x" title="36:262	Regardless of the fact that there are some differences between SLU and NER, we can still apply well-known techniques used in NER to an SLU problem." ></td>
	<td class="line x" title="37:262	Following (Ramshaw and Marcus, 1995), the slot labels are drawn from a set of classes constructed by extending each label by three additional symbols, Beginning/Inside/Outside (B/I/O)." ></td>
	<td class="line x" title="38:262	A two-level hierarchical slot can be considered as an integrated flattened slot." ></td>
	<td class="line x" title="39:262	For example, FROMLOC.CITY NAME and TOLOC.CITY NAMEare different on this slot definition scheme." ></td>
	<td class="line x" title="40:262	Now, we can formalize the SLU problem as a sequential labeling problem, y = argmaxy P(y|x)." ></td>
	<td class="line x" title="41:262	In this case, input word sequences x are not only lexical strings, but also multiple linguistic features." ></td>
	<td class="line x" title="42:262	To extract semantic frames from utterance inputs, we use a linearchain CRF model; a model that assigns a joint probability distribution over labels which is conditional on the input sequences, where the distribution respects the independent relations encoded in a graph (Lafferty et al. , 2001)." ></td>
	<td class="line x" title="43:262	A linear-chain CRF is defined as follows." ></td>
	<td class="line x" title="44:262	Let G be an undirected model over sets of random variables x and y. The graph G with parameters  = {,} defines a conditional probability for a state (or label) sequence y = y1,,yT, given an input x = x1,,xT, to be P(y|x) = 1Z x exp parenleftBigg Tsummationdisplay t=1 summationdisplay k kfk(yt1,yt,x,t) parenrightBigg where Zx is the normalization factor that makes the probability of all state sequences sum to one." ></td>
	<td class="line x" title="45:262	fk(yt1,yt,x,t) is an arbitrary linguistic feature function which is often binary-valued in NLP tasks." ></td>
	<td class="line x" title="46:262	k is a trained parameter associated with feature fk." ></td>
	<td class="line x" title="47:262	The feature functions can encode any aspect of a state transition, yt1  yt, and the observation (a set of observable features), x, centered at the current time, t. Large positive values for k indicate a preference for such an event, while large negative values make the event unlikely." ></td>
	<td class="line x" title="48:262	Parameter estimation of a linear-chain CRF is typically performed by conditional maximum loglikelihood." ></td>
	<td class="line x" title="49:262	To avoid overfitting, the 2-norm regularization is applied to penalize on weight vector whose norm is too large." ></td>
	<td class="line x" title="50:262	We used a limited memory version of the quasi-Newton method (LBFGS) to optimize this objective function." ></td>
	<td class="line x" title="51:262	The L-BFGS method converges super-linearly to the solution, so it can be an efficient optimization technique on large-scale NLP problems (Sha and Pereira, 2003)." ></td>
	<td class="line x" title="52:262	A linear-chain CRF has been previously applied to obtain promising results in various natural language tasks, but the linear-chain structure is deficient in modeling long-distance dependencies because of its limited structure (n-th order Markov chains)." ></td>
	<td class="line x" title="53:262	2.2 Long-distance Dependency in Spoken Language Understanding In most sequential supervised learning problems including SLU, the feature function fk(yt1,yt,xt,t) indicates only local information 413 for practical reasons." ></td>
	<td class="line x" title="54:262	With sufficient local context (e.g. a sliding window of width 5), inference and learning are both efficient." ></td>
	<td class="line x" title="55:262	However, if we only use local features, then we cannot model long-distance dependencies." ></td>
	<td class="line x" title="56:262	Thus, we should incorporate non-local information into the model." ></td>
	<td class="line x" title="57:262	For example, figure 1 shows the long-distance dependency problem in an SLU task." ></td>
	<td class="line x" title="58:262	The same two word tokens dec. should be classified differently, DEPART.MONTH and RETURN.MONTH." ></td>
	<td class="line x" title="59:262	The dotted line boxes represent local information at the current decision point (dec.), but they are exactly the same in two distinct examples." ></td>
	<td class="line x" title="60:262	Moreover, the two states share the same previous sequence (O, O, FROMLOC.CITY NAME-B, O, TOLOC.CITY NAME-B, O)." ></td>
	<td class="line x" title="61:262	If we cannot obtain higher-order dependencies such as fly and return, then the linear-chain CRF cannot classify the correct labels between the two same tokens." ></td>
	<td class="line x" title="62:262	To solve this problem, we propose an approach to exploit non-local information in the next section." ></td>
	<td class="line x" title="63:262	3 Incorporating Non-local Information 3.1 Using Trigger Features To exploit non-local information to sequential labeling for a statistical SLU, we can use two approaches; a syntactic parser-based and a datadriven approach." ></td>
	<td class="line x" title="64:262	Traditionally, information extraction and language understanding fields have usually used a syntactic parser to encode global information (e.g. parse tree path, governing category, or head word) over a local model." ></td>
	<td class="line x" title="65:262	In a semantic role labeling task, the syntax and semantics are correlated with each other (Gildea and Jurafsky, 2002), that is, the global structure of the sentence is useful for identifying ambiguous semantic roles." ></td>
	<td class="line x" title="66:262	However the problem is the poor accuracy of the syntactic parser with this type of feature." ></td>
	<td class="line x" title="67:262	In addition, recognized utterances are erroneous and the spoken language has no capital letters, no additional symbols, and sometimes no grammar, so it is difficult to use a parser in an SLU problem." ></td>
	<td class="line x" title="68:262	Another solution is a data-driven method, which uses statistics to find features that are approximately modeling long-distance dependencies." ></td>
	<td class="line x" title="69:262	The simplest way is to use identical words in history or lexical co-occurrence, but we wish to use a more general tool; triggering." ></td>
	<td class="line x" title="70:262	The trigger word pairs are introduced by (Rosenfeld, 1994)." ></td>
	<td class="line x" title="71:262	A trigger pair is the basic element for extracting information from the long-distance document history." ></td>
	<td class="line x" title="72:262	In language modeling, n-gram based on the Markovian assumption cannot represent higher-order dependencies, but it can automatically extract trigger word pairs from data." ></td>
	<td class="line x" title="73:262	The pair (A  B) means that word A and B are significantly correlated, that is, when A occurs in the document, it triggers B, causing its probability estimate to change." ></td>
	<td class="line x" title="74:262	To select reasonable pairs from arbitrary word pairs, (Rosenfeld, 1994) used averaged mutual information (MI)." ></td>
	<td class="line x" title="75:262	In this scheme, the MI score of one pair is MI(A;B) = P(A,B)log P(B|A)P(B) +P(A, B)log P( B|A) P( B) + P( A,B)log P(B| A) P( B) +P( A, B)log P( B| A) P( B)." ></td>
	<td class="line x" title="76:262	Using the MI criterion, we can select correlated word pairs." ></td>
	<td class="line x" title="77:262	For example, the trigger pair (dec.return) was extracted with score 0.001179 in the training data1." ></td>
	<td class="line x" title="78:262	This trigger word pair can represent long-distance dependency and provide a cue to identify ambiguous classes." ></td>
	<td class="line x" title="79:262	The MI approach, however, considers only lexical collocation without reference labels y, and MI based selection tends to excessively select the irrelevant triggers." ></td>
	<td class="line x" title="80:262	Recall that our goal is to find the significantly correlated trigger pairs which improve the model." ></td>
	<td class="line x" title="81:262	Therefore, we use a more appropriate selection method for sequential supervised learning." ></td>
	<td class="line x" title="82:262	3.2 Selecting Trigger Feature We present another approach to extract relevant triggers and exploit them in a linear-chain CRF." ></td>
	<td class="line x" title="83:262	Our approach is based on an automatic feature induction algorithm, which is a novel method to select a feature in an exponential model (Pietra et al. , 1997; McCallum, 2003)." ></td>
	<td class="line x" title="84:262	We follow McCallums work which is an efficient method to induce features in a linear-chain CRF model." ></td>
	<td class="line x" title="85:262	Following the framework of feature inducing, we start the algorithm with an empty set, and iteratively increase the bundle of features including local features and trigger features." ></td>
	<td class="line x" title="86:262	Our basic assumption, however, is that the local information should be included because the local features are the basis of the decision to identify the classes, and they reduce the 1In our experiment, the pair (dec.fly) cannot be selected because this MI score is too low." ></td>
	<td class="line x" title="87:262	However, the trigger pair is a binary type feature, so the pair (dec.return) is enough to classify the two cases in the previous example." ></td>
	<td class="line x" title="88:262	414 1999dec.onchicagotodenverfromfly 10th 1999dec.onchicagotodenverfrom 10threturn   DEPART.MONTH RETURN.MONTH Figure 1: An example of a long-distance dependency problem in spoken language understanding." ></td>
	<td class="line x" title="89:262	In this case, a word token dec. with local feature set (dotted line box) is ambiguous for determining the correct label (DEPART.MONTH or RETURN.MONTH)." ></td>
	<td class="line x" title="90:262	mismatch between training and testing tasks." ></td>
	<td class="line x" title="91:262	Furthermore, this assumption leads us to faster training in the inducing procedure because we can only consider additional trigger features." ></td>
	<td class="line x" title="92:262	Now, we start the inducing process with local features rather than an empty set." ></td>
	<td class="line x" title="93:262	After training the base model (0), we should calculate the gains, which measure the effect of adding a trigger feature, based on the local model parameter (0)." ></td>
	<td class="line x" title="94:262	The gain of the trigger feature is defined as the improvement in log-likelihood of the current model (i) at the i-th iteration according to the following formula: G(i)(g) = max  G(i)(g,) = max braceleftBig L(i)+g, L(i) bracerightBig where  is a parameter of a trigger feature to be found and g is a corresponding trigger feature function." ></td>
	<td class="line x" title="95:262	The optimal value of  can be calculated by Newtons method." ></td>
	<td class="line x" title="96:262	By adding a new candidate trigger, the equation of the linear-chain CRF model is changed to an additional feature model as P(i)+g,(y|x) = P(i)(y|x)exp parenleftBigsummationtextT t=1 g(yt1,yt,x,t) parenrightBig Zx((i),g,) . Note that Zx((i),g,) is the marginal sum over all states of yprime." ></td>
	<td class="line x" title="97:262	Following (Pietra et al. , 1997; McCallum, 2003), the mean field approximation and agglomerated features allows us to treat the above calculation as the independent inference problem rather than sequential inference." ></td>
	<td class="line x" title="98:262	We can evaluate the probability of state y with an adding trigger pair given observation x separately as follows." ></td>
	<td class="line x" title="99:262	P(i)+g,(y|x,t) = P(i)(y|x,t)exp(g(yt,x,t))Z x((i),g,) Here, we introduce a second approximation." ></td>
	<td class="line x" title="100:262	We use the individual inference problem over the unstructured maximum entropy (ME) model whose state variable is independent from other states in history." ></td>
	<td class="line x" title="101:262	The background of our approximation is that the state independent problem of CRF can be relaxed to ME inference problem without the state-structured model." ></td>
	<td class="line x" title="102:262	In the result, we calculate the gain of candidate triggers, and select trigger features over a light ME model instead of a huge computational CRF model2." ></td>
	<td class="line x" title="103:262	We can efficiently assess many candidate trigger features in parallel by assuming that the old features remain fixed while estimating the gain." ></td>
	<td class="line x" title="104:262	The gain of trigger features can be calculated on the old model that is trained with the local and added trigger pairs in previous iterations." ></td>
	<td class="line x" title="105:262	Rather than summing over all training instances, we only need to use the mislabeled N tokens by the current parameter (i) (McCallum, 2003)." ></td>
	<td class="line x" title="106:262	From misclassified instances, we generate the candidates of trigger pairs, that is, all pairs of current words and others within the sentence." ></td>
	<td class="line x" title="107:262	With the candidate feature set, the gain is G(i)(g) = NE[g]  Nsummationdisplay j=1 log(E(i)[exp(g)|xj])  2 22." ></td>
	<td class="line x" title="108:262	Using the estimated gains, we can select a small portion of all candidates, and retrain the model with selected features." ></td>
	<td class="line x" title="109:262	We iteratively perform the selection algorithm with some stop conditions (excess of maximum iteration or no added feature up to the gain threshold)." ></td>
	<td class="line x" title="110:262	The outline of the induction 2The ME model cannot represent the sequential structure and the resulting model is different from CRF." ></td>
	<td class="line x" title="111:262	Nevertheless, we empirically prove that the effect of additional trigger features on both ME and approximated CRF (without regarding edge-state) are similar (see the experiment section)." ></td>
	<td class="line x" title="112:262	415 Algorithm InduceLearn(x,y) triggers {} and i  0 while |pairs| > 0 and i < maxiter do (i)  TrainME(x,y) P(ye|xe)  Evaluate(x,y,(i)) c  MakeCandidate(xe) G(i)  EstimateGain(c,P(ye|xe)) pairs  SelectTrigger(c,G(i)) x  UpdateObs(x,pairs) triggers  triggerspairs and i  i+1 end while (i+1)  TrainCRF(x,y) return (i+1) Figure 2: Outline of trigger feature induction algorithm algorithms is described in figure 2." ></td>
	<td class="line x" title="113:262	In the next section, we empirically prove the effectiveness of our algorithm." ></td>
	<td class="line x" title="114:262	The trigger pairs introduced by (Rosenfeld, 1994) are just word pairs." ></td>
	<td class="line x" title="115:262	Here, we can generalize the trigger pairs to any arbitrary pairs of features." ></td>
	<td class="line x" title="116:262	For example, the feature pair (ofBPP) is useful in deciding the correct answer PERIOD OF DAY-Iin in the middle of the day. Without constraints on generating the pairs (e.g. at most 3 distant tokens), the candidates can be arbitrary conjunctions of features3." ></td>
	<td class="line x" title="117:262	Therefore we can explore any features including local conjunction or non-local singleton features in a uniform framework." ></td>
	<td class="line x" title="118:262	4 Experiments 4.1 Experimental Setup We evaluate our method on the CU-Communicator corpus." ></td>
	<td class="line x" title="119:262	It consists of 13,983 utterances." ></td>
	<td class="line x" title="120:262	The semantic categories correspond to city names, timerelated information, airlines and other miscellaneous entities." ></td>
	<td class="line x" title="121:262	The semantic labels are automatically generated by a Phoenix parser and manually corrected." ></td>
	<td class="line x" title="122:262	In the data set, the semantic category has a two-level hierarchy: 31 first level classes and 7 second level classes, for a total of 62 class combinations." ></td>
	<td class="line x" title="123:262	The data set is 630k words with 29k entities." ></td>
	<td class="line x" title="124:262	Roughly half of the entities are timerelated information, a quarter of the entities are 3In our experiment, we do not consider the local conjunctions because we wish to capture the effect of long-distance entities." ></td>
	<td class="line x" title="125:262	city names, a tenth are state and country names, and a fifth are airline and airport names." ></td>
	<td class="line x" title="126:262	For the second level hierarchy, approximately three quarters of the entities are NONE, a tenth are TOLOC, a tenth are FROMLOC, and the remaining are RETURN, DEPERT, ARRIVE, and STOPLOC. For spoken inputs, we used the open source speech recognizer Sphinx2." ></td>
	<td class="line x" title="127:262	We trained the recognizer with only the domain-specific speech corpus." ></td>
	<td class="line x" title="128:262	The reported accuracy for Sphinx2 speech recognition is about 85%, but the accuracy of our speech recognizer is 76.27%; we used only a subset of the data without tuning and the sentences of this subset are longer and more complex than those of the removed ones, most of which are single-word responses." ></td>
	<td class="line x" title="129:262	All of our results have averaged over 5-fold cross validation with an 80/20 split of the data." ></td>
	<td class="line x" title="130:262	As it is standard, we compute precision and recall, which are evaluated on a per-entity basis and combined into a micro-averaged F1 score (F1 = 2PR/(P+R))." ></td>
	<td class="line x" title="131:262	A final model (a first-order linear chain CRF) is trained for 100 iterations with a Gaussian prior variance of 20, and 200 or fewer trigger features (down to a gain threshold of 1.0) for each round of inducing iteration (100 iterations of L-BFGS for the ME inducer and 1020 iterations of L-BFGS for the CRF inducer)." ></td>
	<td class="line x" title="132:262	All experiments are implemented in C++ and executed on Linux with XEON 2.8 GHz dual processors and 2.0 Gbyte of main memory." ></td>
	<td class="line x" title="133:262	4.2 Empirical Results We list the feature templates used by our experiment in figure 3." ></td>
	<td class="line x" title="134:262	For local features, we use the indicators for specific words at location i, or locations within five words of i (2,1,0,+1,+2 words on current position i)." ></td>
	<td class="line x" title="135:262	We also use the partof-speech (POS) tags and phrase labels with partial parsing." ></td>
	<td class="line x" title="136:262	Like words, the two basic linguistic features are located within five tokens." ></td>
	<td class="line x" title="137:262	For comparison, we exploit the two groups of nonlocal syntax parser-based features; we use Collins parser and extract this type of features from the parse trees." ></td>
	<td class="line x" title="138:262	The first consists of the head word and POS-tag of the head word." ></td>
	<td class="line x" title="139:262	The second group includes governing category and parse tree paths introduced by semantic role labeling (Gildea and Jurafsky, 2002)." ></td>
	<td class="line x" title="140:262	Following the previous studies 416 Local feature templates -lexical words -part-of-speech (POS) tags -phrase chunk labels Grammar-based feature templates -head word / POS-tag -parse tree path and governing category Trigger feature templates -word pairs (wi  wj), |ij| > 2 -feature pairs between words, POS-tags, and chunk labels (fi  fj), |ij| > 2 -null pairs (  wj) Figure 3: Feature templates of semantic role labeling, the parse tree path improves the classification performance of semantic role labeling." ></td>
	<td class="line x" title="141:262	Finally, we use the trigger pairs that are automatically extracted from the training data." ></td>
	<td class="line x" title="142:262	Avoiding the overlap of local features, we add the constraint |ij| > 2 for the target word wj." ></td>
	<td class="line x" title="143:262	Note that null pairs are equivalent to long-distance singleton word features wj." ></td>
	<td class="line x" title="144:262	To compute feature performance, we begin with word features and iteratively add them one-by-one so that we achieve the best performance." ></td>
	<td class="line x" title="145:262	Table 1 shows the empirical results of local features, syntactic parser-based features, and trigger features respectively." ></td>
	<td class="line x" title="146:262	The two F1 scores for text transcripts (Text) and outputs recognized by an automatic speech recognizer (ASR) are listed." ></td>
	<td class="line x" title="147:262	We achieved F1 scores of 94.79 and 71.79 for Text and ASR inputs using only word features." ></td>
	<td class="line x" title="148:262	The performance is decreased by adding the additional local features (POS-tags and chunk labels) because the pre-processor brings more errors to the system for spoken dialog." ></td>
	<td class="line x" title="149:262	The parser-based and trigger features are added to two baselines: word only and all local features." ></td>
	<td class="line x" title="150:262	The result shows that the trigger feature is more robust to an SLU task than the features generated from the syntactic parser." ></td>
	<td class="line x" title="151:262	The parse tree path and governing category show a small improvement of performance over local features, but it is rather insignificant (word vs. word+path, McNemars test (Gillick and Cox, 1989); p = 0.022)." ></td>
	<td class="line x" title="152:262	In contrast, the trigger features significantly improve the performance of the system for both Text and ASR inputs." ></td>
	<td class="line x" title="153:262	The differences between the trigger and the others are statistically significant (McNemars test; p < 0.001 for both Text and ASR)." ></td>
	<td class="line x" title="154:262	Table 1: The result of local features, parser-based features and trigger features Feature set F1 (Text) F1 (ASR) word (w) 94.79 71.79 w + POStag (p) 94.57 71.61 w + chunk (c) 94.70 71.64 local (w+p+c) 94.41 71.60 w + head (h) 94.55 71.76 w + path (t) 95.07 72.17 w + h + t 94.84 72.09 local + head (h) 94.17 71.39 local + path (t) 94.80 71.89 local + h + t 94.51 71.67 w + trigger 96.18 72.95 local + trigger 96.04 72.72 Next, we compared the two trigger selection methods; mutual information (MI) and feature induction (FI)." ></td>
	<td class="line x" title="155:262	Table 2 shows the experimental results of the comparison between MI and FI approaches (with the local feature set; w+p+c)." ></td>
	<td class="line x" title="156:262	For the MI-based approach, we should calculate an averaged MI for each word pair appearing in a sentence and cut the unreliable pairs (down to threshold of 0.0001) before training the model." ></td>
	<td class="line x" title="157:262	In contrast, the FI-based approach selects reliable triggers which should improve the model in training time." ></td>
	<td class="line x" title="158:262	Our method based on the feature induction algorithm outperforms simple MI-based methods." ></td>
	<td class="line x" title="159:262	Fewer features are selected by FI, that is, our method prunes the event pairs which are highly correlated, but not relevant to models." ></td>
	<td class="line x" title="160:262	The extended feature trigger (fi  fj) and null triggers (  wj) improve the performance over word trigger pairs (wi  wj), but they are not statistically significant (vs." ></td>
	<td class="line x" title="161:262	(fi  fj); p = 0.749, vs." ></td>
	<td class="line x" title="162:262	({,wi}  wj); p = 0.294)." ></td>
	<td class="line x" title="163:262	Nevertheless, the null pairs are effective in reducing the size of trigger features." ></td>
	<td class="line x" title="164:262	Figure 4 shows a sample of triggers selected by MI and FI approaches." ></td>
	<td class="line x" title="165:262	For example, the trigger morning  return is ranked in first of FI but 66th of MI." ></td>
	<td class="line x" title="166:262	Moreover, the top 5 pairs of MI are not meaningful, that is, MI selects many functional word pairs." ></td>
	<td class="line x" title="167:262	The MI approach considers only lexical collocation without reference labels, so the FI method is more appropriate to sequential supervised learning." ></td>
	<td class="line x" title="168:262	Finally, we wish to justify that our modified 417 Table 2: Result of the trigger selection methods Method Avg." ></td>
	<td class="line x" title="169:262	# triggers F1 (Text) F1 (ASR) McNemars test (vs. MI) MI (wi  wj) 1,713 95.20 72.12 FI (wi  wj) 702 96.04 72.72 p < 0.001 FI (fi  fj) 805 96.04 72.76 p < 0.001 FI ({,wi} wj) 545 96.14 72.80 p < 0.001 Mutual Information Feature Induction [1] fromlike [1] morningreturn [2] onto [2] morningon [3] toi [3] morningto [4] onfrom [4] afternoonon [5] fromi [5] afternoonreturn [41] afternoonreturn [6] afternoonto [66] morningreturn [15] morningleaving [89] morningleaving [349] decemberreturn [1738] londonfly [608] illinoisairport Figure 4: A sample of triggers extracted by two methods version of an inducing algorithm is efficient and maintains performance without any drawbacks." ></td>
	<td class="line x" title="170:262	We proposed two approximations: starting with local features (Approx." ></td>
	<td class="line x" title="171:262	1) and using an unstructured model on the selection stage (Approx." ></td>
	<td class="line x" title="172:262	2), Table 3 shows the results of variant versions of the algorithm." ></td>
	<td class="line x" title="173:262	Surprisingly, the selection criterion based on ME (the unstructured model) is better than CRF (the structured model) not only for time cost but also for the performance on our experiment4." ></td>
	<td class="line x" title="174:262	This result shows that local information provides the fundamental decision clues." ></td>
	<td class="line x" title="175:262	Our modification of the algorithm to induce features for CRF is sufficiently fast for practical usage." ></td>
	<td class="line x" title="176:262	5 Related Work and Discussion The most relevant previous work is (He and Young, 2005) who describes an generative approach  hidden vector state (HVS) model." ></td>
	<td class="line x" title="177:262	They used 1,178 test utterances with 18 classes for 1st level label, and published the resulting F1 score of 88.07." ></td>
	<td class="line x" title="178:262	Using the same test data and classes, we achieved the 92.77 F1-performance, as well 4In our analysis, 1020 iterations for each round of inducing procedure are insufficient in optimizing the model in CRF (empty) inducer." ></td>
	<td class="line x" title="179:262	Thus, the resulting parameters are under-fitted and selected features are infeasible." ></td>
	<td class="line x" title="180:262	We need more iteration to fit the parameters, but they require too much learning time (> 1 day)." ></td>
	<td class="line x" title="181:262	as 39% of error reduction compared to the previous result." ></td>
	<td class="line x" title="182:262	Our system uses a discriminative approach, which directly models the conditional distribution, and it is sufficient for classification task." ></td>
	<td class="line x" title="183:262	To capture long-distance dependency, HVS uses a context-free model, which increases the complexity of models." ></td>
	<td class="line x" title="184:262	In contrast, we use non-local trigger features, which are relatively easy to use without having additional complexity of models." ></td>
	<td class="line x" title="185:262	Trigger word pairs are introduced and successfully applied in a language modeling task." ></td>
	<td class="line x" title="186:262	(Rosenfeld, 1994) demonstrated that the trigger word pairs improve the perplexity in ME-based language models." ></td>
	<td class="line x" title="187:262	Our method extends this idea to sequential supervised learning problems." ></td>
	<td class="line x" title="188:262	Our trigger selection criterion is based on the automatic feature inducing algorithm, and it allows us to generalize the arbitrary pairs of features." ></td>
	<td class="line x" title="189:262	Our method is based on two works of feature induction on an exponential model, (Pietra et al. , 1997) and (McCallum, 2003)." ></td>
	<td class="line x" title="190:262	Our induction algorithm builds on McCallums method which presents an efficient procedure to induce features on CRF." ></td>
	<td class="line x" title="191:262	(McCallum, 2003) suggested using only the mislabeled events rather than the whole training events." ></td>
	<td class="line x" title="192:262	This intuitional suggestion has offered us fast training." ></td>
	<td class="line x" title="193:262	We added two additional approximations to reduce the time cost; 1) an inducing procedure over a conditional non-structured inference problem rather than an approximated sequential inference problem, and 2) training with a local feature set, which is the basic information to identify the labels." ></td>
	<td class="line x" title="194:262	In this paper, our approach describes how to exploit non-local information to a SLU problem." ></td>
	<td class="line x" title="195:262	The trigger features are more robust than grammar-based features, and are easily extracted from the data itself by using an efficient selection algorithm." ></td>
	<td class="line x" title="196:262	418 Table 3: Comparison of variations in the induction algorithm (performed on one of the 5-fold validation sets); columns are induction and total training time (h:m:s), number of trigger and total features, and f-score on test data." ></td>
	<td class="line x" title="197:262	Inducer type Approx." ></td>
	<td class="line x" title="198:262	Induction/total time # triggers/features F1 (Text) F1 (ASR) CRF (empty) No approx." ></td>
	<td class="line x" title="199:262	3:55:01 / 5:27:13 682 / 2,693 90.23 67.60 CRF (local) Approx." ></td>
	<td class="line x" title="200:262	1 1:25:28 / 2:56:49 750 / 5,241 94.87 71.65 ME (empty) Approx." ></td>
	<td class="line x" title="201:262	2 20:57 / 1:54:22 618 / 2,080 94.85 71.46 ME (local) Approx." ></td>
	<td class="line x" title="202:262	1+2 6:30 / 1:36:14 608 / 5,099 95.17 71.81 6 Conclusion We have presented a method to exploit non-local information into a sequential supervised learning task." ></td>
	<td class="line x" title="203:262	In a real-world problem such as statistical SLU, our model performs significantly better than the traditional models which are based on syntactic parser-based features." ></td>
	<td class="line x" title="204:262	In comparing our selection criterion, we find that the mutual information tends to excessively select the triggers while our feature induction algorithm alleviates this issue." ></td>
	<td class="line x" title="205:262	Furthermore, the modified version of the algorithm is practically fast enough to maintain its performance particularly when the local features are offered by the starting position of the algorithm." ></td>
	<td class="line x" title="206:262	In this paper, we have focused on a sequential model such as a linear-chain CRF." ></td>
	<td class="line x" title="207:262	However, our method can also be naturally applied to arbitrary structured models, thus the first alternative is to combine our methods with a skip-chain CRF (Sutton and McCallum, 2004)." ></td>
	<td class="line x" title="208:262	Applying and extending our approach to other natural language tasks (which are difficult to apply a parser to) such as information extraction from e-mail data or biomedical named entity recognition is a topic of future work." ></td>
	<td class="line x" title="209:262	Acknowledgements We thank three anonymous reviewers for helpful comments." ></td>
	<td class="line x" title="210:262	This research was supported by the MIC (Ministry of Information and Communication), Korea, under the ITRC (Information Technology Research Center) support program supervised by the IITA (Institute of Information Technology Assessment)." ></td>
	<td class="line x" title="211:262	(IITA-2005-C1090-05010018) References J. R. Finkel, T. Grenager, and C. Manning." ></td>
	<td class="line x" title="212:262	2005." ></td>
	<td class="line x" title="213:262	Incorporating non-local information into information extraction systems by gibbs sampling." ></td>
	<td class="line x" title="214:262	In Proceedings of ACL05, pages 363370." ></td>
	<td class="line x" title="215:262	D. Gildea and D. Jurafsky." ></td>
	<td class="line x" title="216:262	2002." ></td>
	<td class="line x" title="217:262	Automatic labeling of semantic roles." ></td>
	<td class="line x" title="218:262	Computational Linguistics, 28(3):245288." ></td>
	<td class="line x" title="219:262	L. Gillick and S. Cox." ></td>
	<td class="line x" title="220:262	1989." ></td>
	<td class="line x" title="221:262	Some statistical issues in the comparison of speech recognition algorithms." ></td>
	<td class="line x" title="222:262	In Proceedings of ICASSP, pages 532535." ></td>
	<td class="line x" title="223:262	Y. He and S. Young." ></td>
	<td class="line x" title="224:262	2005." ></td>
	<td class="line x" title="225:262	Semantic processing using the hidden vector state model." ></td>
	<td class="line x" title="226:262	Computer Speech & Language, 19(1):85106." ></td>
	<td class="line x" title="227:262	J. Lafferty, A. McCallum, and F. Pereira." ></td>
	<td class="line x" title="228:262	2001." ></td>
	<td class="line x" title="229:262	Conditional random fields: Probabilistic models for segmenting and labeling sequence data." ></td>
	<td class="line x" title="230:262	In Proceedings of ICML, pages 282289." ></td>
	<td class="line x" title="231:262	A. McCallum." ></td>
	<td class="line x" title="232:262	2003." ></td>
	<td class="line x" title="233:262	Efficiently inducing features of conditional random fields." ></td>
	<td class="line x" title="234:262	In Proceedings of UAI, page 403." ></td>
	<td class="line x" title="235:262	B. L. Pellom, W. Ward, and S. S. Pradhan." ></td>
	<td class="line x" title="236:262	2000." ></td>
	<td class="line x" title="237:262	The cu communicator: An architecture for dialogue systems." ></td>
	<td class="line x" title="238:262	In Proceedings of ICSLP." ></td>
	<td class="line x" title="239:262	S. Della Pietra, V. J. Della Pietra, and J. Lafferty." ></td>
	<td class="line x" title="240:262	1997." ></td>
	<td class="line x" title="241:262	Inducing features of random fields." ></td>
	<td class="line x" title="242:262	IEEE Trans." ></td>
	<td class="line x" title="243:262	Pattern Anal." ></td>
	<td class="line x" title="244:262	Mach." ></td>
	<td class="line x" title="245:262	Intell, 19(4):380393." ></td>
	<td class="line x" title="246:262	L. A. Ramshaw and M. P. Marcus." ></td>
	<td class="line x" title="247:262	1995." ></td>
	<td class="line x" title="248:262	Text chunking using transformation-based learning." ></td>
	<td class="line x" title="249:262	In 3rd Workshop on Very Large Corpora, pages 8294." ></td>
	<td class="line x" title="250:262	R. Rosenfeld." ></td>
	<td class="line x" title="251:262	1994." ></td>
	<td class="line x" title="252:262	Adaptive statistical language modeling: A maximum entropy approach." ></td>
	<td class="line x" title="253:262	Technical report, School of Computer Science Carnegie Mellon University." ></td>
	<td class="line x" title="254:262	F. Sha and F. Pereira." ></td>
	<td class="line x" title="255:262	2003." ></td>
	<td class="line x" title="256:262	Shallow parsing with conditional random fields." ></td>
	<td class="line x" title="257:262	In Proceedings of HLT/NAACL03." ></td>
	<td class="line x" title="258:262	C. Sutton and A. McCallum." ></td>
	<td class="line x" title="259:262	2004." ></td>
	<td class="line x" title="260:262	Collective segmentation and labeling of distant entities in information extraction." ></td>
	<td class="line x" title="261:262	In ICML Workshop on Statistical Relational Learning." ></td>
	<td class="line x" title="262:262	419" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-1643
A Skip-Chain Conditional Random Field For Ranking Meeting Utterances By Importance
Galley, Michel;"></td>
	<td class="line x" title="1:186	Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 364372, Sydney, July 2006." ></td>
	<td class="line x" title="2:186	c2006 Association for Computational Linguistics A Skip-Chain Conditional Random Field for Ranking Meeting Utterances by Importance Michel Galley Columbia University Department of Computer Science New York, NY 10027, USA galley@cs.columbia.edu Abstract We describe a probabilistic approach to content selection for meeting summarization." ></td>
	<td class="line x" title="3:186	We use skipchain Conditional Random Fields (CRF) to model non-local pragmatic dependencies between paired utterances such as QUESTION-ANSWER that typically appear together in summaries, and show that these models outperform linear-chain CRFs and Bayesian models in the task." ></td>
	<td class="line x" title="4:186	We also discuss different approaches for ranking all utterances in a sequence using CRFs." ></td>
	<td class="line x" title="5:186	Our best performing system achieves 91.3% of human performance when evaluated with the Pyramid evaluation metric, which represents a 3.9% absolute increase compared to our most competitive non-sequential classifier." ></td>
	<td class="line x" title="6:186	1 Introduction Summarizationofmeetingsfacesmanychallenges not found in texts, i.e., high word error rates, absenceofpunctuation,andsometimeslackofgrammaticality and coherent ordering." ></td>
	<td class="line x" title="7:186	On the other hand, meetings present a rich source of structural and pragmatic information that makes summarization of multi-party speech quite unique." ></td>
	<td class="line x" title="8:186	In particular, our analyses of patterns in the verbal exchange between participants found that adjacency pairs (AP), a concept drawn from the conversational analysis literature (Schegloff and Sacks, 1973),haveparticularrelevancetosummarization." ></td>
	<td class="line x" title="9:186	APs are pairs of utterances such as QUESTIONANSWER or OFFER-ACCEPT, inwhichthesecond utteranceissaidtobeconditionallyrelevantonthe first." ></td>
	<td class="line x" title="10:186	We show that there is a strong correlation between the two elements of an AP in summarization, and that one is unlikely to be included if the other element is not present in the summary." ></td>
	<td class="line x" title="11:186	Most current statistical sequence models in natural language processing (NLP), such as hidden This material is based on research supported in part by the U.S. National Science Foundation (NSF) under Grants No." ></td>
	<td class="line x" title="12:186	IIS-0121396 and IIS-05-34871, and the Defense Advanced Research Projects Agency (DARPA) under Contract No." ></td>
	<td class="line x" title="13:186	HR0011-06-C-0023." ></td>
	<td class="line x" title="14:186	Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect the views of the NSF or DARPA." ></td>
	<td class="line x" title="15:186	Markov models (HMMs) (Rabiner, 1989), are linear chains that only encode local dependencies between utterances to be labeled." ></td>
	<td class="line x" title="16:186	In multi-party speech, the two elements of an AP are generally arbitrarily distant, and such models can only poorly account for dependencies underlying APs in summarization." ></td>
	<td class="line x" title="17:186	We use instead skip-chain sequence models (Sutton and McCallum, 2004), which allow us to explicitly model dependencies between distant utterances, and turn out to be particularly effective in the summarization task." ></td>
	<td class="line x" title="18:186	In this paper, we compare two types of network structureslinear-chain and skip-chainand two types of network semanticsBayesian Networks (BNs) and Conditional Random Fields (CRFs)." ></td>
	<td class="line x" title="19:186	We discuss the problem of estimating the class posterior probability of each utterance in a sequence in order to extract the N most probable ones, and show that the cost assigned by a CRF to each utterance needs to be locally normalized in order to outperform BNs." ></td>
	<td class="line x" title="20:186	After analyzing the predictive power of a large set of durational, acoustical, lexical, structural, and information retrieval features, we perform feature selection to have a competitive set of predictors to test the different models." ></td>
	<td class="line x" title="21:186	Empirical evaluations using two standard summarization metricsthe Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004)show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies, which achieves 91.3% of human performance in Pyramid score, and outperforms our best-performing non-sequential model by 3.9%." ></td>
	<td class="line x" title="22:186	2 Corpus The work presented here was applied to the ICSI Meeting Corpus (Janin et al. , 2003), a corpus of naturally-occurring meetings, i.e. meetings that would have taken place anyway." ></td>
	<td class="line x" title="23:186	Their style is quite informal, and topics are primarily concerned with speech, natural language, artificial 364 intelligence, and networking research." ></td>
	<td class="line x" title="24:186	The corpus contains 75 meetings, which are 60 minutes long on average, and involve a number of participants ranging from 3 to 10 (6 on average)." ></td>
	<td class="line x" title="25:186	The total number of unique speakers is 60, including 26 non-native English speakers." ></td>
	<td class="line x" title="26:186	Experiments in this paper are based either on human orthographic transcriptions or automatic speech recognition output, which were available for all meetings." ></td>
	<td class="line x" title="27:186	Forautomaticrecognition, weusedtheICSISRI-UW speech recognition system (Mirghafori et al. , 2004), a state-of-the-art conversational telephone speech (CTS) recognizer whose language and acoustic models were adapted to the meeting domain." ></td>
	<td class="line x" title="28:186	It achieves 34.8% WER on the ICSI corpus, which is indicative of the difficulty involved in processing meetings automatically." ></td>
	<td class="line x" title="29:186	We also used additional annotation that has been developed to support higher-level analyses of meeting structure, in particular the ICSI Meeting Recorder Dialog act (MRDA) corpus (Shriberg et al. , 2004)." ></td>
	<td class="line x" title="30:186	Dialog act (DA) labels describe the pragmatic function of utterances, e.g. a STATEMENT or a BACKCHANNEL." ></td>
	<td class="line x" title="31:186	This auxiliary corpus consists of over 180,000 human-annotated dialog act labels ( =.8), for which so-called adjacency pair (AP) relations (e.g. , APOLOGYDOWNPLAY) were also labeled." ></td>
	<td class="line x" title="32:186	This latter annotation was used to train an AP classifier that is instrumental in automatically determining the structure of our sequence models." ></td>
	<td class="line x" title="33:186	Note that, in the case of three or more speakers, adjacency pair is admittedly an unfortunate term, since labeled APs are generally not adjacent (e.g. , see Table 1), but we will nevertheless use the same terminology to enforce consistency with previous work." ></td>
	<td class="line x" title="34:186	To train and evaluate our summarizer, we used a corpus of extractive summaries produced at the University of Edinburgh (Murray et al. , 2005)." ></td>
	<td class="line x" title="35:186	For each of the 75 meetings, human judges were asked toselecttranscriptionutterancessegmentedbyDA to include in summaries, resulting in an average compression ratio of 6.26% (though no strict limit was imposed)." ></td>
	<td class="line x" title="36:186	Inter-labeler agreement was measured using six meetings that were summarized by multiple coders (average  = .323)." ></td>
	<td class="line x" title="37:186	While this level of agreement is quite low, this situation is not uncommon to summarization, since there may be many good summaries for a given document; a main challenge lies in using evaluation schemes that properly accounts for this diversity." ></td>
	<td class="line x" title="38:186	3 Content selection State sequence Markov models such as hidden Markov models (Rabiner, 1989) have been highly successful in many speech and natural language processing applications, including summarization." ></td>
	<td class="line x" title="39:186	Following an intuition that the probability of a given sentence may be locally conditioned on the previous one, Conroy (2004) built a HMM-based summarizer that consistently ranked among the top systems in recent Document Understanding Conference (DUC) evaluations." ></td>
	<td class="line x" title="40:186	Inter-sentential influences become more complex in the case of dialogues or correspondences, especially when they involve multiple parties." ></td>
	<td class="line x" title="41:186	In the case of summarization of conversational speech, Zechner (2002) found, for instance, that a simple technique consisting of linking together questions and answers in summariesand thus preventing the selection of orphan questions or answerssignificantly improved their readability according to various human summary evaluations." ></td>
	<td class="line x" title="42:186	In email summarization (Rambow et al. , 2004), ShresthaandMcKeown(2004)obtainedgoodperformance in automatic detection of questions and answers, which can help produce summaries that highlight or focus on the question and answer exchange." ></td>
	<td class="line x" title="43:186	In a combined chat and email summarization task, a technique (Zhou and Hovy, 2005) consisting of identifying APs and appending any relevant responses to topic initiating messages was instrumental in outperforming two competitive summarization baselines." ></td>
	<td class="line x" title="44:186	The need to model pragmatic influences, such asbetweenaquestionandananswer,isalsoprevalent in meeting summarization." ></td>
	<td class="line x" title="45:186	In fact, questionanswer pairs are not the only discourse relations that we need to preserve in order to create coherent summaries, and, as we will see, most instances of APs would need to be preserved together, either inside or outside the summary." ></td>
	<td class="line x" title="46:186	Table 1 displays an AP construction with one statement (A part) and three respondents (B parts)." ></td>
	<td class="line x" title="47:186	This example illustrates that the number of turns between constituents of APs is variable and thus difficult to model with standard sequence models." ></td>
	<td class="line x" title="48:186	This example also illustrates some of the predictors investigated in this paper." ></td>
	<td class="line x" title="49:186	First, many speakers respond to As utterance, which is generally a strong indicator that the A utterance should be included." ></td>
	<td class="line x" title="50:186	Secondly, while APs are generally characterized in terms of pre-defined dialog acts, such 365 Time Speaker AP Transcript 1480.85-1493.91 1 A are are those ddelays adjustable?" ></td>
	<td class="line x" title="51:186	see a lot of people who actually build stuff with human computer interfaces understand that delay, and and so when you by the time you click it itll be right on because itll go back in time to put the 1489.71-1489.94 2 yeah." ></td>
	<td class="line x" title="52:186	1493.95-1495.41 3 B yeah, uh, not in this case." ></td>
	<td class="line x" title="53:186	1494.31-1495.83 2 B it could do that, couldnt it." ></td>
	<td class="line x" title="54:186	1495.1-1497.07 4 B we could program that pretty easily, couldnt we?" ></td>
	<td class="line x" title="55:186	Table 1: Snippet of a meeting displaying an AP construction, where a question (A) initiates three responses (B)." ></td>
	<td class="line x" title="56:186	Sentences in italic are not present in the reference summary." ></td>
	<td class="line x" title="57:186	as OFFER-ACCEPT, we found that the type of dialog act has much less importance than the existence of the AP connection itself (APs in the data represent a great variety of DA pairs, including many that are not characterized as APs in the litteraturee.g. , STATEMENT-STATEMENT in the table)." ></td>
	<td class="line x" title="58:186	Since DAs seem to matter less than adjacency pairs, the aim will be to build techniques to automatically identify such relations and exploit them in utterance selection." ></td>
	<td class="line x" title="59:186	In the current work, we use skip-chain sequence models (Sutton and McCallum, 2004) to represent dependencies between both contiguous utterances and paired utterances appearing in the same AP constructions." ></td>
	<td class="line x" title="60:186	The graphical representations of skip-chain models, such as the CRF represented in Figure 1, are composed of two types of edges: linear-chain and skip-chain edges." ></td>
	<td class="line x" title="61:186	The latter edges model AP links, which we represent as a set of (s,d) index pairs (note that no more than one AP may share the same second element d)." ></td>
	<td class="line x" title="62:186	The intuition that the summarization labels (1 or 1) are highly correlated with APs is confirmed in Table 2." ></td>
	<td class="line x" title="63:186	While contiguous labels yt1 and yt seem to seldom influence each other, the correlation between AP elements ys and yd is particularly strong, and they have a tendency to be either both included or both excluded." ></td>
	<td class="line x" title="64:186	Note that the second table is not symmetric, because the data allows an A part to be linked to multiple B parts, but not vice-versa." ></td>
	<td class="line x" title="65:186	While counts in Table 2 reflect human labels, we only use automatically predicted (s,d) pairs in the experiments of the remaining part of this paper." ></td>
	<td class="line x" title="66:186	To find these pairs automatically, wetrainedanon-sequentiallog-linearmodel that achieves a .902 accuracy (Galley et al. , 2004)." ></td>
	<td class="line x" title="67:186	4 Skip-Chain Sequence Models In this paper, we investigate conditional models for paired sequences of observations and labels." ></td>
	<td class="line x" title="68:186	In the case of utterance selection, the observation sequence x = x1:T = (x1,,xT) represents local c53c74c61c74c65c6dc65c6ec74c78 c31 c78 c32 c78 c33 c78 c34 c78 c35 c42c61c63c6bc43c68c61c6ec6ec65c6c c53c74c61c74c65c6dc65c6ec74 c53c74c61c74c65c6dc65c6ec74 c53c74c61c74c65c6dc65c6ec74 c79 c31 c79 c32 c79 c33 c79 c34 c79 c35 Figure 1: A skip-chain CRF with pragmatic-level links." ></td>
	<td class="line x" title="69:186	Linear-chain edges yt = 1 yt = 1 yt1 = 1 529 7742 yt1 = 1 7742 116040 Skip-chain edges yd = 1 yd = 1 ys = 1 6792 2191 ys = 1 1479 121591 Table 2: Contingency tables: while the correlation between adjacent labels yt1 and yt is not significant (2 = 2.3, p > .05), empirical evidence clearly shows that ys and yd influence each other (2 = 78948, p < .001)." ></td>
	<td class="line x" title="70:186	summarization predictors (see Section 6), and the binary sequence y = y1:T = (y1,,yT) (where yt  {1,1}) determines which utterances must be included in the summary." ></td>
	<td class="line x" title="71:186	In a discriminative framework, we concentrate our modeling effort on estimating p(y|x) from data, and do not explicitly model the prior probability p(x), since x is fixed during testing anyway." ></td>
	<td class="line x" title="72:186	Many probabilistic approaches to modeling sequences have relied on directed graphical models, also known as Bayesian networks (BN),1 in particular hidden Markov models (Rabiner, 1989) and conditional Markov models (McCallum et al. , 2000)." ></td>
	<td class="line x" title="73:186	However, prominent recent approaches have focused on undirected graphical models, in particular conditional random fields (CRF) (Lafferty et al. , 2001), and provided state-of-the-art performance in many NLP tasks." ></td>
	<td class="line x" title="74:186	In our work, we will provide empirical results for state sequence models of both semantics, and we will now de1Intheexistingliterature,sequencemodelsthatsatisfythe Markovian conditioni.e. , the state of the system at time t depend only on its immediate past tk:t1 (typically just t1)are generally termed dynamic Bayesian networks (DBN)." ></td>
	<td class="line x" title="75:186	Since the particular models under investigation, i.e. skip-chain models, do not have this property, we will simply refer to them as Bayesian networks." ></td>
	<td class="line x" title="76:186	366 scribe skip-chain models for both BNs and CRFs." ></td>
	<td class="line x" title="77:186	In a BN, the probability of the sequence y factorizesasaproductofprobabilitiesoflocalpredictions yt conditioned on their parents pi(yt) (Equation1)." ></td>
	<td class="line x" title="78:186	InaCRF,theprobabilityofthesequencey factorizes according to a set of clique potentials {c}cC, where C is represents the cliques of the underlying graphical model (Equation 2)." ></td>
	<td class="line x" title="79:186	pBN(y|x) = Tproductdisplay i=1 pBN(yt|x,pi(yt)) (1) pCRF(y|x)  productdisplay cC c(xc,yc) (2) We parameterize these BNs and CRFs as loglinear models, and factorize both BNs local prediction probabilities and CRFs clique potentials using two types of feature functions." ></td>
	<td class="line x" title="80:186	Linear-chain feature functions fj(ytk:t,x,t) represent local dependencies that are consistent with an order-k Markov assumption." ></td>
	<td class="line x" title="81:186	For instance, one such function could be a predicate that is true if and only if yt1 = 1, yt = 1, and (xt1,xt) indicates that both utterances are produced by the same speaker." ></td>
	<td class="line x" title="82:186	Given a set of skip edges S = {(st,t)} specifying source and destination indices, skip-chain feature functions gj(yst,yt,x,st,t) exploit dependencies between variables that are arbitrarily distant in the chain." ></td>
	<td class="line x" title="83:186	For instance, the finding that OFFERREJECT pairs are often linked in summaries might be encoded as a skip-chain feature predicate that is true if and only if yst = 1, yt = 1, and the first word of the t-th utterance is no." ></td>
	<td class="line x" title="84:186	Log-linear models for skip-chain sequence models are defined in terms of weights {k} and {k}, one for each feature function." ></td>
	<td class="line x" title="85:186	In the case of BNs, we write: logpBN(yt|x,pi(yt))  Jsummationdisplay j=1 jfj(x,ytk:t,t) + Jprimesummationdisplay j=1 jgj(x,yst,yt,st,t) We can reduce a particular skip-chain CRF to represent only the set of cliques along (yt1,yt) adjacency edges and (yst,yt) skip edges, resulting in only two potential functions: logLIN(x,ytk:t,t) = Jsummationdisplay j=1 jfj(x,ytk:t,t) logSKIP(x,yst,yt,t) = Jprimesummationdisplay j=1 jgj(x,yst,yt,st,t) 4.1 Inference and Parameter Estimation Our CRF and BN models were designed using MALLET (McCallum, 2002), which provides tools for training log-linear models with L-BFGS optimization techniques and maximize the loglikelihood of our training dataD = (x(i),y(i))Ni=1, andprovidesprobabilisticinferencealgorithmsfor linear-chain BNs and CRFs." ></td>
	<td class="line oc" title="86:186	Most previous work with CRFs containing nonlocal dependencies used approximate probabilistic inference techniques, including TRP (Sutton and McCallum, 2004) and Gibbs sampling (Finkel et al. , 2005)." ></td>
	<td class="line x" title="87:186	Approximation is needed when the junction tree of a graphical model is associated with prohibitively large cliques." ></td>
	<td class="line x" title="88:186	For example, the worse case reported in (Sutton and McCallum, 2004) is a clique of 61 nodes." ></td>
	<td class="line x" title="89:186	In the case of skip-chain models representing APs, the inference problem is somewhat simpler: loops in the graph are relatively short, 98% of AP edges span no more than 5 time slices, and the maximum clique size in the entire data is 5." ></td>
	<td class="line x" title="90:186	While exact inference might be possible in our case, we used the simpler approach of adapting standard inference algorithms for linear-chain models." ></td>
	<td class="line x" title="91:186	Specifically, to account for skip-edges, we used a technique inspired by (Sha and Pereira, 2003), in which multiple state dependencies, such as an order-2 Markov model, are encoded using auxiliary tags." ></td>
	<td class="line x" title="92:186	For instance, an order-2 Markov model isparameterizedusingstatetriplesyt2:t,andeach possible triple is converted to a label zt = yt2:t. Using these auxiliary labels only, we can then use the standard forward-backward algorithm for computing marginal distributions in linear-chain CRFs, and Viterbi decoding in linear-chain CRFs and BNs." ></td>
	<td class="line x" title="93:186	The only requirement is to ensure that a transition between zt and zt+1 is forbidden if the sub-states yt1:t common to both states differ, i.e., is assigned an infinite cost." ></td>
	<td class="line x" title="94:186	This approach can be extended to the case of skip-chain transitions." ></td>
	<td class="line x" title="95:186	For instance, an order-1 Markov model with skipedgescanbeconstructedusingzt = (yst,yt1,yt) triples, where the first element yst represents the label at the source of the skip-edge." ></td>
	<td class="line x" title="96:186	Similarly to the case of order-2 Markov models, we need to ensure that only valid sequences of labels are considered, which is trivial to enforce if we assume that no skip edge ranges more than a predefined threshold of k time slices." ></td>
	<td class="line x" title="97:186	Whilethisapproachisnotexact, itstillprovides 367 competitive performance as we will see in Section 8." ></td>
	<td class="line x" title="98:186	In future work, we plan to explore more accurate probabilistic inference techniques." ></td>
	<td class="line x" title="99:186	5 Ranking Utterances by Importance As we will see in Section 8, using the actual {1,1} label predictions of our BNs and CRFs leads to significantly sub-optimal results, which mightbeexplainedbythefollowingreasons." ></td>
	<td class="line x" title="100:186	First, our models are optimized to maximize the conditional log-likelihood of the training data, a measure that does not correlate well with utility measures generally used in retrieval oriented tasks such as summarization, especially when faced with a significant class imbalance (only 6.26% of reference instances are positive)." ></td>
	<td class="line x" title="101:186	Second, the MAP decision rule doesnt give us the freedom to select an arbitrary number of sentences in order to satisfy any constraint on length." ></td>
	<td class="line x" title="102:186	Instead of using actual predictions, it seems more reasonable to compute the posterior probability of each local prediction yt, and extract the N most probable summary sentences (yr1,,yrk), where N may depend on a length expressed in number of words, as it is the case in our evaluation in Section 7." ></td>
	<td class="line x" title="103:186	BNs assign probability distributions over entire sequencesbyestimatingtheprobabilityofeachindividual instance yt in the sequence (Equation 1), and seem thus particularly suited for ranking utterances." ></td>
	<td class="line x" title="104:186	A first approach is then to rank utterances according to the cost of predicting yt = 1 at each time step on the Viterbi path." ></td>
	<td class="line x" title="105:186	While these costs are well-formed (negative log) probabilities in the case of BNs, they cannot be interpreted as such in the case of CRFs, and turn out to produce poor results with CRFs." ></td>
	<td class="line x" title="106:186	Indeed, the set of CRF potentials associated with each time step have no immediate probabilistic interpretation, and cannot be used directly to rank sentences." ></td>
	<td class="line x" title="107:186	Since BNs and CRFs are here parameterized as log-linear models and rely on the same set of feature functions, a second approach is to use CRF-trained model parameters to build a BN classifier that assigns a probability to each yt." ></td>
	<td class="line x" title="108:186	Specifically, the CRF model is first used to generate label predicitons y, from which the locally-normalized model estimates the cost of predicting yt = 1 given a label history y1:t1." ></td>
	<td class="line x" title="109:186	This ensures that we have a well-formed probability distribution at each time slice, while capitalizing on the good performance of CRF models." ></td>
	<td class="line x" title="110:186	Lexical features:  n-grams (n  3)  number of words  number of digits  number of consecutive repeats Information retrieval features:  max/sum/mean frequency of all terms in ut  max/sum/mean idf score  max/sum/mean tfidf score  cosine similarity between word vector of ut with centroid of of the meeting  scores of LSA with 5, 10, 50, 100, 200, 300 concepts Acoustic features:  seconds of silence before/during/after the turn  speech rate  min/max/mean/median/stddev/onset/outset f0 of utterance t, and of first and last word  min/max/mean/stddev energy  .05, .25, .5, .75, .95 quantiles of f0 and energy  pitch range  f0 mean absolute slope Durational and structural features:  duration of the previous/current/next utterance  relative position within meeting (i.e. , index t)  relative position within speaker turn  large number of structural predicates, i.e. is the previous utterance of the same speaker?  number of APs initiated in yt Discourse features:  lexical cohesion score (for topic shifts) (Hearst, 1994)  first and second word of utterance, if in cue word list  number of pronouns  numberoffillersandfluencydevices(e.g. , uh, um)  number of backchannel and acknowledgment tokens (e.g. , uh-huh, ok, right) Table 3: Features for extractive summarization." ></td>
	<td class="line x" title="111:186	Unless otherwise mentioned, we refer to features of utterance t whose label yt we are trying to predict." ></td>
	<td class="line x" title="112:186	6 Features for extractive summarization We started our analyses with a large collection of features found to be good predictors in either speech (Inoue et al. , 2004; Maskey and Hirschberg, 2005; Murray et al. , 2005) or text summarization (Mani and Maybury, 1999)." ></td>
	<td class="line x" title="113:186	Our goal is to build a very competitive feature set that capitalizesonrecentadvancesinsummarizationof both genres." ></td>
	<td class="line x" title="114:186	Table 3 lists some important features." ></td>
	<td class="line x" title="115:186	There is strong evidence that lexical cues such as significant and great are strong predictors in many summarization tasks (Edmundson, 1968)." ></td>
	<td class="line x" title="116:186	Such cues are admittedly quite genre specific, so we did not want to commit ourselves to any specific list, which may not carry over well to our specific speech domain, and we automatically selected a list of n-grams (n  3) using crossvalidation on the training data." ></td>
	<td class="line x" title="117:186	More specifically, we computed the mutual information of each n368 c54c72c61c6e c73c63c72c69c70c74c3a c49c20c74c68c69c6ec6bc20c2d c6fc6ec65c20c74c68c69c6ec67c20c74 c68c61c74c20c6dc61c6bc65c73 c20c61c20c64c69c66c66c65c72c65c6ec63c65c20c69c73c20c74c68c69c73c20c44c43c20c6fc66c66c73c65c74c20c63c6fc6dc70c65c6ec73c61c74c69c6fc6ec2e c31c2dc31c33 c44c69c64c20c79c6fc75c20c68c61c76c65c20c61c20c6cc6fc6fc6b c20c61c74c20c6dc65c65c74c69 c6ec67c20c64c69c67c69c74c73c20c69c66c20c74 c68c65c79c20c68c61c76c65c20c61c20c74c68c65c6dc3f c31c34c2dc32c36 c49c20c64c69c64c6ec27c74c2ec20c4ec6fc2e c32c37c2dc32c39 c48c6dc6dc2e c33c30 c4ec6fc2ec20c54c68c65c20c44 c43c20c63c6fc6dc70c6fc6ec65c6ec74c20c69c73c20c6ec65c67c6c c69c67c69c62c6cc65c2ec20c41c6cc6cc20c6dc69c6bc65c73c20c68c61c76 c65c20c44c43c20c72c65c6dc6fc76c61c6cc2e c33c31c2dc34c31 c59c65c61c68c2e c34c32 c42c65c63c61c75c73c65c20c74 c68c65c72c65c27c73c20c61c20c73 c61c6dc70c6cc65c20c61c6ec64c20c68c6fc6cc64c20c69c6ec20c74 c68c65c20c41c2dc74c6fc2dc44c2e c34c33c2dc35c31 c41c6ec64c20c49c20c61c6cc73c6fc2cc20c75c6dc2cc20c64c69c64c20c73 c6fc6dc65c20c65c78c70c65c72c69c6dc65c6ec74c73c20c61c62c6fc75c74c20c6ec6fc72c6dc61c6cc69c7ac69c6ec67c20c74c68c65c20c70c68c61c73c65c2e c35c32c2dc36c32 c41c6ec64c20c63c61c6dc65c20c75c70c20c77c69c74c68c20c61c20c77c65c62c20c70c61c67c65c20c70c65c6fc70c6c c65c20c63c61c6ec20c74c61c6bc65c20c61c20c6cc6fc6fc6bc20c61c74c2e c36c33c2dc37c35 c4dc6fc64c65 c6cc20c31c20c28c6cc65 c6ec3dc32c30c29c3a c33c31c2dc34c31 c34c33c2dc35c31 c4dc6fc64c65 c6cc20c32c20c28c6cc65 c6ec3dc32c32c29c3a c33c31c2dc34c31 c35c32c2dc36c32 c4dc6fc64c65 c6cc20c33c20c28c6cc65 c6ec3dc32c34c29c3a c35c32c2dc36c32 c36c33c2dc37c35 c50c65c65c72c20c28c6cc65c6ec3dc32 c32c29c3a c31c2dc31c33 c34c33c2dc35c31 c4fc70c74c69c6dc61c6cc20c28c6cc65c6ec3dc32c32c29c3a c33c31c2dc34c31 c35c32c2dc36c32 c31 c31 c32 c33 c34 c33 c33 c32 c32 c53c70c65c61c6bc65c72c3a Figure 2: Model, peer, and optimal summaries are all extracts taken from the same transcription." ></td>
	<td class="line x" title="118:186	gram with the class variable, and selected for each n the 200 best scoring n-grams." ></td>
	<td class="line x" title="119:186	Other lexical features include: the number of digits, which is helpful for identifying sections of the meetings where participants collect data by recording digits; the number of repeats, which may indicate the kind of hesitations and disfluencies that negatively correlates with what is included in the summary." ></td>
	<td class="line x" title="120:186	The information retrieval feature set contains many features that are generally found helpful in summarization, in particular tfidf and scores derived from centroid methods." ></td>
	<td class="line x" title="121:186	In particular, we used the latent semantic analysis (LSA) feature discussed in (Murray et al. , 2005), which attempts to determine sentence importance through singular value decomposition, and whose resulting singular values and singular vectors can be exploited toassociateeachutteranceadegreeofrelevanceto oneofthetop-nconceptsofthemeetings(wheren represents the number of dimensions in the LSA)." ></td>
	<td class="line x" title="122:186	We used the same scoring mechanism as (Murray et al. , 2005), though we extracted features for many different n values." ></td>
	<td class="line x" title="123:186	Acoustic features extracted with Praat (Boersma and Weenink, 2006) were normalized by channel and speaker, including many raw features such as f0 and energy." ></td>
	<td class="line x" title="124:186	Structural features listed in the table are those computed from the sequence model before decoding, e.g., the duration that separates the two elements of an AP." ></td>
	<td class="line x" title="125:186	Finally, discourse features represent predictors that may substitute to DA labels." ></td>
	<td class="line x" title="126:186	While DA tagging is not directly our concern, it is presumably helpful to capitalize on discourse characteristics of utterances involved in adjacency pairs, since different types of dialog acts may be unequally likely to appear in a summary." ></td>
	<td class="line x" title="127:186	7 Evaluation Evaluating summarization is a difficult problem and there is no broad consensus on how to best perform this task." ></td>
	<td class="line x" title="128:186	Two metrics have become quite popular in multi-document summarization, namely the Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004)." ></td>
	<td class="line x" title="129:186	Pyramid and ROUGE are techniques looking for content units repeated in different model summaries, i.e.,summarycontentunits(SCUs)suchasclauses and noun phrases for the Pyramid method, and ngrams for ROUGE." ></td>
	<td class="line x" title="130:186	The underlying hypothesis is that different model sentences, clauses, or phrases may convey the same meaning, which is a reasonableassumptionwhendealingwithreferencesummaries produced by different authors, since it is quite unlikely that any two abstractors would use the exact same words to convey the same idea." ></td>
	<td class="line x" title="131:186	Our situation is however quite different, since all model summaries of a given document are utterance extracts of that same document, as this can been seen in the excerpt of Figure 2." ></td>
	<td class="line x" title="132:186	In our own annotation of three meetings with SCUs defined as in (Nenkova and Passonneau, 2004a), we found that repetitions and reformulation of the same information are particularly infrequent, and that textual units that express the same content among model summaries are generally originating from the same document sentence (e.g. , in the figure, the first sentence in model 1 and 2 emanate from the same document sentence)." ></td>
	<td class="line x" title="133:186	Very short SCUs (e.g. , base noun phrases) sometimes appeared in different locations of a meeting, but we think it is problematic to assume that connections between such short units are indicative of any similarity of sentential meaning: the contexts are different, and words may be uttered by different speakers, which may lead to unrelated or conflicting pragmatic forces." ></td>
	<td class="line x" title="134:186	For instance, an SCU realized as DC offset and DC component appears in two different sentences in the figure, i.e. those identified as 1-13 and 31-41." ></td>
	<td class="line x" title="135:186	However, the two sentences have contradictory meanings, and it would be unfortunate to increase the score of a peer summary containing the former sentence because the 369 latter is included in some model summaries." ></td>
	<td class="line x" title="136:186	For all these reasons, we believe that summarization evaluation in our case should rely on the following restrictive matching: two summary units should be considered equivalent if and only if they are extracted from the same location in the original document (e.g. , the DC appearing in models 1 and 2 is not the same as the DC in the peer summary, since they are extracted from different sentences)." ></td>
	<td class="line x" title="137:186	This constraint on the matching is reflected in our Pyramid evaluation, and we define an SCU as a word and its document position, which lets us distinguish (DC,11) from (DC,33)." ></td>
	<td class="line x" title="138:186	While this restriction on SCUs forces us to disregard scarcely occurring paraphrases and repetitions of the same information, it provides the benefit of automated evaluation." ></td>
	<td class="line x" title="139:186	Once all SCUs have been identified, the Pyramid method is applied as in (Nenkova and Passonneau, 2004b): wecomputeascoreD byaddingfor each SCU present in the summary a score equal to the number of model summaries in which that SCU appears." ></td>
	<td class="line x" title="140:186	The Pyramid score P is computed by dividing D by the maximum D value that is obtainable given the constraint on length." ></td>
	<td class="line x" title="141:186	For instance, the peer summary in the figure gets a score D = 9 (since the 9 SCUs in range 43-51 occur in one model), and the maximum obtainable score is D = 44 (all SCUs of the optimal summary appear in exactly two model summaries), hence the peer summarys score is P = .204." ></td>
	<td class="line x" title="142:186	While our evaluation scheme is similar to comparing the binary predictions of model and peer summarieseach prediction determining whether a given transcription word is included or not andaveragingprecisionscoresoverallpeer-model pairs, the Pyramid evaluation differs on an important point, which makes us prefer the Pyramid evaluation method: the maximum possible Pyramid score is always guaranteed to be 1, but average precision scores can become arbitrarily low as the consensus between summary annotators decreases." ></td>
	<td class="line x" title="143:186	For instance, the average precision score of the optimal summary in the figure is PR = 23.2 2Precision scores of the optimal summary compared against the the three model summaries are .5, 1, and .5, respectively, and hence average 23." ></td>
	<td class="line x" title="144:186	We can show that P = PR/PR, where PR is the average precision of the optimal summary." ></td>
	<td class="line x" title="145:186	Lack of space prevent us from providing a proof, so we will just show that the equality holds in our example: since the peer summarys precision scores against the three model summaries are respectively 922, 0, and 0, we have PR/PR = ( 966)/(23) = 944 = P. FEATURE F=1 1 utterance duration .246 2 100-dimension LSA .268 3 duration of utterance t1 .275 4 time between utterances s and d = t .281 5 IDF mean .284 6 meeting position .286 7 number of APs initiated in t .288 8 duration of utterance t + 1 .288 9 number of fillers .289 10 .25-quantile of energy .290 11 number of lexical repeats .292 12 lexical cohesion score .294 13 f0 mean of last word of utterance t .294 14 LSA 50 dimensions .295 15 utterances (t,t + 1) by same speaker .298 16 speech rate .302 17 is that .303 18 for the .303 19 (ut1,ut) by same speaker .305 20 to try .305 21 meetings .305 22 utterance starts with and .306 23 we have .306 24 new .307 25 utterance starts with what .307 Table 4: Forward feature selection." ></td>
	<td class="line x" title="146:186	In the case of the six test meetings, which all have either 3 or 4 model summaries, the maximum possible average precision is .6405." ></td>
	<td class="line x" title="147:186	8 Experiments We follow (Murray et al. , 2005) in using the same six meetings as test data, since each of these meetings has multiple reference summaries." ></td>
	<td class="line x" title="148:186	The remaining69meetingswereusedfortraining,which represent in total more than 103,000 training instances (or DA units), of which 6,464 are positives (6.24%)." ></td>
	<td class="line x" title="149:186	The multi-reference test set contains more than 28,000 instances." ></td>
	<td class="line x" title="150:186	The goal of a preliminary experiment was to devise a set of useful predictors from a full set of 1171." ></td>
	<td class="line x" title="151:186	We performed feature selection by incrementally growing a log-linear model with order0 features f(x,yt) using a forward feature selection procedure similar to (Berger et al. , 1996)." ></td>
	<td class="line x" title="152:186	Probably due to the imbalance between positive and negative samples, we found it more effective to rank candidate features by gains in F-measure (through5-foldcrossvalidationontheentiretrainingset)." ></td>
	<td class="line x" title="153:186	TheincreaseinF1 byaddingnewfeatures to the model is displayed in Table 4; this greedy search resulted in a set S of 217 features." ></td>
	<td class="line x" title="154:186	We now analyze the performance of different sequence models on our test set." ></td>
	<td class="line x" title="155:186	The target length of each summary was set to 12.7% of the number of words of the full document, which is the aver370 age on the entire training data (the average on the test data is 12.9%)." ></td>
	<td class="line x" title="156:186	In Table 5, we use an order-0 CRF to compare S against all features and various categorical groupings." ></td>
	<td class="line x" title="157:186	Overall, we notice lexical predictors and statistics derived from them (e.g. LSA features) represent the most helpful feature group (.497), though all other features combined achieve a competitive performance (.476)." ></td>
	<td class="line x" title="158:186	Table 6 displays performance for sequence models incorporating linear-chain features of increasing order k. Its second column indicates what criterion was used to rank utterances." ></td>
	<td class="line x" title="159:186	In the case of pred, we used actual model {1,1} predictions, which in all cases generated summaries much shorted than the allowable length, and produced poor performance." ></td>
	<td class="line x" title="160:186	Costs and norm-CRF refer to the two ranking criteria presented in Section 5, and it is clear that the performance of CRFs degrades with increasing orders without local normalization." ></td>
	<td class="line x" title="161:186	While the contingency counts in Table 2 only hinted a limited benefit of linear-chain features, empirical results show the contrary especially for order k = 2." ></td>
	<td class="line x" title="162:186	However, the further increase of k causes overfitting, and skip-chain features seem a better way to capture non-local dependencies while keeping the number of model parameters relatively small." ></td>
	<td class="line x" title="163:186	Overall, the addition of skip-chain edges to linear-chain models provide noticeable improvement in Pyramid scores." ></td>
	<td class="line x" title="164:186	Our system that performed best on cross-validation data is an order-2 CRF with skip-chain transitions, which achieves a Pyramid score of P = .554." ></td>
	<td class="line x" title="165:186	We now assess the significance of our results by comparing our best system against: (1) a lead summarizer that always selects the first N utterances to match the predefined length; (2) human performance, which is obtained by leave-one-out comparisons among references (Table 7); (3) optimal summaries generated using the procedure explained in (Nenkova and Passonneau, 2004b) by ranking document utterances by the number of model summaries in which they appear." ></td>
	<td class="line x" title="166:186	It appears that our system is considerably better than the baseline, and achieves 91.3% of human performance in terms of Pyramid scores, and 83% if using ASR transcription." ></td>
	<td class="line x" title="167:186	This last result is particularly positive if we consider our strong reliance on lexical features." ></td>
	<td class="line x" title="168:186	For completeness, we also included standard ROUGE (1, 2, and L) scores in Table 7, which were obtained using parameters defined for the FEATURE SET P lexical .471 IR .415 lexical + IR .497 acoustic .407 structural/durational .478 acoustic + structural/durational .476 all features .507 selected features (S) .515 Table 5: Pyramid score for each feature set." ></td>
	<td class="line x" title="169:186	MODEL RANKING k = 1 2 3 linear-chain BN pred .241 .267 .269 linear-chain BN costs .512 .519 .525 skip-chain BN costs .543 .549 .542 linear-chain CRF pred .326 .36 .348 linear-chain CRF costs .508 .475 .447 linear-chain CRF norm-CRF .53 .548 .54 skip-chain CRF norm-CRF .541 .554 .559 Table6: Pyramidscoresfordifferentsequencemodels,where k stands for the order of linear-chain features." ></td>
	<td class="line x" title="170:186	The value in bold is the performance of the model that was selected after a 5-fold cross validation on the training data, which obtained the highest F1 score." ></td>
	<td class="line x" title="171:186	SUMMARIZER P R-1 R-2 R-L baseline .188 .501 .210 .495 skip-chain CRF (transcript) .554 .715 .442 .709 skip-chain CRF (ASR) .504 .714 .42 .706 human .607 .720 .477 .715 optimal 1 .791 .648 .788 Table 7: Pyramid, and average ROUGE scores for summaries produces by a baseline (lead summarizer), our best system, humans, and the optimal summarizer." ></td>
	<td class="line x" title="172:186	DUC-05 evaluation." ></td>
	<td class="line x" title="173:186	Since system summaries have on average approximately the same length as references, we only report recall measures of ROUGE (precision and F averages are within  .002).3 It may come as a surprise that our best system (both with ASR and true words) performs almost as well as humans; it seems more reasonable to conclude that, in our case, ROUGE has trouble discriminating between systems with moderately close performance." ></td>
	<td class="line x" title="174:186	This seems to confirm our impression that content evaluation in our task should be based on exact matches." ></td>
	<td class="line x" title="175:186	We performed a last experiment to compare our bestsystemagainstMurrayetal.(2005), whoused the same test data, but constrained summary sizes in terms of number of DA units instead of words." ></td>
	<td class="line x" title="176:186	In their experiments, 10% of DAs had to be selected." ></td>
	<td class="line x" title="177:186	Our system achieves .91 recall, .5 precision, and .64 F1 with the same length constraint." ></td>
	<td class="line x" title="178:186	3Human performance with ROUGE was assessed by cross-validating reference summaries of each meeting (i.e. , n references for a given meeting resulted in n evaluations against the other references)." ></td>
	<td class="line x" title="179:186	We used the same leave-oneout procedure with other summarizers, in order to get results comparable to humans." ></td>
	<td class="line x" title="180:186	371 The discrepancy between recall and precision is largely due to the fact that generated summaries areonaveragemuchlongerthanmodelsummaries (10% vs. 6.26% of DAs), which explains why our precision is relatively low in this last evaluation." ></td>
	<td class="line x" title="181:186	The best ROUGE-1 measure reported in (Murray et al. , 2005) is .69 recall, which is significantly lower than ours according to confidence intervals." ></td>
	<td class="line x" title="182:186	9 Conclusion An order-2 CRF with skip-chain dependencies derived from the automatic analysis of participant interaction was shown to outperform linear-chain BNs and CRFs, despite the incorporation in all cases of the same competitive set of predictors resulting from cross-validated feature selection." ></td>
	<td class="line x" title="183:186	Compared to an order-0 CRF model, the absolute increase in performance is 3.9% (7.5% relative increase), which indicates that it is helpful to use skip-chain sequence models in the summarization task." ></td>
	<td class="line x" title="184:186	Our best performing system reaches 91.3% of human performance, and scales relatively well on automatic speech recognition output." ></td>
	<td class="line x" title="185:186	Acknowledgments This work has benefited greatly from suggestions andadvicefromKathleenMcKeown." ></td>
	<td class="line x" title="186:186	Ialsowould like to thank Jean Carletta, Steve Renals and Gabriel Murray for giving me access to their summarization corpus, Ani Nenkova for helpful discussionsaboutsummarizationevaluation, Michael Collins, Daniel Ellis, Julia Hirschberg, and Owen Rambow for useful preliminary discussions, and three anonymous reviewers for their insightful comments on an earlier version of this paper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-1655
A Hybrid Markov/Semi-Markov Conditional Random Field For Sequence Segmentation
Andrew, Galen;"></td>
	<td class="line x" title="1:152	Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 465472, Sydney, July 2006." ></td>
	<td class="line x" title="2:152	c2006 Association for Computational Linguistics A Hybrid Markov/Semi-Markov Conditional Random Field for Sequence Segmentation Galen Andrew Microsoft Research One Microsoft Way Redmond, WA 98052 galena@microsoft.com Abstract Markov order-1 conditional random fields (CRFs) and semi-Markov CRFs are two popular models for sequence segmentation and labeling." ></td>
	<td class="line x" title="3:152	Both models have advantages in terms of the type of features they most naturally represent." ></td>
	<td class="line x" title="4:152	We propose a hybrid model that is capable of representing both types of features, and describe efficient algorithms for its training and inference." ></td>
	<td class="line x" title="5:152	We demonstrate that our hybrid model achieves error reductions of 18% and 25% over a standard order-1 CRF and a semi-Markov CRF (resp)." ></td>
	<td class="line x" title="6:152	on the task of Chinese word segmentation." ></td>
	<td class="line x" title="7:152	We also propose the use of a powerful feature for the semi-Markov CRF: the log conditional odds that a given token sequence constitutes a chunk according to a generative model, which reduces error by an additional 13%." ></td>
	<td class="line x" title="8:152	Our best system achieves 96.8% F-measure, the highest reported score on this test set." ></td>
	<td class="line x" title="9:152	1 Introduction The problem of segmenting sequence data into chunks arises in many natural language applications, such as named-entity recognition, shallow parsing, and word segmentation in East Asian languages." ></td>
	<td class="line x" title="10:152	Two popular discriminative models that have been proposed for these tasks are the conditional random field (CRFs) (Lafferty et al. , 2001) and the semi-Markov conditional random field (semi-CRF) (Sarawagi and Cohen, 2004)." ></td>
	<td class="line x" title="11:152	A CRF in its basic form is a model for labeling tokens in a sequence; however it can easily be adapted to perform segmentation via labeling each token as BEGIN or CONTINUATION, or according to some similar scheme." ></td>
	<td class="line x" title="12:152	CRFs using this technique have been shown to be very successful at the task of Chinese word segmentation (CWS), starting with the model of Peng et al.(2004)." ></td>
	<td class="line x" title="14:152	In the Second International Chinese Word Segmentation Bakeoff (Emerson, 2005), two of the highest scoring systems in the closed track competition were based on a CRF model." ></td>
	<td class="line x" title="15:152	(Tseng et al. , 2005; Asahara et al. , 2005) While the CRF is quite effective compared with other models designed for CWS, one wonders whether it may be limited by its restrictive independence assumptions on non-adjacent labels: an order-M CRF satisfies the order-M Markov assumption that, globally conditioned on the input sequence, each label is independent of all other labels given the M labels to its left and right." ></td>
	<td class="line x" title="16:152	Consequently, the model only sees word boundaries within a moving window of M + 1 characters, which prohibits it from explicitly modeling the tendency of strings longer than that window to form words, or from modeling the lengths of the words." ></td>
	<td class="line x" title="17:152	Although the window can in principle be widened by increasing M, this is not a practical solution as the complexity of training and decoding a linear sequence CRF grows exponentially with the Markov order." ></td>
	<td class="line x" title="18:152	The semi-CRF is a sequence model that is designed to address this difficulty via careful relaxation of the Markov assumption." ></td>
	<td class="line x" title="19:152	Rather than recasting the segmentation problem as a labeling problem, the semi-CRF directly models the distribution of chunk boundaries.1 In terms of inde1As it was originally described, the semi-CRF also assigns labels to each chunk, effectively performing joint segmentation and labeling, but in a pure segmentation problem such as CWS, the use of labels is unnecessary." ></td>
	<td class="line x" title="20:152	465 pendence, using an order-M semi-CRF entails the assumption that, globally conditioned on the input sequence, the position of each chunk boundary is independent of all other boundaries given the positions of the M boundaries to its left and right regardless of how far away they are." ></td>
	<td class="line x" title="21:152	Even with an order-1 model, this enables several classes of features that one would expect to be of great utility to the word segmentation task, in particular word length and word identity." ></td>
	<td class="line x" title="22:152	Despite this, the only work of which we are aware exploring the use of a semi-Markov CRF for Chinese word segmentation did not find significant gains over the standard CRF (Liang, 2005)." ></td>
	<td class="line x" title="23:152	This is surprising, not only because the additional features a semi-CRF enables are intuitively very useful, but because as we will show, an order-M semi-CRF is strictly more powerful than an order-M CRF, in the sense that any feature that can be used in the latter can also be used in the former, or equivalently, the semi-CRF makes strictly weaker independence assumptions." ></td>
	<td class="line x" title="24:152	Given a judicious choice of features (or simply enough training data) the semi-CRF should be superior." ></td>
	<td class="line x" title="25:152	We propose that the reason for this discrepancy may be that despite the greater representational power of the semi-CRF, there are some valuable features that are more naturally expressed in a CRF segmentation model, and so they are not typically included in semi-CRFs (indeed, they have not to date been used in any semi-CRF model for any task, to our knowledge)." ></td>
	<td class="line x" title="26:152	In this paper, we show that semi-CRFs are strictly more expressive, and also demonstrate how CRF-type features can be used in a semi-CRF model for Chinese word segmentation." ></td>
	<td class="line x" title="27:152	Our experiments show that a model incorporating both types of features can outperform models using only one or the other type." ></td>
	<td class="line x" title="28:152	Orthogonally, we explore in this paper the use of a very powerful feature for the semi-CRF derived from a generative model." ></td>
	<td class="line x" title="29:152	It is common in statistical NLP to use as features in a discriminative model the (logarithm of the) estimated probability of some event according to a generative model." ></td>
	<td class="line x" title="30:152	For example, Collins (2000) uses a discriminative classifier for choosing among the top N parse trees output by a generative baseline model, and uses the log-probability of a parse according to the baseline model as a feature in the reranker." ></td>
	<td class="line x" title="31:152	Similarly, the machine translation system of Och and Ney uses log-probabilities of phrasal translations and other events as features in a log-linear model (Och and Ney, 2002; Och and Ney, 2004)." ></td>
	<td class="line x" title="32:152	There are many reasons for incorporating these types of features, including the desire to combine the higher accuracy of a discriminative model with the simple parameter estimation and inference of a generative one, and also the fact that generative models are more robust in data sparse scenarios (Ng and Jordan, 2001)." ></td>
	<td class="line x" title="33:152	For word segmentation, one might want to use as a local feature the log-probability that a segment is a word, given the character sequence it spans." ></td>
	<td class="line x" title="34:152	A curious property of this feature is that it induces a counterintuitive asymmetry between the is-word and is-not-word cases: the component generative model can effectively dictate that a certain chunk is not a word, by assigning it a very low probability (driving the feature value to negative infinity), but it cannot dictate that a chunk is a word, because the log-probability is bounded above.2 If instead the log conditional odds log Pi(y|x)Pi(y|x) is used, the asymmetry disappears." ></td>
	<td class="line x" title="35:152	We show that such a logodds feature provides much greater benefit than the log-probability, and that it is useful to include such a feature even when the model also includes indicator function features for every word in the training corpus." ></td>
	<td class="line x" title="36:152	2 Hybrid Markov/Semi-Markov CRF The model we describe is formally a type of semiMarkov CRF, distinguished only in that it also involves CRF-style features." ></td>
	<td class="line x" title="37:152	So we first describe the semi-Markov model in its general form." ></td>
	<td class="line x" title="38:152	2.1 Semi-Markov CRF An (unlabeled) semi-Markov conditional random field is a log-linear model defining the conditional probability of a segmentation given an observation sequence." ></td>
	<td class="line x" title="39:152	The general form of a log-linear model is as follows: given an input x  X, an output yY, a feature mapping  : XY mapstoRn, and a weight vector w, the conditional probability of y given x is estimated as: P(y|x) = exp(w(x,y))Z(x) where Z : x mapsto R is a normalizing factor." ></td>
	<td class="line x" title="40:152	w is typically chosen to maximize the conditional likelihood of a labeled training set." ></td>
	<td class="line x" title="41:152	In the word 2We assume the weight assigned to the log-probability feature is positive." ></td>
	<td class="line x" title="42:152	466 segmentation task, x is an ordered sequence of characters (x1,x2,,xn), and y is a set of indices corresponding to the start of each word: {y1,y2,,ym} such that y1 = 1, ym  n, and for all j, yj < yj+1." ></td>
	<td class="line x" title="43:152	A log-linear model in this space is an order-1 semi-CRF if its feature map  decomposes according to (x,y) = msummationdisplay j=1 S(yj,yj+1,x) (1) where S is a local feature map that only considers one chunk at a time (defining ym+1 = n+1)." ></td>
	<td class="line x" title="44:152	This decomposition is responsible for the characteristic independence assumptions of the semi-CRF." ></td>
	<td class="line x" title="45:152	Hand-in-hand with the feature decomposition and independence assumptions comes the capacity for exact decoding using the Viterbi algorithm, and exact computation of the objective gradient using the forward-backward algorithm, both in time quadratic in the lengths of the sentences." ></td>
	<td class="line x" title="46:152	Furthermore, if the model is constrained to propose only chunkings with maximum word length k, then the time for inference and training becomes linear in the sentence length (and in k)." ></td>
	<td class="line x" title="47:152	For Chinese word segmentation, choosing a moderate value of k does not pose any significant risk, since the vast majority of Chinese words are only a few characters long: in our training set, 91% of word tokens were one or two characters, and 99% were five characters or less." ></td>
	<td class="line x" title="48:152	Using a semi-CRF as opposed to a traditional Markov CRF allows us to model some aspects of word segmentation that one would expect to be very informative." ></td>
	<td class="line x" title="49:152	In particular, it makes possible the use of local indicator function features of the type the chunk consists of character sequence 1,,lscript, or the chunk is of length lscript. It also enables pseudo-bigram language model features, firing when a given word occurs in the context of a given character unigram or bigram.3 And crucially, although it is slightly less natural to do so, any feature used in an order-1 Markov CRF can also be represented in a semi-CRF." ></td>
	<td class="line x" title="50:152	As Markov CRFs are used in the most competitive Chinese word segmentation models to date, one might expect that incorporating both types of features could yield a superior model." ></td>
	<td class="line x" title="51:152	3We did not experiment with this type of feature." ></td>
	<td class="line x" title="52:152	2.2 CRF vs. Semi-CRF In order to compare the two types of linear CRFs, it is convenient to define a representation of the segmentation problem in terms of character labels as opposed to sets of whole words." ></td>
	<td class="line x" title="53:152	Denote by L(y) {B,C}n (for BEGIN vs. CONTINUATION) the sequence {L1,L2,Ln} of labels such that Li = B if and only if yi y. It is clear that if we constrain L1 = B, the two representations y and L(y) are equivalent." ></td>
	<td class="line x" title="54:152	An order-1 Markov CRF is a log-linear model in which the global feature vector  decomposes into a sum over local feature vectors that consider bigrams of the label sequence: (x,y) = nsummationdisplay i=1 M(Li,Li+1,i,x) (2) (where Ln+1 is defined as B)." ></td>
	<td class="line x" title="55:152	The local features that are most naturally expressed in this context are indicators of some joint event of the label bigram (Li,Li+1) and nearby characters in x. For example, one might use the feature the current character xi is  and Li = C, or the current and next characters are identical and Li = Li+1 = B. Although we have heretofore disparaged the CRF as being incapable of representing such powerful features as word identity, the type of features that it most naturally represents should be helpful in CWS for generalizing to unseen words." ></td>
	<td class="line x" title="56:152	For example, the first feature mentioned above could be valuable to rule out certain word boundaries if  were a character that typically occurs only as a suffix but that combines freely with a variety of root forms to create new words." ></td>
	<td class="line x" title="57:152	This type of feature (specifically, a feature indicating the absence as opposed to the presence of a chunk boundary) is a bit less natural in a semi-CRF, since in that case local features S(yj,yj+1,x) are defined on pairs of adjacent boundaries." ></td>
	<td class="line x" title="58:152	Information about which tokens are not on boundaries is only implicit, making it a bit more difficult to incorporate that information into the features." ></td>
	<td class="line x" title="59:152	Indeed, neither Liang (2005) nor Sarawagi and Cohen (2004) nor any other system using a semi-Markov CRF on any task has included this type of feature to our knowledge." ></td>
	<td class="line x" title="60:152	We hypothesize (and our experiments confirm) that the lack of this feature explains the failure of the semi-CRF to outperform the CRF for word segmentation in the past." ></td>
	<td class="line x" title="61:152	Before showing how CRF-type features can be used in a semi-CRF, we first demonstrate that the semi-CRF is indeed strictly more expressive than 467 the CRF, meaning that any global feature map  that decomposes according to (2) also decomposes according to (1)." ></td>
	<td class="line x" title="62:152	It is sufficient to show that for any feature map M of a Markov CRF, there exists a semi-Markov-type feature map S such that for any x,y, M(x,y) = nsummationdisplay i=1 M(Li,Li+1,i,x) (3) = msummationdisplay j=1 S(yj,yj+1,x) = S(x,y) To this end, note that there are only four possible label bigrams: BB, BC, CB, and CC." ></td>
	<td class="line x" title="63:152	As a direct result of the definition of L(y), we have that (Li,Li+1) = (B,B) if and only if some word of length one begins at i, or equivalently, there exists a word j such that yj = i and yj+1yj = 1." ></td>
	<td class="line x" title="64:152	Similarly, (Li,Li+1) = (B,C) if and only if some word of length > 1 begins at i, etc. Using these conditions, we can define S to satisfy equation 3 as follows: S(yj,yj+1,x) = M(B,B,yj,x) if yj+1yj = 1, and S(yj,yj+1,x) = M(B,C,yj,x) + yj+12summationdisplay k=yj+1 M(C,C,k,x) (4) +M(C,B,yj+11,x) otherwise." ></td>
	<td class="line x" title="65:152	Defined thus,summationtextmj=1 S will contain exactly n M terms, corresponding to the n label bigrams.4 2.3 Order-1 Markov Features in a Semi-CRF While it is fairly intuitive that any feature used in a 1-CRF can also be used in a semi-CRF, the above argument reveals an algorithmic difficulty that is likely another reason that such features are not typically used." ></td>
	<td class="line x" title="66:152	The problem is essentially an effect of the sum for CC label bigrams in (4): quadratic time training and decoding assumes that the features of each chunk S(yj,yj+1,x) can be multiplied with the weight vector w in a number of operations that is roughly constant over all chunks, 4We have discussed the case of Markov order-1, but the argument can be generalized to show that an order-M CRF has an equivalent representation as an order-M semi-CRF, for any M. procedure ComputeScores(x,w) for i = 2(n1) do CCi M(C,C,i,x)w end for for a = 1n do CCsum0 for b = (a+ 1)(n + 1) do if ba = 1 then abM(B,B,a,x)w else abM(B,C,a,x)w +CCsum +M(C,B,b1,x)w CCsumCCsum+CCb1 end if end for end for Figure 1: Dynamic program for computing chunk scores ab with 1-CRF-type features." ></td>
	<td class="line x" title="67:152	but if one navely distributes the product over the sum, longer chunks will take proportionally longer to score, resulting in cubic time algorithms.5 In fact, it is possible to use these features without any asymptotic decrease in efficiency by means of a dynamic program." ></td>
	<td class="line x" title="68:152	Both Viterbi and forward-backward involve the scores ab = w S(a,b,x)." ></td>
	<td class="line x" title="69:152	Suppose that before starting those algorithms, we compute and cache the score ab of each chunk, so that remainder the algorithm runs in quadratic time, as usual." ></td>
	<td class="line x" title="70:152	This pre-computation can be done quickly if we first compute the values CCi = wM(C,C,i,x), and use them to fill in the values of ab as shown in Figure 1." ></td>
	<td class="line x" title="71:152	In addition, computing the gradient of the semiCRF objective requires that we compute the expected value of each feature." ></td>
	<td class="line x" title="72:152	For CRF-type features, this is tantamount to being able to compute the probability that each label bigram (Li,Li+1) takes any value." ></td>
	<td class="line x" title="73:152	Assume that we have already run standard forward-backward inference so that we have for any (a,b) the probability that the subsequence (xa,xa+1,,xb1) segments as a chunk, P(chunk(a,b))." ></td>
	<td class="line x" title="74:152	Computing the probability that (Li,Li+1) takes the values BB, BC or CB is simple to compute: P(Li,Li+1 = BB) = P(chunk(i,i+ 1)) 5Note that the problem would arise even if only zero-order Markov (label unigram) features were used, only in that case the troublesome features would be those that involved the label unigram C. 468 and, e.g., P(Li,Li+1 = BC) = summationdisplay j>i+1 P(chunk(i,j)), but the same method of summing over chunks cannot be used for the value CC since for each label bigram there are quadratically many chunks corresponding to that value." ></td>
	<td class="line x" title="75:152	In this case, the solution is deceptively simple: using the fact that for any given label bigram, the sum of the probabilities of the four labels must be one, we can deduce that P(Li,Li+1 = CC) = 1.0P(Li,Li+1 = BB) P(Li,Li+1 = BC)P(Li,Li+1 = CB)." ></td>
	<td class="line x" title="76:152	One might object that features of the C and CC labels (the ones presenting algorithmic difficulty) are unnecessary, since under certain conditions, their removal would not in fact change the expressivity of the model or the distribution that maximizes training likelihood." ></td>
	<td class="line x" title="77:152	This will indeed be the case when the following conditions are fulfilled: 1." ></td>
	<td class="line x" title="78:152	All label bigram features are of the form M(Li,Li+1,i,x) = 1(Li,Li+1) =  & pred(i,x)} for some label bigram  and predicate pred, and any such feature with a given predicate has variants for all four label bigrams ." ></td>
	<td class="line x" title="79:152	2." ></td>
	<td class="line x" title="80:152	No regularization is used during training." ></td>
	<td class="line x" title="81:152	A proof of this claim would require too much space for this paper, but the key is that, given a model satisfying the above conditions, one can obtain an equivalent model via adding, for each feature type over pred, some constant to the four weights corresponding to the four label bigrams, such that the CC bigram features all have weight zero." ></td>
	<td class="line x" title="82:152	In practice, however, one or both of these conditions is always broken." ></td>
	<td class="line x" title="83:152	It is common knowledge that regularization of log-linear models with a large number of features is necessary to achieve high performance, and typically in NLP one defines feature templates and chooses only those features that occur in some positive example in the training set." ></td>
	<td class="line x" title="84:152	In fact, if both of these conditions are fulfilled, it is very likely that the optimal model will have some weights with infinite values." ></td>
	<td class="line x" title="85:152	We conclude that it is not a practical alternative to omit the C and CC label features." ></td>
	<td class="line x" title="86:152	2.4 Generative Features in a Discriminative Model When using the output of a generative model as a feature in a discriminative model, Raina et al.(2004) provide a justification for the use of log conditional odds as opposed to log-probability: they show that using log conditional odds as features in a logistic regression model is equivalent to discriminatively training weights for the features of a Nave Bayes classifier to maximize conditional likelihood.6 They demonstrate that the resulting classifier, termed a hybrid generative/discriminative classifier, achieves lower test error than either pure Nave Bayes or pure logistic regression on a text classification task, regardless of training set size." ></td>
	<td class="line x" title="88:152	The hybrid generative/discriminative classifier also uses a unique method for using the same data used to estimate the parameters of the component generative models for training the discriminative model parameters w without introducing bias." ></td>
	<td class="line x" title="89:152	A leave-one-out strategy is used to choose w, whereby the feature values of the i-th training example are computed using probabilities estimated with the i-th example held out." ></td>
	<td class="line x" title="90:152	The beauty of this approach is that since the probabilities are estimated according to (smoothed) relative frequency, it is only necessary during feature computation to maintain sufficient statistics and adjust them as necessary for each example." ></td>
	<td class="line x" title="91:152	In this paper, we experiment with the use of a single hybrid local semi-CRF feature, the smoothed log conditional odds that a given subsequence xab = (xa,,xb1) forms a word: log wordcount(xab) + 1nonwordcount(x ab) + 1, where wordcount(xab) is the number of times xab forms a word in the training set, and nonwordcount(xab) is the number of times xab occurs, not segmented into a single word." ></td>
	<td class="line x" title="92:152	The models we test are not strictly speaking hybrid generative/discriminative models, since we also use indicator features not derived from a generative model." ></td>
	<td class="line x" title="93:152	We did however use the leave-one-out approach for computing the log conditional odds feature during training." ></td>
	<td class="line x" title="94:152	6In fact, one more step beyond what is shown in that paper is required to reach the stated conclusion, since their features are not actually log conditional odds, but log P(x|y)P(x|y)." ></td>
	<td class="line x" title="95:152	It is simple to show that in the given context this feature is equivalent to log conditional odds." ></td>
	<td class="line x" title="96:152	469 3 Experiments To test the ideas discussed in this paper, we compared the performance of semi-CRFs using various feature sets on a Chinese word segmentation task." ></td>
	<td class="line x" title="97:152	The data used was the Microsoft Research Beijing corpus from the Second International Chinese Word Segmentation Bakeoff (Emerson, 2005), and we used the same train/test split used in the competition." ></td>
	<td class="line x" title="98:152	The training set consists of 87K sentences of Beijing dialect Chinese, hand segmented into 2.37M words." ></td>
	<td class="line x" title="99:152	The test set contains 107K words comprising roughly 4K sentences." ></td>
	<td class="line x" title="100:152	We used a maximum word length k of 15 in our experiments, which accounted for 99.99% of the word tokens in our training set." ></td>
	<td class="line x" title="101:152	The 249 training sentences that contained words longer than 15 characters were discarded." ></td>
	<td class="line x" title="102:152	We did not discard any test sentences." ></td>
	<td class="line x" title="103:152	In order to be directly comparable to the Bakeoff results, we also worked under the very strict closed test conditions of the Bakeoff, which require that no information or data outside of the training set be used, not even prior knowledge of which characters represent Arabic numerals, Latin characters or punctuation marks." ></td>
	<td class="line x" title="104:152	3.1 Features Used We divide our main features into two types according to whether they are most naturally used in a CRF or a semi-CRF." ></td>
	<td class="line x" title="105:152	The CRF-type features are indicator functions that fire when the character label (or label bigram) takes some value and some predicate of the input at a certain position relative to the label is satisfied." ></td>
	<td class="line x" title="106:152	For each character label unigram L at position i, we use the same set of predicate templates checking:  The identity of xi1 and xi  The identity of the character bigram starting at positions i2, i1 and i  Whether xj and xj+1 are identical, for j = (i2)i  Whether xj and xj+2 are identical, for j = (i3)i  Whether the sequence xjxj+3 forms an AABB sequence for j = (i4)i  Whether the sequence xj xj+3 forms an ABAB sequence for j = (i4)i The latter four feature templates are designed to detect character or word reduplication, a morphological phenomenon that can influence word segmentation in Chinese." ></td>
	<td class="line x" title="107:152	The first two of these were also used by Tseng et al.(2005)." ></td>
	<td class="line x" title="109:152	For label bigrams (Li,Li+1), we use the same templates, but extending the range of positions by one to the right.7 Each label unior bigram also has a prior feature that always fires for that label configuration." ></td>
	<td class="line x" title="110:152	All configurations contain the above features for the label unigram B, since these are easily used in either a CRF or semiCRF model." ></td>
	<td class="line x" title="111:152	To determine the influence of CRFtype features on performance, we also test configurations in which both B and C label features are used, and configurations using all label uniand bigrams." ></td>
	<td class="line x" title="112:152	In the semi-Markov conditions, we also use as feature templates indicators of the length of a word lscript, for lscript = 1k, and indicators of the identity of the corresponding character sequence." ></td>
	<td class="line x" title="113:152	All feature templates were instantiated with values that occur in positive training examples." ></td>
	<td class="line x" title="114:152	We found that excluding CRF-type features that occur only once in the training set consistently improved performance on the development set, so we use a count threshold of two for the experiments." ></td>
	<td class="line x" title="115:152	We do not do any thresholding of the semi-CRF features, however." ></td>
	<td class="line x" title="116:152	Finally, we use the single generative feature, log conditional odds that the given string forms a word." ></td>
	<td class="line x" title="117:152	We also present results using the more typical log conditional probability instead of the odds, for comparison." ></td>
	<td class="line x" title="118:152	In fact, these are both semiMarkov-type features, but we single them out to determine what they contribute over and above the other semi-Markov features." ></td>
	<td class="line x" title="119:152	3.2 Results The results of test set runs are summarized in table 3.2." ></td>
	<td class="line x" title="120:152	The columns indicate which CRF-type features were used: features of only the label B, features of label unigrams B and C, or features of all label unigrams and bigrams." ></td>
	<td class="line x" title="121:152	The rows indicate which semi-Markov-type features were used: 7For both label unigram and label bigram features, the indices are chosen so that the feature set exhibits no asymmetry with respect to direction: for each feature considering some boundary and some property of the character(s) at a given offset to the left, there is a corresponding feature considering that boundary and the same property of the character(s) at the same offset to the right, and vice-versa." ></td>
	<td class="line x" title="122:152	470 Features B only uni uni+bi none 92.33 94.71 95.69 semi 95.28 96.05 96.46 prob 93.86 95.40 96.04 semi+prob 95.51 96.24 96.55 odds 95.10 96.06 96.40 semi+odds 96.27 96.77 96.84 Table 1: Test F-measure for different model configurations." ></td>
	<td class="line x" title="123:152	semi means length and word identity features were used, prob means the log-probability feature was used, and odds means the log-odds feature was used." ></td>
	<td class="line x" title="124:152	To establish the impact of each type of feature (C label unigrams, label bigrams, semi-CRF-type features, and the log-odds feature), we look at the reduction in error brought about by adding each type of feature." ></td>
	<td class="line x" title="125:152	First consider the effect of the CRF-type features." ></td>
	<td class="line x" title="126:152	Adding the C label features reduces error by 31% if no semi-CRF features are used, by 16% when semi-CRF indicator features are turned on, and by 13% when all semi-CRF features (including log-odds) are used." ></td>
	<td class="line x" title="127:152	Using all label bigrams reduces error by 44%, 25%, and 15% in these three conditions, respectively." ></td>
	<td class="line x" title="128:152	Contrary to previous conclusions, our results show a significant impact due to the use of semiCRF-type features, when CRF-type features are held constant." ></td>
	<td class="line x" title="129:152	Adding semi-CRF indicator features results in a 38% error reduction without CRF-type features, and 18% with them." ></td>
	<td class="line x" title="130:152	Adding semi-CRF indicator features plus the log-odds feature gives 52% and 27% in these two conditions, respectively." ></td>
	<td class="line x" title="131:152	Finally, across configurations, the log conditional odds does much better than log conditional probability." ></td>
	<td class="line x" title="132:152	When the log-odds feature is added to the complete CRF model (uni+bi) as the only semi-CRF-type feature, errors are reduced by 24%, compared to only 7.6% for the logprobability." ></td>
	<td class="line x" title="133:152	Even when the other semi-CRF-type features are present as well, log-odds reduces error by 13% compared to 2.5% for log-probability." ></td>
	<td class="line x" title="134:152	Our best model, combining all features, resulted in an error reduction of 12% over the highest score on this dataset from the 2005 Sighan closed test competition (96.4%), achieved by the pure CRF system of Tseng et al.(2005)." ></td>
	<td class="line x" title="136:152	3.3 Discussion Our results indicate that both Markov-type and semi-Markov-type features are useful for generalization to unseen data." ></td>
	<td class="line x" title="137:152	This may be because the two types of features are in a sense complementary: semi-Markov-type features such as wordidentity are valuable for modeling the tendency of known strings to segment as words, while label based features are valuable for modeling properties of sub-lexical components such as affixes, helping to generalize to words that have not previously been encountered." ></td>
	<td class="line x" title="138:152	We did not explicitly test the utility of CRF-type features for improving recall on out-of-vocabulary items, but we note that in the Bakeoff, the model of Tseng et al.(2005), which was very similar to our CRF-only system (only containing a few more feature templates), was consistently among the best performing systems in terms of test OOV recall (Emerson, 2005)." ></td>
	<td class="line x" title="140:152	We also found that for this sequence segmentation task, the use of log conditional odds as a feature results in much better performance than the use of the more typical log conditional probability." ></td>
	<td class="line x" title="141:152	It would be interesting to see the log-odds applied in more contexts where log-probabilities are typically used as features." ></td>
	<td class="line x" title="142:152	We have presented the intuitive argument that the log-odds may be advantageous because it does not exhibit the 0-1 asymmetry of the log-probability, but it would be satisfying to justify the choice on more theoretical grounds." ></td>
	<td class="line oc" title="143:152	4 Relation to Previous Work There is a significant volume of work exploring the use of CRFs for a variety of chunking tasks, including named-entity recognition, gene prediction, shallow parsing and others (Finkel et al. , 2005; Culotta et al. , 2005; Sha and Pereira, 2003)." ></td>
	<td class="line n" title="144:152	The current work indicates that these systems might be improved by moving to a semi-CRF model." ></td>
	<td class="line x" title="145:152	There have not been a large number of studies using the semi-CRF, but the few that have been done found only marginal improvements over pure CRF systems (Sarawagi and Cohen, 2004; Liang, 2005; Daume III and Marcu, 2005)." ></td>
	<td class="line x" title="146:152	Notably, none of those studies experimented with features of chunk non-boundaries, as is achieved by the use of CRF-type features involving the label C, and we take this to be the reason for their not obtaining higher results." ></td>
	<td class="line x" title="147:152	471 Although it has become fairly common in NLP to use the log conditional probabilities of events as features in a discriminative model, we are not aware of any work using the log conditional odds." ></td>
	<td class="line x" title="148:152	5 Conclusion We have shown that order-1 semi-Markov conditional random fields are strictly more expressive than order-1 Markov CRFs, and that the added expressivity enables the use of features that lead to improvements on a segmentation task." ></td>
	<td class="line x" title="149:152	On the other hand, Markov CRFs can more naturally incorporate certain features that may be useful for modeling sub-chunk phenomena and generalization to unseen chunks." ></td>
	<td class="line x" title="150:152	To achieve the best performance for segmentation, we propose that both types of features be used, and we show how this can be done efficiently." ></td>
	<td class="line x" title="151:152	Additionally, we have shown that a log conditional odds feature estimated from a generative model can be superior to the more common log conditional probability." ></td>
	<td class="line x" title="152:152	6 Acknowledgements Many thanks to Kristina Toutanova for her thoughtful discussion and feedback, and also to the anonymous reviewers for their suggestions." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D07-1033
A New Perceptron Algorithm for Sequence Labeling with Non-Local Features
Kazama, Jun'ichi;Torisawa, Kentaro;"></td>
	<td class="line x" title="1:389	Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp." ></td>
	<td class="line x" title="2:389	315324, Prague, June 2007." ></td>
	<td class="line x" title="3:389	c2007 Association for Computational Linguistics A New Perceptron Algorithm for Sequence Labeling with Non-local Features Junichi Kazama and Kentaro Torisawa Japan Advanced Institute of Science and Technology (JAIST) Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan {kazama, torisawa}@jaist.ac.jp Abstract We cannot use non-local features with current major methods of sequence labeling such as CRFs due to concerns about complexity." ></td>
	<td class="line x" title="4:389	We propose a new perceptron algorithm that can use non-local features." ></td>
	<td class="line x" title="5:389	Our algorithm allows the use of all types of non-local features whose values are determinedfromthesequenceandthelabels." ></td>
	<td class="line x" title="6:389	The weights of local and non-local features are learned together in the training process with guaranteed convergence." ></td>
	<td class="line x" title="7:389	We present experimental results from the CoNLL 2003 named entity recognition (NER) task to demonstrate the performance of the proposed algorithm." ></td>
	<td class="line x" title="8:389	1 Introduction Many NLP tasks such as POS tagging and named entity recognition have recently been solved as sequence labeling." ></td>
	<td class="line x" title="9:389	Discriminative methods such as Conditional Random Fields (CRFs) (Lafferty et al. , 2001), Semi-Markov Random Fields (Sarawagi and Cohen, 2004), and perceptrons (Collins, 2002a) have been popular approaches for sequence labeling because of their excellent performance, which is mainly due to their ability to incorporate many kinds of overlapping and non-independent features." ></td>
	<td class="line x" title="10:389	However, the common limitation of these methods is that the features are limited to local features, which only depend on a very small number of labels (usually two: the previous and the current)." ></td>
	<td class="line x" title="11:389	Although this limitation makes training and inference tractable, it also excludes the use of possibly useful non-local features that are accessible after all labels are determined." ></td>
	<td class="line oc" title="12:389	For example, non-local features such as same phrases in a document do not have different entity classes were shown to be useful in named entity recognition (Sutton and McCallum, 2004; Bunescu and Mooney, 2004; Finkel et al. , 2005; Krishnan and Manning, 2006)." ></td>
	<td class="line x" title="13:389	Weproposeanewperceptronalgorithminthispaper that can use non-local features along with local features." ></td>
	<td class="line oc" title="14:389	Although several methods have already been proposed to incorporate non-local features (Sutton and McCallum, 2004; Bunescu and Mooney, 2004; Finkel et al. , 2005; Roth and Yih, 2005; Krishnan and Manning, 2006; Nakagawa and Matsumoto, 2006), these present a problem that the types of non-local features are somewhat constrained." ></td>
	<td class="line oc" title="15:389	For example, Finkel et al.(2005) enabled the use of non-local features by using Gibbs sampling." ></td>
	<td class="line n" title="17:389	However, it is unclear how to apply their methodofdeterminingtheparametersofanon-local model to other types of non-local features, which they did not used." ></td>
	<td class="line x" title="18:389	Roth and Yih (2005) enabled the use of hard constraints on labels by using integer linear programming." ></td>
	<td class="line x" title="19:389	However, this is equivalent to only allowing non-local features whose weights are xed to negative innity." ></td>
	<td class="line x" title="20:389	Krishnan and Manning (2006) divided the model into two CRFs, where the second model uses the output of the rst as a kind of non-local information." ></td>
	<td class="line x" title="21:389	However, it is not possible to use non-local features that depend on the labels of the very candidate to be scored." ></td>
	<td class="line x" title="22:389	Nakagawa and Matsumoto (2006) used a Bolzmann distribution to model the correlation of the POS of words having the same lexical form in a document." ></td>
	<td class="line x" title="23:389	However, their method can only be applied when there are convenient links such as the same lexical form." ></td>
	<td class="line x" title="24:389	Since non-local features have not yet been extensively investigated, it is possible for us to nd new useful non-local features." ></td>
	<td class="line x" title="25:389	Therefore, our objective in this study was to establish a framework, where all 315 types of non-local features are allowed." ></td>
	<td class="line x" title="26:389	With non-local features, we cannot use efcient procedures such as forward-backward procedures and the Viterbi algorithm that are required in training CRFs (Lafferty et al. , 2001) and perceptrons (Collins, 2002a)." ></td>
	<td class="line x" title="27:389	Recently, severalmethods(Collins and Roark, 2004; Daume III and Marcu, 2005; McDonald and Pereira, 2006) have been proposed with similar motivation to ours." ></td>
	<td class="line x" title="28:389	These methods alleviate this problem by using some approximation in perceptron-type learning." ></td>
	<td class="line x" title="29:389	In this paper, we follow this line of research and try to solve the problem by extending Collins perceptron algorithm (Collins, 2002a)." ></td>
	<td class="line x" title="30:389	We exploited the not-so-familiar fact that we can design a perceptron algorithm with guaranteed convergence if we can nd at least one wrong labeling candidate even if we cannot perform exact inference." ></td>
	<td class="line x" title="31:389	We rst ran the A* search only using local features to generate n-best candidates (this can be efciently performed), and then we only calculated the true score with non-local features for these candidates to nd a wrong labeling candidate." ></td>
	<td class="line x" title="32:389	The second key idea was to update the weights of local features during training if this was necessary to generate sufciently good candidates." ></td>
	<td class="line x" title="33:389	The proposed algorithm combined these ideas to achieve guaranteed convergence and effective learning with non-local features." ></td>
	<td class="line x" title="34:389	The remainder of the paper is organized as follows." ></td>
	<td class="line x" title="35:389	Section 2 introduces the Collins perceptron algorithm." ></td>
	<td class="line x" title="36:389	Although this algorithm is the starting point for our algorithm, its baseline performance is not outstanding." ></td>
	<td class="line x" title="37:389	Therefore, we present a margin extension to the Collins perceptron in Section 3." ></td>
	<td class="line x" title="38:389	This margin perceptron became the direct basis of our algorithm." ></td>
	<td class="line x" title="39:389	We then explain our algorithm for nonlocal features in Section 4." ></td>
	<td class="line x" title="40:389	We report the experimental results using the CoNLL 2003 shared task dataset in Section 6." ></td>
	<td class="line x" title="41:389	2 Perceptron Algorithm for Sequence Labeling Collins (2002a) proposed an extension of the perceptron algorithm (Rosenblatt, 1958) to sequence labeling." ></td>
	<td class="line x" title="42:389	Our aim in sequence labeling is to assign label yi  Y to each word xi  X in a sequence." ></td>
	<td class="line x" title="43:389	We denote sequence x1,,xT as x and the corresponding labels as y. We assume weight vector   Rd and feature mapping  that maps each (x,y) to feature vector (x,y) = (1(x,y),,d(x,y))  Rd. The model determines the labels by: y = argmaxyY|x|(x,y), where  denotes the inner product." ></td>
	<td class="line x" title="44:389	The aim of the learning algorithm is to obtain an appropriate weight vector, , given training set {(x1,y1),,(xL,yL)}." ></td>
	<td class="line x" title="45:389	The learning algorithm, which is illustrated in Collins (2002a), proceeds as follows." ></td>
	<td class="line x" title="46:389	The weight vector is initialized to zero." ></td>
	<td class="line x" title="47:389	The algorithm passes over the training examples, and each sequence is decoded using the current weights." ></td>
	<td class="line x" title="48:389	If y is not the correct answery, the weights are updated according to the following rule." ></td>
	<td class="line x" title="49:389	new = + (x,y)(x,y)." ></td>
	<td class="line x" title="50:389	This algorithm is proved to converge (i.e. , there are no more updates) in the separable case (Collins, 2002a).1 Thatis,ifthereexistweightvectorU (with ||U|| = 1),  (> 0), and R (> 0) that satisfy: i,y  Y|xi| (xi,yi)U (xi,y)U  , i,y  Y|xi| ||(xi,yi)(xi,y)||  R, the number of updates is at most R2/2." ></td>
	<td class="line x" title="51:389	The perceptron algorithm only requires one candidatey foreachsequencexi, unlikethetrainingof CRFs where all possible candidates need to be considered." ></td>
	<td class="line x" title="52:389	This inherent property is the key to training with non-local features." ></td>
	<td class="line x" title="53:389	However, note that the tractability of learning and inference relies on how efciently y can be found." ></td>
	<td class="line x" title="54:389	In practice, we can nd y efciently using a Viterbi-type algorithm only when the features are all local, i.e., s(x,y) can be written as the sum of (two label) local features s as s(x,y) = Ti s(x,yi1,yi)." ></td>
	<td class="line x" title="55:389	This locality constraint is also required to make the training of CRFs tractable (Lafferty et al. , 2001)." ></td>
	<td class="line x" title="56:389	One problem with the perceptron algorithm described so far is that it offers no treatment for overtting." ></td>
	<td class="line x" title="57:389	Thus, Collins (2002a) also proposed an averaged perceptron, where the nal weight vector is 1Collins(2002a)alsoprovidedproofthatguaranteedgood learning for the non-separable case." ></td>
	<td class="line x" title="58:389	However, we have only considered the separable case throughout the paper." ></td>
	<td class="line x" title="59:389	316 Algorithm 3.1: Perceptron with margin for sequence labeling (parameters: C)   0 until no more updates do for i  1 to L do8 >> >< >> >: y = argmaxy(xi,y) y = 2nd-besty(xi,y) if y = yi then  = + (xi,yi)(xi,y) else if (xi,yi)(xi,y)  C then  = + (xi,yi)(xi,y) the average of all weight vectors during training." ></td>
	<td class="line x" title="60:389	Howerver, we found in our experiments that the averaged perceptron performed poorly in our setting." ></td>
	<td class="line x" title="61:389	We therefore tried to make the perceptron algorithm more robust to overtting." ></td>
	<td class="line x" title="62:389	We will describe our extension to the perceptron algorithm in the next section." ></td>
	<td class="line x" title="63:389	3 Margin Perceptron Algorithm for Sequence Labeling Weextendedaperceptronwithamargin(Krauthand Mezard, 1987) to sequence labeling in this study, as Collins (2002a) extended the perceptron algorithm to sequence labeling." ></td>
	<td class="line x" title="64:389	In the case of sequence labeling, the margin is dened as: () = minx i miny =yi (xi,yi)(xi,y) |||| Assuming that the best candidate,y, equals the correct answer, y, the margin can be re-written as: = minx i (xi,yi)(xi,y) ||||, wherey = 2nd-besty(xi,y)." ></td>
	<td class="line x" title="65:389	Using this relation, theresultingalgorithmbecomesAlgorithm3.1." ></td>
	<td class="line x" title="66:389	The algorithm tries to enlarge the margin as much as possible, as well as make the best scoring candidate equal the correct answer." ></td>
	<td class="line x" title="67:389	Constant C in Algorithm 3.1 is a tunable parameter, which controls the trade-off between the margin and convergence time." ></td>
	<td class="line x" title="68:389	Based on the proofs in Collins (2002a) and Li et al.(2002), we can prove that the algorithm converges within (2C + R2)/2 updates and that ()  C/(2C + R2) = (/2)(1  (R2/(2C + R2))) after training." ></td>
	<td class="line x" title="70:389	As can be seen, the margin approaches at least half of true margin  (at the cost of innite training time), as C  ." ></td>
	<td class="line x" title="71:389	Note that if the features are all local, the secondbestcandidate(generallyn-bestcandidates)canalso be found efciently by using an A* search that uses the best scores calculated during a Viterbi search as the heuristic estimation (Soong and Huang, 1991)." ></td>
	<td class="line x" title="72:389	There are other methods for improving robustness by making margin larger for the structural output problem." ></td>
	<td class="line x" title="73:389	Such methods include ALMA (Gentile, 2001) used in (Daume III and Marcu, 2005)2, MIRA (Crammer et al. , 2006) used in (McDonald et al. , 2005), and Max-Margin Markov Networks (Taskar et al. , 2003)." ></td>
	<td class="line x" title="74:389	However, to the best of our knowledge, there has been no prior work that has applied a perceptron with a margin (Krauth and Mezard, 1987) to structured output.3 Our method described in this section is one of the easiest to implement, while guaranteeingalargemargin." ></td>
	<td class="line x" title="75:389	WefoundintheexperimentsthatourmethodoutperformedtheCollinsaveraged perceptron by a large margin." ></td>
	<td class="line x" title="76:389	4 Algorithm 4.1 Denition and Basic Idea Having described the basic perceptron algorithms, we will know explain our algorithm that learns the weights of local and non-local features in a unied way." ></td>
	<td class="line x" title="77:389	Assume that we have local features and nonlocal features." ></td>
	<td class="line x" title="78:389	We use the superscript, l, for local features as li(x,y) and g for non-local features as gi(x,y)." ></td>
	<td class="line x" title="79:389	Then, feature mapping is written as a(x,y) = l(x,y) + g(x,y) = (l1(x,y),,ln(x,y),gn+1(x,y),,gd(x,y))." ></td>
	<td class="line x" title="80:389	Here, we dene: l(x,y) = (l1(x,y),,ln(x,y),0,,0) g(x,y) = (0,,0,gn+1(x,y),,gd(x,y)) Ideally, we want to determine the labels using the whole feature set as: y = argmaxyY|x|a(x,y)." ></td>
	<td class="line x" title="81:389	2(Daume III and Marcu, 2005) also presents the method using the averaged perceptron (Collins, 2002a) 3For re-ranking problems, Shen and Joshi (2004) proposed a perceptron algorithm that also uses margins." ></td>
	<td class="line x" title="82:389	The difference is that our algorithm trains the sequence labeler itself and is much simpler because it only aims at labeling." ></td>
	<td class="line x" title="83:389	317 Algorithm 4.1: Candidate algorithm (parameters: n, C)   0 until no more updates do for i  1 to L do8 >> >> >< >> >> >: {yn} = n-bestyl(xi,y) y = argmaxy{yn}a(xi,y) y = 2nd-besty{yn}a(xi,y) if y = yi & a(xi,yi)a(xi,y)  C then  = + a(xi,yi)a(xi,y) else if a(xi,yi)a(xi,y)  C then  = + a(xi,yi)a(xi,y) However, if there are non-local features, it is impossible to nd the highest scoring candidate efciently, since we cannot use the Viterbi algorithm." ></td>
	<td class="line x" title="84:389	Thus, we cannot use the perceptron algorithms described in the previous sections." ></td>
	<td class="line x" title="85:389	The training of CRFs is also intractable for the same reason." ></td>
	<td class="line x" title="86:389	To deal with this problem, we rst relaxed our objective." ></td>
	<td class="line x" title="87:389	The modied objective was to nd a good model from those with the form: {yn} = n-bestyl(x,y) y = argmaxy{yn}a(x,y), (1) That is, we rst generate n-best candidates {yn} under the local model, l(x,y)  ." ></td>
	<td class="line x" title="88:389	This can be done efciently using the A* algorithm." ></td>
	<td class="line x" title="89:389	We then ndthebestscoringcandidateunderthetotalmodel, a(x,y), only from these n-best candidates." ></td>
	<td class="line x" title="90:389	If n is moderately small, this can also be done in a practical amount of time." ></td>
	<td class="line x" title="91:389	This resembles the re-ranking approach (Collins and Duffy, 2002; Collins, 2002b)." ></td>
	<td class="line x" title="92:389	However, unlike the re-ranking approach, the local model, l(x,y) , and the total model, a(x,y), correlate since they share a part of the vector and are trained at the same time in our algorithm." ></td>
	<td class="line x" title="93:389	The re-ranking approach has the disadvantage that it is necessary to use different training corpora for the rst model and for the second, or to use cross validation type training, to make the training for the second meaningful." ></td>
	<td class="line x" title="94:389	This reduces the effective size of training data or increases training time substantially." ></td>
	<td class="line x" title="95:389	On the other hand, our algorithm has no such disadvantage." ></td>
	<td class="line x" title="96:389	However, we are no longer able to nd the highest scoring candidate under a(x,y)   exactly with this approach." ></td>
	<td class="line x" title="97:389	We cannot thus use the perceptron algorithms directly." ></td>
	<td class="line x" title="98:389	However, by examining the Algorithm 4.2: Perceptron with local and non-local features (parameters: n, Ca, Cl)   0 until no more updates do for i  1 to L do8 >> >> >> >> >> < >> >> >> >> >> : {yn} = n-bestyl(xi,y) y = argmaxy{yn}a(xi,y) y = 2nd-besty{yn}a(xi,y) if y = yi & a(xi,yi)a(xi,y)  Ca then  = + a(xi,yi)a(xi,y) (A) else if a(xi,yi)a(xi,y)  Ca then  = + a(xi,yi)a(xi,y) (A) else (B) 8> < >: if y1 = yi then (y1 represents the best in {yn})  = + l(xi,yi)l(xi,y1) else if l(xi,yi)l(xi,y2)  Cl then  = + l(xi,yi)l(xi,y2) proofs in Collins (2002a), we can see that the essential condition for convergence is that the weights are always updated using some y (= y) that satises: (xi,yi)(xi,y)  0 ( C in the case of a perceptron with a margin)." ></td>
	<td class="line x" title="99:389	(2) That is, y does not necessarily need to be the exact best candidate or the exact second-best candidate." ></td>
	<td class="line x" title="100:389	The algorithm also converges in a nite number of iterations even with Eq." ></td>
	<td class="line x" title="101:389	(1) as long as Eq." ></td>
	<td class="line x" title="102:389	(2) is satised." ></td>
	<td class="line x" title="103:389	4.2 Candidate Algorithm The algorithm we came up with rst based on the above idea, is Algorithm 4.1." ></td>
	<td class="line x" title="104:389	We rst nd the nbest candidates using the local model, l(x,y)." ></td>
	<td class="line x" title="105:389	At this point, we can determine the value of the nonlocal features, g(x,y), to form the whole feature vector, a(x,y), for the n-best candidates." ></td>
	<td class="line x" title="106:389	Next, we re-score and sort them using the total model, a(x,y)  , to nd a candidate that violates the margin condition." ></td>
	<td class="line x" title="107:389	We call this algorithm the candidate algorithm." ></td>
	<td class="line x" title="108:389	After the training has nished, a(xi,yi)    a(xi,y)   > C is guaranteed for all (xi,y) where y  {yn},y = y." ></td>
	<td class="line x" title="109:389	At rst glance, this seems sufcient condition for good models." ></td>
	<td class="line x" title="110:389	However, this is not true because if y  {yn}, the inference dened by Eq." ></td>
	<td class="line x" title="111:389	(1) is not guaranteed to nd the correct answer, y." ></td>
	<td class="line x" title="112:389	In fact, this algorithm does not work well with non-local features as we found in the experiments." ></td>
	<td class="line x" title="113:389	318 4.3 Final Algorithm Our idea for improving the above algorithm is that thelocalmodel,l(x,y),mustatleastbesogood that y  {yn}." ></td>
	<td class="line x" title="114:389	To achieve this, we added a modication term that was intended to improve the local model when the local model was not good enough even when the total model was good enough." ></td>
	<td class="line x" title="115:389	The nal algorithm resulted in Algorithm 4.2." ></td>
	<td class="line x" title="116:389	As canbeseen, thepartmarked(B)hasbeenadded." ></td>
	<td class="line x" title="117:389	We call this algorithm the proposed algorithm." ></td>
	<td class="line x" title="118:389	Note that the algorithm prioritizes the update of the total model, (A), over that of the local model, (B), although the opposite is also possible." ></td>
	<td class="line x" title="119:389	Also note that the update of the local model in (B) is aggressive since it updates the weights until the best candidate output by the local model becomes the correct answer and satises the margin condition." ></td>
	<td class="line x" title="120:389	A conservative updating, where we cease the update when the n-best candidates contain the correct answer, is also possible from our idea above." ></td>
	<td class="line x" title="121:389	We made these choices since they worked better than the other alternatives." ></td>
	<td class="line x" title="122:389	The tunable parameters are the local margin parameter, Cl, the total margin parameter, Ca, and n for the n-best search." ></td>
	<td class="line x" title="123:389	We used C = Cl = Ca in this study to reduce the search space." ></td>
	<td class="line x" title="124:389	We can prove that the algorithm in Algorithm 4.2 also converges in a nite number of iterations." ></td>
	<td class="line x" title="125:389	It converges within (2C + R2)/2 updates, assuming that there exist weight vector Ul (with ||Ul|| = 1 and Uli = 0 (n+1  i  d)),  (> 0), and R (> 0) that satisfy: i,y  Y|xi| l(xi,yi)Ull(xi,y)Ul  , i,y  Y|xi| ||a(xi,yi)a(xi,y)||  R. In addition, we can prove that ()  C/(2C + R2) for the margin after convergence, where () is dened as: minx i miny {yn},=yi a(xi,yi)a(xi,y) |||| See Appendix A for the proofs." ></td>
	<td class="line x" title="126:389	We also incorporated the idea behind Bayes point machines (BPMs) (Herbrich and Graepel, 2000) to improvetherobustnessofourmethodfurther." ></td>
	<td class="line x" title="127:389	BPMs try to cancel out overtting caused by the order of examples, by training several models by shufing the training examples.4 However, it is very time consuming to run the complete training process several times." ></td>
	<td class="line x" title="128:389	We thus ran the training in only one pass over the shufed examples several times, and used the averaged output weight vectors as a new initial weight vector, because we thought that the early part of training would be more seriously affected by the order of examples." ></td>
	<td class="line x" title="129:389	We call this BPM initialization." ></td>
	<td class="line x" title="130:389	5 5 Named Entity Recognition and Non-Local Features We evaluated the performance of the proposed algorithm using the named entity recognition task." ></td>
	<td class="line x" title="131:389	We adopted IOB (IOB2) labeling (Ramshaw and Marcus, 1995), where the rst word of an entity of class C is labeled B-C, the words in the entity are labeled I-C, and other words are labeled O." ></td>
	<td class="line oc" title="132:389	We used non-local features based on Finkel et al.(2005)." ></td>
	<td class="line x" title="134:389	These features are based on observations such as same phrases in a document tend to have the same entity class (phrase consistency) and a sub-phrase of a phrase tends to have the same entity class as the phrase (sub-phrase consistency)." ></td>
	<td class="line x" title="135:389	We also implemented the majority version of these features as used in Krishnan and Manning (2006)." ></td>
	<td class="line x" title="136:389	In addition, we used non-local features, which are based on the observation that entities tend to have the same entity class if they are in the same conjunctiveordisjunctiveexpressionasin inU.S., EU,andJapan(conjunctionconsistency)." ></td>
	<td class="line nc" title="137:389	Thistype of non-local feature was not used by Finkel et al.(2005) or Krishnan and Manning (2006)." ></td>
	<td class="line x" title="139:389	6 Experiments 6.1 Data and Setting We used the English dataset of the CoNLL 2003 named entity shared task (Tjong et al. , 2003) for the experiments." ></td>
	<td class="line x" title="140:389	It is a corpus of English newspaper articles, where four entity classes, PER, LOC, ORG, and MISC are annotated." ></td>
	<td class="line x" title="141:389	It consists of training, development, and testing sets (14,987, 3,466, 4The results for the perceptron algorithms generally depend on the order of the training examples." ></td>
	<td class="line x" title="142:389	5Note that we can prove that the perceptron algorithms converge even though the weight vector is not initialized as = 0." ></td>
	<td class="line x" title="143:389	319 and 3,684 sentences, respectively)." ></td>
	<td class="line x" title="144:389	Automatically assigned POS tags and chunk tags are also provided." ></td>
	<td class="line x" title="145:389	TheCoNLL2003datasetcontainsdocumentboundary markers." ></td>
	<td class="line x" title="146:389	We concatenated the sentences in the same document according to these markers.6 This generated 964 documents for the training set, 216 documents for the development set, and 231 documents for the testing set." ></td>
	<td class="line x" title="147:389	The documents generated as above become the sequence, x, in the learning algorithms." ></td>
	<td class="line x" title="148:389	We rst evaluated the baseline performance of a CRF model, the Collins perceptron, and the Collins averaged perceptron, as well as the margin perceptron, with only local features." ></td>
	<td class="line x" title="149:389	We next evaluated the performance of our perceptron algorithm proposed for non-local features." ></td>
	<td class="line x" title="150:389	We used the local features summarized in Table 1, which are similar to those used in other studies on named entity recognition." ></td>
	<td class="line x" title="151:389	We omitted features whose surface part listed in Table 1 occurred less than twice in the training corpus." ></td>
	<td class="line x" title="152:389	We used CRF++ (ver." ></td>
	<td class="line x" title="153:389	0.44)7 as the basis of our implementation." ></td>
	<td class="line x" title="154:389	We implemented scaling, which is similar to that for HMMs (see such as (Rabiner, 1989)),intheforward-backwardphaseofCRFtraining to deal with very long sequences due to sentence concatenation.8 We used Gaussian regularization (Chen and Rosenfeld, 2000) for CRF training to avoid overtting." ></td>
	<td class="line x" title="155:389	The parameter of the Gaussian, 2, was tuned usingthedevelopmentset." ></td>
	<td class="line x" title="156:389	Wealsotunedthemargin parameter, C, for the margin perceptron algorithm.9 TheconvergenceofCRFtrainingwasdeterminedby checking the log-likelihood of the model." ></td>
	<td class="line x" title="157:389	The convergence of perceptron algorithms was determined by checking the per-word labeling error, since the 6We used sentence concatenation even when only using local features, since we found it does not degrade accuracy (rather we observed a slight increase)." ></td>
	<td class="line x" title="158:389	7http://chasen.org/taku/software/CRF++ 8We also replaced the optimization module in the original package with that used in the Amis maximum entropy estimator (http://www-tsujii.is.s.u-tokyo.ac.jp/amis) since we encountered problems with the provided module in some cases." ></td>
	<td class="line x" title="159:389	9For the Gaussian parameter, we tested {13, 25, 50, 100, 200, 400, 800} (the accuracy did not change drastically among these values and it seems that there is no accuracy hump even if we use smaller values)." ></td>
	<td class="line x" title="160:389	We tested {500, 1000, 1414, 2000, 2828, 4000, 5657, 8000, 11313, 16000, 32000} for the margin parameters." ></td>
	<td class="line x" title="161:389	Table 1: Local features used." ></td>
	<td class="line x" title="162:389	The value of a node feature is determined from the current label, y0, and a surface feature determined only fromx." ></td>
	<td class="line x" title="163:389	The value of an edge feature is determined by the previous label, y1, the current label, y0, and a surface feature." ></td>
	<td class="line x" title="164:389	Used surface features are the word (w), the downcased word (wl), the POS tag (pos), the chunk tag (chk), the prex of the word of length n (pn), the sufx(sn), thewordformfeatures: 2d-cp(theseare based on (Bikel et al. , 1999)), and the gazetteer features: go for ORG, gp for PER, and gm for MISC." ></td>
	<td class="line x" title="165:389	These represent the (longest) match with an entry in the gazetteer by using IOB2 tags." ></td>
	<td class="line x" title="166:389	Node features: {,x2,x1,x0,x+1,x+2}y0 x =, w, wl, pos, chk, p1, p2, p3, p4, s1, s2, s3, s4, 2d, 4d, d&a, d&-, d&/, d&,, d&., n, ic, ac, l, cp, go, gp, gm Edge features: {,x2,x1,x0,x+1,x+2}y1 y0 x =, w, wl, pos, chk, p1, p2, p3, p4, s1, s2, s3, s4, 2d, 4d, d&a, d&-, d&/, d&,, d&., n, ic, ac, l, cp, go, gp, gm Bigram node features: {x2x1,x1x0,x0x+1}y0 x = wl, pos, chk, go, gp, gm Bigram edge features: {x2x1,x1x0,x0x+1}y1 y0 x = wl, pos, chk, go, gp, gm number of updates was not zero even after a large number of iterations in practice." ></td>
	<td class="line x" title="169:389	We stopped training when the relative change in these values became less than a pre-dened threshold (0.0001) for at least three iterations." ></td>
	<td class="line x" title="170:389	We used n = 20 (n of the n-best) for training sincewecouldnotusetooalargenbecauseitwould have slowed down training." ></td>
	<td class="line x" title="171:389	However, we could examinealargernduringtesting,sincethetestingtime did not dominate the time for the experiment." ></td>
	<td class="line x" title="172:389	We found an interesting property for n in our preliminary experiment." ></td>
	<td class="line x" title="173:389	We found that an even larger n in testing (written as n) achieved higher accuracy, although it is natural to assume that the same n that was used in training would also be appropriate for testing." ></td>
	<td class="line x" title="174:389	We thus used n = 100 to evaluate performance during parameter tuning." ></td>
	<td class="line x" title="175:389	After nding the best C with n = 100, we varied n to investigate its 320 Table 2: Summary of performance (F1)." ></td>
	<td class="line x" title="176:389	Method dev test C (or 2) local features CRF 91.10 86.26 100 Perceptron 89.01 84.03 Averaged perceptron 89.32 84.08 Margin perceptron 90.98 85.64 11313 + non-local features Candidate (n = 100) 90.71 84.90 4000 Proposed (n = 100) 91.95 86.30 5657 Table 3: Effect of n." ></td>
	<td class="line x" title="177:389	Method dev test C Proposed (n = 20) 91.76 86.19 5657 Proposed (n = 100) 91.95 86.30 5657 Proposed (n = 400) 92.13 86.39 5657 Proposed (n = 800) 92.09 86.39 5657 Proposed (n = 1600) 92.13 86.46 5657 Proposed (n = 6400) 92.19 86.38 5657 effects further." ></td>
	<td class="line x" title="178:389	6.2 Results Table 2 compares the results." ></td>
	<td class="line x" title="179:389	CRF outperformed the perceptron by a large margin." ></td>
	<td class="line x" title="180:389	Although the averaged perceptron outperformed the perceptron, the improvement was slight." ></td>
	<td class="line x" title="181:389	However, the margin perceptron greatly outperformed compared to the averaged perceptron." ></td>
	<td class="line x" title="182:389	Yet, CRF still had the best baseline performance with only local features." ></td>
	<td class="line x" title="183:389	The proposed algorithm with non-local features improved the performance on the test set by 0.66 points over that of the margin perceptron without non-local features." ></td>
	<td class="line x" title="184:389	The row Candidate refers to the candidate algorithm (Algorithm 4.1)." ></td>
	<td class="line x" title="185:389	From the results for the candidate algorithm, we can see that the modication part, (B), in Algorithm 4.2 was essential to make learning with non-local features effective." ></td>
	<td class="line x" title="186:389	We next examined the effect of n." ></td>
	<td class="line x" title="187:389	As can be seen from Table 3, an n larger than that for training yields higher performance." ></td>
	<td class="line x" title="188:389	The highest performance with the proposed algorithm was achieved when n = 6400, where the improvement due to non-local features became 0.74 points." ></td>
	<td class="line oc" title="189:389	The performance of the related work (Finkel et al. , 2005; Krishnan and Manning, 2006) is listed in Table4." ></td>
	<td class="line p" title="190:389	Wecanseethatthenalperformanceofour algorithm was worse than that of the related work." ></td>
	<td class="line x" title="191:389	We changed the experimental setting slightly to investigate our algorithm further." ></td>
	<td class="line x" title="192:389	Instead of Table 4: The performance of the related work." ></td>
	<td class="line oc" title="193:389	Method dev test Finkel et al. , 2005 (Finkel et al. , 2005) baseline CRF 85.51 + non-local features 86.86 Krishnan and Manning, 2006 (Krishnan and Manning, 2006) baseline CRF 85.29 + non-local features 87.24 Table 5: Summary of performance with POS/chunk tags by TagChunk." ></td>
	<td class="line x" title="194:389	Method dev test C (or 2) local features CRF 91.39 86.30 200 Perceptron 89.36 84.35 Averaged perceptron 89.76 84.50 Margin perceptron 91.06 86.24 32000 + non-local features Proposed (n = 100) 92.23 87.04 5657 Proposed (n = 6400) 92.54 87.17 5657 the POS/chunk tags provided in the CoNLL 2003 dataset, we used the tags assigned by TagChunk (Daume III and Marcu, 2005)10 with the intention of using more accurate tags." ></td>
	<td class="line x" title="195:389	The results with this setting are summarized in Table 5." ></td>
	<td class="line x" title="196:389	Performance was better than that in the previous experiment for all algorithms." ></td>
	<td class="line x" title="197:389	We think this was due to the quality of the POS/chunk tags." ></td>
	<td class="line x" title="198:389	It is interesting that the effect of non-local features rose to 0.93 points with n = 6400, even though the baseline performance was also improved." ></td>
	<td class="line pc" title="199:389	The resulting performance of the proposed algorithm with non-local features is higher than that of Finkel et al.(2005) and comparable with that of Krishnan and Manning (2006)." ></td>
	<td class="line x" title="201:389	This comparison, of course, is not fair because the setting was different." ></td>
	<td class="line x" title="202:389	However, we think the results demonstrate a potential of our new algorithm." ></td>
	<td class="line x" title="203:389	The effect of BPM initialization was also examined." ></td>
	<td class="line x" title="204:389	The number of BPM runs was 10 in this experiment." ></td>
	<td class="line x" title="205:389	The performance of the proposed algorithm dropped from 91.95/86.30 to 91.89/86.03 without BPM initialization as expected in the setting of the experiment of Table 2." ></td>
	<td class="line x" title="206:389	The performance of the margin perceptron, on the other hand, changed from 90.98/85.64 to 90.98/85.90 without BPM initialization." ></td>
	<td class="line x" title="207:389	This result was unexpected from the result of our preliminary experiment." ></td>
	<td class="line x" title="208:389	However, the performance was changed from 91.06/86.24 to 10http://www.cs.utah.edu/hal/TagChunk/ 321 Table 6: Comparison with re-ranking approach." ></td>
	<td class="line x" title="209:389	Method dev test C local features Margin Perceptron 91.06 86.24 32000 + non-local features Re-ranking 1 (n = 100) 91.62 86.57 4000 Re-ranking 1 (n = 80) 91.71 86.58 4000 Re-ranking 2 (n = 100) 92.08 86.86 16000 Re-ranking 2 (n = 800) 92.26 86.95 16000 Proposed (n = 100) 92.23 87.04 5657 Proposed (n = 6400) 92.54 87.17 5657 Table 7: Comparison of training time (C = 5657)." ></td>
	<td class="line x" title="210:389	Method dev test time (sec.)" ></td>
	<td class="line x" title="211:389	local features Margin Perceptron 91.04 86.28 15,977 + non-local features Re-ranking 1 (n = 100) 91.48 86.53 86,742 Re-ranking 2 (n = 100) 92.02 86.85 112,138 Proposed (n = 100) 92.23 87.04 28,880 91.17/86.08 (i.e. , dropped for the evaluation set as expected), in the setting of the experiment of Table 5." ></td>
	<td class="line x" title="212:389	Since the effect of BPM initialization is not conclusive only from these results, we need more experiments on this." ></td>
	<td class="line x" title="213:389	6.3 Comparison with re-ranking approach Finally, we compared our algorithm with the reranking approach (Collins and Duffy, 2002; Collins, 2002b), where we rst generate the n-best candidates using a model with only local features (the rst model) and then re-rank the candidates using a model with non-local features (the second model)." ></td>
	<td class="line x" title="214:389	We implemented two re-ranking models, reranking 1 and re-ranking 2." ></td>
	<td class="line x" title="215:389	These models differ in how to incorporate the local information in the second model." ></td>
	<td class="line x" title="216:389	re-ranking 1 uses the score of the rst model as a feature in addition to the non-local features as in Collins (2002b)." ></td>
	<td class="line x" title="217:389	re-ranking 2 uses the same local features as the rst model11 in addition to the non-local features." ></td>
	<td class="line x" title="218:389	The rst models were trained using the margin perceptron algorithm in Algorithm 3.1." ></td>
	<td class="line x" title="219:389	The second models were trained using the algorithm, which is obtained by replacing {yn} with the n-best candidates by the rst model." ></td>
	<td class="line x" title="220:389	The rstmodelusedtogeneraten-bestcandidatesforthe development set and the test set was trained using the whole training data." ></td>
	<td class="line x" title="221:389	However, CRFs or perceptrons generally have nearly zero error on the training data, although the rst model should mis-label 11The weights were re-trained for the second model." ></td>
	<td class="line x" title="222:389	to some extent to make the training of the second model meaningful." ></td>
	<td class="line x" title="223:389	To avoid this problem, we adopt cross-validation training as used in Collins (2002b)." ></td>
	<td class="line x" title="224:389	Wesplitthetrainingdatainto5sets." ></td>
	<td class="line x" title="225:389	Wethentrained ve rst models using 4/5 of the data, each of which was used to generate n-best candidates for the remaining 1/5 of the data." ></td>
	<td class="line x" title="226:389	As in the previous experiments, we tuned C using the development set with n = 100 and then tested othervaluesforn." ></td>
	<td class="line x" title="227:389	Table6showstheresults." ></td>
	<td class="line x" title="228:389	Ascan be seen, re-ranking models were outperformed by our proposed algorithm, although they also outperformed the margin perceptron with only local features (re-ranking 2 seems better than re-ranking 1)." ></td>
	<td class="line x" title="229:389	Table 7 shows the training time of each algorithm.12 Our algorithm is much faster than the reranking approach that uses cross-validation training, while achieving the same or higher level of performance." ></td>
	<td class="line x" title="230:389	7 Discussion As we mentioned, there are some algorithms similar to ours (Collins and Roark, 2004; Daume III and Marcu, 2005; McDonald and Pereira, 2006; Liang et al. , 2006)." ></td>
	<td class="line x" title="231:389	The differences of our algorithm from these algorithms are as follows." ></td>
	<td class="line x" title="232:389	Daume III and Marcu (2005) presented the method called LaSO (Learning as Search Optimization), in which intractable exact inference is approximated by optimizing the behavior of the search process." ></td>
	<td class="line x" title="233:389	The method can access non-local features at each search point, if their values can be determinedfromthesearchdecisionsalreadymade." ></td>
	<td class="line x" title="234:389	They provided robust training algorithms with guaranteed convergence for this framework." ></td>
	<td class="line x" title="235:389	However, a difference is that our method can use non-local features whose value depends on all labels throughout training, and it is unclear whether the features whose values can only be determined at the end of the search (e.g. , majority features) can be learned effectively with such an incremental manner of LaSO." ></td>
	<td class="line x" title="236:389	The algorithm proposed by McDonald and Pereira (2006) is also similar to ours." ></td>
	<td class="line x" title="237:389	Their target was non-projective dependency parsing, where exact inference is intractable." ></td>
	<td class="line x" title="238:389	Instead of using 12Training time was measured on a machine with 2.33 GHz QuadCore Intel Xeons and 8 GB of memory." ></td>
	<td class="line x" title="239:389	C was xed to 5657." ></td>
	<td class="line x" title="240:389	322 n-best/re-scoring approach as ours, their method modies the single best projective parse, which can be found efciently, to nd a candidate with higher score under non-local features." ></td>
	<td class="line x" title="241:389	Liang et al.(2006) used n candidates of a beam search in the Collins perceptron algorithm for machine translation." ></td>
	<td class="line x" title="243:389	CollinsandRoark(2004)proposedanapproximate incremental method for parsing." ></td>
	<td class="line x" title="244:389	Their method can be used for sequence labeling as well." ></td>
	<td class="line x" title="245:389	These studies, however, did not explain the validity of their updating methods in terms of convergence." ></td>
	<td class="line x" title="246:389	To achieve robust training, Daume III and Marcu (2005) employed the averaged perceptron (Collins, 2002a) and ALMA (Gentile, 2001)." ></td>
	<td class="line x" title="247:389	Collins and Roark (2004) used the averaged perceptron (Collins, 2002a)." ></td>
	<td class="line x" title="248:389	McDonald and Pereira (2006) used MIRA (Crammer et al. , 2006)." ></td>
	<td class="line x" title="249:389	On the other hand, we employed the margin perceptron (Krauth and Mezard, 1987),extendingittosequencelabeling." ></td>
	<td class="line x" title="250:389	Wedemonstrated that this greatly improved robustness." ></td>
	<td class="line x" title="251:389	With regard to the local update, (B), in Algorithm 4.2, early updates (Collins and Roark, 2004) and y-good requirement in (Daume III and Marcu, 2005) resemble our local update in that they tried to avoid the situation where the correct answer cannot be output." ></td>
	<td class="line x" title="252:389	Considering such commonality, the way of combining the local update and the non-local updatemightbeoneimportantkeyforfurtherimprovement." ></td>
	<td class="line x" title="253:389	It is still open whether these differences are advantages or disadvantages." ></td>
	<td class="line x" title="254:389	However, we think our algorithm can be a contribution to the study for incorporating non-local features." ></td>
	<td class="line x" title="255:389	The convergence guarantee is important for the condence in the training results, although it does not mean high performance directly." ></td>
	<td class="line x" title="256:389	Our algorithm could at least improve the accuracy of NER with non-local features and it was indicated that our algorithm was superior to the re-ranking approach in terms of accuracy and training cost." ></td>
	<td class="line pc" title="257:389	However, the achieved accuracy was not better than that of related work (Finkel et al. , 2005; Krishnan and Manning, 2006) based on CRFs." ></td>
	<td class="line x" title="258:389	Although this might indicate the limitation of perceptron-based methods, it has also been shown that there is still room for improvement in perceptron-based algorithms as our margin perceptron algorithm demonstrated." ></td>
	<td class="line x" title="259:389	8 Conclusion In this paper, we presented a new perceptron algorithm for learning with non-local features." ></td>
	<td class="line x" title="260:389	We think the proposed algorithm is an important step towards achieving our nal objective." ></td>
	<td class="line x" title="261:389	We would like to investigate various types of new non-local features using the proposed algorithm in future work." ></td>
	<td class="line x" title="262:389	Appendix A: Convergence of Algorithm 4.2 Letk be a weight vector before the kth update and epsilon1k be a variable that takes 1 when the kth update is done in (A) and 0 when done in (B)." ></td>
	<td class="line x" title="263:389	The update rule can then be written ask+1 = k +epsilon1k(aa + (1epsilon1k)(l l).13 First, we obtain k+1 Ul = k Ul + epsilon1k(a Ul a Ul) +(1epsilon1k)(l Ul l Ul)  k Ul + epsilon1k + (1epsilon1k) = k Ul +   1 Ul + k = k Therefore, (k)2  (k+1  Ul)2  (||k+1||||Ul||)2 = ||k+1||2  (1)." ></td>
	<td class="line x" title="264:389	On the other hand, we also obtain ||k+1||2  ||k||2 + 2epsilon1kk(a a) +2(1epsilon1k)k(l l) +{epsilon1k(a a) + (1epsilon1k)(l l)}2  ||k||2 + 2C + R2  ||1||2 + k(R2 + 2C) = k(R2 + 2C) (2) We used k(a  a)  Ca, k(l  l)  Cl and Cl = Ca = C to derive 2C in the second inequality." ></td>
	<td class="line x" title="265:389	We used||ll||  ||aa||  R to derive R2." ></td>
	<td class="line x" title="266:389	Combining (1) and (2), we obtain k  (R2 + 2C)/2." ></td>
	<td class="line x" title="267:389	Substituting this into (2) gives ||k||  (R2+2C)/." ></td>
	<td class="line x" title="268:389	Sincey = y andaa > C after convergence, we obtain () = minx i a a  ||||  C/(2C + R 2)." ></td>
	<td class="line x" title="269:389	13We use the shorthand a = a(xi,y i),  a = a(xi,y), l = l(xi,yi), and l = l(xi,y) where y represents the candidate used to update (y, y, y1, or y2)." ></td>
	<td class="line x" title="270:389	323 References D. M. Bikel, R. L. Schwartz, and R. M. Weischedel." ></td>
	<td class="line x" title="271:389	1999." ></td>
	<td class="line x" title="272:389	An algorithm that learns whats in a name." ></td>
	<td class="line x" title="273:389	Machine Learning, 34(1-3):211231." ></td>
	<td class="line x" title="274:389	R. Bunescu and R. J. Mooney." ></td>
	<td class="line x" title="275:389	2004." ></td>
	<td class="line x" title="276:389	Collective information extraction with relational markov networks." ></td>
	<td class="line x" title="277:389	In ACL 2004." ></td>
	<td class="line x" title="278:389	S. F. Chen and R. Rosenfeld." ></td>
	<td class="line x" title="279:389	2000." ></td>
	<td class="line x" title="280:389	A survey of smoothing techniques for ME models." ></td>
	<td class="line x" title="281:389	IEEE Transactions on Speech and Audio Processing, 8(1):3750." ></td>
	<td class="line x" title="282:389	M. Collins and N. Duffy." ></td>
	<td class="line x" title="283:389	2002." ></td>
	<td class="line x" title="284:389	New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron." ></td>
	<td class="line x" title="285:389	In ACL 2002, pages 263270." ></td>
	<td class="line x" title="286:389	M.Collins and B.Roark." ></td>
	<td class="line x" title="287:389	2004." ></td>
	<td class="line x" title="288:389	Incremental parsing with the perceptron algorithm." ></td>
	<td class="line x" title="289:389	In ACL 2004." ></td>
	<td class="line x" title="290:389	M. Collins." ></td>
	<td class="line x" title="291:389	2002a." ></td>
	<td class="line x" title="292:389	Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms." ></td>
	<td class="line x" title="293:389	In EMNLP 2002." ></td>
	<td class="line x" title="294:389	M. Collins." ></td>
	<td class="line x" title="295:389	2002b." ></td>
	<td class="line x" title="296:389	Ranking algorithms for named-entity extraction: Boosting and the voted perceptron." ></td>
	<td class="line x" title="297:389	In ACL 2002." ></td>
	<td class="line x" title="298:389	K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer." ></td>
	<td class="line x" title="299:389	2006." ></td>
	<td class="line x" title="300:389	Online passive-aggressive algorithms." ></td>
	<td class="line x" title="301:389	Journal of Machine Learning Research." ></td>
	<td class="line x" title="302:389	H. Daume III and D. Marcu." ></td>
	<td class="line x" title="303:389	2005." ></td>
	<td class="line x" title="304:389	Learning as search optimization: Approximate large margin methods for structured prediction." ></td>
	<td class="line x" title="305:389	In ICML 2005." ></td>
	<td class="line x" title="306:389	J. R. Finkel, T. Grenager, and C. Manning." ></td>
	<td class="line x" title="307:389	2005." ></td>
	<td class="line x" title="308:389	Incorporating non-local informationin to information extraction systems by Gibbs sampling." ></td>
	<td class="line x" title="309:389	In ACL 2005." ></td>
	<td class="line x" title="310:389	C. Gentile." ></td>
	<td class="line x" title="311:389	2001." ></td>
	<td class="line x" title="312:389	A new approximate maximal margin classication algorithm." ></td>
	<td class="line x" title="313:389	JMLR, 3." ></td>
	<td class="line x" title="314:389	R. Herbrich and T. Graepel." ></td>
	<td class="line x" title="315:389	2000." ></td>
	<td class="line x" title="316:389	Large scale Bayes point machines." ></td>
	<td class="line x" title="317:389	In NIPS 2000." ></td>
	<td class="line x" title="318:389	W. Krauth and M. Mezard." ></td>
	<td class="line x" title="319:389	1987." ></td>
	<td class="line x" title="320:389	Learning algorithms with optimal stability in neural networks." ></td>
	<td class="line x" title="321:389	Journal of Physics A 20, pages 745752." ></td>
	<td class="line x" title="322:389	V. Krishnan and C. D. Manning." ></td>
	<td class="line x" title="323:389	2006." ></td>
	<td class="line x" title="324:389	An effective two-stage model for exploiting non-local dependencies in named entity recognitioin." ></td>
	<td class="line x" title="325:389	In ACL-COLING 2006." ></td>
	<td class="line x" title="326:389	J. Lafferty, A. McCallum, and F. Pereira." ></td>
	<td class="line x" title="327:389	2001." ></td>
	<td class="line x" title="328:389	Conditional random elds: Probabilistic models for segmenting and labeling sequence data." ></td>
	<td class="line x" title="329:389	In ICML 2001, pages 282289." ></td>
	<td class="line x" title="330:389	Y. Li, H. Zaragoza, R. Herbrich, J. Shawe-Taylor, and J. Kandola." ></td>
	<td class="line x" title="331:389	2002." ></td>
	<td class="line x" title="332:389	The perceptron algorithm with uneven margins." ></td>
	<td class="line x" title="333:389	In ICML 2002." ></td>
	<td class="line x" title="334:389	P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar." ></td>
	<td class="line x" title="335:389	2006." ></td>
	<td class="line x" title="336:389	An end-to-end discriminative approach to machine translation." ></td>
	<td class="line x" title="337:389	In ACL-COLING 2006." ></td>
	<td class="line x" title="338:389	R. McDonald and F. Pereira." ></td>
	<td class="line x" title="339:389	2006." ></td>
	<td class="line x" title="340:389	Online learning of approximate dependency parsing algorithms." ></td>
	<td class="line x" title="341:389	In EACL 2006." ></td>
	<td class="line x" title="342:389	R. McDonald, K. Crammer, and F. Pereira." ></td>
	<td class="line x" title="343:389	2005." ></td>
	<td class="line x" title="344:389	Online large-margin training of dependency parsers." ></td>
	<td class="line x" title="345:389	In ACL 2005." ></td>
	<td class="line x" title="346:389	T. Nakagawa and Y. Matsumoto." ></td>
	<td class="line x" title="347:389	2006." ></td>
	<td class="line x" title="348:389	Guessing parts-of-speech of unknown words using global information." ></td>
	<td class="line x" title="349:389	In ACL-COLING 2006." ></td>
	<td class="line x" title="350:389	L. R. Rabiner." ></td>
	<td class="line x" title="351:389	1989." ></td>
	<td class="line x" title="352:389	A tutorial on hidden Markov models and selected applications in speech recognition." ></td>
	<td class="line x" title="353:389	Proceedings of the IEEE, 77(2):257286." ></td>
	<td class="line x" title="354:389	L. A. Ramshaw and M. P. Marcus." ></td>
	<td class="line x" title="355:389	1995." ></td>
	<td class="line x" title="356:389	Text chunking using transformation-based learning." ></td>
	<td class="line x" title="357:389	In third ACL Workshop on very large corpora." ></td>
	<td class="line x" title="358:389	F. Rosenblatt." ></td>
	<td class="line x" title="359:389	1958." ></td>
	<td class="line x" title="360:389	The perceptron: A probabilistic model for information storage and organization in the brain." ></td>
	<td class="line x" title="361:389	Psycological Review, pages 386407." ></td>
	<td class="line x" title="362:389	D. Roth and W. Yih." ></td>
	<td class="line x" title="363:389	2005." ></td>
	<td class="line x" title="364:389	Integer linear programming inference for conditional random elds." ></td>
	<td class="line x" title="365:389	InICML 2005." ></td>
	<td class="line x" title="366:389	S. Sarawagi and W. W. Cohen." ></td>
	<td class="line x" title="367:389	2004." ></td>
	<td class="line x" title="368:389	Semi-Markov random elds for information extraction." ></td>
	<td class="line x" title="369:389	In NIPS 2004." ></td>
	<td class="line x" title="370:389	L. Shen and A. K. Joshi." ></td>
	<td class="line x" title="371:389	2004." ></td>
	<td class="line x" title="372:389	Flexible margin selection for reranking with full pairwise samples." ></td>
	<td class="line x" title="373:389	In IJCNLP 2004." ></td>
	<td class="line x" title="374:389	F. K. Soong and E. Huang." ></td>
	<td class="line x" title="375:389	1991." ></td>
	<td class="line x" title="376:389	A tree-trellis based fast search for nding the n best sentence hypotheses in continuous speech recognition." ></td>
	<td class="line x" title="377:389	In ICASSP-91." ></td>
	<td class="line x" title="378:389	C. Sutton and A. McCallum." ></td>
	<td class="line x" title="379:389	2004." ></td>
	<td class="line x" title="380:389	Collective segmenation and labeling of distant entitites in information extraction." ></td>
	<td class="line x" title="381:389	University of Massachusetts Technical Report TR 04-49." ></td>
	<td class="line x" title="382:389	B. Taskar, C. Guestrin, and D. Koller." ></td>
	<td class="line x" title="383:389	2003." ></td>
	<td class="line x" title="384:389	Max-margin Markov networks." ></td>
	<td class="line x" title="385:389	In NIPS 2003." ></td>
	<td class="line x" title="386:389	E. F. Tjong, K. Sang, and F. De Meulder." ></td>
	<td class="line x" title="387:389	2003." ></td>
	<td class="line x" title="388:389	Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition." ></td>
	<td class="line x" title="389:389	In CoNLL 2003." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-2058
PSNUS: Web People Name Disambiguation by Simple Clustering with Rich Features
Elmacioglu, Ergin;Tan, Yee Fan;Yan, Su;Kan, Min-Yen;Lee, Dongwon;"></td>
	<td class="line x" title="1:108	Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 268271, Prague, June 2007." ></td>
	<td class="line x" title="2:108	c2007 Association for Computational Linguistics PSNUS: Web People Name Disambiguation by Simple Clustering with Rich Features Ergin Elmacioglu1 Yee Fan Tan2 Su Yan1 Min-Yen Kan2 Dongwon Lee1 1The Pennsylvania State University, USA 2National University of Singapore, Singapore {ergin,syan,dongwon}@psu.edu, {tanyeefa,kanmy}@comp.nus.edu.sg Abstract We describe about the system description of the PSNUS team for the SemEval-2007 Web People Search Task." ></td>
	<td class="line x" title="3:108	The system is based on the clustering of the web pages by using a variety of features extracted and generated from the data provided." ></td>
	<td class="line x" title="4:108	This system achieves F=0.5 = 0.75 and F=0.2 = 0.78 for the final test data set of the task." ></td>
	<td class="line x" title="5:108	1 Introduction We consider the problem of disambiguating person names in a Web searching scenario as described by the Web People Search Task in SemEval 2007 (Artiles et al. , 2007)." ></td>
	<td class="line x" title="6:108	Here, the system receives as input a set of web pages retrieved from a search engine using a given person name as a query." ></td>
	<td class="line x" title="7:108	The goal is to determine how many different people are represented for that name in the input web pages, and correctly assign each namesake to its corresponding subset of web pages." ></td>
	<td class="line x" title="8:108	There are many challenges towards an effective solution." ></td>
	<td class="line x" title="9:108	We are to correctly estimate the number of namesakes for a given person name and group documents referring to the same individual." ></td>
	<td class="line x" title="10:108	Moreover, the information sources to be processed are unstructured web pages and there is no certain way of correctly establishing a relation between any two web pages belonging to the same or different individuals." ></td>
	<td class="line x" title="11:108	We have taken several approaches to analyze different sources of information provided with the input data, and also compared strategies to combine these individual features together." ></td>
	<td class="line x" title="12:108	The configuration that achieved the best performance (which were submitted for our run) used a single named entity feature as input to clustering." ></td>
	<td class="line x" title="13:108	In the remainder of this paper, we first describe our system in terms of the clustering approach used and alternative features investigated." ></td>
	<td class="line x" title="14:108	We then analyze the results on the training set before concluding the paper." ></td>
	<td class="line x" title="15:108	2 Clustering Algorithm Clustering is the key part for such a task." ></td>
	<td class="line x" title="16:108	We have chosen to view the problem as an unsupervised hard clustering problem." ></td>
	<td class="line x" title="17:108	First, we view the problem as unsupervised, using the training data for parameter validation, to optimally tune the parameters in the clustering algorithm." ></td>
	<td class="line x" title="18:108	Secondly, we observed that the majority of the input pages reference a single individual, although there are a few that reference multiple individuals sharing the same name." ></td>
	<td class="line x" title="19:108	Hence, we view the problem as hard clustering, assigning input pages to exactly one individual, so that the produced clusters do not overlap." ></td>
	<td class="line x" title="20:108	Hard clustering algorithms can be classified as either partitive or hierarchical." ></td>
	<td class="line x" title="21:108	Agglomerative hierarchical clustering generates a series of nested clusters by merging simple clusters into larger ones, while partitive methods try to find a pre-specified number of clusters that best capture the data." ></td>
	<td class="line x" title="22:108	As the correct number of clusters is not given a priori, we chose a method from the second group." ></td>
	<td class="line x" title="23:108	We use the Hierarchical Agglomerative Clustering (HAC) algorithm (Jain et al. , 1999) for all experiments reported in this paper." ></td>
	<td class="line x" title="24:108	HAC views each input web page as a separate cluster and iteratively combines the most similar pair of clusters to form a new cluster that re268 places the pair." ></td>
	<td class="line x" title="25:108	3 Features As input to the clustering, we consider several different representations of the input documents." ></td>
	<td class="line x" title="26:108	Each representation views the input web pages as a vector of features." ></td>
	<td class="line x" title="27:108	HAC then computes the cosine similarity between the feature vectors for each pair of clusters to determine which clusters to merge." ></td>
	<td class="line x" title="28:108	We now review the inventory of features studied in our work." ></td>
	<td class="line x" title="29:108	Tokens (T)." ></td>
	<td class="line x" title="30:108	Identical to the task baseline by (Artiles et al. , 2005), we stemmed the words in the web pages using the Porter stemmer (Porter, 1980), to conflate semantically similar English words with the stem." ></td>
	<td class="line x" title="31:108	Each stemmed word is considered to be a feature and weighted by its Term Frequency  Inverse Document Frequency (TFIDF)." ></td>
	<td class="line x" title="32:108	Named Entities (NE)." ></td>
	<td class="line oc" title="33:108	We extract the named entities from the web pages using the Stanford Named Entity Recognizer (Finkel et al. , 2005)." ></td>
	<td class="line o" title="34:108	This tagger identifies and labels names of places, organizations and people in the input." ></td>
	<td class="line x" title="35:108	Each named entity token is treated as a separate feature, again weighted by TFIDF." ></td>
	<td class="line x" title="36:108	We do not perform stemming for NE features." ></td>
	<td class="line x" title="37:108	We also consider a more target-centric form of the NE feature, motivated by the observation that person names can be differentiated using their middle names or titles." ></td>
	<td class="line x" title="38:108	We first discard all named entities that do not contain any token of the search target, and then discard any token from the remaining named entities that appears in the search target." ></td>
	<td class="line x" title="39:108	The remaining tokens are then used as features, and weighted by their TFIDF." ></td>
	<td class="line x" title="40:108	For example, for the search target Edward Fox, the features generated from the name Edward Charles Morrice Fox are Charles and Morrice." ></td>
	<td class="line x" title="41:108	We call this variation NE targeted (NE-T)." ></td>
	<td class="line x" title="42:108	Hostnames and domains (H and D)." ></td>
	<td class="line x" title="43:108	If two web pages have links pointing to the exact same URL, then there is a good chance that these two web pages refer the same person." ></td>
	<td class="line x" title="44:108	However, we find such exact matches of URLs are rare, so we relax the condition and consider their hostnames or domain names instead." ></td>
	<td class="line x" title="45:108	For example, the URL http://portal.acm.org/guide.cfm has hostname portal.acm.org and domain name acm.org." ></td>
	<td class="line x" title="46:108	As such, for each web page, we can extract the list of hostnames from the links in this page." ></td>
	<td class="line x" title="47:108	We observe that some host/domain names serve as more discriminative evidence than others (e.g. , a link to a university homepage is more telling than a link to the list of publications page of Google Scholar when disambiguating computer science scholars)." ></td>
	<td class="line x" title="48:108	To model this, we weight each host/domain name by its IDF." ></td>
	<td class="line x" title="49:108	Note that we do not use TF as web pages often contain multiple internal links in the form of menus or navigation bars." ></td>
	<td class="line x" title="50:108	Using IDF and cosine similarity has been proven effective for disambiguating bibliographic citation records sharing a common author name (Tan et al. , 2006)." ></td>
	<td class="line x" title="51:108	We also considered a variant where we include the URL of the input web page itself as a link." ></td>
	<td class="line x" title="52:108	We tried this variation only with hostnames, calling this Host with Self URL (H-S)." ></td>
	<td class="line x" title="53:108	Page URLs (U)." ></td>
	<td class="line x" title="54:108	Uniform resource locations (URLs) themselves contain a rich amount of information." ></td>
	<td class="line x" title="55:108	For example, the URL http://www.cs.ualberta.ca/lindek/ itself suggests a home page of lindek in the Computer Science department, University of Alberta, Canada." ></td>
	<td class="line x" title="56:108	We used the MeURLin system (Kan and Nguyen Thi, 2005) to segment the URL of each web page into tokens as well as to generate additional features." ></td>
	<td class="line x" title="57:108	These features include (a) segmentation of tokens such as www.allposters.com to www, all, posters and com; (b) the parts in the URL where the tokens occur, e.g., protocol, domain name, and directory paths; (c) length of the tokens; (d) orthographic features; (e) sequential n-grams; and (f) sequential bigrams." ></td>
	<td class="line x" title="58:108	As each of these features can be seen as a token, the output of the MeURLin segmenter for a web page can be seen as a document, and hence it is possible to compute the TFIDF cosine similarity between two such documents." ></td>
	<td class="line x" title="59:108	3.1 Feature Combination The features described above represent largely orthogonal sources of information in the input: input content, hyperlinks, and source location." ></td>
	<td class="line x" title="60:108	We hypothesize that by combining these different features we can obtain better performance." ></td>
	<td class="line x" title="61:108	To combine these features for use with HAC, we consider simply concatenating individual feature vectors together to cre269 ate a single feature vector, and compute cosine similarity." ></td>
	<td class="line x" title="62:108	We used this method in two configurations: namely, (T + NE + H-S), (T + D + NE + NE-T + U)." ></td>
	<td class="line x" title="63:108	We also tried using the maximum and average component-wise similarities of individual features." ></td>
	<td class="line x" title="64:108	(max(NE, H-S)) uses the maximum value of the Named Entity and Host with Self features." ></td>
	<td class="line x" title="65:108	For the (avg(T, H-S)) and (avg(T, D, NE, NE-T, U)) runs, we compute the average similarity over the two and five sets of individual features, respectively." ></td>
	<td class="line x" title="66:108	4 Results We present the clustering performances of the various methods in our system based on the different features that we extracted." ></td>
	<td class="line x" title="67:108	Each experiment uses HAC with single linkage clustering." ></td>
	<td class="line x" title="68:108	Since the number of clusters is not known, when to terminate the agglomeration process is a crucial point and significantly affects the quality of the clustering result." ></td>
	<td class="line x" title="69:108	We empirically determine the best similarity thresholds to be 0.1 and 0.2 for all the experiments on the three different data sets provided." ></td>
	<td class="line x" title="70:108	We found that larger values for these data sets do not allow the HAC algorithm to create enough clustering hierarchy by causing it to terminate early, and therefore result in many small clusters increasing purity but dramatically suffering from inverse purity performance." ></td>
	<td class="line x" title="71:108	Table 1 shows the results of our experiments on the training data sets (ECDL, Wikipedia and Census)." ></td>
	<td class="line x" title="72:108	Two different evaluation measures are reported as described by the task: F=0.5 is a harmonic mean of purity and inverse purity of the clustering result, and F=0.2 is a version of F that gives more importance to inverse purity (Artiles et al. , 2007)." ></td>
	<td class="line x" title="73:108	Among the individual features, Tokens and Named Entity features consistently show close to best performance for all training data sets." ></td>
	<td class="line x" title="74:108	In most cases, NE is better than Tokens because some web pages contain lots of irrelevant text for this task (e.g. , headers and footers, menus etc)." ></td>
	<td class="line x" title="75:108	Also, we found that the NEs have far more discriminative power than most other tokens in determining similarity between web pages." ></td>
	<td class="line x" title="76:108	The NE variation, NE targeted, performs worse among the token based methods." ></td>
	<td class="line x" title="77:108	Although NE targeted aims for highly precise disambiguation, it seems that it throws away too much information so that inverse purity is very much reduced." ></td>
	<td class="line x" title="78:108	The other NEs, such as locations and organizations are also very helpful for this task." ></td>
	<td class="line x" title="79:108	For example, the organization may indicate the affiliation of a particular name." ></td>
	<td class="line x" title="80:108	This explains the superiority of NE over NE targeted for all three data sets." ></td>
	<td class="line x" title="81:108	Among the link based features, Domain gives better performance over Host as it leads to better inverse purity." ></td>
	<td class="line x" title="82:108	The reason is that there are usually many pages on different hosts from a single domain for a given name (e.g. , the web pages belonging to a researcher from university domain)." ></td>
	<td class="line x" title="83:108	This greatly helps in resolving the name while results in a slight drop in purity." ></td>
	<td class="line x" title="84:108	Using a web pages URL itself in the features Host+Self and Domain+Self shows a larger increase in inverse purity at a smaller decrease in purity, hence these have improved F-measure in comparison to Domain and Host." ></td>
	<td class="line x" title="85:108	Not surprisingly, these link based features perform very well for the ECDL data set, compared to the other two." ></td>
	<td class="line x" title="86:108	A significant portion of the people in the ECDL data set are most likely present-day computer scientists, likely having extensive an web presence, which makes the task much easier." ></td>
	<td class="line x" title="87:108	Although the other two data sets may have popular people with many web pages, their web presence are usually created by others and often scatter across many domains with little hyperlinkage between them." ></td>
	<td class="line x" title="88:108	This explains why our link based methods are not very effective for such data sets." ></td>
	<td class="line x" title="89:108	Our final individual feature URL performs worst among all." ></td>
	<td class="line x" title="90:108	Although highly precise, its resulting inverse purity is poor." ></td>
	<td class="line x" title="91:108	While the features generated by MeURLin do improve the performance over pure host name and domain on the page URLs, its incorporation in a richer feature set does not lead to better results, as the other features which have richer information to process." ></td>
	<td class="line x" title="92:108	Each of the individual features has different degree of discriminative power in many different cases." ></td>
	<td class="line x" title="93:108	By combining them, we expect to get better performance than individually." ></td>
	<td class="line x" title="94:108	However, we do not obtain significant improvement in any of the data sets." ></td>
	<td class="line x" title="95:108	Furthermore, in the Census data set, the combined features fail to outperform the individual NE and Tokens features." ></td>
	<td class="line x" title="96:108	The relatively poor performance of the remaining features also degrades the performance of Tokens and NE when combined." ></td>
	<td class="line x" title="97:108	Considering the performances using the harmonic mean, we do not see any clear winner in all of three 270 Feature ECDL Wikipedia CensusF =0.5 F=0.2 F=0.5 F=0.2 F=0.5 F=0.2 Tokens (T).72 / .77 .83 / .84 .72 / .76 .85 / .84 .82 / .84 .88 / .86 Named Entities (NE) .75 / .80 .84 / .79 .75 / .77 .85 / .78 .89 / .78 .89 / .73 NE targeted (NE-T) .54 / .55 .49 / .47 .66 / .64 .60 / .57 .64 / .64 .57 / .58 Host (H) .72 / .57 .64 / .48 .67 / .51 .58 / .41 .67 / .63 .59 / .55 Host + Self (H-S) .73 / .59 .66 / .49 .68 / .54 .60 / .43 .68 / .63 .60 / .56 Domain (D) .78 / .69 .72 / .60 .71 / .59 .66 / .50 .69 / .65 .61 / .58 Domain + Self (D-S) .79 / .70 .74 / .61 .72 / .62 .67 / .52 .70 / .66 .62 / .59 URL (U) .50 / .43 .43 / .35 .56 / .42 .50 / .33 .64 / .58 .56 / .51 (T + NE + H-S) .71 / .77 .83 / .83 .72 / .76 .85 / .83 .65 / .67 .78 / .76 (T + D + NE + NE-T + U) .72 / .76 .83 / .80 .72 / .77 .84 / .83 .66 / .66 .78 / .74 (max(NE, H-S)) .74 / .80 .84 / .82 .74 / .77 .86 / .82 .71 / .66 .80 / .70 (avg(T, H-S)) .77 / .81 .86 / .76 .75 / .77 .86 / .76 .70 / .64 .80 / .67 (avg(T, D, NE, NE-T, U)) .78 / .77 .86 / .73 .75 / .78 .86 / .76 .69 / .61 .77 / .62 Table 1: Experimental results for each training data set of the task: ECDL, Wikipedia and Census." ></td>
	<td class="line x" title="98:108	Each experiment uses single link HAC with the similarity threshold values of 0.1 / 0.2." ></td>
	<td class="line x" title="99:108	Best F=0.5 performances are shown in bold." ></td>
	<td class="line x" title="100:108	training data sets." ></td>
	<td class="line x" title="101:108	In addition, the method showing the best performance does not result in a win with a large margin in each data set." ></td>
	<td class="line x" title="102:108	Relatively complicated methods do not always perform better over simpler, single featured based methods on all training data sets." ></td>
	<td class="line x" title="103:108	Considering the results and Occams razor (Thorburn, 1915), we conclude that a simple method should most likely work relatively well in many other different settings as well." ></td>
	<td class="line x" title="104:108	Therefore, we selected the method based on the individual NE feature with the similarity threshold value of 0.2 for the final test submission run." ></td>
	<td class="line x" title="105:108	We are able to achieve the following results for this submission run: purity = 0.73, inverse purity = 0.82, F=0.5 = 0.75, F=0.2 = 0.78." ></td>
	<td class="line x" title="106:108	5 Conclusion We described our PSNUS system that disambiguates people mentions in web pages returned by a web search scenario, as defined in the inaugural Web People Search Task." ></td>
	<td class="line x" title="107:108	As such, we mainly focus on extracting various kinds of information from web pages and utilizing them in the similarity computation of the clustering algorithm." ></td>
	<td class="line x" title="108:108	The experimental results show that a simple Hierarchical Agglomerative Clustering approach using a single named entity feature seems promising as a robust solution for the various datasets." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1061
Classifying What-Type Questions by Head Noun Tagging
Li, Fangtao;Zhang, Xian;Yuan, Jinhui;Zhu, Xiaoyan;"></td>
	<td class="line x" title="1:297	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 481488 Manchester, August 2008 Classifying What-type Questions by Head Noun Tagging Fangtao Li, Xian Zhang, Jinhui Yuan, Xiaoyan Zhu State Key Laboratory on Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology Department of Computer Sci." ></td>
	<td class="line x" title="2:297	and Tech., Tsinghua University, Beijing 100084, China zxy-dcs@tsinghua.edu.cn  Abstract Classifying what-type questions into proper semantic categories is found more challenging than classifying other types in question answering systems." ></td>
	<td class="line x" title="3:297	In this paper, we propose to classify what-type questions by head noun tagging." ></td>
	<td class="line x" title="4:297	The approach highlights the role of head nouns as the category discriminator of whattype questions." ></td>
	<td class="line x" title="5:297	To reduce the semantic ambiguities of head noun, we integrate local syntactic feature, semantic feature and category dependency among adjacent nouns with Conditional Random Fields (CRFs)." ></td>
	<td class="line x" title="6:297	Experiments on standard question classification data set show that the approach achieves state-of-the-art performances." ></td>
	<td class="line x" title="7:297	1 Introduction Question classification is a crucial component of modern question answering system." ></td>
	<td class="line x" title="8:297	It classifies questions into several semantic categories which indicate the expected semantic type of answers to the questions." ></td>
	<td class="line x" title="9:297	The semantic category helps to filter out irrelevant answer candidates, and determine the answer selection strategies." ></td>
	<td class="line x" title="10:297	1  The widely used question category criteria is a two-layered taxonomy developed by Li and Roth (2002) from UIUC." ></td>
	<td class="line x" title="11:297	The hierarchy contains 6 coarse classes and 50 fine classes as shown in Table 1." ></td>
	<td class="line x" title="12:297	In this paper, we focus on fine-category classification." ></td>
	<td class="line x" title="13:297	Each fine category will be denoted as Coarse:fine, such as HUM:individual." ></td>
	<td class="line x" title="14:297	A what-type question is defined as the one whose question word is what, which, name or list." ></td>
	<td class="line x" title="15:297	It is a dominant type in question answering system." ></td>
	<td class="line x" title="16:297	Li and Roth (2006) find   2008." ></td>
	<td class="line x" title="17:297	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/)." ></td>
	<td class="line x" title="18:297	Some rights reserved." ></td>
	<td class="line x" title="19:297	Coarse Fine ABBR abbreviation, expression DESC definition, description, manner, reason ENTY animal, body, color, creation, currency, disease/medicine, event, food, instrument, language, letter, other, plant, product, religion, sport, substance, symbol, technique, term, vehicle, word HUM description, group, individual, title LOC city, country, mountain, other, state NUM code, count, date, distance, money, order, other, percent, period, speed, temperature, size, weight Table 1." ></td>
	<td class="line x" title="20:297	Question Ontology that the distribution of what-type questions over the semantic classes is quite diverse, and they are more difficult to be classified than other types." ></td>
	<td class="line x" title="21:297	Table 2 shows the classification accuracies of each question word in UIUC data set using Support Vector Machine (SVM) with unigram features." ></td>
	<td class="line x" title="22:297	What-type questions account for more than 70 percent in the data set, but the classification accuracy of this type only achieves 75.50%." ></td>
	<td class="line x" title="23:297	In this experiment, 90.53% (86 over 95) of the errors are generated by what-type questions." ></td>
	<td class="line x" title="24:297	Due to its challenge, this paper focuses on what-type question classification." ></td>
	<td class="line x" title="25:297	Total Wrong Accuracy What-type 351 86 75.50% Where 26 2 92.31% When 26 0 100.0% Who 47 3 93.62% How 46 4 91.30% Why 4 0 100.0% Total 500 95 81.00% Table 2." ></td>
	<td class="line x" title="26:297	Classification performance for each question words with unigram Head noun has been presented to  play an important role in classifying what-type questions (Metzler and Croft, 2005)." ></td>
	<td class="line x" title="27:297	It refers to the noun reflecting the focus of a question, such as flower in the question What is Hawaii's state 481 flower?." ></td>
	<td class="line x" title="28:297	These nouns can effectively reduce the noise generated by other words." ></td>
	<td class="line x" title="29:297	If the head noun length is identified from the question What is the length of the coastline of the state of Alaska?, this question can be easily classified into NUM:distance." ></td>
	<td class="line x" title="30:297	However, the above SVM misclassified this question into LOC:-state, as the words state and Alaska confused the classifier." ></td>
	<td class="line x" title="31:297	Considering another two questions expressed in (Zhang and Lee, 2002), Which university did the president graduate from? and Which president is a graduate of the Harvard University, although they contain similar words, it is not difficult to distinguish them with the head nouns university and president respectively." ></td>
	<td class="line x" title="32:297	Nevertheless, a head noun may correspond to several semantic categories." ></td>
	<td class="line x" title="33:297	In this situation, we need to incorporate the head noun context for disambiguation." ></td>
	<td class="line x" title="34:297	The potentially useful context features include local syntactic features, semantic features and neighbors semantic category." ></td>
	<td class="line x" title="35:297	Take the noun money as an example, it possibly corresponds to two categories: NUM:money and ENTY:currency." ></td>
	<td class="line x" title="36:297	If there is an adjacent word falling into Loc:country category, the money tends to belong to ENTY:currency." ></td>
	<td class="line x" title="37:297	Otherwise, if the ENTY:product or HUM:individual surrounds it, the word money may refer to NUM:money." ></td>
	<td class="line x" title="38:297	Based on the above notions, we propose a new strategy to classify what-type questions by word tagging, and the selected head noun determines question category." ></td>
	<td class="line x" title="39:297	The question classification task is formulated into word sequence tagging problem." ></td>
	<td class="line x" title="40:297	All the question words are divided into semantic words and non-semantic words." ></td>
	<td class="line x" title="41:297	The semantic word expresses certain semantic category, such as dog corresponding to category ENTITY:animal, while have corresponding to no category." ></td>
	<td class="line x" title="42:297	The label for semantic words is one of the question categories, and O is for non-semantic word." ></td>
	<td class="line x" title="43:297	Here, we just consider the nouns as semantic words, others as non-semantic words." ></td>
	<td class="line x" title="44:297	Each word in a question will be tagged as a label using Conditional Random Fields model, and the head nouns label is chosen as the question category." ></td>
	<td class="line x" title="45:297	In conclusion, the CRFs based approach has two main steps: the first step is to tag all the words in questions using CRFs, and the second step is choosing the head nouns label as the question category." ></td>
	<td class="line x" title="46:297	It can use the head noun to eliminate the noisy words, and take advantages of CRFs model to integrate not only the syntactic and semantic features, but also the adjacent categories to tag head noun." ></td>
	<td class="line x" title="47:297	The rest of this paper is organized as follows: Section 2 discusses related work." ></td>
	<td class="line x" title="48:297	Section 3 introduces the Condition Random Fields(CRFs) and the defined Long-Dependency CRFs (LDCRFs)." ></td>
	<td class="line x" title="49:297	Section 4 describes the features used in the LDCRFs." ></td>
	<td class="line x" title="50:297	The head noun extraction method is presented in Section 5." ></td>
	<td class="line x" title="51:297	We evaluate the proposed approach in Section 6." ></td>
	<td class="line x" title="52:297	Section 7 concludes this paper and discusses future work." ></td>
	<td class="line x" title="53:297	2 Related works Question Answering Track was first introduced in the Text REtrieval Conference (TREC) in 1999." ></td>
	<td class="line x" title="54:297	Since then, question classification has been a popular topic in the research community of text mining." ></td>
	<td class="line x" title="55:297	Simple question classification approaches usually employ hand-crafted rules (such as Prager et." ></td>
	<td class="line x" title="56:297	al, 1999), which are effective for specific question taxonomy." ></td>
	<td class="line x" title="57:297	However, laborious human effort is required to create these rules." ></td>
	<td class="line x" title="58:297	Some other systems employed machine learning approaches to classify questions." ></td>
	<td class="line x" title="59:297	Li and Roth (2002) presented a hierarchical classifier based on the Sparse Network of Winnows (Snow) architecture." ></td>
	<td class="line x" title="60:297	Tow classifiers were involved in this work: the first one classified questions into the coarse categories; and the other classified questions into fine categories." ></td>
	<td class="line x" title="61:297	Several syntactic and semantic features, including semiautomatically constructed class-specific relational features, were extracted and compared in their experiments." ></td>
	<td class="line x" title="62:297	The results showed that the hierarchical classifier was effective for question classification task." ></td>
	<td class="line x" title="63:297	Metzler and Croft (2005) used prior knowledge about correlations between question words and types to train word-specific question classifiers." ></td>
	<td class="line x" title="64:297	They identified the question words firstly, and trained separate classifier for each question word." ></td>
	<td class="line x" title="65:297	WordNet was used as semantic features to boost the classification performance." ></td>
	<td class="line x" title="66:297	In this paper, according to question word, all the questions are classifie into two categories: what-type ones and non-what-type one." ></td>
	<td class="line x" title="67:297	Recent question classification methods have paid more attention on the syntactic structure of sentence." ></td>
	<td class="line x" title="68:297	They used a parser to get the syntactic tree, and then took advantage of the structure information." ></td>
	<td class="line x" title="69:297	Zhang and Lee (2002) proposed a tree kernel Support Vector Machine classifier and experiment results showed that syntactic information and tree kernel could solve this prob482 lem." ></td>
	<td class="line x" title="70:297	Nguyen et al.(2007) proposed a subtree mining method for question classification." ></td>
	<td class="line x" title="72:297	They formulated question classification as tree category determination, and maximum entropy and boosting model with subtree features were used." ></td>
	<td class="line x" title="73:297	The experiment results showed that the subtree mining method can achieve a higher accuracy in question classification task." ></td>
	<td class="line x" title="74:297	In this paper, we formulate the what-type question classification as word sequence tagging problem." ></td>
	<td class="line x" title="75:297	The tagged label is either one of the question categories for nouns s or O for other words." ></td>
	<td class="line x" title="76:297	Since head noun can be the discriminator for a question, its tag is extracted as the question category in our work." ></td>
	<td class="line x" title="77:297	A long-dependency Conditional Random Fields Classifier is defined to tag question words with the features which not only include the syntactic and semantic features, but also the semantic categories transition features." ></td>
	<td class="line x" title="78:297	3 Conditional Random Fields Conditional Random Fields (CRFs) are a type of discriminative probabilistic model proposed for labeling sequential data (Lafferty et al. 2001)." ></td>
	<td class="line x" title="79:297	Its definition is as follows:  Definition: Let ()GVE=,  be a graph such that () vvV Y  =Y , so that Y  is indexed by the vertices of G . Then (),XY  is a conditional random field in case, when conditioned on X , the random variables v Y  obey the Markov property with respect to the graph: () vw p YYwv|,, =X () vw p YYwv|,,X  , where w v means that w  and v  are neighbors in G .  The joint distribution over the label sequence Y given X  has the form 1 () exp ( ) ( ) () ii i i eEi vVi ptesv Z  , , |= ,|,+ ,|, ,     YX Y X Y X X where ()Z X  is a normalization factor, i s  is a state feature function and i t  is a transition feature function, i   and i   are the corresponding weights." ></td>
	<td class="line x" title="80:297	Here we assume the features are given, then the parameter estimation problem is to determine the parameters 12 1 2 (, )  =, ,,, from training data." ></td>
	<td class="line x" title="81:297	The inference problem is to find the most probable label sequence y  for input sequence x .   In the training set, we label all the noun words with semantic question categories, and other words will be labeled by O." ></td>
	<td class="line x" title="82:297	We suppose that only adjacent noun words connect with each other, and there is no edge between noun and non-noun words, i.e., noun word and non-noun words may share neighbors state features, but they are not connected by an edge." ></td>
	<td class="line x" title="83:297	A labeled example is shown as What/O was/O Queen/HUM:individual Victria/HUM:individual s/O title/HUM:title regarding/O India/LOC:country." ></td>
	<td class="line x" title="84:297	In this labeled sentence, only three edges connect four noun words: Queen, Victria, title and India." ></td>
	<td class="line x" title="85:297	Figure 1." ></td>
	<td class="line x" title="86:297	Long-Dependency CRFs, the dotted lines summarize many outgoing edges   With this assumption, we define a LongDependency Conditional Random Fields (LDCRFs) model (see Figure 1)." ></td>
	<td class="line x" title="87:297	The long dependency means that the target words may have no edge with its neighbors, but connect with other words at a long distance." ></td>
	<td class="line x" title="88:297	It can be considered as a type of linearchain CRFs." ></td>
	<td class="line x" title="89:297	Its parameter estimation problem and inference problem can be solved by the algorithm for chain-structure CRFs (Sutton and McCallum, 2007)." ></td>
	<td class="line x" title="90:297	4 Feature Sets One of the most attractive advantages  of CRFs is that they can integrate rich features, including not only state features, but also transition features." ></td>
	<td class="line x" title="91:297	In this section, we will introduce the syntactic, semantic and transition features used in our sequence tagging approach." ></td>
	<td class="line x" title="92:297	4.1 Syntactic Features The questions, which have similar syntactic style, intend to belong to the same category." ></td>
	<td class="line x" title="93:297	Besides words, part-of-speech, chunker, parser information and question length are used as syntactic features." ></td>
	<td class="line x" title="94:297	All the words are lemmatized to root forms, and a window size (here is 4) is set to utilize the surrounding words." ></td>
	<td class="line x" title="95:297	The part-of-speech (POS) tagging is completed by SS Tagger (Tsuruoka and Tsujii, 2005), with our own improvement." ></td>
	<td class="line x" title="96:297	The noun phrase chunking (NP chunking) module uses the basic NP chunker software from 483 (Ramshaw and Marcus, 1995) to recognize the noun phrases in the question." ></td>
	<td class="line x" title="97:297	The importance of question syntactic structure is reported in (Zhang and Lee, 2002; Nguyen et al. 2007)." ></td>
	<td class="line x" title="98:297	They used complex machine learning method to capture the tree architecture." ></td>
	<td class="line x" title="99:297	The LDCRFs based approach just selects parent node, relation with parent and governor for each target word generated from Minipar(Lin, 1999)." ></td>
	<td class="line x" title="100:297	The length of question is another important syntactic feature." ></td>
	<td class="line x" title="101:297	In our experiment, a threshold is set to denote the length as high or low." ></td>
	<td class="line x" title="102:297	4.2 Semantic Features Semantic features concern what words mean and how these meanings combine in sentence to form sentence meanings." ></td>
	<td class="line x" title="103:297	Named Entity is a predefined semantic category for noun word." ></td>
	<td class="line x" title="104:297	WordNet (Fellbaum, 1998) is a public semantic lexicon for English language, and it is used to get hypernym for noun word and synset for head verb which is the first notional verb in the sentence." ></td>
	<td class="line x" title="105:297	Named Entity: Named entity recognizer assigns a semantic category to the noun phrase." ></td>
	<td class="line x" title="106:297	It is widely used to provide semantic information in text mining." ></td>
	<td class="line oc" title="107:297	In this paper, Stanford Named Entity Recognizer (Finkel et al. 2005) is used to classify noun phrases into four semantic categories: PERSON, LOCATION, ORGANIZARION and MISC." ></td>
	<td class="line x" title="108:297	Noun Hypernym: Hypernyms can be considered as semantic abstractions." ></td>
	<td class="line x" title="109:297	It helps to narrow the gap between training set and testing set." ></td>
	<td class="line x" title="110:297	For example, What is Maryland's state bird?, if we recursively find the birds hypernym animal, which appeared in training set, this question can be easily classified." ></td>
	<td class="line x" title="111:297	In training set, we try to select appropriate hypernyms for each category." ></td>
	<td class="line x" title="112:297	An correct WordNet sense is first assigned for each polysemous noun, and then all its hypernyms are recursively extracted." ></td>
	<td class="line x" title="113:297	The sense determination step is processed with the algorithm in (Pedersen et al. 2005)." ></td>
	<td class="line x" title="114:297	They disambiguate word sense by assigning a target word the sense, which is most related to the senses of its neighboring words." ></td>
	<td class="line x" title="115:297	Since the word sense disambiguation method has low performance, with F1-measure below 50% reported in (Pedersen et al. 2005), a feature selection method is used to extract the most discriminative hypernyms." ></td>
	<td class="line x" title="116:297	The hypernyms selection method is processed as follows: we first remove the low frequency hypernyms, and select the hypernyms using a chi-square method." ></td>
	<td class="line x" title="117:297	The chisquare value measures the lack of independence between a hypernym h and category j c . It is defined as: 2 2 ()() (, ) ()()()() j ABCD ADCB hc A CBDABCD  +++   = ++++  where A is the number of hypernym h, which belongs to category j c ; B is the number of  h out of j c ; C is the number of other hypernyms in j c ; D is the number of other hypernyms out of j c . We set a threshold to select the most discriminative hypernym set." ></td>
	<td class="line x" title="118:297	Extracted examples are shown in Figure 2." ></td>
	<td class="line x" title="119:297	Figure 2." ></td>
	<td class="line x" title="120:297	Examples of extracted hypernym  It can be seen that these hypernyms are appropriate to describe the semantic meaning of the category." ></td>
	<td class="line x" title="121:297	They are expected to work as the class-specific relational features which are semiconstructed by (Li and Roth, 2002)." ></td>
	<td class="line x" title="122:297	In our approach, we just use the nouns minimum upper hypernym, existing in training set, as the feature." ></td>
	<td class="line x" title="123:297	Head Verb Synset:  To avoid losing question verb information, we extract head verb, which is the first notional verb in a question, and expand it using WordNet synset as feature." ></td>
	<td class="line x" title="124:297	The head verb extraction is based on the following simple rules: If the first word is name or list, the head verb will be denoted as this word." ></td>
	<td class="line x" title="125:297	If the first verb following question word is do or other auxiliary verb, the next verb is extracted as head verb." ></td>
	<td class="line x" title="126:297	Otherwise the head verb is extracted as the first verb after question word." ></td>
	<td class="line x" title="127:297	4.3 Transition Features State transition feature captures the contextual constraints among labels." ></td>
	<td class="line x" title="128:297	We define it as (1 )( ) yy teii e e yy , =<,+>, | ,= | =< , > .YX Y ENTITY:animal: animal, carnivore, chordate, equine, horse, living_thing, vertebrate, mammal, odd-toed_ungulate, organism, placental ENTITY:food: alcohol, beer, beverage, brew, cereal, condiment, crop, drink, drug_of_abuse, flavorer, food, foodstuff, helping, indefinite_quantity, ingredient, liquid, output, produce, small_indefinite_quantity, production, solid, substance, vegetable 484 Where e represents the edge between adjacent nouns." ></td>
	<td class="line x" title="129:297	It captures adjacent categories as features to tag the target noun." ></td>
	<td class="line x" title="130:297	Note that, for simplicity, the value of above feature is independent of the observations X. 5 Head Noun Extraction After tagging all the words in a question, we will extract head noun and assign its tagged label to the question as the final question classification result." ></td>
	<td class="line x" title="131:297	The head noun extraction is a simple heuristic method inspired by  (Metzler and Croft, 2005)." ></td>
	<td class="line x" title="132:297	We first run a POS tagger on each question, and post-process them to make sure that each sentence has at least one noun word." ></td>
	<td class="line x" title="133:297	Next, the first NP chunk after the question word is extracted by shallow parsing." ></td>
	<td class="line x" title="134:297	The head noun is determined by the following heuristic rules: 1." ></td>
	<td class="line x" title="135:297	If the NP chunker is before the first verb, or the NP chunk is after the first verb but there is no possessive case after the NP chunker, we mark the rightmost word in the chunker as head noun." ></td>
	<td class="line x" title="136:297	2." ></td>
	<td class="line x" title="137:297	Otherwise, extract the next NP chunker and recursively process the above rules." ></td>
	<td class="line x" title="138:297	Although this method may depend on the performance of POS tagger and shallow parser, it achieves the accuracy of over 95% on the UIUC data set in our implementation." ></td>
	<td class="line x" title="139:297	6 Experiments 6.1 Experiment Settings Data Set: We evaluate the proposed approach on the UIUC data set (Li and Roth, 2002)." ></td>
	<td class="line x" title="140:297	5500 questions are selected for training, and 500 questions are selected for testing." ></td>
	<td class="line x" title="141:297	The classification categories have been introduced as question ontology in section 1." ></td>
	<td class="line x" title="142:297	This paper only focuses on 50 fine classes." ></td>
	<td class="line x" title="143:297	To train the LDCRFs, we manually labeled all the noun words with one of 50 fine categories." ></td>
	<td class="line x" title="144:297	Other words are labeled with O." ></td>
	<td class="line x" title="145:297	One of the labeled examples is What/O was/O Queen/HUM:individual Victria/HUM:individual s/O title/HUM:title regarding/O India/LOC:country." ></td>
	<td class="line x" title="146:297	Ten people labeled 3407 what-type questions as training set." ></td>
	<td class="line x" title="147:297	Each question was independently annotated by two people and reviewed by the third." ></td>
	<td class="line x" title="148:297	For words which have more than one category, the annotators selected the most salient one according to the context." ></td>
	<td class="line x" title="149:297	For testing set, 351 what-type questions were selected for experiments evaluation." ></td>
	<td class="line x" title="150:297	Evaluation metric: Accuracy performance is widely used to evaluate question classification methods [Li and Roth, 2002; Zhang and Lee, 2003, Melter and Croft, 2004; Nguyen et al. 2007]." ></td>
	<td class="line x" title="151:297	6.2 Approach Performance Evaluation  # Wrong Accuracy SVM 86 75.50% LDCRFsbased 80 77.20% Table 3." ></td>
	<td class="line x" title="152:297	LDCRFS-based Approach V.S. SVM Table 3 shows the compared results between the proposed LDCRFs based approach and SVM with unigram feature." ></td>
	<td class="line x" title="153:297	The LDCRFs based approach achieves accuracy of 77.20%, compared with 75.50% of SVM." ></td>
	<td class="line x" title="154:297	Observing the detailed classification results, we conclude two advantages of LDCRFs over SVMs." ></td>
	<td class="line x" title="155:297	First LDCRFs based approach focuses on head noun to reduce the noise generated by other words." ></td>
	<td class="line x" title="156:297	The question What is the length of the coastline of the state of Alaska? is misclassified as LOC:state by SVM, whereas it is correctly classified by our approach." ></td>
	<td class="line x" title="157:297	Second, LDCRFs based approach can utilize rich features, including not only state features, but also transition features." ></td>
	<td class="line x" title="158:297	With the new features involved, LDCRFs is expected to improve classification performance." ></td>
	<td class="line x" title="159:297	This unigram result is used as our baseline." ></td>
	<td class="line x" title="160:297	The following experiments are conducted to test the new feature contribution." ></td>
	<td class="line x" title="161:297	Syntactic Features: In addition to words, four types of features, including part-of-speech (POS), chunker, parser information (Parser), and question length (Length), are extracted as syntactic features." ></td>
	<td class="line x" title="162:297	Accuracy Unigram (U) 77.20% U+POS 78.35% U+Chunker 77.20% U+Parser 79.20% U+Length 77.49% Total Syn 80.06% Table 4." ></td>
	<td class="line x" title="163:297	Syntactic Feature Performance From the syntactic feature results in Table 4, we can draw the following conclusions (a)." ></td>
	<td class="line x" title="164:297	Among four types of syntactic features, pars485 er information contributes mostly." ></td>
	<td class="line x" title="165:297	(Metzler and Croft, 2005) once claimed that it didnt make improvement by just incorporating these information as explicit feature, and they should be used implicitly via a tree structure." ></td>
	<td class="line x" title="166:297	Without using the complex tree mining and representing technique, our LDCRFs-based approach just incorporates the word parent, relation with parent and word governor from Minipar as features." ></td>
	<td class="line x" title="167:297	The experiments show that the parser information feature is able to capture the syntactic structure information, and it makes much improvement in this sequence tagging approach." ></td>
	<td class="line x" title="168:297	(b) Question length makes small improvement." ></td>
	<td class="line x" title="169:297	However, the chunker features make no improvement, consistent with the observation reported by (Li and Roth, 2006)." ></td>
	<td class="line x" title="170:297	 The best accuracy (80.06%) is achieved by integrating all the syntactic features." ></td>
	<td class="line x" title="171:297	Semantic Features:   Accuracy Unigram(U) 77.20% U+NE 77.20% U+HVSyn 78.63% U+NHype 78.35% Total Sem 80.06% Table 5." ></td>
	<td class="line x" title="172:297	Semantic Feature Performance The semantic features include Named Entity (NE), Noun Hypernym (NHype) and Head Verb Synset (HVSyn)." ></td>
	<td class="line x" title="173:297	From Table 5 we can draw the following conclusions: (a) NE makes no improvement in classification task." ></td>
	<td class="line o" title="174:297	The reason is that the named entity recognizer contains only four semantic categories." ></td>
	<td class="line n" title="175:297	It is too coarse to distinguish 50 fined-categories." ></td>
	<td class="line x" title="176:297	(b) The LDCRFs-based approach just considers the noun words as semantic words." ></td>
	<td class="line x" title="177:297	The head verb synsets (HVSyn) are imported as one of semantic features." ></td>
	<td class="line x" title="178:297	The experiment results show that it is effective to incorporate the head verb as features, which achieves the best individual accuracy among semantic features." ></td>
	<td class="line x" title="179:297	(c) Noun hypernyms (NHype) are the most important semantic features." ></td>
	<td class="line x" title="180:297	They narrow the semantic gap between training set and testing set." ></td>
	<td class="line x" title="181:297	From Section 4.2, we can see that the selected noun hypernyms are appropriate for each category." ></td>
	<td class="line x" title="182:297	While, the experiment with NHype features doesnt make considerable improvement as we previously thought." ></td>
	<td class="line x" title="183:297	The reason may come from the fact that the word sense disambiguation method has low performance." ></td>
	<td class="line x" title="184:297	A hypernym selection method is used in training set, but we didnt tackle the error in testing set." ></td>
	<td class="line x" title="185:297	Once the word sense disambiguation is wrong, it will not make improvement, but generate noise (see the discussion examples in next section)." ></td>
	<td class="line x" title="186:297	(d) It is an interesting result that using all the semantic features can achieve the same accuracy as the syntactic features (80.06%)." ></td>
	<td class="line x" title="187:297	Feature Combination: In this section, we carry out experiments to examine whether the performance can be boosted by integrating syntactic features and semantic features." ></td>
	<td class="line x" title="188:297	Several results are shown in Table 6." ></td>
	<td class="line x" title="189:297	The experiments show that: (a)  Parser Information and Head Verb Synset are both the most contributive features for syntactic set and semantic feature set." ></td>
	<td class="line x" title="190:297	While the performance with these two features cant beat the performance by combining Parser Information and Noun Hypernyms." ></td>
	<td class="line x" title="191:297	Accuracy U+POS+NE+HVSyn 80.91% U+Parser+NHype 81.77% U+Parser+HVSyn 80.91% U+POS+Length+NHype 80.63% Total 82.05% Table 6." ></td>
	<td class="line x" title="192:297	Combined Feature Performance (b) The best result for classifying what-type questions with our approach is achieved by integrating all the features." ></td>
	<td class="line x" title="193:297	The accuracy is 82.05%, which is 18.7 percent error reduction (from 22.08% to 17.95%) over unigram feature set." ></td>
	<td class="line x" title="194:297	It shows that the features we extract are effectively used in our CRFs based approach." ></td>
	<td class="line x" title="195:297	Transition Feature: Transition feature can capture the information between adjacent categories." ></td>
	<td class="line x" title="196:297	It offers another semantic feature for LDCRFs-based approach." ></td>
	<td class="line x" title="197:297	No transition features With transition features Syn 79.20% 80.06% Sem 79.49% 80.06% Total 81.48% 82.05% Table 7." ></td>
	<td class="line x" title="198:297	Transition Feature Performance The performances of all these three experiment decline without the transition features." ></td>
	<td class="line x" title="199:297	It shows that the dependency between adjacent se486 mantic categories contributes to the classifier performance." ></td>
	<td class="line x" title="200:297	6.3 System Performance Comparison and Discussion In this section, the what-type questions and nonwhat-type questions are combined to show the final result." ></td>
	<td class="line x" title="201:297	Non-what-type questions are classified using SVM with unigrams as reported in Section 1, and what-type questions are classified by the LDCRFs based approach." ></td>
	<td class="line x" title="202:297	The combined results are used to compare with the current question classification methods." ></td>
	<td class="line x" title="203:297	Classifier Accuracy Lis Hierarchical method 84.20% Nguyens tree method 83.60% Metzlers U+ WordNet method 82.20% LDCRFs-based with U+Parser 83.60% LDCRFs-based with U+NHype 83.00% LDCRFs-based with total features 85.60% Table 8." ></td>
	<td class="line x" title="204:297	Comparison with related work Table 8 shows the accuracies of the LDCRFs based question classification approach with different feature sets, in comparison with the tree method (Nguyen et al. 2007), the WordNet Method (Metzler and Croft, 2005) and the hierarchical method (Li and Roth, 2002)." ></td>
	<td class="line x" title="205:297	We can see the LDCRFs-based approach is effective: (a) Without formulating the syntactic structure as a tree, the LDCRFs-based approach still achieves accuracy 83.60% with unigram and parser information, which is the same as Nguyens tree classifier." ></td>
	<td class="line x" title="206:297	(b) Although the LDCRFs-based approach with unigrams and Noun Hypernyms generates noise as described in Section 6.2, it still outperforms the Metzlers method using WordNet and unigram features (83.00% v.s. 82.20%)." ></td>
	<td class="line x" title="207:297	(c) The experiment with total features achieves the accuracy of 85.60%." ></td>
	<td class="line x" title="208:297	It outperforms Lis Hierarchical classifier, even they use semi-automatic constructed features." ></td>
	<td class="line x" title="209:297	6.3.1 Analysis and Discussion Even the sequence tagging model achieves high accuracy performance, there still exists many problems." ></td>
	<td class="line x" title="210:297	We use the matrix defined in Li and Roth (2002) to show the performance errors." ></td>
	<td class="line x" title="211:297	The metric is defined as follows: *2/( ) ij i j i j DEr NN=+  Where ij Err  is the number of questions in class i that are misclassified as belong to class j, Ni and Nj are the numbers of questions in class i and j respectively." ></td>
	<td class="line x" title="212:297	From the matrix in Figure 3, we can see two major mistake pairs are ENTY:substance and ENTY:other, ENTY:currency and NUM:money." ></td>
	<td class="line x" title="213:297	They really have similar meanings, which confuses even human beings." ></td>
	<td class="line x" title="214:297	Figure 3." ></td>
	<td class="line x" title="215:297	The gray-scale map of Matrix D[n,n]." ></td>
	<td class="line x" title="216:297	The gray scale of the small box in position [i,j] denotes D[i,j]." ></td>
	<td class="line x" title="217:297	The larger Dij is, the darker the color is.  Several factors influence the performance: (a) Head noun extraction error: This error is mainly caused by errors of POS tagger and shallow parser." ></td>
	<td class="line x" title="218:297	For the wrong POS example what/WP hemisphere/EX is/VBZ the/DT Philippines/NNPS in/IN ?/., Philippines is extracted as head word." ></td>
	<td class="line x" title="219:297	The result is misclassified into LOC:country." ></td>
	<td class="line x" title="220:297	For the shallow parser error example what/WP/B-NP is/VBZ/B-VP the/D T/BNP speed/NN/I-NP humminbirds/NNS /I-NP fly/VBP/B-VP ?/./O, hummingbirds is extract as head word, rather than speed." ></td>
	<td class="line x" title="221:297	The question is misclassified into ENTY:animal." ></td>
	<td class="line x" title="222:297	(b) WordNet sense disambiguation errors: In question What is the highest dam in the U.S. ? The real sense for dam is dam#1: a barrier constructed to contain the flow of water or to keep out the sea; while the disambiguation method determine the second sense as dam#2: a metric unit of length equal to ten meters." ></td>
	<td class="line x" title="223:297	(c) Lack of head nouns: the CRFs based approach is sensitive to the Head Noun." ></td>
	<td class="line x" title="224:297	If the question doesnt contain the head noun, it is difficult to produce the correct result, such as the question What is done with worn or outdated flags? In the future work, we will focus on the head noun absence problem." ></td>
	<td class="line x" title="225:297	487 7 Conclusion In this paper, we propose a novel approach with Conditional Random Fields to classify what-type questions." ></td>
	<td class="line x" title="226:297	We first use the CRFs model to label all the words in a question, and then choose the label of head noun as the question category." ></td>
	<td class="line x" title="227:297	As far as we know, this is the first trial to formulate question classification into word sequence tagging problem." ></td>
	<td class="line x" title="228:297	We believe that the model has two distinguished advantages: 1." ></td>
	<td class="line x" title="229:297	Extracting head noun can eliminate the noise generated by the non-head words 2." ></td>
	<td class="line x" title="230:297	The Conditional Random Fields model can integrate rich features, including not only the syntactic and semantic features, but also the transition features between labels." ></td>
	<td class="line x" title="231:297	Experiments show that the LDCRFs-based approach can achieve comparable performance to those of the state-of-the-art question answering systems." ></td>
	<td class="line x" title="232:297	With the addition of more features, the performance of the LDCRFs based approach can be expected to be further improved." ></td>
	<td class="line x" title="233:297	Acknowledgement This work is supported by National Natural Science Foundation of China (60572084, 60621062), Hi-tech Research and Development Program of China (2006AA02Z321), National Basic Research Program of China (2007CB311003)." ></td>
	<td class="line x" title="234:297	Thank Shuang Lin and Jiao Li for revising this paper." ></td>
	<td class="line x" title="235:297	Thanks for the reviewers comments." ></td>
	<td class="line x" title="236:297	References Christiane Fellbaum." ></td>
	<td class="line x" title="237:297	1998." ></td>
	<td class="line x" title="238:297	WordNet: an Electronic Lexical Database." ></td>
	<td class="line x" title="239:297	MIT Press." ></td>
	<td class="line x" title="240:297	Prager, J., D. Radev, E. Brown, A. Coden, and V. Samn." ></td>
	<td class="line x" title="241:297	1999." ></td>
	<td class="line x" title="242:297	`The use of predictive annotation for question answering in TREC'." ></td>
	<td class="line x" title="243:297	In: Proceedings of the 8th Text Retrieval Conference (TREC-8)." ></td>
	<td class="line x" title="244:297	John Lafferty, Andrew McCallum, Fernando Pereira." ></td>
	<td class="line x" title="245:297	2001." ></td>
	<td class="line x" title="246:297	Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data." ></td>
	<td class="line x" title="247:297	In Proceedings of ICML-2001." ></td>
	<td class="line x" title="248:297	Li, X. and D. Roth." ></td>
	<td class="line x" title="249:297	2002." ></td>
	<td class="line x" title="250:297	Learning question classifiers." ></td>
	<td class="line x" title="251:297	In Proceedings of the 19 th  International Conference on Compuatational Linguistics (COLING), pages 556562." ></td>
	<td class="line x" title="252:297	Zhang, D. and W. Lee." ></td>
	<td class="line x" title="253:297	2003." ></td>
	<td class="line x" title="254:297	Question classification using support vector machines." ></td>
	<td class="line x" title="255:297	In Proceedings of the 26th Annual International ACM SIGIR conference, pages 2632 Donald Metzler and W. Bruce Croft." ></td>
	<td class="line x" title="256:297	2004." ></td>
	<td class="line x" title="257:297	Analysis of Statistical Question Classfication for Fact-based Questions." ></td>
	<td class="line x" title="258:297	In Journal of Information Retrieval." ></td>
	<td class="line x" title="259:297	Ted Pedersen, Satanjeev Banerjee, and Siddharth Patwardhan . 2005." ></td>
	<td class="line x" title="260:297	Maximizing Semantic Relatedness to Perform Word Sense Disambiguation." ></td>
	<td class="line x" title="261:297	University of Minnesota Supercomputing Institute Research Report UMSI 2005/25, March." ></td>
	<td class="line x" title="262:297	Xin Li, Dan Roth." ></td>
	<td class="line x" title="263:297	2006." ></td>
	<td class="line x" title="264:297	Learning Question Classifiers: The Role of Semantic Information." ></td>
	<td class="line x" title="265:297	In Natural Language Engineering, 12(3):229-249 Minh Le Nguyen, Thanh Tri Nguyen and Akira Shimazu." ></td>
	<td class="line x" title="266:297	2007." ></td>
	<td class="line x" title="267:297	Subtree Mining for Question Classification Problem." ></td>
	<td class="line x" title="268:297	In Proceedings of the 20th International Conference on Artificial Intelligence." ></td>
	<td class="line x" title="269:297	Pages 1695-1700." ></td>
	<td class="line x" title="270:297	C. Sutton and A. McCallum." ></td>
	<td class="line x" title="271:297	2007." ></td>
	<td class="line x" title="272:297	An introduction to conditional random fields for relational learning." ></td>
	<td class="line x" title="273:297	In L. Getoor and B. Taskar (Eds.)." ></td>
	<td class="line x" title="274:297	Introduction to statistical relational learning." ></td>
	<td class="line x" title="275:297	MIT Press." ></td>
	<td class="line x" title="276:297	Y. Tsuruoka and J. Tsujii,." ></td>
	<td class="line x" title="277:297	2005." ></td>
	<td class="line x" title="278:297	Bidirectional inference with the easiest-first strategy for tagging sequence data." ></td>
	<td class="line x" title="279:297	In Proc." ></td>
	<td class="line x" title="280:297	HLT/EMNLP05, Vancouver, October, pp." ></td>
	<td class="line x" title="281:297	467-474." ></td>
	<td class="line x" title="282:297	L. Ramshaw and M. Marcus." ></td>
	<td class="line x" title="283:297	1995." ></td>
	<td class="line x" title="284:297	Text chunking using transformation-based learning, Proc." ></td>
	<td class="line x" title="285:297	3rd Workshop on Very Large Corpora, pp." ></td>
	<td class="line x" title="286:297	8294." ></td>
	<td class="line x" title="287:297	J.R. Finkel, T. Grenager and C. Manning." ></td>
	<td class="line x" title="288:297	2005." ></td>
	<td class="line x" title="289:297	Incorporating non-local information into information extraction systems by Gibbs sampling." ></td>
	<td class="line x" title="290:297	Proc." ></td>
	<td class="line x" title="291:297	43rd Annual Meeting of ACL, pp." ></td>
	<td class="line x" title="292:297	363370." ></td>
	<td class="line x" title="293:297	D. Lin." ></td>
	<td class="line x" title="294:297	1999." ></td>
	<td class="line x" title="295:297	MINIPAR: a minimalist parser." ></td>
	<td class="line x" title="296:297	In Maryland Linguistics Colloquium, University of Maryland, College Park." ></td>
	<td class="line x" title="297:297	488" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="I08-4013
Chinese Word Segmentation and Named Entity Recognition Based on Conditional Random Fields
Mao, Xinnian;Dong, Yuan;He, Saike;Bao, Sencheng;Wang, Haila;"></td>
	<td class="line x" title="1:80	Chinese Word Segmentation and Named Entity Recognition Based on Conditional Random Fields Xinnian Mao France Telecom R&D Center (Beijing), Beijing, 100080, P.R.China xinnian.mao@orangeftgroup.com Saike He University of Posts and Telecommunications, Beijing, 100876, P.R.China  Sencheng Bao University of Posts and Telecommunications, Beijing, 100876, P.R.China Yuan Dong1,2 1France Telecom R&D Center (Beijing), Beijing, 100080, P.R.China 2University of Posts and Telecommunications, Beijing, 100876, P.R.China yuan.dong@orangeftgroup.com Haila Wang France Telecom R&D Center (Beijing), Beijing, 100080, P.R.China haila.wang@orangeftgroup.com  Abstract Chinese word segmentation (CWS), named entity recognition (NER) and part-ofspeech tagging is the lexical processing in Chinese language." ></td>
	<td class="line x" title="2:80	This paper describes the work on these tasks done by France Telecom Team (Beijing) at the fourth International Chinese Language Processing Bakeoff." ></td>
	<td class="line x" title="3:80	In particular, we employ Conditional Random Fields with different features for these tasks." ></td>
	<td class="line x" title="4:80	In order to improve NER relatively low recall; we exploit non-local features and alleviate class imbalanced distribution on NER dataset to enhance the recall and keep its relatively high precision." ></td>
	<td class="line x" title="5:80	Some other post-processing measures such as consistency checking and transformation-based error-driven learning are used to improve word segmentation performance." ></td>
	<td class="line x" title="6:80	Our systems participated in most CWS and POS tagging evaluations and all the NER tracks." ></td>
	<td class="line x" title="7:80	As a result, our NER system achieves the first ranks on MSRA open track and MSRA/CityU closed track." ></td>
	<td class="line x" title="8:80	Our CWS system achieves the first rank on CityU open track, which means that our systems achieve state-of-the-art performance on Chinese lexical processing." ></td>
	<td class="line x" title="9:80	1 Introduction Different from most European languages, there is no space to mark word boundary between Chinese characters, so Chinese word segmentation (CWS) is the first step for Chinese language processing." ></td>
	<td class="line x" title="10:80	From another point that there is no capitalization information to indicate entity boundary, which makes Chinese named entity recognition (NER) more difficult than European languages." ></td>
	<td class="line x" title="11:80	And partof-speech tagging (POS tagging) provides valuable information for deep language processing such as parsing, semantic role labeling and etc. This paper presents recent research progress on CWS, NER and POS tagging done by France Telecom Team (Beijing)." ></td>
	<td class="line x" title="12:80	Recently, Conditional Random Fields1 (CRFs) (Lafferty et al., 2001) have been successfully employed in various natural language processing tasks and achieve the state-of-the-art performance, in our system, we use it as the basic framework and incorporate some other postprocessing measures for CWS, NER and POS tagging tasks." ></td>
	<td class="line x" title="13:80	2 Chinese Named Entity Recognition NER is always limited by its lower recall due to the imbalanced distribution where the NONE class dominates the entity classes." ></td>
	<td class="line x" title="14:80	Classifiers built on such dataset typically have a higher precision and a lower recall and tend to overproduce the NONE  1 We use the CRF++ V4.5 software from http://chasen.org/~taku/software/CRF++/ 90 Sixth SIGHAN Workshop on Chinese Language Processing class (Kambhatla, 2006)." ></td>
	<td class="line x" title="15:80	Taking SIGHAN Bakeoff 2006 (Levow, 2006) as an example, the recall is lower about 5% than the precision for each submitted system on MSRA and CityU closed track." ></td>
	<td class="line x" title="16:80	If we could improve NER recall but keep its relatively high precision, the overall F-measure will be improved as a result." ></td>
	<td class="line x" title="17:80	We design two kinds of effective features: 0/1 features and non-local features to achieve this objective." ></td>
	<td class="line x" title="18:80	Our final systems utilize these features together with the local features to perform NER task." ></td>
	<td class="line x" title="19:80	2.1 Local Features The local features are character-based and are instantiated from the following temples: Unigram: Cn (n=-2,-1, 0, 1, 2)." ></td>
	<td class="line x" title="20:80	Bigram: CnCn+1 (n=-2,-1, 0, 1) and C-1C1." ></td>
	<td class="line x" title="21:80	Where C0 is the current character, C1 the next character, C2 the second character after C0, C-1 the character preceding C0, and C-2 the second character before C0." ></td>
	<td class="line x" title="22:80	2.2 0/1 Features In order to alleviate the imbalanced class distribution, we assign 1 to all the characters which are labeled as entity and 0 to all the characters which are labeled as NONE in training data." ></td>
	<td class="line x" title="23:80	In such way, the class distribution can be alleviated greatly, taking Bakeoff 2006 MSRA NER training data for example, if we label the corpus with 10 classes, the class distribution is 0.81(B-PER):1.70(B-LOC):0.95(BORG):0.81(I-PER):0.88(I-LOC):2.87(I-ORG):0.76(EPER):1.42(E-LOC):0.94(E-ORG):88.86(NONE), if we change the label scheme to 2 labels (0/1), the class distribution is 11.14 (entity):88.86(NONE)." ></td>
	<td class="line x" title="24:80	We train the 0/1 CRFs tagger using the local features alone." ></td>
	<td class="line x" title="25:80	For the 0/1 features, during the training stage, they are assigned with 2-fold cross validation, and during the testing stage, they are assigned with the 0/1 tagger." ></td>
	<td class="line x" title="26:80	2.3 Non-local Features Most empirical approaches including CRFs currently employed in NER task make decision only on local context for extract inference, which is based on the data independent assumption." ></td>
	<td class="line x" title="27:80	But often this assumption does not hold because nonlocal dependencies are prevalent in natural language (including the NER task)." ></td>
	<td class="line x" title="28:80	How to utilize the non-local dependencies is a key issue in NER task." ></td>
	<td class="line x" title="29:80	Up to now, few researches have been devoted to this issue; existing works mainly focus on using the non-local information for improving NER label consistency (Krishnan and Manning, 2006)." ></td>
	<td class="line o" title="30:80	There are two methods to use non-local information." ></td>
	<td class="line o" title="31:80	One is to add additional edges to graphical model structure to represent the distant dependencies and the other is to encode the non-locality with non-local features." ></td>
	<td class="line oc" title="32:80	In the first approach, heuristic rules are used to find the dependencies (Bunescu and Mooney, 2004) or penalties for label inconsistency are required to handset ad-hoc (Finkel et al., 2005)." ></td>
	<td class="line n" title="33:80	Furthermore, high computational cost is spent for approximate inference." ></td>
	<td class="line x" title="34:80	In order to establish the long dependencies easily and overcome the disadvantage of the approximate inference, Krishnan and Manning (2006) propose a two-stage approach using CRFs framework with extract inference." ></td>
	<td class="line x" title="35:80	They represent the non-locality with non-local features, and extract them from the output of the first stage CRF with local context alone; then they incorporate the non-local features into the second CRF." ></td>
	<td class="line x" title="36:80	But the features in this approach are only used to improve label consistency in European languages." ></td>
	<td class="line x" title="37:80	Similar with their work encoding the non-local information with non-local feature, and we also exploit the non-local features under twostage architecture." ></td>
	<td class="line x" title="38:80	Different from their features are activated on the recognized entities coming from the first CRF, the non-local features we design are used to recall more missed entities which are seen in the training data or unseen entities but some of their occurrences being recognized correctly in the first stage, so our non-local features are activated on the raw character sequence." ></td>
	<td class="line x" title="39:80	Different NER in European languages, where entity semantic classification is more difficult compared with boundary detection, in Chinese, the situation is opposite." ></td>
	<td class="line x" title="40:80	So we encode different useful information for Chinese NER two subtasks: entity boundary detection and entity semantic classification." ></td>
	<td class="line x" title="41:80	Three kinds of non-local features are designed; they are fired on the token sequences if they are matched with certain entity in the entity list in forward maximum matching (FMM) way." ></td>
	<td class="line x" title="42:80	Token-position features (NF1): These refer to the position information (start, middle and last) assigned to the token sequence which is matched with the entity list exactly." ></td>
	<td class="line x" title="43:80	These features enable us to capture the dependencies between the identical candidate entities and their boundaries." ></td>
	<td class="line x" title="44:80	91 Sixth SIGHAN Workshop on Chinese Language Processing Entity-majority features (NF2): These refer to the majority label assigned to the token sequence which is matched with the entity list exactly." ></td>
	<td class="line x" title="45:80	These features enable us to capture the dependencies between the identical entities and their classes, so that the same candidate entities of different occurrences can be recalled favorably, and their label consistencies can be considered too." ></td>
	<td class="line x" title="46:80	Token-position & entity-majority features (NF3): These features capture non-local information from NF1 and NF2 simultaneously." ></td>
	<td class="line x" title="47:80	They take into account the entity boundary and semantic class information at the same time." ></td>
	<td class="line x" title="48:80	Figure 1 shows the flow of using non-local features under CRFs framework in two-stage architecture." ></td>
	<td class="line x" title="49:80	The first CRF is trained with local features alone, and then we test the testing data with the first CRF and get the entities plus their type from the output." ></td>
	<td class="line x" title="50:80	The second CRF utilizes the 0/1 features and the non-local features derived from the entity list which is merged by the output of the first CRF from the testing data and the entities extracted directly from the training data." ></td>
	<td class="line x" title="51:80	We compare the three kinds of non-local features on MSRA and CityU closed track in SIGHAN 2006 and we find that the NF3 is the best (Mao etc, 2007)." ></td>
	<td class="line x" title="52:80	So we only incorporate the NF3 into our final NER system." ></td>
	<td class="line x" title="53:80	Figure 1." ></td>
	<td class="line x" title="54:80	The flow using non-local features in two-stage architecture 2.4 Results We employ BIOE1 label scheme for the NER task because we found it performs better than IOB2 on Bakeoff 2006 (Levow, 2006) NER MSRA and CityU corpora." ></td>
	<td class="line x" title="55:80	Table 1 presents the official results on the MSRA and CityU corpus." ></td>
	<td class="line x" title="56:80	The F-measure on MSRA open track is so high just because the testing data in Bakeoff 2007 is part of its Bakeoff 2006 training dataset and we utilize this corpus for training the final CRFs classifier." ></td>
	<td class="line x" title="57:80	The F-measure on CityU open track is not much superior to its closed track because we only use its Bakeoff 2006 corpus to train the 0/1 CRFs, but not use the Bakeoff 2006 corpus to train final classifier." ></td>
	<td class="line x" title="58:80	Run ID  F-Score  Run ID  F-Score cityu_c  84.99 cityu_o  87.92 msra_c  92.81 msra_o 99.88  Table 1: The official results on NER closed(c) tracks and open(o) tracks 3 Chinese Word Segmentation Type Feature Unigram Cn (n=-2,-1, 0, 1, 2)." ></td>
	<td class="line x" title="59:80	Bigram CnCn+1 (n=-2,-1,0, 1) Jump C-1C1 Punc Pu (C0) Date, Digit, Letter T-1T0T1 Table 2: The features used in our CWS systems  Table 2 lists the features we used in our CWS systems." ></td>
	<td class="line x" title="60:80	After the raw corpus is processed by CRFs, two other post-processing measures are performed." ></td>
	<td class="line x" title="61:80	We utilize transformation-based error-driven learning (TBL)2 to further improve CWS and perform consistency checking among different occurrences of a particular character sequence." ></td>
	<td class="line x" title="62:80	For TBL, we use the template defined in (He et al.)." ></td>
	<td class="line x" title="63:80	Our CWS system participate almost all the tracks and table 3 lists the official results." ></td>
	<td class="line x" title="64:80	Run ID F-Score Run ID F-Score cityu_c_a 94.43 cityu_o_a 96.97 cityu_c_b error (94.31) cityu_o_b 96.86 ckip_c_a 93.17 ckip_o_a 93.25 ckip_c_b 93.06 ckip_o_b 93.64 ctb_c_a 94.86 ctb_o_a 97.93 ctb_c_b 94.74 ctb_o_b 97.28 ncc_c_a 92.99 sxu_c_a 95.46 ncc_c_b 92.89 sxu_c_b 95.17 Table 3: The official results on CWS closed(c) tracks and open(o) tracks  In the table 3, run (a) means that we only perform consistency checking; run (b) means that  2 We use the TBL software from http://nlp.cs.jhu.edu/~rflorian/fntbl/index.html 92 Sixth SIGHAN Workshop on Chinese Language Processing TBL is performed after consistency checking is done." ></td>
	<td class="line x" title="65:80	We make a mistake on cityu_c_b because we rename cityu_c_a as cityu_c_b, so the two results are the same, after we correct the mistake and score again; we achieve an F-measure of 94.31%." ></td>
	<td class="line x" title="66:80	In the closed tracks, we first train initial CRFs with 3-fold cross-validation; then we test the training data (three parts) with the three trained CRFs, we train the TBL learner on the training data compared it with the testing result from the initial CRFs." ></td>
	<td class="line x" title="67:80	The consistency checking is inspired by (Ng and Low, 2004)." ></td>
	<td class="line x" title="68:80	Table 4 lists the corpus used to train the CRFs and TBL learner in the open tracks." ></td>
	<td class="line x" title="69:80	CRFs  TBL CityU 2005,2006,2007 2003 CKIP 2007 2006 CTB 2006,2007 2007 Table 4." ></td>
	<td class="line x" title="70:80	Corpora used to train the CRFs classifier and the TBL learner  In the open track, we collect the consistency list from all its correspondent Bakeoff corpora, the gazetteer extract from People Daily 2000 and idioms, slang from GKB." ></td>
	<td class="line x" title="71:80	From the table 3 in the closed test, we can confirm that TBL may not improve CWS performance, while in most cases, performance will surely draw back." ></td>
	<td class="line x" title="72:80	The reason lies in the fact that the learning capability of CRFs is superior to that of TBL, if they are trained with the same corpus, TBL may modify some correctly tags by CRFs." ></td>
	<td class="line x" title="73:80	This can be seen from Table 3 that results without TBL (in run (a)) are almost superior to that with TBL (in run (b))." ></td>
	<td class="line x" title="74:80	4 Part-of-speech Tagging For POS tagging task, apart from the local features same as used in NER, two other features are designed to improve the performance." ></td>
	<td class="line x" title="75:80	 Ambiguous part-of-speech: this feature is true when the word has more than 2 kinds of part-of-speech." ></td>
	<td class="line x" title="76:80	 Major part-of-speech: The feature is assigned as the major part-of-speech for any word." ></td>
	<td class="line x" title="77:80	We do not assign the value to the new words." ></td>
	<td class="line x" title="78:80	Table 5 shows the performance in the closed tracks." ></td>
	<td class="line x" title="79:80	Because we only used the simple features and do not process the unknown word specially, our performance is not satisfactory." ></td>
	<td class="line x" title="80:80	Run ID F-Score Run ID F-Score cityu_c 87.93 ctb_c 92.03 ckip_c 87.93 ncc_c 91.72 ctb_c 92.03 Table 5: The official results on POS tagging in closed tracks" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="I08-6004
Some Experiments in Mining Named Entity Transliteration Pairs from Comparable Corpora
Saravanan, K.;Kumaran, A.;"></td>
	<td class="line x" title="1:161	Some Experiments in Mining Named Entity Transliteration Pairs from Comparable Corpora K Saravanan Microsoft Research India Bangalore, India v-sarak@microsoft.com A Kumaran Microsoft Research India Bangalore, India kumarana@microsoft.com   Abstract Parallel Named Entity pairs are important resources in several NLP tasks, such as, CLIR and MT systems." ></td>
	<td class="line x" title="2:161	Further, such pairs may also be used for training transliteration systems, if they are transliterations of each other." ></td>
	<td class="line x" title="3:161	In this paper, we profile the performance of a mining methodology in mining parallel named entity transliteration pairs in English and an Indian language, Tamil, leveraging linguistic tools in English, and article-aligned comparable corpora in  the two languages." ></td>
	<td class="line x" title="4:161	We adopt a methodology parallel to that of [Klementiev and Roth, 2006], but we focus instead on mining parallel named entity transliteration pairs, using a well-trained linear classifier to identify transliteration pairs." ></td>
	<td class="line x" title="5:161	We profile the performance at several operating parameters of our algorithm and present the results that show the potential of the approach in mining transliterations pairs; in addition, we uncover a host of issues that need to be resolved, for effective mining of parallel named entity transliteration pairs." ></td>
	<td class="line x" title="6:161	1 Introduction & Motivation Parallel Named Entity (NE) pairs are important resources in several NLP tasks, from supporting Cross-Lingual Information Retrieval (CLIR) systems, to improving Machine Translation (MT) systems." ></td>
	<td class="line x" title="7:161	In addition, such pairs may also be used for developing transliteration systems, if they are transliterations of each other." ></td>
	<td class="line x" title="8:161	Transliteration of a name, for the purpose of this work, is defined as its transcription in a different language, preserving the phonetics, perhaps in a different orthography [Knight and Graehl, 1997] 1 .  While traditional transliteration systems have relied on hand-crafted linguistic rules, more recently, statistical machine learning techniques have been shown to be effective in transliteration tasks [Jung et al., 2000] [AbdulJaleel and Larkey, 2003] [Virga  and Kudhanpur , 2003] [Haizhou et al., 2004]." ></td>
	<td class="line x" title="9:161	However, such data-driven approaches require significant amounts of training data, namely pairs of names in two different languages, possibly in different orthography, referred to as transliteration pairs, which are not readily available in many resource-poor languages." ></td>
	<td class="line x" title="10:161	It is important to note at this point, that NEs are found typically in news corpora in any given language." ></td>
	<td class="line x" title="11:161	In addition, news articles covering the same event in two different languages may reasonably be expected to contain the same NEs in the respective languages." ></td>
	<td class="line x" title="12:161	The perpetual availability of news corpora in the worlds languages, points to the promise of  mining transliteration pairs endlessly, provided an effective identification of such NEs in specific languages and pairing them appropriately, could be devised." ></td>
	<td class="line x" title="13:161	Recently, [Klementiev and Roth, 2006] outlined an approach by leveraging the availability of articlealigned news corpora between English and Russian, and tools in English, for discovering transliteration pairs between the two languages, and progressively refining the discovery process." ></td>
	<td class="line x" title="14:161	In this paper, we adopt their basic methodology, but we focus on 3 different issues:  1 London rewritten as   in Tamil, or   in Arabic (both pronounced as London), are considered as transliterations, but not the rewriting of New Delhi as     (puthu thilli) in Tamil." ></td>
	<td class="line x" title="15:161	1." ></td>
	<td class="line x" title="16:161	mining comparable corpora for NE pairs, leveraging a well trained classifier, 2." ></td>
	<td class="line x" title="17:161	calibrating the performance of this mining framework, systematically under different parameters for mining, and, 3." ></td>
	<td class="line x" title="18:161	uncovering further research issues in mining NE pairs between English and an Indian language, Tamil." ></td>
	<td class="line x" title="19:161	While our analysis points to a promising approach for mining transliteration pairs, it also uncovers several issues that may need to be resolved, to make this process highly effective." ></td>
	<td class="line x" title="20:161	As in [Klementiev and Roth, 2006] no language specific knowledge was used to refine our mining process, making the approach broadly applicable." ></td>
	<td class="line x" title="21:161	2 Transliteration Pairs Discovery In this section, we outline briefly the methodology presented in [Klementiev and Roth, 2006], and refer interested readers to the source for details." ></td>
	<td class="line x" title="22:161	They present a methodology to automatically discover parallel NE transliteration pairs between English and Russian, leveraging the availability of a good-quality Named Entity Recognizer (NER) in English, and article-aligned bilingual comparable corpora, in English and Russian." ></td>
	<td class="line x" title="23:161	The key idea of their approach is to extract all NEs in English, and identify a set of potential transliteration pairs in Russian for these NEs using a simple classifier trained on a small seed corpus, and re-ranking the identified pairs using the similarity between the frequency distributions of the NEs in the comparable corpora." ></td>
	<td class="line x" title="24:161	Once re-ranked, the candidate pairs, whose scores are above a threshold are used to re-train the classifier, and the process is repeated to make the discovery process more effective." ></td>
	<td class="line x" title="25:161	To discriminate transliteration pairs from other content words, a simple perceptron-based linear classifier, which is trained on n-gram features extracted from a small seed list of NE pairs, is employed leveraging the fact that transliteration relies on approximately monotonic alignment between the names in two languages." ></td>
	<td class="line x" title="26:161	The potential transliteration pairs identified by this classifier are subsequently re-ranked using a Discrete Fourier Transform based similarity metric, computed based on the frequency of words of the candidate pair, found in the article-aligned comparable corpora." ></td>
	<td class="line x" title="27:161	For the frequency analysis, equivalence classes of the words are formed, using a common prefix of 5 characters, to account for the rich morphology of Russian language." ></td>
	<td class="line x" title="28:161	The representative prefix of each of the classes are used for classification." ></td>
	<td class="line x" title="29:161	Finally, the high scoring pairs of words are used to re-train the perceptron-based linear classifier, to improve the quality of the subsequent rounds." ></td>
	<td class="line x" title="30:161	The quality of the extracted NE pairs is shown to improve, demonstrating viability of such an approach for successful discovery of NE pairs between English and Russian." ></td>
	<td class="line x" title="31:161	3 Adoption for Transliteration Pairs Mining We adopt the basic methodology presented in [Klementiev and Roth, 2006], but we focus on three specific issues described in the introduction." ></td>
	<td class="line x" title="32:161	3.1 Mining of Transliteration Pairs We start with comparable corpora in English and Tamil, similar in size to that used in [Klementiev and Roth, 2006], and using the English side of this corpora, first, we extract all the NEs that occur more than a given threshold parameter, FE, using a standard NER tool." ></td>
	<td class="line x" title="33:161	The higher the threshold is, the more will be the evidence for legitimate transliteration pairs, in the comparable corpora, which may be captured by the mining methodology." ></td>
	<td class="line x" title="34:161	The extracted list of NEs provides the set of NEs in English, for which we mine for transliteration pairs from the Tamil side of the comparable corpora." ></td>
	<td class="line x" title="35:161	We need to identify all NEs in the Tamil side of the corpora, in order to appropriately pair-up with English NEs." ></td>
	<td class="line x" title="36:161	However, given that there is no publicly available NER tool in Tamil (as the case may be in many resource-poor languages) we start with an assumption that all words found in the Tamil corpus are potentially NEs." ></td>
	<td class="line x" title="37:161	However, since Tamil is a highly morphologically inflected language, the same NE may occur in its various inflected forms in the Tamil side of the corpora; hence, we collect those words with the same prefix (of fixed size) into a single bucket, called equivalence class, and consider a representative prefix, referred to as signature of the collection for comparison." ></td>
	<td class="line x" title="38:161	The assumption here is that the common prefix would stand for a Tamil NE, and all the members of the equivalence class are the various inflected forms of the NE." ></td>
	<td class="line x" title="39:161	We use such a signature to classify a Tamil word as potential transliteration of an English word." ></td>
	<td class="line x" title="40:161	Again, we consider only those signatures that have occurred more than a threshold parameter, FT, in the Tamil side of the comparable corpora, in order to strengthen support for a meaningful similarity in their frequency of occurrence." ></td>
	<td class="line x" title="41:161	We used a linear Support Vector Machine classifier (details given in a later section) trained on a sizable seed corpus of transliterations between English and Tamil, and use it to identify potential Tamil signatures with any of the NEs extracted from the English side." ></td>
	<td class="line x" title="42:161	We try to match each of the NEs extracted from the English side, to every signature from the Tamil side, and produce an ordered list of Tamil signatures that may be potential transliterations for a given English NE." ></td>
	<td class="line x" title="43:161	Every Tamil signature, thus, would get a score, which is used to rank the signatures in the decreasing order of similarity." ></td>
	<td class="line x" title="44:161	Subsequently, we consider only those above a certain threshold for analysis, and in addition, consider only the top-n candidates." ></td>
	<td class="line x" title="45:161	3.2 Quality Refinement Since a number of such transliteration candidates are culled from the Tamil corpus for a given NE in English, we further cull out unlikely candidates, by re-ranking them using frequency cues from the aligned comparable corpora." ></td>
	<td class="line x" title="46:161	For this, we start with the hypothesis, that the NEs will have similar normalized frequency distributions with respect to time, in the two corpora." ></td>
	<td class="line x" title="47:161	Given that the news corpora are expected to contain same names in similar time periods in the two different languages, the frequency distribution of words in the two languages provides a strong clue about possible transliteration pairs; however, such potential pairs might also include other content words, such as,    (soshaliSt),   (kavanamaaka),   (keetpathu), etc., which are common nouns, adjectives or even adverbs and verbs." ></td>
	<td class="line x" title="48:161	On the other hand, function words are expected to be uniformly distributed in the corpus, and hence may not have high variability like content words." ></td>
	<td class="line x" title="49:161	Note that the NEs in English are not usually inflected." ></td>
	<td class="line x" title="50:161	Since Tamil NEs usually have inflections, the frequency of occurrence of a NE in Tamil must be normalized across all forms, to make it reasonably comparable to the frequency of the corresponding English NE." ></td>
	<td class="line x" title="51:161	This was taken care of by considering the signature and its equivalence class." ></td>
	<td class="line x" title="52:161	Hence the frequency of occurrence of a NE (i.e., its signature) in Tamil is the sum of frequencies of all members in its equivalence class." ></td>
	<td class="line x" title="53:161	For identifying the names between the languages, we first create a frequency distribution of every word in English and Tamil, by creating temporal bins of specific duration, covering the entire timeline of the corpus." ></td>
	<td class="line x" title="54:161	The frequency is calculated as the number of occurrences of each signature in the bin interval." ></td>
	<td class="line x" title="55:161	Once the frequency distributions are formed, they are normalized for every signature." ></td>
	<td class="line x" title="56:161	Given the normalized frequencies, two words are considered to have same (or, similar) pattern of occurrence in the corpus, if the normalized frequency vectors of the two words are the same (or, close within a threshold)." ></td>
	<td class="line x" title="57:161	Figure 1 shows the frequency of the word Abishek, and its Tamil version,   (apishek) as a frequency plot, where a high correlation between the frequencies can be observed." ></td>
	<td class="line x" title="58:161	Figure 1: Names Frequency Plot in Comparable Corpora  Hence, to refine the quality of the classifier output, we re-rank the list of candidates, using the distance between the frequency vectors of the English NE, and the Tamil candidate signature." ></td>
	<td class="line x" title="59:161	This step moves up those signatures that have similar patterns of occurrence, and moves down those that do not." ></td>
	<td class="line x" title="60:161	It is likely that such frequency cues from the comparable corpora will make the quality of matched transliteration pairs better, yielding better mined data." ></td>
	<td class="line x" title="61:161	4 Experimental Setup & Results In this section, we present the experimental setup and the data that we used for mining transliteration pairs from comparable corpora in two languages: English and the Indian language, Tamil." ></td>
	<td class="line x" title="62:161	We evaluate and present the effectiveness of the methodology in extracting NE pairs, between these languages, under various parameters." ></td>
	<td class="line x" title="63:161	4.1 Comparable Corpora We used a set of news articles from the New Indian Express (in English) and Dinamani (in Tamil) roughly covering similar events in English and Tamil respective, and covering a period of about 8 months, between January and August of 2007." ></td>
	<td class="line x" title="64:161	The articles were verified to contain similar set of NEs, though only a fraction of them are expected to be legitimate transliteration pairs." ></td>
	<td class="line x" title="65:161	Others related NEs could be translations,  for example, chief minister in English vs  (muthalvar) in Tamil, abbreviation which are not usually transliterated but spelled out , for example, ICC in English, and     (aicici) in Tamil, or co-references , for example, New Delhi in English, and   (puthu thilli) in Tamil." ></td>
	<td class="line x" title="66:161	While the number of      articles used were roughly the same (~2,400), the number of words in Tamil were only about 70% of that in English." ></td>
	<td class="line x" title="67:161	This is partially due to the fact Tamil is a highly agglutinative language, where various affixes (prefixes and suffixes of other content words) stand for function words and prepositions in English, thus do not contribute to the word count." ></td>
	<td class="line x" title="68:161	Further, since our focus is on mining names, we expect the same NEs to be covered in both the corpora, and hence we do not expect a severe impact on mining." ></td>
	<td class="line oc" title="69:161	Corpus Time Period Size Articles Words New Indian Express (English) 2007.01.01 to 2007.08.31 2,359 347,050 Dinamani (Tamil) 2007.01.01 to 2007.08.31 2,359 256,456 Table 1: Statistics on Comparable Corpora  From the above corpora, we first extracted all the NEs from the English side, using the Stanford NER tool [Finkel et al, 2005]." ></td>
	<td class="line x" title="70:161	No multiword expressions were considered for this experiment." ></td>
	<td class="line x" title="71:161	Also, only those NEs that have a frequency count of more than a threshold value of FE were considered, in order to avoid unusual names that are hard to identify in the comparable corpora." ></td>
	<td class="line x" title="72:161	Thus, we extracted from the above corpora, only a subset of NEs found in the English side to be matched with their potential transliteration pairs; for example, for a parameter setting of FE to 10, we extract only 274 legitimate NEs." ></td>
	<td class="line x" title="73:161	From the Tamil side of the corpora, we extracted all words, and grouped them in to equivalence classes, by considering a prefix of 5 characters." ></td>
	<td class="line x" title="74:161	That is, all words that share the same 5 characters were considered to be morphological variations of the same root word or NE in Tamil." ></td>
	<td class="line x" title="75:161	After they were grouped, the longest common prefix of the group is extracted, and is used as the signature of the equivalence class." ></td>
	<td class="line x" title="76:161	It should be noted here that though the number of unique words in the corpus is about 46,503, the number of equivalence classes to be considered changes depending on the filtering threshold that we use in the Tamil side." ></td>
	<td class="line x" title="77:161	For example, at a threshold (FT) value of 1, the number of equivalence classes is 14,101." ></td>
	<td class="line x" title="78:161	It changes to 4,612 at a threshold (FT) value of 5, to 2,888 at a threshold (FT) value of 10 and to 1779 at a threshold (FT) value of 20." ></td>
	<td class="line x" title="79:161	However, their signature (i.e., longest common prefix) sizes ranged from 5 to 13 characters." ></td>
	<td class="line x" title="80:161	Thus, we had about 14,101 equivalence classes, covering all the words from the Tamil corpus." ></td>
	<td class="line x" title="81:161	The equivalence classes thus formed were as shown in Figure 2:  Tamil Signature Tamil Equiv." ></td>
	<td class="line x" title="82:161	Class  (aiSvaryaa)   (aiSvaryaa),   (aiSvaryaavin),   (aiSvaryaavukku),   (aiSvaryaavai),   (aiSvaryaaviRkum),   (aiSvaryaavutan)  (piram)   (pirammapuththiraa),   (pirammaaNdamaana),   (pirampu),   (pirammaa)  (kaaveeri)   (kaaveeri)    (aicici)     (aicici),     (aicicyin),     (aicici kku),     (aicicithaan),     (aiciciyidam) Figure 2: Signatures and Equivalence Classes  As can be seen in the table, all elements of an equivalence class share the same signature (by definition)." ></td>
	<td class="line x" title="83:161	However, some signatures, such as   (aiSvaryaa), correspond to an equivalence class in which every element is a morphological variation of the signature." ></td>
	<td class="line x" title="84:161	Such equivalence classes, we name them pure." ></td>
	<td class="line x" title="85:161	Some signatures represent only a subset of the members, as this set includes some members unrelated to this stem; for example, the signature   (piram), correctly corresponds to   (pirammaa), and incorrectly to the noun   (pirambu), as well as incorrectly to the adjective   (pirammaandamaana)." ></td>
	<td class="line x" title="86:161	We name such equivalence classes fuzzy." ></td>
	<td class="line x" title="87:161	Some are well formed, but may not ultimately contribute to our mining, being an abbreviation, such as ICC (in Tamil,    ), even though they are used similar to any NE in Tamil." ></td>
	<td class="line x" title="88:161	While most equivalence classes contained inflections of single stems, we also found morphological variations of several compound names in the same equivalence class such as,   (akamathakar),   (akamathaapaath), with  (akamath)." ></td>
	<td class="line x" title="89:161	4.2 Classifier for Transliteration Pair Identification We used SVM-light [Joachims, 1999], a Supportvector Machine (SVM) from Cornell University, to identify near transliterations between English and Tamil." ></td>
	<td class="line x" title="90:161	We used a seed corpus consisting of 5000 transliteration pair samples collected from a different resource, unrelated to the experimental comparable corpora." ></td>
	<td class="line x" title="91:161	In addition to the 5000 positive examples from this seed corpus, 5000 negative examples were extracted randomly, but incorrectly, aligned names from this same seed corpus and used for the classifier." ></td>
	<td class="line x" title="92:161	The features used for the classification are binary features based on the length of the pair of strings and all aligned unigram and bigram pairs, in each direction, between the two strings in the seed corpus in English and Tamil." ></td>
	<td class="line x" title="93:161	The length features include the difference in lengths between them (up to 3), and a separate binary feature if they differ by more than 3." ></td>
	<td class="line x" title="94:161	For unigram pairs, the ith character in a language string is matched to (i-1)st,  ith and (i+1)st characters of the other language string." ></td>
	<td class="line x" title="95:161	Each string is padded with special characters at the beginning and the end, for appropriately forming the unigrams for the first and the last characters of the string." ></td>
	<td class="line x" title="96:161	In the same manner, for binary features, every bigram extracted with a sliding window of size 2 from a language string, is matched with those extracted from the other language string." ></td>
	<td class="line x" title="97:161	After the classifier is trained on the seed corpus of hand crafted transliteration pairs, during the mining phase, it compares every English NE extracted from the English corpus, to every signature from the Tamil corpus." ></td>
	<td class="line x" title="98:161	While classifier provided ranked list of all the signatures from Tamil side, we consider only the top30 signatures (and the words in the equivalence classes) for subsequent steps of our methodology." ></td>
	<td class="line x" title="99:161	We hand-verified a random sample of about 100 NEs from English side, and report in Table 5, the fraction of the English NEs for which we found at least one legitimate transliteration in the top-30 candidates (for example, the  recall of the classifier is 0.56, in identifying a right signature in the top30 candidates, when the threshold FE is 10 & FT is 1)." ></td>
	<td class="line x" title="100:161	It is interesting to note that as the two threshold factors are increased, the number of NEs extracted from the English side decreases (as expected), and the average number of positive classifications per English NE reduces (as shown in Table 2), considering all NEs." ></td>
	<td class="line x" title="101:161	This makes sense as the classifier for identifying potential transliterations is trained with sizable corpora and is hence accurate; but, as the thresholds increase, it has less data to work with, and possibly a fraction of legitimate transliterations also gets filtered with noise." ></td>
	<td class="line x" title="102:161	Parameters Extracted English NEs Ave. Positive Classifications/ English NE FE: 10, FT: 1 274 79.34 FE: 5, FT: 5 588 29.50 FE: 10, FT: 10 274 17.49 FE: 20, FT: 20 125 10.55 Table 2: Threshold Parameters vs Mining Quantity  Table 3 shows some sample results after the classification step with parameter values as (FE: 10, FT: 1)." ></td>
	<td class="line x" title="103:161	Right signature for Aishwarya (corresponding to all correct transliterations) has been ranked 10 and Gandhi (with only a subset of the equivalence class corresponding to the right transliterations) has been ranked at 8." ></td>
	<td class="line x" title="104:161	Three different variations of Argentina can be found, ranked 2nd, 3rd and 13th." ></td>
	<td class="line x" title="105:161	While, in general no abbreviations are found (usually their Tamil equivalents are spelled out), a rare case of abbreviation (SAARC) and its right transliteration is ranked 1st." ></td>
	<td class="line x" title="106:161	English Named Entity Tamil Equivalence Class Signature Precision Rank aishwarya   (aiSvaryaa) 1 10 argentina   (arjantinaavila) 1 2 argentina   (aarjantinaavi) 1 3 argentina   (aarjantinaavil) 1 13 gandhi   (kaatha) 0.2121 8 saarc    (saark) 1 1 Table 3: Ranked List after Classification Step  4.3 Enhancing the Quality of TransliterationPairs For the frequency analysis, we use the frequency distribution of the words in English and Tamil side of the comparable corpora, counting the number of occurrences of NEs in English and the Tamil signatures in each temporal bin spanning the entire corpus." ></td>
	<td class="line x" title="107:161	We consider one temporal bin to be equal to two successive days." ></td>
	<td class="line x" title="108:161	Thus, each of the English NEs and the Tamil signatures is represented by a vector of dimension approximately 120." ></td>
	<td class="line x" title="109:161	We compute the distance between the two vectors, and hypothesize that they may represent the same (or, similar) name, if the difference between them is zero (or, small)." ></td>
	<td class="line x" title="110:161	Note that, as mentioned earlier, the frequency vector of the Tamil signature will contain the sum of individual frequencies of the elements in the equivalence class corresponding to it." ></td>
	<td class="line x" title="111:161	Given that the classifier step outputs a list of English NEs, and associated with each entry, a ranked list of Tamil signatures that are identified as potential transliteration by the classifier, we compute the distance between the frequency vector of every English NE, with each of the top-30 signatures in the ranked list." ></td>
	<td class="line x" title="112:161	We re-rank the top-30 candidate strings, using this distance measure." ></td>
	<td class="line x" title="113:161	The output is similar to that shown in Table 4, but with possibly a different rank order." ></td>
	<td class="line x" title="114:161	English Named Entity Tamil Equivalence Class Signature Precision Rank aishwarya   (aiSvaryaa) 1 1 argentina   (arjantinaavila) 1 1 argentina   (aarjantinaavi) 1 3 argentina   (aarjantinaavil) 1 14 gandhi   (kaatha) 0.2121 16 saarc    (saark) 1 1 Table 4: Ranked List after Frequency Analysis Step  On comparing Table 3 and 4, we observe that some of the ranks have moved for the better, and some of them for the worse." ></td>
	<td class="line x" title="115:161	It is interesting to note that the ranking of different stems corresponding to Argentina has moved differently." ></td>
	<td class="line x" title="116:161	It is quite likely that merging these three equivalence classes corresponding to the English NE Argentina might result in a frequency profile that is more closely aligned to that of the English NE." ></td>
	<td class="line x" title="117:161	4.4 Overall Performance of Transliteration Pairs Mining To find the effectiveness of each step of the mining process in identifying the right signatures (and hence, the equivalence classes) for a given English NE, we computed the Mean Reciprocal Rank (MRR) of the random sample of 100 transliteration pairs mined, in two different ways:  First, we computed MRRpure, which corresponded to the first occurrence of a pure equivalence class, and MRRfuzzy, which corresponded to the first occurrence of a fuzzy equivalence class in the random samples." ></td>
	<td class="line x" title="118:161	MRRfuzzy captures how successful the mining was in identifying one possible transliteration, MRRpure, captures how successful we were in identifying an equivalence class that contains only right transliterations2." ></td>
	<td class="line x" title="119:161	In addition, these metrics were computed, corresponding to different frequency thresholds for the occurrence of a English NE (FE) and a Tamil signature (FT)." ></td>
	<td class="line x" title="120:161	The overall quality profile of the mining framework in mining the NE transliteration pairs in English and Tamil is shown in Table 5." ></td>
	<td class="line x" title="121:161	Additionally, we also report the recall metric (the fraction of English NEs, for which at least one le 2 However, it should be noted that the current metrics neither capture how pure an equivalence class is (fraction of the set that are correct transliterations), nor the size of the equivalence class." ></td>
	<td class="line x" title="122:161	We hope to specify these as part of quality of mining, in our subsequent work." ></td>
	<td class="line x" title="123:161	gitimate Tamil signature was identified) computed on a randomly chosen 100 entity pairs." ></td>
	<td class="line x" title="124:161	Parameters Classification Step Frequency Analysis Step Recall MRR fuzzy MRR pure MRR fuzzy MRR pure FE: 10, FT: 1 0.3579 0.2831 0.3990 0.3145 0.56 FE: 5, FT: 5 0.4490 0.3305 0.5064 0.3529 0.61 FE: 10, FT: 10 0.4081 0.2731 0.4930 0.3494 0.57 FE: 20, FT: 20 0.3489 0.2381 0.4190 0.2779 0.47 Table 5: Quality Profile of NE Pairs Extraction  First, it should be noted that the recalls are the same for both the steps, since Frequency Analysis step merely re-arranges the output of the Classification step." ></td>
	<td class="line x" title="125:161	Second, the recall figures drop, as more filtering is applied to the NEs on both sides." ></td>
	<td class="line x" title="126:161	This trend makes sense, since the classifier gets less data to work with, as more legitimate words are filtered out with noise." ></td>
	<td class="line x" title="127:161	Third, as can be expected, MRRpure is less than the MRRfuzzy at every step of the mining process." ></td>
	<td class="line x" title="128:161	Fourth, we see that the MRRpure and the MRRfuzzy improve between the two mining steps, indicating that the time-series analysis has, in general, made the output better." ></td>
	<td class="line x" title="129:161	Finally, we find that the MRRpure and the MRRfuzzy keep dropping with increased filtering of English NEs and Tamil signatures based on their frequency, in both the classification and frequency analysis steps." ></td>
	<td class="line x" title="130:161	The fall of the MRRs after the classification steps is due to the fact that the classifier has less and less data with the increasing threshold, and hence some legitimate transliterations may be filtered out as noise." ></td>
	<td class="line x" title="131:161	However, the frequency analysis step critically depends on availability of sufficient words from the Tamil side for similarity testing." ></td>
	<td class="line x" title="132:161	In frequency analysis step, the fall of MRRs from threshold 5 to 10 is 0.0134 on MRRfuzzy and 0.0035 on MRRpure." ></td>
	<td class="line x" title="133:161	This fall is comparatively less to the fall of MRRs from threshold 10 to 20 which is 0.074 on MRRfuzzy and 0.0715 on MRRpure." ></td>
	<td class="line x" title="134:161	This may be due to the fact that the number of legitimate transliterations filtered out from threshold 5 to 10 is less when compared to the number of legitimate transliterations filtered out from threshold 10 to 20." ></td>
	<td class="line x" title="135:161	These results show that with less number of words filtered, it can get reasonable recall and MRR values." ></td>
	<td class="line x" title="136:161	More profiling experiments may be needed to  validate this claim." ></td>
	<td class="line x" title="137:161	5 Open Issues in NE pair Mining In this paper, we outline our experience in mining parallel NEs between English and Tamil, in an approach similar to the one discussed in [Klementiev and Roth, 2006]." ></td>
	<td class="line x" title="138:161	Over and above, we made parameter choices, and some procedural modifications to bridge the underspecified methodology given in the above work." ></td>
	<td class="line x" title="139:161	While the results are promising, we find several issues that need further research." ></td>
	<td class="line x" title="140:161	We outline some of them below: 5.1 Indistinguishable Signatures Table 7 shows a signature that offers little help in distinguishing a set of words." ></td>
	<td class="line x" title="141:161	Both the words,    (cennai) and morphological variations of    (cen), share the same 5-character signature, namely,    (cenna), affecting the frequency distribution of the signature adversely." ></td>
	<td class="line x" title="142:161	English Named Entity Tamil Named Entity Tamil Equivalent Class chennai   (cennai)    (cennai),    (cennaiyil),    (cennaiyilirunthu),    (cennin),              (cennukku),   (cennaiyai) Table 7: Multiple-Entity Equivalence Class 5.2 Abbreviations Table 8 shows a set of abbreviations, that are not identified well in our NE pair mining." ></td>
	<td class="line x" title="143:161	Between the two languages, the abbreviations may be either expanded, as BJP expanded to (the equivalent translation for Bharatiya Janatha Party in Tamil), or spelled out, as in BSNL referred to as   (pieSenel)." ></td>
	<td class="line x" title="144:161	The last example is very interesting, as each W in English is written out as   (tapiLyuu)." ></td>
	<td class="line x" title="145:161	All these are hard to capture by a simple classifier that is trained on well-formed transliteration pairs." ></td>
	<td class="line x" title="146:161	English Named Entity Tamil Named Entity BJP    (paajaka),  . . ." ></td>
	<td class="line x" title="147:161	(paa." ></td>
	<td class="line x" title="148:161	ja." ></td>
	<td class="line x" title="149:161	ka.),       (paarathiiya janathaa katci) BSNL   (pieSenel),  (pieSenellin),   (piesenellai) WWW  (tapiLyuutapiLyuutapiLyuu) Table 8: Multiple-Entity Equivalence Class 5.3 Multiword Expressions This methodology is currently designed for mining only single word expressions." ></td>
	<td class="line x" title="150:161	It may be an interesting line of research to mine multiword expressions automatically." ></td>
	<td class="line x" title="151:161	6 Related Work Our work essentially follows a similar procedure as reported in [Klementiev and Roth, 2006] paper, but applied to English-Tamil language pair." ></td>
	<td class="line x" title="152:161	Earlier works, such as [Cucerzan and Yarowsky, 1999] and [Collins and Singer, 1999] addressed identification of NEs from untagged corpora." ></td>
	<td class="line x" title="153:161	They relied on significant contextual and morphological clues." ></td>
	<td class="line x" title="154:161	[Hetland, 2004] outlined methodologies based on time distribution of terms in a corpus to identify NEs, but only in English." ></td>
	<td class="line x" title="155:161	While a large body of literature exists on transliteration, we merely point out that the focus of this work (based on [Klementiev and Roth, 2006]) is not on transliteration, but mining transliteration pairs, which may be used for developing a transliteration system." ></td>
	<td class="line x" title="156:161	7 Conclusions In this paper, we focused on mining NE transliteration pairs in two different languages, namely English and an Indian language, Tamil." ></td>
	<td class="line x" title="157:161	While we adopted a methodology similar to that in [Klementiev and Roth, 2006], our focus was on mining parallel NE transliteration pairs, leveraging the availability of comparable corpora and a well-trained linear classifier to identify transliteration pairs." ></td>
	<td class="line x" title="158:161	We profiled the performance of our mining framework on several parameters, and presented the results." ></td>
	<td class="line x" title="159:161	Our experiment results are inline with those reported by [Klementiev and Roth, 2006]." ></td>
	<td class="line x" title="160:161	Given that the NE pairs are an important resource for several NLP tasks, we hope that such a methodology to mine the comparable corpora may be fruitful, as comparable corpora may be freely available in perpetuity in several of the worlds languages." ></td>
	<td class="line x" title="161:161	8 Acknowledgements We would like to thank Raghavendra Udupa, Chris Quirk, Aasish Pappu, Baskaran Sankaran, Jagadeesh Jagarlamudi and Debapratim De for their help." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-4003
BART: A Modular Toolkit for Coreference Resolution
Versley, Yannick;Ponzetto, Simone Paolo;Poesio, Massimo;Eidelman, Vladimir;Jern, Alan;Smith, Jason;Yang, Xiaofeng;Moschitti, Alessandro;"></td>
	<td class="line x" title="1:54	Proceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 912, Columbus, June 2008." ></td>
	<td class="line x" title="2:54	c2008 Association for Computational Linguistics BART: A Modular Toolkit for Coreference Resolution Yannick Versley University of Tubingen versley@sfs.uni-tuebingen.de Simone Paolo Ponzetto EML Research gGmbH ponzetto@eml-research.de Massimo Poesio University of Essex poesio@essex.ac.uk Vladimir Eidelman Columbia University vae2101@columbia.edu Alan Jern UCLA ajern@ucla.edu Jason Smith Johns Hopkins University jsmith@jhu.edu Xiaofeng Yang Inst." ></td>
	<td class="line x" title="3:54	for Infocomm Research xiaofengy@i2r.a-star.edu.sg Alessandro Moschitti University of Trento moschitti@dit.unitn.it Abstract Developing a full coreference system able to run all the way from raw text to semantic interpretation is a considerable engineering effort, yet there is very limited availability of off-the shelf tools for researchers whose interests are not in coreference, or for researchers who want to concentrate on a specific aspect of the problem." ></td>
	<td class="line x" title="4:54	We present BART, a highly modular toolkit for developing coreference applications." ></td>
	<td class="line x" title="5:54	In the Johns Hopkins workshop on using lexical and encyclopedic knowledge for entity disambiguation, the toolkit was used to extend a reimplementation of the Soon et al.(2001) proposal with a variety of additional syntactic and knowledge-based features, and experiment with alternative resolution processes, preprocessing tools, and classifiers." ></td>
	<td class="line x" title="7:54	1 Introduction Coreference resolution refers to the task of identifying noun phrases that refer to the same extralinguistic entity in a text." ></td>
	<td class="line x" title="8:54	Using coreference information has been shown to be beneficial in a number of other tasks, including information extraction (McCarthy and Lehnert, 1995), question answering (Morton, 2000) and summarization (Steinberger et al., 2007)." ></td>
	<td class="line x" title="9:54	Developing a full coreference system, however, is a considerable engineering effort, which is why a large body of research concerned with feature engineering or learning methods (e.g. Culotta et al. 2007; Denis and Baldridge 2007) uses a simpler but non-realistic setting, using pre-identified mentions, and the use of coreference information in summarization or question answering techniques is not as widespread as it could be." ></td>
	<td class="line x" title="10:54	We believe that the availability of a modular toolkit for coreference will significantly lower the entrance barrier for researchers interested in coreference resolution, as well as provide a component that can be easily integrated into other NLP applications." ></td>
	<td class="line x" title="11:54	A number of systems that perform coreference resolution are publicly available, such as GUITAR (Steinberger et al., 2007), which handles the full coreference task, and JAVARAP (Qiu et al., 2004), which only resolves pronouns." ></td>
	<td class="line x" title="12:54	However, literature on coreference resolution, if providing a baseline, usually uses the algorithm and feature set of Soon et al.(2001) for this purpose." ></td>
	<td class="line x" title="14:54	Using the built-in maximum entropy learner with feature combination, BART reaches 65.8% F-measure on MUC6 and 62.9% F-measure on MUC7 using Soon et al.s features, outperforming JAVARAP on pronoun resolution, as well as the Soon et al. reimplementation of Uryupina (2006)." ></td>
	<td class="line x" title="15:54	Using a specialized tagger for ACE mentions and an extended feature set including syntactic features (e.g. using tree kernels to represent the syntactic relation between anaphor and antecedent, cf.Yang et al. 2006), as well as features based on knowledge extracted from Wikipedia (cf.Ponzetto and Smith, in preparation), BART reaches state-of-the-art results on ACE-2." ></td>
	<td class="line x" title="18:54	Table 1 compares our results, obtained using this extended feature set, with results from Ng (2007)." ></td>
	<td class="line x" title="19:54	Pronoun resolution using the extended feature set gives 73.4% recall, coming near specialized pronoun resolution systems such as (Denis and Baldridge, 2007)." ></td>
	<td class="line x" title="20:54	9 Figure 1: Results analysis in MMAX2 2 System Architecture The BART toolkit has been developed as a tool to explore the integration of knowledge-rich features into a coreference system at the Johns Hopkins Summer Workshop 2007." ></td>
	<td class="line x" title="21:54	It is based on code and ideas from the system of Ponzetto and Strube (2006), but also includes some ideas from GUITAR (Steinberger et al., 2007) and other coreference systems (Versley, 2006; Yang et al., 2006)." ></td>
	<td class="line x" title="22:54	1 The goal of bringing together state-of-the-art approaches to different aspects of coreference resolution, including specialized preprocessing and syntax-based features has led to a design that is very modular." ></td>
	<td class="line x" title="23:54	This design provides effective separation of concerns across several several tasks/roles, including engineering new features that exploit different sources of knowledge, designing improved or specialized preprocessing methods, and improving the way that coreference resolution is mapped to a machine learning problem." ></td>
	<td class="line x" title="24:54	Preprocessing To store results of preprocessing components, BART uses the standoff format of the MMAX2 annotation tool (Muller and Strube, 2006) with MiniDiscourse, a library that efficiently implements a subset of MMAX2s functions." ></td>
	<td class="line x" title="25:54	Using a generic format for standoff annotation allows the use of the coreference resolution as part of a larger system, but also performing qualitative error analysis using integrated MMAX2 functionality (annotation 1An open source version of BART is available from http://www.sfs.uni-tuebingen.de/versley/BART/." ></td>
	<td class="line x" title="26:54	diff, visual display)." ></td>
	<td class="line x" title="27:54	Preprocessing consists in marking up noun chunks and named entities, as well as additional information such as part-of-speech tags and merging these information into markables that are the starting point for the mentions used by the coreference resolution proper." ></td>
	<td class="line oc" title="28:54	Starting out with a chunking pipeline, which uses a classical combination of tagger and chunker, with the Stanford POS tagger (Toutanova et al., 2003), the YamCha chunker (Kudoh and Matsumoto, 2000) and the Stanford Named Entity Recognizer (Finkel et al., 2005), the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnsons reranking parser (Charniak and Johnson, 2005) to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors." ></td>
	<td class="line x" title="29:54	BART also supports using the Berkeley parser (Petrov et al., 2006), yielding an easy-to-use Java-only solution." ></td>
	<td class="line x" title="30:54	To provide a better starting point for mention detection on the ACE corpora, the Carafe pipeline uses an ACE mention tagger provided by MITRE (Wellner and Vilain, 2006)." ></td>
	<td class="line x" title="31:54	A specialized merger then discards any base NP that was not detected to be an ACE mention." ></td>
	<td class="line x" title="32:54	To perform coreference resolution proper, the mention-building module uses the markables created by the pipeline to create mention objects, which provide an interface more appropriate for coreference resolution than the MiniDiscourse markables." ></td>
	<td class="line x" title="33:54	These objects are grouped into equivalence classes by the resolution process and a coreference layer is written into the document, which can be used for detailed error analysis." ></td>
	<td class="line x" title="34:54	Feature Extraction BARTs default resolver goes through all mentions and looks for possible antecedents in previous mentions as described by Soon et al.(2001)." ></td>
	<td class="line x" title="36:54	Each pair of anaphor and candidate is represented as a PairInstance object, which is enriched with classification features by feature extractors, and then handed over to a machine learning-based classifier that decides, given the features, whether anaphor and candidate are coreferent or not." ></td>
	<td class="line x" title="37:54	Feature extractors are realized as separate classes, allowing for their independent develop10 Figure 2: Example system configuration ment." ></td>
	<td class="line x" title="38:54	The set of feature extractors that the system uses is set in an XML description file, which allows for straightforward prototyping and experimentation with different feature sets." ></td>
	<td class="line x" title="39:54	Learning BART provides a generic abstraction layer that maps application-internal representations to a suitable format for several machine learning toolkits: One module exposes the functionality of the the WEKA machine learning toolkit (Witten and Frank, 2005), while others interface to specialized state-of-the art learners." ></td>
	<td class="line x" title="40:54	SVMLight (Joachims, 1999), in the SVMLight/TK (Moschitti, 2006) variant, allows to use tree-valued features." ></td>
	<td class="line x" title="41:54	SVM Classification uses a Java Native Interface-based wrapper replacing SVMLight/TKs svm classify program to improve the classification speed." ></td>
	<td class="line x" title="42:54	Also included is a Maximum entropy classifier that is based upon Robert Dodiers translation of Liu and Nocedals (1989) L-BFGS optimization code, with a function for programmatic feature combination.2 Training/Testing The training and testing phases slightly differ from each other." ></td>
	<td class="line x" title="43:54	In the training phase, the pairs that are to be used as training examples have to be selected in a process of sample selection, whereas in the testing phase, it has to be decided which pairs are to be given to the decision function and how to group mentions into equivalence relations given the classifier decisions." ></td>
	<td class="line x" title="44:54	This functionality is factored out into the en2see http://riso.sourceforge.net coder/decoder component, which is separate from feature extraction and machine learning itself." ></td>
	<td class="line x" title="45:54	It is possible to completely change the basic behavior of the coreference system by providing new encoders/decoders, and still rely on the surrounding infrastructure for feature extraction and machine learning components." ></td>
	<td class="line x" title="46:54	3 Using BART Although BART is primarily meant as a platform for experimentation, it can be used simply as a coreference resolver, with a performance close to state of the art." ></td>
	<td class="line x" title="47:54	It is possible to import raw text, perform preprocessing and coreference resolution, and either work on the MMAX2-format files, or export the results to arbitrary inline XML formats using XSL stylesheets." ></td>
	<td class="line x" title="48:54	Adapting BART to a new coreferentially annotated corpus (which may have different rules for mention extraction  witness the differences between the annotation guidelines of MUC and ACE corpora) usually involves fine-tuning of mention creation (using pipeline and MentionFactory settings), as well as the selection and fine-tuning of classifier and features." ></td>
	<td class="line x" title="49:54	While it is possible to make radical changes in the preprocessing by re-engineering complete pipeline components, it is usually possible to achieve the bulk of the task by simply mixing and matching existing components for preprocessing and feature extraction, which is possible by modifying only configuration settings and an XML11 BNews NPaper NWire Recl Prec F Recl Prec F Recl Prec F basic feature set 0.594 0.522 0.556 0.663 0.526 0.586 0.608 0.474 0.533 extended feature set 0.607 0.654 0.630 0.641 0.677 0.658 0.604 0.652 0.627 Ng 2007 0.561 0.763 0.647 0.544 0.797 0.646 0.535 0.775 0.633 : expanded feature set in Ng 2007; Ng trains on the entire ACE training corpus." ></td>
	<td class="line x" title="50:54	Table 1: Performance on ACE-2 corpora, basic vs. extended feature set based description of the feature set and learner(s) used." ></td>
	<td class="line x" title="51:54	Several research groups focusing on coreference resolution, including two not involved in the initial creation of BART, are using it as a platform for research including the use of new information sources (which can be easily incorporated into the coreference resolution process as features), different resolution algorithms that aim at enhancing global coherence of coreference chains, and also adapting BART to different corpora." ></td>
	<td class="line x" title="52:54	Through the availability of BART as open source, as well as its modularity and adaptability, we hope to create a larger community that allows both to push the state of the art further and to make these improvements available to users of coreference resolution." ></td>
	<td class="line x" title="53:54	Acknowledgements We thank the CLSP at Johns Hopkins, NSF and the Department of Defense for ensuring funding for the workshop and to EML Research, MITRE, the Center for Excellence in HLT, and FBK-IRST, that provided partial support." ></td>
	<td class="line x" title="54:54	Yannick Versley was supported by the Deutsche Forschungsgesellschaft as part of SFB 441 Linguistic Data Structures; Simone Paolo Ponzetto has been supported by the Klaus Tschira Foundation (grant 09.003.2004)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1016
A Unified Model of Phrasal and Sentential Evidence for Information Extraction
Patwardhan, Siddharth;Riloff, Ellen;"></td>
	<td class="line x" title="1:249	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 151160, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:249	c 2009 ACL and AFNLP A Unified Model of Phrasal and Sentential Evidence for Information Extraction Siddharth Patwardhan and Ellen Riloff School of Computing University of Utah Salt Lake City, UT 84112 {sidd,riloff}@cs.utah.edu Abstract Information Extraction (IE) systems that extract role fillers for events typically look at the local context surrounding a phrase when deciding whether to extract it." ></td>
	<td class="line x" title="3:249	Often, however, role fillers occur in clauses that are not directly linked to an event word." ></td>
	<td class="line x" title="4:249	We present a new model for event extraction that jointly considers both the local context around a phrase along with the wider sentential context in a probabilistic framework." ></td>
	<td class="line x" title="5:249	Our approach uses a sentential event recognizer and a plausible role-fillerrecognizerthatisconditionedon event sentences." ></td>
	<td class="line x" title="6:249	We evaluate our system on two IE data sets and show that our model performs well in comparison to existing IE systems that rely on local phrasal context." ></td>
	<td class="line x" title="7:249	1 Introduction Information Extraction (IE) systems typically use extraction patterns (e.g., Soderland et al.(1995), Riloff (1996), Yangarber et al.(2000), Califf and Mooney (2003)) or classifiers (e.g., Freitag (1998), Freitag and McCallum (2000), Chieu et al.(2003), Bunescu and Mooney (2004)) to extract role fillers for events." ></td>
	<td class="line x" title="11:249	Most IE systems consider only the immediate context surrounding a phrase whendecidingwhethertoextractit." ></td>
	<td class="line x" title="12:249	Fortaskssuch as named entity recognition, immediate context is usually sufficient." ></td>
	<td class="line x" title="13:249	But for more complex tasks, such as event extraction, a larger field of view is often needed to understand how facts tie together." ></td>
	<td class="line x" title="14:249	Most IE systems are designed to identify role fillers that appear as arguments to event verbs or nouns, either explicitly via syntactic relations or implicitly via proximity (e.g., John murdered Tom or the murder of Tom by John)." ></td>
	<td class="line x" title="15:249	But many facts are presented in clauses that do not contain event words, requiring discourse relations or deep structural analysis to associate the facts with event roles." ></td>
	<td class="line x" title="16:249	For example, consider the sentences below: Seven people have died and 30 were injured in India after terrorists launched an attack on the Taj Hotel." ></td>
	<td class="line x" title="17:249	in Mexico City and its surrounding suburbs in a Swine Flu outbreak." ></td>
	<td class="line x" title="18:249	after a tractor-trailer collided with a bus in Arkansas." ></td>
	<td class="line x" title="19:249	Two bridges were destroyed in Baghdad last night in a resurgence of bomb attacks in the capital city." ></td>
	<td class="line x" title="20:249	and $50 million in damage was caused by a hurricane that hit Miami on Friday." ></td>
	<td class="line x" title="21:249	to make way for modern, safer bridges that will be constructed early next year." ></td>
	<td class="line x" title="22:249	These examples illustrate a common phenomenon in text where information is not explicitly stated as filling an event role, but readers have no trouble making this inference." ></td>
	<td class="line x" title="23:249	The role fillers above (seven people, two bridges) occur as arguments to verbs that reveal state information (death, destruction) but are not event-specific (i.e., death and destruction can result from a wide variety of incident types)." ></td>
	<td class="line x" title="24:249	IE systems often fail to extract these role fillers because these systems do not recognize the immediate context as being relevant to the specific type of event that they are looking for." ></td>
	<td class="line x" title="25:249	We propose a new model for information extraction that incorporates both phrasal and sentential evidence in a unified framework." ></td>
	<td class="line x" title="26:249	Our unifiedprobabilisticmodel, called GLACIER, consists of two components: a model for sentential event recognition and a model for recognizing plausible role fillers." ></td>
	<td class="line x" title="27:249	The Sentential Event Recognizer offers a probabilistic assessment of whether a sentence is discussing a domain-relevant event." ></td>
	<td class="line x" title="28:249	The 151 Plausible Role-Filler Recognizer is then conditioned to identify phrases as role fillers based upon the assumption that the surrounding context is discussing a relevant event." ></td>
	<td class="line x" title="29:249	This unified probabilistic model allows the two components to jointly make decisions based upon both the local evidence surrounding each phrase and the peripheral vision afforded by the sentential event recognizer." ></td>
	<td class="line x" title="30:249	This paper is organized as follows." ></td>
	<td class="line x" title="31:249	Section 2 positions our research with respect to related work." ></td>
	<td class="line x" title="32:249	Section 3 presents our unified probabilistic model for information extraction." ></td>
	<td class="line x" title="33:249	Section 4 shows experimental results on two IE data sets, and Section 5 discusses directions for future work." ></td>
	<td class="line x" title="34:249	2 Related Work Many event extraction systems rely heavily on the local context around words or phrases that are candidates for extraction." ></td>
	<td class="line x" title="35:249	Some systems use extraction patterns (Soderland et al., 1995; Riloff, 1996; Yangarber et al., 2000; Califf and Mooney, 2003), which represent the immediate contexts surrounding candidate extractions." ></td>
	<td class="line x" title="36:249	Similarly, classifierbased approaches (Freitag, 1998; Freitag and McCallum, 2000; Chieu et al., 2003; Bunescu and Mooney, 2004) rely on features in the immediate context of the candidate extractions." ></td>
	<td class="line x" title="37:249	Our work seeks to incorporate additional context into IE." ></td>
	<td class="line x" title="38:249	Indeed, several recent approaches have shown the need for global information to improve IE performance." ></td>
	<td class="line x" title="39:249	Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context." ></td>
	<td class="line oc" title="40:249	Finkel et al.(2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents." ></td>
	<td class="line x" title="42:249	In contrast, our approach simply creates a richer IE model for individual extractions by expanding the field of view to include the surrounding sentence." ></td>
	<td class="line x" title="43:249	The two components of the unified model presented in this paper are somewhat similar to our previous work (Patwardhan and Riloff, 2007), where we employ a relevant region identification phase prior to pattern-based extraction." ></td>
	<td class="line x" title="44:249	In that work we adopted a pipeline paradigm, where a classifier identifies relevant sentences and only those sentences are fed to the extraction module." ></td>
	<td class="line x" title="45:249	Our unified probabilistic model described in this paper does not draw a hard line between relevant and irrelevant sentences, but gently balances the influence of both local and sentential contexts through probability estimates." ></td>
	<td class="line x" title="46:249	3 A Unified IE Model that Combines Phrasal and Sentential Evidence We introduce a probabilistic model for eventbased IE that balances the influence of two kinds of contextual information." ></td>
	<td class="line x" title="47:249	Our goal is to create a model that has the flexibility to make extraction decisions based upon strong evidence from the localcontext, orstrongevidencefromthewidercontextcoupledwithamoregenerallocalcontext." ></td>
	<td class="line x" title="48:249	For example, some phrases explicitly refer to an event, so they almost certainly warrant extraction regardless of the wider context (e.g., terrorists launched an attack).1 In contrast, some phrases are potentially relevant but too general to warrant extraction on their own (e.g., people died could be the result of different incident types)." ></td>
	<td class="line x" title="49:249	If we are confident that the sentence discusses an event of interest, however, then such phrases could be reliably extracted." ></td>
	<td class="line x" title="50:249	Our unified model for IE (GLACIER) combines two types of contextual information by incorporating it into a probabilistic framework." ></td>
	<td class="line x" title="51:249	To determine whether a noun phrase instance NPi should be extracted as a filler for an event role, GLACIER computes the joint probability that NPi: (1) appears in an event sentence, and (2)isalegitimatefillerfortheeventrole." ></td>
	<td class="line x" title="52:249	Thus, GLACIER is designed for noun phrase extraction and, mathematically, its decisions are based on the following joint probability: P(EvSent(SNPi),PlausFillr(NPi)) whereSNPi isthesentencecontainingnounphrase NPi." ></td>
	<td class="line x" title="53:249	This probability estimate is based on contextual features F appearing within SNPi and in the local context of NPi." ></td>
	<td class="line x" title="54:249	Including F in the joint probability, and applying the product rule, we can split our probability into two components: P(EvSent(SNPi),PlausFillr(NPi)|F) = P(EvSent(SNPi)|F)  P(PlausFillr(NPi)|EvSent(SNPi),F) These two probability components, in the expression above, form the basis of the two modules in 1There are always exceptions of course, such as hypothetical statements, but they are relatively uncommon." ></td>
	<td class="line x" title="55:249	152 ourIEsystemthesententialeventrecognizerand the plausible role-filler recognizer." ></td>
	<td class="line x" title="56:249	In arriving at a decision to extract a noun phrase, our unified model for IE uses these modules to estimate the two probabilities based on the set of contextual features F. Note that having these two probability components allows the system to gently balance the influence from the sentential and phrasal contexts, without having to make hard decisions about sentence relevance or phrases in isolation." ></td>
	<td class="line x" title="57:249	In this system, the sentential event recognizer is embodied in the probability component P(EvSent(SNPi)|F)." ></td>
	<td class="line x" title="58:249	This is essentially the probability of a sentence describing a relevant event." ></td>
	<td class="line x" title="59:249	Similarly, the plausible rolefiller recognizer is embodied by the probability P(PlausFillr(NPi)|EvSent(SNPi),F)." ></td>
	<td class="line x" title="60:249	This component, therefore, estimates the probability that a noun phrase fills a specific event role, assuming that the noun phrase occurs in an event sentence." ></td>
	<td class="line x" title="61:249	Manydifferenttechniquescouldbeused to produce these probability estimates." ></td>
	<td class="line x" title="62:249	In the rest of this section, we present the specific models that we used for each of these components." ></td>
	<td class="line x" title="63:249	3.1 Plausible Role-Filler Recognizer The plausible role-filler recognizer is similar to most traditional IE systems, where the goal is to determine whether a noun phrase can be a legitimatefillerforaspecifictypeofeventrolebasedon its local context." ></td>
	<td class="line x" title="64:249	Pattern-based approaches match the context surrounding a phrase using lexicosyntactic patterns or rules." ></td>
	<td class="line x" title="65:249	However, most of these approaches do not produce probability estimates for the extractions." ></td>
	<td class="line x" title="66:249	Classifier-based approaches use machine learning classifiers to make extraction decisions, based on features associated with the local context." ></td>
	<td class="line x" title="67:249	Any classifier that can generate probability estimates, or similar confidence values, could be plugged into our model." ></td>
	<td class="line x" title="68:249	In our work, we use a Nave Bayes classifier as our plausible role-filler recognizer." ></td>
	<td class="line x" title="69:249	The probabilities are computed using a generative Nave Bayes framework, based on local contextual features surrounding a noun phrase." ></td>
	<td class="line x" title="70:249	These clues include lexical matches, semantic features, and syntactic relations, and will be described in more detail in Section 3.3." ></td>
	<td class="line x" title="71:249	The Nave Bayes (NB) plausible rolefiller recognizer is defined as follows: P(PlausFillr(NPi)|EvSent(SNPi),F) = 1 ZP(PlausFillr(NPi)|EvSent(SNPi)) productdisplay fiF P(fi|PlausFillr(NPi),EvSent(SNPi)) where F is the set of local contextual features and Z is the normalizing constant." ></td>
	<td class="line x" title="72:249	The prior P(PlausFillr(NPi)|EvSent(SNPi)) is estimated from the fraction of role fillers in the training data." ></td>
	<td class="line x" title="73:249	The product term in the equation is the likelihood, which makes the simplifying assumption that all of the features in F are independent of one another." ></td>
	<td class="line x" title="74:249	It is important to note that these probabilities are conditioned on the noun phrase NPi appearing in an event sentence." ></td>
	<td class="line x" title="75:249	Most IE systems need to extract several different types of role fillers for each event." ></td>
	<td class="line x" title="76:249	For instance, to extract information about terrorist incidents a system may extract the names of perpetrators, victims, targets, and weapons." ></td>
	<td class="line x" title="77:249	We create a separate IE model for each type of event role." ></td>
	<td class="line x" title="78:249	To construct a unified IE model for an event role, we must specifically create a plausible role-filler recognizer for that event role, but we can use a single sentential event recognizer for all of the role filler types." ></td>
	<td class="line x" title="79:249	3.2 Sentential Event Recognizer Thetaskathandforthesententialeventrecognizer is to analyze features in a sentence and estimate the probability that the sentence is discussing a relevant event." ></td>
	<td class="line x" title="80:249	This is very similar to the task performed by text classification systems, with some minor differences." ></td>
	<td class="line x" title="81:249	Firstly, we are dealing with the classification of sentences, as opposed to entire documents." ></td>
	<td class="line x" title="82:249	Secondly, we need to generate a probability estimate of the class, and not just a class label." ></td>
	<td class="line x" title="83:249	Like the plausible role-filler recognizer, here too we employ machine learning classifiers to estimate the desired probabilities." ></td>
	<td class="line x" title="84:249	3.2.1 Nave Bayes Event Recognizer Since Nave Bayes classifiers estimate class probabilities, we employ such a classifier to create a sentential event recognizer: P(EvSent(SNPi)|F) = 1 ZP(EvSent(SNPi))  productdisplay fiF P(fi|EvSent(SNPi)) where Z is the normalizing constant and F is the set of contextual features in the sentence." ></td>
	<td class="line x" title="85:249	The 153 prior P(EvSentS(NPi)) is obtained from the ratio of event and non-event sentences in the training data." ></td>
	<td class="line x" title="86:249	The product term in the equation is the likelihood, which makes the simplifying assumption that the features in F are independent of one another." ></td>
	<td class="line x" title="87:249	The features used by the model will be described in Section 3.3." ></td>
	<td class="line x" title="88:249	A known issue with Nave Bayes classifiers is that, even though their classification accuracy is often quite reasonable, their probability estimates are often poor (Domingos and Pazzani, 1996; Zadrozny and Elkan, 2001; Manning et al., 2008)." ></td>
	<td class="line x" title="89:249	Theproblemisthattheseclassifierstendtooverestimatetheprobabilityofthepredictedclass,resulting in a situation where most probability estimates fromtheclassifiertendtobeeitherextremelyclose to 0.0 or extremely close to 1.0." ></td>
	<td class="line x" title="90:249	We observed this problem in our classifier too, so we decided to explore an additional model to estimate probabilities for the sentential event recognizer." ></td>
	<td class="line x" title="91:249	This second model, based on SVMs, is described next." ></td>
	<td class="line x" title="92:249	3.2.2 SVM Event Recognizer Given the all-or-nothing nature of the probability estimates that we observed from the Nave Bayes model, we decided to try using a Support Vector Machine (SVM) (Vapnik, 1995; Joachims, 1998) classifier as an alternative to Nave Bayes." ></td>
	<td class="line x" title="93:249	One of the issues with doing this is that SVMs are not probabilisticclassifiers." ></td>
	<td class="line x" title="94:249	SVMsmakeclassification decisions using on a decision boundary defined by support vectors identified during training." ></td>
	<td class="line x" title="95:249	A decision function is applied to unseen test examples to determine which side of the decision boundary those examples lie." ></td>
	<td class="line x" title="96:249	While the values obtained from the decision function only indicate class assignments for the examples, we used these values to produce confidence scores for our sentential event recognizer." ></td>
	<td class="line x" title="97:249	To produce a confidence score from the SVM classifier,wetakethevaluesgeneratedbythedecision function for each test instance and normalize them based on the minimum and maximum values produced across all of the test instances." ></td>
	<td class="line x" title="98:249	This normalization process produces values between 0 and 1thatweuseasaroughindicatoroftheconfidence in the SVMs classification." ></td>
	<td class="line x" title="99:249	We observed that we could effect a consistent recall/precision trade-off by using these values as thresholds for classification decisions, which suggests that this approach worked reasonably well for our task." ></td>
	<td class="line x" title="100:249	3.3 Contextual Features We used a variety of contextual features in both components of our system." ></td>
	<td class="line x" title="101:249	The plausible rolefiller recognizer uses the following types of features for each candidate noun phrase NPi: lexical headofNPi, semanticclassofNPislexicalhead, named entity tags associated with NPi and lexicosyntactic patterns that represent the local context surrounding NPi." ></td>
	<td class="line x" title="102:249	The feature set is automatically generated from the texts." ></td>
	<td class="line x" title="103:249	Each feature is assigned a binary value for each instance, indicating either the presence or absence of the feature." ></td>
	<td class="line pc" title="104:249	The named-entity features are generated by the freely available Stanford NER tagger (Finkel et al., 2005)." ></td>
	<td class="line x" title="105:249	We use the pre-trained NER model that comes with the software to identify person, organization and location names." ></td>
	<td class="line x" title="106:249	The syntactic and semantic features are generated by the Sundance/AutoSlog system (Riloff and Phillips, 2004)." ></td>
	<td class="line x" title="107:249	We use the Sundance shallow parser to identify lexical heads, and use its semantic dictionaries to assign semantic features to words." ></td>
	<td class="line x" title="108:249	The AutoSlog pattern generator (Riloff, 1996) is used to create the lexico-syntactic pattern features that capture local context around each noun phrase." ></td>
	<td class="line x" title="109:249	Our training sets produce a very large number of features, which initially bogged down our classifiers." ></td>
	<td class="line x" title="110:249	Consequently, we reduced the size of the feature set by discarding all features that appeared four times or less in the training set." ></td>
	<td class="line x" title="111:249	Our sentential event recognizer uses the same contextual features as the plausible role-filler recognizer, except that features are generated for every NP in the sentence." ></td>
	<td class="line x" title="112:249	In addition, it uses three types of sentence-level features: sentence length, bag of words, and verb tense, which are also binary features." ></td>
	<td class="line x" title="113:249	We have two binary sentence length features indicating that the sentence is long (greater than 35 words) or is short (shorter than 5 words)." ></td>
	<td class="line x" title="114:249	Additionally, all of the words in each sentence in the training data are generated as bag of words features for the sentential model." ></td>
	<td class="line x" title="115:249	Finally, we generate verb tense features from all verbs appearing in each sentence." ></td>
	<td class="line x" title="116:249	Here too we apply a frequency cutoff and eliminate all features that appear four times or less in the training data." ></td>
	<td class="line x" title="117:249	4 IE Evaluation 4.1 Data Sets We evaluated the performance of our IE system on two data sets: the MUC-4 terrorism corpus (Sund154 heim, 1992), and a ProMed disease outbreaks corpus (Phillips and Riloff, 2007; Patwardhan and Riloff, 2007)." ></td>
	<td class="line x" title="118:249	The MUC-4 data set is a standard IE benchmark collection of news stories about terrorist events." ></td>
	<td class="line x" title="119:249	It contains 1700 documents divided into 1300 development (DEV) texts, and four test sets of 100 texts each (TST1, TST2, TST3, and TST4)." ></td>
	<td class="line x" title="120:249	Unless otherwise stated, our experiments adopted the same training/test split used in previous research: the 1300 DEV texts for training, 200 texts (TST1+TST2) for tuning, and 200 texts (TST3+TST4) as the blind test set." ></td>
	<td class="line x" title="121:249	We evaluated our system on five MUC-4 string roles: perpetrator individuals, perpetrator organizations, physical targets, victims, and weapons." ></td>
	<td class="line x" title="122:249	The ProMed corpus consists of 120 documents obtained from ProMed-mail2, a freely accessible global electronic reporting system for outbreaks of diseases." ></td>
	<td class="line x" title="123:249	These 120 documents are paired with corresponding answer key templates." ></td>
	<td class="line x" title="124:249	Unless otherwise noted, all of our experiments on this data set used 5-fold cross validation." ></td>
	<td class="line x" title="125:249	We extracted two types of event roles: diseases and victims3." ></td>
	<td class="line x" title="126:249	Unlike some other IE data sets, many of the texts in these collections do not describe a relevant event." ></td>
	<td class="line x" title="127:249	Only about half of the MUC-4 articlesdescribeaspecificterroristincident4,andonly about 80% of the ProMed articles describe a disease outbreak." ></td>
	<td class="line x" title="128:249	The answer keys for the irrelevant documents are therefore empty." ></td>
	<td class="line x" title="129:249	IE systems are especially susceptible to false hits when they can be given texts that contain no relevant events." ></td>
	<td class="line x" title="130:249	The complete IE task involves the creation of answer key templates, one template per incident (many documents in our data sets describe multiple events)." ></td>
	<td class="line x" title="131:249	Our work focuses on accurately extracting the facts from the text and not on template generation per se (e.g., we are not concerned with coreference resolution or which extraction belongs in which template)." ></td>
	<td class="line x" title="132:249	Consequently, our experiments evaluate the accuracy of the extractions individually." ></td>
	<td class="line x" title="133:249	We used head noun scoring, where an extraction is considered to be correct if its head noun matches the head noun in the answer key.5 2http://www.promedmail.org 3The victims can be people, animals, or plants." ></td>
	<td class="line x" title="134:249	4With respect to the definition of terrorist incidents in the MUC-4 guidelines (Sundheim, 1992)." ></td>
	<td class="line x" title="135:249	5Pronounswerediscardedfromboththesystemresponses and the answer keys since we do not perform coreference resolution." ></td>
	<td class="line x" title="136:249	Duplicate extractions (e.g., the same string extracted multiple times from the same document) were conflated before being scored, so they count as just one hit or one miss. 4.2 Baselines We generated three baselines to use as comparisons with our IE system." ></td>
	<td class="line x" title="137:249	As our first baseline, we used AutoSlog-TS (Riloff, 1996), which is a weakly-supervised, pattern-based IE system available as part of the Sundance/AutoSlog software package (Riloff and Phillips, 2004)." ></td>
	<td class="line x" title="138:249	Our previous work in event-based IE (Patwardhan and Riloff, 2007) also used a pattern-based approach that applied semantic affinity patterns to relevant regions in text." ></td>
	<td class="line x" title="139:249	We use this system as our second baseline." ></td>
	<td class="line x" title="140:249	As a third baseline, we trained a Nave Bayes IE classifier that is analogous to the plausible rolefiller recognizer in our unified IE model, except that this baseline system is not conditioned on the assumption of having an event sentence." ></td>
	<td class="line x" title="141:249	Consequently, this baseline NB classifier is akin to a traditional supervised learning-based IE system that uses only local contextual features to make extraction decisions." ></td>
	<td class="line x" title="142:249	Formally, the baseline NB classifier uses the formula: P(PlausFillr(NPi)|F) = 1 ZP(PlausFillr(NPi))  productdisplay fiF P(fi|PlausFillr(NPi)) where F is the set of local features, P(PlausFillr(NPi)) is the prior probability, and Z is the normalizing constant." ></td>
	<td class="line x" title="143:249	We used the Weka (Witten and Frank, 2005) implementation of Nave Bayes for this baseline NB system." ></td>
	<td class="line x" title="144:249	New Jersey, February, 26." ></td>
	<td class="line x" title="145:249	An outbreak of Ebola has been confirmed in Mercer County, New Jersey." ></td>
	<td class="line x" title="146:249	Five teenage boys appear to have contracted the deadly virus from an unknown source." ></td>
	<td class="line x" title="147:249	The CDC is investigating the cases and is taking measures to prevent the spread Disease: Ebola Victims: Five teenage boys Location: Mercer County, New Jersey Date: February 26 Figure 1: A Disease Outbreak Event Template Both the MUC-4 and ProMed data sets have separate answer keys rather than annotated source documents." ></td>
	<td class="line x" title="148:249	Figure 1 shows an example of a document and its corresponding answer key template." ></td>
	<td class="line x" title="149:249	To train the baseline NB system, we identify all instances of each answer key string in the source document and consider every instance a positive training example." ></td>
	<td class="line x" title="150:249	This produces noisy training data, however, because some instances occur in 155 PerpInd PerpOrg Target Victim Weapon P R F P R F P R F P R F P R F AutoSlog-TS .33 .49 .40 .52 .33 .41 .54 .59 .56 .49 .54 .51 .38 .44 .41 Sem Affinity .48 .39 .43 .36 .58 .45 .56 .46 .50 .46 .44 .45 .53 .46 .50 NB .50 .36 .34 .35 .35 .46 .40 .53 .49 .51 .50 .50 .50 1.00 .05 .10 NB .70 .41 .25 .31 .43 .31 .36 .58 .42 .48 .58 .37 .45 1.00 .04 .07 NB .90 .51 .17 .25 .56 .15 .24 .67 .30 .41 .75 .23 .36 1.00 .02 .04 Table 1: Baseline Results on MUC-4 Disease Victim P R F P R F AutoSlog-TS .33 .60 .43 .36 .49 .41 Sem Affinity .31 .49 .38 .41 .47 .44 NB .50 .20 .73 .31 .29 .56 .39 NB .70 .23 .67 .34 .37 .52 .44 NB .90 .34 .59 .43 .47 .39 .43 Table 2: Baseline Results on ProMed undesirable contexts." ></td>
	<td class="line x" title="151:249	For example, if the string man appears in an answer key as a victim, one instance of man may refer to the actual victim in an event sentence, while another instance of man may occur in a non-event context (e.g., background information) or may refer to a completely different person." ></td>
	<td class="line x" title="152:249	We report three evaluation metrics in our experiments: precision (P), recall (R), and F-score (F), where recall and precision are equally weighted." ></td>
	<td class="line x" title="153:249	For the Nave Bayes classifier, the natural threshold for distinguishing between positive and negative classes is 0.5, but we also evaluated this classifier with thresholds of 0.7 and 0.9 to see if we could effect a recall/precision trade-off." ></td>
	<td class="line x" title="154:249	Tables 1 and 2 present the results of our three baseline systems." ></td>
	<td class="line x" title="155:249	The NB classifier performs comparably to AutoSlog-TS and Semantic Affinity on most event roles, although a threshold of 0.90 is needed to reach comparable performance on ProMed." ></td>
	<td class="line x" title="156:249	The relatively low numbers across the board indicate that these corpora are challenging, but these results suggest that our plausible role-filler recognizer is competitive with other existing IE systems." ></td>
	<td class="line x" title="157:249	In Section 4.4 we will show how our unified IE model compares to these baselines." ></td>
	<td class="line x" title="158:249	But before that (in the next section) we evaluate the quality of the second component of our IE system: the sentential event recognizer." ></td>
	<td class="line x" title="159:249	4.3 Sentential Event Recognizer Models The sentential event recognizer is one of the core contributions of this research, so in this section we evaluate it by itself, before we employ it within the unified framework." ></td>
	<td class="line x" title="160:249	The purpose of the sentential event recognizer is to determine whether a sentence is discussing a domain-relevant event." ></td>
	<td class="line x" title="161:249	For our data sets, the classifier must decide whether a sentence is discussing a terrorist incident (MUC4) or a disease outbreak (ProMed)." ></td>
	<td class="line x" title="162:249	Ideally, we want such a classifier to operate independently from the answer keys and the extraction task per se." ></td>
	<td class="line x" title="163:249	For example, a terrorism IE system could be designed to extract only perpetrators and victims of terrorist events, or it could be designed to extract only targets and locations." ></td>
	<td class="line x" title="164:249	The job of the sentential event recognizer remains the same: to identifysentencesthatdiscussaterroristevent." ></td>
	<td class="line x" title="165:249	Howto train and evaluate such a system is a difficult question." ></td>
	<td class="line x" title="166:249	In this section, we present two approaches that we explored to generate the training data: (a) using the IE answer keys, and (b) using human judgements." ></td>
	<td class="line x" title="167:249	4.3.1 Sentence Annotation via Answer Keys We have argued that the event relevance of a sentence should not be tied to a specific set of event roles." ></td>
	<td class="line x" title="168:249	However, the IE answer keys can be used to identify some sentences that describe an event, because they contain an answer string." ></td>
	<td class="line x" title="169:249	So we can map the answer strings back to sentences in the source documents to automatically generate event sentence annotations.6 These annotations will be noisy,though,becauseananswerstringcanappear in a non-event sentence, and some event sentences may not contain any answer strings." ></td>
	<td class="line x" title="170:249	The alternative, however, is sentence annotations by humans, which (as we will discuss in Section 4.3.2) is challenging." ></td>
	<td class="line x" title="171:249	4.3.2 Sentence Annotation via Human Judgements For many sentences there is a clear consensus amongpeoplethataneventisbeingdiscussed." ></td>
	<td class="line x" title="172:249	For example, most readers would agree that sentence (1) below is describing a terrorist event, while sen6A similar strategy was used in previous work (Patwardhan and Riloff, 2007) to generate a test set for the evaluation of a relevant region classifier." ></td>
	<td class="line x" title="173:249	156 Evaluation on Answer Keys Evaluation on Human Annotations Event Non-Event Event Non-Event Acc Pr Rec F Pr Rec F Acc Pr Rec F Pr Rec F MUC-4 (Terrorism) An s NB .80 .57 .55 .56 .86 .87 .87 .81 .46 .60 .52 .91 .85 .88 SVM .80 .68 .42 .52 .84 .93 .88 .83 .55 .44 .49 .88 .91 .90 Hu m NB .82 .64 .48 .55 .85 .92 .88 .85 .56 .57 .57 .91 .91 .91 SVM .79 .64 .41 .50 .83 .91 .87 .84 .62 .51 .56 .90 .91 .91 ProMed (Disease Outbreaks) An s NB .75 .62 .61 .61 .81 .82 .82 .72 .43 .58 .50 .86 .77 .81 SVM .74 .78 .31 .44 .74 .95 .83 .76 .51 .26 .35 .80 .92 .86 Hu m NB .73 .61 .46 .52 .77 .86 .81 .79 .56 .57 .56 .87 .86 .86 SVM .70 .62 .32 .42 .73 .89 .81 .79 .62 .42 .50 .84 .90 .87 Table 3: Sentential Event Recognizers Results (5-fold Cross-Validation) Evaluation on Human Annotations Event Non-Event Acc Pr Rec F Pr Rec F NB .83 .50 .70 .58 .94 .86 .90 SVM .89 .83 .39 .53 .89 .98 .94 Table 4: Sentential Event Recognizer Results for MUC-4 using 1300 Documents for Training tence (2) is not." ></td>
	<td class="line x" title="174:249	However it is difficult to draw a clear line." ></td>
	<td class="line x" title="175:249	Sentence (3), for example, describes an action taken in response to a terrorist event." ></td>
	<td class="line x" title="176:249	Is this a terrorist event sentence?" ></td>
	<td class="line x" title="177:249	Precisely how to define an event sentence is not obvious." ></td>
	<td class="line x" title="178:249	(1) Al Qaeda operatives launched an attack on the Madrid subway system." ></td>
	<td class="line x" title="179:249	(2) Madrid has a population of about 3.2 million people." ></td>
	<td class="line x" title="180:249	(3) City officials stepped up security in response to the attacks." ></td>
	<td class="line x" title="181:249	We tackled this issue by creating detailed annotation guidelines to define the notion of an event sentence, and conducting a human annotation study." ></td>
	<td class="line x" title="182:249	The guidelines delineated a general time frame for the beginning and end of an event, and constrained the task to focus on specific incidents that were reported in the IE answer key." ></td>
	<td class="line x" title="183:249	We gave the annotators a brief description (e.g., murder in Peru) of each event that had a filled answer key in the data set." ></td>
	<td class="line x" title="184:249	They only labeled sentences that discussed those particular events." ></td>
	<td class="line x" title="185:249	We employed two human judges, who annotated 120 documents from the ProMed test set, and 100 documents from the MUC-4 test set." ></td>
	<td class="line x" title="186:249	We asked both judges to label 30 of the same documentsfromeach data setsothat we couldcompute inter-annotator agreement." ></td>
	<td class="line x" title="187:249	The annotators had an agreement of 0.72 Cohens  on the ProMed data, and 0.77 Cohens  on the MUC-4 data." ></td>
	<td class="line x" title="188:249	Given the difficulty of this task, we were satisfied that this task is reasonably well-defined and the annotations are of good quality." ></td>
	<td class="line x" title="189:249	4.3.3 Event Recognizer Results We evaluated the two sentential event recognizer models described in Section 3.2 in two ways: (1) using the answer key sentence annotations for training/testing, and (2) using the human annotations for training/testing." ></td>
	<td class="line x" title="190:249	Table 3 shows the results for all combinations of training/testing data." ></td>
	<td class="line x" title="191:249	Since we only have human annotations for 100 MUC-4textsand120ProMedtexts, weperformed 5-fold cross-validation on these documents." ></td>
	<td class="line x" title="192:249	For our classifiers, we used the Weka (Witten and Frank, 2005) implementation of Nave Bayes and the SVMLight (Joachims, 1998) implementation of the SVM." ></td>
	<td class="line x" title="193:249	For each classifier we report overall accuracy, and precision, recall and F-scores with respect to both the positive and negative classes (event vs. non-event sentences)." ></td>
	<td class="line x" title="194:249	The rows labeled Ans show the results for models trained via answer keys, and the rows labeled Hum show the results for the models trained with human annotations." ></td>
	<td class="line x" title="195:249	The left side of the table shows the results using the answer key annotations forevaluation, andtherightsideofthetableshows theresultsusingthehumanannotationsforevaluation." ></td>
	<td class="line x" title="196:249	One expects classifiers to perform best when they are trained and tested on the same type of data, and our results bear this out  the classifiers that were trained and tested on the same kind of annotations do best." ></td>
	<td class="line x" title="197:249	The boldfaced numbers represent the best accuracies achieved for each domain." ></td>
	<td class="line x" title="198:249	As we would expect, the classifiers that are both trained and tested with human annotations (Hum) show the best performance, with the Nave Bayes achieving the best accuracy of 85% on the 157 PerpInd PerpOrg Target Victim Weapon P R F P R F P R F P R F P R F AutoSlog-TS .33 .49 .40 .52 .33 .41 .54 .59 .56 .49 .54 .51 .38 .44 .41 Sem Affinity .48 .39 .43 .36 .58 .45 .56 .46 .50 .46 .44 .45 .53 .46 .50 NB (baseline) .36 .34 .35 .35 .46 .40 .53 .49 .51 .50 .50 .50 1.00 .05 .10 GLACIER NB/NB .90 .39 .59 .47 .33 .51 .40 .39 .72 .51 .52 .54 .53 .47 .55 .51 NB/SVM .40 .51 .58 .54 .34 .45 .38 .42 .72 .53 .55 .58 .56 .57 .53 .55 NB/SVM .50 .66 .47 .55 .41 .26 .32 .50 .62 .55 .62 .36 .45 .64 .43 .52 Table 5: Unified IE Model on MUC-4 MUC-4 texts, and the SVM achieving the best accuracy of 79% on the ProMed texts." ></td>
	<td class="line x" title="199:249	Therecallandprecisionfornon-eventsentences is much higher than for event sentences." ></td>
	<td class="line x" title="200:249	This classifier is forced to draw a hard line between the event and non-event sentences, which is a difficult task even for people." ></td>
	<td class="line x" title="201:249	One of the advantages of our unified IE model, which will be described in the next section, is that it does not require hard decisions but instead uses a probabilistic estimate of how event-ish a sentence is. Table 3 showed that models trained on human annotations outperform models trained on answer key annotations." ></td>
	<td class="line x" title="202:249	But with the MUC-4 data, we have the luxury of 1300 training documents with answer keys, while we only have 100 documents with human annotations." ></td>
	<td class="line x" title="203:249	Even though the answer key annotations are noisier, we have 13 times as much training data." ></td>
	<td class="line x" title="204:249	So we trained another sentential event recognizer using the entire MUC-4 training set." ></td>
	<td class="line x" title="205:249	These results are shown in Table 4." ></td>
	<td class="line x" title="206:249	Observe that using this larger (albeit noisy) training data does not appear to affect the Nave Bayes model very much." ></td>
	<td class="line x" title="207:249	Compared with the model trained on 100 manually annotated documents, its accuracy decreases by 2% from 85% to 83%." ></td>
	<td class="line x" title="208:249	The SVM model, on the other hand, achieves an 89% accuracy when trained with the larger MUC-4 training data, compared to 84% accuracy for the model trained from the 100 manually labeled documents." ></td>
	<td class="line x" title="209:249	Consequently, the sentential event recognizer models used in our unified IE framework (described in Section 4.4) are trained with this 1300 document training set." ></td>
	<td class="line x" title="210:249	4.4 Evaluation of the Unified IE Model WenowevaluatetheperformanceofourunifiedIE model, GLACIER, which allows a plausible rolefiller recognizer and a sentential event recognizer to make joint decisions about phrase extractions." ></td>
	<td class="line x" title="211:249	Tables 5 and 6 present the results of the unified Disease Victim P R F P R F AutoSlog-TS .33 .60 .43 .36 .49 .41 Sem Affinity .31 .49 .38 .41 .47 .44 NB (baseline) .34 .59 .43 .47 .39 .43 GLACIER NB/NB .90 .41 .61 .49 .38 .52 .44 NB/SVM .40 .31 .66 .42 .32 .55 .41 NB/SVM .50 .38 .54 .44 .42 .47 .44 Table 6: Unified IE Model on ProMed IE model on the MUC-4 and ProMed data sets." ></td>
	<td class="line x" title="212:249	The NB/NB systems use Nave Bayes models for both components, while the NB/SVM systems use a Nave Bayes model for the plausible role-filler recognizer and an SVM for the sentential event recognizer." ></td>
	<td class="line x" title="213:249	As with our baseline system, we obtain good results using a threshold of 0.90 for our NB/NB model (i.e., only NPs with probability  0.90 are extracted)." ></td>
	<td class="line x" title="214:249	For our NB/SVM models, we evaluatedusingthedefaultthreshold(0.50)butobserved that recall was sometimes low." ></td>
	<td class="line x" title="215:249	So we also use a threshold of 0.40, which produces superior results." ></td>
	<td class="line x" title="216:249	Here too, we used the Weka (Witten and Frank, 2005) implementation of the Nave Bayes model and the SVMLight (Joachims, 1998) implementation of the SVM." ></td>
	<td class="line x" title="217:249	For the MUC-4 data, our unified IE model using the SVM (0.40) outperforms all 3 baselines on three roles (PerpInd, Victim, Weapon) and outperforms 2 of the 3 baselines on the Target role." ></td>
	<td class="line x" title="218:249	When GLACIER outperforms the other systems it is often by a wide margin: the F-score for PerpInd jumped from 0.43 for the best baseline (Sem Affinity) to 0.54 for GLACIER, and the F-scores for Victim and Weapon each improved by 5% over the best baseline." ></td>
	<td class="line x" title="219:249	These gains came frombothincreasedrecallandincreasedprecision, demonstrating that GLACIER extracts some information that was missed by the other systems and is also less prone to false hits." ></td>
	<td class="line x" title="220:249	Only the PerpOrg role shows inferior performance." ></td>
	<td class="line x" title="221:249	Organizations perpetrating a terrorist 158 event are often discussed later in a document, far removed from the main event description." ></td>
	<td class="line x" title="222:249	For example, a statement that Al Qaeda is believed to be responsible for an attack would typically appear after the event description." ></td>
	<td class="line x" title="223:249	As a result, the sentential event recognizer tends to generate low probabilities for such sentences." ></td>
	<td class="line x" title="224:249	We believe that addressing this issue would require the use of discourse relations or the use of even larger context sizes." ></td>
	<td class="line x" title="225:249	We intend to explore these avenues of research in future work." ></td>
	<td class="line x" title="226:249	On the ProMed data, GLACIER produces results thataresimilartothebaselinesfortheVictimrole, but it outperforms the baselines for the Disease role." ></td>
	<td class="line x" title="227:249	We find that for this domain, the unified IE model with the Nave Bayes sentential event recognizer is superior to the unified IE model with the SVM classifier." ></td>
	<td class="line x" title="228:249	For the Disease role, the Fscore jumped 6%, from 0.43 for the best baseline systems (AutoSlog-TS and the NB baseline) to 0.49 for GLACIERNB/NB." ></td>
	<td class="line x" title="229:249	In contrast to the MUC-4 data, this improvement was mostly due to an increase in precision (up to 0.41), indicating that our unified IE model was effective at eliminating many false hits." ></td>
	<td class="line x" title="230:249	For the Victim role, the performance of the unified model is comparable to the baselines." ></td>
	<td class="line x" title="231:249	On this event role, the F-score of GLACIERNB/NB (0.44) matches that of the best baseline system (Sem Affinity, with 0.44)." ></td>
	<td class="line x" title="232:249	However, note that GLACIERNB/NB can achieve a 5% gain in recall over this baseline, at the cost of a 3% precision loss." ></td>
	<td class="line x" title="233:249	4.5 Specific Examples Figure 2 presents some specific examples of extractions that are failed to be extracted by the baseline models, but are correctly identified by GLACIER becauseofitsuseofsententialevidence." ></td>
	<td class="line x" title="234:249	Observe that in each of these examples, GLACIER correctly extracts the underlined phrases, in spite of the inconclusive evidence in the local contexts around them." ></td>
	<td class="line x" title="235:249	In the last sentence in Figure 2, for example, GLACIER correctly makes the inference that the policemen in the bus (which was traveling on the bridge) are likely the victims of the terrorist event." ></td>
	<td class="line x" title="236:249	Thus, we see that our system manages to balance the influence of the two probability components to make extraction decisions that would be impossible to make by relying only on the local phrasal context." ></td>
	<td class="line x" title="237:249	In addition, the sentential event recognizer can also help improve precision by preTHE MNR REPORTED ON 12 JANUARY THAT HEAVILY ARMED MEN IN CIVILIAN CLOTHES HAD INTERCEPTED A VEHICLE WITH OQUELI AND FLORES ENROUTE FOR LA AURORA AIRPORT AND THAT THE TWO POLITICAL LEADERS HAD BEEN KIDNAPPED AND WERE REPORTED MISSING." ></td>
	<td class="line x" title="238:249	PerpInd: HEAVILY ARMED MEN THE SCANT POLICE INFORMATION SAID THAT THE DEVICES WERE APPARENTLY LEFT IN FRONT OF THE TWO BANK BRANCHES MINUTES BEFORE THE CURFEW BEGAN FOR THE 6TH CONSECUTIVE DAY  PRECISELY TO COUNTER THE WAVE OF TERRORISM CAUSED BY DRUG TRAFFICKERS." ></td>
	<td class="line x" title="239:249	Weapon: THE DEVICES THOSE WOUNDED INCLUDE THREE EMPLOYEES OF THE GAS STATION WHERE THE CAR BOMB WENT OFF AND TWO PEOPLE WHO WERE WALKING BY THE GAS STATION AT THE MOMENT OF THE EXPLOSION." ></td>
	<td class="line x" title="240:249	Victim: THREE EMPLOYEES OF THE GAS STATION Victim: TWO PEOPLE MEMBERS OF THE BOMB SQUAD HAVE DEACTIVATED A POWERFUL BOMB PLANTED AT THE ANDRES AVELINO CACERES PARK, WHERE PRESIDENT ALAN GARCIA WAS DUE TO PARTICIPATE IN THE COMMEMORATION OF THE BATTLE OF TARAPACA." ></td>
	<td class="line x" title="241:249	Victim: PRESIDENT ALAN GARCIA EPL [POPULAR LIBERATION ARMY] GUERRILLAS BLEW UP A BRIDGE AS A PUBLIC BUS, IN WHICH SEVERAL POLICEMEN WERE TRAVELING, WAS CROSSING IT." ></td>
	<td class="line x" title="242:249	Victim: SEVERAL POLICEMEN Figure 2: Examples of GLACIER Extractions venting extractions from non-event sentences." ></td>
	<td class="line x" title="243:249	5 Conclusions We presented a unified model for IE that balances the influence of sentential context with local contextual evidence to improve the performance of event-based IE." ></td>
	<td class="line x" title="244:249	Our experimental results showed that using sentential contexts indeed produced better results on two IE data sets." ></td>
	<td class="line x" title="245:249	Our current model uses supervised learning, so one direction for future work is to adapt the model for weakly supervised learning." ></td>
	<td class="line x" title="246:249	We also plan to incorporate discourse features and investigate even wider contexts to capture broader discourse effects." ></td>
	<td class="line x" title="247:249	Acknowledgments This work has been supported in part by the Department of Homeland Security Grant N0014-071-0152." ></td>
	<td class="line x" title="248:249	We are grateful to Nathan Gilbert and Adam Teichert for their help with the annotation of event sentences." ></td>
	<td class="line x" title="249:249	159" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1057
Investigation of Question Classifier in Question Answering
Huang, Zhiheng;Thint, Marcus;Celikyilmaz, Asli;"></td>
	<td class="line x" title="1:206	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 543550, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:206	c 2009 ACL and AFNLP Investigation of Question Classifier in Question Answering Zhiheng Huang EECS Department University of California at Berkeley CA 94720-1776, USA zhiheng@cs.berkeley.edu Marcus Thint Intelligent Systems Research Center British Telecom Group Chief Technology Office marcus.2.thint@bt.com Asli Celikyilmaz EECS Department University of California at Berkeley CA 94720-1776, USA asli@cs.berkeley.edu Abstract In this paper, we investigate how an accurate question classifier contributes to a question answering system." ></td>
	<td class="line x" title="3:206	We first present a Maximum Entropy (ME) based question classifier which makes use of head word features and their WordNet hypernyms." ></td>
	<td class="line x" title="4:206	We show that our question classifier can achieve the state of the art performance in the standard UIUC question dataset." ></td>
	<td class="line x" title="5:206	We then investigate quantitatively the contribution of this question classifier toafeaturedrivenquestionansweringsystem." ></td>
	<td class="line x" title="6:206	With our accurate question classifier and some standard question answer features, our question answering system performs close to the state of the art using TREC corpus." ></td>
	<td class="line x" title="7:206	1 Introduction Question answering has drawn significant attention from the last decade (Prager, 2006)." ></td>
	<td class="line x" title="8:206	It attempts to answer the question posed in natural language by providing the answer phrase rather than the whole documents." ></td>
	<td class="line x" title="9:206	An important step in question answering (QA) is to classify the question to the anticipated type of the answer." ></td>
	<td class="line x" title="10:206	For example, the question of Who discovered x-rays should be classified into the type of human (individual)." ></td>
	<td class="line x" title="11:206	This information would narrow down the search space to identify the correct answer string." ></td>
	<td class="line x" title="12:206	In addition, this information can suggest different strategies to search and verify a candidate answer." ></td>
	<td class="line x" title="13:206	In fact, the combination of question classification andthenamedentityrecognitionisakeyapproach in modern question answering systems (Voorhees and Dang, 2005)." ></td>
	<td class="line x" title="14:206	The question classification is by no means trivial: Simply using question wh-words can not achieve satisfactory results." ></td>
	<td class="line x" title="15:206	The difficulty lies in classifying the what and which type questions." ></td>
	<td class="line x" title="16:206	Consideringtheexample What is the capital of Yugoslavia, it is of location (city) type, while What is the pH scale is of definition type." ></td>
	<td class="line x" title="17:206	As with the previous work of (Li and Roth, 2002; Li and Roth, 2006; Krishnan et al., 2005; Moschitti et al., 2007), we propose a feature driven statistical question classifier (Huang et al., 2008)." ></td>
	<td class="line x" title="18:206	In particular, we propose head word feature and augment semantic features of such head words using WordNet." ></td>
	<td class="line x" title="19:206	In addition, Lesks word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized." ></td>
	<td class="line x" title="20:206	With further augment of other standard features such as unigrams, we can obtain accuracy of 89.0% using ME model for 50 fine classes over UIUC dataset." ></td>
	<td class="line x" title="21:206	In addition to building an accurate question classifier, we investigate the contribution of this question classifier to a feature driven question answering rank model." ></td>
	<td class="line x" title="22:206	It is worth noting that, most of the features we used in question answering rank model, depend on the question type information." ></td>
	<td class="line x" title="23:206	For instance, if a question is classified as a type of sport, we then only care about whether there are sport entities existing in the candidate sentences." ></td>
	<td class="line o" title="24:206	It is expected that a fine grained named entity recognizer (NER) should make good use of the accurate question type information." ></td>
	<td class="line nc" title="25:206	However, due to the lack of a fine grained NER tool at hand, we employ the Stanford NER package (Finkel et al., 2005) which identifies only four types of named entities." ></td>
	<td class="line o" title="26:206	Even with such a coarse named entity recognizer, the experiments show that the question classifier plays an important role in determining the performance of a question answering system." ></td>
	<td class="line x" title="27:206	The rest of the paper is organized as following." ></td>
	<td class="line x" title="28:206	Section 2 reviews the maximum entropy model which are used in both question classification and question answering ranking." ></td>
	<td class="line x" title="29:206	Section 3 presents the features used in question classification." ></td>
	<td class="line x" title="30:206	Section 4 presents the question classification 543 accuracy over UIUC question dataset." ></td>
	<td class="line x" title="31:206	Section 5 presents the question answer features." ></td>
	<td class="line x" title="32:206	Section 6 illustrates the results based on TREC question answer dataset." ></td>
	<td class="line x" title="33:206	And Section 7 draws the conclusion." ></td>
	<td class="line x" title="34:206	2 Maximum Entropy Models Maximum entropy (ME) models (Berger et al., 1996; Manning and Klein, 2003), also known as log-linear and exponential learning models, provideageneralpurposemachinelearningtechnique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging, named entity recognition etc. Maximum entropy models can integrate features from many heterogeneous information sources for classification." ></td>
	<td class="line x" title="35:206	Each feature corresponds to a constraint on the model." ></td>
	<td class="line x" title="36:206	Given a training set of (C,D), where C is a set of class labels and D is a set of feature represented data points, the maximal entropy model attempts to maximize the log likelihood logP(C|D,) = summationdisplay (c,d)(C,D) log exp summationtext i ifi(c,d)summationtext c exp summationtext j jfi(c,d) , (1) where fi(c,d) are feature indicator functions." ></td>
	<td class="line x" title="37:206	We use ME models for both question classification and question answer ranking." ></td>
	<td class="line x" title="38:206	In question answer context, such function, for instance, could be the presence or absence of dictionary entities (as presented in Section 5.2) associated with a particular class type (either true or false, indicating a sentence can or cannot answer the question)." ></td>
	<td class="line x" title="39:206	i are the parameters need to be estimated which reflects the importance of fi(c,d) in prediction." ></td>
	<td class="line x" title="40:206	3 Question Classification Features Li and Roth (2002) have developed a machine learning approach which uses the SNoW learning architecture." ></td>
	<td class="line x" title="41:206	They have compiled the UIUC question classification dataset1 which consists of 5500 training and 500 test questions.2 All questions in the dataset have been manually labeled according to the coarse and fine grained categories as shown in Table 1, with coarse classes (in bold) followed by their fine classes." ></td>
	<td class="line x" title="42:206	The UIUC dataset has laid a platform for the follow-up research including (Hacioglu and Ward, 2003; Zhang and Lee, 2003; Li and Roth, 2006; 1Availableathttp://12r.cs.uiuc.edu/cogcomp/Data/QA/QC." ></td>
	<td class="line x" title="43:206	2Test questions are from TREC 10." ></td>
	<td class="line x" title="44:206	Table 1: 6 coarse and 50 fine Question types defined in UIUC question dataset." ></td>
	<td class="line x" title="45:206	ABBR letter desc NUM abb other manner code exp plant reason count ENTITY product HUMAN date animal religion group distance body sport individual money color substance title order creative symbol desc other currency technique LOC period dis.med." ></td>
	<td class="line x" title="46:206	term city percent event vehicle country speed food word mountain temp instrument DESC other size lang definition state weight Krishnan et al., 2005; Moschitti et al., 2007)." ></td>
	<td class="line x" title="47:206	In contrast to Li and Roth (2006)s approach which makes use of a very rich feature set, we propose to use a compact yet effective feature set." ></td>
	<td class="line x" title="48:206	The features are briefly described as following." ></td>
	<td class="line x" title="49:206	More detailed information can be found at (Huang et al., 2008)." ></td>
	<td class="line x" title="50:206	Question wh-word The wh-word feature is the question wh-word in given questions." ></td>
	<td class="line x" title="51:206	For example, the wh-word of question What is the population of China is what." ></td>
	<td class="line x" title="52:206	Head Word head word is defined as one single word specifying the object that the question seeks." ></td>
	<td class="line x" title="53:206	For example the head word of What is a group of turkeys called, is turkeys." ></td>
	<td class="line x" title="54:206	This is different to previous work including (Li and Roth, 2002; Krishnan et al., 2005) which has suggested a contiguous span of words (a group of turkeys in this example)." ></td>
	<td class="line x" title="55:206	The single word definition effectively avoids the noisy information brought by non-head word of the span (group in this case)." ></td>
	<td class="line x" title="56:206	A syntactic parser (Petrov and Klein, 2007) and the Collins rules (Collins, 1999) are modified to extract such head words." ></td>
	<td class="line x" title="57:206	WordNet Hypernym WordNet hypernyms are extracted for the head word of a given question." ></td>
	<td class="line x" title="58:206	TheclassicLeskalgorithm(Lesk,1986) is used to compute the most probable sense for a head word in the question context, and then the hypernyms are extracted based on that sense." ></td>
	<td class="line x" title="59:206	The depth of hypernyms is set to 544 six with trial and error.3 Hypernyms features capture the general terms of extracted head word." ></td>
	<td class="line x" title="60:206	For instance, the head word of question What is the proper name for a female walrus is extracted as walrus and its direct hypernyms such as mammal and animal are extracted as informative features to predict the correct question type of ENTY:animal." ></td>
	<td class="line x" title="61:206	Unigram words Bag of words features." ></td>
	<td class="line x" title="62:206	Such features provide useful question context information." ></td>
	<td class="line x" title="63:206	Word shape Fivewordshapefeatures,namelyall upper case, all lower case, mixed case, all digits, and other are used to serve as a coarse named entity recognizer." ></td>
	<td class="line x" title="64:206	4 Question Classification Experiments We train a Maximum Entropy model using the UIUC 5500 training questions and test over the 500 test questions." ></td>
	<td class="line x" title="65:206	Tables 2 shows the accuracy of 6 coarse class and 50 fine grained class, with features being fed incrementally." ></td>
	<td class="line x" title="66:206	The question classificationperformanceismeasuredbyaccuracy, i.e., the proportion of the correctly classified questions among all test questions." ></td>
	<td class="line x" title="67:206	The baseline using the Table 2: Question classification accuracy using incremental feature sets for 6 and 50 classes over UIUC split." ></td>
	<td class="line x" title="68:206	6 class 50 class wh-word 46.0 46.8 + head word 92.2 82.0 + hypernym 91.8 85.6 + unigram 93.0 88.4 + word shape 93.6 89.0 wh-head word results in 46.0% and 46.8% respectively for 6 coarse and 50 fine class classification." ></td>
	<td class="line x" title="69:206	Theincrementaluseof headwordbooststhe accuracysignificantlyto92.2%and82.0%for6and50 classes." ></td>
	<td class="line x" title="70:206	This reflects the informativeness of such feature." ></td>
	<td class="line x" title="71:206	The inclusion of hypernym feature within 6 depths boosts 3.6% for 50 classes, while resulting in slight loss for 6 coarse classes." ></td>
	<td class="line x" title="72:206	The further use of unigram feature leads to 2.8% gain in 50 classes." ></td>
	<td class="line x" title="73:206	Finally, the use of word shape leads to 0.6% accuracy increase for 50 classes." ></td>
	<td class="line x" title="74:206	The best 3We performed 10 cross validation experiment over training data and tried various depths of 1, 3, 6, 9 and , with  signifies that no depth constraint is imposed." ></td>
	<td class="line x" title="75:206	accuracies achieved are 93.6% and 89.0% for 6 and 50 classes respectively." ></td>
	<td class="line x" title="76:206	The individual feature contributions were discussed in greater detail in (Huang et al., 2008)." ></td>
	<td class="line x" title="77:206	Also, The SVM (rathern than ME model) was employed using the same feature set and the results were very close (93.4% for 6 class and 89.2% for 50 class)." ></td>
	<td class="line x" title="78:206	Table 3 shows the feature ablation experiment4 which is missing in that paper." ></td>
	<td class="line x" title="79:206	The experiment shows that the proposed head word and its hypernym features play an essential role in building an accurate question classifier." ></td>
	<td class="line x" title="80:206	Table 3: Question classification accuracy by removing one feature at a time for 6 and 50 classes over UIUC split." ></td>
	<td class="line x" title="81:206	6 class 50 class overall 93.6 89.0 wh-word 93.6 89.0 head word 92.8 88.2 hypernym 90.8 84.2 unigram 93.6 86.8 word shape 93.0 88.4 Our best result feature space only consists of 13697 binary features and each question has 10 to30 activefeatures." ></td>
	<td class="line x" title="82:206	Compared tothe over feature size of 200000 in Li and Roth (2002), our feature space is much more compact, yet turned out to be moreinformativeassuggestedbytheexperiments." ></td>
	<td class="line x" title="83:206	Table 4 shows the summary of the classification accuracy of all question classifiers which were applied to UIUC dataset.5 Our results are summarized in the last row." ></td>
	<td class="line x" title="84:206	In addition, we have performed the 10 cross validation experiment over the 5500 UIUC training corpus using our best model." ></td>
	<td class="line x" title="85:206	The result is 89.051.25and83.731.61for6and50classes,6 which outperforms the best result of 86.11.1 for 6 classes as reported in (Moschitti et al., 2007)." ></td>
	<td class="line x" title="86:206	5 Question Answer Features For a pair of a question and a candidate sentence, we extract binary features which include CoNLL named entities presence feature (NE), dictionary 4Remove one feature at a time from the entire feature set." ></td>
	<td class="line x" title="87:206	5Note (1) that SNoW accuracy without the related word dictionary was not reported." ></td>
	<td class="line x" title="88:206	With the semantically related word dictionary, it achieved 91%." ></td>
	<td class="line x" title="89:206	Note (2) that SNoW with a semantically related word dictionary achieved 84.2% but the other algorithms did not use it." ></td>
	<td class="line x" title="90:206	6These results are worse than the result over UIUC split; as the UIUC test data includes a larger percentage of easily classified question types." ></td>
	<td class="line x" title="91:206	545 Table 4: Accuracy of all question classifiers which were applied to UIUC dataset." ></td>
	<td class="line x" title="92:206	Algorithm 6 class 50 class Li and Roth, SNoW (1) 78.8(2) Hacioglu et al., SVM+ECOC  80.2-82 Zhang & Lee, Linear SVM 87.4 79.2 Zhang & Lee, Tree SVM 90.0  Krishnan et al., SVM+CRF 93.4 86.2 Moschitti et al., Kernel 91.8  Maximum Entropy Model 93.6 89.0 entities presence feature (DIC), numerical entities presence feature (NUM), question specific feature (SPE), and dependency validity feature (DEP)." ></td>
	<td class="line oc" title="93:206	5.1 CoNLL named entities presence feature We use Stanford named entity recognizer (NER) (Finkel et al., 2005) to identify CoNLL style NEs7 as possible answer strings in a candidate sentence for a given type of question." ></td>
	<td class="line x" title="94:206	In particular, if the question is ABBR type, we tag CoNLL LOC, ORG and MISC entities as candidate answers; If thequestionisHUMANtype, wetagCoNLLPER and ORG entities; And if the question is LOC type, we tag CoNLL LOC and MISC entities." ></td>
	<td class="line x" title="95:206	For other types of questions, we assume there is no candidate CoNLL NEs to tag." ></td>
	<td class="line x" title="96:206	We create a binary feature NE to indicate the presence or absence of tagged CoNLL entities." ></td>
	<td class="line x" title="97:206	Further more, we create four binary features NE-PER, NE-LOC, NEORG, and NE-MISC to indicate the presence of tagged CoNLL PER, LOC, ORG and MISC entities." ></td>
	<td class="line x" title="98:206	5.2 Dictionary entities presence feature As four types of CoNLL named entities are not enough to cover 50 question types, we include the 101dictionaryfilescompiledintheEphyraproject (Schlaeferetal.,2007)." ></td>
	<td class="line x" title="99:206	Thesedictionaryfilescontain names for specific semantic types." ></td>
	<td class="line x" title="100:206	For example, the actor dictionary comprises a list of actor names such as Tom Hanks and Kevin Spacey." ></td>
	<td class="line x" title="101:206	For each question, if the head word of such question (see Section 3) matches the name of a dictionary file, then each noun phrase in a candidate sentence is looked up to check its presence in the dictionary." ></td>
	<td class="line x" title="102:206	If so, a binary DIC feature is created." ></td>
	<td class="line x" title="103:206	For example, for the question What rank did Chester 7Person (PER), location (LOC), organization (ORG), and miscellaneous (MISC)." ></td>
	<td class="line x" title="104:206	Nimitz reach, as there is a military rank dictionary matches the head word rank, then all the noun phrases in a candidate sentence are looked up in the military rank dictionary." ></td>
	<td class="line x" title="105:206	As a result, a sentence contains word Admiral will result in the DIC feature being activated, as such word is present in the military rank dictionary." ></td>
	<td class="line x" title="106:206	Note that an implementation tip is to allow the proximity match in the dictionary look up." ></td>
	<td class="line x" title="107:206	Consider the question What film introduced Jar Jar Binks." ></td>
	<td class="line x" title="108:206	As there is a match between the question head word film and the dictionary named film, each noun phrase in the candidate sentence is checked." ></td>
	<td class="line x" title="109:206	However, no dictionary entities have been found from the candidate sentence Best plays Jar Jar Binks, a floppy-eared, two-legged creature in Star Wars: Episode I  The Phantom Menace, although there is movie entitled Star Wars Episode I: The Phantom Menace in the dictionary." ></td>
	<td class="line x" title="110:206	Notice that Star Wars: Episode I  The Phantom Menace in the sentence and the dictionary entity Star Wars Episode I: The Phantom Menace do not have exactly identical spelling." ></td>
	<td class="line x" title="111:206	The use of proximitylookupwhichallowseditdistancebeingless than 10% error can resolve this." ></td>
	<td class="line x" title="112:206	5.3 Numerical entities presence feature There are so far no match for question types of NUM (as shown in Table 1) including NUM:count and NUM:date etc. These types of questions seek the numerical answers such as the amount of money and the duration of period." ></td>
	<td class="line x" title="113:206	It is natural to compile regular expression patterns to match such entities." ></td>
	<td class="line x" title="114:206	For example, for a NUM:money typed question What is Rohm and Haass annual revenue, we compile NUM:money regular expression pattern which matches the strings of number followed by a currency sign ($ and dollars etc)." ></td>
	<td class="line x" title="115:206	Such pattern is able to identify 4 billion $ as a candidate answer in the candidate sentence Rohm and Haas, with 4 billion $ in annual sales There are 13 patterns compiled to cover all numerical types." ></td>
	<td class="line x" title="116:206	We create a binary feature NUM to indicate the presence of possible numerical answers in a sentence." ></td>
	<td class="line x" title="117:206	5.4 Specific features Specific features are question dependent." ></td>
	<td class="line x" title="118:206	For example, for question When was James Dean born, any candidate sentence matches the pattern James Dean (number number) is likely to answer such question." ></td>
	<td class="line x" title="119:206	We create a binary feature SPE to indicate the presence of such match between a ques546 tion and a candidate sentence." ></td>
	<td class="line x" title="120:206	We list all question and sentence match patterns which are used in our experiments as following: when born feature 1 The question begins with when is/was and follows by a person name and then follows by key word born; The candidate sentence contains such person name which follows by the pattern of (number number)." ></td>
	<td class="line x" title="121:206	when born feature 2 The question begins with when is/was and follows by a person name and then follows by key word born; The candidate sentence contains such person name, a NUM:date entity, and a key word born." ></td>
	<td class="line x" title="122:206	where born feature 1 The question begins with where is/was and follows by a person name and then follows by key word born; The candidate sentence contains such person name, a NER LOC entity, and a key word born." ></td>
	<td class="line x" title="123:206	when die feature 1 The question begins with when did and followsbyapersonnameandthenfollowsbykeyword die; The candidate sentence contains such person name which follows by the pattern of (number number)." ></td>
	<td class="line x" title="124:206	when die feature 2 The question begins with when did and follows by a person name and then follows by key word die; The candidate sentence contains such person name, a NUM:date entity, and a key word died." ></td>
	<td class="line x" title="125:206	how many feature The question begins with how many and follows by a noun; The candidate sentence contains a number and then follows by such noun." ></td>
	<td class="line x" title="126:206	cooccurrent Feature This feature takes two phrase arguments, if the question contains the first phrase and the candidate sentence contains the second, such feature would be activated." ></td>
	<td class="line x" title="127:206	Note that the construction of specific features require the access to aforementioned extracted named entities." ></td>
	<td class="line x" title="128:206	For example, the when born feature 2 pattern needs the information whether a candidate sentence contains a NUM:date entity and where born feature 1 pattern needs the information whether a candidate sentence contains a NER LOC entity." ></td>
	<td class="line x" title="129:206	Note also that the patterns of when born feature and when die feature have similar structure and thus can be simplified in implementation." ></td>
	<td class="line x" title="130:206	How many feature can be used to identify the sentence Amtrak annually serves about 21 million passengers for question How manypassengersdoesAmtrakserveannually." ></td>
	<td class="line x" title="131:206	The cooccurrent feature is the most general one." ></td>
	<td class="line x" title="132:206	An example of cooccurrent feature would take the arguments of marry and husband, or marry and wife." ></td>
	<td class="line x" title="133:206	Such feature would be activated for question Whom did Eileen Marie Collins marry and candidate sentence  were Collins husband, Pat Youngs, an airline pilot It is worth noting that the two arguments are not necessarily different." ></td>
	<td class="line x" title="134:206	For example, they could be both established, which makes such feature activated for question When was the IFC established and candidate sentence IFC was established in 1956 as a member of the World Bank Group." ></td>
	<td class="line x" title="135:206	Thereasonwhyweusethe cooccurrence of the word established is due to its mainverbrole, whichmaycarrymoreinformation than other words." ></td>
	<td class="line x" title="136:206	5.5 Dependency validity features Like (Cui et al., 2004), we extract the dependency path from the question word to the common word (existing in both question and sentence), and the path from candidate answer (such as CoNLL NE and numerical entity) to the common word for each pair of question and candidate sentence using Stanford dependency parser (Klein and Manning, 2003; Marneffe et al., 2006)." ></td>
	<td class="line x" title="137:206	For example, for question When did James Dean die and candidate sentence In 1955, actor James Dean was killed in a two-car collision near Cholame, Calif., we extract the pathes of When:advmod:nsubj:Dean and 1955:prep-in:nsubjpass:Dean for question and sentence respectively, where advmod and nsubj etc. are grammatical relations." ></td>
	<td class="line x" title="138:206	We propose the dependency validity feature (DEP) as following." ></td>
	<td class="line x" title="139:206	For all paired paths between a question and a candidatesentence,ifatleastonepairofpathinwhich all pairs of grammatical relations have been seen in the training, then the DEP feature is set to be true, false otherwise." ></td>
	<td class="line x" title="140:206	That is, the true validity feature indicates that at least one pair of path between the question and candidate sentence is possible to be a true pair (ie, the candidate noun phrase in the sentence path is the true answer)." ></td>
	<td class="line x" title="141:206	6 Question Answer Experiments Recall that most of the question answer features depend on the question classifier." ></td>
	<td class="line x" title="142:206	For instance, the NE feature checks the presence or absence of CoNLL style named entities subject to the classified question type." ></td>
	<td class="line x" title="143:206	In this section, we evaluate how the quality of question classifiers affects the question answering performance." ></td>
	<td class="line x" title="144:206	6.1 Experiment setup We use TREC99-03 factoid questions for training and TREC04 factoid questions for testing." ></td>
	<td class="line x" title="145:206	To facilitate the comparison to others work (Cui et al., 2004; Shen and Klakow, 2006), we first retrieve allrelevantdocumentswhicharecompiledbyKen Litkowski8 to create training and test datasets." ></td>
	<td class="line x" title="146:206	We 8Available at http://trec.nist.gov/data/qa.html." ></td>
	<td class="line x" title="147:206	547 then apply key word search for each question and retrieve the top 20 relevant sentences." ></td>
	<td class="line x" title="148:206	We create a feature represented data point using each pair of question and candidate sentence and label it either true or false depending on whether the sentence can answer the given question or not." ></td>
	<td class="line x" title="149:206	The labeling is conducted by matching the gold factoid answer pattern against the candidate sentence." ></td>
	<td class="line x" title="150:206	There are two extra steps performed for training set but not for test data." ></td>
	<td class="line x" title="151:206	In order to construct a high quality training set, we manually check the correctness of the training data points and remove the false positive ones which cannot support the question although there is a match to gold answer." ></td>
	<td class="line x" title="152:206	In addition, in order to keep the training data well balanced, wekeepmaximumfourfalsedatapoints (question answer pair) for each question but no limit over the true label data points." ></td>
	<td class="line x" title="153:206	In doing so, we use 1458 questions to compile 8712 training data points and among them 1752 have true labels." ></td>
	<td class="line x" title="154:206	Similarly, we use 202 questions to compile 4008 test data points and among them 617 have true labels." ></td>
	<td class="line x" title="155:206	We use the training data to train a maximum entropy model and use such model to rank test data set." ></td>
	<td class="line x" title="156:206	Compared with a classification task (such as the question classifier), the ranking process requires one extra step: For data points which share the same question, the probabilities of being predictedastruelabelareusedtorankthedatapoints." ></td>
	<td class="line x" title="157:206	In align with the previous work, performance is evaluated using mean reciprocal rank (MRR), top 1 prediction accuracy (top1) and top 5 prediction accuracy (top5)." ></td>
	<td class="line x" title="158:206	For the test data set, 157 among the 202 questions have correct answers found in retrieved sentences." ></td>
	<td class="line x" title="159:206	This leads to the upper bound of MRR score being 77.8%." ></td>
	<td class="line x" title="160:206	To evaluate how the quality of question classifiers affects the question answering, we have created three question classifiers: QC1, QC2 and QC3." ></td>
	<td class="line x" title="161:206	The features which are used to train these question classifiers and their performance are shown in Table 5." ></td>
	<td class="line x" title="162:206	Note that QC3 is the best question classifier we obtained in Section 4." ></td>
	<td class="line x" title="163:206	Table 5: Features used to train and the performance of three question classifiers." ></td>
	<td class="line x" title="164:206	Name features 6 class 50 class QC1 wh-word 46.0 46.8 QC2 wh-word+ head 92.2 82.0 QC3 All 93.6 89.0 6.2 Experiment results The first experiment is to evaluate the individual contribution of various features derived using three question classifiers." ></td>
	<td class="line x" title="165:206	Table 6 shows the baseline result and results using DIC, NE, NE-4, REG, SPE, and DEP features." ></td>
	<td class="line x" title="166:206	The baseline is the key word search without the use of maximum entropy model." ></td>
	<td class="line x" title="167:206	As can be seen, the question classifiers do not affect the DIC feature at all, as DIC feature does not depend on question classifiers." ></td>
	<td class="line x" title="168:206	Better question classifier boosts considerable gain for NE, NE-4 and REG in their contribution to question answering." ></td>
	<td class="line x" title="169:206	For example, the best question classifier QC3 outperforms the worst one (QC1) by 1.5%, 2.0%, and 2.0% MRR scores for NE, NE-4 and REG respectively." ></td>
	<td class="line x" title="170:206	However, it is surprising that the MRR and top5 contribution of NE andNE-4decreasesifQC1isreplacedbyQC2,although the top1 score results in performance gain slightly." ></td>
	<td class="line x" title="171:206	This unexpected results can be partially explained as follows." ></td>
	<td class="line x" title="172:206	For some questions, even QC2 produces correct predictions, the errors of NE and NE-4 features may cause over-confident scoresforcertaincandidatesentences." ></td>
	<td class="line x" title="173:206	AsSPEand DEP are not directly dependent on question classifier, their individual contribution only changes slightly or remains the same for different question classifiers." ></td>
	<td class="line x" title="174:206	If the best question classifier is used, the most important features are SPE and REG,whichcanindividuallyboosttheMRRscore over54%,whiletheothersresultinlesssignificant gains." ></td>
	<td class="line x" title="175:206	We now incrementally use various features and the results are show in Table 6 as well." ></td>
	<td class="line x" title="176:206	As can be seen, the more features and the better question classifier are used, the higher performance the ME model has." ></td>
	<td class="line x" title="177:206	The inclusion of REG and SPE results in significant boost for the performance." ></td>
	<td class="line x" title="178:206	For example, if the best question classifier QC3 is used, the REG results in 6.9% and 8% gain for MRR and top1 scores respectively." ></td>
	<td class="line x" title="179:206	This is due to a large portionofNUMtypequestionsintestdataset." ></td>
	<td class="line x" title="180:206	The SPE feature contributes significantly to the performance due to its high precision in answering birth/death time/location questions." ></td>
	<td class="line x" title="181:206	NE and NE-4 result in reasonable gains while DEP feature contributes little." ></td>
	<td class="line x" title="182:206	However, this does not mean that DEP is not important, as once the model reaches a high MRR score, it becomes hard to improve." ></td>
	<td class="line x" title="183:206	Table 6 clearly shows that the question type classifier plays an essential role in a high perfor548 Table 6: Performance of individual and incremental feature sets for three question classifiers." ></td>
	<td class="line x" title="184:206	Individual Feature MRR Top1 Top5 QC1 QC2 QC3 QC1 QC2 QC3 QC1 QC2 QC3 Baseline 49.9 49.9 49.9 40.1 40.1 40.1 59.4 59.4 59.4 DIC 49.5 49.5 49.5 42.6 42.6 42.6 60.4 60.4 60.4 NE 48.5 47.5 50.0 40.6 40.6 42.6 61.9 60.9 63.4 NE-4 49.5 48.5 51.5 41.6 42.1 44.6 62.4 61.9 64.4 REG 52.0 54.0 54.0 44.1 47.0 47.5 64.4 65.3 65.3 SPE 55.0 55.0 55.0 48.5 48.5 48.5 64.4 64.4 64.4 DEP 51.0 51.5 52.0 43.6 44.1 44.6 65.3 65.8 65.8 Incremental Baseline 49.9 49.9 49.9 40.1 40.1 40.1 59.4 59.4 59.4 +DIC 49.5 49.5 49.5 42.6 42.6 42.6 60.4 60.4 60.4 +NE 50.0 48.5 51.0 43.1 42.1 44.6 62.9 61.4 64.4 +NE-4 51.5 50.0 53.0 44.1 43.6 46.0 63.4 62.9 65.8 +REG 55.0 56.9 59.9 48.0 51.0 54.0 68.3 68.8 71.8 +SPE 60.4 62.4 65.3 55.4 58.4 61.4 70.8 70.8 73.8 +DEP 61.4 62.9 66.3 55.9 58.4 62.4 71.8 71.8 73.8 mance question answer system." ></td>
	<td class="line x" title="185:206	Assume all the features are used, the better question classifier significantly boosts the overall performance." ></td>
	<td class="line x" title="186:206	For example, the best question classifier QC3 outperforms the worst QC1 by 4.9%, 6.5%, and 2.0% for MRR, top1 and top5 scores respectively." ></td>
	<td class="line x" title="187:206	Even compared to a good question classifier QC2, the gain of using QC3 is still 3.4%, 4.0% and 2.0% for MRR, top1 and top5 scores respectively." ></td>
	<td class="line n" title="188:206	One can imagine that if a fine grained NER is available (rather than the current four type coarse NER), the potential gain is much significant." ></td>
	<td class="line x" title="189:206	The reason that the question classifier affects the question answering performance is straightforward." ></td>
	<td class="line x" title="190:206	As a upstream source, the incorrect classification of question type would confuse the downstream answer search process." ></td>
	<td class="line x" title="191:206	For example, for question What is Rohm and Haass annual revenue, our best question classifier is able to classify it into the correct type of NUM:money and thus would put $ 4 billion as a candidate answer." ></td>
	<td class="line x" title="192:206	However, the inferior question classifiers misclassify it into HUM:ind type and thereby could not return a correct answer." ></td>
	<td class="line x" title="193:206	Figure 1 shows the individual MRR scores for the 42 questions (among the 202 test questions) which have different predicted question types using QC3 and QC2." ></td>
	<td class="line x" title="194:206	For almost all test questions, the accurate question classifier QC3 achieves higher MRR scores compared to QC2." ></td>
	<td class="line x" title="195:206	Table 7 shows performance of various question answer systems including (Tanev et al., 2004; Wu et al., 2005; Cui et al., 2004; Shen and Klakow, 0 5 10 15 20 25 30 35 40 450 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 question id MRR   QC3 QC2 Figure 1: Individual MRR scores for questions which have different predicted question types using QC3 and QC2." ></td>
	<td class="line x" title="196:206	2006) and this work which were applied to the sametrainingandtestdatasets." ></td>
	<td class="line x" title="197:206	Amongallthesystems, our model can achieve the best MRR score of 66.3%, which is close to the state of the art of 67.0%." ></td>
	<td class="line x" title="198:206	Considering the question answer features used in this paper are quite standard, the boost is mainly due to our accurate question classifier." ></td>
	<td class="line x" title="199:206	Table 7: Various system performance comparison." ></td>
	<td class="line x" title="200:206	System MRR Top1 Top5 Tanev et al. 2004 57.0 49.0 67.0 Cui et al. 2004 60.0 53.0 70.0 Shen and Klakow, 2006 67.0 62.0 74.0 This work 66.3 62.4 73.8 549 7 Conclusion In this paper, we have presented a question classifier which makes use of a compact yet efficient feature set." ></td>
	<td class="line x" title="201:206	The question classifier outperforms previous question classifiers over the standard UIUC question dataset." ></td>
	<td class="line x" title="202:206	We further investigated quantitatively how the quality of question classifier impacts the performance of question answer system." ></td>
	<td class="line x" title="203:206	The experiments showed that an accurate question classifier plays an essential role in question answering system." ></td>
	<td class="line x" title="204:206	With our accurate question classifier and some standard question answer features, our question answering system performs close to the state of the art." ></td>
	<td class="line x" title="205:206	Acknowledgments We wish to thank the three anonymous reviewers for their invaluable comments." ></td>
	<td class="line x" title="206:206	This research was supported by British Telecom grant CT1080028046 and BISC Program of UC Berkeley." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1101
Supervised Models for Coreference Resolution
Rahman, Altaf;Ng, Vincent;"></td>
	<td class="line x" title="1:257	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 968977, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:257	c 2009 ACL and AFNLP Supervised Models for Coreference Resolution Altaf Rahman and Vincent Ng Human Language Technology Research Institute University of Texas at Dallas Richardson, TX 75083-0688 {altaf,vince}@hlt.utdallas.edu Abstract Traditional learning-based coreference resolvers operate by training a mentionpair classifier for determining whether two mentions are coreferent or not." ></td>
	<td class="line x" title="3:257	Two independent lines of recent research have attempted to improve these mention-pair classifiers, one by learning a mentionranking model to rank preceding mentions for a given anaphor, and the other by training an entity-mention classifier to determine whether a preceding cluster is coreferent with a given mention." ></td>
	<td class="line x" title="4:257	We propose a cluster-ranking approach to coreference resolution that combines the strengths of mention rankers and entitymention models." ></td>
	<td class="line x" title="5:257	We additionally show how our cluster-ranking framework naturally allows discourse-new entity detection to be learned jointly with coreference resolution." ></td>
	<td class="line x" title="6:257	Experimental results on the ACE data sets demonstrate its superior performance to competing approaches." ></td>
	<td class="line x" title="7:257	1 Introduction Noun phrase (NP) coreference resolution is the task of identifying which NPs (or mentions) refer to the same real-world entity or concept." ></td>
	<td class="line x" title="8:257	Traditional learning-based coreference resolvers operate by training a model for classifying whether two mentions are co-referring or not (e.g., Soon et al.(2001), Ng and Cardie (2002b), Kehler et al.(2004), Ponzetto and Strube (2006))." ></td>
	<td class="line x" title="11:257	Despite their initial successes, these mention-pair models have at least two major weaknesses." ></td>
	<td class="line x" title="12:257	First, since each candidate antecedent for a mention to be resolved (henceforth an active mention) is considered independently of the others, these models only determine how good a candidate antecedent is relative to the active mention, but not how good a candidate antecedent is relative to other candidates." ></td>
	<td class="line x" title="13:257	In other words, they fail to answer the critical question of which candidate antecedent is most probable." ></td>
	<td class="line x" title="14:257	Second, they have limitations in their expressiveness: the information extracted from the two mentions alone may not be sufficient for making an informed coreference decision, especially if the candidate antecedent is a pronoun (which is semantically empty) or a mention that lacks descriptive information such as gender (e.g., Clinton)." ></td>
	<td class="line x" title="15:257	To address the first weakness, researchers have attempted to train a mention-ranking model for determining which candidate antecedent is most probable given an active mention (e.g., Denis and Baldridge (2008))." ></td>
	<td class="line x" title="16:257	Ranking is arguably a more natural reformulation of coreference resolution than classification, as a ranker allows all candidate antecedents to be considered simultaneously and therefore directly captures the competition among them." ></td>
	<td class="line x" title="17:257	Another desirable consequence is that there exists a natural resolution strategy for a ranking approach: a mention is resolved to the candidate antecedent that has the highest rank." ></td>
	<td class="line x" title="18:257	This contrasts with classification-based approaches, where many clustering algorithms have been employed to co-ordinate the pairwise coreference decisions (because it is unclear which one is the best)." ></td>
	<td class="line x" title="19:257	To address the second weakness, researchers have investigated the acquisition of entity-mention coreference models (e.g., Luo et al.(2004), Yang et al.(2004))." ></td>
	<td class="line x" title="22:257	Unlike mention-pair models, these entity-mention models are trained to determine whether an active mention belongs to a preceding, possibly partially-formed, coreference cluster." ></td>
	<td class="line x" title="23:257	Hence, they can employ cluster-level features (i.e., features that are defined over any subset of mentions in a preceding cluster), which makes them more expressive than mention-pair models." ></td>
	<td class="line x" title="24:257	Motivated in part by these recently developed models, we propose in this paper a clusterranking approach to coreference resolution that combines the strengths of mention-ranking mod968 els and entity-mention models." ></td>
	<td class="line x" title="25:257	Specifically, we recast coreference as the problem of determining which of a set of preceding coreference clusters is the best to link to an active mention using a learned cluster ranker." ></td>
	<td class="line x" title="26:257	In addition, we show how discourse-new detection (i.e., the task of determining whether a mention introduces a new entity in a discourse) can be learned jointly with coreference resolution in our cluster-ranking framework." ></td>
	<td class="line x" title="27:257	It is worth noting that researchers typically adopt a pipeline coreference architecture, performing discourse-new detection prior to coreference resolution and using the resulting information to prevent a coreference system from resolving mentions that are determined to be discourse-new (see Poesio et al.(2004) for an overview)." ></td>
	<td class="line x" title="29:257	As a result, errors in discourse-new detection could be propagated to the resolver, possibly leading to a deterioration of coreference performance (see Ng and Cardie (2002a))." ></td>
	<td class="line x" title="30:257	Jointly learning discoursenew detection and coreference resolution can potentially address this error-propagation problem." ></td>
	<td class="line x" title="31:257	In sum, we believe our work makes three main contributions to coreference resolution: Proposing a simple, yet effective coreference model." ></td>
	<td class="line x" title="32:257	Our work advances the state-of-the-art in coreference resolution by bringing learningbased coreference systems to the next level of performance." ></td>
	<td class="line x" title="33:257	When evaluated on the ACE 2005 coreference data sets, cluster rankers outperform three competing models  mention-pair, entitymention, and mention-ranking models  by a large margin." ></td>
	<td class="line x" title="34:257	Also, our joint-learning approach to discourse-new detection and coreference resolution consistently yields cluster rankers that outperform those adopting the pipeline architecture." ></td>
	<td class="line x" title="35:257	Equally importantly, cluster rankers are conceptually simple and easy to implement and do not rely on sophisticated training and inference procedures to make coreference decisions in dependent relation to each other, unlike relational coreference models (see McCallum and Wellner (2004))." ></td>
	<td class="line x" title="36:257	Bridging the gap between machine-learning approaches and linguistically-motivated approaches to coreference resolution." ></td>
	<td class="line x" title="37:257	While machine learning approaches to coreference resolution have received a lot of attention since the mid90s, popular learning-based coreference frameworks such as the mention-pair model are arguably rather unsatisfactory from a linguistic point of view." ></td>
	<td class="line x" title="38:257	In particular, they have not leveraged advances in discourse-based anaphora resolution research in the 70s and 80s." ></td>
	<td class="line x" title="39:257	Our work bridges this gap by realizing in a new machine learning framework ideas rooted in Lappin and Leasss (1994) heuristic-based pronoun resolver, which in turn was motivated by classic salience-based approaches to anaphora resolution." ></td>
	<td class="line x" title="40:257	Revealing the importance of adopting the right model." ></td>
	<td class="line x" title="41:257	While entity-mention models have previously been shown to be worse or at best marginally better than their mention-pair counterparts (Luo et al., 2004; Yang et al., 2008), our cluster-ranking models, which are a natural extension of entity-mention models, significantly outperformed all competing approaches." ></td>
	<td class="line x" title="42:257	This suggests that the use of an appropriate learning framework can bring us a long way towards highperformance coreference resolution." ></td>
	<td class="line x" title="43:257	The rest of the paper is structured as follows." ></td>
	<td class="line x" title="44:257	Section 2 discusses related work." ></td>
	<td class="line x" title="45:257	Section 3 describes our baseline coreference models: mentionpair, entity-mention, and mention-ranking." ></td>
	<td class="line x" title="46:257	We discuss our cluster-ranking approach in Section 4, evaluate it in Section 5, and conclude in Section 6." ></td>
	<td class="line x" title="47:257	2 Related Work Heuristic-based cluster ranking." ></td>
	<td class="line x" title="48:257	As mentioned previously, the work most related to ours is Lappin and Leass (1994), whose goal is to perform pronoun resolution by assigning an anaphoric pronoun to the highest-scored preceding cluster." ></td>
	<td class="line x" title="49:257	Nevertheless, Lappin and Leasss work differs from ours in several respects." ></td>
	<td class="line x" title="50:257	First, they only tackle pronoun resolution rather than the full coreference task." ></td>
	<td class="line x" title="51:257	Second, their algorithm is heuristic-based; in particular, the score assigned to a preceding cluster is computed by summing over the weights associated with the factors applicable to the cluster, where the weights are determined heuristically, rather than learned, unlike ours." ></td>
	<td class="line x" title="52:257	Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors." ></td>
	<td class="line x" title="53:257	As a result, their cluster-ranking model employs only factors that capture the salience of a cluster, and can therefore be viewed as a simple model of attentional state (see Grosz and Sidner (1986)) realized by coreference clusters." ></td>
	<td class="line x" title="54:257	By contrast, our resolution strategy is learned without applying hand-coded con969 straints in a separate filtering step." ></td>
	<td class="line x" title="55:257	In particular, we attempt to determine the compatibility between a cluster and an active mention, using factors that determine not only salience (e.g., the distance between the cluster and the mention) but also lexical and grammatical compatibility, for instance." ></td>
	<td class="line x" title="56:257	Entity-mention coreference models." ></td>
	<td class="line x" title="57:257	Luo et al.(2004) represent one of the earliest attempts to investigate learning-based entity-mention models." ></td>
	<td class="line x" title="59:257	They use the ANY predicate to generate clusterlevel features as follows: given a binary-valued feature X defined over a pair of mentions, they introduce an ANY-X cluster-level feature, which has the value TRUE if X is true between the active mention and any mention in the preceding cluster under consideration." ></td>
	<td class="line x" title="60:257	Contrary to common wisdom, this entity-mention model underperforms its mention-pair counterpart in spite of the generalization from mention-pair to cluster-level features." ></td>
	<td class="line x" title="61:257	In Yang et al.s (2004) entity-mention model, a training instance is composed of an active mention mk, a preceding cluster C, and a mention mj in C that is closest in distance to mk in the associated text." ></td>
	<td class="line x" title="62:257	The feature set used to represent the instance is primarily composed of features that describe the relationship between mj and mk, as well as a few cluster-level features." ></td>
	<td class="line x" title="63:257	In other words, the model still relies heavily on features used in a mention-pair model." ></td>
	<td class="line x" title="64:257	In particular, the inclusion of mj in the feature vector representation to some extent reflects the authors lack of confidence that a strong entity-mention model can be trained without mention-pair-based features." ></td>
	<td class="line x" title="65:257	Our ranking model, on the other hand, is trained without such features." ></td>
	<td class="line x" title="66:257	More recently, Yang et al.(2008) have proposed another entity-mention model trained by inductive logic programming." ></td>
	<td class="line x" title="68:257	Like their previous work, the scarcity of clusterlevel predicates (only two are used) under-exploits the expressiveness of entity-mention models." ></td>
	<td class="line x" title="69:257	Mention ranking." ></td>
	<td class="line x" title="70:257	The notion of ranking candidate antecedents can be traced back to centering algorithms, many of which use grammatical roles to rank forward-looking centers (see Grosz et al.(1995), Walker et al.(1998), and Mitkov (2002))." ></td>
	<td class="line x" title="73:257	However, mention ranking has been employed in learning-based coreference resolvers only recently." ></td>
	<td class="line x" title="74:257	As mentioned before, Denis and Baldridge (2008) train a mention-ranking model." ></td>
	<td class="line x" title="75:257	Their work can be viewed as an extension of Yang et al.s (2003) twin-candidate coreference model, which ranks only two candidate antecedents at a time." ></td>
	<td class="line x" title="76:257	Unlike ours, however, their model ranks mentions rather than clusters, and relies on an independently-trained discourse-new detector." ></td>
	<td class="line x" title="77:257	Discourse-new detection." ></td>
	<td class="line x" title="78:257	Discourse-new detection is often tackled independently of coreference resolution." ></td>
	<td class="line x" title="79:257	Pleonastic its have been detected using heuristics (e.g., Kennedy and Boguraev (1996)) and learning-based techniques such as rule learning (e.g., Muller (2006)), kernels (e.g., Versley et al.(2008)), and distributional methods (e.g., Bergsma et al.(2008))." ></td>
	<td class="line x" title="82:257	Non-anaphoric definite descriptions have been detected using heuristics (e.g., Vieira and Poesio (2000)) and unsupervised methods (e.g., Bean and Riloff (1999))." ></td>
	<td class="line x" title="83:257	General discourse-new detectors that are applicable to different types of NPs have been built using heuristics (e.g., Byron and Gegg-Harrison (2004)) and modeled generatively (e.g., Elsner and Charniak (2007)) and discriminatively (e.g., Uryupina (2003))." ></td>
	<td class="line x" title="84:257	There have also been attempts to perform joint inference for discourse-new detection and coreference resolution using integer linear programming (ILP), where a discourse-new classifier and a coreference classifier are trained independently of each other, and then ILP is applied as a post-processing step to jointly infer discourse-new and coreference decisions so that they are consistent with each other (e.g., Denis and Baldridge (2007))." ></td>
	<td class="line x" title="85:257	Joint inference is different from our jointlearning approach, which allows the two tasks to be learned jointly and not independently." ></td>
	<td class="line x" title="86:257	3 Baseline Coreference Models In this section, we describe three coreference models that will serve as our baselines: the mentionpair model, the entity-mention model, and the mention-ranking model." ></td>
	<td class="line x" title="87:257	For illustrative purposes, we will use the text segment shown in Figure 1." ></td>
	<td class="line x" title="88:257	Each mention m in the segment is annotated as [m]cidmid, where mid is the mention id and cid is the id of the cluster to which m belongs." ></td>
	<td class="line x" title="89:257	As we can see, the mentions are partitioned into four sets, with Barack Obama, his, and he in one cluster, and each of the remaining mentions in its own cluster." ></td>
	<td class="line x" title="90:257	3.1 Mention-Pair Model As noted before, a mention-pair model is a classifier that decides whether or not an active mention mk is coreferent with a candidate antecedent mj." ></td>
	<td class="line x" title="91:257	Each instance i(mj,mk) represents mj and 970 [Barack Obama]11 nominated [Hillary Rodham Clinton]22 as [[his]13 secretary of state]34 on [Monday]45." ></td>
	<td class="line x" title="92:257	[He]16  Figure 1: An illustrative example mk and consists of the 39 features shown in Table 1." ></td>
	<td class="line x" title="93:257	These features have largely been employed by state-of-the-art learning-based coreference systems (e.g., Soon et al.(2001), Ng and Cardie (2002b), Bengtson and Roth (2008)), and are computed automatically." ></td>
	<td class="line x" title="95:257	As can be seen, the features are divided into four blocks." ></td>
	<td class="line x" title="96:257	The first two blocks consist of features that describe the properties of mj and mk, respectively, and the last two blocks of features describe the relationship between mj and mk." ></td>
	<td class="line x" title="97:257	The classification associated with a training instance is either positive or negative, depending on whether mj and mk are coreferent." ></td>
	<td class="line x" title="98:257	If one training instance were created from each pair of mentions, the negative instances would significantly outnumber the positives, yielding a skewed class distribution that will typically have an adverse effect on model training." ></td>
	<td class="line x" title="99:257	As a result, only a subset of mention pairs will be generated for training." ></td>
	<td class="line x" title="100:257	Following Soon et al.(2001), we create (1) a positive instance for each discourse-old mention mk and its closest antecedent mj; and (2) a negative instance for mk paired with each of the intervening mentions, mj+1,mj+2,,mk1." ></td>
	<td class="line x" title="102:257	In our running example shown in Figure 1, three training instances will be generated for He: i(Monday, He), i(secretary of state, He), and i(his, He)." ></td>
	<td class="line x" title="103:257	The first two of these instances will be labeled as negative, and the last one will be labeled as positive." ></td>
	<td class="line x" title="104:257	To train a mention-pair classifier, we use the SVM learning algorithm from the SVMlight package (Joachims, 2002), converting all multi-valued features into an equivalent set of binary-valued features." ></td>
	<td class="line x" title="105:257	After training, the resulting SVM classifier is used to identify an antecedent for a mention in a test text." ></td>
	<td class="line x" title="106:257	Specifically, an active mention mk selects as its antecedent the closest preceding mention that is classified as coreferent with mk." ></td>
	<td class="line x" title="107:257	If mk is not classified as coreferent with any preceding mention, it will be considered discourse-new (i.e., no antecedent will be selected for mk)." ></td>
	<td class="line x" title="108:257	3.2 Entity-Mention Model Unlike a mention-pair model, an entity-mention model is a classifier that decides whether or not an active mention mk is coreferent with a partial cluster cj that precedes mk." ></td>
	<td class="line x" title="109:257	Each training instance, i(cj,mk), represents cj and mk." ></td>
	<td class="line x" title="110:257	The features for an instance can be divided into two types: (1) features that describe mk (i.e, those shown in the second block of Table 1), and (2) cluster-level features, which describe the relationship between cj and mk." ></td>
	<td class="line x" title="111:257	Motivated by previous work (Luo et al., 2004; Culotta et al., 2007; Yang et al., 2008), we create cluster-level features from mention-pair features using four predicates: NONE, MOST-FALSE, MOST-TRUE, and ALL." ></td>
	<td class="line x" title="112:257	Specifically, for each feature X shown in the last two blocks in Table 1, we first convert X into an equivalent set of binary-valued features if it is multi-valued." ></td>
	<td class="line x" title="113:257	Then, for each resulting binaryvalued feature Xb, we create four binary-valued cluster-level features: (1) NONE-Xb is true when Xb is false between mk and each mention in cj; (2) MOST-FALSE-Xb is true when Xb is true between mk and less than half (but at least one) of the mentions in cj; (3) MOST-TRUE-Xb is true when Xb is true between mk and at least half (but not all) of the mentions in cj; and (4) ALL-Xb is true when Xb is true between mk and each mention in cj." ></td>
	<td class="line x" title="114:257	Hence, for each Xb, exactly one of these four cluster-level features evaluates to true." ></td>
	<td class="line x" title="115:257	Following Yang et al.(2008), we create (1) a positive instance for each discourse-old mention mk and the preceding cluster cj to which it belongs; and (2) a negative instance for mk paired with each partial cluster whose last mention appears between mk and its closest antecedent (i.e., the last mention of cj)." ></td>
	<td class="line x" title="117:257	Consider again our running example." ></td>
	<td class="line x" title="118:257	Three training instances will be generated for He: i({Monday}, He), i({secretary of state}, He), and i({Barack Obama, his}, He)." ></td>
	<td class="line x" title="119:257	The first two of these instances will be labeled as negative, and the last one will be labeled as positive." ></td>
	<td class="line x" title="120:257	As in the mention-pair model, we train an entity-mention classifier using the SVM learner." ></td>
	<td class="line x" title="121:257	After training, the resulting classifier is used to identify a preceding cluster for a mention in a test text." ></td>
	<td class="line x" title="122:257	Specifically, the mentions are processed in a left-to-right manner." ></td>
	<td class="line x" title="123:257	For each active mention mk, a test instance is created between mk and each of the preceding clusters formed so far." ></td>
	<td class="line x" title="124:257	All the test instances are then presented to the classifier." ></td>
	<td class="line x" title="125:257	Finally, mk will be linked to the closest preceding cluster that is classified as coreferent with mk." ></td>
	<td class="line x" title="126:257	If mk is not classified as coreferent with any 971 Features describing mj, a candidate antecedent 1 PRONOUN 1 Y if mj is a pronoun; else N 2 SUBJECT 1 Y if mj is a subject; else N 3 NESTED 1 Y if mj is a nested NP; else N Features describing mk, the mention to be resolved 4 NUMBER 2 SINGULAR or PLURAL, determined using a lexicon 5 GENDER 2 MALE, FEMALE, NEUTER, or UNKNOWN, determined using a list of common first names 6 PRONOUN 2 Y if mk is a pronoun; else N 7 NESTED 2 Y if mk is a nested NP; else N 8 SEMCLASS 2 the semantic class of mk; can be one of PERSON, LOCATION, ORGANIZATION, DATE, TIME, MONEY, PERCENT, OBJECT, OTHERS, determined using WordNet and an NE recognizer 9 ANIMACY 2 Y if mk is determined as HUMAN or ANIMAL by WordNet and an NE recognizer; else N 10 PRO TYPE 2 the nominative case of mk if it is a pronoun; else NA." ></td>
	<td class="line x" title="127:257	E.g., the feature value for him is HE Features describing the relationship between mj, a candidate antecedent and mk, the mention to be resolved 11 HEAD MATCH C if the mentions have the same head noun; else I 12 STR MATCH C if the mentions are the same string; else I 13 SUBSTR MATCH C if one mention is a substring of the other; else I 14 PRO STR MATCH C if both mentions are pronominal and are the same string; else I 15 PN STR MATCH C if both mentions are proper names and are the same string; else I 16 NONPRO STR MATCH C if the two mentions are both non-pronominal and are the same string; else I 17 MODIFIER MATCH C if the mentions have the same modifiers; NA if one of both of them dont have a modifier; else I 18 PRO TYPE MATCH C if both mentions are pronominal and are either the same pronoun or different only w.r.t. case; NA if at least one of them is not pronominal; else I 19 NUMBER C if the mentions agree in number; I if they disagree; NA if the number for one or both mentions cannot be determined 20 GENDER C if the mentions agree in gender; I if they disagree; NA if the gender for one or both mentions cannot be determined 21 AGREEMENT C if the mentions agree in both gender and number; I if they disagree in both number and gender; else NA 22 ANIMACY C if the mentions match in animacy; I if they dont; NA if the animacy for one or both mentions cannot be determined 23 BOTH PRONOUNS C if both mentions are pronouns; I if neither are pronouns; else NA 24 BOTH PROPER NOUNS C if both mentions are proper nouns; I if neither are proper nouns; else NA 25 MAXIMALNP C if the two mentions does not have the same maximial NP projection; else I 26 SPAN C if neither mention spans the other; else I 27 INDEFINITE C if mk is an indefinite NP and is not in an appositive relationship; else I 28 APPOSITIVE C if the mentions are in an appositive relationship; else I 29 COPULAR C if the mentions are in a copular construction; else I 30 SEMCLASS C if the mentions have the same semantic class; I if they dont; NA if the semantic class information for one or both mentions cannot be determined 31 ALIAS C if one mention is an abbreviation or an acronym of the other; else I 32 DISTANCE binned values for sentence distance between the mentions Additional features describing the relationship between mj, a candidate antecedent and mk, the mention to be resolved 33 NUMBER the concatenation of the NUMBER 2 feature values of mj and mk." ></td>
	<td class="line x" title="128:257	E.g., if mj is Clinton and mk is they, the feature value is SINGULAR-PLURAL, since mj is singular and mk is plural 34 GENDER the concatenation of the GENDER 2 feature values of mj and mk 35 PRONOUN the concatenation of the PRONOUN 2 feature values of mj and mk 36 NESTED the concatenation of the NESTED 2 feature values of mj and mk 37 SEMCLASS the concatenation of the SEMCLASS 2 feature values of mj and mk 38 ANIMACY the concatenation of the ANIMACY 2 feature values of mj and mk 39 PRO TYPE the concatenation of the PRO TYPE 2 feature values of mj and mk Table 1: The feature set for coreference resolution." ></td>
	<td class="line x" title="129:257	Non-relational features describe a mention and in most cases take on a value of YES or NO." ></td>
	<td class="line x" title="130:257	Relational features describe the relationship between the two mentions and indicate whether they are COMPATIBLE, INCOMPATIBLE or NOT APPLICABLE." ></td>
	<td class="line x" title="131:257	preceding cluster, it will be considered discoursenew." ></td>
	<td class="line x" title="132:257	Note that all partial clusters preceding mk are formed incrementally based on the predictions of the classifier for the first k  1 mentions." ></td>
	<td class="line x" title="133:257	3.3 Mention-Ranking Model As noted before, a ranking model imposes a ranking on all the candidate antecedents of an active mention mk." ></td>
	<td class="line x" title="134:257	To train a ranker, we use the SVM ranker-learning algorithm from the SVMlight package." ></td>
	<td class="line x" title="135:257	Like the mention-pair model, each training instance i(mj,mk) represents mk and a preceding mention mj." ></td>
	<td class="line x" title="136:257	In fact, the features that represent the instance as well as the method for creating training instances are identical to those employed by the mention-pair model." ></td>
	<td class="line x" title="137:257	972 The only difference lies in the assignment of class values to training instances." ></td>
	<td class="line x" title="138:257	Assuming that Sk is the set of training instances created for anaphoric mention mk, the class value for an instance i(mj,mk) in Sk is the rank of mj among competing candidate antecedents, which is 2 if mj is the closest antecedent of mk, and 1 otherwise.1 To exemplify, consider our running example." ></td>
	<td class="line x" title="139:257	As in the mention-pair model, three training instances will be generated for He: i(Monday, He), i(secretary of state, He), i(his, He)." ></td>
	<td class="line x" title="140:257	The third instance will have a class value of 2, and the remaining two will have a class value of 1." ></td>
	<td class="line x" title="141:257	After training, the mention-ranking model is applied to rank the candidate antecedents for an active mention in a test text as follows." ></td>
	<td class="line x" title="142:257	Given an active mention mk, we follow Denis and Baldridge (2008) and use an independently-trained classifier to determine whether mk is discourse-new." ></td>
	<td class="line x" title="143:257	If so, mk will not be resolved." ></td>
	<td class="line x" title="144:257	Otherwise, we create test instances for mk by pairing it with each of its preceding mentions." ></td>
	<td class="line x" title="145:257	The test instances are then presented to the ranker, and the preceding mention that is assigned the largest value by the ranker is selected as the antecedent of mk." ></td>
	<td class="line x" title="146:257	The discourse-new classifier used in the resolution step is trained with 26 of the 37 features2 described in Ng and Cardie (2002a) that are deemed useful for distinguishing between anaphoric and non-anaphoric mentions." ></td>
	<td class="line x" title="147:257	These features can be broadly divided into two types: (1) features that encode the form of the mention (e.g., NP type, number, definiteness), and (2) features that compare the mention to one of its preceding mentions." ></td>
	<td class="line x" title="148:257	4 Coreference as Cluster Ranking In this section, we describe our cluster-ranking approach to NP coreference." ></td>
	<td class="line x" title="149:257	As noted before, our approach aims to combine the strengths of entitymention models and mention-ranking models." ></td>
	<td class="line x" title="150:257	4.1 Training and Applying a Cluster Ranker For ease of exposition, we will describe in this subsection how to train and apply a cluster ranker when it is used in a pipeline architecture, where discourse-new detection is performed prior to coreference resolution." ></td>
	<td class="line x" title="151:257	In the next subsection, we will show how the two tasks can be learned jointly." ></td>
	<td class="line x" title="152:257	1A larger class value implies a better rank in SVMlight." ></td>
	<td class="line x" title="153:257	2The 11 features that we did not employ are CONJ, POSSESSIVE, MODIFIER, POSTMODIFIED, SPECIAL NOUNS, POST, SUBCLASS, TITLE, and the positional features." ></td>
	<td class="line x" title="154:257	Recall that a cluster ranker ranks a set of preceding clusters for an active mention mk." ></td>
	<td class="line x" title="155:257	Since a cluster ranker is a hybrid of a mention-ranking model and an entity-mention model, the way it is trained and applied is also a hybrid of the two." ></td>
	<td class="line x" title="156:257	In particular, the instance representation employed by a cluster ranker is identical to that used by an entity-mention model, where each training instance i(cj, mk) represents a preceding cluster cj and a discourse-old mention mk and consists of cluster-level features formed from predicates." ></td>
	<td class="line x" title="157:257	Unlike in an entity-mention model, however, in a cluster ranker, (1) a training instance is created between each discourse-old mention mk and each of its preceding clusters; and (2) since we are training a model for ranking clusters, the assignment of class values to training instances is similar to that of a mention ranker." ></td>
	<td class="line x" title="158:257	Specifically, the class value of a training instance i(cj, mk) created for mk is the rank of cj among the competing clusters, which is 2 if mk belongs to cj, and 1 otherwise." ></td>
	<td class="line x" title="159:257	Applying the learned cluster ranker to a test text is similar to applying a mention ranker." ></td>
	<td class="line x" title="160:257	Specifically, the mentions are processed in a left-to-right manner." ></td>
	<td class="line x" title="161:257	For each active mention mk, we first apply an independently-trained classifier to determine if mk is discourse-new." ></td>
	<td class="line x" title="162:257	If so, mk will not be resolved." ></td>
	<td class="line x" title="163:257	Otherwise, we create test instances for mk by pairing it with each of its preceding clusters." ></td>
	<td class="line x" title="164:257	The test instances are then presented to the ranker, and mk is linked to the cluster that is assigned the highest value by the ranker." ></td>
	<td class="line x" title="165:257	Note that these partial clusters preceding mk are formed incrementally based on the predictions of the ranker for the first k1 mentions; no gold-standard coreference information is used in their formation." ></td>
	<td class="line x" title="166:257	4.2 Joint Discourse-New Detection and Coreference Resolution The cluster ranker described above can be used to determine which preceding cluster a discourseold mention should be linked to, but it cannot be used to determine whether a mention is discoursenew or not." ></td>
	<td class="line x" title="167:257	The reason is simple: all the training instances are generated from discourse-old mentions." ></td>
	<td class="line x" title="168:257	Hence, to jointly learn discourse-new detection and coreference resolution, we must train the ranker using instances generated from both discourse-old and discourse-new mentions." ></td>
	<td class="line x" title="169:257	Specifically, when training the ranker, we provide each active mention with the option to start 973 a new cluster by creating an additional instance that (1) contains features that solely describe the active mention (i.e., the features shown in the second block of Table 1), and (2) has the highest rank value among competing clusters (i.e., 2) if it is discourse-new and the lowest rank value (i.e., 1) otherwise." ></td>
	<td class="line x" title="170:257	The main advantage of jointly learning the two tasks is that it allows the ranking model to evaluate all possible options for an active mention (i.e., whether to resolve it, and if so, which preceding cluster is the best) simultaneously." ></td>
	<td class="line x" title="171:257	After training, the resulting cluster ranker processes the mentions in a test text in a left-to-right manner." ></td>
	<td class="line x" title="172:257	For each active mention mk, we create test instances for it by pairing it with each of its preceding clusters." ></td>
	<td class="line x" title="173:257	To allow for the possibility that mk is discourse-new, we create an additional test instance that contains features that solely describe the active mention (similar to what we did in the training step above)." ></td>
	<td class="line x" title="174:257	All these test instances are then presented to the ranker." ></td>
	<td class="line x" title="175:257	If the additional test instance is assigned the highest rank value by the ranker, then mk is classified as discourse-new and will not be resolved." ></td>
	<td class="line x" title="176:257	Otherwise, mk is linked to the cluster that has the highest rank." ></td>
	<td class="line x" title="177:257	As before, all partial clusters preceding mk are formed incrementally based on the predictions of the ranker for the first k  1 mentions." ></td>
	<td class="line x" title="178:257	5 Evaluation 5.1 Experimental Setup Corpus." ></td>
	<td class="line x" title="179:257	We use the ACE 2005 coreference corpus as released by the LDC, which consists of the 599 training documents used in the official ACE evaluation.3 To ensure diversity, the corpus was created by selecting documents from six different sources: Broadcast News (bn), Broadcast Conversations (bc), Newswire (nw), Webblog (wb), Usenet (un), and conversational telephone speech (cts)." ></td>
	<td class="line x" title="180:257	The number of documents belonging to each source is shown in Table 2." ></td>
	<td class="line x" title="181:257	For evaluation, we partition the 599 documents into a training set and a test set following a 80/20 ratio, ensuring that the two sets have the same proportion of documents from the six sources." ></td>
	<td class="line x" title="182:257	Mention extractor." ></td>
	<td class="line x" title="183:257	We evaluate each coreference model using both true mentions (i.e., gold standard mentions4) and system mentions (i.e., au3Since we did not participate in ACE 2005, we do not have access to the official test set." ></td>
	<td class="line x" title="184:257	4Note that only mention boundaries are used." ></td>
	<td class="line x" title="185:257	Dataset bn bc nw wl un cts # of documents 60 226 106 119 49 39 Table 2: Statistics for the ACE 2005 corpus tomatically identified mentions)." ></td>
	<td class="line x" title="186:257	To extract system mentions from a test text, we trained a mention extractor on the training texts." ></td>
	<td class="line x" title="187:257	Following Florian et al.(2004), we recast mention extraction as a sequence labeling task, where we assign to each token in a test text a label that indicates whether it begins a mention, is inside a mention, or is outside a mention." ></td>
	<td class="line x" title="189:257	Hence, to learn the extractor, we create one training instance for each token in a training text and derive its class value (one of b, i, and o) from the annotated data." ></td>
	<td class="line x" title="190:257	Each instance represents wi, the token under consideration, and consists of 29 linguistic features, many of which are modeled after the systems of Bikel et al.(1999) and Florian et al.(2004), as described below." ></td>
	<td class="line x" title="193:257	Lexical (7): Tokens in a window of 7: {wi3,,wi+3}." ></td>
	<td class="line x" title="194:257	Capitalization (4): Determine whether wi IsAllCap, IsInitCap, IsCapPeriod, and IsAllLower (see Bikel et al.(1999))." ></td>
	<td class="line x" title="196:257	Morphological (8): wis prefixes and suffixes of length one, two, three, and four." ></td>
	<td class="line x" title="197:257	Grammatical (1): The part-of-speech (POS) tag of wi obtained using the Stanford log-linear POS tagger (Toutanova et al., 2003)." ></td>
	<td class="line oc" title="198:257	Semantic (1): The named entity (NE) tag of wi obtained using the Stanford CRF-based NE recognizer (Finkel et al., 2005)." ></td>
	<td class="line x" title="199:257	Gazetteers (8): Eight dictionaries containing pronouns (77 entries), common words and words that are not names (399.6k), person names (83.6k), person titles and honorifics (761), vehicle words (226), location names (1.8k), company names (77.6k), and nouns extracted from WordNet that are hyponyms of PERSON (6.3k)." ></td>
	<td class="line x" title="200:257	We employ CRF++5, a C++ implementation of conditional random fields, for training the mention detector, which achieves an F-score of 86.7 (86.1 recall, 87.2 precision) on the test set." ></td>
	<td class="line x" title="201:257	These extracted mentions are to be used as system mentions in our coreference experiments." ></td>
	<td class="line x" title="202:257	Scoring programs." ></td>
	<td class="line x" title="203:257	To score the output of a coreference model, we employ three scoring programs: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and 3-CEAF (Luo, 2005)." ></td>
	<td class="line x" title="204:257	5Available from http://crfpp.sourceforge.net 974 There is a complication, however." ></td>
	<td class="line x" title="205:257	When scoring a response (i.e., system-generated) partition against a key (i.e., gold-standard) partition, a scoring program needs to construct a mapping between the mentions in the response and those in the key." ></td>
	<td class="line x" title="206:257	If the response is generated using true mentions, then every mention in the response is mapped to some mention in the key and vice versa; in other words, there are no twinless (i.e., unmapped) mentions (Stoyanov et al., 2009)." ></td>
	<td class="line x" title="207:257	However, this is not the case when system mentions are used." ></td>
	<td class="line x" title="208:257	The aforementioned complication does not arise from the construction of the mapping, but from the fact that Bagga and Baldwin (1998) and Luo (2005) do not specify how to apply B3 and CEAF to score partitions generated from system mentions." ></td>
	<td class="line x" title="209:257	We propose a simple solution to this problem: we remove all and only those twinless system mentions that are singletons before applying B3 and CEAF." ></td>
	<td class="line x" title="210:257	The reason is simple: since the coreference resolver has successfully identified these mentions as singletons, it should not be penalized, and removing them allows us to avoid such penalty." ></td>
	<td class="line x" title="211:257	Note that we only remove twinless (as opposed to all) system mentions that are singletons: this allows us to reward a resolver for successful identification of singleton mentions that have twins, thus overcoming a major weakness of and common criticism against the MUC scorer." ></td>
	<td class="line x" title="212:257	Also, we retain twinless system mentions that are nonsingletons, as the resolver should be penalized for identifying spurious coreference relations." ></td>
	<td class="line x" title="213:257	On the other hand, we do not remove twinless mentions in the key partition, as we want to ensure that the resolver makes the correct (non-)coreference decisions for them." ></td>
	<td class="line x" title="214:257	We believe that our proposal addresses Stoyanov et al.s (2009) problem of having very low precision when applying the CEAF scorer to score partitions of system mentions." ></td>
	<td class="line x" title="215:257	5.2 Results and Discussions The mention-pair baseline." ></td>
	<td class="line x" title="216:257	We train our first baseline, the mention-pair coreference classifier, using the SVM learning algorithm as implemented in the SVMlight package (Joachims, 2002).6 Results of this baseline using true mentions and system mentions, shown in row 1 of Tables 3 and 4, are reported in terms of recall (R), precision (P), and F-score (F) provided by the three scoring pro6For this and subsequent uses of the SVM learner in our experiments, we set all parameters to their default values." ></td>
	<td class="line x" title="217:257	grams." ></td>
	<td class="line x" title="218:257	As we can see, this baseline achieves Fscores of 54.370.0 and 53.462.5 for true mentions and system mentions, respectively." ></td>
	<td class="line x" title="219:257	The entity-mention baseline." ></td>
	<td class="line x" title="220:257	Next, we train our second baseline, the entity-mention coreference classifier, using the SVM learner." ></td>
	<td class="line x" title="221:257	Results of this baseline are shown in row 2 of Tables 3 and 4." ></td>
	<td class="line x" title="222:257	For true mentions, this baseline achieves an Fscore of 54.870.7." ></td>
	<td class="line x" title="223:257	In comparison to the mentionpair baseline, F-score rises insignificantly according to all three scorers.7 Similar trends can be observed for system mentions, where the F-scores between the two models are statistically indistinguishable across the board." ></td>
	<td class="line x" title="224:257	While the insignificant performance difference is somewhat surprising given the improved expressiveness of entitymention models over mention-pair models, similar trends have been reported by Luo et al.(2004)." ></td>
	<td class="line x" title="226:257	The mention-ranking baseline." ></td>
	<td class="line x" title="227:257	Our third baseline is the mention-ranking coreference model, trained using the ranker-learning algorithm in SVMlight." ></td>
	<td class="line x" title="228:257	To identify discourse-new mentions, we employ two methods." ></td>
	<td class="line x" title="229:257	In the first method, we adopt a pipeline architecture, where we train an SVM classifier for discourse-new detection independently of the mention ranker on the training set using the 26 features described in Section 3.3." ></td>
	<td class="line x" title="230:257	We then apply the resulting classifier to each test text to filter discourse-new mentions prior to coreference resolution." ></td>
	<td class="line x" title="231:257	Results of the mention ranker are shown in row 3 of Tables 3 and 4." ></td>
	<td class="line x" title="232:257	As we can see, the ranker achieves F-scores of 57.871.2 and 54.165.4 for true mentions and system mentions, respectively, yielding a significant improvement over the entity-mention baseline in all but one case (MUC/true mentions)." ></td>
	<td class="line x" title="233:257	In the second method, we perform discoursenew detection jointly with coreference resolution using the method described in Section 4.2." ></td>
	<td class="line x" title="234:257	While we discussed this joint learning method in the context of cluster ranking, it should be easy to see that the method is equally applicable to a mention ranker." ></td>
	<td class="line x" title="235:257	Results of the mention ranker using this joint architecture are shown in row 4 of Tables 3 and 4." ></td>
	<td class="line x" title="236:257	As we can see, the ranker achieves F-scores of 61.673.4 and 55.667.1 for true mentions and system mentions, respectively." ></td>
	<td class="line x" title="237:257	For both types of mentions, the improvements over the corresponding results for the entity-mention baseline 7We use Approximate Randomization (Noreen, 1989) for testing statistical significance, with p set to 0.05." ></td>
	<td class="line x" title="238:257	975 MUC CEAF B3 Coreference Model R P F R P F R P F 1 Mention-pair model 71.7 69.2 70.4 54.3 54.3 54.3 53.3 63.6 58.0 2 Entity-mention model 71.7 69.7 70.7 54.8 54.8 54.8 53.2 65.1 58.5 3 Mention-ranking model (Pipeline) 68.7 73.9 71.2 57.8 57.8 57.8 55.8 63.9 59.6 4 Mention-ranking model (Joint) 69.4 77.8 73.4 61.6 61.6 61.6 57.0 70.1 62.9 5 Cluster-ranking model (Pipeline) 71.7 78.2 74.8 61.8 61.8 61.8 58.2 69.1 63.2 6 Cluster-ranking model (Joint) 69.9 83.3 76.0 63.3 63.3 63.3 56.0 74.6 64.0 Table 3: MUC, CEAF, and B3 coreference results using true mentions." ></td>
	<td class="line x" title="239:257	MUC CEAF B3 Coreference Model R P F R P F R P F 1 Mention-pair model 70.0 56.4 62.5 56.1 51.0 53.4 50.8 57.9 54.1 2 Entity-mention model 68.5 57.2 62.3 56.3 50.2 53.1 51.2 57.8 54.3 3 Mention-ranking model (Pipeline) 62.2 68.9 65.4 51.6 56.7 54.1 52.3 61.8 56.6 4 Mention-ranking model (Joint) 62.1 73.0 67.1 53.0 58.5 55.6 50.4 65.5 56.9 5 Cluster-ranking model (Pipeline) 65.3 72.3 68.7 54.1 59.3 56.6 55.3 63.7 59.2 6 Cluster-ranking model (Joint) 64.1 75.4 69.3 56.7 62.6 59.5 54.4 70.5 61.4 Table 4: MUC, CEAF, and B3 coreference results using system mentions." ></td>
	<td class="line x" title="240:257	are significant, and suggest that mention ranking is a precision-enhancing device." ></td>
	<td class="line x" title="241:257	Moreover, in comparison to the pipeline architecture in row 3, we see that F-score rises significantly by 2.23.8% for true mentions, and improves by a smaller margin of 0.31.7% for system mentions." ></td>
	<td class="line x" title="242:257	These results demonstrate the benefits of joint modeling." ></td>
	<td class="line x" title="243:257	Our cluster-ranking model." ></td>
	<td class="line x" title="244:257	Finally, we evaluate our cluster-ranking model." ></td>
	<td class="line x" title="245:257	As in the mentionranking baseline, we employ both the pipeline architecture and the joint architecture for discoursenew detection." ></td>
	<td class="line x" title="246:257	Results are shown in rows 5 and 6 of Tables 3 and 4, respectively, for the two architectures." ></td>
	<td class="line x" title="247:257	When true mentions are used, the pipeline architecture yields an F-score of 61.8 74.8, which represents a significant improvement over the mention ranker adopting the pipeline architecture." ></td>
	<td class="line x" title="248:257	With the joint architecture, the cluster ranker achieves an F-score of 63.376.0." ></td>
	<td class="line x" title="249:257	This also represents a significant improvement over the mention ranker adopting the joint architecture, the best of the baselines, and suggests that cluster ranking is a better precision-enhancing model than mention ranking." ></td>
	<td class="line x" title="250:257	Moreover, comparing the results in these two rows reveals the superiority of the joint architecture over the pipeline architecture, particularly in terms of its ability to enhance system precision." ></td>
	<td class="line x" title="251:257	Similar performance trends can be observed when system mentions are used." ></td>
	<td class="line x" title="252:257	6 Conclusions We have presented a cluster-ranking approach that recasts the mention resolution process as the problem of finding the best preceding cluster to link an active mention to." ></td>
	<td class="line x" title="253:257	Crucially, our approach combines the strengths of entity-mention models and mention-ranking models." ></td>
	<td class="line x" title="254:257	Experimental results on the ACE 2005 corpus show that (1) jointly learning coreference resolution and discourse-new detection allows the cluster ranker to achieve better performance than adopting a pipeline coreference architecture; and (2) our cluster ranker significantly outperforms the mention ranker, the best of the three baseline coreference models, under both the pipeline architecture and the joint architecture." ></td>
	<td class="line x" title="255:257	Overall, we believe that our cluster-ranking approach advances the state-of-the-art in coreference resolution both theoretically and empirically." ></td>
	<td class="line x" title="256:257	Acknowledgments We thank the three anonymous reviewers for their invaluable comments on the paper." ></td>
	<td class="line x" title="257:257	This work was supported in part by NSF Grant IIS-0812261." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1119
On the Role of Lexical Features in Sequence Labeling
Goldberg, Yoav;Elhadad, Michael;"></td>
	<td class="line x" title="1:258	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 11421151, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:258	c 2009 ACL and AFNLP On the Role of Lexical Features in Sequence Labeling Yoav Goldberg and Michael Elhadad Ben Gurion University of the Negev Department of Computer Science POB 653 Beer Sheva, 84105, Israel {yoavg|elhadad}@cs.bgu.ac.il Abstract We use the technique of SVM anchoring to demonstrate that lexical features extracted from a training corpus are not necessary to obtain state of the art results on tasks such as Named Entity Recognition and Chunking." ></td>
	<td class="line x" title="3:258	While standard models require as many as 100K distinct features, we derive models with as little as 1K features that perform as well or better on different domains." ></td>
	<td class="line x" title="4:258	These robust reduced models indicate that the way rare lexical features contribute to classification in NLP is not fully understood." ></td>
	<td class="line x" title="5:258	Contrastive error analysis (with and without lexical features) indicates that lexical features do contribute to resolving some semantic and complex syntactic ambiguities  but we find this contribution does not generalize outside the training corpus." ></td>
	<td class="line x" title="6:258	As a general strategy, we believe lexical features should not be directly derived from a training corpus but instead, carefully inferred and selected from other sources." ></td>
	<td class="line x" title="7:258	1 Introduction Common NLP tasks, such as Named Entity Recognition and Chunking, involve the identification of spans of words belonging to the same phrase." ></td>
	<td class="line x" title="8:258	These tasks are traditionally reduced to a tagging task, in which each word is to be classified as either Beginning a span, Inside a span, or Outside of a span." ></td>
	<td class="line x" title="9:258	The decision is based on the word to be classified and its neighbors." ></td>
	<td class="line x" title="10:258	Features supporting the classification usually include Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University the word forms themselves and properties derived from the word forms, such as prefixes, suffixes, capitalization information, and parts-of-speech." ></td>
	<td class="line x" title="11:258	While early approaches to the NP-chunking task (Cardie and Pierce, 1998) relied on part-of-speech information alone, it is widely accepted that lexical information (word forms) is crucial for building accurate systems for these tasks." ></td>
	<td class="line x" title="12:258	Indeed, all the better-performing systems in the CoNLL shared tasks competitions for Chunking (Sang and Buchholz, 2000) and Named Entity Recognition (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) make extensive use of such lexical information." ></td>
	<td class="line x" title="13:258	Is this belief justified?" ></td>
	<td class="line x" title="14:258	In this paper, we show that the influence of lexical features on such sequence labeling tasks is more complex than is generally assumed." ></td>
	<td class="line x" title="15:258	We find that exact word forms arent necessary for accurate classification." ></td>
	<td class="line x" title="16:258	This observation is important because relying on the exact word forms that appear in a training corpus leads to over-fitting, as well as to larger models." ></td>
	<td class="line x" title="17:258	In this work, we focus on learning with Support Vector Machines (SVMs) (Vapnik, 1995)." ></td>
	<td class="line x" title="18:258	SVM classifiers can handle very large feature spaces, and produce state-of-the-art results for NLP applications (see e.g.(Kudo and Matsumoto, 2000; Nivre et al., 2006))." ></td>
	<td class="line x" title="20:258	Alas, when trained on pruned feature sets, in which rare lexical items are removed, SVM models suffer a loss in classification accuracy." ></td>
	<td class="line x" title="21:258	It would seem that rare lexical items are indeed crucial for SVM classification performance." ></td>
	<td class="line x" title="22:258	However, in Goldberg and Elhadad (2007), we suggested that the SVM learner is using the rare lexical features for singling out hard cases rather than for learning meaningful generalizations." ></td>
	<td class="line x" title="23:258	We provide further evidence to support this claim in this paper." ></td>
	<td class="line x" title="24:258	1142 We show that by using a variant of SVM  Anchored SVM Learning (Goldberg and Elhadad, 2007) with a polynomial kernel, one can learn accurate models for English NP-chunking (Marcus and Ramshaw, 1995), base-phrase chunking (CoNLL 2000), and Dutch Named Entity Recognition (CoNLL 2002), on a heavily pruned feature space." ></td>
	<td class="line x" title="25:258	Our models make use of only a fraction of the lexical features available in the training set (less than 1%), and yet provide highly-competitive accuracies." ></td>
	<td class="line x" title="26:258	For the Chunking and NP-Chunking tasks, the most heavily pruned experiments, in which we consider only features appearing at least 100 times in the training corpus, do show a small but significant drop in accuracy on the testing corpus compared to the non-pruned models exposed to all available features in the training data." ></td>
	<td class="line x" title="27:258	We provide detailed error analysis of a development set in Section 6, revealing the causes for these differences." ></td>
	<td class="line x" title="28:258	We suggest one additional binary feature in order to account for some of the performance gap." ></td>
	<td class="line x" title="29:258	Moreover, we show that the differences in accuracy vanish when the lexicalized and unlexicalized models are tested on text from slightly different sources than the training corpus (Section 7)." ></td>
	<td class="line x" title="30:258	This goes to show that with an appropriate learning method, orthographic and structural (in the form of POS tag sequences) information is sufficient for achieving state-of-the-art performance on these kind of sequence labeling tasks." ></td>
	<td class="line x" title="31:258	This does not mean semantic information is not needed for these tasks." ></td>
	<td class="line x" title="32:258	It does mean that current models capture only a tiny amount of such semantic information through rare lexical features, and in a manner that does not generalize well." ></td>
	<td class="line x" title="33:258	We believe this data motivates a different strategy to incorporate lexical features into classification models: instead of collecting the raw lexical forms appearing in a training corpus, we should attempt to actively construct a feature space including lexical features derived from external sources." ></td>
	<td class="line x" title="34:258	The feature representation of (Collobert and Weston, 2008) could be a step in that direction." ></td>
	<td class="line x" title="35:258	We also believe that hard cases for sequence labeling (POS ambiguity, coordination, long syntactic constructs) could be directly approached with specialized classifiers." ></td>
	<td class="line x" title="36:258	1.1 Related Work This work complements a similar line of results from the parsing literature." ></td>
	<td class="line x" title="37:258	While it was initially believed that lexicalization of PCFG parsers (Collins, 1997; Charniak, 2000) is crucial for obtaining good parsing results, Gildea (2001) demonstrated that the lexicalized Model-1 parser of Collins (1997) does not benefit from bilexical information when tested on a new text domain, and only marginally benefits from such information when tested on the same text domain as the training corpora." ></td>
	<td class="line x" title="38:258	This was followed by (Bikel, 2004) who showed that bilexical-information is used in only 1.49% of the decisions in Collins Model-2 parser, and that removing this information results in an exceedingly small drop in performance." ></td>
	<td class="line x" title="39:258	However, uni-lexical information was still considered crucial." ></td>
	<td class="line x" title="40:258	Klein and Manning (2003) bridged the gap between lexicalized and unlexicalized parsing performance, providing a competitive unlexicalized parsing model, relying on lexical information for only a few closed-class lexical items." ></td>
	<td class="line x" title="41:258	This was recently followed by (Matsuzaki et al., 2005; Petrov et al., 2006) who introduce state-of-the-art nearly unlexicalized PCFG parsers." ></td>
	<td class="line x" title="42:258	Similarly for discriminative dependency parsing, state-of-the-art parsers (McDonald, 2006; Nivre et al., 2006) are highly lexicalized." ></td>
	<td class="line x" title="43:258	However, the model analysis in (McDonald, 2006) reveals that bilexical features hardly contribute to the performance of a discriminative MSTbased dependency parser, while Kawahara and Uchimoto (2007) demonstrate that minimallylexicalized shift-reduce based dependency parsers can produce near state-of-the-art accuracy." ></td>
	<td class="line x" title="44:258	In this work, we address the same question of determining the impact of lexical features on a different family of tasks: sequence labeling, as illustrated by named entity recognition and chunking." ></td>
	<td class="line oc" title="45:258	As discussed above, all state-of-the-art published methods rely on lexical features for such tasks (Zhang et al., 2001; Sha and Pereira, 2003; Finkel et al., 2005; Ratinov and Roth, 2009)." ></td>
	<td class="line x" title="46:258	Sequence labeling includes both a structural aspect (bracketing the chunks) and a tagging aspect (classifying the chunks)." ></td>
	<td class="line x" title="47:258	While we expect the structural aspect can benefit from techniques similar to those used in the parsing literature, it is unclear whether the tagging component could perform well without detailed lexical information." ></td>
	<td class="line x" title="48:258	We demonstrate in this work that, indeed, lexical features are not necessary to obtain competitive performance." ></td>
	<td class="line x" title="49:258	Our approach consists in performing a detailed analysis 1143 of the role played by rare lexical features in SVM models." ></td>
	<td class="line x" title="50:258	We distinguish the information brought to the model by such features from the role they play in a specific learning method." ></td>
	<td class="line x" title="51:258	2 Learning with Less Features We adopt the common feature representation in which each data-point is represented as a sparse D dimensional binary-valued vector f. Each of the D possible features fi is an indicator function." ></td>
	<td class="line x" title="52:258	The indicator functions look at properties of the current or neighbouring words." ></td>
	<td class="line x" title="53:258	An example of such function fi is 1 iff the previous word-form is DOG, 0 otherwise." ></td>
	<td class="line x" title="54:258	The lexical (word-form) features result in extremely high-dimensional (yet very sparse) feature vectors  each word-form in the vocabulary of the training set correspond to (at-least) one indicator function." ></td>
	<td class="line x" title="55:258	Due to the Zipfian distribution of language data, many of the lexical features are very rare, and appear only a couple times in the training set." ></td>
	<td class="line x" title="56:258	Ideally, we would like our classifiers to learn only from robust features: consider only features that appear at least k times in the training data (rarefeature pruning)." ></td>
	<td class="line x" title="57:258	These features are more likely to appear in unseen test data, and thus such features can support more robust generalization." ></td>
	<td class="line x" title="58:258	However, we find empirically that performing such feature pruning prior to learning SVM models hurts the performance of the learned models." ></td>
	<td class="line x" title="59:258	Our intuition is that this sensitivity to rare lexical features is not explained by the richness of information such rare features bring to the model." ></td>
	<td class="line x" title="60:258	Instead, we believe that rare lexical features help the classifier because they make the data artificially more separable." ></td>
	<td class="line x" title="61:258	To demonstrate this claim, we experiment with anchored SVM, which introduces artificial mechanical anchors into the model to achieve separability, and make rare lexical features unnecessary." ></td>
	<td class="line x" title="62:258	3 Learning Method SVM are discriminative, max-margin, linear classifiers (Vapnik, 1995), which can be kernelized." ></td>
	<td class="line x" title="63:258	For the formulation of SVMs in the context of NLP applications, see (Kudo and Matsumoto, 2001)." ></td>
	<td class="line x" title="64:258	SVMs with a polynomial kernel of degree 2 were shown to provide state-of-the-art performance in many NLP application, see for example (Kudo and Matsumoto, 2000; Nivre et al., 2006; Isozaki and Kazawa, 2002; Goldberg et al., 2006)." ></td>
	<td class="line x" title="65:258	SVMs cope with inseparable data by introducing a soft-margin  allowing some of the training instances to be classified incorrectly subject to a penalty, controlled by a parameter C. Anchored SVM As we show in Section 5, the soft-margin heuristic performs sub-optimally for NLP tasks when the data is inseparable." ></td>
	<td class="line x" title="66:258	We use instead the Anchored Learning heuristic, introduced in (Goldberg and Elhadad, 2007)." ></td>
	<td class="line x" title="67:258	The idea behind anchored learning is that some training instances are inherently ambiguous." ></td>
	<td class="line x" title="68:258	This ambiguity stems from ambiguity in language structure, which cannot be resolved with a given feature representation." ></td>
	<td class="line x" title="69:258	When a data-point cannot be classified, it might be due to missing information, which is not available in the data representation." ></td>
	<td class="line x" title="70:258	Instead of allowing ambiguous items to be misclassified during training, we make the training data artificially separable." ></td>
	<td class="line x" title="71:258	This is achieved by adding a unique feature to each training example (an anchor)." ></td>
	<td class="line x" title="72:258	These anchor features cause each data-point to be slightly more similar to itself than to any other data point." ></td>
	<td class="line x" title="73:258	At test time, we remove anchor features." ></td>
	<td class="line x" title="74:258	In terms of kernel-based learning, anchored learning can be achieved by redefining the dot product between two vectors to take into account the identity of the vectors: xiancxj = xixj +ij." ></td>
	<td class="line x" title="75:258	The classifier learned over the anchored data takes into account the fine interactions between the various inseparable data points." ></td>
	<td class="line x" title="76:258	In our experiments, SVM models over anchored data have many more support vectors than soft-margin SVM models." ></td>
	<td class="line x" title="77:258	However, the anchored models generalize much better when less features are available." ></td>
	<td class="line x" title="78:258	Relation to L2 SVM The classic soft-margin SVM formulation uses L1-penalty for misclassified instances." ></td>
	<td class="line x" title="79:258	Specifically, the objective of the learner is to minimize 12||w||2 + Csummationtextii subject to some margin constraints, where w is a weight vector to be learned and i is the misclassification error for instancei." ></td>
	<td class="line x" title="80:258	This is equivalent to maximizing the dual problem:summationtext M i=1i 12 summationtext i,jijyiyjK(xi,xj) Another variant is L2-penalty SVM (Koshiba and Abe, 2003), in which there is a quadratic penalty for misclassified instances." ></td>
	<td class="line x" title="81:258	Here, the learning objective is to minimize: 1 2||w|| 2 + 1 2C summationtext i2i or alternatively maximize the dual: summationtextii 12 summationtexti,jijyiyj(K(xi,xj) + ijC )." ></td>
	<td class="line x" title="82:258	Interestingly, for the linear kernel, SVM1144 anchoring reduces to L2-SVM with C=1." ></td>
	<td class="line x" title="83:258	However, for the case of non-linear kernels, anchored and L2-SVM produce different results, as the anchoring is applied prior to the kernel expansion." ></td>
	<td class="line x" title="84:258	Specifically for the case of the second-degree polynomial kernel, L2-SVM aims to maximize:summationtext ii 12 summationtext i,jijyiyj((xixj + 1)2 + ij C ), while the anchored-SVM variant would maximizes: summationtextii12 summationtexti,jijyiyj(xixj+ij+1)2." ></td>
	<td class="line x" title="85:258	In our experiments, as discussed in Section 5.4, we find that anchored-SVM and soft-margin SVM with tuned C value both reach good results when we reduce the amount of lexical features." ></td>
	<td class="line x" title="86:258	Anchored-SVM, however, does not require fine-tuning of the error-parameter C since it insures separability." ></td>
	<td class="line x" title="87:258	As a result, we learn anchoredSVM models quickly (few hours) as opposed to several days per model for C-tuned soft-margin SVM." ></td>
	<td class="line x" title="88:258	Anchored-SVMs also provide an easy explanation of the role of features in terms of separability." ></td>
	<td class="line x" title="89:258	Therefore, we use anchored-SVMs in our experiments as the learning method, but we expect that other learning methods are capable of learning with the same reduced feature sets." ></td>
	<td class="line x" title="90:258	4 Experiment Setup How important are the rare lexical features for learning accurate NLP models?" ></td>
	<td class="line x" title="91:258	To investigate this question, we experiment with 3 different NLP sequence-labeling tasks." ></td>
	<td class="line x" title="92:258	For each task, we train a sequence of polynomial kernel (d=2) SVM classifiers, using both soft-margin (C=1) and anchored SVM." ></td>
	<td class="line x" title="93:258	Each classifier is trained on a pruned feature set, in which only features appearing at least k times in the training data are kept." ></td>
	<td class="line x" title="94:258	We vary the pruning parameter k. Pruning is performed over all the features in the model, but lexical features are most affected by it." ></td>
	<td class="line x" title="95:258	For all the models, we use the B-I-O representation, and perform multiclass classification using pairwise-voting." ></td>
	<td class="line x" title="96:258	For our features, we consider properties of tokens in a 5-token window centered around the token to be classified, as well as the two previous classifier predictions." ></td>
	<td class="line x" title="97:258	Results are reported as F-measure over labeled identified spans." ></td>
	<td class="line x" title="98:258	Polynomial vs. Linear models The polynomial kernel of degree 2 allows us to efficiently and implicitly include in our models all feature pairs." ></td>
	<td class="line x" title="99:258	Syntactic structure information as captured by pairs of POS-tags and Word-POS pairs is certainly important for such syntactic tasks as Chunking and NER, as demonstrated by the many systems described in (Sang and Buchholz, 2000; Tjong Kim Sang, 2002)." ></td>
	<td class="line x" title="100:258	By using the polynomial kernel, we can easily make use of this information without intensive feature-tuning for the most successful feature pairs." ></td>
	<td class="line x" title="101:258	L1-SVM, L2-SVM and the choice of the C parameter Throughout our experiments, we use the standard variant of SVM, L1-penalty soft margin SVM, as implemented by the TinySVM1 software package, with the default C value of 1." ></td>
	<td class="line x" title="102:258	This setting is shown to produce good results for sequence labeling tasks in previous work (Kudo and Matsumoto, 2000), and is what most end-users of SVM classifiers are likely to use." ></td>
	<td class="line x" title="103:258	As we show in Sect.5.4, fine-tuning the C parameter reaches better accuracy than L1-SVM with C=1." ></td>
	<td class="line x" title="104:258	However, as this fine-tuning is computationally expensive, we first report the comparison L1-SVM/C=1 vs. anchored-SVM, which consistently reached the best results, and was the quickest to train." ></td>
	<td class="line x" title="105:258	Feature Pruning vs. Feature Selection Our aim in this set of experiments is not to find the optimal set of lexical features, but rather to demonstrate that most lexical items are not needed for accurate classification in sequence labeling tasks." ></td>
	<td class="line x" title="106:258	To this end, we perform very crude frequency based feature pruning." ></td>
	<td class="line x" title="107:258	We believe better motivated feature selection technique taking into account linguistic (e.g. prune only open-class words) or statistic information could result in slightly more accurate models with even fewer lexical items." ></td>
	<td class="line x" title="108:258	5 Experiments and Results 5.1 Named Entity Recognition (NER) We use the Dutch data set from the CoNLL 2002 shared task (Tjong Kim Sang, 2002)." ></td>
	<td class="line x" title="109:258	The aim is to identify named entities (persons, locations, organizations and miscellaneous) in text." ></td>
	<td class="line x" title="110:258	The task has two stages: identification of the entities, and classification of the identified entities into their corresponding types." ></td>
	<td class="line x" title="111:258	We focus here on the identification task." ></td>
	<td class="line x" title="112:258	Features: We use the following properties for each of the relevant tokens: word-form, POS, ORT, prefix1, prefix2, prefix3, suffix1, suffix2, suffix3." ></td>
	<td class="line x" title="113:258	The ORT feature can take one of the following values: {number, contains-digit, containshyphen, capitalized, all-capitalized, URL, punctuation, regular}." ></td>
	<td class="line x" title="114:258	1http://chasen.org/taku/software/TinySVM/ 1145 PRUNING #FEATURES SOFT-MARGIN ANCHORED 0 186,421 90.92 90.78 100 5,804 90.73 90.75 1000 1,207 88.56 90.10 1500 821 85.92 89.29 Table 1: Named Entity Identification results (Fscore) on dev set, with various pruning thresholds." ></td>
	<td class="line x" title="115:258	Results are presented in Table 1." ></td>
	<td class="line x" title="116:258	Without feature pruning, we achieve an F-score of 90.9." ></td>
	<td class="line x" title="117:258	This dataset proved to be quite resilient to feature pruning." ></td>
	<td class="line x" title="118:258	Pruning features appearing less than 100 times results in just a slight decrease in F-score." ></td>
	<td class="line x" title="119:258	Extremely aggressive pruning, keeping only features appearing more than 1,000 or 1,500 times in the training data, results in a big drop in F-score for the soft-margin SVM (from about 91 to 86)." ></td>
	<td class="line x" title="120:258	Much less so for the Anchored-SVM." ></td>
	<td class="line x" title="121:258	Using Anchored SVM we achieve an F-score of 90.1 after pruning with k = 1,000." ></td>
	<td class="line x" title="122:258	This model has 1207 active features, and 27 unique active lexical forms." ></td>
	<td class="line x" title="123:258	5.2 NP Chunking The goal of this task (Marcus and Ramshaw, 1995) is the identification of non-recursive NPs." ></td>
	<td class="line x" title="124:258	We use the data from the CoNLL 2000 shared task: NP chunks are extracted from Sections 15-18 (train) and 20 (test) of the Penn WSJ corpus." ></td>
	<td class="line x" title="125:258	POS tagged are automatically assigned by the Brill Tagger." ></td>
	<td class="line x" title="126:258	Features: We consider the POS and word-form of each token." ></td>
	<td class="line x" title="127:258	PRUNING #FEATURES SOFT-MARGIN ANCHORED 0 92,805 94.12 94.08 1 46,527 93.78 94.09 2 32,583 93.58 94.00 5 18,092 93.42 94.01 10 10,812 93.00 93.98 20 5,952 92.48 93.92 50 2,436 92.33 93.96 100 1,168 91.94 93.83 Table 2: NP-Chunking results (F-score), with various pruning thresholds." ></td>
	<td class="line x" title="128:258	Results are presented in Table 2." ></td>
	<td class="line x" title="129:258	Without feature pruning (k = 0), the soft-margin SVM performs slightly better than the Anchored-SVM." ></td>
	<td class="line x" title="130:258	Either of the results are state-of-the-art for this task." ></td>
	<td class="line x" title="131:258	However, even modest pruning (k = 2) hurts the soft-margin model significantly." ></td>
	<td class="line x" title="132:258	Not so for the anchored-SVM." ></td>
	<td class="line x" title="133:258	Even with relatively aggressive pruning (k = 100), the anchored model still achieves an impressive F-score of 93.83." ></td>
	<td class="line x" title="134:258	Remarkably, in that last model, there are only 1,168 active features, and only 209 unique active lexical forms." ></td>
	<td class="line x" title="135:258	5.3 Chunking The goal of the Chunking task (Sang and Buchholz, 2000) is the identification of an assortment of linguistic base-phrases." ></td>
	<td class="line x" title="136:258	We use the data from the CoNLL 2000 shared task." ></td>
	<td class="line x" title="137:258	Features: We perform two experiments." ></td>
	<td class="line x" title="138:258	In the first experiment, we consider the POS and wordform of each token." ></td>
	<td class="line x" title="139:258	In this setting, feature pruning resulted in a bigger loss in performance than in the two previous tasks." ></td>
	<td class="line x" title="140:258	Preliminary error analysis revealed that many errors are due to tagger errors, especially of the present participle forms." ></td>
	<td class="line x" title="141:258	This led us to the second experiment, in which we added as features the 2and 3letter suffixes for the word to be classified (but not for the surrounding words)." ></td>
	<td class="line x" title="142:258	Results are presented in Tables 3 and 4." ></td>
	<td class="line x" title="143:258	In the first experiment (POS + Word), the non-pruned soft-margin model is the same system as the topperforming system in the original shared task, and yields state-of-the-art results." ></td>
	<td class="line x" title="144:258	Unlike the NPchunking case, here feature pruning has a relatively large impact on the results even for the anchored models." ></td>
	<td class="line x" title="145:258	However, the anchored models are still far more robust than the soft-margin ones." ></td>
	<td class="line x" title="146:258	With k = 100 pruning, the soft-margin model suffers a drop of 2.5 F points, while the anchored model suffers a drop of only 0.84 F points." ></td>
	<td class="line x" title="147:258	Even after this drop, the anchored k = 100 model still performs above the top-third system in the CoNLL 2000 shared task." ></td>
	<td class="line x" title="148:258	This anchored k = 100 model has 1,180 active features, and only 209 unique active lexical features." ></td>
	<td class="line x" title="149:258	The second experiment (POS + word-form + suffixes for main word) adds crude morphological information to the learner, helping it to avoid common tagger mistakes." ></td>
	<td class="line x" title="150:258	This additional information is helpful: pruning with k = 100 leads to an accurate anchored model (93.12 F) with only 209 unique lexical items." ></td>
	<td class="line x" title="151:258	Note that with the addition of the suffix features, the pruned model k = 20 beats the purely lexical model (no suffix features) with no pruning (93.51 vs. 93.44) with 10 times less features." ></td>
	<td class="line x" title="152:258	When we combine suffixes and all lexical forms, we still see a slight advantage to the lexical model (93.73 vs. 93.12 with pruning at k = 100)." ></td>
	<td class="line x" title="153:258	Even less lexicalization How robust are the suffixes?" ></td>
	<td class="line x" title="154:258	We performed a third experiment, in which 1146 PRUNING #FEATURES SOFT-MARGIN ANCHORED 0 92,837 93.44 93.40 1 46,557 93.20 93.32 2 32,614 93.10 93.31 5 18,126 92.89 93.29 10 10,834 92.73 93.23 20 5,975 92.18 93.16 50 2,463 91.80 92.89 100 1,180 90.94 92.56 Table 3: Chunking results (F), with various pruning thresholds." ></td>
	<td class="line x" title="155:258	Experiment 1." ></td>
	<td class="line x" title="156:258	Features: POS, Word." ></td>
	<td class="line x" title="157:258	PRUNING #FEATURES SOFT-MARGIN ANCHORED 0 104,304 93.73 93.69 1 72,228 93.56 93.68 2 57,578 93.50 93.64 5 37,210 93.35 93.62 10 23,968 93.26 93.56 20 14,060 92.84 93.51 50 6,326 92.28 93.37 100 3,340 91.83 93.12 Table 4: Chunking results (F), with various pruning thresholds." ></td>
	<td class="line x" title="158:258	Experiment 2." ></td>
	<td class="line x" title="159:258	Features: POS, Word,{Suff2, Suff3}of main Word." ></td>
	<td class="line x" title="160:258	we replaced any explicit word-forms by 2and 3letter suffixes." ></td>
	<td class="line x" title="161:258	This gives us the complete word form of many function words, and a reasonable amount of morphological marking." ></td>
	<td class="line x" title="162:258	Results are presented in Table 5." ></td>
	<td class="line x" title="163:258	Surprisingly, this information proves to be quite robust." ></td>
	<td class="line x" title="164:258	Without feature pruning, both the anchored and soft-margin model achieve near state-of-the-art performance of 93.25F." ></td>
	<td class="line x" title="165:258	Pruning with k = 100 hurts the result of the soft-margin model, but the anchored model remains robust with an F-score of 93.18." ></td>
	<td class="line x" title="166:258	This last model has 2,563 active features." ></td>
	<td class="line x" title="167:258	With further pruning (k = 250), the result of the anchored model drops to 92.87F (still 3rd place in the CoNLL shared task), with only 1,508 active features in the model." ></td>
	<td class="line x" title="168:258	5.4 Fine-tuned soft-margin SVMs For the sake of completeness, and to serve as a better comparison to the soft-margin SVM, we report results of some experiments with both L1 and L2 SVMs, with tuned C values." ></td>
	<td class="line x" title="169:258	NP-chunking performance with tuned C values and various pruning thresholds is presented in Table 6." ></td>
	<td class="line x" title="170:258	For these results, the C parameter was tuned on a development set using Brents 1-dimension minimization method (Brent, 1973)." ></td>
	<td class="line x" title="171:258	While taking about 40 hours of computation to fit, the fiPRUNING #FEATURES SOFT-MARGIN ANCHORED 0 19,910 93.25 93.23 100 2,563 92.87 93.18 250 1,508 92.40 92.87 Table 5: Chunking results (F), with various pruning thresholds." ></td>
	<td class="line x" title="172:258	Experiment 3." ></td>
	<td class="line x" title="173:258	Features: POS , Suff2, Suff3 . K L1 (C) L2 (C) ANCHORED 0 94.12(1.0001) 94.09(2.6128) 94.08 50 93.79(0.0524) 93.71(0.0082) 93.96 100 93.72(0.0567) 93.59(0.0072) 93.83 Table 6: NP-Chunking results (F), with various pruning thresholds K, for L1 and L2 SVMs with tuned C values nal results catch up with those of the anchoredSVM but still remain slightly lower." ></td>
	<td class="line x" title="174:258	This further highlights our main point: accurate models can be achieved also with mostly unlexicalized models, and the lexical features do not contribute substantial semantic information, but rather affect the separability of the data." ></td>
	<td class="line x" title="175:258	This is nicely demonstrated by SVM-anchoring, in which lexical information is practically replaced by artificial semantically void indexes, but similar performance can also be achieved by fine-tuning other learning parameters." ></td>
	<td class="line x" title="176:258	6 Error Analysis Our experiments so far indicate that very aggressive feature pruning hurts performance slightly (by about 0.5F point)." ></td>
	<td class="line x" title="177:258	The feature-pruned models are still accurate, indicating that lexical features contribute little to the classification accuracy." ></td>
	<td class="line x" title="178:258	We now investigate the differences between the lexicalized and pruned models, in order to characterize the kind of information that is available to the lexicalized models but missing from the pruned ones." ></td>
	<td class="line x" title="179:258	In the next section, we also verify that prunedmodels are more stable than the fully lexicalized ones when tested over different text genres and domains." ></td>
	<td class="line x" title="180:258	We focus our analysis on the chunking task, which is a superset of the NP-chunking task." ></td>
	<td class="line x" title="181:258	We compare the fully lexicalized soft-margin SVM model with the POS+suffix2+suffix3 anchored-SVM model with k = 100 pruning." ></td>
	<td class="line x" title="182:258	We analyze the models respective performance on section 05 of the WSJ corpus." ></td>
	<td class="line x" title="183:258	This dataset is different than the official test set." ></td>
	<td class="line x" title="184:258	It is, however, part of the same annotated corpus as both the training and test sets." ></td>
	<td class="line x" title="185:258	On this dataset, the fully lexicalized SVM model 1147 achieves an F-score of 93.24, vs. 92.59 for the suffix-based pruned anchored-SVM model." ></td>
	<td class="line x" title="186:258	(The pruned anchored-SVM model (k = 100) from experiment 2, achieve a slightly higher F-score of 92.84) We investigate only those chunks which are identified correctly by one model but not by the other." ></td>
	<td class="line x" title="187:258	Overall, there are 440 chunks (363 unique) which are identified correctly only by the lexicalized model, and 258 chunks (232 unique) only by the pruned model." ></td>
	<td class="line x" title="188:258	Where the pruned model is always wrong Some errors are unique to the pruned model." ></td>
	<td class="line x" title="189:258	Over 45 of the cases that are identified correctly only in the lexicalized model (more than 10%) are due to the words including (18 cases) and If (9 cases), as well as other -ing forms such as following, according, rising and suspecting. The word including appears 80 times in the training data, always tagged as VBG and functioning as a PP chunk, which is an odd chunk for VBGs." ></td>
	<td class="line x" title="190:258	The lexicalized model easily picked up on this behaviour, while the pruned model couldnt. Similarly, the word following/VBG appears 32 times, 20 of which as PP, and the word according/VBG 53 times, all of them as PP." ></td>
	<td class="line x" title="191:258	The pruned model could not distinguish those from the rest of the VBGs and tagged them as VPs." ></td>
	<td class="line x" title="192:258	What seems to happen in these cases, is that certain verbal forms participate in idiomatic constructions and behave syntactically as prepositions." ></td>
	<td class="line x" title="193:258	The POS tagger does not pick this ambiguity in function and contributes only the most likely tag for the words (VBG)." ></td>
	<td class="line x" title="194:258	Lexical models learn that certain VBGs are becoming prepositions in the observed dataset." ></td>
	<td class="line x" title="195:258	These words do not appear as specific features in the pruned models, and hence these usage shifts are often misclassified." ></td>
	<td class="line x" title="196:258	Interestingly, the pruned model did learn that verbal forms can sometimes be PPs: it made use of that information by mis-identifying 11 verbal VBGs and 6 verbal VBNs as PPs." ></td>
	<td class="line x" title="197:258	The word If/IN, unlike most prepositions, it always starts an SBAR rather than a PP chunk in the corpus." ></td>
	<td class="line x" title="198:258	The pruned model learned this behaviour correctly for the lower-cased if/IN, but missed the upper-cased version appearing in 79 sentence initial locations in the corpus." ></td>
	<td class="line x" title="199:258	These cases are caused by a mismatch between the POS tag and the syntactic function observed in the chunked dataset." ></td>
	<td class="line x" title="200:258	Additional cases include the adverbs (Already, Nearby, Soon, Maybe, Perhaps, once, Then): they are sometimes not chunked as ADVP but are left outside of any chunk." ></td>
	<td class="line x" title="201:258	Some one-word ADJP chunks being chunked as NPs (short, general, sure, worse, . . ." ></td>
	<td class="line x" title="202:258	) (6 cases) and some are chunked as ADVPs (hard, British-born, . . ." ></td>
	<td class="line x" title="203:258	) (4 cases)." ></td>
	<td class="line x" title="204:258	There are 10 cases where the pruned model splits an NP into ADVP and NP, such as: [later][thisweek] , [roughly][18moreU.S.stores] . In addition, the pruned model failed to learn the construction typical of, resulting in 2 NP chunks such as: [Themoreintuitive approachtypical] . Some mistakes of the pruned model seem like mistakes/pecularities of the annotated corpus, which the lexicalized model found a way to work around." ></td>
	<td class="line x" title="205:258	Consider the following gold-standard cases from the annotated corpus: [ VPseems] [ ADVPrarely] [ VPtocut][ ADVPjust] [ PPafter] [ VPis] [ NPanything] [ Obut] [ VPfixing][ ADJPashigh] [ PPas] [ NP8.3%] [ ADJPless] [ PPthan] [ ADJProsy][ NP40%] [ PPto] [ NP45%] Which were each identified as a single chunk by the pruned model." ></td>
	<td class="line x" title="206:258	It can be argued these are mistakes in the tagged dataset." ></td>
	<td class="line x" title="207:258	Where the lexical model is sometimes better Both models fail on conjunctions, but the lexicalized model do slightly better." ></td>
	<td class="line x" title="208:258	Conjunction error types come in two main varieties, either chunking [x][and][y] instead of [x and y] (pruned: 21 cases, lex: 14 cases) or chunking [x and y] instead of [x][and][y] (pruned: 26 cases, lex: 24 cases)." ></td>
	<td class="line x" title="209:258	Joining VP and NP into an NP, due to a verb/adj ambiguity." ></td>
	<td class="line x" title="210:258	For example chunking [NPfiredsixexecutives] instead of [VPfired][NPsixexecutives] , or [NPkeepingviewers] instead of [VPkeeping][NPviewers] . 12 such cases are resolved correctly only by the lexicalized model, and 5 only by the pruned one." ></td>
	<td class="line x" title="211:258	SBAR/PP confusion for words such as: as,after,with,since (both ways)." ></td>
	<td class="line x" title="212:258	13 cases for the pruned model, 6 for lexicalized one." ></td>
	<td class="line x" title="213:258	Where both model are similar Merging back-to-back NPs: Both models tend to erroneously join back-to-back NPs to a single NP, e.g. : [NPWestinghousethisyear] , or [NPthemselves fashionenterprises] . No model is better than the other on these cases, each model failed 1148 on 16 cases the other model succeeded on." ></td>
	<td class="line x" title="214:258	Joining NP and VP into an NP due to Verb/Noun ambiguity and tagger mistakes: [NPtheweekend][VPmaking] [NPtheweekendmaking][NPthecompetition][VPcan] [NPthecompetitioncan] (lexicalized: 6 errors, pruned: 8 errors) And splitting some NPs to VP+NP due to the same reasons: [VPoperating][NPprofit][VPimproved][NPaverageyield] (lexicalized: 5 errors, pruned: 7 errors) The word that is confused between SBAR and NP (5 mistakes for each model) Erroneously splitting range NPs, e.g. : [about$115][to][$125] (2 cases for each model)." ></td>
	<td class="line x" title="215:258	Where the pruned model is better There are some cases where the pruned models is doing better than the lexicalized one: VP wrongly split into VP and ADJP: [remains][banned] 4 mistakes for lexicalized, 1 for pruned VP wrongly split into VP and VP: [werescheduled][tomeet][used][tocomplain] 3 mistakes for lexicalized, 1 for pruned VP wrongly split into ADVP and VP: [largly][reflecting][selectively][leaking] 6 mistakes for lexicalized, 1 for pruned PP and SBAR confusion: of, with, As, after 9 mistakes for lex, 5 for pruned VP chunked as NP due to tagger mistake: [NPruling],[NPdrives],[NPcuts] 6 mistakes for lex, 2 for pruned that tagged as NP instead of SBAR: 2 mistakes for lex, 0 for pruned To conclude Both the pruned and the fully lexicalized models have problems dealing with non-local phenomena such as coordination and relative clauses, as well as verb/adjective ambiguities and VBG/Noun ambiguities." ></td>
	<td class="line x" title="216:258	They also perform poorly on embeded syntactic constructions (such as an NP containing an ADJP), and on identification of back-to-back NPs, which often requires semantic knowledge." ></td>
	<td class="line x" title="217:258	Both models suffer from tagging mistakes of the underlying tagger and systematic ambiguity between the morphological tag assigned by the tagger and the syntactic tag in which the word operates (e.g., including used as a preposition)." ></td>
	<td class="line x" title="218:258	The main advantage of the fully lexcialized model is in dealing with:  Some coordinated constructions." ></td>
	<td class="line x" title="219:258	 Some cases of verb/adjective ambiguities." ></td>
	<td class="line x" title="220:258	 Specific function words not seen much in training." ></td>
	<td class="line x" title="221:258	 Idiomatic usages of some VBG/VBN forms functioning as prepositions." ></td>
	<td class="line x" title="222:258	The first two items are semantic in nature, and hint that lexical features do capture some semantic information." ></td>
	<td class="line x" title="223:258	While this might be true on the specific corpus, we believe that such corpus-derived semantic knowledge is very restricted, is not generalizable, and will not transfer well to other corpora, even on the same genre." ></td>
	<td class="line x" title="224:258	We provide evidence for this claim in Section 7." ></td>
	<td class="line x" title="225:258	The last two items are syntactic." ></td>
	<td class="line x" title="226:258	We address them by introducing a slightly modified feature model." ></td>
	<td class="line x" title="227:258	6.1 Another chunking Experiment Based on the observations from the error analysis, we performed another pruned-chunking experiment, with the following features:  Word and POS for a -2,+2 window around the current token, and 2-and-3-letter suffixes of the token to be classified (same as Experiment 2 in Section 5.2 above)." ></td>
	<td class="line x" title="228:258	 Features of words appearing as a preposition (IN) anywhere in the training set are not pruned (this result in a model with 310 unique lexical items after k = 100 pruning)." ></td>
	<td class="line x" title="229:258	 An additional binary feature indicating for each token whether it can function as a PP." ></td>
	<td class="line x" title="230:258	The list of possible-PP forms is generated by considering all tokens seen inside a PP in the training corpus." ></td>
	<td class="line x" title="231:258	It can be easily extended if additional lexicographic resources are available, without retraining the model." ></td>
	<td class="line x" title="232:258	This last proposed feature incorporates important lexical knowledge without relying on features for specific lexical forms, and is more generalizable." ></td>
	<td class="line x" title="233:258	The accuracy of this new model on the development and test set with various pruning thresholds is presented in Table 7." ></td>
	<td class="line x" title="234:258	The addition of the CanBePrep feature improves the fully-lexicalized model accuracy on the development set (93.24 to 93.68), and does not affect fully lexicalized result on the test set (93.71 1149 CORPUS SOURCE CONTENT #TOKENS WSJ 4articlesfromwsj.combusiness Magazine,business 2,671 Jaguar WikipediapageonJaguar Welleditedtext,animals 5,396 FreeWill WikipediapageonFreeWill Welleditedtext,philosophy 9,428 LJ-Life 4LiveJournalposts Noisyteenagewriting,life 870 Table 8: Corpus Variation Text Sources PRUNING #FEATURES SOFT-MARGIN ANCHOREDDevSet 0 92,989 93.71 100 4,066  93.22 TestSet0 92,989 93.68  100 4,066  93.26 Table 7: Chunking results (F), with various pruning thresholds." ></td>
	<td class="line x" title="235:258	Experiment 4." ></td>
	<td class="line x" title="236:258	Features: POS, Word , Suff2, Suff3 for main word, CanBePrep . vs. 93.73)." ></td>
	<td class="line x" title="237:258	The pruned model performance improves in both cases, more so on the development set (93.12 to 93.22 on the test set, 92.84 to 93.26 on the development set)." ></td>
	<td class="line x" title="238:258	The new model helps bridging the gap between the fully lexicalized and the pruned model, yet we still observe a lead of 0.4F for the fully lexicalized model." ></td>
	<td class="line x" title="239:258	We now turn to explore how meaningful this difference is in real-world situation in which one does not operate on the Penn-WSJ corpus." ></td>
	<td class="line x" title="240:258	7 Corpus Variation and Model Performance When tested on the exact same resource as the models are trained on, the fully lexicalized model still has a slight edge over the pruned ones." ></td>
	<td class="line x" title="241:258	How well does this lexical knowledge transfer to different text genres?" ></td>
	<td class="line x" title="242:258	We compare the models performance on text from various genres, ranging from very similar to the training material (recent articles from the WSJ Business section) to a well-edited but different domain text (Featuredcontent wikipedia pages) to a non-edited noisy text (live-journal blog posts from the life category)." ></td>
	<td class="line x" title="243:258	As we do not have gold-annotated data for these text genres, we analyze the few differences between the models, manually inspecting the instances on which the models disagree." ></td>
	<td class="line x" title="244:258	Table 8 describes our test corpora for this experiment." ></td>
	<td class="line x" title="245:258	We applied the fully-lexicalized and the pruned (k = 100) anchored models described in Section 6.1 to these texts, and compared the chunking results." ></td>
	<td class="line x" title="246:258	The results are presented in Table 9." ></td>
	<td class="line x" title="247:258	When moving outside of the canonic training TEXT #DIFF PRUNED LEX BOTHC ORRECT CORRECT WRONGWSJ 13 9 4 0 Jaguar 45 20 20 7FreeWill 118 51 38 29 LJ-Life 15 8 6 1 Table 9: Comparison of Models performance on different text genres corpus, the fully lexicalized model have no advantage over the heavily pruned one." ></td>
	<td class="line x" title="248:258	On the contrary, the pruned models seem to have a small advantage in most cases (though it is hard to tell if the differences are significant)." ></td>
	<td class="line x" title="249:258	This is true even for texts in the very same domain, genre and editing guidelines as the training corpus was derived from." ></td>
	<td class="line x" title="250:258	8 Discussion For all the sequence labeling tasks we analyzed, the anchored-SVM proved to be robust to feature pruning." ></td>
	<td class="line x" title="251:258	The experiments support the claim that rare lexical features do not provide substantial information to the model, but instead play a role in maintaining separability." ></td>
	<td class="line x" title="252:258	When this role is taken over by anchoring, we can obtain the same level of performance with very few robust lexical features." ></td>
	<td class="line x" title="253:258	Yet, we cannot conclude that lexical information is not needed." ></td>
	<td class="line x" title="254:258	There is a significant difference between the pruned and non-pruned models for the chunking task." ></td>
	<td class="line x" title="255:258	We showed that this difference can be bridged to some extent by a binary feature relating to idiomatic word usage, and that the difference vanishes when testing outside of the annotated corpus." ></td>
	<td class="line x" title="256:258	The high classification accuracies achieved with the heavily pruned anchoredSVM models sheds new light on the actual role of lexical features, and indicating that there is still a lot to be learned regarding the effective incorporation of lexical and semantic information into our models." ></td>
	<td class="line x" title="257:258	It is our view that semantic knowledge should not be expected to be learned by inspection of raw lexical counts from an annotated text corpus, but instead collected from sources external to the annotated corpora  either based on a very large unannotated corpora, or on manually constructed lexical resources." ></td>
	<td class="line x" title="258:258	1150" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1120
Simple Coreference Resolution with Rich Syntactic and Semantic Features
Haghighi, Aria;Klein, Dan;"></td>
	<td class="line x" title="1:207	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 11521161, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:207	c 2009 ACL and AFNLP Simple Coreference Resolution with Rich Syntactic and Semantic Features Aria Haghighi and Dan Klein Computer Science Division UC Berkeley {aria42, klein}@cs.berkeley.edu Abstract Coreference systems are driven by syntactic, semantic, and discourse constraints." ></td>
	<td class="line x" title="3:207	We present a simple approach which completely modularizes these three aspects." ></td>
	<td class="line x" title="4:207	In contrast to much current work, which focuses on learning and on the discourse component, our system is deterministic and is driven entirely by syntactic and semantic compatibility as learned from a large, unlabeled corpus." ></td>
	<td class="line x" title="5:207	Despite its simplicity and discourse naivete, our system substantially outperforms all unsupervised systems and most supervised ones." ></td>
	<td class="line x" title="6:207	Primary contributions include (1) the presentation of a simpleto-reproduce, high-performing baseline and (2) the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon (and perhaps best addressed by non-coreference systems)." ></td>
	<td class="line x" title="7:207	1 Introduction The resolution of entity reference is influenced by a variety of constraints." ></td>
	<td class="line x" title="8:207	Syntactic constraints like the binding theory, the i-within-i filter, and appositive constructions restrict reference by configuration." ></td>
	<td class="line x" title="9:207	Semanticconstraintslikeselectionalcompatibility (e.g. a spokesperson can announce things) and subsumption (e.g. Microsoft is a company) rule out many possible referents." ></td>
	<td class="line x" title="10:207	Finally, discourse phenomena such as salience and centering theory are assumed to heavily influence reference preferences." ></td>
	<td class="line x" title="11:207	As these varied factors have given rise to a multitude of weak features, recent work has focused on how best to learn to combine them using models over reference structures (Culotta et al., 2007; Denis and Baldridge, 2007; Klenner and Ailloud, 2007)." ></td>
	<td class="line x" title="12:207	In this work, we break from the standard view." ></td>
	<td class="line x" title="13:207	Instead,weconsideravastlymoremodularsystem inwhich coreferenceis predictedfrom adeterministic function of a few rich features." ></td>
	<td class="line x" title="14:207	In particular, we assume a three-step process." ></td>
	<td class="line x" title="15:207	First, a selfcontained syntactic module carefully represents syntacticstructuresusinganaugmentedparserand extracts syntactic paths from mentions to potential antecedents." ></td>
	<td class="line x" title="16:207	Some of these paths can be ruled in or out by deterministic but conservative syntactic constraints." ></td>
	<td class="line x" title="17:207	Importantly, the bulk of the work in the syntactic module is in making sure the parses are correctly constructed and used, and this modules most important training data is a treebank." ></td>
	<td class="line x" title="18:207	Second, a self-contained semantic module evaluates the semantic compatibility of headwords and individual names." ></td>
	<td class="line x" title="19:207	These decisions are made from compatibility lists extracted from unlabeled data sources such as newswire and web data." ></td>
	<td class="line x" title="20:207	Finally, of the antecedents which remain after rich syntactic and semantic filtering, reference is chosen to minimize tree distance." ></td>
	<td class="line x" title="21:207	Thisprocedureistrivialwheremostsystemsare rich, and so does not need any supervised coreference data." ></td>
	<td class="line x" title="22:207	However, it is rich in important ways which we argue are marginalized in recent coreference work." ></td>
	<td class="line x" title="23:207	Interestingly, error analysis from our final system shows that its failures are far more often due to syntactic failures (e.g. parsing mistakes) and semantic failures (e.g. missing knowledge) than failure to model discourse phenomena or appropriately weigh conflicting evidence." ></td>
	<td class="line x" title="24:207	One contribution of this paper is the exploration of strong modularity, including the result that our system beats all unsupervised systems and approaches the state of the art in supervised ones." ></td>
	<td class="line x" title="25:207	Another contribution is the error analysis result that, even with substantial syntactic and semantic richness, the path to greatest improvement appears to be to further improve the syntactic and semantic modules." ></td>
	<td class="line x" title="26:207	Finally, we offer our approach as a very strong, yet easy to implement, baseline." ></td>
	<td class="line x" title="27:207	We make no claim that learning to reconcile disparate features in a joint model offers no benefit, only that it must not be pursued to the exclusion of rich, nonreference analysis." ></td>
	<td class="line x" title="28:207	2 Coreference Resolution In coreference resolution, we are given a document which consists of a set of mentions; each 1152 mention is a phrase in the document (typically an NP) and we are asked to cluster mentions according to the underlying referent entity." ></td>
	<td class="line x" title="29:207	There are three basic mention types: proper (Barack Obama), nominal (president), and pronominal (he).1 For comparison to previous work, we evaluate in the setting where mention boundaries are given at test time; however our system can easily annotate reference on all noun phrase nodes in a parse tree (see Section 3.1.1)." ></td>
	<td class="line x" title="30:207	2.1 Data Sets In this work we use the following data sets: Development: (see Section 3)  ACE2004-ROTH-DEV: Dev set split of the ACE 2004 training set utilized in Bengston and Roth (2008)." ></td>
	<td class="line x" title="31:207	The ACE data also annotates pre-nominal mentions which we map onto nominals." ></td>
	<td class="line x" title="32:207	68 documents and 4,536 mentions." ></td>
	<td class="line x" title="33:207	Testing: (see Section 4)  ACE2004-CULOTTA-TEST: Test set split of the ACE 2004 training set utilized in Culotta et al.(2007) and Bengston and Roth (2008)." ></td>
	<td class="line x" title="35:207	Consists of 107 documents.2  ACE2004-NWIRE: ACE 2004 Newswire set to compare against Poon and Domingos (2008)." ></td>
	<td class="line x" title="36:207	Consists of 128 documents and 11,413 mentions; intersects with the other ACE data sets." ></td>
	<td class="line x" title="37:207	 MUC-6-TEST: MUC6 formal evaluation set consisting of 30 documents and 2,068 mentions." ></td>
	<td class="line x" title="38:207	Unlabeled: (see Section 3.2)  BLIPP: 1.8 million sentences of newswire parsed with the Charniak (2000) parser." ></td>
	<td class="line x" title="39:207	No labeled coreference data; used for mining semantic information." ></td>
	<td class="line x" title="40:207	 WIKI: 25k articles of English Wikipedia abstracts parsed by the Klein and Manning (2003) parser.3 No labeled coreference data; used for mining semantic information." ></td>
	<td class="line x" title="41:207	1Othermentiontypesexistandareannotated(suchasprenominal), which are treated as nominals in this work." ></td>
	<td class="line x" title="42:207	2The evaluation set was not made available to nonparticipants." ></td>
	<td class="line x" title="43:207	3Wikipediaabstractsconsistofroughlythefirstparagraph of the corresponding article 2.2 Evaluation We will present evaluations on multiple coreference resolution metrics, as no single one is clearly superior:  Pairwise F1: precision, recall, and F1 over all pairs of mentions in the same entity cluster." ></td>
	<td class="line x" title="44:207	Note that this over-penalizes the merger or separation of clusters quadratically in the size of the cluster." ></td>
	<td class="line x" title="45:207	 b3 (Amit and Baldwin, 1998): For each mention, form the intersection between the predicted cluster and the true cluster for that mention." ></td>
	<td class="line x" title="46:207	The precision is the ratio of the intersection and the true cluster sizes and recall the ratio of the intersection to the predicted sizes; F1 is given by the harmonic mean over precision and recall from all mentions." ></td>
	<td class="line x" title="47:207	 MUC(Vilainetal., 1995): Foreachtruecluster, compute the number of predicted clusters which need to be merged to cover the true cluster." ></td>
	<td class="line x" title="48:207	Divide this quantity by true cluster size minus one." ></td>
	<td class="line x" title="49:207	Recall is given by the same procedure with predicated and true clusters reversed.4  CEAF (Luo, 2005): For a similarity function between predicted and true clusters, CEAF scores the best match between true and predicted clusters using this function." ></td>
	<td class="line x" title="50:207	We use the 3 similarity function from Luo (2005)." ></td>
	<td class="line x" title="51:207	3 System Description In this section we develop our system and report developmental results on ACE2004-ROTHDEV (see Section 2.1); we report pairwise F1 figures here, but report on many more evaluation metrics in Section 4." ></td>
	<td class="line x" title="52:207	At a high level, our system resembles a pairwise coreference model (Soon et al., 1999; Ng and Cardie, 2002; Bengston and Roth, 2008); for each mention mi, we select either a single-best antecedent amongst the previous mentions m1,,mi1, or the NULL mention to indicate the underlying entity has not yet been evoked." ></td>
	<td class="line x" title="53:207	Mentions are linearly ordered according to the position of the mention head with ties being broken by the larger node coming first." ></td>
	<td class="line x" title="54:207	4The MUC measure is problematic when the system predicts many more clusters than actually exist (Luo, 2005; Finkel and Manning, 2008); also, singleton clusters do not contribute to evaluation." ></td>
	<td class="line x" title="55:207	1153 While much research (Ng and Cardie, 2002; Culotta et al., 2007; Haghighi and Klein, 2007; Poon and Domingos, 2008; Finkel and Manning, 2008) has explored how to reconcile pairwise decisions to form coherent clusters, we simply take the transitive closure of our pairwise decision (as in Ng and Cardie (2002) and Bengston and Roth (2008)) which can and does cause system errors." ></td>
	<td class="line x" title="56:207	In contrast to most recent research, our pairwise decisions are not made with a learned model which outputs a probability or confidence, but insteadforeachmentionmi,weselectanantecedent amongst m1,,mi1 or the NULL mention as follows:  Syntactic Constraint: Based on syntactic configurations, either force or disallow coreference between the mention and an antecedent." ></td>
	<td class="line x" title="57:207	Propagate this constraint (see Figure 4)." ></td>
	<td class="line x" title="58:207	 Semantic/Syntactic Filter: Filter the remaining possible antecedents based upon compatibility with the mention (see Figure 2)." ></td>
	<td class="line x" title="59:207	 Selection: Select the closest mention from thesetofremainingpossibleantecedents(see Figure 1) or the NULL antecedent if empty." ></td>
	<td class="line x" title="60:207	Initially, there is no syntactic constraint (improved in Section 3.1.3), the antecedent compatibility filter allows proper and nominal mentions to corefer only with mentions that have the same head (improved in Section 3.2), and pronounshavenocompatibilityconstraints(improved in Section 3.1.2)." ></td>
	<td class="line x" title="61:207	Mention heads are determined by parsing the given mention span with the Stanford parser (Klein and Manning, 2003) and using the Collins head rules (Collins, 1999); Poon and Domingos (2008) showed that using syntactic heads strongly outperformed a simple rightmost headword rule." ></td>
	<td class="line x" title="62:207	The mention type is determined by the head POS tag: proper if the head tag is NNP or NNPS,pronouniftheheadtagis PRP, PRP$, WP, or WP$, and nominal otherwise." ></td>
	<td class="line x" title="63:207	For the selection phase, we order mentions m1,,mi1 according to the position of the head word and select the closest mention that remains after constraint and filtering are applied." ></td>
	<td class="line x" title="64:207	This choice reflects the intuition of Grosz et al.(1995) that speakers only use pronominal mentions when there are not intervening compatible S a104 a104 a104 a104 a104 a104 a104 a40 a40 a40 a40 a40 a40 a40 NP#1 a97 a97 a97 a33 a33 a33 NP NNP Nintendo PP a81 a81 a17 a17 IN of NP#2 NNP America VP a80 a80 a80 a80 a16 a16 a16 a16 VBD announced NP#3 a72 a72 a72 a8 a8 a8 NP#1 PRP$ its NP a81 a81 a17 a17 JJ new NN console Figure1: Examplesentencewhereclosesttreedistancebetweenmentionsoutperformsrawdistance." ></td>
	<td class="line x" title="66:207	For clarity, each mention NP is labeled with the underlying entity id. mentions." ></td>
	<td class="line x" title="67:207	This system yields a rather low 48.9 pairwise F1 (see BASE-FLAT in Table 2)." ></td>
	<td class="line x" title="68:207	There are many, primarily recall, errors made choosing antecedents for all mention types which we willaddressbyaddingsyntacticandsemanticconstraints." ></td>
	<td class="line x" title="69:207	3.1 Adding Syntactic Information In this section, we enrich the syntactic representation and information in our system to improve results." ></td>
	<td class="line x" title="70:207	3.1.1 Syntactic Salience We first focus on fixing the pronoun antecedent choices." ></td>
	<td class="line x" title="71:207	A common error arose from the use of mention head distance as a poor proxy for discourse salience." ></td>
	<td class="line x" title="72:207	For instance consider the example in Figure 1, the mention America is closest to its in flat mention distance, but syntactically Nintendo of America holds a more prominent syntactic position relative to the pronoun which, as Hobbs (1977) argues, is key to discourse salience." ></td>
	<td class="line x" title="73:207	MappingMentionstoParseNodes: Inorderto usethesyntacticpositionofmentionstodetermine anaphoricity, we must associate each mention in the document with a parse tree node." ></td>
	<td class="line x" title="74:207	We parse all document sentences with the Stanford parser, and then for each evaluation mention, we find the largest-span NP which has the previously determined mention head as its head.5 Often, this results in a different, typically larger, mention span than annotated in the data." ></td>
	<td class="line x" title="75:207	Now that each mention is situated in a parse tree, we utilize the length of the shortest tree path between mentions as our notion of distance." ></td>
	<td class="line x" title="76:207	In 5If there is no NP headed by a given mention head, we add an NP over just that word." ></td>
	<td class="line x" title="77:207	1154 S a104a104 a104a104 a104a104 a104a104 a40a40 a40a40 a40a40 a40a40 NP-ORG#1 a72 a72 a72 a8 a8 a8 TheIsraelis VP a104a104 a104a104 a104a104 a104a104 a104a104 a104 a33 a33 a33 a33 a40a40 a40a40 a40a40 a40a40 a40a40 a40 VBP regard NP#2 a72 a72 a72 a8 a8 a8 NP thesite PP a81 a81 a17 a17 IN as NP#2 a81 a81 a17 a17 ashrine SBAR a96 a96 a96 a96 a96a96a1 a32 a32 a32 a32 a32a32 IN because PP a90 a90 a26 a26 TO to NP#1 PRP them S a72 a72 a72 a8 a8 a8 itissacred Figure 2: Example of a coreference decision fixed by agreement constraints (see Section 3.1.2)." ></td>
	<td class="line x" title="78:207	The pronounthemisclosesttothesitemention, buthas an incompatible number feature with it." ></td>
	<td class="line x" title="79:207	The closest (in tree distance, see Section 3.1.1) compatible mention is The Israelis, which is correct particular, this fixes examples such as those in Figure 1 where the true antecedent has many embedded mentions between itself and the pronoun." ></td>
	<td class="line x" title="80:207	This change by itself yields 51.7 pairwise F1 (see BASE-TREE inTable2), whichissmalloverall, but reduces pairwise pronoun antecedent selection error from 51.3% to 42.5%." ></td>
	<td class="line x" title="81:207	3.1.2 Agreement Constraints We now refine our compatibility filtering to incorporate simple agreement constraints between coreferent mentions." ></td>
	<td class="line x" title="82:207	Since we currently allow proper and nominal mentions to corefer only with matching head mentions, agreement is only a concern for pronouns." ></td>
	<td class="line x" title="83:207	Traditional linguistic theory stipulates that coreferent mentions must agree in number, person, gender, and entity type (e.g. animacy)." ></td>
	<td class="line x" title="84:207	Here, we implement person, number and entity type agreement.6 A number feature is assigned to each mention deterministically based on the head and its POS tag." ></td>
	<td class="line x" title="85:207	For entity type, we use NER labels." ></td>
	<td class="line x" title="86:207	Ideally, we would like to have information about the entity type of each referential NP, however this information is not easily obtainable." ></td>
	<td class="line oc" title="87:207	Instead, we opt to utilize the Stanford NER tagger (Finkel et al., 2005) over the sentences in a document and annotate each NP with the NER label assigned to that mention head." ></td>
	<td class="line x" title="88:207	For each mention, when its NP is assigned an NER label we allow it to only be compatible with that NER label.7 For pronouns, we deterministically assign a set of compatible NER values (e.g. personal pronouns can only be a PER6Gender agreement, while important for general coreference resolution, did not contribute to the errors in our largely newswire data sets." ></td>
	<td class="line x" title="89:207	7Or allow it to be compatible with all NER labels if the NER tagger doesnt predict a label." ></td>
	<td class="line x" title="90:207	gore president florida state bush governor lebanese territory nation people arafat leader inc. company aol company nation country assad president Table 1: Most common recall (missed-link) errors amongst non-pronoun mention heads on our development set." ></td>
	<td class="line x" title="91:207	Detecting compatibility requires semantic knowledge which we obtain from a large corpus (see Section 3.2)." ></td>
	<td class="line x" title="92:207	Sa96a96a96a96a32a32a32a32 NP#1 NNP Wal-Mart VPa104a104a104a104a40a40a40a40 VBZ says Sa104a104a104a104a40a40a40a40 NP#2a88a88a88a24a24a24 NP NNP Gitano , , NP-APPOS#2a80a80a16a16 NP#1 PRP its JJ top NNS brand VPa80a80a16a16 is underselling Figure 4: Example of interaction between the appositive and i-within-i constraint." ></td>
	<td class="line x" title="93:207	The i-withini constraint disallows coreference between parent and child NPs unless the child is an appositive." ></td>
	<td class="line x" title="94:207	Hashed numbers indicate ground truth but are not in the actual trees." ></td>
	<td class="line x" title="95:207	SON, but its can be an ORGANIZATION or LOCATION)." ></td>
	<td class="line x" title="96:207	Since the NER tagger typically does not label non-proper NP heads, we have no NER compatibility information for nominals." ></td>
	<td class="line x" title="97:207	We incorporate agreement constraints by filtering the set of possible antecedents to those which have compatible number and NER types with the target mention." ></td>
	<td class="line x" title="98:207	This yields 53.4 pairwise F1, and reduces pronoun antecedent errors to 42.5% from 34.4%." ></td>
	<td class="line x" title="99:207	An example of the type of error fixed by these agreement constraints is given by Figure 2." ></td>
	<td class="line x" title="100:207	3.1.3 Syntactic Configuration Constraints Our system has so far focused only on improving pronounanaphoraresolution." ></td>
	<td class="line x" title="101:207	However, aplurality of the errors made by our system are amongst nonpronominal mentions.8 We take the approach that in order to align a non-pronominal mention to an antecedent without an identical head, we require evidence that the mentions are compatible." ></td>
	<td class="line x" title="102:207	Judging compatibility of mentions generally requires semantic knowledge, to which we return later." ></td>
	<td class="line x" title="103:207	However, some syntactic configurations 8There are over twice as many nominal mentions in our development data as pronouns." ></td>
	<td class="line x" title="104:207	1155 NP#1 a104 a104 a104 a104 a104 a104 a104a104 a40 a40 a40 a40 a40 a40 a40a40 NP a80 a80 a80 a80 a16 a16 a16 a16 NN#1 painter NNP Pablo NNP Picasso , , NP#1 a96 a96 a96 a96 a96 a96 a32 a32 a32 a32 a32 a32 subjectofthe[exhibition] 2 NP-PERS#1 a104 a104 a104 a104 a104 a104 a104 a104 a74 a74 a40 a40 a40 a40 a40 a40 a40 a40 NP a80 a80 a80 a80 a16 a16 a16 a16 NP-APPOS#1 NN painter NP-PERS a98 a98 a34 a34 NNP Pablo NNP Picasso , , NP-APPOS#1 a96 a96 a96 a96 a96 a96 a32 a32 a32 a32 a32 a32 subjectofthe[exhibition] 2 (a) (b) Figure 3: NP structure annotation: In (a) we have the raw parse from the Klein and Manning (2003) parser with the mentions annotated by entity." ></td>
	<td class="line x" title="105:207	In (b), we demonstrate the annotation we have added." ></td>
	<td class="line x" title="106:207	NER labels are added to all NP according to the NER label given to the head (see Section 3.1.1)." ></td>
	<td class="line x" title="107:207	Appositive NPs are also annotated." ></td>
	<td class="line x" title="108:207	Hashes indicate forced coreferent nodes guarantee coreference." ></td>
	<td class="line x" title="109:207	The one exploited most in coreference work (Soon et al., 1999; Ng and Cardie,2002; Luoetal.,2004; Culottaetal.,2007; Poon and Domingos, 2008; Bengston and Roth, 2008)istheappositiveconstruction." ></td>
	<td class="line x" title="110:207	Here, werepresent apposition as a syntactic feature of an NP indicating that it is coreferent with its parent NP (e.g. it is an exception to the i-within-i constraint that parent and child NPs cannot be coreferent)." ></td>
	<td class="line x" title="111:207	We deterministically mark a node as NP-APPOS (see Figure 3) when it is the third child in of a parent NP whose expansion begins with (NP , NP), and there is not a conjunction in the expansion (to avoid marking elements in a list as appositive)." ></td>
	<td class="line x" title="112:207	Role Appositives: During development, we discovered many errors which involved a variant of appositives which we call role appositives (see painter in Figure 3), where an NP modifying the head NP describes the role of that entity (typicallyapersonentity)." ></td>
	<td class="line x" title="113:207	Thereareseveralchallenges to correctly labeling these role NPs as being appositives." ></td>
	<td class="line x" title="114:207	First, the NPs produced by Treebank parsers are flat and do not have the required internal structure (see Figure 3(a))." ></td>
	<td class="line x" title="115:207	While fully solving this problem is difficult, we can heuristically fix many instances of the problem by placing an NP around maximum length sequences of NNP tags or NN (and JJ) tags within an NP; note that this will fail for many constructions such as U.S. President Barack Obama, which is analyzed as a flat sequence of proper nouns." ></td>
	<td class="line x" title="116:207	Once this internal NP structure has been added, whether the NP immediately to the left of the head NP is an appositive depends on the entity type." ></td>
	<td class="line x" title="117:207	For instance, Rabbi Ashi is an apposition but Iranian army is not." ></td>
	<td class="line x" title="118:207	Again, a full solution would require its own model, here we mark as appositions any NPs immediately to the left of a head child NP where the head child NP is identified as a person by the NER tagger.9 We incorporate NP appositive annotation as a constraint during filtering." ></td>
	<td class="line x" title="119:207	Any mention which corresponds to an appositive node has its set of possible antecedents limited to its parent." ></td>
	<td class="line x" title="120:207	Along with the appositive constraint, we implement the i-within-i constraint that any non-appositive NP cannot be be coreferent with its parent; this constraint is then propagated to any node its parent is forced to agree with." ></td>
	<td class="line x" title="121:207	The order in which these constraints are applied is important, as illustrated by the example in Figure 4: First the list of possible antecedents for the appositive NP is constrained to only its parent." ></td>
	<td class="line x" title="122:207	Now that all appositiveshavebeenconstrained, weapplythei-withiniconstraint, whichpreventsitsfromhavingtheNP headedbybrand inthesetofpossibleantecedents, and by propagation, also removes the NP headed by Gitano." ></td>
	<td class="line x" title="123:207	This leaves the NP Wal-Mart as the closest compatible mention." ></td>
	<td class="line x" title="124:207	Addingthesesyntacticconstraintstooursystem yields 55.4 F1, a fairly substantial improvement, but many recall errors remain between mentions with differing heads." ></td>
	<td class="line x" title="125:207	Resolving such cases will require external semantic information, which we will automatically acquire (see Section 3.2)." ></td>
	<td class="line x" title="126:207	Predicate Nominatives: Another syntactic constraint exploited in Poon and Domingos (2008) is the predicate nominative construction, where the object of a copular verb (forms of the verb be) is constrained to corefer with its subject (e.g. Microsoft is a company in Redmond)." ></td>
	<td class="line x" title="127:207	While much less frequent than appositive configurations (there are only 17 predicate nominatives in our devel9Arguably, we could also consider right modifying NPs (e.g., [Microsoft [Company]1]1) to be role appositive, but we do not do so here." ></td>
	<td class="line x" title="128:207	1156 Path Example NP a97 a97 a97 a33 a33 a33 NP-NNP PRN-NNP NP a88 a88 a88 a88 a88 a76 a76 a24 a24 a24 a24 a24 NP-president CC NP-NNP America Online Inc." ></td>
	<td class="line x" title="129:207	(AOL) NP a97 a97 a97 a33 a33 a33 NP-NNP PRN-NNP NP a88 a88 a88 a88 a88 a76 a76 a24 a24 a24 a24 a24 NP-president CC NP-NNP [President and C.E.O] Bill Gates Figure 5: Example paths extracted via semantic compatibility mining (see Section 3.2) along with example instantiations." ></td>
	<td class="line x" title="130:207	In both examples the left child NP is coreferent with the rightmost NP." ></td>
	<td class="line x" title="131:207	Each category in the interior of the tree path is annotated with the head word as well as its subcategorization." ></td>
	<td class="line x" title="132:207	The examples given here collapse multiple instances of extracted paths." ></td>
	<td class="line x" title="133:207	opment set), predicate nominatives are another highly reliable coreference pattern which we will leverage in Section 3.2 to mine semantic knowledge." ></td>
	<td class="line x" title="134:207	As with appositives, we annotate object predicate-nominative NPs and constrain coreference as before." ></td>
	<td class="line x" title="135:207	This yields a minor improvement to 55.5 F1." ></td>
	<td class="line x" title="136:207	3.2 Semantic Knowledge While appositives and related syntactic constructions can resolve some cases of non-pronominal reference, most cases require semantic knowledge about the various entities as well as the verbs used in conjunction with those entities to disambiguate references (Kehler et al., 2008)." ></td>
	<td class="line x" title="137:207	However, givenasemanticallycompatiblemention head pair, say AOL and company, one might expect to observe a reliable appositive or predicative-nominative construction involving these mentions somewhere in a large corpus." ></td>
	<td class="line x" title="138:207	In fact, the Wikipedia page for AOL10 has a predicate-nominative construction which supports the compatibility of this head pair: AOL LLC (formerly America Online) is an American global Internet services and media company operated by Time Warner." ></td>
	<td class="line x" title="139:207	In order to harvest compatible head pairs, we utilize our BLIPP and WIKI data sets (see Section2), andforeachnoun(properorcommon)and pronoun, we assign a maximal NP mention node for each nominal head as in Section 3.1.1; we then annotate appositive and predicate-nominative NPs as in Section 3.1.3." ></td>
	<td class="line x" title="140:207	For any NP which is annotated as an appositive or predicate-nominative, we extract the head pair of that node and its constrained antecedent." ></td>
	<td class="line x" title="141:207	10http://en.wikipedia.org/wiki/AOL The resulting set of compatible head words, while large, covers a little more than half of the examples given in Table 1." ></td>
	<td class="line x" title="142:207	The problem is that these highly-reliable syntactic configurations are too sparse and cannot capture all the entity information present." ></td>
	<td class="line x" title="143:207	For instance, the first sentence of Wikipedia abstract for Al Gore is: Albert Arnold Al Gore, Jr. is an American environmental activist who served as the 45th Vice President of the United States from 1993 to 2001 under President Bill Clinton." ></td>
	<td class="line x" title="144:207	The required lexical pattern X who served as Y is a general appositive-like pattern that almost surely indicates coreference." ></td>
	<td class="line x" title="145:207	Rather than opt to manually create a set of these coreference patterns as in Hearst (1992), we instead opt to automatically extract these patterns from large corpora as in Snow et al.(2004) and Phillips and Riloff (2007)." ></td>
	<td class="line x" title="147:207	We take a simple bootstrapping technique: given a set of mention pairs extracted from appositives and predicate-nominative configurations, we extract counts over tree fragments between nodes which have occurred in this set of head pairs (see Figure 5); the tree fragments are formed by annotating the internal nodes in the tree path with the head word and POS along with the subcategorization." ></td>
	<td class="line x" title="148:207	We limit the paths extracted in this way in several ways: paths are only allowed to go between adjacent sentences and have a length of at most 10." ></td>
	<td class="line x" title="149:207	We then filter the set of paths to those which occur more than a hundred times and with at least 10 distinct seed head word pairs." ></td>
	<td class="line x" title="150:207	The vast majority of the extracted fragments are variants of traditional appositives and predicatenominatives with some of the structure of the NPs 1157 MUC b3 Pairwise CEAF System P R F1 P R F1 P R F1 P R F1 ACE2004-ROTH-DEV BASIC-FLAT 73.5 66.8 70.0 80.6 68.6 74.1 63.6 39.7 48.9 68.4 68.4 68.4 BASIC-TREE 75.8 68.9 72.2 81.9 69.9 75.4 65.6 42.7 51.7 69.8 69.8 69.8 +SYN-COMPAT 77.8 68.5 72.9 84.1 69.7 76.2 71.0 43.1 53.4 69.8 69.8 69.8 +SYN-CONSTR 78.3 70.5 74.2 84.0 71.0 76.9 71.3 45.4 55.5 70.8 70.8 70.8 +SEM-COMPAT 77.9 74.1 75.9 81.8 74.3 77.9 68.2 51.2 58.5 72.5 72.5 72.5 ACE2004-CULOTTA-TEST BASIC-FLAT 68.6 60.9 64.5 80.3 68.0 73.6 57.1 30.5 39.8 66.5 66.5 66.5 BASIC-TREE 71.2 63.2 67.0 81.6 69.3 75.0 60.1 34.5 43.9 67.9 67.9 67.9 +SYN-COMPAT 74.6 65.2 69.6 84.2 70.3 76.6 66.7 37.2 47.8 69.2 69.2 69.2 +SYN-CONSTR 74.3 66.4 70.2 83.6 71.0 76.8 66.4 38.0 48.3 69.6 69.6 69.6 +SEM-COMPAT 74.8 77.7 79.6 79.6 78.5 79.0 57.5 57.6 57.5 73.3 73.3 73.3 SupervisedResults Culottaetal.(2007) 86.7 73.2 79.3 BengstonandRoth(2008)82.7 69.9 75.8 88.3 74.5 80.8 55.4 63.7 59.2 MUC6-TEST +SEM-COMPAT 87.2 77.3 81.9 84.7 67.3 75.0 80.5 57.8 67.3 72.0 72.0 72.0 UnsupervisedResults PoonandDomingos(2008)83.0 75.8 79.2 63.0 57.0 60.0 SupervisedResults FinkelandManning(2008)89.7 55.1 68.3 90.9 49.7 64.3 74.1 37.1 49.5 ACE2004-NWIRE +SEM-COMPAT 77.0 75.9 76.5 79.4 74.5 76.9 66.9 49.2 56.7 71.5 71.5 71.5 UnsupervisedResults PoonandDomingos(2008)71.3 70.5 70.9 62.6 38.9 48.0 Table 2: Experimental Results (See Section 4): When comparisons between systems are presented, the largest result is bolded." ></td>
	<td class="line x" title="151:207	The CEAF measure has equal values for precision, recall, and F1." ></td>
	<td class="line x" title="152:207	specified." ></td>
	<td class="line x" title="153:207	However there are some tree fragments which correspond to the novel coreference patterns (see Figure 5) of parenthetical alias as well as conjunctions of roles in NPs." ></td>
	<td class="line x" title="154:207	We apply our extracted tree fragments to our BLIPP and WIKI data sets and extract a set of compatible word pairs which match these fragments; these words pairs will be used to relax the semantic compatibility filter (see the start of the section); mentions are compatible with prior mentions with the same head or with a semantically compatible headword." ></td>
	<td class="line x" title="155:207	Thisyields58.5pairwiseF1(see SEMCOMPAT in Table 2) as well as similar improvements across other metrics." ></td>
	<td class="line x" title="156:207	By and large the word pairs extracted in this way are correct (in particular we now have coverage for over two-thirds of the head pair recall errors from Table 1.)" ></td>
	<td class="line x" title="157:207	There are however wordpairs which introduce errors." ></td>
	<td class="line x" title="158:207	In particular citystate constructions (e.g. Los Angeles, California) appears to be an appositive and incorrectly allows our system to have angeles as an antecedent for california." ></td>
	<td class="line x" title="159:207	Another common error is that the % symbol is made compatible with a wide variety of common nouns in the financial domain." ></td>
	<td class="line x" title="160:207	4 Experimental Results We present formal experimental results here (see Table 2)." ></td>
	<td class="line x" title="161:207	We first evaluate our model on the ACE2004-CULOTTA-TEST dataset used in the state-of-the-art systems from Culotta et al.(2007) and Bengston and Roth (2008)." ></td>
	<td class="line x" title="163:207	Both of these systems were supervised systems discriminatively trained to maximize b3 and used features from many different structured resources including WordNet, as well as domain-specific features (Culotta et al., 2007)." ></td>
	<td class="line x" title="164:207	Our best b3 result of 79.0 is broadly in the range of these results." ></td>
	<td class="line x" title="165:207	We should note that in our work we use neither the gold mention types (we do not model pre-nominals separately) nor do we use the gold NER tags which Bengston and Roth (2008) does." ></td>
	<td class="line x" title="166:207	Across metrics, the syntactic constraints and semantic compatibility components contribute most to the overall final result." ></td>
	<td class="line x" title="167:207	On the MUC6-TEST dataset, our system outper1158 PROP ER NOMI NAL PRON OUN NULL TOTA L PROPER 21/451 8/20 72/288101/759 NOMINAL 16/150 99/432 158/351323/933 PRONOUN 29/149 60/128 15/97 1/2 105/376 Table 3: Errors for each type of antecedent decision made by the system." ></td>
	<td class="line x" title="168:207	Each row is a mention type and the column the predicted mention type antecedent." ></td>
	<td class="line x" title="169:207	The majority of errors are made in the NOMINAL category." ></td>
	<td class="line x" title="170:207	forms both Poon and Domingos (2008) (an unsupervised Markov Logic Network system which uses explicit constraints) and Finkel and Manning (2008) (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures.11 Similarly, on the ACE2004-NWIRE dataset, we also outperform the state-of-the-art unsupervised system of Poon and Domingos (2008)." ></td>
	<td class="line x" title="171:207	Overall, we conclude that our system outperforms state-of-the-art unsupervised systems12 and isintherangeofthestate-of-theartsystemsofCulotta et al.(2007) and Bengston and Roth (2008)." ></td>
	<td class="line x" title="173:207	5 Error Analysis There are several general trends to the errors made by our system." ></td>
	<td class="line x" title="174:207	Table 3 shows the number of pairwise errors made on MUC6-TEST dataset by mention type; note these errors are not equally weighted in the final evaluations because of the transitive closure taken at the end." ></td>
	<td class="line x" title="175:207	The most errors are made on nominal mentions with pronouns coming in a distant second." ></td>
	<td class="line x" title="176:207	In particular, we most frequently say a nominal is NULL when it has an antecedent; this is typically due to not having the necessary semantic knowledge to link a nominal to a prior expression." ></td>
	<td class="line x" title="177:207	In order to get a more thorough view of the cause of pairwise errors, we examined 20 random errors made in aligning each mention type to an antecedent." ></td>
	<td class="line x" title="178:207	We categorized the errors as follows:  SEM." ></td>
	<td class="line x" title="179:207	COMPAT: Missing information about the compatibility of two words e.g. pay and wage." ></td>
	<td class="line x" title="180:207	For pronouns, this is used to mean that 11KlennerandAilloud(2007)tookessentiallythesameapproach but did so on non-comparable data." ></td>
	<td class="line x" title="181:207	12Poon and Domingos (2008) outperformed Haghighi and Klein (2007)." ></td>
	<td class="line x" title="182:207	Unfortunately, we cannot compare against Ng (2008) since we do not have access to the version of the ACE data used in their evaluation." ></td>
	<td class="line x" title="183:207	we incorrectly aligned a pronoun to a mention with which it is not semantically compatible (e.g. he aligned to board)." ></td>
	<td class="line x" title="184:207	 SYN." ></td>
	<td class="line x" title="185:207	COMPAT: Error in assigning linguistic features of nouns for compatibility with pronouns (e.g. disallowing they to refer to team)." ></td>
	<td class="line x" title="186:207	 HEAD: Errors involving the assumption that mentionswiththesameheadarealwayscompatible." ></td>
	<td class="line x" title="187:207	Includes modifier and specificity errors such as allowing Lebanon and Southern Lebanon to corefer." ></td>
	<td class="line x" title="188:207	This also includes errors of definiteness in nominals (e.g. the people in the room and Chinese people)." ></td>
	<td class="line x" title="189:207	Typically, these errors involve a combination of missing syntactic and semantic information." ></td>
	<td class="line x" title="190:207	 INTERNAL NP: Errors involving lack of internal NP structure to mark role appositives (see Section 3.1.3)." ></td>
	<td class="line x" title="191:207	 PRAG." ></td>
	<td class="line x" title="192:207	/ DISC.: Errorswherediscoursesalience or pragmatics are needed to disambiguate mention antecedents." ></td>
	<td class="line x" title="193:207	 PROCESS ERROR: Errors which involved a tokenization, parse, or NER error." ></td>
	<td class="line x" title="194:207	The result of this error analysis is given in Table 4; note that a single error may be attributed to more than one cause." ></td>
	<td class="line x" title="195:207	Despite our efforts in Section 3 to add syntactic and semantic information to our system, the largest source of error is still a combination of missing semantic information or annotated syntactic structure rather than the lack of discourse or salience modeling." ></td>
	<td class="line x" title="196:207	Our error analysis suggests that in order to improve the state-of-the-art in coreference resolution, futureresearchshouldconsiderrichersyntacticandsemanticinformationthantypicallyusedin current systems." ></td>
	<td class="line x" title="197:207	6 Conclusion Our approach is not intended as an argument against the more complex, discourse-focused approaches that typify recent work." ></td>
	<td class="line x" title="198:207	Instead, we note that rich syntactic and semantic processing vastly reduces the need to rely on discourse effects or evidence reconciliation for reference resolution." ></td>
	<td class="line x" title="199:207	Indeed, we suspect that further improving the syntactic and semantic modules in our system may produce greater error reductions than any other 1159 MentionType SEM." ></td>
	<td class="line x" title="200:207	COMPAT SYN." ></td>
	<td class="line x" title="201:207	COMPAT HEAD INTENALNP PRAG/DISC." ></td>
	<td class="line x" title="202:207	PROCESSERROR OTHER Comment NOMINAL 7 5 6 2 2 1 2generalappos.patterns PRONOUN 6 3 6 3 3 3 2cataphora PROPER 6 3 4 4 4 1 Table 4: Error analysis on ACE2004-CULOTTA-TEST data by mention type." ></td>
	<td class="line x" title="203:207	The dominant errors are in either semantic or syntactic compatibility of mentions rather than discourse phenomena." ></td>
	<td class="line x" title="204:207	See Section 5." ></td>
	<td class="line x" title="205:207	route forward." ></td>
	<td class="line x" title="206:207	Of course, a system which is rich in all axes will find some advantage over any simplified approach." ></td>
	<td class="line x" title="207:207	Nonetheless, our coreference system, despite being relatively simple and having no tunable parameters or complexity beyond the non-reference complexity of its component modules, manages to outperform state-of-the-art unsupervised coreference resolution and be broadly comparable to state-of-the-art supervised systems." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1158
Domain adaptive bootstrapping for named entity recognition
Wu, Dan;Lee, Wee Sun;Ye, Nan;Chieu, Hai Leong;"></td>
	<td class="line x" title="1:234	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 15231532, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:234	c 2009 ACL and AFNLP Domain adaptive bootstrapping for named entity recognition Dan Wu1, Wee Sun Lee2, Nan Ye2 1Singapore MIT Alliance 2Department of Computer Science National University of Singapore {dwu@,leews@comp,g0701171@}nus.edu.sg Hai Leong Chieu DSO National Laboratories chaileon@dso.org.sg Abstract Bootstrapping is the process of improving the performance of a trained classifier by iteratively adding data that is labeled by the classifier itself to the training set, and retraining the classifier." ></td>
	<td class="line x" title="3:234	It is often used in situations where labeled training data is scarce but unlabeled data is abundant." ></td>
	<td class="line x" title="4:234	In this paper, we consider the problem of domain adaptation: the situation where training data may not be scarce, but belongs to a different domain from the target application domain." ></td>
	<td class="line x" title="5:234	As the distribution of unlabeled data is different from the training data, standard bootstrapping often has difficulty selecting informative data to add to the training set." ></td>
	<td class="line x" title="6:234	We propose an effective domain adaptive bootstrapping algorithm that selects unlabeled target domain data that are informative about the target domain and easy to automatically label correctly." ></td>
	<td class="line x" title="7:234	We call these instances bridges, as they are used to bridge the source domain to the target domain." ></td>
	<td class="line x" title="8:234	We show that the method outperforms supervised, transductive and bootstrapping algorithms on the named entity recognition task." ></td>
	<td class="line x" title="9:234	1 Introduction Most recent researches on natural language processing (NLP) problems are based on machine learning algorithms." ></td>
	<td class="line x" title="10:234	High performance can often be achieved if the system is trained and tested on data from the same domain." ></td>
	<td class="line x" title="11:234	However, the performance of NLP systems often degrades badly when the test data is drawn from a source that is different from the labeled data used to train the system." ></td>
	<td class="line x" title="12:234	For named entity recognition (NER), for example, Ciaramita and Altun (2005) reported that a system trained on a labeled Reuters corpus achieved an F-measure of 91% on a Reuters test set, but only 64% on a Wall Street Journal test set." ></td>
	<td class="line x" title="13:234	The task of adapting a system trained on one domain (called the source domain) to a new domain (called the target domain) is called domain adaptation." ></td>
	<td class="line x" title="14:234	In domain adaptation, it is generally assumed that we have labeled data in the source domain while labeled data may or may not be available in the target domain." ></td>
	<td class="line x" title="15:234	Previous work in domain adaptation can be classified into two categories: [S+T+], where a small, labeled target domain data is available, e.g.(Blitzer et al., 2006; Jiang and Zhai, 2007; Daume III, 2007; Finkel and Manning, 2009), or [S+T-], where no labeled target domain data is available, e.g.(Blitzer et al., 2006; Jiang and Zhai, 2007)." ></td>
	<td class="line x" title="18:234	In both cases, and especially for [S+T-], domain adaptation can leverage on large amounts of unlabeled data in the target domain." ></td>
	<td class="line x" title="19:234	In practice, it is often unreasonable to expect labeled data for every new domain that we come across, such as blogs, emails, a different newspaper agency, or simply articles from a different topic or period in time." ></td>
	<td class="line x" title="20:234	Thus although [S+T+] is easier to handle, [S+T-] is of higher practical importance." ></td>
	<td class="line x" title="21:234	In this paper, we propose a domain adaptive bootstrapping (DAB) approach to tackle the domain adaptation problem under the setting [S+T-]." ></td>
	<td class="line x" title="22:234	Bootstrapping is an iterative process that uses a trained classifier to label and select unlabeled instances to add to the training set for retraining the classifier." ></td>
	<td class="line x" title="23:234	It is often used when labeled training data is scarce but unlabeled data is abundant." ></td>
	<td class="line x" title="24:234	In contrast, for domain adaptation problems, we may have a lot of training data but the target application domain has a different data distribution." ></td>
	<td class="line x" title="25:234	Standard bootstrapping usually selects instances that are most confidently labeled from the unlabeled data." ></td>
	<td class="line x" title="26:234	In domain adaptation situations, usually the most confidently labeled instances are the ones that are most similar to the source domain in1523 stances these instances tend to contain very little information about the target domain." ></td>
	<td class="line x" title="27:234	For domain adaptive bootstrapping, we propose a selection criterion that selects instances that are informative and easy to automatically label correctly." ></td>
	<td class="line x" title="28:234	In addition, we propose a criterion for stopping the process of bootstrapping before it adds uninformative and incorrectly labeled instances that can reduce performance." ></td>
	<td class="line x" title="29:234	Our approach leverages on instances in the target domain called bridges." ></td>
	<td class="line x" title="30:234	These instances contain domain-independent features, as well as features specific to the target domain." ></td>
	<td class="line x" title="31:234	As they contain domain-independent features, they can be classified correctly by classifiers trained on the source domain labeled data." ></td>
	<td class="line x" title="32:234	We argue that these instances act as a bridge between the source and the target domain." ></td>
	<td class="line x" title="33:234	We show that, on the NER task, DAB outperforms supervised, transductive and standard bootstrapping algorithms, as well as a bootstrapping variant, called balanced bootstrapping (Jiang and Zhai, 2007), that has recently been proposed for domain adaptation." ></td>
	<td class="line x" title="34:234	2 Related work One general class of approaches to domain adaptation is to consider that the instances from the source and the target domain are drawn from different distributions." ></td>
	<td class="line x" title="35:234	Bickel et al.(Bickel et al., 2007) discriminatively learns a scaling factor for source domain training data, so as to adapt the source domain data distribution to resemble the target domain data distribution, under the [S+T-] setting." ></td>
	<td class="line x" title="37:234	Daume III and Marcu (Daume III and Marcu, 2006) considers that the data distribution is a mixture distribution over general, source domain and target domain data." ></td>
	<td class="line x" title="38:234	They learn the underlying mixture distribution using the conditional expectation maximization algorithm, under the [S+T+] setting." ></td>
	<td class="line x" title="39:234	Jiang and Zhai (2007) proposed an instance re-weighting framework that handles both the [S+T+] and [S+T-] settings." ></td>
	<td class="line x" title="40:234	For [S+T-], the resulting algorithm is a balanced bootstrapping algorithm, which was shown to outperform the standard bootstrapping algorithm." ></td>
	<td class="line x" title="41:234	In this paper, we assume the [S+T-] settings, and we show that the approach proposed in this paper, domain adaptive bootstrapping (DAB), outperforms the balanced bootstrapping algorithm on NER." ></td>
	<td class="line x" title="42:234	Another class of approaches to domain adaptation is feature-based." ></td>
	<td class="line x" title="43:234	Daume III (Daume III, 2007) divided features into three classes: domainindependent features, source-domain features and target-domain features." ></td>
	<td class="line x" title="44:234	He assumed the existence of training data in the target-domain (under the setting [S+T+]), so that the three classes of features can be jointly trained using source and target domain labeled data." ></td>
	<td class="line x" title="45:234	This cannot be done in the setting [S+T-], where no training data is available in the target domain." ></td>
	<td class="line x" title="46:234	Using a different approach, Blitzer et al.(2006) induces correspondences between feature spaces in different domains, by detecting pivot features." ></td>
	<td class="line x" title="48:234	Pivot features are features that occur frequently and behave similarly in different domains." ></td>
	<td class="line x" title="49:234	Pivot features are used to put domain-specific features in correspondence." ></td>
	<td class="line x" title="50:234	In this paper, instead of pivot features, we attempt to leverage on pivot instances that we call bridges, which are instances that bridge the source and target domain." ></td>
	<td class="line x" title="51:234	This will be illustrated in Section 3." ></td>
	<td class="line x" title="52:234	It is generally recognized that adding informative and correctly labeled instances is more useful for learning." ></td>
	<td class="line x" title="53:234	Active learning queries the user for labels of most informative or relevant instances." ></td>
	<td class="line x" title="54:234	Active learning, which has been applied to the problem of NER in (Shen et al., 2004), is used in situations where a large amount of unlabeled data exists and data labeling is expensive." ></td>
	<td class="line x" title="55:234	It has also been applied to the problem of domain adaptation for word sense disambiguation in (Chan and Ng, 2007)." ></td>
	<td class="line x" title="56:234	However, active learning requires human intervention." ></td>
	<td class="line x" title="57:234	Here, we want to achieve the same goal without human intervention." ></td>
	<td class="line x" title="58:234	3 Bootstrapping for domain adaptation We first define the notations used for domain adaptation in the [S+T-] setting." ></td>
	<td class="line x" title="59:234	A set of training data DS = {xi,yi}1i|DS| is given in the source domain, where the notation |X| denotes the size of a set X. Each instance xi in DS has been manually annotated with a label, yi, from a given set of labels Y . The objective of domain adaptation is to label a set of unlabeled data, DT = {xi}1i|DT| with labels from Y . A machine learning algorithm will take a labeled data set (for e.g. DS) and outputs a classifier, which can then be used to classify unlabeled data, i.e. assign labels to unlabeled instances." ></td>
	<td class="line x" title="60:234	A special class of machine learning algorithms, called transductive learning algorithms, is able to take the unlabeled data DT into account during the learning process (see e.g.(Joachims, 1999))." ></td>
	<td class="line x" title="62:234	1524 However, such algorithms do not take into account the shift in domain of the test data." ></td>
	<td class="line x" title="63:234	Jiang and Zhai (2007) recently proposed an instance re-weighting framework to take domain shift into account." ></td>
	<td class="line x" title="64:234	For [S+T-], the resulting algorithm is a balanced bootstrapping algorithm, which we describe below." ></td>
	<td class="line x" title="65:234	3.1 Standard and balanced bootstrapping We define a general bootstrapping algorithm in Algorithm 1." ></td>
	<td class="line x" title="66:234	The algorithm can be applied to any machine learning algorithm that allows training instances to be weighted, and that gives confidence scores for the labels when used to classify test data." ></td>
	<td class="line x" title="67:234	The bootstrapping procedure iteratively improves the performance of a classifier SCt over a number of iterations." ></td>
	<td class="line x" title="68:234	In Algorithm 1, we have left a number of parameters unspecified." ></td>
	<td class="line x" title="69:234	These parameters are (1) the selection-criterion for instances to be added to the training data, (2) the terminationcriterion for the bootstrapping process, and (3) the weights (wS,wT) given to the labeled and bootstrapped training sets." ></td>
	<td class="line x" title="70:234	Standard bootstrapping: (Jiang and Zhai, 2007) the selection-criterion is based on selecting the top k most-confidently labeled instances in Rt." ></td>
	<td class="line x" title="71:234	The weight wSt is equal to wTt . The value of k is a parameter for the bootstrapping algorithm." ></td>
	<td class="line x" title="72:234	Balanced bootstrapping: (Jiang and Zhai, 2007) the selection-criterion is still based on selecting the top k most-confidently labeled instances in Rt." ></td>
	<td class="line x" title="73:234	Balanced bootstrapping was formulated for domain adaptation, and hence they set the weights to satisfy the ratio wStwT t = |Tt||DS|." ></td>
	<td class="line x" title="74:234	This allows the small amount of target data added, Tt, to have an equal weight to the large source domain training set DS." ></td>
	<td class="line x" title="75:234	In this paper, we formulate a selection-criterion and a termination-criterion which are better than those used in standard and balanced bootstrapping." ></td>
	<td class="line x" title="76:234	Regarding the selection-criterion, standard and balanced bootstrapping both select instances which are confidently labeled by SCt to be used for training SCt+1, in the hope of avoiding using wrongly labeled data in bootstrapping." ></td>
	<td class="line x" title="77:234	However, instances that are already confidently labeled by SCt may not contain sufficient information which is not in DS, and using them to train SCt+1 may result in SCt+1 performing similarly to SCt." ></td>
	<td class="line x" title="78:234	This motivates us to select samples which are both informative and easy to automatically label correctly." ></td>
	<td class="line x" title="79:234	Regarding the termination-criterion, which Algorithm 1 Bootstrapping algorithm Input: labeled data DS, test data DT and a machine learning algorithm." ></td>
	<td class="line x" title="80:234	Output: the predicted labels of the set DT . Set T0 = , R0 = DT , and t = 0 Repeat 1." ></td>
	<td class="line x" title="81:234	learn a classifier SCt with (DS,Tt) with weights (wSt ,wTt ) 2." ></td>
	<td class="line x" title="82:234	label the set Rt with SCt 3." ></td>
	<td class="line x" title="83:234	select St  Rt based on selection-criterion 4." ></td>
	<td class="line x" title="84:234	Tt+1 = Tt St, and Rt+1 = Rt \St. Until termination-criterion Output the predicted labels of DT by SCt." ></td>
	<td class="line x" title="85:234	is not mentioned in the paper (Jiang and Zhai, 2007), we assume that bootstrapping is simply run for either a single iteration, or a small and fixed number of iterations." ></td>
	<td class="line x" title="86:234	However, it is known that such simple criterion may result in stopping too early or too late, leading to sub-optimal performance." ></td>
	<td class="line x" title="87:234	We propose a more effective terminationcriterion here." ></td>
	<td class="line x" title="88:234	3.2 Domain adaptive bootstrapping (DAB) Our selection-criterion relies on the observation that in domain adaptation, instances (from the source or the target domain) can be divided into three types according to their information content: generalists are instances that contain only domainindependent information and are present in all domains; specialists are instances containing only domain-specific information and are present only in their respective domains; bridges are instances containing both domain-independent and domainspecific information, also present only in their respective domains but are useful as a bridge between the source and the target domains." ></td>
	<td class="line x" title="89:234	The implication of the above observation is that when choosing unlabeled target domain data for bootstrapping, we should exploit the bridges, because the generalists are not likely to contain much information not in DS due to their domainindependence, and the specialists are difficult to be labeled correctly due to their domain-specificity." ></td>
	<td class="line x" title="90:234	In contrast, the bridges are informative and easier to label correctly." ></td>
	<td class="line x" title="91:234	Choosing confidently classified instances for bootstrapping, as in standard bootstrapping and balanced bootstrapping, is simple, but results in choosing mostly generalists, and is too conservative." ></td>
	<td class="line x" title="92:234	We design a scoring function 1525 on instances, which has high value when the instance is informative and sufficiently likely to be correctly labeled in order to identify correctly labeled bridges." ></td>
	<td class="line x" title="93:234	Intuitively, informativeness of an instance can be measured by the prediction results of the ideal classifier IS for the source domain and the ideal classifier IT for the target domain." ></td>
	<td class="line x" title="94:234	If IS and IT are both probabilistic classifiers, IS should return a noninformative distribution while IT should return an informative one." ></td>
	<td class="line x" title="95:234	The ideal classifier for the source domain is approximated with a source classifier SC trained on DS, while the ideal classifier for the target domain is approximated by training a classifier, TC, on target domain instances labeled by the source classifier." ></td>
	<td class="line x" title="96:234	We also try to ensure that instances that are selected are correctly classified." ></td>
	<td class="line x" title="97:234	As the label used is provided by the target classifier, we estimate the precision of the target classification." ></td>
	<td class="line x" title="98:234	The final ranking function is constructed by combining this estimate with the informativeness of the instance." ></td>
	<td class="line x" title="99:234	We show the algorithm for the instance selection in Algorithm 2." ></td>
	<td class="line x" title="100:234	The notations used follow those used in Algorithm 1." ></td>
	<td class="line x" title="101:234	For simplicity, we assume that wSt = wTt = 1 for all t. We expect TC to be a reasonable classifier on DT due to the presence of generalists and bridges." ></td>
	<td class="line x" title="102:234	Note that the target classifier is constructed by randomly splitting DT into two partitions, training a classifier on each partition and using the prediction of the trained classifier on the partition it is not trained on." ></td>
	<td class="line x" title="103:234	This is because classifiers tend to fit the data that they have been trained on too well making the probability estimates on their training data unreliable." ></td>
	<td class="line x" title="104:234	Also, a random partition is used to ensure that the data in each partition is representative of Du." ></td>
	<td class="line x" title="105:234	3.3 The scoring function: score(p(s),p(t)) The scoring function score(p(s),p(t)) in Algorithm 2 is simply implemented as the product of two components: a measure of the informativeness and the probability that SCs label is correct." ></td>
	<td class="line x" title="106:234	We show how the intuitive ideas (described above) behind these two components are formalized." ></td>
	<td class="line x" title="107:234	Informativeness of a distribution p on a set of discrete labels Y is measured by its entropy h(p) defined by h(p) = summationdisplay yY p(y)logp(y)." ></td>
	<td class="line x" title="108:234	Algorithm 2 Algorithm for selecting instances for bootstrapping at iteration t Input: Labeled source domain data DS, target domain training data Tt, remaining data Rt, the classifier SCt trained on DS Tt, and a scoring function score(p(s),p(t)) Output: k instances for bootstrapping." ></td>
	<td class="line x" title="109:234	1." ></td>
	<td class="line x" title="110:234	Label Rt with SCt, and to each instance xi  Rt, SCt outputs a distribution p(s)i (yi) over its labels." ></td>
	<td class="line x" title="111:234	2." ></td>
	<td class="line x" title="112:234	Randomly split Rt into two partitions, R0t and R1t with their labels assigned by SCt." ></td>
	<td class="line x" title="113:234	3." ></td>
	<td class="line x" title="114:234	Train each target classifier, TCxt with the data Rxt , for x = {0,1}." ></td>
	<td class="line x" title="115:234	4." ></td>
	<td class="line x" title="116:234	Label R(1x)t with the classifier TCxt , which to each instance xi  Rt, outputs a distribution p(t)i (yi) over its labels." ></td>
	<td class="line x" title="117:234	5." ></td>
	<td class="line x" title="118:234	Score each instance from xi  Rt with the function score(p(s)i ,p(t)i )." ></td>
	<td class="line x" title="119:234	6." ></td>
	<td class="line x" title="120:234	Select top k instances from Rt with the highest scores." ></td>
	<td class="line x" title="121:234	h(p) is nonnegative; h(p) = 0 if and only if p has probability 1 on one of the labels; h(p) attains its maximum value when the distribution p is uniform over all labels." ></td>
	<td class="line x" title="122:234	Hence, an instance is classified with high confidence when the distribution over its labels has low entropy." ></td>
	<td class="line x" title="123:234	We measure the informativeness of an instance using h(p(s))h(p(t)), where p(s) and p(t) are as in Algorithm 2." ></td>
	<td class="line x" title="124:234	We argue that a larger value of this expression implies that the instance is more likely to be a bridge instance." ></td>
	<td class="line x" title="125:234	This expression has a high value when the source classifier is uncertain, and the target classifier is certain." ></td>
	<td class="line x" title="126:234	Uncertain classification by the source classifier indicates that the instance is unlikely to be a generalist." ></td>
	<td class="line x" title="127:234	Moreover, if the target classifier is certain on xi, it means that instances similar to the instance xi are consistently labeled with the same label by the source classifier SCt, indicating that it is likely to be a bridge instance." ></td>
	<td class="line x" title="128:234	The probability that TCs label is correct cannot be estimated directly because we do not have labeled target domain data." ></td>
	<td class="line x" title="129:234	Instead, we use the source domain to give an estimate." ></td>
	<td class="line x" title="130:234	We do this with a simple pre-processing step: we split the data DS into two partitions of equal size, train a classifier on each partition, and test each classifier on the 1526 other partition." ></td>
	<td class="line x" title="131:234	We then measure the resulting accuracy given each label: (y) = # correctly labeled instances of label y# total instances of label y . Summarizing the above discussion, the scoring function is as shown below." ></td>
	<td class="line x" title="132:234	score(p(s),p(t)) = (y) bracketleftBig h(p(s))h(p(t)) bracketrightBig , where y = argmaxyY p(s)(y) The scoring function has a high value when the information content of the example is high and the label has high precision." ></td>
	<td class="line x" title="133:234	3.4 The termination criterion Intuitively, our algorithm terminates when there are not enough informative instances." ></td>
	<td class="line x" title="134:234	Formally, we define the termination criterion as follows: we terminate the bootstrapping process when, there exists an instance xi in the top k instances satisfying the following condition: 1." ></td>
	<td class="line x" title="135:234	h(p(s)i ) < h(p(t)i ), or 2." ></td>
	<td class="line x" title="136:234	maxyY p(s)i (y) > maxyY p(t)i (y) The second case is used to check for instances where the classifier SCt is more confident than the target classifiers TCxt , on their respective predicted labels." ></td>
	<td class="line x" title="137:234	This shows that the instance xi is more of a generalist than a bridge." ></td>
	<td class="line x" title="138:234	4 NER task and implementation The algorithm described in Section 3 is not specific to any particular application." ></td>
	<td class="line x" title="139:234	In this paper, we apply it to the problem of named entity recognition (NER)." ></td>
	<td class="line x" title="140:234	In this section, we describe the NER classifier and the features used in our experiments." ></td>
	<td class="line oc" title="141:234	4.1 NER features We used the features generated by the CRF package (Finkel et al., 2005)." ></td>
	<td class="line x" title="142:234	These features include the word string feature, the case feature for the current word, the context words for the current word and their cases, the presence in dictionaries for the current word, the position of the current word in the sentence, prefix and suffix of the current word as well as the case information of the multiple occurrences of the current word." ></td>
	<td class="line x" title="143:234	We use the same set of features for all classifiers used in the bootstrapping process, and for all baselines used in the experimental section." ></td>
	<td class="line x" title="144:234	4.2 Machine learning algorithms A base machine learning algorithm is required in bootstrapping approaches." ></td>
	<td class="line x" title="145:234	We describe the two machine learning algorithms used in this paper." ></td>
	<td class="line x" title="146:234	We chose these algorithms for their good performance on the NER task." ></td>
	<td class="line x" title="147:234	Maximum entropy classification (MaxEnt): The MaxEnt approach, or logistic regression, is one of the most competitive methods for named entity recognition (Tjong and Meulder, 2003)." ></td>
	<td class="line x" title="148:234	MaxEnt is a discriminative method that learns a distribution, p(yi|xi), over the labels, yi, given the vector of features, xi." ></td>
	<td class="line x" title="149:234	We used the implementation of MaxEnt classifier described in (Manning and Klein, 2003)." ></td>
	<td class="line x" title="150:234	For NER, each instance represents a single word token within a sentence, with the feature vector xi derived from the sentence as described in the previous section." ></td>
	<td class="line x" title="151:234	MaxEnt is not designed for sequence classification." ></td>
	<td class="line x" title="152:234	To deal with sequences, each name-class (e.g. PERSON) is divided into sub-classes: first token (e.g. PERSON-begin), unique token (e.g. PERSONunique), or subsequent tokens (e.g. PERSONcontinue) in the name-class." ></td>
	<td class="line x" title="153:234	To ensure that the results returned by MaxEnt is coherent, we define deterministic transition probabilities that disallow transitions such as one from PERSON-begin to LOCATION-continue." ></td>
	<td class="line x" title="154:234	A Viterbi parse is used to find the valid sequence of name-classes with the highest probability." ></td>
	<td class="line x" title="155:234	Support vector machines (SVM): The basic idea behind SVM for binary classification problems is to consider the data points in their feature space, and to separate the two classes with a hyper-plane, by maximizing the shortest distance between the data points and the hyper-plane." ></td>
	<td class="line x" title="156:234	If there exists no hyperplane that can split the two labels, the soft margin version of SVM will choose a hyperplane that splits the examples as cleanly as possible, while still maximizing the distance to the nearest cleanly split examples (Joachims, 2002)." ></td>
	<td class="line x" title="157:234	We used the SVMlight package for our experiments (Joachims, 2002)." ></td>
	<td class="line x" title="158:234	For the multi-label NER classification with N classes, we learn N SVM classifiers, and use a softmax function to obtain the distribution." ></td>
	<td class="line x" title="159:234	Formally, denoting by s(y) the confidence returned by the classifier for each label y  Y , the probability of the label yi is given by p(yi|xi) = exp(s(yi))summationtext yY exp(s(y)) 1527 Similarly to MaxEnt, we subdivide name-classes into begin, continue, and unique sub-classes, and use a Viterbi parse for the sequence of highest probability." ></td>
	<td class="line x" title="160:234	The SVMlight package also implements a transductive version of the SVM algorithm." ></td>
	<td class="line x" title="161:234	We also compare our approach with the transductive SVM (Joachims, 1999) in our experimental results." ></td>
	<td class="line x" title="162:234	5 Experimental results In this paper, we use the annotated data provided by the Automatic Content Extraction (ACE) program." ></td>
	<td class="line x" title="163:234	The ACE data set is annotated for an Entity Detection task, and the annotation consists of the labeling of entity names (e.g. Powell) and mentions for each entity (e.g. pronouns such as he)." ></td>
	<td class="line x" title="164:234	In this paper, we are interested in the problem of recognition of the proper names (the named entity recognition task), and hence use only entities labeled with the type NAM (LDC, 2005)." ></td>
	<td class="line x" title="165:234	Entities are classified into seven types: Person entities are humans mentioned in a document; Organization entities are limited to established associations of people; Geo-political entities are geographical areas defined by political and/or social groups; Location entities are geographical items like landmasses and bodies of water; Facility entities refer to buildings and real estate improvements; Vehicle entities are devices used for transportation; and Weapon entities are devices used for harming or destruction." ></td>
	<td class="line x" title="166:234	We compare performances of a few algorithms: MaxEnt classifier (MaxEnt); MaxEnt classifier with standard bootstrapping (MaxEnt-SB); balanced bootstrapping based on MaxEnt classifier (MaxEnt-BB); MaxEnt with DAB (MaxEntDAB); SVM classifier (SVM); transductive SVM classifier (SVM-Trans); and DAB based on SVM classifier (SVM-DAB)." ></td>
	<td class="line x" title="167:234	No regularization is used for MaxEnt classifiers." ></td>
	<td class="line x" title="168:234	SVM classifiers use a value of 10 for parameter C (trade-off between training error and margin)." ></td>
	<td class="line x" title="169:234	Bootstrapping based algorithms are run for 30 iterations and 100 instances are selected in every iteration." ></td>
	<td class="line x" title="170:234	The evaluation measure used is the F-measure." ></td>
	<td class="line x" title="171:234	F-measure is the harmonic mean of precision and recall, and is commonly used to evaluate NER systems." ></td>
	<td class="line x" title="172:234	We use the scorer for CONLL 2003 shared task (Tjong and Meulder, 2003) where the F-measure is computed by averaging F-measures for name-classes, weighted by the number of ocCode Source Num docs NW Newswire 81 BC Broadcast conversation 52 WL Weblog 114 CTS Conversational Telephone Speech 34 Table 1: The sources, and the number of documents in each source, in the ACE 2005 data set." ></td>
	<td class="line x" title="173:234	currences." ></td>
	<td class="line x" title="174:234	5.1 Cross-source transfer The ACE 2005 data set consists of articles drawn from a variety of sources." ></td>
	<td class="line x" title="175:234	We use the four categories shown in Table 1." ></td>
	<td class="line x" title="176:234	Each category is considered to be a domain, and we consider each pair of categories as the source and the target domain in turn." ></td>
	<td class="line x" title="177:234	Figure 1 compares the performance of MaxEntSB, MaxEnt-BB and MaxEnt-DAB over multiple iterations." ></td>
	<td class="line x" title="178:234	Figure 2 compares the performance of SVM, SVM-Trans and SVM-DAB." ></td>
	<td class="line x" title="179:234	Each line in the figures represents the average F-measure across all the domains over many iterations." ></td>
	<td class="line x" title="180:234	When the termination condition is met for one domain, its F-measure remains at the value of the final iteration." ></td>
	<td class="line x" title="181:234	Despite a large number of iterations, both standard and balanced bootstrapping fail to improve performance." ></td>
	<td class="line x" title="182:234	Supervised learning performance on each domain is shown in Table 3 (by 2-fold crossvalidation with random ordering) as a reference." ></td>
	<td class="line x" title="183:234	In Table 5, we compare the F-measures obtained by different algorithms at the last iteration they were run." ></td>
	<td class="line x" title="184:234	We will discuss more on this in Section 5.3." ></td>
	<td class="line x" title="185:234	5.2 Cross-topic transfer This data set is constructed from 175 articles from the ACE 2005 corpus." ></td>
	<td class="line x" title="186:234	The data set is used to evaluate transfer across topics." ></td>
	<td class="line x" title="187:234	We manually classify the articles into 4 categories: military operations (MO), political relationship or politicians (POL), terrorism-related (TER), and those which are not in the above categories (OTH)." ></td>
	<td class="line x" title="188:234	A detailed breakdown of the number of documents in the each topic is given in Table 2." ></td>
	<td class="line x" title="189:234	Supervised learning performance on each domain is shown in Table 4 (by 2-fold crossvalidation with random ordering) as a reference." ></td>
	<td class="line x" title="190:234	Experimental results on cross-topic evaluation are shown in Table 6." ></td>
	<td class="line x" title="191:234	Figure 3 compares the performance of MaxEnt-SB, MaxEnt-BB and MaxEnt1528 57.558.0 58.559.0 59.560.0 60.561.0 61.562.0  0  5  10 15 20 25 30 35 F-measure Number of iterations MaxEnt-DABMaxEnt-SBMaxEnt-BB Figure 1: Average performance on the crosssource transfer using MaxEnt classifier." ></td>
	<td class="line x" title="192:234	35.040.0 45.050.0 55.060.0 65.070.0  1  2  3  4  5  6 F-measure Number of iterations SVM-DABSVMSVM-Trans Figure 2: Average performance on the crosssource transfer using SVM classifier." ></td>
	<td class="line x" title="193:234	66.266.4 66.666.8 67.067.2 67.467.6 67.868.0 68.2  0  5  10 15 20 25 30 35 F-measure Number of iterations MaxEnt-DABMaxEnt-SBMaxEnt-BB Figure 3: Average performance on the cross-topic transfer using MaxEnt classifier." ></td>
	<td class="line x" title="194:234	56.058.0 60.062.0 64.066.0 68.070.0 72.0  1  2  3  4 F-measure Number of iterations SVM-DABSVMSVM-Trans Figure 4: Average performance on the cross-topic transfer using SVM classifier." ></td>
	<td class="line x" title="195:234	Topic Topic description # docs MO Military operations 92 POL Political relationships 40 TER Terrorist-related 28 OTH None of the above 15 Table 2: The topics, their descriptions, and the number of training and test documents in each topic." ></td>
	<td class="line x" title="196:234	Domain MaxEnt SVM NW 82.47 82.32 BC 78.21 77.91 WL 71.41 71.84 CTS 93.90 94.01 Table 3: F-measure of supervised learning on the cross-source target domains." ></td>
	<td class="line x" title="197:234	DAB over multiple iterations." ></td>
	<td class="line x" title="198:234	Figure 4 compares the performance of SVM, SVM-Trans and SVMDAB." ></td>
	<td class="line x" title="199:234	Similar to cross-source transfer, standard and balanced bootstrapping perform badly." ></td>
	<td class="line x" title="200:234	This will be discussed in Section 5.3." ></td>
	<td class="line x" title="201:234	Domain MaxEnt SVM MO 80.52 80.6 POL 77.99 79.05 TER 81.74 82.12 OTH 71.33 72.08 Table 4: F-measure of supervised learning on the cross-topic target domains." ></td>
	<td class="line x" title="202:234	5.3 Discussion We show in our experiments that DAB outperforms standard and balanced bootstrapping, as well as the transductive SVM." ></td>
	<td class="line x" title="203:234	We have also shown DAB to be robust across two state-of-the-art classifiers, MaxEnt and SVM." ></td>
	<td class="line x" title="204:234	Balanced bootstrapping has been shown to be more effective for domain adaptation than standard bootstrapping (Jiang and Zhai, 2007) for named entity classification on a subset of the dataset used here." ></td>
	<td class="line x" title="205:234	In contrast, we found that both methods perform poorly on domain adaptation for NER." ></td>
	<td class="line x" title="206:234	In named entity classification, the names have already been segmented out and only need to be classified with the appropriate class." ></td>
	<td class="line x" title="207:234	However, for NER, the names also 1529 Train Test MaxEnt MaxEnt-SB MaxEnt-BB MaxEnt-DAB SVM SVM-Trans SVM-DAB BC CTS 74.26 74.19 74.16 81.03 72.47 43.27 75.43 BC NW 64.81 64.76 64.80 66.20 64.08 43.01 64.39 BC WL 47.81 47.80 47.76 49.52 47.98 36.58 47.93 CTS BC 46.19 46.12 46.40 54.62 46.02 40.44 49.64 CTS NW 54.25 54.15 54.26 53.07 55.63 23.61 58.99 CTS WL 40.42 40.43 40.72 41.27 39.96 29.05 42.04 NW BC 59.90 59.83 59.80 60.55 59.89 45.71 58.42 NW CTS 66.64 66.48 66.59 66.73 68.28 28.80 73.47 NW WL 52.52 52.53 52.47 53.44 52.19 36.39 52.30 WL BC 58.58 58.79 58.65 56.00 58.43 52.64 58.64 WL CTS 64.63 63.89 64.50 80.45 65.96 45.04 81.04 WL NW 67.79 67.72 67.92 68.46 68.38 43.40 69.33 Average 58.15 58.06 58.17 60.95 58.27 39.00 60.97 Table 5: F-measure of the cross-source transfer." ></td>
	<td class="line x" title="208:234	Train Test MaxEnt MaxEnt-SB MaxEnt-BB MaxEnt-DAB SVM SVM-Trans SVM-DAB MO OTH 81.70 81.48 81.57 81.95 81.78 75.68 81.94 MO POL 73.21 73.11 73.28 74.97 72.56 58.13 72.66 MO TER 68.13 68.07 68.24 69.89 69.40 65.02 69.38 OTH MO 63.30 63.80 63.94 63.91 64.18 61.03 65.45 OTH POL 67.96 68.05 67.86 69.13 68.29 56.50 70.67 OTH TER 45.34 44.82 45.30 51.06 45.71 48.77 52.87 POL MO 62.14 62.12 61.95 61.94 61.98 51.67 62.32 POL OTH 77.91 77.72 77.79 76.58 78.11 65.71 78.13 POL TER 66.55 66.38 66.08 66.38 66.44 51.29 67.24 TER MO 58.35 58.62 58.02 57.29 58.30 49.80 58.14 TER OTH 66.83 67.61 66.83 68.97 66.28 58.25 68.12 TER POL 67.34 66.94 67.16 72.00 67.54 50.55 70.65 Average 66.56 66.56 66.50 67.84 66.71 57.70 68.13 Table 6: F-measure of the cross-topic transfer." ></td>
	<td class="line x" title="209:234	need to be separated from not-a-name instances." ></td>
	<td class="line x" title="210:234	We find that the addition of not-a-name instances changes the problem the not-a-names form most of the instances classified with high confidence." ></td>
	<td class="line x" title="211:234	As a result, we find that both standard and balanced bootstrapping fail to improve performance: the selection of the most confident instances no longer provide sufficient new information to improve performance." ></td>
	<td class="line x" title="212:234	We also find that transductive SVM performs poorly on this task." ></td>
	<td class="line x" title="213:234	This is because it assumes that the unlabeled data comes from the same distribution as the labeled data." ></td>
	<td class="line x" title="214:234	In general, applying semi-supervised learning methods directly to [S+T-] type domain adaptation problems do not work and appropriate modifications need to be made to the methods." ></td>
	<td class="line x" title="215:234	The ACE 2005 data set also contains a set of ariticles from the broadcast news (BN) source which is written entirely in lower case." ></td>
	<td class="line x" title="216:234	This makes NER much more difficult." ></td>
	<td class="line x" title="217:234	However, when BN is the source domain, the capitalization information can be discovered by DAB." ></td>
	<td class="line x" title="218:234	Figures 5 and 6 show the average performance when BN is used as the source domain and all other domains in Table 1 as the target domains." ></td>
	<td class="line x" title="219:234	The source domain classifier tends to have high precision and low recall, DAB results in an increase in recall, with a small decrease in precision." ></td>
	<td class="line x" title="220:234	Testing the significance of the F-measure is not trivial because the named entities wrongly labeled by two classifiers are not directly comparable." ></td>
	<td class="line x" title="221:234	We tested the labeling disagreements instead, using a McNemar paired test." ></td>
	<td class="line x" title="222:234	The significance test is performed on the improvement of MaxEnt-DAB over MaxEnt and SVM-DAB over SVM." ></td>
	<td class="line x" title="223:234	In most of the domains for the cross-source transfer, the improvements are significant at a significance level of 0.05, using MaxEnt classifier." ></td>
	<td class="line x" title="224:234	The exceptional train-test pairs are NW-WL and WL-BC." ></td>
	<td class="line x" title="225:234	In the case of WL-BC, this means the slight decrement in performance is not statistically significant." ></td>
	<td class="line x" title="226:234	Similar result is achieved for the cross-source transfer using SVM classifier." ></td>
	<td class="line x" title="227:234	In the cross-topic transfer, the source domain and the target domain are not very different." ></td>
	<td class="line x" title="228:234	When we have a large amount of training data and little testing data, the gain of DAB can be not statistically significant, as in the case when we train with MO and POL domains." ></td>
	<td class="line x" title="229:234	1530 20.025.0 30.035.0 40.045.0 50.0  1  2  3  4  5  6  7  8  9 F-measure Number of iterations MaxEnt-DABMaxEnt-SBMaxEnt-BB Figure 5: Performance on recovering capitalization using MaxEnt classifier." ></td>
	<td class="line x" title="230:234	28.030.0 32.034.0 36.038.0 40.042.0 44.046.0  1  2  3  4 F-measure Number of iterations SVM-DABSVMSVM-Trans Figure 6: Performance on recovering capitalization using SVM classifier." ></td>
	<td class="line x" title="231:234	6 Conclusion We proposed a bootstrapping approach for domain adaptation, and we applied it to the named entity recognition task." ></td>
	<td class="line x" title="232:234	Our approach leverages on instances that serve as bridges between the source and target domain." ></td>
	<td class="line x" title="233:234	Empirically, our method outperforms baseline approaches including supervised, transductive and standard bootstrapping approaches." ></td>
	<td class="line x" title="234:234	It also outperforms balanced bootstrapping, an approach designed for domain adaptation (Jiang and Zhai, 2007)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E09-1007
Clique-Based Clustering for Improving Named Entity Recognition Systems
Ah-Pine, Julien;Jacquet, Guillaume;"></td>
	<td class="line x" title="1:216	Proceedings of the 12th Conference of the European Chapter of the ACL, pages 5159, Athens, Greece, 30 March  3 April 2009." ></td>
	<td class="line x" title="2:216	c2009 Association for Computational Linguistics Clique-Based Clustering for improving Named Entity Recognition systems Julien Ah-Pine Xerox Research Centre Europe 6, chemin de Maupertuis 38240 Meylan, France julien.ah-pine@xrce.xerox.com Guillaume Jacquet Xerox Research Centre Europe 6, chemin de Maupertuis 38240 Meylan, France guillaume.jacquet@xrce.xerox.com Abstract We propose a system which builds, in a semi-supervised manner, a resource that aims at helping a NER system to annotate corpus-specific named entities." ></td>
	<td class="line x" title="3:216	This system is based on a distributional approach which uses syntactic dependencies for measuring similarities between named entities." ></td>
	<td class="line x" title="4:216	The specificity of the presented method however, is to combine a clique-based approach and a clustering technique that amounts to a soft clustering method." ></td>
	<td class="line x" title="5:216	Our experiments show that the resource constructed by using this cliquebased clustering system allows to improve different NER systems." ></td>
	<td class="line x" title="6:216	1 Introduction In Information Extraction domain, named entities (NEs) are one of the most important textual units as they express an important part of the meaning of a document." ></td>
	<td class="line x" title="7:216	Named entity recognition (NER) is not a new domain (see MUC1 and ACE2 conferences) but some new needs appeared concerning NEs processing." ></td>
	<td class="line x" title="8:216	For instance the NE Oxford illustrates the different ambiguity types that are interesting to address:  intra-annotation ambiguity: Wikipedia lists more than 25 cities named Oxford in the world  systematic inter-annotation ambiguity: the name of cities could be used to refer to the university of this city or the football club of this city." ></td>
	<td class="line x" title="9:216	This is the case for Oxford or Newcastle  non-systematic inter-annotation ambiguity: Oxford is also a company unlike Newcastle." ></td>
	<td class="line x" title="10:216	The main goal of our system is to act in a complementary way with an existing NER system, in order to enhance its results." ></td>
	<td class="line x" title="11:216	We address two kinds 1http://www-nlpir.nist.gov/related projects/muc/ 2http://www.nist.gov/speech/tests/ace of issues: first, we want to detect and correctly annotate corpus-specific NEs3 that the NER system could have missed; second, we want to correct some wrong annotations provided by the existing NER system due to ambiguity." ></td>
	<td class="line x" title="12:216	In section 3, we give some examples of such corrections." ></td>
	<td class="line x" title="13:216	The paper is organized as follows." ></td>
	<td class="line x" title="14:216	We present, in section 2, the global architecture of our system and from2.1 to2.6, we give details about each of its steps." ></td>
	<td class="line x" title="15:216	In section 3, we present the evaluation of our approach when it is combined with other classic NER systems." ></td>
	<td class="line x" title="16:216	We show that the resulting hybrid systems perform better with respect to F-measure." ></td>
	<td class="line x" title="17:216	In the best case, the latter increased by 4.84 points." ></td>
	<td class="line x" title="18:216	Furthermore, we give examples of successful correction of NEs annotation thanks to our approach." ></td>
	<td class="line x" title="19:216	Then, in section 4, we discuss about related works." ></td>
	<td class="line x" title="20:216	Finally we sum up the main points of this paper in section 5." ></td>
	<td class="line x" title="21:216	2 Description of the system Given a corpus, the main objectives of our system are: to detect potential NEs; to compute the possible annotations for each NE and then; to annotate each occurrence of these NEs with the right annotation by analyzing its local context." ></td>
	<td class="line x" title="22:216	We assume that this corpus dependent approach allows an easier NE annotation." ></td>
	<td class="line x" title="23:216	Indeed, even if a NE such as Oxford can have many annotation types, it will certainly have less annotation possibilities in a specific corpus." ></td>
	<td class="line x" title="24:216	Figure 1 presents the global architecture of our system." ></td>
	<td class="line x" title="25:216	The most important part concerns steps 3 (2.3) and 4 (2.4)." ></td>
	<td class="line x" title="26:216	The aim of these subprocesses is to group NEs which have the same annotation with respect to a given context." ></td>
	<td class="line x" title="27:216	On the one hand, clique-based methods (see2.3 for 3In our definition a corpus-specific NE is the one which doesnotappearinaclassicNEslexicon." ></td>
	<td class="line x" title="28:216	Recentnewsarticles for instance, are often constituted of NEs that are not in a classic NEs lexicon." ></td>
	<td class="line x" title="29:216	51 Figure 1: General description of our system details on cliques) are interesting as they allow the same NE to be in different cliques." ></td>
	<td class="line x" title="30:216	In other words, cliques allow to represent the different possible annotations of a NE." ></td>
	<td class="line x" title="31:216	The clique-based approach drawback however, is the over production of cliques which corresponds to an artificial over production of possible annotations for a NE." ></td>
	<td class="line x" title="32:216	On the other hand, clustering methods aim at structuring a data set and such techniques can be seen as data compression processes." ></td>
	<td class="line x" title="33:216	However, a simple NEs hard clustering doesnt allow a NE to be in several clusters and thus to express its different annotations." ></td>
	<td class="line x" title="34:216	Then, our proposal is to combine both methods in a clique-based clustering framework." ></td>
	<td class="line x" title="35:216	This combination leads to a soft-clustering approach that we denote CBC system." ></td>
	<td class="line x" title="36:216	The following paragraphs, from 2.1 to 2.6, describe the respective steps mentioned in Figure 1." ></td>
	<td class="line x" title="37:216	2.1 Detection of potential Named Entities Different methods exist for detecting potential NEs." ></td>
	<td class="line x" title="38:216	In our system, we used some lexicosyntactic constraints to extract expressions from a corpus because it allows to detect some corpusspecific NEs." ></td>
	<td class="line x" title="39:216	In our approach, a potential NE is a noun starting with an upper-case letter or a noun phrase which is (see (Ehrmann and Jacquet, 2007) for similar use):  a governor argument of an attribute syntactic relation with a noun as governee argument (e.g. president attributeGeorge Bush)  a governee argument of a modifier syntactic relation with a noun as a governor argument (e.g. company modifierCoca-Cola)." ></td>
	<td class="line x" title="40:216	The list of potential NEs extracted from the corpus will be denoted NE and the number of NEs |NE|." ></td>
	<td class="line x" title="41:216	2.2 Distributional space of NEs The distributional approach aims at evaluating a distance between words based on their syntactic distribution." ></td>
	<td class="line x" title="42:216	This method assumes that words which appear in the same contexts are semantically similar (Harris, 1951)." ></td>
	<td class="line x" title="43:216	To construct the distributional space associated to a corpus, we use a robust parser (in our experiments, we used XIP parser (At et al., 2002)) to extract chunks (i.e. nouns, noun phrases, ) and syntactic dependencies between these chunks." ></td>
	<td class="line x" title="44:216	Given this parsers output, we identify triple instances." ></td>
	<td class="line x" title="45:216	Each triple has the form w1.R.w2 where w1 and w2 are chunks and R is a syntactic relation (Lin, 1998), (Kilgarriff et al., 2004)." ></td>
	<td class="line x" title="46:216	One triple gives two contexts (1.w1.R and 2.w2.R) and two chunks (w1 and w2)." ></td>
	<td class="line x" title="47:216	Then, we only select chunks w which belong to NE." ></td>
	<td class="line x" title="48:216	Each point in the distributional space is a NE and each dimension is a syntactic context." ></td>
	<td class="line x" title="49:216	CT denotes the set of all syntactic contexts and|CT|represents its cardinal." ></td>
	<td class="line x" title="50:216	We illustrate this construction on the sentence provide Albania with food aid." ></td>
	<td class="line x" title="51:216	We obtain the three following triples (note that aid and food aid are considered as two different chunks): provide VERBI-OBJAlbania NOUN provide VERBPREP WITHaid NOUN provide VERBPREP WITHfood aid NP From these triples, we have the following chunks and contexts4: Chunks: Contexts: provide VERB 1.provide VERB.I-OBJ Albania NOUN 1.provide VERB.PREP WITH aid NOUN 2.Albania NOUN.I-OBJ food aid NP 2.aid NOUN.PREP WITH 2.food aid NP.PREP WITH According to the NEs detection method described previously, we only keep the chunks and contexts which are in bold in the above table." ></td>
	<td class="line x" title="52:216	4In the context 1.VERB:provide.I-OBJ, the figure 1 means that the verb provide is the governor argument of the Indirect OBJect relation." ></td>
	<td class="line x" title="53:216	52 We also use an heuristic in order to reduce the over production of chunks and contexts: in our experiments for example, each NE and each context should appear more than 10 times in the corpus for being considered." ></td>
	<td class="line x" title="54:216	D is the resulting (|NE||CT|) NE-Context matrix where ei : i = 1,,|NE| is a NE and cj : j = 1,,|CT|is a syntactic context." ></td>
	<td class="line x" title="55:216	Then we have: D(ei,cj) = Nb." ></td>
	<td class="line x" title="56:216	of occ." ></td>
	<td class="line x" title="57:216	of cj associated to ei (1) 2.3 Cliques of NEs computation A clique in a graph is a set of pairwise adjacent nodes which is equivalent to a complete subgraph." ></td>
	<td class="line x" title="58:216	A maximal clique is a clique that is not a subset of any other clique." ></td>
	<td class="line x" title="59:216	Maximal cliques computation was already employed for semantic space representation (Ploux and Victorri, 1998)." ></td>
	<td class="line x" title="60:216	In this work, cliques of lexical units are used to represent a precise meaning." ></td>
	<td class="line x" title="61:216	Similarly, we compute cliques of NEs in order to represent a precise annotation." ></td>
	<td class="line x" title="62:216	For example, Oxford is an ambiguous NE but a clique such as <Cambridge, Oxford, Edinburgh University, Edinburgh, Oxford University> allows to focus on the specific annotation <organization> (see (Ehrmann and Jacquet, 2007) for similar use)." ></td>
	<td class="line x" title="63:216	Given the distributional space described in the previous paragraph, we use a probabilistic framework for computing similarities between NEs." ></td>
	<td class="line x" title="64:216	The approach that we propose is inspired from the language modeling framework introduced in the information retrieval field (see for example (Lavrenko and Croft, 2003))." ></td>
	<td class="line x" title="65:216	Then, we construct cliques of NEs based on these similarities." ></td>
	<td class="line x" title="66:216	2.3.1 Similarity measures between NEs We first compute the maximum likelihood estimation for a NE ei to be associated with a context cj: Pml(cj|ei) = D(ei,cj)|ei| , where |ei| = summationtext|CT| j=1 D(ei,cj) is the total occurrences of the NE ei in the corpus." ></td>
	<td class="line x" title="67:216	This leads to sparse data which is not suitable for measuring similarities." ></td>
	<td class="line x" title="68:216	In order to counter this problem, we use the Jelinek-Mercer smoothing method: Dprime(ei,cj) = Pml(cj|ei) + (1  )Pml(cj|CORP) where CORP is the corpus and Pml(cj|CORP) = P i D(ei,cj)P i,j D(ei,cj) . In our experiments we took  = 0.5." ></td>
	<td class="line x" title="69:216	Given Dprime, we then use the cross-entropy as a similarity measure between NEs." ></td>
	<td class="line x" title="70:216	Let us denote by s this similarity matrix, we have: s(ei,eprimei) = summationdisplay cjCT Dprime(ei,cj)log(Dprime(eiprime,cj)) (2) 2.3.2 From similarity matrix to adjacency matrix Next, we convert s into an adjacency matrix denoted s. In a first step, we binarize s as follows." ></td>
	<td class="line x" title="71:216	Letusdenote{ei1,,ei|NE|}, thelistofNEs ranked according to the descending order of their similarity with ei." ></td>
	<td class="line x" title="72:216	Then, L(ei) is the list of NEs which are considered as the nearest neighbors of ei according to the following definition: L(ei) = (3) {ei1,,eip : summationtextp iprime=1 s(ei,e i iprime)summationtext |NE| iprime=1 s(ei,eiprime) a;pb} where a  [0,1] and b {1,,|NE|}." ></td>
	<td class="line x" title="73:216	L(ei) gathersthemostsignificantnearestneighborsofei by choosing the ones which bring the a most relevant similarities providing that the neighborhoods size doesnt exceed b. This approach can be seen as a flexible k-nearest neighbor method." ></td>
	<td class="line x" title="74:216	In our experiments we chose a = 20% and b = 10." ></td>
	<td class="line x" title="75:216	Finally, we symmetrize the similarity matrix as follows and we obtain s: s(ei,eiprime) = braceleftbigg 1 if e iprime L(ei) or eiL(eiprime) 0 otherwise (4) 2.3.3 Cliques computation Given s, the adjacency matrix between NEs, we compute the set of maximal cliques of NEs denoted CLI." ></td>
	<td class="line x" title="76:216	Then, we construct the matrix T of general term: T(clik,ei) = braceleftbigg 1 if e iclik 0 otherwise (5) where clik is an element of CLI." ></td>
	<td class="line x" title="77:216	T will be the input matrix for the clustering method." ></td>
	<td class="line x" title="78:216	In the following, we also use clik for denoting the vector represented by (T(clik,e1),,T(clik,e|NE|))." ></td>
	<td class="line x" title="79:216	Figure 2 shows some cliques which contain Oxford that we can obtain with this method." ></td>
	<td class="line x" title="80:216	This figure also illustrates the over production of cliques since at least cli8, cli10 and cli12 can be annotated as <organization>." ></td>
	<td class="line x" title="81:216	53 Figure 2: Examples of cliques containing Oxford 2.4 Cliques clustering We use a clustering technique in order to group cliques of NEs which are mutually highly similar." ></td>
	<td class="line x" title="82:216	The clusters of cliques which contain a NE allow to find the different possible annotations of this NE." ></td>
	<td class="line x" title="83:216	This clustering technique must be able to construct pure clusters in order to have precise annotations." ></td>
	<td class="line x" title="84:216	In that case, it is desirable to avoid fixing the number of clusters." ></td>
	<td class="line x" title="85:216	Thats the reason why we propose to use the Relational Analysis approach described below." ></td>
	<td class="line x" title="86:216	2.4.1 The Relational Analysis approach We propose to apply the Relational Analysis approach (RA) which is a clustering model that doesnt require to fix the number of clusters (Michaud and Marcotorchino, 1980), (Bedecarrax and Warnesson, 1989)." ></td>
	<td class="line x" title="87:216	This approach takes as input a similarity matrix." ></td>
	<td class="line x" title="88:216	In our context, since we want to cluster cliques of NEs, the corresponding similarity matrix S between cliques is given by the dot products matrix taken from T: S = T Tprime." ></td>
	<td class="line x" title="89:216	The general term of this similarity matrix is: S(clik,clikprime) = Skkprime =clik,clikprime." ></td>
	<td class="line x" title="90:216	Then, we want to maximize the following clustering function: (S,X) = (6) |CLI|summationdisplay k,kprime=1 parenleftBigg Skkprime  summationtext (kprimeprime,kprimeprimeprime)S+ Skprimeprimekprimeprimeprime |S+| parenrightBigg bracehtipupleft bracehtipdownrightbracehtipdownleft bracehtipupright contkkprime Xkkprime where S+ ={(clik,clikprime) : Skkprime > 0}." ></td>
	<td class="line x" title="91:216	Inotherwords,clik andclikprime havemorechances to be in the same cluster providing that their similarity measure, Skkprime, is greater or equal to the mean average of positive similarities." ></td>
	<td class="line x" title="92:216	X is the solution we are looking for." ></td>
	<td class="line x" title="93:216	It is a binary relational matrix with general term: Xkkprime = 1, ifclik is in the same cluster asclikprime; andXkkprime = 0, otherwise." ></td>
	<td class="line x" title="94:216	X represents an equivalence relation." ></td>
	<td class="line x" title="95:216	Thus, it must respect the following properties:  binarity: Xkkprime {0,1};k,kprime,  reflexivity: Xkk = 1;k,  symmetry: Xkkprime Xkprimek = 0;k,kprime,  transitivity: Xkkprime + Xkprimekprimeprime  Xkkprimeprime  1;k,kprime,kprimeprime." ></td>
	<td class="line x" title="96:216	As the objective function is linear with respect toX and as the constraints thatX must respect are linear equations, we can solve the clustering problem using an integer linear programming solver." ></td>
	<td class="line x" title="97:216	However, this problem is NP-hard." ></td>
	<td class="line x" title="98:216	As a result, in practice, we use heuristics for dealing with large data sets." ></td>
	<td class="line x" title="99:216	2.4.2 The Relational Analysis heuristic The presented heuristic is quite similar to another algorithm described in (Hartigan, 1975) known as the leader algorithm." ></td>
	<td class="line x" title="100:216	But unlike this last approach which is based upon euclidean distances and inertial criteria, the RA heuristic aims at maximizing the criterion given in (6)." ></td>
	<td class="line x" title="101:216	A sketch of this heuristic is given in Algorithm 1, (see (Marcotorchino and Michaud, 1981) for further details)." ></td>
	<td class="line x" title="102:216	Algorithm 1 RA heuristic Require: nbitr = number of iterations; max = maximal number of clusters; S the similarity matrix m P (k,kprime)S+ Skkprime |S+| Take the first clique clik as the first element of the first cluster  = 1 where  is the current number of cluster for q = 1 to nbitr do for k = 1 to|CLI|do for l = 1 to  do Compute the contribution of clique clik with cluster clul: contl =Pcli kprimeclul (Skkprime m) end for clul is the cluster id which has the highest contribution with clique clik and contl is the corresponding contribution value if (contl < (Skkm))( < max) then Create a new cluster where clique clik is the first element and +1 else Assign clique clik to cluster clul if the cluster where was taken clik before its new assignment, is empty then 1 end if end if end for end for We have to provide a number of iterations 54 or/andadeltathresholdinordertohaveanapproximate solution in a reasonable processing time." ></td>
	<td class="line x" title="103:216	Besides, it is also required a maximum number of clusters but since we dont want to fix this parameter, we put by default max =|CLI|." ></td>
	<td class="line x" title="104:216	Basically, this heuristic has a O(nbitrmax |CLI|) computation cost." ></td>
	<td class="line x" title="105:216	In general terms, we can assume that nbitr << |CLI|, but not max << |CLI|." ></td>
	<td class="line x" title="106:216	Thus, in the worst case, the algorithm has a O(max|CLI|) computation cost." ></td>
	<td class="line x" title="107:216	Figure 3 gives some examples of clusters of cliques5 obtained using the RA approach." ></td>
	<td class="line x" title="108:216	Figure 3: Examples of clusters of cliques (only the NEs are represented) and their associated contexts 2.5 NE resource construction using the CBC systems outputs Now, we want to exploit the clusters of cliques in order to annotate NE occurrences." ></td>
	<td class="line x" title="109:216	Then, we need toconstructaNEresourcewhereforeachpair(NE x syntactic context) we have an annotation." ></td>
	<td class="line x" title="110:216	To this end, we need first, to assign a cluster to each pair (NE x syntactic context) (2.5.1) and second, to assign each cluster an annotation (2.5.2)." ></td>
	<td class="line x" title="111:216	2.5.1 Cluster assignment to each pair (NE x syntactic context) For each cluster clul we provide a score Fc(cj,clul) for each context cj and a score 5We only represent the NEs and their frequency in the cluster which corresponds to the number of cliques which contain the NEs." ></td>
	<td class="line x" title="112:216	Furthermore, we represent the most relevant contexts for this cluster according to equation (7) introduced in the following." ></td>
	<td class="line x" title="113:216	Fe(ei,clul) for each NE ei." ></td>
	<td class="line x" title="114:216	These scores6 are given by: Fc(cj,clul) = (7) summationdisplay eiclul D(ei,cj)summationtext |NE| i=1 D(ei,cj) summationdisplay eiclul 1{D(ei,cj)negationslash=0} where 1{P} equals 1 if P is true and 0 otherwise." ></td>
	<td class="line x" title="115:216	Fe(ei,clul) = #(clul,ei) (8) Given a NE ei and a syntactic context cj, we now introduce the contextual cluster assignment matrix Actxt(ei,cj) as follows: Actxt(ei,cj) = clu where: clu = Argmax{clul:clulownerei;Fe(ei,clul)>1}Fc(cj,clul)." ></td>
	<td class="line x" title="116:216	In other words, clu is the cluster for which we find more than one occurrence of ei and the highest score related to the context cj." ></td>
	<td class="line x" title="117:216	Furthermore, we compute a default cluster assignment matrix Adef, which does not depend on the local context: Adef(ei) = clu where: clu = Argmax{clul:clulowner{clik:clikownerei}}|clik|." ></td>
	<td class="line x" title="118:216	In other words, clu is the cluster containing the biggest clique clik containing ei." ></td>
	<td class="line x" title="119:216	2.5.2 Clusters annotation So far, the different steps that we have introduced were unsupervised." ></td>
	<td class="line x" title="120:216	In this paragraph, our aim is to give a correct annotation to each cluster (hence, to all NEs in this cluster)." ></td>
	<td class="line x" title="121:216	To this end, we need some annotation seeds and we propose two different semi-supervised approaches (regarding the classification given in (Nadeau and Sekine, 2007))." ></td>
	<td class="line x" title="122:216	The first one is the manual annotation of some clusters." ></td>
	<td class="line x" title="123:216	The second one proposes an automatic cluster annotation and assumes that we have some NEs that are already annotated." ></td>
	<td class="line x" title="124:216	Manual annotation of clusters This method is fastidious but it is the best way to match the corpus data with a specific guidelines for annotating NEs." ></td>
	<td class="line x" title="125:216	It also allows to identify new types of annotation." ></td>
	<td class="line x" title="126:216	We used the ACE2007 guidelines for manually annotating each cluster." ></td>
	<td class="line x" title="127:216	However, our CBC system leads to a high number of clusters of cliques and we cant annotate each of them." ></td>
	<td class="line x" title="128:216	Fortunately, it also leads to a distribution of the clusters size (number of cliques by cluster) which is 6For data fusion tasks in information retrieval field, the scoring method in equation (7) is denoted CombMNZ (Fox and Shaw, 1994)." ></td>
	<td class="line x" title="129:216	Other scoring approaches can be used see for example (Cucchiarelli and Velardi, 2001)." ></td>
	<td class="line x" title="130:216	55 similar to a Zipf distribution." ></td>
	<td class="line x" title="131:216	Consequently, in our experiments, if we annotate the 100 biggest clusters, we annotate around eighty percent of the detected NEs (see3)." ></td>
	<td class="line x" title="132:216	Automatic annotation of clusters We suppose in this context that many NEs in NE are already annotated." ></td>
	<td class="line x" title="133:216	Thus, under this assumption, we have in each cluster provided by the CBC system, both annotated and non-annotated NEs." ></td>
	<td class="line x" title="134:216	Our goal is to exploit the available annotations for refining the annotation of a cluster by implicitly taking into account the syntactic contexts and for propagating the available annotations to NEs which have no annotation." ></td>
	<td class="line x" title="135:216	Given a clusterclul of cliques, #(clul,ei) is the weight of the NE ei in this cluster: it is the number of cliques in clul that contain ei." ></td>
	<td class="line x" title="136:216	For all annotations ap in the set of all possible annotations AN, we compute its associated score in cluster clul: it is the sum of the weights of NEs in clul that is annotated ap." ></td>
	<td class="line x" title="137:216	Then, if the maximal annotation score is greater thanasimplemajority(half)ofthetotalvotes7, we assign the corresponding annotation to the cluster." ></td>
	<td class="line x" title="138:216	We precise that the annotation <none>8 is processed in the same way as any other annotations." ></td>
	<td class="line x" title="139:216	Thus, a cluster can be globally annotated <none>." ></td>
	<td class="line x" title="140:216	The limit of this automatic approach is that it doesnt allow to annotate new NE types than the ones already available." ></td>
	<td class="line x" title="141:216	In the following, we will denote by Aclu(clul) the annotation of the cluster clul." ></td>
	<td class="line x" title="142:216	The cluster annotation matrix Aclu associated to the contextual cluster assignment matrix Actxt and the default cluster assignment matrix Adef introduced previously will be called the CBC systems NE resource (or shortly the NE resource)." ></td>
	<td class="line x" title="143:216	2.6 NEs annotation processes using the NE resource In this paragraph, we describe how, given the CBC systems NE resource, we annotate occurrences of NEs in the studied corpus with respect to its local context." ></td>
	<td class="line x" title="144:216	We precise that for an occurrence of a NE ei its associated local context is the set of syntactical dependencies cj in which ei is involved." ></td>
	<td class="line x" title="145:216	7The total votes number is given byP eiclul #(clul,ei)." ></td>
	<td class="line x" title="146:216	8The NEs which dont have any annotation." ></td>
	<td class="line x" title="147:216	2.6.1 NEs annotation process for the CBC system Given a NE occurrence and its local context we can use Actxt(ei,cj) and Adef(ei) in order to get the default annotation Aclu(Adef(ei)) and the list of contextual annotations{Aclu(Actxt(ei,cj))}j. Then for annotating this NE occurrence using our NE resource, we apply the following rules:  if the list of contextual annotations {Aclu(Actxt(ei,cj))}j is conflictual, we annotate the NE occurrence as <none>,  if the list of contextual annotations is nonconflictual, then we use the corresponding annotation to annotate the NE occurrence  if the list of contextual annotations is empty, we use the default annotation Aclu(Adef(ei))." ></td>
	<td class="line x" title="148:216	TheNEresourceplustheannotationprocessdescribed in this paragraph lead to a NER system based on the CBC system." ></td>
	<td class="line x" title="149:216	This NER system will be called CBC-NER system and it will be tested in our experiments both alone and as a complementary resource." ></td>
	<td class="line x" title="150:216	2.6.2 NEs annotation process for an hybrid system We place ourselves into an hybrid situation where we have two NER systems (NER 1 + NER 2) which provide two different lists of annotated NEs." ></td>
	<td class="line x" title="151:216	Wewanttocombinethesetwosystemswhen annotating NEs occurrences." ></td>
	<td class="line x" title="152:216	Therefore, we resolve any conflicts by applying the following rules:  IfthesameNEoccurrencehastwodifferentannotations from the two systems then there are two cases." ></td>
	<td class="line x" title="153:216	If one of the two system is CBCNER system then we take its annotation; otherwise we take the annotation provided by the NER system which gave the best precision." ></td>
	<td class="line x" title="154:216	 If a NE occurrence is included in another one we only keep the biggest one and its annotation." ></td>
	<td class="line x" title="155:216	For example, if Jacques Chirac is annotated <person> by one system and Chirac by <person> by the other system, then we only keep the first annotation." ></td>
	<td class="line x" title="156:216	 If two NE occurrences are contiguous and have the same annotation, we merge the two NEs in one NE occurrence." ></td>
	<td class="line x" title="157:216	3 Experiments The system described in this paper rather target corpus-specific NE annotation." ></td>
	<td class="line x" title="158:216	Therefore, our ex56 periments will deal with a corpus of recent news articles (see (Shinyama and Sekine, 2004) for motivations regarding our corpus choice) rather than well-known annotated corpora." ></td>
	<td class="line x" title="159:216	Our corpus is constituted of news in English published on the web during two weeks in June 2008." ></td>
	<td class="line x" title="160:216	This corpus is constituted of around 300,000 words (10Mb) which doesnt represent a very large corpus." ></td>
	<td class="line x" title="161:216	These texts were taken from various press sources and they involve different themes (sports, technology, )." ></td>
	<td class="line x" title="162:216	We extracted randomly a subset of articles and manually annotated 916 NEs (in our experiments, we deal with three types of annotation namely <person>, <organization> and <location>)." ></td>
	<td class="line x" title="163:216	This subset constitutes our test set." ></td>
	<td class="line x" title="164:216	In our experiments, first, we applied the XIP parser (At et al., 2002) to the whole corpus in order to construct the frequency matrix D given by (1)." ></td>
	<td class="line x" title="165:216	Next, we computed the similarity matrix between NEs according to (2) in order to obtain sdefined by (4)." ></td>
	<td class="line x" title="166:216	Using the latter, we computed cliques of NEs that allow us to obtain the assignment matrixT given by (5)." ></td>
	<td class="line x" title="167:216	Then we applied the clustering heuristic described in Algorithm 1." ></td>
	<td class="line x" title="168:216	At this stage, we want to build the NE resource using the clusters of cliques." ></td>
	<td class="line x" title="169:216	Therefore, as described in 2.5, we applied two kinds of clusters annotations: the manual and the automatic processes." ></td>
	<td class="line x" title="170:216	For the first one, we manually annotated the 100 biggest clusters of cliques." ></td>
	<td class="line x" title="171:216	For the second one, we exploited the annotations provided by XIP NER (Brun and Hag`ege, 2004) and we propagated these annotations to the different clusters (see2.5.2)." ></td>
	<td class="line x" title="172:216	The different materials that we obtained constitute the CBC systems NE resource." ></td>
	<td class="line x" title="173:216	Our aim now istoexploitthisresourceandtoshowthatitallows to improve the performances of different classic NER systems." ></td>
	<td class="line o" title="174:216	The different NER systems that we tested are the following ones:  CBC-NER system M (in short CBC M) based on the CBC systems NE resource using the manual cluster annotation (line 1 in Table 1),  CBC-NER system A (in short CBC A) based on the CBC systems NE resource using the automatic cluster annotation (line 1 in Table 1),  XIP NER or in short XIP (Brun and Hag`ege, 2004) (line 2 in Table 1),  Stanford NER (or in short Stanford) associated to the following model provided by the tool and which was trained on different news Systems Prec." ></td>
	<td class="line x" title="175:216	Rec." ></td>
	<td class="line oc" title="176:216	F-me. 1 CBC-NER system M 71.67 23.47 35.36CBC-NER system A 70.66 32.86 44.86 2 XIP NER 77.77 56.55 65.48 XIP + CBC M 78.41 60.26 68.15 XIP + CBC A 76.31 60.48 67.48 3 Stanford NER 67.94 68.01 67.97 Stanford + CBC M 69.40 71.07 70.23 Stanford + CBC A 70.09 72.93 71.48 4 GATE NER 63.30 56.88 59.92 GATE + CBC M 66.43 61.79 64.03 GATE + CBC A 66.51 63.10 64.76 5 Stanford + XIP 72.85 75.87 74.33 Stanford + XIP + CBC M 72.94 77.70 75.24 Stanford + XIP + CBC A 73.55 78.93 76.15 6 GATE + XIP 69.38 66.04 67.67 GATE + XIP + CBC M 69.62 67.79 68.69 GATE + XIP + CBC A 69.87 69.10 69.48 7 GATE + Stanford 63.12 69.32 66.07 GATE + Stanford + CBC M 65.09 72.05 68.39 GATE + Stanford + CBC A 65.66 73.25 69.25 Table 1: Results given by different hybrid NER systems and coupled with the CBC-NER system corpora (CoNLL, MUC6, MUC7 and ACE): ner-eng-ie.crf-3-all2008-distsim.ser.gz (Finkel et al., 2005) (line 3 in Table 1),  GATE NER or in short GATE (Cunningham et al., 2002) (line 4 in Table 1),  and several hybrid systems which are given by the combination of pairs taken among the set of the three last-mentioned NER systems (lines 5 to 7 in Table 1)." ></td>
	<td class="line x" title="177:216	Notice that these baseline hybrid systems use the annotation combination process described in2.6.1." ></td>
	<td class="line x" title="178:216	In Table 1 we first reported in each line, the results given by each system when they are applied alone (figures in italics)." ></td>
	<td class="line x" title="179:216	These performances represent our baselines." ></td>
	<td class="line x" title="180:216	Second, we tested for each baseline system, an extended hybrid system that integrates the CBC-NER systems (with respect to the combination process detailed in2.6.2)." ></td>
	<td class="line x" title="181:216	The first two lines of Table 1 show that the two CBC-NER systems alone lead to rather poor results." ></td>
	<td class="line x" title="182:216	However, our aim is to show that the CBC-NER system is, despite its low performances alone, complementary to other basic NER systems." ></td>
	<td class="line x" title="183:216	In other words, we want to show that the exploitation of the CBC systems NE resource is beneficial and non-redundant compared to other baseline NER systems." ></td>
	<td class="line x" title="184:216	This is actually what we obtained in Table 1 as for each line from 2 to 7, the extended hybrid systems that integrate the CBC-NER systems (M or 57 A) always perform better than the baseline either in terms of precision9 or recall." ></td>
	<td class="line x" title="185:216	For each line, we put in bold the best performance according to the F-measure." ></td>
	<td class="line x" title="186:216	These results allow us to show that the NE resource built using the CBC system is complementary to any baseline NER systems and that it allows to improve the results of the latter." ></td>
	<td class="line x" title="187:216	InordertoillustratewhytheCBC-NERsystems arebeneficial, wegivebelowsomeexamplestaken from the test corpus for which the CBC system A had allowed to improve the performances by respectively disambiguating or correcting a wrong annotation or detecting corpus-specific NEs." ></td>
	<td class="line o" title="188:216	First, in the sentence From the start, his parents, Lourdes and Hemery, were with him., the baseline hybrid system Stanford + XIP annotated the ambiguous NE Lourdes as <location> whereas Stanford + XIP + CBC A gave the correct annotation <person>." ></td>
	<td class="line o" title="189:216	Second, in the sentence Got 3 percent chance of survival, what ya gonna do? The back read, A) Fight Through, b) Stay Strong, c) Overcome Because I Am a Warrior., the baseline hybrid system Stanford + XIP annotated Warrior as <organization> whereas Stanford + XIP + CBC A corrected this annotation with <none>." ></td>
	<td class="line o" title="190:216	Finally, in the sentence Matthew, also a favorite to win in his fifth and final appearance, was stunningly eliminated during the semifinal round Friday when he misspelled secernent., the baseline hybrid system Stanford + XIP didnt give any annotation to Matthew whereas Stanford + XIP + CBC A allowed to give the annotation <person>." ></td>
	<td class="line x" title="191:216	4 Related works Many previous works exist in NEs recognition and classification." ></td>
	<td class="line x" title="192:216	However, most of them do not build a NEs resource but exploit external gazetteers (Bunescu and Pasca, 2006), (Cucerzan, 2007)." ></td>
	<td class="line x" title="193:216	A recent overview of the field is given in (Nadeau and Sekine, 2007)." ></td>
	<td class="line x" title="194:216	According to this paper, we can classify our method in the category of semi-supervised approaches." ></td>
	<td class="line x" title="195:216	Our proposal is close to (Cucchiarelli and Velardi, 2001) as it uses syntactic relations (2.2) and as it relies on existing NER systems (2.6.2)." ></td>
	<td class="line x" title="196:216	However, the particularity of our method concerns the clustering of 9Except for XIP+CBC A in line 2 where the precision is slightly lower than XIPs one." ></td>
	<td class="line x" title="197:216	cliques of NEs that allows both to represent the different annotations of the NEs and to group the latter with respect to one precise annotation according to a local context." ></td>
	<td class="line x" title="198:216	Regarding this aspect, (Lin and Pantel, 2001) and (Ngomo, 2008) also use a clique computation step and a clique merging method." ></td>
	<td class="line x" title="199:216	However, they do not deal with ambiguity of lexical units nor with NEs." ></td>
	<td class="line x" title="200:216	This means that, in their system, a lexical unit can be in only one merged clique." ></td>
	<td class="line x" title="201:216	From a methodological point of view, our proposal is also close to (Ehrmann and Jacquet, 2007) as the latter proposes a system for NEs finegrained annotation, which is also corpus dependent." ></td>
	<td class="line x" title="202:216	However, in the present paper we use all syntactic relations for measuring the similarity between NEs whereas in the previous mentioned work, only specific syntactic relations were exploited." ></td>
	<td class="line x" title="203:216	Moreover, we use clustering techniques for dealing with the issue related to over production of cliques." ></td>
	<td class="line x" title="204:216	In this paper, we construct a NE resource from the corpus that we want to analyze." ></td>
	<td class="line x" title="205:216	In that context, (Pasca, 2004) presents a lightly supervised method for acquiring NEs in arbitrary categories from unstructured text of Web documents." ></td>
	<td class="line x" title="206:216	However, Pasca wants to improve web search whereas we aim at annotating specific NEs of an analyzed corpus." ></td>
	<td class="line x" title="207:216	Besides, as we want to focus on corpus-specific NEs, our work is also related to (Shinyama and Sekine, 2004)." ></td>
	<td class="line x" title="208:216	In this work, the authors found a significant correlation between the similarity of the time series distribution of a word and the likelihood of being a NE." ></td>
	<td class="line x" title="209:216	This result motivated our choice to test our approach on recent news articles rather than on well-known annotated corpora." ></td>
	<td class="line x" title="210:216	5 Conclusion We propose a system that allows to improve NE recognition." ></td>
	<td class="line x" title="211:216	The core of this system is a cliquebased clustering method based upon a distributional approach." ></td>
	<td class="line x" title="212:216	It allows to extract, analyze and discover highly relevant information for corpusspecific NEs annotation." ></td>
	<td class="line x" title="213:216	As we have shown in our experiments, this system combined with another one can lead to strong improvements." ></td>
	<td class="line x" title="214:216	Other applications are currently addressed in our team using this approach." ></td>
	<td class="line x" title="215:216	For example, we intend to use the concept of clique-based clustering as a soft clustering method for other issues." ></td>
	<td class="line x" title="216:216	58" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E09-1011
Syntactic Phrase Reordering for English-to-Arabic Statistical Machine Translation
Badr, Ibrahim;Zbib, Rabih;Glass, James R.;"></td>
	<td class="line x" title="1:224	Proceedings of the 12th Conference of the European Chapter of the ACL, pages 8693, Athens, Greece, 30 March  3 April 2009." ></td>
	<td class="line x" title="2:224	c2009 Association for Computational Linguistics Syntactic Phrase Reordering for English-to-Arabic Statistical Machine Translation Ibrahim Badr Rabih Zbib Computer Science and Artificial Intelligence Lab Massachusetts Institute of Technology Cambridge, MA 02139, USA {iab02, rabih, glass}@csail.mit.edu James Glass Abstract Syntactic Reordering of the source language to better match the phrase structure of the target language has been shown to improve the performance of phrase-based Statistical Machine Translation." ></td>
	<td class="line x" title="3:224	This paper applies syntactic reordering to English-to-Arabic translation." ></td>
	<td class="line x" title="4:224	It introduces reordering rules, and motivates them linguistically." ></td>
	<td class="line x" title="5:224	It also studies the effect of combining reordering with Arabic morphological segmentation, a preprocessing technique that has been shown to improve Arabic-English and EnglishArabic translation." ></td>
	<td class="line x" title="6:224	We report on results in the news text domain, the UN text domain and in the spoken travel domain." ></td>
	<td class="line x" title="7:224	1 Introduction Phrase-based Statistical Machine Translation has proven to be a robust and effective approach to machine translation, providing good performance without the need for explicit linguistic information." ></td>
	<td class="line x" title="8:224	Phrase-based SMT systems, however, have limited capabilities in dealing with long distance phenomena, since they rely on local alignments." ></td>
	<td class="line x" title="9:224	Automatically learned reordering models, which can be conditioned on lexical items from both the source and the target, provide some limited reordering capability when added to SMT systems." ></td>
	<td class="line x" title="10:224	One approach that explicitly deals with long distance reordering is to reorder the source side to better match the target side, using predefined rules." ></td>
	<td class="line x" title="11:224	The reordered source is then used as input to the phrase-based SMT system." ></td>
	<td class="line x" title="12:224	This approach indirectly incorporates structure information since the reordering rules are applied on the parse trees of the source sentence." ></td>
	<td class="line x" title="13:224	Obviously, the same reordering has to be applied to both training data and test data." ></td>
	<td class="line x" title="14:224	Despite the added complexity of parsing the data, this technique has shown improvements, especially when good parses of the source side exist." ></td>
	<td class="line x" title="15:224	It has been successfully applied to German-toEnglish and Chinese-to-English SMT (Collins et al., 2005; Wang et al., 2007)." ></td>
	<td class="line x" title="16:224	In this paper, we propose the use of a similar approach for English-to-Arabic SMT." ></td>
	<td class="line x" title="17:224	Unlike most other work on Arabic translation, our work is in the direction of the more morphologically complex language, which poses unique challenges." ></td>
	<td class="line x" title="18:224	We propose a set of syntactic reordering rules on the English source to align it better to the Arabic target." ></td>
	<td class="line x" title="19:224	The reordering rules exploit systematic differences between the syntax of Arabic and the syntax of English; they specifically address two syntactic constructs." ></td>
	<td class="line x" title="20:224	The first is the Subject-Verb order in independent sentences, where the preferred order in written Arabic is Verb-Subject." ></td>
	<td class="line x" title="21:224	The second is the noun phrase structure, where many differences exist between the two languages, among them the order of adjectives, compound nouns and genitive constructs, as well as the way definiteness is marked." ></td>
	<td class="line x" title="22:224	The implementation of these rules is fairly straightforward since they are applied to the parse tree." ></td>
	<td class="line x" title="23:224	It has been noted in previous work (Habash, 2007) that syntactic reordering does not improve translation if the parse quality is not good enough." ></td>
	<td class="line x" title="24:224	Since in this paper our source language is English, the parses are more reliable, and result in more correct reorderings." ></td>
	<td class="line x" title="25:224	We show that using the reordering rules results in gains in the translation scores and study the effect of the training data size on those gains." ></td>
	<td class="line x" title="26:224	This paper also investigates the effect of using morphological segmentation of the Arabic target 86 in combination with the reordering rules." ></td>
	<td class="line x" title="27:224	Morphological segmentation has been shown to benefit Arabic-to-English (Habash and Sadat, 2006) and English-to-Arabic (Badr et al., 2008) translation, although the gains tend to decrease with increasing training data size." ></td>
	<td class="line x" title="28:224	Section 2 provides linguistic motivation for the paper." ></td>
	<td class="line x" title="29:224	It describes the rich morphology of Arabic, and its implications on SMT." ></td>
	<td class="line x" title="30:224	It also describes the syntax of the verb phrase and noun phrase in Arabic, and how they differ from their English counterparts." ></td>
	<td class="line x" title="31:224	In Section 3, we describe some of the relevant previous work." ></td>
	<td class="line x" title="32:224	In Section 4, we present the preprocessing techniques used in the experiments." ></td>
	<td class="line x" title="33:224	Section 5 describes the translation system, the data used, and then presents and discusses the experimental results from three domains: news text, UN data and spoken dialogue from the travel domain." ></td>
	<td class="line x" title="34:224	The final section provides a brief summary and conclusion." ></td>
	<td class="line x" title="35:224	2 Arabic Linguistic Issues 2.1 Arabic Morphology Arabic has a complex morphology compared to English." ></td>
	<td class="line x" title="36:224	The Arabic noun and adjective are inflected for gender and number; the verb is inflected in addition for tense, voice, mood and person." ></td>
	<td class="line x" title="37:224	Various clitics can attach to words as well: Conjunctions, prepositions and possessive pronouns attach to nouns, and object pronouns attach to verbs." ></td>
	<td class="line x" title="38:224	The example below shows the decomposition into stems and clitics of the Arabic verb phrase wsyqAblhm1 and noun phrase wbydh, both of which are written as one word: (1) a. w+ and s+ will yqAbl meet-3SM +hm them and he will meet them b. w+ and b+ with yd hand +h his and with his hand An Arabic corpus will, therefore, have more surface forms than an equivalent English corpus, and will also be sparser." ></td>
	<td class="line x" title="39:224	In the LDC news corpora used in this paper (see Section 5.2), the average English sentence length is 33 words compared to the Arabic 25 words." ></td>
	<td class="line x" title="40:224	1All examples in this paper are written in the Buckwalter Transliteration System (http://www.qamus.org/transliteration.htm) Although the Arabic language family consists of many dialects, none of them has a standard orthography." ></td>
	<td class="line x" title="41:224	This affects the consistency of the orthography of Modern Standard Arabic (MSA), the only written variety of Arabic." ></td>
	<td class="line x" title="42:224	Certain characters are written inconsistently in different data sources: Final y is sometimes written as Y (Alif mqSwrp), and initial Alif hamza (The Buckwalter characters < and {) are written as bare alif (A)." ></td>
	<td class="line x" title="43:224	Arabic is usually written without the diacritics that denote short vowels." ></td>
	<td class="line x" title="44:224	This creates an ambiguity at the word level, since a word can have more than one reading." ></td>
	<td class="line x" title="45:224	These factors adversely affect the performance of Arabic-to-English SMT, especially in the English-to-Arabic direction." ></td>
	<td class="line x" title="46:224	Simple pattern matching is not enough to perform morphological analysis and decomposition, since a certain string of characters can, in principle, be either an affixed morpheme or part of the base word itself." ></td>
	<td class="line x" title="47:224	Word-level linguistic information as well as context analysis are needed." ></td>
	<td class="line x" title="48:224	For example the written form wly can mean either ruler or and for me, depending on the context." ></td>
	<td class="line x" title="49:224	Only in the latter case should it be decomposed." ></td>
	<td class="line x" title="50:224	2.2 Arabic Syntax In this section, we describe a number of syntactic facts about Arabic which are relevant to the reordering rules described in Section 4.2." ></td>
	<td class="line x" title="51:224	Clause Structure In Arabic, the main sentence usually has the order Verb-Subject-Object (VSO)." ></td>
	<td class="line x" title="52:224	The order Subject-Verb-Object (SVO) also occurs, but is less frequent than VSO." ></td>
	<td class="line x" title="53:224	The verb agrees with the subject in gender and number in the SVO order, but only in gender in the VSO order (Examples 2c and 2d)." ></td>
	<td class="line x" title="54:224	(2) a. Akl ate-3SM Alwld the-boy AltfAHp the-apple the boy ate the apple b. Alwld the-boy Akl ate-3SM AltfAHp the-apple the boy ate the apple c. Akl ate-3SM AlAwlAd the-boys AltfAHAt the-apples the boys ate the apples d. AlAwlAd the-boys AklwA ate-3PM AltfAHAt the-apples the boys ate the apples 87 In a dependent clause, the order must be SVO, as illustrated by the ungrammaticality of Example 3b below." ></td>
	<td class="line x" title="55:224	As we discuss in more detail later, this distinction between dependent and independent clauses has to be taken into account when the syntactic reordering rules are applied." ></td>
	<td class="line x" title="56:224	(3) a. qAl said-3SM An that Alwld the-boy Akl ate AltfAHp the-apple he said that the boy ate the apple b. *qAl said-3SM An that Akl ate Alwld the-boy AltfAHp the-apple he said that the boy ate the apple Another pertinent fact is that the negation particle has to always preceed the verb: (4) lm not yAkl eat-3SM Alwld the-boy AltfAHp the-apple the boy did not eat the apple Noun Phrase The Arabic noun phrase can have constructs that are quite different from English." ></td>
	<td class="line x" title="57:224	The adjective in Arabic follows the noun that it modifies, and it is marked with the definite article, if the head noun is definite: (5) AlbAb the-door Alkbyr the-big the big door The Arabic equivalent of the English possessive, compound nouns and the of -relationship is the Arabic idafa construct, which compounds two or more nouns." ></td>
	<td class="line x" title="58:224	Therefore, N1s N2 and N2 of N1 are both translated as N2 N1 in Arabic." ></td>
	<td class="line x" title="59:224	As Example 6b shows, this construct can also be chained recursively." ></td>
	<td class="line x" title="60:224	(6) a. bAb door Albyt the-house the houses door b. mftAH key bAb door Albyt the-house The key to the door of the house Example 6 also shows that an idafa construct is made definite by adding the definite article Alto the last noun in the noun phrase." ></td>
	<td class="line x" title="61:224	Adjectives follow the idafa noun phrase, regardless of which noun in the chain they modify." ></td>
	<td class="line x" title="62:224	Thus, Example 7 is ambiguous in that the adjective kbyr (big) can modify any of the preceding three nouns." ></td>
	<td class="line x" title="63:224	The same is true for relative clauses that modify a noun." ></td>
	<td class="line x" title="64:224	(7) mftAH key bAb door Albyt the-house Alkbyr the-big These and other differences between the Arabic and English syntax are likely to affect the quality of automatic alignments, since corresponding words will occupy positions in the sentence that are far apart, especially when the relevant words (e.g. the verb and its subject) are separated by subordinate clauses." ></td>
	<td class="line x" title="65:224	In such cases, the lexicalized distortion models used in phrase-based SMT do not have the capability of performing reorderings correctly." ></td>
	<td class="line x" title="66:224	This limitation adversely affects the translation quality." ></td>
	<td class="line x" title="67:224	3 Previous Work Most of the work in Arabic machine translation is done in the Arabic-to-English direction." ></td>
	<td class="line x" title="68:224	The other direction, however, is also important, since it opens the wealth of information in different domains that is available in English to the Arabic speaking world." ></td>
	<td class="line x" title="69:224	Also, since Arabic is a morphologically richer language, translating into Arabic poses unique issues that are not present in the opposite direction." ></td>
	<td class="line x" title="70:224	The only works on Englishto-Arabic SMT that we are aware of are Badr et al.(2008), and Sarikaya and Deng (2007)." ></td>
	<td class="line x" title="72:224	Badr et al. show that using segmentation and recombination as preand postprocessing steps leads to significant gains especially for smaller training data corpora." ></td>
	<td class="line x" title="73:224	Sarikaya and Deng use Joint Morphological-Lexical Language Models to rerank the output of an English-to-Arabic MT system." ></td>
	<td class="line x" title="74:224	They use regular expression-based segmentation of the Arabic so as not to run into recombination issues on the output side." ></td>
	<td class="line x" title="75:224	Similarly, for Arabic-to-English, Lee (2004), and Habash and Sadat (2006) show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size." ></td>
	<td class="line x" title="76:224	They use a trigram language model and the Arabic morphological analyzer MADA (Habash and Rambow, 2005) respectively, to segment the Arabic side of their corpora." ></td>
	<td class="line x" title="77:224	Other work on Arabicto-English SMT tries to address the word reordering problem." ></td>
	<td class="line x" title="78:224	Habash (2007) automatically learns syntactic reordering rules that are then applied to the Arabic side of the parallel corpora." ></td>
	<td class="line x" title="79:224	The words are aligned in a sentence pair, then the Arabic sentence is parsed to extract reordering rules based on how the constituents in the parse tree are reordered on the English side." ></td>
	<td class="line x" title="80:224	No significant improvement is 88 shown with reordering when compared to a baseline that uses a non-lexicalized distance reordering model." ></td>
	<td class="line x" title="81:224	This is attributed in the paper to the poor quality of parsing." ></td>
	<td class="line x" title="82:224	Syntax-based reordering as a preprocessing step has been applied to many language pairs other than English-Arabic." ></td>
	<td class="line x" title="83:224	Most relevant to the approach in this paper are Collins et al.(2005) and Wang et al.(2007)." ></td>
	<td class="line x" title="86:224	Both parse the source side and then reorder the sentence based on predefined, linguistically motivated rules." ></td>
	<td class="line x" title="87:224	Significant gain is reported for German-to-English and Chinese-to-English translation." ></td>
	<td class="line x" title="88:224	Both suggest that reordering as a preprocessing step results in better alignment, and reduces the reliance on the distortion model." ></td>
	<td class="line x" title="89:224	Popovic and Ney (2006) use similar methods to reorder German by looking at the POS tags for German-to-English and German-toSpanish." ></td>
	<td class="line x" title="90:224	They show significant improvements on test set sentences that do get reordered as well as those that dont, which is attributed to the improvement of the extracted phrases." ></td>
	<td class="line x" title="91:224	(Xia and McCord, 2004) present a similar approach, with a notable difference: the re-ordering rules are automatically learned from aligning parse trees for both the source and target sentences." ></td>
	<td class="line x" title="92:224	They report a 10% relative gain for English-to-French translation." ></td>
	<td class="line x" title="93:224	Although target-side parsing is optional in this approach, it is needed to take full advantage of the approach." ></td>
	<td class="line x" title="94:224	This is a bigger issue when no reliable parses are available for the target language, as is the case in this paper." ></td>
	<td class="line x" title="95:224	More generally, the use of automatically-learned rules has the advantage of readily applicable to different language pairs." ></td>
	<td class="line x" title="96:224	The use of deterministic, pre-defined rules, however, has the advantage of being linguistically motivated, since differences between the two languages are addressed explicitly." ></td>
	<td class="line x" title="97:224	Moreover, the implementation of pre-defined transfer rules based on target-side parses is relatively easy and cheap to implement in different language pairs." ></td>
	<td class="line x" title="98:224	Generic approaches for translating from English to more morphologically complex languages have been proposed." ></td>
	<td class="line x" title="99:224	Koehn and Hoang (2007) propose Factored Translation Models, which extend phrase-based statistical machine translation by allowing the integration of additional morphological features at the word level." ></td>
	<td class="line x" title="100:224	They demonstrate improvements for English-to-German and English-to-Czech." ></td>
	<td class="line x" title="101:224	Tighter integration of features is claimed to allow for better modeling of the morphology and hence is better than using pre-processing and post-processing techniques." ></td>
	<td class="line x" title="102:224	Avramidis and Koehn (2008) enrich the English side by adding a feature to the Factored Model that models noun case agreement and verb person conjugation, and show that it leads to a more grammatically correct output for English-to-Greek and English-to-Czech translation." ></td>
	<td class="line x" title="103:224	Although Factored Models are well equipped for handling languages that differ in terms of morphology, they still use the same distortion reordering model as a phrasebased MT system." ></td>
	<td class="line x" title="104:224	4 Preprocessing Techniques 4.1 Arabic Segmentation and Recombination It has been shown previously work (Badr et al., 2008; Habash and Sadat, 2006) that morphological segmentation of Arabic improves the translation performance for both Arabic-to-English and English-to-Arabic by addressing the problem of sparsity of the Arabic side." ></td>
	<td class="line x" title="105:224	In this paper, we use segmented and non-segmented Arabic on the target side, and study the effect of the combination of segmentation with reordering." ></td>
	<td class="line x" title="106:224	As mentioned in Section 2.1, simple pattern matching is not enough to decompose Arabic words into stems and affixes." ></td>
	<td class="line x" title="107:224	Lexical information and context are needed to perform the decomposition correctly." ></td>
	<td class="line x" title="108:224	We use the Morphological Analyzer MADA (Habash and Rambow, 2005) to decompose the Arabic source." ></td>
	<td class="line x" title="109:224	MADA uses SVMbased classifiers of features (such as POS, number, gender, etc.) to score the different analyses of a given word in context." ></td>
	<td class="line x" title="110:224	We apply morphological decomposition before aligning the training data." ></td>
	<td class="line x" title="111:224	We split the conjunction and preposition prefixes, as well as possessive and object pronoun suffixes." ></td>
	<td class="line x" title="112:224	We then glue the split morphemes into one prefix and one suffix, such that any given word is split into at most three parts: prefix+ stem +suffix." ></td>
	<td class="line x" title="113:224	Note that plural markers and subject pronouns are not split." ></td>
	<td class="line x" title="114:224	For example, the word wlAwlAdh (and for his children) is segmented into wl+ AwlAd +P:3MS." ></td>
	<td class="line x" title="115:224	Since training is done on segmented Arabic, the output of the decoder must be recombined into its original surface form." ></td>
	<td class="line x" title="116:224	We follow the approach of Badr et." ></td>
	<td class="line x" title="117:224	al (2008) in combining the Arabic output, which is a non-trivial task for several reasons." ></td>
	<td class="line x" title="118:224	First, the ending of a stem sometimes changes when a suffix is attached to it." ></td>
	<td class="line x" title="119:224	Second, word end89 ings are normalized to remove orthographic inconsistency between different sources (Section 2.1)." ></td>
	<td class="line x" title="120:224	Finally, some words can recombine into more than one grammatically correct form." ></td>
	<td class="line x" title="121:224	To address these issues, a lookup table is derived from the training data that maps the segmented form of the word to its original form." ></td>
	<td class="line x" title="122:224	The table is also useful in recombining words that are erroneously segmented." ></td>
	<td class="line x" title="123:224	If a certain word does not occur in the table, we back off to a set of manually defined recombination rules." ></td>
	<td class="line x" title="124:224	Word ambiguity is resolved by picking the more frequent surface form." ></td>
	<td class="line x" title="125:224	4.2 Arabic Reordering Rules This section presents the syntax-based rules used for re-ordering the English source to better match the syntax of the Arabic target." ></td>
	<td class="line x" title="126:224	These rules are motivated by the Arabic syntactic facts described in Section 2.2." ></td>
	<td class="line x" title="127:224	Much like Wang et al.(2007), we parse the English side of our corpora and reorder using predefined rules." ></td>
	<td class="line x" title="129:224	Reordering the English can be done more reliably than other source languages, such as Arabic, Chinese and German, since the stateof-the-art English parsers are considerably better than parsers of other languages." ></td>
	<td class="line x" title="130:224	The following rules for reordering at the sentence level and the noun phrase level are applied to the English parse tree: 1." ></td>
	<td class="line x" title="131:224	NP: All nouns, adjectives and adverbs in the noun phrase are inverted." ></td>
	<td class="line x" title="132:224	This rule is motivated by the order of the adjective with respect to its head noun, as well as the idafa construct (see Examples 6 and 7 in Section 2.2." ></td>
	<td class="line x" title="133:224	As a result of applying this rule, the phrase the blank computer screen becomes the screen computer blank . 2." ></td>
	<td class="line x" title="134:224	PP: All prepositional phrases of the form N1ofN2ofNn are transformed to N1N2Nn." ></td>
	<td class="line x" title="135:224	All Ni are also made indefinite, and the definite article is added to Nn, the last noun in the chain." ></td>
	<td class="line x" title="136:224	For example, the phrase the general chief of staff of the armed forces becomes general chief staff the armed forces." ></td>
	<td class="line x" title="137:224	We also move all adjectives in the top noun phrase to the end of the construct." ></td>
	<td class="line x" title="138:224	So the real value of the Egyptian pound becomes value the Egyptian pound real." ></td>
	<td class="line x" title="139:224	This rule is motivated by the idafa construct and its properties (see Example 6)." ></td>
	<td class="line x" title="140:224	3." ></td>
	<td class="line x" title="141:224	the: The definite article the is replicated before adjectives (see Example 5 above)." ></td>
	<td class="line x" title="142:224	So the blank computer screen becomes the blank the computer the screen." ></td>
	<td class="line x" title="143:224	This rule is applied after NP rule abote." ></td>
	<td class="line x" title="144:224	Note that we do not replicate the before proper names." ></td>
	<td class="line x" title="145:224	4." ></td>
	<td class="line x" title="146:224	VP: This rule transforms SVO sentences to VSO." ></td>
	<td class="line x" title="147:224	All verbs are reordered on the condition that they have their own subject noun phrase and are not in the participle form, since in these cases the Arabic subject occurs before the verb participle." ></td>
	<td class="line x" title="148:224	We also check that the verb is not in a relative clause with a that complementizer (Example 3 above)." ></td>
	<td class="line x" title="149:224	The following example illustrates all these cases: the health minister stated that 11 police officers were wounded in clashes with the demonstratorsstated the health minister that 11 police officers were wounded in clashes with the demonstrators." ></td>
	<td class="line x" title="150:224	If the verb is negated, the negative particle is moved with the verb (Example 4." ></td>
	<td class="line x" title="151:224	Finally, if the object of the reordered verb is a pronoun, it is reordered with the verb." ></td>
	<td class="line x" title="152:224	Example: the authorities gave us all the necessary help becomes gave us the authorities all the necessary help." ></td>
	<td class="line x" title="153:224	The transformation rules 1, 2 and 3 are applied in this order, since they interact although they do not conflict." ></td>
	<td class="line x" title="154:224	So, the real value of the Egyptian pound  value the Egyptian the pound the real The VP reordering rule is independent." ></td>
	<td class="line x" title="155:224	5 Experiments 5.1 System description For the English source, we first tokenize using the Stanford Log-linear Part-of-Speech Tagger (Toutanova et al., 2003)." ></td>
	<td class="line x" title="156:224	We then proceed to split the data into smaller sentences and tag them using Ratnaparkhis Maximum Entropy Tagger (Ratnaparkhi, 1996)." ></td>
	<td class="line oc" title="157:224	We parse the data using the Collins Parser (Collins, 1997), and then tag person, location and organization names using the Stanford Named Entity Recognizer (Finkel et al., 2005)." ></td>
	<td class="line x" title="158:224	On the Arabic side, we normalize the data by changing final Y to y, and changing the various forms of Alif hamza to bare Alif, since these characters are written inconsistently in some Arabic sources." ></td>
	<td class="line x" title="159:224	We then segment the data using MADA according to the scheme explained in Section 4.1." ></td>
	<td class="line x" title="160:224	90 The English source is aligned to the segmented Arabic target using the standard MOSES (MOSES, 2007) configuration of GIZA++ (Och and Ney, 2000), which is IBM Model 4, and decoding is done using the phrasebased SMT system MOSES." ></td>
	<td class="line x" title="161:224	We use a maximum phrase length of 15 to account for the increase in length of the segmented Arabic." ></td>
	<td class="line x" title="162:224	We also use a lexicalized bidirectional reordering model conditioned on both the source and target sides, with a distortion limit set to 6." ></td>
	<td class="line x" title="163:224	We tune using Ochs algorithm (Och, 2003) to optimize weights for the distortion model, language model, phrase translation model and word penalty over the BLEU metric (Papineni et al., 2001)." ></td>
	<td class="line x" title="164:224	For the segmented Arabic experiments, we experiment with tuning using non-segmented Arabic as a reference." ></td>
	<td class="line x" title="165:224	This is done by recombining the output before each tuning iteration is scored and has been shown by Badr et." ></td>
	<td class="line x" title="166:224	al (2008) to perform better than using segmented Arabic as reference." ></td>
	<td class="line x" title="167:224	5.2 Data Used We report results on three domains: newswire text, UN data and spoken dialogue from the travel domain." ></td>
	<td class="line x" title="168:224	It is important to note that the sentences in the travel domain are much shorter than in the news domain, which simplifies the alignment as well as reordering during decoding." ></td>
	<td class="line x" title="169:224	Also, since the travel domain contains spoken Arabic, it is more biased towards the Subject-Verb-Object sentence order than the Verb-Subject-Object order more common in the news domain." ></td>
	<td class="line x" title="170:224	Also note that since most of our data was originally intended for Arabic-to-English translation, our test and tuning sets have only one reference, and therefore, the BLEU scores we report are lower than typical scores reported in the literature on Arabic-toEnglish." ></td>
	<td class="line x" title="171:224	The news training data consists of several LDC corpora2." ></td>
	<td class="line x" title="172:224	We construct a test set by randomly picking 2000 sentences." ></td>
	<td class="line x" title="173:224	We pick another 2000 sentences randomly for tuning." ></td>
	<td class="line x" title="174:224	Our final training set consists of 3 million English words." ></td>
	<td class="line x" title="175:224	We also test on the NIST MT 05 test set while tuning on both the NIST MT 03 and 04 test sets." ></td>
	<td class="line x" title="176:224	We use the first English reference of the NIST test sets as the source, and the Arabic source as our reference." ></td>
	<td class="line x" title="177:224	For 2LDC2003E05 LDC2003E09 LDC2003T18 LDC2004E07 LDC2004E08 LDC2004E11 LDC2004E72 LDC2004T18 LDC2004T17 LDC2005E46 LDC2005T05 LDC2007T24 Scheme RandT MT 05S NoS S NoS Baseline 21.6 21.3 23.88 23.44 VP 21.9 21.5 23.98 23.58 NP 21.9 21.8 NP+PP 21.8 21.5 23.72 23.68 NP+PP+VP 22.2 21.8 23.74 23.16 NP+PP+VP+The 21.3 21.0 Table 1: Translation Results for the News Domain in terms of the BLEU Metric." ></td>
	<td class="line x" title="178:224	the language model, we use 35 million words from the LDC Arabic Gigaword corpus, plus the Arabic side of the 3 million word training corpus." ></td>
	<td class="line x" title="179:224	Experimentation with different language model orders shows that the optimal model orders are 4-grams for the baseline system and 6-grams for the segmented Arabic." ></td>
	<td class="line x" title="180:224	The average sentence length is 33 for English, 25 for non-segmented Arabic and 36 for segmented Arabic." ></td>
	<td class="line x" title="181:224	To study the effect of syntactic reordering on larger training data sizes, we use the UN EnglishArabic parallel text (LDC2003T05)." ></td>
	<td class="line x" title="182:224	We experiment with two training data sizes: 30 million and 3 million words." ></td>
	<td class="line x" title="183:224	The test and tuning sets are comprised of 1500 and 500 sentences respectively, chosen at random." ></td>
	<td class="line x" title="184:224	For the spoken domain, we use the BTEC 2007 Arabic-English corpus." ></td>
	<td class="line x" title="185:224	The training set consists of 200K words, the test set has 500 sentences and the tuning set has 500 sentences." ></td>
	<td class="line x" title="186:224	The language model consists of the Arabic side of the training data." ></td>
	<td class="line x" title="187:224	Because of the significantly smaller data size, we use a trigram LM for the baseline, and a 4-gram for segmented Arabic." ></td>
	<td class="line x" title="188:224	In this case, the average sentence length is 9 for English, 8 for Arabic, and 10 for segmented Arabic." ></td>
	<td class="line x" title="189:224	5.3 Translation Results The translation scores for the News domain are shown in Table 1." ></td>
	<td class="line x" title="190:224	The notation used in the table is as follows:  S: Segmented Arabic  NoS: Non-Segmented Arabic  RandT: Scores for test set where sentences were picked at random from NEWS data  MT 05: Scores for the NIST MT 05 test set The reordering notation is explained in Section 4.2." ></td>
	<td class="line x" title="191:224	All results are in terms of the BLEU met91 S NoS Short Long Short Long Baseline 22.57 25.22 22.40 24.33 VP 22.95 25.05 22.95 24.02 NP+PP 22.71 24.76 23.16 24.067 NP+PP+VP 22.84 24.62 22.53 24.56 Table 2: Translation Results depending on sentence length for NIST test set." ></td>
	<td class="line x" title="192:224	Scheme Score % Oracle reord VP 25.76 59% NP+PP 26.07 58% NP+PP+VP 26.17 53% Table 3: Oracle scores for combining baseline system with other reordered systems." ></td>
	<td class="line x" title="193:224	ric." ></td>
	<td class="line x" title="194:224	It is important to note that the gain that we report in terms of BLEU are more significant that comparable gains on test sets that have multiple references, since our test sets have only one reference." ></td>
	<td class="line x" title="195:224	Any amount of gain is a result of additional n-gram precision with one reference." ></td>
	<td class="line x" title="196:224	We note that the gain achieved from the reordering of the nonsegmented and segmented systems are comparable." ></td>
	<td class="line x" title="197:224	Replicating the before adjectives hurts the scores, possibly because it increases the sentence length noticeably, and thus deteriorates the alignments quality." ></td>
	<td class="line x" title="198:224	We note that the gains achieved by reordering on the NIST test set are smaller than the improvements on the random test set." ></td>
	<td class="line x" title="199:224	This is due to the fact that the sentences in the NIST test set are longer, which adversely affects the parsing quality." ></td>
	<td class="line x" title="200:224	The average English sentence length is 33 words in the NIST test set, while the random test set has an average sentence length of 29 words." ></td>
	<td class="line x" title="201:224	Table 2 shows the reordering gains of the nonsegmented Arabic by sentence length." ></td>
	<td class="line x" title="202:224	Short sentences are sentences that have less that 40 words of English, while long sentences have more than 40 words." ></td>
	<td class="line x" title="203:224	Out of the 1055 sentence in the NIST test set 719 are short and 336 are long." ></td>
	<td class="line x" title="204:224	We also report oracle scores in Table 3 for combining the baseline system with the reordering systems, as well as the percentage of oracle sentences produced by the reordered system." ></td>
	<td class="line x" title="205:224	The oracle score is computed by starting with the reordered systems candidate translations and iterating over all the sentences one by one: we replace each sentence with its corresponding baseline system translation then Scheme 30M 3M Baseline 32.17 28.42 VP 32.46 28.60 NP+PP 31.73 28.80 Table 4: Translation Results on segmentd UN data in terms of the BLEU Metric." ></td>
	<td class="line x" title="206:224	compute the total BLEU score of the entire set." ></td>
	<td class="line x" title="207:224	If the score improves, then the sentence in question is replaced with the baseline systems translation, otherwise it remains unchanged and we move on to the next one." ></td>
	<td class="line x" title="208:224	In Table 4, we report results on the UN corpus for different training data sizes." ></td>
	<td class="line x" title="209:224	It is important to note that although gains from VP reordering stay constant when scaled to larger training sets, gains from NP+PP reordering diminish." ></td>
	<td class="line x" title="210:224	This is due to the fact that NP reordering tend to be more localized then VP reorderings." ></td>
	<td class="line x" title="211:224	Hence with more training data the lexicalized reordering model becomes more effective in reordering NPs." ></td>
	<td class="line x" title="212:224	In Table 5, we report results on the BTEC corpus for different segmentation and reordering scheme combinations." ></td>
	<td class="line x" title="213:224	We should first point out that all sentences in the BTEC corpus are short, simple and easy to align." ></td>
	<td class="line x" title="214:224	Hence, the gain introduced by reordering might not be enough to offset the errors introduced by the parsing." ></td>
	<td class="line x" title="215:224	We also note that spoken Arabic usually prefers the SubjectVerb-Object sentence order, rather than the VerbSubject-Object sentence order of written Arabic." ></td>
	<td class="line x" title="216:224	This explains the fact that no gain is observed when the verb phrase is reordered." ></td>
	<td class="line x" title="217:224	Noun phrase reordering produces a significant gain with nonsegmented Arabic." ></td>
	<td class="line x" title="218:224	Replicating the definite article the in the noun phrase does not create alignment problems as is the case with the newswire data, since the sentences are considerably shorter, and hence the 0.74 point gain observed on the segmented Arabic system." ></td>
	<td class="line x" title="219:224	That gain does not translate to the non-segmented Arabic system since in that case the definite article Al remains attached to its head word." ></td>
	<td class="line x" title="220:224	6 Conclusion This paper presented linguistically motivated rules that reorder English to look like Arabic." ></td>
	<td class="line x" title="221:224	We showed that these rules produce significant gains." ></td>
	<td class="line x" title="222:224	We also studied the effect of the interaction between Arabic morphological segmentation and 92 Scheme S NoS Baseline 29.06 25.4 VP 26.92 23.49 NP 27.94 26.83 NP+PP 28.59 26.42 The 29.8 25.1 Table 5: Translation Results for the Spoken Language Domain in the BLEU Metric." ></td>
	<td class="line x" title="223:224	syntactic reordering on translation results, as well as how they scale to bigger training data sizes." ></td>
	<td class="line x" title="224:224	Acknowledgments We would like to thank Michael Collins, Ali Mohammad and Stephanie Seneff for their valuable comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E09-1037
Cube Summing, Approximate Inference with Non-Local Features, and Dynamic Programming without Semirings
Gimpel, Kevin;Smith, Noah A.;"></td>
	<td class="line x" title="1:224	Proceedings of the 12th Conference of the European Chapter of the ACL, pages 318326, Athens, Greece, 30 March  3 April 2009." ></td>
	<td class="line x" title="2:224	c2009 Association for Computational Linguistics Cube Summing, Approximate Inference with Non-Local Features, and Dynamic Programming without Semirings Kevin Gimpel and Noah A. Smith Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA {kgimpel,nasmith}@cs.cmu.edu Abstract We introduce cube summing, a technique that permits dynamic programming algorithms for summing over structures (like the forward and inside algorithms) to be extended with non-local features that violate the classical structural independence assumptions." ></td>
	<td class="line x" title="3:224	It is inspired by cube pruning (Chiang, 2007; Huang and Chiang, 2007) in its computation of non-local features dynamically using scored k-best lists, but also maintains additional residual quantities used in calculating approximate marginals." ></td>
	<td class="line x" title="4:224	When restricted to local features, cube summing reduces to a novel semiring (k-best+residual) that generalizes many of the semirings of Goodman (1999)." ></td>
	<td class="line x" title="5:224	When non-local features are included, cube summing does not reduce to any semiring, but is compatible with generic techniques for solving dynamic programming equations." ></td>
	<td class="line x" title="6:224	1 Introduction Probabilistic NLP researchers frequently make independence assumptions to keep inference algorithms tractable." ></td>
	<td class="line x" title="7:224	Doing so limits the features that are available to our models, requiring features to be structurally local." ></td>
	<td class="line x" title="8:224	Yet many problems in NLPmachine translation, parsing, named-entity recognition, and othershave benefited from the addition of non-local features that break classical independence assumptions." ></td>
	<td class="line x" title="9:224	Doing so has required algorithms for approximate inference." ></td>
	<td class="line x" title="10:224	Recently cube pruning (Chiang, 2007; Huang and Chiang, 2007) was proposed as a way to leverage existing dynamic programming algorithms that find optimal-scoring derivations or structures when only local features are involved." ></td>
	<td class="line x" title="11:224	Cube pruning permits approximate decoding with non-local features, but leaves open the question of how the featureweightsorprobabilitiesarelearned." ></td>
	<td class="line x" title="12:224	Meanwhile, some learning algorithms, like maximum likelihood for conditional log-linear models (Lafferty et al., 2001), unsupervised models (Pereira and Schabes, 1992), and models with hidden variables (Koo and Collins, 2005; Wang et al., 2007; Blunsom et al., 2008), require summing over the scores of many structures to calculate marginals." ></td>
	<td class="line x" title="13:224	We first review the semiring-weighted logic programming view of dynamic programming algorithms (Shieber et al., 1995) and identify an intuitive property of a program called proof locality that follows from feature locality in the underlying probability model (2)." ></td>
	<td class="line x" title="14:224	We then provide an analysis of cube pruning as an approximation to the intractableproblemofexactoptimizationoverstructures with non-local features and show how the use of non-local features with k-best lists breaks certain semiring properties (3)." ></td>
	<td class="line x" title="15:224	The primary contribution of this paper is a novel technique cube summingfor approximate summing over discrete structures with non-local features, which we relate to cube pruning (4)." ></td>
	<td class="line x" title="16:224	We discuss implementation (5) and show that cube summing becomes exact and expressible as a semiring when restricted to local features; this semiring generalizes many commonly-used semirings in dynamic programming (6)." ></td>
	<td class="line x" title="17:224	2 Background In this section, we discuss dynamic programming algorithms as semiring-weighted logic programs." ></td>
	<td class="line x" title="18:224	We then review the definition of semirings and important examples." ></td>
	<td class="line x" title="19:224	We discuss the relationship between locally-factored structure scores and proofs in logic programs." ></td>
	<td class="line x" title="20:224	2.1 Dynamic Programming Many algorithms in NLP involve dynamic programming (e.g., the Viterbi, forward-backward, 318 probabilistic Earleys, and minimum edit distance algorithms)." ></td>
	<td class="line x" title="21:224	Dynamic programming (DP) involves solving certain kinds of recursive equations with shared substructure and a topological ordering of the variables." ></td>
	<td class="line x" title="22:224	Shieber et al.(1995) showed a connection between DP (specifically, as used in parsing) and logic programming, and Goodman (1999) augmented such logic programs with semiring weights, giving an algebraic explanation for the intuitive connections among classes of algorithms with the same logical structure." ></td>
	<td class="line x" title="24:224	For example, in Goodmans framework, the forward algorithm and the Viterbi algorithm are comprised of the same logic program with different semirings." ></td>
	<td class="line x" title="25:224	Goodman defined other semirings, including ones we will use here." ></td>
	<td class="line x" title="26:224	This formal framework was the basis for the Dyna programming language, which permits a declarative specification of the logic program and compiles it into an efficient, agendabased, bottom-up procedure (Eisner et al., 2005)." ></td>
	<td class="line x" title="27:224	Forourpurposes, aDPconsistsofasetofrecursive equations over a set of indexed variables." ></td>
	<td class="line x" title="28:224	For example, the probabilistic CKY algorithm (run on sentence w1w2wn) is written as CX,i1,i = pXwi (1) CX,i,k = max Y,ZN;j{i+1,,k1} pXYZ CY,i,j CZ,j,k goal = CS,0,n where N is the nonterminal set and S  N is the start symbol." ></td>
	<td class="line x" title="29:224	Each CX,i,j variable corresponds to the chart value (probability of the most likely subtree) of an X-constituent spanning the substring wi+1wj." ></td>
	<td class="line x" title="30:224	goal is a special variable of greatest interest, though solving for goal correctly may (in general, but not in this example) require solving for all the other values." ></td>
	<td class="line x" title="31:224	We will use the term index to refer to the subscript values on variables (X,i,j on CX,i,j)." ></td>
	<td class="line x" title="32:224	Where convenient, we will make use of Shieber et al.s logic programming view of dynamic programming." ></td>
	<td class="line x" title="33:224	In this view, each variable (e.g., CX,i,j in Eq." ></td>
	<td class="line x" title="34:224	1) corresponds to the value of a theorem, the constants in the equations (e.g., pXYZ in Eq." ></td>
	<td class="line x" title="35:224	1) correspond to the values of axioms, and the DP defines quantities corresponding to weighted proofs of the goal theorem (e.g., finding the maximum-valued proof, or aggregating proof values)." ></td>
	<td class="line x" title="36:224	The value of a proof is a combination of the values of the axioms it starts with." ></td>
	<td class="line x" title="37:224	Semirings define these values and define two operators over them, called aggregation (max in Eq." ></td>
	<td class="line x" title="38:224	1) and combination ( in Eq." ></td>
	<td class="line x" title="39:224	1)." ></td>
	<td class="line x" title="40:224	GoodmanandEisneretal." ></td>
	<td class="line x" title="41:224	assumedthatthevalues of the variables are in a semiring, and that the equations are defined solely in terms of the two semiring operations." ></td>
	<td class="line x" title="42:224	We will often refer to the probability of a proof, by which we mean a nonnegative R-valued score defined by the semantics of the dynamic program variables; it may not be a normalized probability." ></td>
	<td class="line x" title="43:224	2.2 Semirings A semiring is a tuple A,,,0,1, in which A is a set,  : A  A  A is the aggregation operation,  : A  A  A is the combination operation, 0 is the additive identity element (a  A,a  0 = a), and 1 is the multiplicative identity element (a  A,a  1 = a)." ></td>
	<td class="line x" title="44:224	A semiring requires  to be associative and commutative, and  to be associative and to distribute over." ></td>
	<td class="line x" title="45:224	Finally, we require a0 = 0a = 0for all a  A.1 Examples include the inside semiring, R0,+,,0,1, and the Viterbi semiring, R0,max,,0,1." ></td>
	<td class="line x" title="46:224	The former sums the probabilities of all proofs of each theorem." ></td>
	<td class="line x" title="47:224	The latter (used in Eq." ></td>
	<td class="line x" title="48:224	1) calculates the probability of the most probable proof of each theorem." ></td>
	<td class="line x" title="49:224	Two more examples follow." ></td>
	<td class="line x" title="50:224	Viterbi proof semiring." ></td>
	<td class="line x" title="51:224	We typically need to recover the steps in the most probable proof in addition to its probability." ></td>
	<td class="line x" title="52:224	This is often done using backpointers, but can also be accomplished by representing the most probable proof for each theorem in its entirety as part of the semiring value (Goodman, 1999)." ></td>
	<td class="line x" title="53:224	For generality, we define a proof as a string that is constructed from strings associated with axioms, but the particular form of a proof is problem-dependent." ></td>
	<td class="line x" title="54:224	The Viterbi proof semiring includes the probability of the most probable proof and the proof itself." ></td>
	<td class="line x" title="55:224	Letting L   be the proof language on some symbol set , this semiring is defined on the set R0 L with 0 element 0,epsilon1 and 1 element 1,epsilon1." ></td>
	<td class="line x" title="56:224	For two values u1,U1 and u2,U2, the aggregation operator returns max(u1,u2),Uargmaxi{1,2} ui." ></td>
	<td class="line x" title="57:224	1When cycles are permitted, i.e., where the value of one variable depends on itself, infinite sums can be involved." ></td>
	<td class="line x" title="58:224	We must ensure that these infinite sums are well defined under the semiring." ></td>
	<td class="line x" title="59:224	So-called complete semirings satisfy additional conditions to handle infinite sums, but for simplicity we will restrict our attention to DPs that do not involve cycles." ></td>
	<td class="line x" title="60:224	319 Semiring A Aggregation () Combination () 0 1 inside R0 u1 + u2 u1u2 0 1 Viterbi R0 max(u1,u2) u1u2 0 1 Viterbi proof R0 L max(u1,u2),Uargmaxi{1,2} ui u1u2,U1.U2 0,epsilon1 1,epsilon1 k-best proof (R0 L)k max-k(u1 u2) max-k(u1 staru2)  {1,epsilon1} Table 1: Commonly used semirings." ></td>
	<td class="line x" title="61:224	An element in the Viterbi proof semiring is denoted u1,U1, where u1 is the probability of proof U1." ></td>
	<td class="line x" title="62:224	The max-k function returns a sorted list of the top-k proofs from a set." ></td>
	<td class="line x" title="63:224	The star function performs a cross-product on two k-best proof lists (Eq." ></td>
	<td class="line x" title="64:224	2)." ></td>
	<td class="line x" title="65:224	The combination operator returns u1u2,U1.U2, where U1.U2 denotes the string concatenation of U1 and U2.2 k-best proof semiring." ></td>
	<td class="line x" title="66:224	The k-best proof semiring computes the values and proof strings of the k most-probable proofs for each theorem." ></td>
	<td class="line x" title="67:224	The set is (R0  L)k, i.e., sequences (up to length k) of sorted probability/proof pairs." ></td>
	<td class="line x" title="68:224	The aggregation operator  uses max-k, which chooses the k highest-scoring proofs from its argument (a set of scored proofs) and sorts them in decreasing order." ></td>
	<td class="line x" title="69:224	To define the combination operator , we require a cross-product that pairs probabilities and proofs from two k-best lists." ></td>
	<td class="line x" title="70:224	We call this star, defined on two semiring values u = u1,U1,,uk,Uk and v = v1,V1,,vk,Vk by: ustarv = {uivj,Ui.Vj | i,j  {1,,k}} (2) Then, uv = max-k(u star v)." ></td>
	<td class="line x" title="71:224	This is similar to the k-best semiring defined by Goodman (1999)." ></td>
	<td class="line x" title="72:224	These semirings are summarized in Table 1." ></td>
	<td class="line x" title="73:224	2.3 Features and Inference Let X be the space of inputs to our logic program, i.e., x  X is a set of axioms." ></td>
	<td class="line x" title="74:224	Let L denote the proof language and let Y  L denote the set of proof strings that constitute full proofs, i.e., proofs of the special goal theorem." ></td>
	<td class="line x" title="75:224	We assume an exponential probabilistic model such that p(y | x) producttextMm=1 hm(x,y)m (3) where each m  0 is a parameter of the model and each hm is a feature function." ></td>
	<td class="line x" title="76:224	There is a bijection betweenYand the space of discrete structures that our model predicts." ></td>
	<td class="line x" title="77:224	Given such a model, DP is helpful for solving two kinds of inference problems." ></td>
	<td class="line x" title="78:224	The first problem, decoding, is to find the highest scoring proof 2We assume for simplicity that the best proof will never be a tie among more than one proof." ></td>
	<td class="line x" title="79:224	Goodman (1999) handles this situation more carefully, though our version is more likely to be used in practice for both the Viterbi proof and k-best proof semirings." ></td>
	<td class="line x" title="80:224	y  Y for a given input x  X: y(x) = argmaxyYproducttextMm=1 mhm(x,y) (4) The second is the summing problem, which marginalizes the proof probabilities (without normalization): s(x) = summationtextyYproducttextMm=1 mhm(x,y) (5) Asdefined,thefeaturefunctionshm candepend on arbitrary parts of the input axiom set x and the entire output proof y. 2.4 Proof and Feature Locality An important characteristic of problems suited for DP is that the global calculation (i.e., the value of goal) depend only on local factored parts." ></td>
	<td class="line x" title="81:224	In DP equations, this means that each equation connects a relatively small number of indexed variables related through a relatively small number of indices." ></td>
	<td class="line x" title="82:224	In the logic programming formulation, it means thateachstepoftheproofdependsonlyonthetheorems being used at that step, not the full proofs of those theorems." ></td>
	<td class="line x" title="83:224	We call this property proof locality." ></td>
	<td class="line x" title="84:224	In the statistical modeling view of Eq." ></td>
	<td class="line x" title="85:224	3, classical DP requires that the probability model make strong Markovian conditional independence assumptions (e.g., in HMMs, St1  St+1 | St); in exponential families over discrete structures, this corresponds to feature locality." ></td>
	<td class="line x" title="86:224	For a particular proof y of goal consisting of t intermediate theorems, we define a set of proof strings lscripti  L for i  {1,,t}, where lscripti corresponds to the proof of the ith theorem.3 We can break the computation of feature function hm into a summation over terms corresponding to each lscripti: hm(x,y) = summationtextti=1 fm(x,lscripti) (6) This is simply a way of noting that feature functions fire incrementally at specific points in the 3The theorem indexing scheme might be based on a topological ordering given by the proof structure, but is not important for our purposes." ></td>
	<td class="line x" title="87:224	320 proof, normally at the first opportunity." ></td>
	<td class="line x" title="88:224	Any feature function can be expressed this way." ></td>
	<td class="line x" title="89:224	For local features, we can go farther; we define a function top(lscript) that returns the proof string corresponding to the antecedents and consequent of the last inference step in lscript." ></td>
	<td class="line x" title="90:224	Local features have the property: hlocm (x,y) = summationtextti=1 fm(x,top(lscripti)) (7) Local features only have access to the most recent deductive proof step (though they may fire repeatedly in the proof), while non-local features have access to the entire proof up to a given theorem." ></td>
	<td class="line x" title="91:224	For both kinds of features, the f terms are used within the DP formulation." ></td>
	<td class="line x" title="92:224	When taking an inference step to prove theorem i, the valueproducttext M m=1  fm(x,lscripti)m is combined into the calculation of that theorems value, along with the values of the antecedents." ></td>
	<td class="line x" title="93:224	Note that typically only a small number of fm are nonzero for theorem i. Whennon-localhm/fm thatdependonarbitrary parts of the proof are involved, the decoding and summing inference problems are NP-hard (they instantiate probabilistic inference in a fully connected graphical model)." ></td>
	<td class="line x" title="94:224	Sometimes, it is possible toachieveprooflocalitybyaddingmoreindicesto the DP variables (for example, consider modifyingthebigramHMMViterbialgorithmfortrigram HMMs)." ></td>
	<td class="line x" title="95:224	This increases the number of variables and hence computational cost." ></td>
	<td class="line x" title="96:224	In general, it leads to exponential-time inference in the worst case." ></td>
	<td class="line x" title="97:224	There have been many algorithms proposed for approximately solving instances of these decoding and summing problems with non-local features." ></td>
	<td class="line oc" title="98:224	Some stem from work on graphical models,includingloopybeliefpropagation(Suttonand McCallum, 2004; Smith and Eisner, 2008), Gibbs sampling (Finkel et al., 2005), sequential Monte Carlo methods such as particle filtering (Levy et al., 2008), and variational inference (Jordan et al., 1999; MacKay, 1997; Kurihara and Sato, 2006)." ></td>
	<td class="line x" title="99:224	Also relevant are stacked learning (Cohen and Carvalho, 2005), interpretable as approximation of non-local feature values (Martins et al., 2008), and M-estimation (Smith et al., 2007), which allows training without inference." ></td>
	<td class="line x" title="100:224	Several other approaches used frequently in NLP are approximate methods for decoding only." ></td>
	<td class="line x" title="101:224	These include beam search (Lowerre, 1976), cube pruning, which we discuss in 3, integer linear programming (Roth and Yih, 2004), in which arbitrary features can act as constraints on y, and approximate solutions like McDonald and Pereira (2006), in which an exact solution to a related decoding problem is found and then modified to fit the problem of interest." ></td>
	<td class="line x" title="102:224	3 Approximate Decoding Cube pruning (Chiang, 2007; Huang and Chiang, 2007) is an approximate technique for decoding (Eq." ></td>
	<td class="line x" title="103:224	4); it is used widely in machine translation." ></td>
	<td class="line x" title="104:224	Given proof locality, it is essentially an efficient implementation of the k-best proof semiring." ></td>
	<td class="line x" title="105:224	Cube pruning goes farther in that it permits nonlocal features to weigh in on the proof probabilities, at the expense of making the k-best operation approximate." ></td>
	<td class="line x" title="106:224	We describe the two approximations cube pruning makes, then propose cube decoding, which removes the second approximation." ></td>
	<td class="line x" title="107:224	Cube decoding cannot be represented as a semiring; we propose a more general algebraic structure that accommodates it." ></td>
	<td class="line x" title="108:224	3.1 Approximations in Cube Pruning Cubepruning isan approximatesolution tothe decoding problem (Eq." ></td>
	<td class="line x" title="109:224	4) in two ways." ></td>
	<td class="line x" title="110:224	Approximation 1: k < ." ></td>
	<td class="line x" title="111:224	Cube pruning uses a finite k for the k-best lists stored in each value." ></td>
	<td class="line x" title="112:224	If k = , the algorithm performs exact decoding with non-local features (at obviously formidable expense in combinatorial problems)." ></td>
	<td class="line x" title="113:224	Approximation 2: lazy computation." ></td>
	<td class="line x" title="114:224	Cube pruning exploits the fact that k <  to use lazy computation." ></td>
	<td class="line x" title="115:224	When combining the k-best proof lists of d theorems values, cube pruning does not enumerate all kd proofs, apply non-local features to all of them, and then return the top k. Instead, cube pruning uses a more efficient but approximate solution that only calculates the non-local factors on O(k) proofs to obtain the approximate top k. This trick is only approximate if non-local features are involved." ></td>
	<td class="line x" title="116:224	Approximation 2 makes it impossible to formulate cube pruning using separate aggregation and combination operations, as the use of lazy computation causes these two operations to effectively be performed simultaneously." ></td>
	<td class="line x" title="117:224	To more directly relate our summing algorithm (4) to cube pruning, we suggest a modified version of cube pruning that does not use lazy computation." ></td>
	<td class="line x" title="118:224	We call this algorithm cube decoding." ></td>
	<td class="line x" title="119:224	This algorithm can be written down in terms of separate aggregation 321 and combination operations, though we will show it is not a semiring." ></td>
	<td class="line x" title="120:224	3.2 Cube Decoding We formally describe cube decoding, show that it does not instantiate a semiring, then describe a more general algebraic structure that it does instantiate." ></td>
	<td class="line x" title="121:224	ConsiderthesetGofnon-localfeaturefunctions that map XL  R0.4 Our definitions in 2.2 for the k-best proof semiring can be expanded to accommodate these functions within the semiring value." ></td>
	<td class="line x" title="122:224	Recall that values in the k-best proof semiring fall inAk = (R0L)k. For cube decoding, we use a different set Acd defined as Acd = (R0 L)kbracehtipupleft bracehtipdownrightbracehtipdownleft bracehtipupright Ak G{0,1} where the binary variable indicates whether the value contains a k-best list (0, which we call an ordinary value) or a non-local feature function in G (1, which we call a function value)." ></td>
	<td class="line x" title="123:224	We denote a value u  Acd by u = u1,U1,u2,U2,,uk,Ukbracehtipupleft bracehtipdownrightbracehtipdownleft bracehtipupright u ,gu,us where each ui  R0 is a probability and each Ui  L is a proof string." ></td>
	<td class="line x" title="124:224	We use k and k to denote the k-best proof semirings operators, defined in 2.2." ></td>
	<td class="line x" title="125:224	We let g0 be such that g0(lscript) is undefined for all lscript  L. For two values u = u,gu,us,v = v,gv,vs  Acd, cube decodings aggregation operator is: ucd v = uk v,g0,0 if us vs (8) Under standard models, only ordinary values will beoperandsofcd, socd isundefinedwhenus vs. We define the combination operator cd: ucd v = (9)     uk v,g0,0 if us vs, max-k(exec(gv,u)),g0,0 if us vs, max-k(exec(gu,v)),g0,0 if us vs, ,z.(gu(z)gv(z)),1 if us vs. where exec(g,u) executes the function g upon each proof in the proof list u, modifies the scores 4In our setting, gm(x,lscript) will most commonly be defined as fm(x,lscript)m in the notation of 2.3." ></td>
	<td class="line x" title="126:224	But functions in G could also be used to implement, e.g., hard constraints or other nonlocal score factors." ></td>
	<td class="line x" title="127:224	in place by multiplying in the function result, and returns the modified proof list: gprime = lscript.g(x,lscript) exec(g,u) = u1gprime(U1),U1,u2gprime(U2),U2, ,ukgprime(Uk),Uk Here, max-k is simply used to re-sort the k-best proof list following function evaluation." ></td>
	<td class="line x" title="128:224	The semiring properties fail to hold when introducing non-local features in this way." ></td>
	<td class="line x" title="129:224	In particular, cd is not associative when 1 < k < ." ></td>
	<td class="line x" title="130:224	Forexample, considertheprobabilisticCKYalgorithmasabove, butusingthecubedecodingsemiring with the non-local feature functions collectively known as NGramTree features (Huang, 2008) that score the string of terminals and nonterminals along the path from word j to word j + 1 when two constituents CY,i,j and CZ,j,k are combined." ></td>
	<td class="line x" title="131:224	The semiring value associated with such a feature is u = ,NGramTreepi(),1 (for a specific path pi), and we rewrite Eq." ></td>
	<td class="line x" title="132:224	1 as follows (where ranges for summation are omitted for space): CX,i,k = circleplustextcd pXYZ cdCY,i,j cdCZ,j,kcdu The combination operator is not associative since the following will give different answers:5 (pXYZ cd CY,i,j)cd (CZ,j,k cd u) (10) ((pXYZ cd CY,i,j)cd CZ,j,k)cd u (11) In Eq." ></td>
	<td class="line x" title="133:224	10, the non-local feature function is executed on the k-best proof list for Z, while in Eq." ></td>
	<td class="line x" title="134:224	11, NGramTreepi is called on the k-best proof list for the X constructed from Y and Z. Furthermore, neither of the above gives the desired result, since we actually wish to expand the full set of k2 proofs of X and then apply NGramTreepi to each of them (or a higher-dimensional cube if more operands are present) before selecting the k-best." ></td>
	<td class="line x" title="135:224	The binary operations above retain only the top k proofs of X in Eq." ></td>
	<td class="line x" title="136:224	11 before applying NGramTreepi to each of them." ></td>
	<td class="line x" title="137:224	We actually would like to redefine combination so that it can operate on arbitrarily-sized sets of values." ></td>
	<td class="line x" title="138:224	We can understand cube decoding through an algebraic structure with two operations  and , where  need not be associative and need not distributeover,andfurthermorewhereandare 5Distributivity of combination over aggregation fails for related reasons." ></td>
	<td class="line x" title="139:224	We omit a full discussion due to space." ></td>
	<td class="line x" title="140:224	322 defined on arbitrarily many operands." ></td>
	<td class="line x" title="141:224	We will refer here to such a structure as a generalized semiring.6 To define cd on a set of operands with Nprime ordinary operands and N function operands, we first compute the full O(kNprime) cross-product of the ordinary operands, then apply each of the N functionsfromtheremainingoperandsinturnuponthe full Nprime-dimensional cube, finally calling max-k on the result." ></td>
	<td class="line x" title="142:224	4 Cube Summing We present an approximate solution to the summing problem when non-local features are involved, which we call cube summing." ></td>
	<td class="line x" title="143:224	It is an extension of cube decoding, and so we will describe it as a generalized semiring." ></td>
	<td class="line x" title="144:224	The key addition is to maintainineachvalue,inadditiontothek-bestlist of proofs from Ak, a scalar corresponding to the residual probability(possiblyunnormalized)ofall proofs not among the k-best.7 The k-best proofs are still used for dynamically computing non-local features but the aggregation and combination operations are redefined to update the residual as appropriate." ></td>
	<td class="line x" title="145:224	We define the set Acs for cube summing as Acs = R0 (R0 L)k G{0,1} A value u  Acs is defined as u = u0,u1,U1,u2,U2,,uk,Ukbracehtipupleft bracehtipdownrightbracehtipdownleft bracehtipupright u ,gu,us For a proof list u, we use bardblubardbl to denote the sum of all proof scores,summationtexti:ui,Uiu ui." ></td>
	<td class="line x" title="146:224	The aggregation operator over operands {ui}Ni=1, all such that uis = 0,8 is defined by: circleplustextN i=1 ui = (12)angbracketleftBigsummationtext N i=1 ui0 + vextenddoublevextenddouble vextenddoubleRes parenleftBiguniontextN i=1 ui parenrightBigvextenddoublevextenddouble vextenddouble, max-k parenleftBiguniontextN i=1 ui parenrightBig ,g0,0 angbracketrightBig 6Algebraicstructuresaretypicallydefinedwithbinaryoperators only, so we were unable to find a suitable term for this structure in the literature." ></td>
	<td class="line x" title="147:224	7Blunsom and Osborne (2008) described a related approach to approximate summing using the chart computed during cube pruning, but did not keep track of the residual terms as we do here." ></td>
	<td class="line x" title="148:224	8We assume that operands ui to cs will never be such that uis = 1 (non-local feature functions)." ></td>
	<td class="line x" title="149:224	This is reasonable in the widely used log-linear model setting we have adopted, where weights m are factors in a proofs product score." ></td>
	<td class="line x" title="150:224	where Res returns the residual set of scored proofs not in the k-best among its arguments, possibly the empty set." ></td>
	<td class="line x" title="151:224	ForasetofN+Nprime operands{vi}Ni=1{wj}Nprimej=1 such that vis = 1 (non-local feature functions) and wjs = 1 (ordinary values), the combination operator  is shown in Eq." ></td>
	<td class="line x" title="152:224	13 Fig." ></td>
	<td class="line x" title="153:224	1." ></td>
	<td class="line x" title="154:224	Note that the case where Nprime = 0 is not needed in this application; an ordinary value will always be included in combination." ></td>
	<td class="line x" title="155:224	In the special case of two ordinary operands (where us = vs = 0), Eq." ></td>
	<td class="line x" title="156:224	13 reduces to uv = (14) u0v0 + u0 bardblvbardbl+ v0 bardblubardbl+bardblRes(ustar v)bardbl, max-k(ustar v),g0,0 We define 0 as 0,,g0,0; an appropriate definition for the combination identity element is less straightforward and of little practical importance; we leave it to future work." ></td>
	<td class="line x" title="157:224	If we use this generalized semiring to solve a DP and achieve goal value of u, the approximate sumof allproof probabilitiesis givenbyu0+bardblubardbl." ></td>
	<td class="line x" title="158:224	Ifallfeaturesarelocal, theapproachisexact." ></td>
	<td class="line x" title="159:224	With non-local features, the k-best list may not contain the k-best proofs, and the residual score, while including all possible proofs, may not include all of the non-local features in all of those proofs probabilities." ></td>
	<td class="line x" title="160:224	5 Implementation We have so far viewed dynamic programming algorithms in terms of their declarative specifications as semiring-weighted logic programs." ></td>
	<td class="line x" title="161:224	Solvers have been proposed by Goodman (1999), by Klein and Manning (2001) using a hypergraph representation, and by Eisner et al.(2005)." ></td>
	<td class="line x" title="163:224	BecauseGoodmansandEisneretal.salgorithmsassume semirings, adapting them for cube summing is non-trivial.9 To generalize Goodmans algorithm, we suggest using the directed-graph data structure known variously as an arithmetic circuit or computation graph.10 Arithmetic circuits have recently drawn interest in the graphical model community as a 9The bottom-up agenda algorithm in Eisner et al.(2005) might possibly be generalized so that associativity, distributivity, and binary operators are not required (John Blatz, p.c.)." ></td>
	<td class="line x" title="165:224	10This data structure is not specific to any particular set of operations." ></td>
	<td class="line x" title="166:224	We have also used it successfully with the inside semiring." ></td>
	<td class="line x" title="167:224	323 Ncirclemultiplydisplay i=1 vi  Nprimecirclemultiplydisplay j=1 wj = angbracketleftBigg  summationdisplay BP(S) productdisplay bB wb0 productdisplay cS\B bardblwcbardbl   (13) +bardblRes(exec(gv1,exec(gvN, w1 starstar wNprime)))bardbl, max-k(exec(gv1,exec(gvN, w1 starstar wNprime))),g0,0 angbracketrightBig Figure 1: Combination operation for cube summing, where S = {1,2,,Nprime} and P(S) is the power set of S excluding ." ></td>
	<td class="line x" title="168:224	tool for performing probabilistic inference (Darwiche, 2003)." ></td>
	<td class="line x" title="169:224	In the directed graph, there are vertices corresponding to axioms (these are sinks in the graph),  vertices corresponding to theorems, and  vertices corresponding to summands in the dynamic programming equations." ></td>
	<td class="line x" title="170:224	Directed edges point from each node to the nodes it depends on; vertices depend onvertices, which depend on  and axiom vertices." ></td>
	<td class="line x" title="171:224	Arithmetic circuits are amenable to automatic differentiation in the reverse mode (Griewank and Corliss, 1991), commonly used in backpropagation algorithms." ></td>
	<td class="line x" title="172:224	Importantly, this permits us to calculate the exact gradient of the approximate summation with respect to axiom values, following Eisner et al.(2005)." ></td>
	<td class="line x" title="174:224	This is desirable when carrying out the optimization problems involved in parameter estimation." ></td>
	<td class="line x" title="175:224	Another differentiation technique, implemented within the semiring, is given by Eisner (2002)." ></td>
	<td class="line x" title="176:224	Cube pruning is based on the k-best algorithms of Huang and Chiang (2005), which save time over generic semiring implementations through lazycomputationinboththeaggregationandcombination operations." ></td>
	<td class="line x" title="177:224	Their techniques are not as clearly applicable here, because our goal is to sum over all proofs instead of only finding a small subset of them." ></td>
	<td class="line x" title="178:224	If computing non-local features is a computational bottleneck, they can be computed only for the O(k) proofs considered when choosing the best k as in cube pruning." ></td>
	<td class="line x" title="179:224	Then, the computational requirements for approximate summing are nearly equivalent to cube pruning, but the approximation is less accurate." ></td>
	<td class="line x" title="180:224	6 Semirings Old and New We now consider interesting special cases and variations of cube summing." ></td>
	<td class="line x" title="181:224	6.1 The k-best+residual Semiring When restricted to local features, cube pruning and cube summing can be seen as proper semirk-best proof (Goodman, 1999) k-best + residual Viterbi proof (Goodman, 1999) all proof (Goodman, 1999) Viterbi (Viterbi, 1967) ignore proof inside (Baum et al., 1970) i g n o r e  r e s i d u a l k  =  0 k  =   k  =  1 Figure 2: Semirings generalized by k-best+residual." ></td>
	<td class="line x" title="182:224	ings." ></td>
	<td class="line x" title="183:224	Cube pruning reduces to an implementation of thek-best semiring (Goodman, 1998), and cube summing reduces to a novel semiring we call the k-best+residual semiring." ></td>
	<td class="line x" title="184:224	Binary instantiations of  and  can be iteratively reapplied to give the equivalent formulations in Eqs." ></td>
	<td class="line x" title="185:224	12 and 13." ></td>
	<td class="line x" title="186:224	We define 0 as 0, and 1 as 1,1,epsilon1." ></td>
	<td class="line x" title="187:224	The  operator is easily shown to be commutative." ></td>
	<td class="line x" title="188:224	That  is associative follows from associativity of max-k, shown by Goodman (1998)." ></td>
	<td class="line x" title="189:224	Showing that  is associative and that  distributes over  are less straightforward; proof sketches are provided in Appendix A. The k-best+residual semiring generalizes many semirings previously introduced in the literature; see Fig." ></td>
	<td class="line x" title="190:224	2." ></td>
	<td class="line x" title="191:224	6.2 Variations Once we relax requirements about associativity anddistributivityandpermitaggregationandcombination operators to operate on sets, several extensions to cube summing become possible." ></td>
	<td class="line x" title="192:224	First, when computing approximate summations with non-local features, we may not always be interested in the best proofs for each item." ></td>
	<td class="line x" title="193:224	Since the purpose of summing is often to calculate statistics 324 under a model distribution, we may wish instead to sample from that distribution." ></td>
	<td class="line x" title="194:224	We can replace the max-k function with a sample-k function that samples k proofs from the scored list in its argument, possibly using the scores or possibly uniformly at random." ></td>
	<td class="line x" title="195:224	This breaks associativity of ." ></td>
	<td class="line x" title="196:224	We conjecture that this approach can be used to simulate particle filtering for structured models." ></td>
	<td class="line x" title="197:224	Another variation is to vary k for different theorems." ></td>
	<td class="line x" title="198:224	This might be used to simulate beam search, or to reserve computation for theorems closer to goal, which have more proofs." ></td>
	<td class="line x" title="199:224	7 Conclusion This paper has drawn a connection between cube pruning, a popular technique for approximately solving decoding problems, and the semiringweighted logic programming view of dynamic programming." ></td>
	<td class="line x" title="200:224	We have introduced a generalization called cube summing, to be used for solving summing problems, and have argued that cube pruningandcubesummingarebothsemiringsthat can be used generically, as long as the underlying probability models only include local features." ></td>
	<td class="line x" title="201:224	With non-local features, cube pruning and cubesummingcanbeusedforapproximatedecoding and summing, respectively, and although they no longer correspond to semirings, generic algorithms can still be used." ></td>
	<td class="line x" title="202:224	Acknowledgments We thank three anonymous EACL reviewers, John Blatz, Pedro Domingos, Jason Eisner, Joshua Goodman, and members of the ARK group for helpful comments and feedback that improved this paper." ></td>
	<td class="line x" title="203:224	This research was supported by NSF IIS-0836431 and an IBM faculty award." ></td>
	<td class="line x" title="204:224	A k-best+residual is a Semiring In showing that k-best+residual is a semiring, we will restrict our attention to the computation of the residuals." ></td>
	<td class="line x" title="205:224	The computation over proof lists is identical to that performed in the k-best proof semiring, which was shown to be a semiring by Goodman (1998)." ></td>
	<td class="line x" title="206:224	We sketch the proofs that  is associative and that  distributes over ; associativity of  is straightforward." ></td>
	<td class="line x" title="207:224	For a proof list a, bardblabardbl denotes the sum of proof scores,P i:ai,Aia ai." ></td>
	<td class="line x" title="208:224	Note that: bardblRes(a)bardbl+bardblmax-k(a)bardbl =bardblabardbl (15) astar b =bardblabardblb (16) Associativity." ></td>
	<td class="line x" title="209:224	Given three semiring values u, v, and w, we need to show that(uv)w = u(vw)." ></td>
	<td class="line x" title="210:224	After expanding the expressions for the residuals using Eq." ></td>
	<td class="line x" title="211:224	14, there are 10 terms on each side, five of which are identical and cancel out immediately." ></td>
	<td class="line x" title="212:224	Three more cancel using Eq." ></td>
	<td class="line x" title="213:224	15, leaving: LHS = bardblRes(ustar v)bardblbardblwbardbl+bardblRes(max-k(ustar v) star w)bardbl RHS = bardblubardblbardblRes(v star w)bardbl+bardblRes(ustar max-k(v star w))bardbl If LHS = RHS, associativity holds." ></td>
	<td class="line x" title="214:224	Using Eq." ></td>
	<td class="line x" title="215:224	15 again, we can rewrite the second term in LHS to obtain LHS =bardblRes(ustar v)bardblbardblwbardbl+bardblmax-k(ustar v) star wbardbl bardblmax-k(max-k(ustar v) star w)bardbl Using Eq.16 and pullingout thecommon termbardblwbardbl, wehave LHS =(bardblRes(ustar v)bardbl+bardblmax-k(ustar v)bardbl)bardblwbardbl bardblmax-k(max-k(ustar v) star w)bardbl =bardbl(ustar v) star wbardblbardblmax-k(max-k(ustar v) star w)bardbl =bardbl(ustar v) star wbardblbardblmax-k((ustar v) star w)bardbl Theresultingexpressionisintuitive: theresidualof(uv) w is the difference between the sum of all proof scores and the sum of the k-best." ></td>
	<td class="line x" title="216:224	RHS can be transformed into this same expression with a similar line of reasoning (and using associativity of star)." ></td>
	<td class="line x" title="217:224	Therefore, LHS = RHS and  is associative." ></td>
	<td class="line x" title="218:224	Distributivity." ></td>
	<td class="line x" title="219:224	To prove that  distributes over , we must showleft-distributivity,i.e.,thatu(vw) = (uv)(u w), and right-distributivity." ></td>
	<td class="line x" title="220:224	We show left-distributivity here." ></td>
	<td class="line x" title="221:224	As above, we expand the expressions, finding 8 terms on the LHS and 9 on the RHS." ></td>
	<td class="line x" title="222:224	Six on each side cancel, leaving: LHS = bardblRes(v w)bardblbardblubardbl+bardblRes(ustar max-k(v w))bardbl RHS = bardblRes(ustar v)bardbl+bardblRes(ustar w)bardbl +bardblRes(max-k(ustar v)max-k(ustar w))bardbl We can rewrite LHS as: LHS =bardblRes(v w)bardblbardblubardbl+bardblustar max-k(v w)bardbl bardblmax-k(ustar max-k(v w))bardbl =bardblubardbl(bardblRes(v w)bardbl+bardblmax-k(v w)bardbl) bardblmax-k(ustar max-k(v w))bardbl =bardblubardblbardblv wbardblbardblmax-k(ustar (v w))bardbl =bardblubardblbardblv wbardblbardblmax-k((ustar v)(ustar w))bardbl where the last line follows because star distributes over  (Goodman, 1998)." ></td>
	<td class="line x" title="223:224	We now work with the RHS: RHS =bardblRes(ustar v)bardbl+bardblRes(ustar w)bardbl +bardblRes(max-k(ustar v)max-k(ustar w))bardbl =bardblRes(ustar v)bardbl+bardblRes(ustar w)bardbl +bardblmax-k(ustar v)max-k(ustar w)bardbl bardblmax-k(max-k(ustar v)max-k(ustar w))bardbl Since max-k(u star v) and max-k(u star w) are disjoint (we assume no duplicates; i.e., two different theorems cannot have exactly the same proof), the third term becomes bardblmax-k(ustar v)bardbl+bardblmax-k(ustar w)bardbl and we have =bardblustar vbardbl+bardblustar wbardbl bardblmax-k(max-k(ustar v)max-k(ustar w))bardbl =bardblubardblbardblvbardbl+bardblubardblbardblwbardbl bardblmax-k((ustar v)(ustar w))bardbl =bardblubardblbardblv wbardblbardblmax-k((ustar v)(ustar w))bardbl." ></td>
	<td class="line x" title="224:224	325" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E09-1091
MINT: A Method for Effective and Scalable Mining of Named Entity Transliterations from Large Comparable Corpora
Udupa, Raghavendra;Saravanan, K;Kumaran, A.;Jagarlamudi, Jagadeesh;"></td>
	<td class="line x" title="1:186	Proceedings of the 12th Conference of the European Chapter of the ACL, pages 799807, Athens, Greece, 30 March  3 April 2009." ></td>
	<td class="line x" title="2:186	c2009 Association for Computational Linguistics MINT: A Method for Effective and Scalable Mining of Named Entity Transliterations from Large Comparable Corpora Raghavendra Udupa         K Saravanan         A Kumaran        Jagadeesh Jagarlamudi* Microsoft Research India Bangalore 560080 INDIA  [raghavu,v-sarak,kumarana,jags}@microsoft.com  Abstract In this paper, we address the problem of mining transliterations of Named Entities (NEs) from large comparable corpora." ></td>
	<td class="line x" title="3:186	We leverage the empirical fact that multilingual news articles with similar news content are rich in Named Entity Transliteration Equivalents (NETEs)." ></td>
	<td class="line x" title="4:186	Our mining algorithm, MINT, uses a cross-language document similarity model to align multilingual news articles and then mines NETEs from the aligned articles using a transliteration similarity model." ></td>
	<td class="line x" title="5:186	We show that our approach is highly effective on 6 different comparable corpora between English and 4 languages from 3 different language families." ></td>
	<td class="line x" title="6:186	Furthermore, it performs substantially better than a state-of-the-art competitor." ></td>
	<td class="line x" title="7:186	1 Introduction Named Entities (NEs) play a critical role in many Natural Language Processing and Information Retrieval (IR) tasks." ></td>
	<td class="line x" title="8:186	In Cross-Language Information Retrieval (CLIR) systems, they play an even more important role as the accuracy of their transliterations is shown to correlate highly with the performance of the CLIR systems (Mandl and Womser-Hacker, 2005, Xu and Weischedel, 2005)." ></td>
	<td class="line x" title="9:186	Traditional methods for transliterations have not proven to be very effective in CLIR." ></td>
	<td class="line x" title="10:186	Machine Transliteration systems (AbdulJaleel and Larkey, 2003; Al-Onaizan and Knight, 2002; Virga and Khudanpur, 2003) usually produce incorrect transliterations and translation lexcions such as hand-crafted or statistical dictionaries are too static to have good coverage of NEs1 occurring in the current news events." ></td>
	<td class="line x" title="11:186	Hence, there is a critical need for creating and continually updat * Currently with University of Utah." ></td>
	<td class="line x" title="12:186	1 New NEs are introduced to the vocabulary of a language every day." ></td>
	<td class="line x" title="13:186	On an average, 260 and 452 new NEs appeared daily in the XIE and AFE segments of the LDC English Gigaword corpora respectively." ></td>
	<td class="line x" title="14:186	ing multilingual Named Entity transliteration lexicons." ></td>
	<td class="line x" title="15:186	The ubiquitous availability of comparable news corpora in multiple languages suggests a promising alternative to Machine Transliteration, namely, the mining of Named Entity Transliteration Equivalents (NETEs) from such corpora." ></td>
	<td class="line x" title="16:186	News stories are typically rich in NEs and therefore, comparable news corpora can be expected to contain NETEs (Klementiev and Roth, 2006; Tao et al., 2006)." ></td>
	<td class="line x" title="17:186	The large quantity and the perpetual availability of news corpora in many of the worlds languages, make mining of NETEs a viable alternative to traditional approaches." ></td>
	<td class="line x" title="18:186	It is this opportunity that we address in our work." ></td>
	<td class="line x" title="19:186	In this paper, we detail an effective and scalable mining method, called MINT (MIning Named-entity Transliteration equivalents), for mining of NETEs from large comparable corpora." ></td>
	<td class="line x" title="20:186	MINT addresses several challenges in mining NETEs from large comparable corpora: exhaustiveness (in mining sparse NETEs), computational efficiency (in scaling on corpora size), language independence (in being applicable to many language pairs) and linguistic frugality (in requiring minimal external linguistic resources)." ></td>
	<td class="line x" title="21:186	Our contributions are as follows:  We give empirical evidence for the hypothesis that news articles in different languages with reasonably similar content are rich sources of NETEs (Udupa, et al., 2008)." ></td>
	<td class="line x" title="22:186	 We demonstrate that the above insight can be translated into an effective approach for mining NETEs from large comparable corpora even when similar articles are not known a priori." ></td>
	<td class="line x" title="23:186	 We demonstrate MINTs effectiveness on 4 language pairs involving 5 languages (English, Hindi, Kannada, Russian, and Tamil) from 3 different language families, and its scalability on corpora of vastly different sizes (2,000 to 200,000 articles)." ></td>
	<td class="line x" title="24:186	 We show that MINTs performance is significantly better than a state of the art method (Klementiev and Roth, 2006)." ></td>
	<td class="line x" title="25:186	799 We discuss the motivation behind our approach in Section 2 and present the details in Section 3." ></td>
	<td class="line x" title="26:186	In Section 4, we describe the evaluation process and in Section 5, we present the results and analysis." ></td>
	<td class="line x" title="27:186	We discuss related work in Section 6." ></td>
	<td class="line x" title="28:186	2 Motivation MINT is based on the hypothesis that news articles in different languages with similar content contain highly overlapping set of NEs." ></td>
	<td class="line x" title="29:186	News articles are typically rich in NEs as news is about events involving people, locations, organizations, etc2." ></td>
	<td class="line x" title="30:186	It is reasonable to expect that multilingual news articles reporting the same news event mention the same NEs in the respective languages." ></td>
	<td class="line x" title="31:186	For instance, consider the English and Hindi news reports from the New York Times and the BBC on the second oath taking of President Barack Obama (Figure 1)." ></td>
	<td class="line x" title="32:186	The articles are not parallel but discuss the same event." ></td>
	<td class="line x" title="33:186	Naturally, they mention the same NEs (such as Barack Obama, John Roberts, White House) in the respective languages, and hence, are rich sources of NETEs." ></td>
	<td class="line x" title="34:186	Our empirical investigation of comparable corpora confirmed the above insight." ></td>
	<td class="line x" title="35:186	A study of  2 News articles from the BBC corpus had, on an average, 12.9 NEs and new articles from the The New Indian Express, about 11.8 NEs." ></td>
	<td class="line x" title="36:186	200 pairs of similar news articles published by The New Indian Express in 2007 in English and Tamil showed that 87% of the single word NEs in the English articles had at least one transliteration equivalent in the conjugate Tamil articles." ></td>
	<td class="line x" title="37:186	The MINT method leverages this empirically backed insight to mine NETEs from such comparable corpora." ></td>
	<td class="line x" title="38:186	However, there are several challenges to the mining process: firstly, vast majority of the NEs in comparable corpora are very sparse; our analysis showed that 80% of the NEs in The New Indian Express news corpora appear less than 5 times in the entire corpora." ></td>
	<td class="line x" title="39:186	Hence, any mining method that depends mainly on repeated occurrences of the NEs in the corpora is likely to miss vast majority of the NETEs." ></td>
	<td class="line x" title="40:186	Secondly, the mining method must restrict the candidate NETEs that need to be examined for match to a reasonably small number, not only to minimize false positives but also to be computationally efficient." ></td>
	<td class="line x" title="41:186	Thirdly, the use of linguistic tools and resources must be kept to a minimum as resources are available only in a handful of languages." ></td>
	<td class="line x" title="42:186	Finally, it is important to use as little language-specific knowledge as possible in order to make the mining method applicable across a vast majority of languages of the world." ></td>
	<td class="line x" title="43:186	The MINT method proposed in this paper addresses all the above issues." ></td>
	<td class="line x" title="44:186	800 3 The MINT Mining Method MINT has two stages." ></td>
	<td class="line x" title="45:186	In the first stage, for every document in the source language side, the set of documents in the target language side with similar news content are found using a crosslanguage document similarity model." ></td>
	<td class="line x" title="46:186	In the second stage, the NEs in the source language side are extracted using a Named Entity Recognizer (NER) and, subsequently, for each NE in a source language document, its transliterations are mined from the corresponding target language documents." ></td>
	<td class="line x" title="47:186	We present the details of the two stages of MINT in the remainder of this section." ></td>
	<td class="line x" title="48:186	3.1 Finding Similar Document Pairs The first stage of MINT method (Figure 2) works on the documents from the comparable corpora (CS, CT) in languages S and T and produces a collection AS,T  of similar article pairs (DS, DT)." ></td>
	<td class="line x" title="49:186	Each article pair (DS, DT) in AS,T consists of an article (DS) in language S and an article (DT) in language T, that have similar content." ></td>
	<td class="line x" title="50:186	The cross-language similarity between DS and DT, as measured by the cross-language similarity model MD, is at least  > 0." ></td>
	<td class="line x" title="51:186	Cross-language Document Similarity Model: The cross-language document similarity model measures the degree of similarity between a pair of documents in source and target languages." ></td>
	<td class="line x" title="52:186	We use the negative KL-divergence between source and target document probability distributions as the similarity measure." ></td>
	<td class="line x" title="53:186	Given two documents DS, DT in source and target languages respectively, with TSVV, denoting the vocabulary of source and target languages, the similarity between the two documents is given by the KL-divergence measure, -KL(DS || DT), as:  TTw ST TTST V Dwp DwpDwp )|( )|(lo g)|( where p(w | D) is the likelihood of word w in D. As we are interested in target documents which are similar to a given source document, we can ignore the numerator as it is independent of the target document." ></td>
	<td class="line x" title="54:186	Finally, expanding p(wT | Ds) as )|()|( SVw TSS wwpDwpSS we specify the cross-language similarity score as follows:  Cross-language similarity = )|(lo g)|()|( TTSTw w SS DwpwwpDwp TVT SVS     3.2 Mining NETEs from Document Pairs The second stage of the MINT method works on each pair of articles (DS, DT) in the collection AS,T and produces a set PS,T of NETEs." ></td>
	<td class="line x" title="55:186	Each pair ( S,  T) in PS,T  consists of an NE  S in language S, and a token  T in language T, that are transliteration equivalents of each other." ></td>
	<td class="line x" title="56:186	Furthermore, the transliteration similarity between  S and  T, as measured by the transliteration similarity model MT, is at least  > 0." ></td>
	<td class="line x" title="57:186	Figure 3 outlines this algorithm." ></td>
	<td class="line x" title="58:186	Discriminative Transliteration Similarity Model: The transliteration similarity model MT measures the degree of transliteration equivalence between a source language and a target language term." ></td>
	<td class="line x" title="59:186	Input: Comparable news corpora (CS, CT) in languages (S,T)             Crosslanguage Document Similarity Model MD for (S, T)            Threshold score ." ></td>
	<td class="line x" title="60:186	Output: Set AS,T of pairs of similar articles (DS, DT) from (CS, CT)." ></td>
	<td class="line x" title="61:186	1 AS,T    ;         // Set of Similar articles (DS, DT) 2 for each article DS in CS do 3     XS     ;       // Set of candidates for DS." ></td>
	<td class="line x" title="62:186	4      for each article dT  in CT  do 5         score = CrossLanguageDocumentSimilarity(DS,dT,MD); 6         if (score  ) then XS   XS   (dT , score) ; 7      end 8     DT  = BestScoringCandidate(XS); 9    if (DT   ) then AS,T   AS,T   (DS, DT) ; 10 end CrossLanguageSimilarDocumentPairs Figure 2." ></td>
	<td class="line x" title="63:186	Stage 1 of MINT Input:       Set AS,T  of similar documents (DS, DT)  in languages (S,T),       Transliteration Similarity Model MT for (S, T),       Threshold score ." ></td>
	<td class="line x" title="64:186	Output: Set PS,T  of NETEs ( S,  T) from  AS,T ; 1   PS,T    ; 2   for each pair of articles (DS, DT) in AS,T  do 3        for each named entity  S in DS do 4            YS   ; // Set of candidates for  S. 5            for each candidate eT  in DT  do 6                 score = TransliterationSimilarity( S, eT, MT) ; 7                 if (score  )   then   YS    YS  (eT , score) ; 8            end 9             T  = BestScoringCandidate(YS) ; 10          if ( T   null) then PS,T    PS,T   ( S,  T) ; 11      end 12 end TransliterationEquivalents Figure 3." ></td>
	<td class="line x" title="65:186	Stage 2 of MINT 801 We employ a logistic function as our transliteration similarity model MT, as follows:   TransliterationSimilarity ( S,eT,MT) = ),( TS1 1 ewte   where  ( S, eT) is the feature vector for the pair ( S, eT) and w is the weights vector." ></td>
	<td class="line x" title="66:186	Note that the transliteration similarity takes a value in the range [01]." ></td>
	<td class="line x" title="67:186	The weights vector w is learnt discriminatively over a training corpus of known transliteration equivalents in the given pair of languages." ></td>
	<td class="line x" title="68:186	Features: The features employed by the model capture interesting cross-language associations observed in ( S, eT):   All unigrams and bigrams from the source and target language strings." ></td>
	<td class="line x" title="69:186	 Pairs of source string n-grams and target string n-grams such that difference in the start positions of the source and target ngrams is at most 2." ></td>
	<td class="line x" title="70:186	Here n  2,1 .  Difference in the lengths of the two strings." ></td>
	<td class="line x" title="71:186	Generative Transliteration Similarity Model: We also experimented with an extension of Hes W-HMM model (He, 2007)." ></td>
	<td class="line x" title="72:186	The transition probability depends on both the jump width and the previous source character as in the W-HMM model." ></td>
	<td class="line x" title="73:186	The emission probability depends on the current source character and the previous target character unlike the W-HMM model (Udupa et al., 2009)." ></td>
	<td class="line x" title="74:186	Instead of using any single alignment of characters in the pair (wS, wT), we marginalize over all possible alignments:      11111 ,|,|| 1    jajajjA mjnm tstpsaapstP jj  Here, jt (and resp." ></td>
	<td class="line x" title="75:186	is ) denotes the jth (and resp." ></td>
	<td class="line x" title="76:186	ith) character in wT (and resp." ></td>
	<td class="line x" title="77:186	wS) and maA 1 is the hidden alignment between wT and wS where jt is aligned to jas , ,m,j 1 . We estimate the parameters of the model using the EM algorithm." ></td>
	<td class="line x" title="78:186	The transliteration similarity score of a pair (wS, wT) is log P(wT  | wS) appropriately transformed." ></td>
	<td class="line x" title="79:186	4 Experimental Setup Our empirical investigation consists of experiments in three data environments, with each environment providing answer to specific set of questions, as listed below:  1." ></td>
	<td class="line x" title="80:186	Ideal Environment (IDEAL): Given a collection AS,T of oracle-aligned article pairs (DS, DT) in S and T, how effective is Stage 2 of MINT in mining NETE from AS,T? 2." ></td>
	<td class="line x" title="81:186	Near Ideal Environment (NEAR-IDEAL): Let AS,T  be a collection of similar article pairs (DS, DT) in S and T. Given comparable corpora (CS, CT) consisting of only articles from AS,T, but without the knowledge of pairings between the articles, a. How effective is Stage 1 of MINT in recovering AS,T  from (CS, CT) ? b. What is the effect of Stage 1 on the overall effectiveness of MINT?" ></td>
	<td class="line x" title="82:186	3." ></td>
	<td class="line x" title="83:186	Real Environment (REAL): Given large comparable corpora (CS, CT), how effective is MINT, end-to-end?" ></td>
	<td class="line x" title="84:186	The IDEAL environment is indeed ideal for MINT since every article in the comparable corpora is paired with exactly one similar article in the other language and the pairing of articles in the comparable corpora is known in advance." ></td>
	<td class="line x" title="85:186	We want to emphasize here that such corpora are indeed available in many domains such as technical documents and interlinked multilingual Wikipedia articles." ></td>
	<td class="line x" title="86:186	In the IDEAL environment, only Stage 2 of MINT is put to test, as article alignments are given." ></td>
	<td class="line x" title="87:186	In the NEAR-IDEAL data environment, every article in the comparable corpora is known to have exactly one conjugate article in the other language though the pairing itself is not known in advance." ></td>
	<td class="line x" title="88:186	In such a setting, MINT needs to discover the article pairing before mining NETEs and therefore, both stages of MINT are put to test." ></td>
	<td class="line x" title="89:186	The best performance possible in this environment should ideally be the same as that of IDEAL, and any degradation points to the shortcoming of the Stage 1 of MINT." ></td>
	<td class="line x" title="90:186	These two environments quantify the stage-wise performance of the MINT method." ></td>
	<td class="line x" title="91:186	Finally, in the data environment REAL, we test MINT on large comparable corpora, where even the existence of a conjugate article in the target side for a given article in the source side of the comparable corpora is not guaranteed, as in 802 any normal large multilingual news corpora." ></td>
	<td class="line x" title="92:186	In this scenario both the stages of MINT are put to test." ></td>
	<td class="line x" title="93:186	This is the toughest, and perhaps the typical setting in which MINT would be used." ></td>
	<td class="line x" title="94:186	4.1 Comparable Corpora In our experiments, the source language is English whereas the 4 target languages are from three different language families (Hindi from the Indo-Aryan family, Russian from the Slavic family, Kannada and Tamil from the Dravidian family)." ></td>
	<td class="line x" title="95:186	Note that none of the five languages use a common script and hence identification of cognates, spelling variations, suffix transformations, and other techniques commonly used for closely related languages that have a common script are not applicable for mining NETEs." ></td>
	<td class="line x" title="96:186	Table 1 summarizes the 6 different comparable corpora that were used for the empirical investigation; 4 for the IDEAL and NEAR-IDEAL environments (in 4 language pairs), and 2 for the REAL environment (in 2 language pairs)." ></td>
	<td class="line x" title="97:186	Corpus Source Target Data Environment Articles (in Thousands) Words (in Millions) Src Tgt Src Tgt EK-S EnglishKannada IDEAL& NEAR-IDEAL 2.90 2.90 0.42 0.34 ET-S EnglishTamil IDEAL& NEAR-IDEAL 2.90 2.90 0.42 0.32 ER-S EnglishRussian IDEAL& NEAR-IDEAL 2.30 2.30 1.03 0.40 EH-S EnglishHindi IDEAL& NEAR-IDEAL 11.9 11.9 3.77 3.57 EK-L EnglishKannada REAL 103.8 111.0 27.5 18.2 ET-L EnglishTamil REAL 103.8 144.3 27.5 19.4 Table 1: Comparable Corpora  The corpora can be categorized into two separate groups, group S (for Small) consisting of EK-S, ET-S, ER-S, and EH-S and group L (for Large) consisting of EK-L and ET-L." ></td>
	<td class="line x" title="98:186	Corpora in group S are relatively small in size, and contain pairs of articles that have been judged by human annotators as similar." ></td>
	<td class="line x" title="99:186	Corpora in group L are two orders of magnitude larger in size than those in group S and contain a large number of articles that may not have conjugates in the target side." ></td>
	<td class="line x" title="100:186	In addition the pairings are unknown even for the articles that have conjugates." ></td>
	<td class="line x" title="101:186	All comparable corpora had publication dates, except EH-S, which is known to have been published over the same year." ></td>
	<td class="line x" title="102:186	The EK-S, ET-S, EK-L and ET-L corpora are from The New Indian Express news paper, whereas the EH-S corpora are from Web Dunia and the ER-S corpora are from BBC/Lenta News Agency respectively." ></td>
	<td class="line x" title="103:186	4.2 Cross-language Similarity Model The cross-language document similarity model requires a bilingual dictionary in the appropriate language pair." ></td>
	<td class="line x" title="104:186	Therefore, we generated statistical dictionaries for 3 language pairs (from parallel corpora of the following sizes: 11K sentence pairs in English-Kannada, 54K in English-Hindi, and 14K in English-Tamil) using the GIZA++ statistical alignment tool (Och et al., 2003), with 5 iterations each of IBM Model 1 and HMM." ></td>
	<td class="line x" title="105:186	We did not have access to an English-Russian parallel corpus and hence could not generate a dictionary for this language pair." ></td>
	<td class="line x" title="106:186	Hence, the NEAR-IDEAL experiments were not run for the English-Russian language pair." ></td>
	<td class="line x" title="107:186	Although the coverage of the dictionaries was low, this turned out to be not a serious issue for our cross-language document similarity model as it might have for topic based CLIR (Ballesteros and Croft, 1998)." ></td>
	<td class="line x" title="108:186	Unlike CLIR, where the query is typically smaller in length compared to the documents, in our case we are dealing with news articles of comparable size in both source and target languages." ></td>
	<td class="line x" title="109:186	When many translations were available for a source word, we considered only the top-4 translations." ></td>
	<td class="line x" title="110:186	Further, we smoothed the document probability distributions with collection frequency as described in (Ponte and Croft, 1998)." ></td>
	<td class="line x" title="111:186	4.3 Transliteration Similarity Model The transliteration similarity models for each of the 4 language pairs were produced by learning over a training corpus consisting of about 16,000 single word NETEs, in each pair of languages." ></td>
	<td class="line x" title="112:186	The training corpus in English-Hindi, EnglishKannada and English-Tamil were hand-crafted by professionals, the English-Russian name pairs were culled from Wikipedia interwiki links and were cleaned heuristically." ></td>
	<td class="line x" title="113:186	Equal number of negative samples was used for training the models." ></td>
	<td class="line x" title="114:186	To produce the negative samples, we paired each source language NE with a random nonmatching target language NE." ></td>
	<td class="line x" title="115:186	No language specific features were used and the same feature set was used in each of the 4 language pairs making MINT language neutral." ></td>
	<td class="line oc" title="116:186	In all the experiments, our source side language is English, and the Stanford Named Entity Recognizer (Finkel et al, 2005) was used to extract NEs from the source side article." ></td>
	<td class="line n" title="117:186	It should be noted here that while the precision of the NER 803 used was consistently high, its recall was low, (~40%) especially in the New Indian Express corpus, perhaps due to the differences in the data used for training the NER and the data on which we used it." ></td>
	<td class="line x" title="118:186	4.4 Performance Measures Our intention is to measure the effectiveness of MINT by comparing its performance with the oracular (human annotator) performance." ></td>
	<td class="line x" title="119:186	As transliteration equivalents must exist in the paired articles to be found by MINT, we focus only on those NEs that actually have at least one transliteration equivalent in the conjugate article." ></td>
	<td class="line x" title="120:186	Three performance measures are of interest to us: the fraction of distinct NEs from source language for which we found at least one transliteration in the target side (Recall on distinct NEs), the fraction of distinct NETEs (Recall on distinct NETEs) and the Mean Reciprocal Rank (MRR) of the NETEs mined." ></td>
	<td class="line x" title="121:186	Since we are interested in mining not only the highly frequent but also the infrequent NETEs, recall metrics measure how effective our method is in mining NETEs exhaustively." ></td>
	<td class="line x" title="122:186	The MRR score indicates how effective our method is in preferring the correct ones among candidates." ></td>
	<td class="line x" title="123:186	To measure the performance of MINT, we created a test bed for each of the language pairs." ></td>
	<td class="line x" title="124:186	The test beds are summarized in Table 2." ></td>
	<td class="line x" title="125:186	The test beds consist of pairs of similar articles in each of the language pairs." ></td>
	<td class="line x" title="126:186	It should be noted here that as transliteration equivalents must exist in the paired articles to be found by MINT, we focus only on those NEs that actually have at least one transliteration equivalent in the conjugate article." ></td>
	<td class="line x" title="127:186	5 Results & Analysis In this section, we present qualitative and quantitative performance of the MINT algorithm, in mining NETEs from comparable news corpora." ></td>
	<td class="line x" title="128:186	All the results in Sections 5.1 to 5.3 were obtained using the discriminative transliteration similarity model described in Section 3.2." ></td>
	<td class="line x" title="129:186	The results using the generative transliteration similarity model are discussed in Section 5.4." ></td>
	<td class="line x" title="130:186	5.1 IDEAL Environment Our first set of experiments investigated the effectiveness of Stage 2 of MINT, namely the mining of NETEs in an IDEAL environment." ></td>
	<td class="line x" title="131:186	As MINT is provided with paired articles in this experiment, all experiments for this environment were run on test beds created from group S corpora (Table 2)." ></td>
	<td class="line x" title="132:186	Results in the IDEAL Environment: The recall measures for distinct NEs and distinct NETEs for the IDEAL environment are reported in Table 3." ></td>
	<td class="line x" title="133:186	Test Bed Recall (%) Distinct NEs Distinct NETEs EK-ST 97.30 95.07 ET-ST 99.11 98.06 EH-ST 98.55 98.66 ER-ST 93.33 85.88  Table 3: Recall of MINT in IDEAL  Note that in the first 3 language pairs MINT was able to mine a transliteration equivalent for almost all the distinct NEs." ></td>
	<td class="line x" title="134:186	The performance in English-Russian pair was relatively worse, perhaps due to the noisy training data." ></td>
	<td class="line x" title="135:186	In order to compare the effectiveness of MINT with a state-of-the-art NETE mining approach, we implemented the time series based Co-Ranking algorithm based on (Klementiev and Roth, 2006)." ></td>
	<td class="line x" title="136:186	Table 4 shows the MRR results in the IDEAL environment  both for MINT and the CoRanking baseline: MINT outperformed CoRanking on all the language pairs, despite not using time series similarity in the mining process." ></td>
	<td class="line x" title="137:186	The high MRRs (@1 and @5) indicate that in almost all the cases, the top-ranked candidate is a correct NETE." ></td>
	<td class="line x" title="138:186	Note that Co-Ranking could not be run on the EH-ST test bed as the articles did not have a date stamp." ></td>
	<td class="line x" title="139:186	Co-Ranking is crucially dependent on time series and hence requires date stamps for the articles." ></td>
	<td class="line x" title="140:186	Test Bed Comparable Corpora Article Pairs Distinct NEs Distinct NETEs EK-ST EK-S 200 481 710 ET-ST ET-S 200 449 672 EH-ST EH-S 200 347 373 ER-ST ER-S 100 195 347 Table 2: Test Beds for IDEAL & NEAR-IDEAL Test Bed MRR@1 MRR@5 MINT CoRanking MINT CoRanking EK-ST 0.94 0.26 0.95 0.29 ET-ST 0.91 0.26 0.94 0.29 EH-ST 0.93 0.95 ER-ST 0.80 0.38 0.85 0.43 Table 4: MINT & Co-Ranking in IDEAL 804 5.2 NEAR-IDEAL Environment The second set of experiments investigated the effectiveness of Stage 1 of MINT on comparable corpora that are constituted by pairs of similar articles, where the pairing information between the articles is with-held." ></td>
	<td class="line x" title="141:186	MINT reconstructed the pairings using the cross-language document similarity model and subsequently mined NETEs." ></td>
	<td class="line x" title="142:186	As in previous experiments, we ran our experiments on test beds described in Section 4.4." ></td>
	<td class="line x" title="143:186	Results in the NEAR-IDEAL Environment: There are two parts to this set of experiments." ></td>
	<td class="line x" title="144:186	In the first part, we investigated the effectiveness of the cross-language document similarity model described in Section 3.1." ></td>
	<td class="line x" title="145:186	Since we know the identity of the conjugate article for every article in the test bed, and articles can be ranked according to the cross-language document similarity score, we simply computed the MRR for the documents identified in each of the test beds, considering only the top-2 results." ></td>
	<td class="line x" title="146:186	Further, where available, we made use of the publication date of articles to restrict the number of target articles that are considered in lines 4 and 5 of the MINT algorithm in Figure 2." ></td>
	<td class="line x" title="147:186	Table 5 shows the results for two date windows  3 days and 1 year." ></td>
	<td class="line x" title="148:186	Test Bed MRR@1 MRR@2 3 days 1 year 3 days 1 year EK-ST 0.99 0.91 0.99 0.93 ET-ST 0.96 0.83 0.97 0.87 EH-ST 0.81 0.82 Table 5: MRR of Stage 1 in NEAR-IDEAL  Subsequently, the output of the Stage 1 was given as the input to the Stage 2 of the MINT method." ></td>
	<td class="line x" title="149:186	In Table 6 we report the MRR @1 and @5 for the second stage, for both time windows (3 days & 1 year)." ></td>
	<td class="line x" title="150:186	It is interesting to compare the results of MINT in NEAR-IDEAL data environment (Table 6) with MINTs results in IDEAL environment (Table 4)." ></td>
	<td class="line x" title="151:186	The drop in MRR@1 is small: ~2% for EK-ST and ~3% for ET-ST. For EH-ST the drop is relatively more (~12%) as may be expected since the time window (3 days) could not be applied for this test bed." ></td>
	<td class="line x" title="152:186	5.3 REAL Environment The third set of experiments investigated the effectiveness of MINT on large comparable corpora." ></td>
	<td class="line x" title="153:186	We ran the experiments on test beds created from group L corpora." ></td>
	<td class="line x" title="154:186	Test-beds for the REAL Environment: The test beds for the REAL environment (Table 7) consisted of only English articles since we do not know in advance whether these articles have any similar articles in the target languages." ></td>
	<td class="line x" title="155:186	Results in the REAL Environment: In real environment, we examined the top 2 articles of returned by Stage 1 of MINT, and mined NETEs from them." ></td>
	<td class="line x" title="156:186	We used a date window of 3 in Stage 1." ></td>
	<td class="line x" title="157:186	Table 8 summarizes the results for the REAL environment." ></td>
	<td class="line x" title="158:186	We observe that the performance of MINT is impressive, considering the fact that the comparable corpora used in the REAL environment is two orders of magnitude larger than those used in IDEAL and NEAR-IDEAL environments." ></td>
	<td class="line x" title="159:186	This implies that MINT is able to effectively mine NETEs whenever the Stage 1 algorithm was able to find a good conjugate for each of the source language articles." ></td>
	<td class="line x" title="160:186	5.4 Generative Transliteration Similarity Model We employed the extended W-HMM transliteration similarity model in MINT and used it in the IDEAL data environment." ></td>
	<td class="line x" title="161:186	Table 9 shows the results." ></td>
	<td class="line x" title="162:186	Test Bed MRR@1 MRR@5 3 days 1 year 3 days 1 year EK-ST 0.92 0.87 0.94 0.90 ET-ST 0.88 0.74 0.91 0.78 EH-ST 0.82 0.87 Table 6: MRR of Stage 2 in NEAR-IDEAL Test Bed Comparable Corpora Articles Distinct NEs EK-LT EK-L 100 306 ET-LT ET-L 100 228 Table 7: Test Beds for REAL  Test Bed MRR @1 @5 EK-LT 0.86 0.88 ET-LT 0.82 0.85 Table 8: MRR of Stage 2 in REAL Test Bed MRR @1 @5 EK-S 0.85 0.86 ET-S 0.81 0.82 EH-S 0.91 0.93 Table 9:  MRR of Stage 2 in IDEAL using generative transliteration similarity model 805 We see that the results for the generative transliteration similarity model are good but not as good as those for the discriminative transliteration similarity model." ></td>
	<td class="line x" title="163:186	As we did not stem either the English NEs or the target language words, the generative model made more mistakes on inflected words compared to the discriminative model." ></td>
	<td class="line x" title="164:186	5.5  Examples of Mined NETEs Table 10 gives some examples of the NETEs mined from the comparable news corpora." ></td>
	<td class="line x" title="165:186	6  Related Work CLIR systems have been studied in several works (Ballesteros and Croft, 1998; Kraiij et al, 2003)." ></td>
	<td class="line x" title="166:186	The limited coverage of dictionaries has been recognized as a problem in CLIR and MT (Demner-Fushman & Oard, 2002; Mandl & Womser-hacker, 2005; Xu &Weischedel, 2005)." ></td>
	<td class="line x" title="167:186	In order to address this problem, different kinds of approaches have been taken, from learning transformation rules from dictionaries and applying the rules to find cross-lingual spelling variants (Pirkola et al., 2003), to  learning translation lexicon from monolingual and/or comparable corpora (Fung, 1995; Al-Onaizan and Knight, 2002; Koehn and Knight, 2002; Rapp, 1996)." ></td>
	<td class="line x" title="168:186	While these works have focused on finding translation equivalents of all class of words, we focus specifically on transliteration equivalents of NEs." ></td>
	<td class="line x" title="169:186	(Munteanu and Marcu, 2006; Quirk et al., 2007) addresses mining of parallel sentences and fragments from nearly parallel sentences." ></td>
	<td class="line x" title="170:186	In contrast, our approach mines NETEs from article pairs that may not even have any parallel or nearly parallel sentences." ></td>
	<td class="line x" title="171:186	NETE discovery from comparable corpora using time series and transliteration model was proposed in (Klementiev and Roth, 2006), and extended for NETE mining for several languages in (Saravanan and Kumaran, 2007)." ></td>
	<td class="line x" title="172:186	However, such methods miss vast majority of the NETEs due to their dependency on frequency signatures." ></td>
	<td class="line x" title="173:186	In addition, (Klementiev and Roth, 2006) may not scale for large corpora, as they examine every word in the target side as a potential transliteration equivalent." ></td>
	<td class="line x" title="174:186	NETE mining from comparable corpora using phonetic mappings was proposed in (Tao et al., 2006), but the need for language specific knowledge restricts its applicability across languages." ></td>
	<td class="line x" title="175:186	We proposed the idea of mining NETEs from multilingual articles with similar content in (Udupa, et al., 2008)." ></td>
	<td class="line x" title="176:186	In this work, we extend the approach and provide a detailed description of the empirical studies." ></td>
	<td class="line x" title="177:186	7  Conclusion In this paper, we showed that MINT, a simple and intuitive technique employing crosslanguage document similarity and transliteration similarity models, is capable of mining NETEs effectively from large comparable news corpora." ></td>
	<td class="line x" title="178:186	Our three stage empirical investigation showed that MINT performed close to optimal on comparable corpora consisting of pairs of similar articles when the pairings are known in advance." ></td>
	<td class="line x" title="179:186	MINT induced fairly good pairings and performs exceedingly well even when the pairings are not known in advance." ></td>
	<td class="line x" title="180:186	Further, MINT outperformed a state-of-the-art baseline and scaled to large comparable corpora." ></td>
	<td class="line x" title="181:186	Finally, we demonstrated the language neutrality of MINT, by mining NETEs from 4 language pairs (between English and one of Russian, Hindi, Kannada or Tamil) from 3 vastly different linguistic families." ></td>
	<td class="line x" title="182:186	As a future work, we plan to use the extended W-HMM model to get features for the discriminative transliteration similarity model." ></td>
	<td class="line x" title="183:186	We also want to use a combination of the crosslanguage document similarity score and the transliteration similarity score for scoring the NETEs." ></td>
	<td class="line x" title="184:186	Finally, we would like to use the mined NETEs to improve the performance of the first stage of MINT." ></td>
	<td class="line x" title="185:186	Acknowledgments We thank Abhijit Bhole for his help and Chris Quirk for valuable comments." ></td>
	<td class="line x" title="186:186	Language Pair Source NE Transliteration EnglishKannada Woolmer  Kafeel  Baghdad  English-Tamil Lloyd  Mumbai   Manchester  English-Hindi Vanhanen  Trinidad  Ibuprofen  EnglishRussian Kreuzberg  Gaddafi  Karadzic  Table 10: Examples of Mined NETEs 806" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-1037
Joint Parsing and Named Entity Recognition
Finkel, Jenny Rose;Manning, Christopher D.;"></td>
	<td class="line x" title="1:200	Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 326334, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:200	c 2009 Association for Computational Linguistics Joint Parsing and Named Entity Recognition Jenny Rose Finkel and Christopher D. Manning Computer Science Department Stanford University Stanford, CA 94305 {jrfinkel|manning}@cs.stanford.edu Abstract For many language technology applications, such as question answering, the overall system runs several independent processors over the data (such as a named entity recognizer, a coreference system, and a parser)." ></td>
	<td class="line x" title="3:200	This easily results in inconsistent annotations, which are harmful to the performance of the aggregate system." ></td>
	<td class="line x" title="4:200	We begin to address this problem with a joint model of parsing and named entity recognition, based on a discriminative feature-based constituency parser." ></td>
	<td class="line x" title="5:200	Our model produces a consistent output, where the named entity spans do not conflict with the phrasal spans of the parse tree." ></td>
	<td class="line x" title="6:200	The joint representation also allows the information from each type of annotation to improve performance on the other, and, in experiments with the OntoNotes corpus, we found improvements of up to 1.36% absolute F1 for parsing, and up to 9.0% F1 for named entity recognition." ></td>
	<td class="line x" title="7:200	1 Introduction In order to build high quality systems for complex NLP tasks, such as question answering and textual entailment, it is essential to first have high quality systems for lower level tasks." ></td>
	<td class="line x" title="8:200	A good (deep analysis) question answering system requires the data to first be annotated with several types of information: parse trees, named entities, word sense disambiguation, etc. However, having high performing, lowlevel systems is not enough; the assertions of the various levels of annotation must be consistent with one another." ></td>
	<td class="line x" title="9:200	When a named entity span has crossing brackets with the spans in the parse tree it is usually impossible to effectively combine these pieces of information, and system performance suffers." ></td>
	<td class="line x" title="10:200	But, unfortunately, it is still common practice to cobble together independent systems for the various types of annotation, and there is no guarantee that their outputs will be consistent." ></td>
	<td class="line x" title="11:200	This paper begins to address this problem by building a joint model of both parsing and named entity recognition." ></td>
	<td class="line x" title="12:200	Vapnik has observed (Vapnik, 1998; Ng and Jordan, 2002) that one should solve the problem directly and never solve a more general problem as an intermediate step, implying that building a joint model of two phenomena is more likely to harm performance on the individual tasks than to help it." ></td>
	<td class="line x" title="13:200	Indeed, it has proven very difficult to build a joint model of parsing and semantic role labeling, either with PCFG trees (Sutton and McCallum, 2005) or with dependency trees." ></td>
	<td class="line x" title="14:200	The CoNLL 2008 shared task (Surdeanu et al., 2008) was intended to be about joint dependency parsing and semantic role labeling, but the top performing systems decoupled the tasks and outperformed the systems which attempted to learn them jointly." ></td>
	<td class="line x" title="15:200	Despite these earlier results, we found that combining parsing and named entity recognition modestly improved performance on both tasks." ></td>
	<td class="line x" title="16:200	Our joint model produces an output which has consistent parse structure and named entity spans, and does a better job at both tasks than separate models with the same features." ></td>
	<td class="line x" title="17:200	We first present the joint, discriminative model that we use, which is a feature-based CRF-CFG parser operating over tree structures augmented with NER information." ></td>
	<td class="line x" title="18:200	We then discuss in detail how we make use of the recently developed OntoNotes corpus both for training and testing the model, and then finally present the performance of the model and some discussion of what causes its superior performance, and how the model relates to prior work." ></td>
	<td class="line x" title="19:200	326 NP DT the NP NNP [District PP IN of NP NNP Columbia] GPE = NP DT the NamedEntity-GPE* NP-GPE NNP-GPE District PP-GPE IN-GPE of NP-GPE NNP-GPE Columbia Figure 1: An example of a (sub)tree which is modified for input to our learning algorithm." ></td>
	<td class="line x" title="20:200	Starting from the normalized tree discussed in section 4.1, a new NamedEntity node is added, so that the named entity corresponds to a single phrasal node." ></td>
	<td class="line x" title="21:200	That node, and its descendents, have their labels augmented with the type of named entity." ></td>
	<td class="line x" title="22:200	The * on the NamedEntity node indicates that it is the root of the named entity." ></td>
	<td class="line x" title="23:200	2 The Joint Model When constructing a joint model of parsing and named entity recognition, it makes sense to think about how the two distinct levels of annotation may help one another." ></td>
	<td class="line x" title="24:200	Ideally, a named entity should correspond to a phrase in the constituency tree." ></td>
	<td class="line x" title="25:200	However, parse trees will occasionally lack some explicit structure, such as with right branching NPs." ></td>
	<td class="line x" title="26:200	In these cases, a named entity may correspond to a contiguous set of children within a subtree of the entire parse." ></td>
	<td class="line x" title="27:200	The one thing that should never happen is for a named entity span to have crossing brackets with any spans in the parse tree." ></td>
	<td class="line x" title="28:200	For named entities, the joint model should help with boundaries." ></td>
	<td class="line x" title="29:200	The internal structure of the named entity, and the structural context in which it appears, can also help with determining the type of entity." ></td>
	<td class="line x" title="30:200	Finding the best parse for a sentence can be helped by the named entity information in similar ways." ></td>
	<td class="line x" title="31:200	Because named entities should correspond to phrases, information about them should lead to better bracketing." ></td>
	<td class="line x" title="32:200	Also, knowing that a phrase is a named entity, and the type of entity, may help in getting the structural context, and internal structure, of that entity correct." ></td>
	<td class="line x" title="33:200	2.1 Joint Representation After modifying the OntoNotes dataset to ensure consistency, which we will discuss in Section 4, we augment the parse tree with named entity information, for input to our learning algorithm." ></td>
	<td class="line x" title="34:200	In the cases where a named entity corresponds to multiple contiguous children of a subtree, we add a new NamedEntity node, which is the new parent to those children." ></td>
	<td class="line x" title="35:200	Now, all named entities correspond to a single phrasal node in the entire tree." ></td>
	<td class="line x" title="36:200	We then augment the labels of the phrasal node and its descendents with the type of named entity." ></td>
	<td class="line x" title="37:200	We also distinguish between the root node of an entity, and the descendent nodes." ></td>
	<td class="line x" title="38:200	See Figure 1 for an illustration." ></td>
	<td class="line x" title="39:200	This representation has several benefits, outlined below." ></td>
	<td class="line x" title="40:200	2.1.1 Nested Entities The OntoNotes data does not contain any nested entities." ></td>
	<td class="line x" title="41:200	Consider the named entity portions of the rules seen in the training data." ></td>
	<td class="line x" title="42:200	These will look, for instance, like none none person, and organization  organization organization." ></td>
	<td class="line x" title="43:200	Because we only allow named entity derivations which we have seen in the data, nested entities are impossible." ></td>
	<td class="line x" title="44:200	However, there is clear benefit in a representation allowing nested entities." ></td>
	<td class="line x" title="45:200	For example, it would be beneficial to recognize that the United States Supreme Court is a an organization, but that it also contains a nested GPE.1 Fortunately, if we encounter data which has been annotated with nested entities, this representation will be able to handle them in a natural way." ></td>
	<td class="line x" title="46:200	In the given example, we would have a derivation which includes organization  GPE organization." ></td>
	<td class="line x" title="47:200	This information will be helpful for correctly labeling nested entities such as New Jersey Supreme Court, because the model will learn how nested entities tend to decompose." ></td>
	<td class="line x" title="48:200	2.1.2 Feature Representation for Named Entities Currently, named entity recognizers are usually constructed using sequence models, with linear chain 1As far as we know, GENIA (Kim et al., 2003) is the only corpus currently annotated with nested entities." ></td>
	<td class="line x" title="49:200	327 conditional random fields (CRFs) being the most common." ></td>
	<td class="line x" title="50:200	While it is possible for CRFs to have links that are longer distance than just between adjacent words, most of the benefit is from local features, over the words and labels themselves, and from features over adjacent pairs of words and labels." ></td>
	<td class="line x" title="51:200	Our joint representation allows us to port both types of features from such a named entity recognizer." ></td>
	<td class="line x" title="52:200	The local features can be computed at the same time the features over parts of speech are computed." ></td>
	<td class="line x" title="53:200	These are the leaves of the tree, when only the named entity for the current word is known.2 The pairwise features, over adjacent labels, are computed at the same time as features over binary rules." ></td>
	<td class="line x" title="54:200	Binarization of the tree is necessary for efficient computation, so the trees consist solely of unary and binary productions." ></td>
	<td class="line x" title="55:200	Because of this, for all pairs of adjacent words within an entity, there will be a binary rule applied where one word will be under the left child and the other word will be under the right child." ></td>
	<td class="line x" title="56:200	Therefore, we compute features over adjacent words/labels when computing the features for the binary rule which joins them." ></td>
	<td class="line x" title="57:200	2.2 Learning the Joint Model We construct our joint model as an extension to the discriminatively trained, feature-rich, conditional random field-based, CRF-CFG parser of (Finkel and Manning, 2008)." ></td>
	<td class="line x" title="58:200	Their parser is similar to a chartbased PCFG parser, except that instead of putting probabilities over rules, it puts clique potentials over local subtrees." ></td>
	<td class="line x" title="59:200	These unnormalized potentials know what span (and split) the rule is over, and arbitrary features can be defined over the local subtree, the span/split and the words of the sentence." ></td>
	<td class="line x" title="60:200	The insideoutside algorithm is run over the clique potentials to produce the partial derivatives and normalizing constant which are necessary for optimizing the log likelihood." ></td>
	<td class="line x" title="61:200	2.3 Grammar Smoothing Because of the addition of named entity annotations to grammar rules, if we use the grammar as read off the treebank, we will encounter problems with sparseness which severely degrade performance." ></td>
	<td class="line x" title="62:200	This degradation occurs because of CFG 2Note that features can include information about other words, because the entire sentence is observed." ></td>
	<td class="line x" title="63:200	The features cannot include information about the labels of those words." ></td>
	<td class="line x" title="64:200	rules which only occur in the training data augmented with named entity information, and because of rules which only occur without the named entity information." ></td>
	<td class="line x" title="65:200	To combat this problem, we added extra rules, unseen in the training data." ></td>
	<td class="line x" title="66:200	2.3.1 Augmenting the Grammar For every rule encountered in the training data which has been augmented with named entity information, we add extra copies of that rule to the grammar." ></td>
	<td class="line x" title="67:200	We add one copy with all of the named entity information stripped away, and another copy for each other entity type, where the named entity augmentation has been changed to the other entity type." ></td>
	<td class="line x" title="68:200	These additions help, but they are not sufficient." ></td>
	<td class="line x" title="69:200	Most entities correspond to noun phrases, so we took all rules which had an NP as a child, and made copies of that rule where the NP was augmented with each possible entity type." ></td>
	<td class="line x" title="70:200	These grammar additions sufficed to improve overall performance." ></td>
	<td class="line x" title="71:200	2.3.2 Augmenting the Lexicon The lexicon is augmented in a similar manner to the rules." ></td>
	<td class="line x" title="72:200	For every part of speech tag seen with a named entity annotation, we also add that tag with no named entity information, and a version which has been augmented with each type of named entity." ></td>
	<td class="line x" title="73:200	It would be computationally infeasible to allow any word to have any part of speech tag." ></td>
	<td class="line x" title="74:200	We therefore limit the allowed part of speech tags for common words based on the tags they have been observed with in the training data." ></td>
	<td class="line x" title="75:200	We also augment each word with a distributional similarity tag, which we discuss in greater depth in Section 3, and allow tags seen with other words which belong to the same distributional similarity cluster." ></td>
	<td class="line x" title="76:200	When deciding what tags are allowed for each word, we initially ignore named entity information." ></td>
	<td class="line x" title="77:200	Once we determine what base tags are allowed for a word, we also allow that tag, augmented with any type of named entity, if the augmented tag is present in the lexicon." ></td>
	<td class="line x" title="78:200	3 Features We defined features over both the parse rules and the named entities." ></td>
	<td class="line x" title="79:200	Most of our features are over one or the other aspects of the structure, but not both." ></td>
	<td class="line x" title="80:200	Both the named entity and parsing features utilize the words of the sentence, as well as orthographic and distributional similarity information." ></td>
	<td class="line x" title="81:200	For each word we computed a word shape which encoded 328 information about capitalization, length, and inclusion of numbers and other non-alphabetic characters." ></td>
	<td class="line x" title="82:200	For the distributional similarity information, we had to first train a distributional similarity model." ></td>
	<td class="line x" title="83:200	We trained the model described in (Clark, 2000), with code downloaded from his website, on several hundred million words from the British national corpus, and the English Gigaword corpus." ></td>
	<td class="line x" title="84:200	The model we trained had 200 clusters, and we used it to assign each word in the training and test data to one of the clusters." ></td>
	<td class="line oc" title="85:200	For the named entity features, we used a fairly standard feature set, similar to those described in (Finkel et al., 2005)." ></td>
	<td class="line x" title="86:200	For parse features, we used the exact same features as described in (Finkel and Manning, 2008)." ></td>
	<td class="line x" title="87:200	When computing those features, we removed all of the named entity information from the rules, so that these features were just over the parse information and not at all over the named entity information." ></td>
	<td class="line x" title="88:200	Lastly, we have the joint features." ></td>
	<td class="line x" title="89:200	We included as features each augmented rule and each augmented label." ></td>
	<td class="line x" title="90:200	This allowed the model to learn that certain types of phrasal nodes, such as NPs are more likely to be named entities, and that certain entities were more likely to occur in certain contexts and have particular types of internal structure." ></td>
	<td class="line x" title="91:200	4 Data For our experiments we used the LDC2008T04 OntoNotes Release 2.0 corpus (Hovy et al., 2006)." ></td>
	<td class="line x" title="92:200	The OntoNotes project leaders describe it as a large, multilingual richly-annotated corpus constructed at 90% internanotator agreement. The corpus has been annotated with multiple levels of annotation, including constituency trees, predicate structure, word senses, coreference, and named entities." ></td>
	<td class="line x" title="93:200	For this work, we focus on the parse trees and named entities." ></td>
	<td class="line x" title="94:200	The corpus has English and Chinese portions, and we used only the English portion, which itself has been split into seven sections: ABC, CNN, MNB, NBC, PRI, VOA, and WSJ." ></td>
	<td class="line x" title="95:200	These sections represent a mix of speech and newswire data." ></td>
	<td class="line x" title="96:200	4.1 Data Inconsistencies While other work has utilized the OntoNotes corpus (Pradhan et al., 2007; Yu et al., 2008), this is the first work to our knowledge to simultaneously model the multiple levels of annotation available." ></td>
	<td class="line x" title="97:200	Because this is a new corpus, still under development, it is not surprising that we found places where the data was inconsistently annotated, namely with crossing brackets between named entity and tree annotations." ></td>
	<td class="line x" title="98:200	In the places where we found inconsistent annotation it was rarely the case that the different levels of annotation were inherently inconsistent, but rather inconsistency results from somewhat arbitrary choices made by the annotators." ></td>
	<td class="line x" title="99:200	For example, when the last word in a sentence ends with a period, such as Corp., one period functions both to mark the abbreviation and the end of the sentence." ></td>
	<td class="line x" title="100:200	The convention of the Penn Treebank is to separate the final period and treat it as the end of sentence marker, but when the final word is also part of an entity, that final period was frequently included in the named entity annotation, resulting in the sentence terminating period being part of the entity, and the entity not corresponding to a single phrase." ></td>
	<td class="line x" title="101:200	See Figure 2 for an illustration from the data." ></td>
	<td class="line x" title="102:200	In this case, we removed the terminating period from the entity, to produce a consistent annotation." ></td>
	<td class="line x" title="103:200	Overall, we found that 656 entities, out of 55,665 total, could not be aligned to a phrase, or multiple contiguous children of a node." ></td>
	<td class="line x" title="104:200	We identified and corrected the following sources of inconsistencies: Periods and abbreviations." ></td>
	<td class="line x" title="105:200	This is the problem described above with the Corp. example." ></td>
	<td class="line x" title="106:200	We corrected it by removing the sentence terminating final period from the entity annotation." ></td>
	<td class="line x" title="107:200	Determiners and PPs." ></td>
	<td class="line x" title="108:200	Noun phrases composed of a nested noun phrase and a prepositional phrase were problematic when they also consisted of a determiner followed by an entity." ></td>
	<td class="line x" title="109:200	We dealt with this by flattening the nested NP, as illustrated in Figure 3." ></td>
	<td class="line x" title="110:200	As we discussed in Section 2.1, this tree will then be augmented with an additional node for the entity (see Figure 1)." ></td>
	<td class="line x" title="111:200	Adjectives and PPs." ></td>
	<td class="line x" title="112:200	This problem is similar to the previous problem, with the difference being that there are also adjectives preceding the entity." ></td>
	<td class="line x" title="113:200	The solution is also similar to the solution to the previous problem." ></td>
	<td class="line x" title="114:200	We moved the adjectives from the nested NP into the main NP." ></td>
	<td class="line x" title="115:200	These three modifications to the data solved most, but not all, of the inconsistencies." ></td>
	<td class="line x" title="116:200	Another source of problems was conjunctions, such as North and South Korea, where North and South are a phrase, 329 S NP NNP [Mr. NNP Todt]PER VP VBD had VP VBN been NP NP NN president PP IN of NP NNP [Insilco NNP Corp . .]ORG Figure 2: An example from the data of inconsistently labeled named entity and parse structure." ></td>
	<td class="line x" title="117:200	The inclusion of the final period in the named entity results in the named entity structure having crossing brackets with the parse structure." ></td>
	<td class="line x" title="118:200	NP NP DT the NNP [District PP IN of NP NNP Columbia] GPE NP DT the NP NNP [District PP IN of NP NNP Columbia] GPE (a) (b) Figure 3: (a) Another example from the data of inconsistently labeled named entity and parse structure." ></td>
	<td class="line x" title="119:200	In this instance, we flatten the nested NP, resulting in (b), so that the named entity corresponds to a contiguous set of children of the top-level NP." ></td>
	<td class="line x" title="120:200	but South Korea is an entity." ></td>
	<td class="line x" title="121:200	The rest of the errors seemed to be due to annotation errors and other random weirdnesses." ></td>
	<td class="line x" title="122:200	We ended up unable to make 0.4% of the entities consistent with the parses, so we omitted those entities from the training and test data." ></td>
	<td class="line x" title="123:200	One more change we made to the data was with respect to possessive NPs." ></td>
	<td class="line x" title="124:200	When we encountered noun phrases which ended with (POS s) or (POS ), we modified the internal structure of the NP." ></td>
	<td class="line x" title="125:200	Originally, these NPs were flat, but we introduced a new nested NP which contained the entire contents of the original NP except for the POS." ></td>
	<td class="line x" title="126:200	The original NP label was then changed to PossNP." ></td>
	<td class="line x" title="127:200	This change is motivated by the status of s as a phrasal affix or clitic: It is the NP preceding s that is structurally equivalent to other NPs, not the larger unit that includes s. This change has the additional benefit in this context that more named entities will correspond to a single phrase in the parse tree, rather than a contiguous set of phrases." ></td>
	<td class="line x" title="128:200	4.2 Named Entity Types The data has been annotated with eighteen types of entities." ></td>
	<td class="line x" title="129:200	Many of these entity types do not occur very often, and coupled with the relatively small amount of data, make it difficult to learn accurate entity models." ></td>
	<td class="line x" title="130:200	Examples are work of art, product, and law." ></td>
	<td class="line x" title="131:200	Early experiments showed that it was difficult for even our baseline named entity recognizer, based on a state-of-the-art CRF, to learn these types of entities.3 As a result, we decided to merge all but the three most dominant entity types into into one general entity type called misc." ></td>
	<td class="line x" title="132:200	The result was four distinct entity types: person, organization, GPE (geo-political entity, such as a city or a country), and misc." ></td>
	<td class="line x" title="133:200	3The difficulties were compounded by somewhat inconsistent and occasionally questionable annotations." ></td>
	<td class="line x" title="134:200	For example, the word today was usually labeled as a date, but about 10% of the time it was not labeled as anything." ></td>
	<td class="line x" title="135:200	We also found several strange work of arts, including Stanley Cup and the U.S.S. Cole." ></td>
	<td class="line x" title="136:200	330 Training Testing Range # Sent." ></td>
	<td class="line x" title="137:200	Range # Sent." ></td>
	<td class="line x" title="138:200	ABC 055 1195 5669 199 CNN 0375 5092 376437 1521 MNB 017 509 1825 245 NBC 029 552 3039 149 PRI 089 1707 90112 394 VOA 0198 1512 199264 383 Table 1: Training and test set sizes for the six datasets in sentences." ></td>
	<td class="line x" title="139:200	The file ranges refer to the numbers within the names of the original OntoNotes files." ></td>
	<td class="line x" title="140:200	5 Experiments We ran our model on six of the OntoNotes datasets described in Section 4,4 using sentences of length 40 and under (approximately 200,000 annotated English words, considerably smaller than the Penn Treebank (Marcus et al., 1993))." ></td>
	<td class="line x" title="141:200	For each dataset, we aimed for roughly a 75% train / 25% test split." ></td>
	<td class="line x" title="142:200	See Table 1 for the the files used to train and test, along with the number of sentences in each." ></td>
	<td class="line x" title="143:200	For comparison, we also trained the parser without the named entity information (and omitted the NamedEntity nodes), and a linear chain CRF using just the named entity information." ></td>
	<td class="line x" title="144:200	Both the baseline parser and CRF were trained using the exact same features as the joint model, and all were optimized using stochastic gradient descent." ></td>
	<td class="line x" title="145:200	The full results can be found in Table 2." ></td>
	<td class="line x" title="146:200	Parse trees were scored using evalB (the extra NamedEntity nodes were ignored when computing evalB for the joint model), and named entities were scored using entity F-measure (as in the CoNLL 2003 conlleval).5 While the main benefit of our joint model is the ability to get a consistent output over both types of annotations, we also found that modeling the parse 4These datasets all consistently use the new conventions for treebank annotation, while the seventh WSJ portion is currently still annotated in the original 1990s style, and so we left the WSJ portion aside." ></td>
	<td class="line x" title="147:200	5Sometimes the parser would be unable to parse a sentence (less than 2% of sentences), due to restrictions in part of speech tags." ></td>
	<td class="line x" title="148:200	Because the underlying grammar (ignoring the additional named entity information) was the same for both the joint and baseline parsers, it is the case that whenever a sentence is unparseable by either the baseline or joint parser it is in fact unparsable by both of them, and would affect the parse scores of both models equally." ></td>
	<td class="line x" title="149:200	However, the CRF is able to named entity tag any sentence, so these unparsable sentences had an effect on the named entity score." ></td>
	<td class="line x" title="150:200	To combat this, we fell back on the baseline CRF model to get named entity tags for unparsable sentences." ></td>
	<td class="line x" title="151:200	and named entities jointly resulted in improved performance on both." ></td>
	<td class="line x" title="152:200	When looking at these numbers, it is important to keep in mind that the sizes of the training and test sets are significantly smaller than the Penn Treebank." ></td>
	<td class="line x" title="153:200	The largest of the six datasets, CNN, has about one seventh the amount of training data as the Penn Treebank, and the smallest, MNB, has around 500 sentences from which to train." ></td>
	<td class="line x" title="154:200	Parse performance was improved by the joint model for five of the six datasets, by up to 1.36%." ></td>
	<td class="line x" title="155:200	Looking at the parsing improvements on a per-label basis, the largest gains came from improved identication of NML consituents, from an F-score of 45.9% to 57.0% (on all the data combined, for a total of 420 NML constituents)." ></td>
	<td class="line x" title="156:200	This label was added in the new treebank annotation conventions, so as to identify internal left-branching structure inside previously flat NPs." ></td>
	<td class="line x" title="157:200	To our surprise, performance on NPs only increased by 1%, though over 12,949 constituents, for the largest improvement in absolute terms." ></td>
	<td class="line x" title="158:200	The second largest gain was on PPs, where we improved by 1.7% over 3,775 constituents." ></td>
	<td class="line x" title="159:200	We tested the significance of our results (on all the data combined) using Dan Bikels randomized parsing evaluation comparator6 and found that both the precision and recall gains were significant at p  0.01." ></td>
	<td class="line x" title="160:200	Much greater improvements in performance were seen on named entity recognition, where most of the domains saw improvements in the range of 3 4%, with performance on the VOA data improving by nearly 9%, which is a 45% reduction in error." ></td>
	<td class="line x" title="161:200	There was no clear trend in terms of precision versus recall, or the different entity types." ></td>
	<td class="line x" title="162:200	The first place to look for improvements is with the boundaries for named entities." ></td>
	<td class="line x" title="163:200	Once again looking at all of the data combined, in the baseline model there were 203 entities where part of the entity was found, but one or both boundaries were incorrectly identified." ></td>
	<td class="line x" title="164:200	The joint model corrected 72 of those entities, while incorrectly identifying the boundaries of 37 entities which had previously been correctly identified." ></td>
	<td class="line x" title="165:200	In the baseline NER model, there were 243 entities for which the boundaries were correctly identified, but the type of entity was incorrect." ></td>
	<td class="line x" title="166:200	The joint model corrected 80 of them, while changing the labels of 39 entities which had previously been correctly identified." ></td>
	<td class="line x" title="167:200	Additionally, 190 entities were found which the baseline model had missed entirely, and 68 enti6Available at http://www.cis.upenn.edu/ dbikel/software.html 331 Parse Labeled Bracketing Named Entities Training Precision Recall F1 Precision Recall F1 Time ABC Just Parse 70.18% 70.12% 70.15%  25m Just NER  76.84% 72.32% 74.51% Joint Model 69.76% 70.23% 69.99% 77.70% 72.32% 74.91% 45m CNN Just Parse 76.92% 77.14% 77.03%  16.5h Just NER  75.56% 76.00% 75.78% Joint Model 77.43% 77.99% 77.71% 78.73% 78.67% 78.70% 31.7h MNB Just Parse 63.97% 67.07% 65.49%  12m Just NER  72.30% 54.59% 62.21% Joint Model 63.82$ 67.46% 65.59% 71.35% 62.24% 66.49% 19m NBC Just Parse 59.72% 63.67% 61.63%  10m Just NER  67.53% 60.65% 63.90% Joint Model 60.69% 65.34% 62.93% 71.43% 64.81% 67.96% 17m PRI Just Parse 76.22% 76.49% 76.35%  2.4h Just NER  82.07% 84.86% 83.44% Joint Model 76.88% 77.95% 77.41% 86.13% 86.56% 86.34% 4.2h VOA Just Parse 76.56% 75.74% 76.15%  2.3h Just NER  82.79% 75.96% 79.23% Joint Model 77.58% 77.45% 77.51% 88.37% 87.98% 88.18% 4.4h Table 2: Full parse and NER results for the six datasets." ></td>
	<td class="line x" title="168:200	Parse trees were evaluated using evalB, and named entities were scored using macro-averaged F-measure (conlleval)." ></td>
	<td class="line x" title="169:200	ties were lost." ></td>
	<td class="line x" title="170:200	We tested the statistical significance of the gains (of all the data combined) using the same sentence-level, stratified shuffling technique as Bikels parse comparator and found that both precision and recall gains were significant at p < 104." ></td>
	<td class="line x" title="171:200	An example from the data where the joint model helped improve both parse structure and named entity recognition is shown in Figure 4." ></td>
	<td class="line x" title="172:200	The output from the individual models is shown in part (a), with the output from the named entity recognizer shown in brackets on the words at leaves of the parse." ></td>
	<td class="line x" title="173:200	The output from the joint model is shown in part (b), with the named entity information encoded within the parse." ></td>
	<td class="line x" title="174:200	In this example, the named entity Egyptian Islamic Jihad helped the parser to get its surrounding context correct, because it is improbable to attach a PP headed by with to an organization." ></td>
	<td class="line x" title="175:200	At the same time, the surrounding context helped the joint model correctly identify Egyptian Islamic Jihad as an organization and not a person." ></td>
	<td class="line x" title="176:200	The baseline parser also incorrectly added an extra level of structure to the person name Osama Bin Laden, while the joint model found the correct structure." ></td>
	<td class="line x" title="177:200	6 Related Work A pioneering antecedent for our work is (Miller et al., 2000), who trained a Collins-style generative parser (Collins, 1997) over a syntactic structure augmented with the template entity and template relations annotations for the MUC-7 shared task." ></td>
	<td class="line x" title="178:200	Their sentence augmentations were similar to ours, but they did not make use of features due to the generative nature of their model." ></td>
	<td class="line x" title="179:200	This approach was not followed up on in other work, presumably because around this time nearly all the activity in named entity and relation extraction moved to the use of discriminative sequence models, which allowed the flexible specification of feature templates that are very useful for these tasks." ></td>
	<td class="line x" title="180:200	The present model is able to bring together both these lines of work, by integrating the strengths of both approaches." ></td>
	<td class="line x" title="181:200	There have been other attempts in NLP to jointly model multiple levels of structure, with varying degrees of success." ></td>
	<td class="line x" title="182:200	Most work on joint parsing and semantic role labeling (SRL) has been disappointing, despite obvious connections between the two tasks." ></td>
	<td class="line x" title="183:200	Sutton and McCallum (2005) attempted to jointly model PCFG parsing and SRL for the CoNLL 2005 shared task, but were unable to improve performance on either task." ></td>
	<td class="line x" title="184:200	The CoNLL 2008 shared task (Surdeanu et al., 2008) was joint dependency parsing and SRL, but the top performing systems decoupled the tasks, rather than building joint models." ></td>
	<td class="line x" title="185:200	Zhang and Clark (2008) successfully built a joint 332 VP VBD were NP NP NNS members PP IN of NP NP the [Egyptian Islamic Jihad]PER PP IN with NP NP NNS ties PP TO to NP NML NNP [Osama NNP Bin NNP Laden]PER (a) VP VBD were NP NNS members PP IN of NP DT the NamedEntity-ORG* Egyptian Islamic Jihad PP IN with NP NP NNS ties PP TO to NP-PER* NNP-PER Osama NNP-PER Bin NNP-PER Laden (b) Figure 4: An example for which the joint model helped with both parse structure and named entity recognition." ></td>
	<td class="line x" title="186:200	The individual models (a) incorrectly attach the PP, label Egyptian Islamic Jihad as a person, and incorrectly add extra internal structure to Osama Bin Laden." ></td>
	<td class="line x" title="187:200	The joint model (b) gets both the structure and the named entity correct." ></td>
	<td class="line x" title="188:200	model of Chinese word segmentation and parts of speech using a single perceptron." ></td>
	<td class="line x" title="189:200	An alternative approach to joint modeling is to take a pipelined approach." ></td>
	<td class="line x" title="190:200	Previous work on linguistic annotation pipelines (Finkel et al., 2006; Hollingshead and Roark, 2007) has enforced consistency from one stage to the next." ></td>
	<td class="line x" title="191:200	However, these models are only used at test time; training of the components is still independent." ></td>
	<td class="line x" title="192:200	These models also have the potential to suffer from search errors and are not guaranteed to find the optimal output." ></td>
	<td class="line x" title="193:200	7 Conclusion We presented a discriminatively trained joint model of parsing and named entity recognition, which improved performance on both tasks." ></td>
	<td class="line x" title="194:200	Our model is based on a discriminative constituency parser, with the data, grammar, and features carefully constructed for the joint task." ></td>
	<td class="line x" title="195:200	In the future, we would like to add other levels of annotation available in the OntoNotes corpus to our model, including word sense disambiguation and semantic role labeling." ></td>
	<td class="line x" title="196:200	Acknowledgements The first author is supported by a Stanford Graduate Fellowship." ></td>
	<td class="line x" title="197:200	This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM." ></td>
	<td class="line x" title="198:200	The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred." ></td>
	<td class="line x" title="199:200	We also wish to thank the creators of OntoNotes, without which this project would not have been possible." ></td>
	<td class="line x" title="200:200	333" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-1068
Hierarchical Bayesian Domain Adaptation
Finkel, Jenny Rose;Manning, Christopher D.;"></td>
	<td class="line x" title="1:220	Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 602610, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:220	c 2009 Association for Computational Linguistics Hierarchical Bayesian Domain Adaptation Jenny Rose Finkel and Christopher D. Manning Computer Science Department Stanford University Stanford, CA 94305 {jrfinkel|manning}@cs.stanford.edu Abstract Multi-task learning is the problem of maximizing the performance of a system across a number of related tasks." ></td>
	<td class="line x" title="3:220	When applied to multiple domains for the same task, it is similar to domain adaptation, but symmetric, rather than limited to improving performance on a target domain." ></td>
	<td class="line x" title="4:220	We present a more principled, better performing model for this problem, based on the use of a hierarchical Bayesian prior." ></td>
	<td class="line x" title="5:220	Each domain has its own domain-specific parameter for each feature but, rather than a constant prior over these parameters, the model instead links them via a hierarchical Bayesian global prior." ></td>
	<td class="line x" title="6:220	This prior encourages the features to have similar weights across domains, unless there is good evidence to the contrary." ></td>
	<td class="line x" title="7:220	We show that the method of (Daume III, 2007), which was presented as a simple preprocessing step, is actually equivalent, except our representation explicitly separates hyperparameters which were tied in his work." ></td>
	<td class="line x" title="8:220	We demonstrate that allowing different values for these hyperparameters significantly improves performance over both a strong baseline and (Daume III, 2007) within both a conditional random field sequence model for named entity recognition and a discriminatively trained dependency parser." ></td>
	<td class="line x" title="9:220	1 Introduction The goal of multi-task learning is to improve performance on a set of related tasks, when provided with (potentially varying quantities of) annotated data for each of the tasks." ></td>
	<td class="line x" title="10:220	It is very closely related to domain adaptation, a far more common task in the natural language processing community, but with two primary differences." ></td>
	<td class="line x" title="11:220	Firstly, in domain adaptation the different tasks are actually just different domains." ></td>
	<td class="line x" title="12:220	Secondly, in multi-task learning the focus is on improving performance across all tasks, while in domain adaptation there is a distinction between source data and target data, and the goal is to improve performance on the target data." ></td>
	<td class="line x" title="13:220	In the present work we focus on domain adaptation, but like the multi-task setting, we wish to improve performance across all domains and not a single target domains." ></td>
	<td class="line x" title="14:220	The word domain is used here somewhat loosely: it may refer to a topical domain or to distinctions that linguists might term mode (speech versus writing) or register (formal written prose versus SMS communications)." ></td>
	<td class="line x" title="15:220	For example, one may have a large amount of parsed newswire, and want to use it to augment a much smaller amount of parsed e-mail, to build a higher quality parser for e-mail data." ></td>
	<td class="line x" title="16:220	We also consider the extension to the task where the annotation is not the same, but is consistent, across domains (that is, some domains may be annotated with more information than others)." ></td>
	<td class="line x" title="17:220	This problem is important because it is omnipresent in real life natural language processing tasks." ></td>
	<td class="line x" title="18:220	Annotated data is expensive to produce and limited in quantity." ></td>
	<td class="line x" title="19:220	Typically, one may begin with a considerable amount of annotated newswire data, some annotated speech data, and a little annotated e-mail data." ></td>
	<td class="line x" title="20:220	It would be most desirable if the aggregated training data could be used to improve the performance of a system on each of these domains." ></td>
	<td class="line x" title="21:220	From the baseline of building separate systems for each domain, the obvious first attempt at domain adaptation is to build a system from the union of the training data, and we will refer to this as a second baseline." ></td>
	<td class="line x" title="22:220	In this paper we propose a more principled, formal model of domain adaptation, which not only outperforms previous work, but maintains attractive 602 performance characteristics in terms of training and testing speed." ></td>
	<td class="line x" title="23:220	We also show that the domain adaptation work of (Daume III, 2007), which is presented as an ad-hoc preprocessing step, is actually equivalent to our formal model." ></td>
	<td class="line x" title="24:220	However, our representation of the model conceptually separates some of the hyperparameters which are not separated in (Daume III, 2007), and we found that setting these hyperparameters with different values from one another was critical for improving performance." ></td>
	<td class="line x" title="25:220	We apply our model to two tasks, named entity recognition, using a linear chain conditional random field (CRF), and dependency parsing, using a discriminative, chart-based model." ></td>
	<td class="line x" title="26:220	In both cases, we find that our model improves performance over both baselines and prior work." ></td>
	<td class="line x" title="27:220	2 Hierarchical Bayesian Domain Adaptation 2.1 Motivation We call our model hierarchical Bayesian domain adaptation, because it makes use of a hierarchical Bayesian prior." ></td>
	<td class="line x" title="28:220	As an example, take the case of building a logistic classifier to decide if a word is part of a persons name." ></td>
	<td class="line x" title="29:220	There will be a parameter (weight) for each feature, and usually there is a zero-mean Gaussian prior over the parameter values so that they dont get too large.1 In the standard, single-domain, case the log likelihood of the data and prior is calculated, and the optimal parameter values are found." ></td>
	<td class="line x" title="30:220	Now, lets extend this model to the case of two domains, one containing American newswire and the other containing British newswire." ></td>
	<td class="line x" title="31:220	The data distributions will be similar for the two domains, but not identical." ></td>
	<td class="line x" title="32:220	In our model, we have separate parameters for each feature in each domain." ></td>
	<td class="line x" title="33:220	We also have a top level parameter (also to be learned) for each feature." ></td>
	<td class="line x" title="34:220	For each domain, the Gaussian prior over the parameter values is now centered around these top level parameters instead of around zero." ></td>
	<td class="line x" title="35:220	A zero-mean Gaussian prior is then placed over the top level parameters." ></td>
	<td class="line x" title="36:220	In this example, if some feature, say word=Nigel, only appears in the British newswire, the corresponding weight for the American newswire will have a similar value." ></td>
	<td class="line x" title="37:220	This happens because the evidence in the British domain will push the British parameter 1This can be regarded as a Bayesian prior or as weight regularization; we adopt the former perspective here." ></td>
	<td class="line x" title="38:220	to have a high value, and this will in turn influence the top-level parameter to have a high value, which will then influence the American newswire to have a high value, because there will be no evidence in the American data to override the prior." ></td>
	<td class="line x" title="39:220	Conversely, if some feature is highly indicative of isName=true for the British newswire, and of isName=false for the American newswire, then the British parameter will have a high (positive) value while the American parameter will have a low (negative) value, because in both cases the domain-specific evidence will outweigh the effect of the prior." ></td>
	<td class="line x" title="40:220	2.2 Formal Model Our domain adaptation model is based on a hierarchical Bayesian prior, through which the domainspecific parameters are tied." ></td>
	<td class="line x" title="41:220	The model is very general-purpose, and can be applied to any discriminative learning task for which one would typically put a prior with a mean over the parameters." ></td>
	<td class="line x" title="42:220	We will build up to it by first describing a general, singledomain, discriminative learning task, and then we will show how to modify this model to construct our hierarchical Bayesian domain adaptation model." ></td>
	<td class="line x" title="43:220	In a typical discriminative probabilistic model, the learning process consists of optimizing the log conditional likelihood of the data with respect to the parameters, Lorig(D;)." ></td>
	<td class="line x" title="44:220	This likelihood function can take on many forms: logistic regression, a conditional Markov model, a conditional random field, as well as others." ></td>
	<td class="line x" title="45:220	It is common practice to put a zeromean Gaussian prior over the parameters, leading to the following objective, for which we wish to find the optimal parameter values: argmax  parenleftBigg Lorig(D;) i 2i 2 2 parenrightBigg (1) From a graphical models perspective, this looks like Figure 1(a), where  is the mean for the prior (in our case, zero),  2 is the variance for the prior,  are the parameters, or feature weights, and D is the data." ></td>
	<td class="line x" title="46:220	Now we will extend this single-domain model into a multi-domain model (illustrated in Figure 1(b))." ></td>
	<td class="line x" title="47:220	Each feature weight i is replicated once for each domain, as well as for a top-level set of parameters." ></td>
	<td class="line x" title="48:220	We will refer to the parameters for domain d as d, with individual components d,i, the toplevel parameters as , and all parameters collectively as ." ></td>
	<td class="line x" title="49:220	All of the power of our model stems from the relationship between these sets of param603    D N    d d Dd N M    txt txt sp sp d d d d Dd Dd (a) (b) (c) Figure 1: (a) No domain adaptation." ></td>
	<td class="line x" title="50:220	The model parameters, , are normally distributed, with mean  (typically zero) and variance 2." ></td>
	<td class="line x" title="51:220	The likelihood of the data,D, is dependent on the model parameters." ></td>
	<td class="line x" title="52:220	The form of the data distribution depends on the underlying model (e.g., logistic regression, or a CRF)." ></td>
	<td class="line x" title="53:220	(b) Our hierarchical domain adaptation model." ></td>
	<td class="line x" title="54:220	The top-level parameters, , are normally distributed, with mean  (typically zero) and variance 2 . There is a plate for each domain." ></td>
	<td class="line x" title="55:220	Within each plate, the domain-specific parameters, d are normally distributed, with mean  and variance 2d ." ></td>
	<td class="line x" title="56:220	(c) Our hierarchical domain adaptation model, with an extra level of structure." ></td>
	<td class="line x" title="57:220	In this example, the domains are further split into text and speech super-domains, each of which has its own set of parameters (txt and txt for text and sp and sp for speech)." ></td>
	<td class="line x" title="58:220	d is normally distributed with mean txt if domain d is in the text super-domain, and sp if it is in the speech super-domain." ></td>
	<td class="line x" title="59:220	eters." ></td>
	<td class="line x" title="60:220	First, we place a zero-mean Gaussian prior over the top level parameters ." ></td>
	<td class="line x" title="61:220	Then, these top level parameters are used as the mean for a Gaussian prior placed over each of the domain-specific parameters d. These domain-specific parameters are then the parameters used in the original conditional log likelihood functions for each domain." ></td>
	<td class="line x" title="62:220	The domainspecific parameter values jointly influence an appropriate value for the higher-level parameters." ></td>
	<td class="line x" title="63:220	Conversely, the higher-level parameters will largely determine the domain-specific parameters when there is little or no evidence from within a domain, but can be overriden by domain-specific evidence when it clearly goes against the general picture (for instance Leeds is normally a location, but within the sports domain is usually an organization (football team))." ></td>
	<td class="line x" title="64:220	The beauty of this model is that the degree of influence each domain exerts over the others, for each parameter, is based on the amount of evidence each domain has about that parameter." ></td>
	<td class="line x" title="65:220	If a domain has a lot of evidence for a feature weight, then that evidence will outweigh the effect of the prior." ></td>
	<td class="line x" title="66:220	However, when a domain lacks evidence for a parameter the opposite occurs, and the prior (whose value is determined by evidence in the other domains) will have a greater effect on the parameter value." ></td>
	<td class="line x" title="67:220	To achieve this, we modify the objective function." ></td>
	<td class="line x" title="68:220	We now sum over the log likelihood for all domains, including a Gaussian prior for each domain, but which is now centered around , the top-level parameters." ></td>
	<td class="line x" title="69:220	Outside of this summation, we have a Gaussian prior over the top-level parameters which is identical to the prior in the original model: Lhier(D;) = (2)  d parenleftBigg Lorig(Dd;d) i (d,i ,i)2 2 2d parenrightBigg  i (,i)2 2 2 where  2d and  2 are variances on the priors over the parameters for all the domains, as well as the top-level parameters." ></td>
	<td class="line x" title="70:220	The graphical models representation is shown in Figure 1(b)." ></td>
	<td class="line x" title="71:220	One potential source of confusion is with respect to the directed or undirected nature of our domain adaptation model, and the underlying model of the data." ></td>
	<td class="line x" title="72:220	Our hierarchical Bayesian domain adaptation model is directed, as illustrated in Figure 1." ></td>
	<td class="line x" title="73:220	However, somewhat counterintuitively, the underlying (original) model of the data can be either directed or undirected, and for our experiments we use undi604 rected, conditional random field-based models." ></td>
	<td class="line x" title="74:220	The directed domain adaptation model can be viewed as a model of the parameters, and those parameter weights are used by the underlying data model." ></td>
	<td class="line x" title="75:220	In Figure 1, the entire data model is represented by a single node, D, conditioned on the parameters,  or d. The form of that model can then be almost anything, including an undirected model." ></td>
	<td class="line x" title="76:220	From an implementation perspective, the objective function is not much more difficult to implement than the original single-domain model." ></td>
	<td class="line x" title="77:220	For all of our experiments, we optimized the log likelihood using L-BFGS, which requires the function value and partial derivatives of each parameter." ></td>
	<td class="line x" title="78:220	The new partial derivatives for the domain-specific parameters (but not the top-level parameters) utilize the same partial derivatives as in the original model." ></td>
	<td class="line x" title="79:220	The only change in the calculations is with respect to the priors." ></td>
	<td class="line x" title="80:220	The partial derivatives for the domain-specific parameters are: Lhier(D;) d,i = Ld(Dd,d) d,i  d,i ,i  2d (3) and the derivatives for the top level parameters  are: Lhier(D;) ,i = parenleftBigg  d ,i d,i  2d parenrightBigg  ,i 2  (4) This function is convex." ></td>
	<td class="line x" title="81:220	Once the optimal parameters have been learned, the top level parameters can be discarded, since the runtime model for each domain is the same as the original (single-domain) model, parameterized by the parameters learned for that domain in the hierarchical model." ></td>
	<td class="line x" title="82:220	However, it may be useful to retain the top-level parameters for use in adaptation to further domains in the future." ></td>
	<td class="line x" title="83:220	In our model there are d extra hyper-parameters which can be tuned." ></td>
	<td class="line x" title="84:220	These are the variances  2d for each domain." ></td>
	<td class="line x" title="85:220	When this value is large then the prior has little influence, and when set high enough will be equivalent to training each model separately." ></td>
	<td class="line x" title="86:220	When this value is close to zero the prior has a strong influence, and when it is sufficiently close to zero then it will be equivalent to completely tying the parameters, such that d1,i = d2,i for all domains." ></td>
	<td class="line x" title="87:220	Despite having many more parameters, for both of the tasks on which we performed experiments, we found that our model did not take much more time to train that a baseline model trained on all of the data concatenated together." ></td>
	<td class="line x" title="88:220	2.3 Model Generalization The model as presented thus far can be viewed as a two level tree, with the top-level parameters at the root, and the domain-specific ones at the leaves." ></td>
	<td class="line x" title="89:220	However, it is straightforward to generalize the model to any tree structure." ></td>
	<td class="line x" title="90:220	In the generalized version, the domain-specific parameters would still be at the leaves, the top-level parameters at the root, but new mid-level parameters can be added based on beliefs about how similar the various domains are." ></td>
	<td class="line x" title="91:220	For instance, if one had four datasets, two of which contained speech data and two of which contained newswire, then it might be sensible to have two sets of mid-level parameters, one for the speech data and one for the newswire data, as illustrated in Figure 1(c)." ></td>
	<td class="line x" title="92:220	This would allow the speech domains to influence one another more than the newswire domains, and vice versa." ></td>
	<td class="line x" title="93:220	2.4 Formalization of (Daume III, 2007) As mentioned earlier, our model is equivalent to that presented in (Daume III, 2007), and can be viewed as a formal version of his model.2 In his presentation, the adapation is done through feature augmentation." ></td>
	<td class="line x" title="94:220	Specifically, for each feature in the original version, a new version is created for each domain, as well as a general, domain-independent version of the feature." ></td>
	<td class="line x" title="95:220	For each datum, two versions of each original feature are present: the version for that datums domain, and the domain independent one." ></td>
	<td class="line x" title="96:220	The equivalence between the two models can be shown with simple arithmetic." ></td>
	<td class="line x" title="97:220	Recall that the log likelihood of our model is:  d parenleftBigg Lorig(Dd;d) i (d,i ,i)2 2 2d parenrightBigg  i (,i)2 2 2 We now introduce a new variable d = d , and plug it into the equation for log likelihood:  d parenleftBigg Lorig(Dd;d +) i (d,i)2 2 2d parenrightBigg  i (,i)2 2 2 The result is the model of (Daume III, 2007), where the d are the domain-specific feature weights, and d are the domain-independent feature weights." ></td>
	<td class="line x" title="98:220	In his formulation, the variances  2d =  2 for all domains d. This separation of the domain-specific and independent variances was critical to our improved performance." ></td>
	<td class="line x" title="99:220	When using a Gaussian prior there are 2Many thanks to David Vickrey for pointing this out to us." ></td>
	<td class="line x" title="100:220	605 two parameters set by the user: the mean,  (usually zero), and the variance,  2." ></td>
	<td class="line x" title="101:220	Technically, each of these parameters is actually a vector, with an entry for each feature, but almost always the vectors are uniform and the same parameter is used for each feature (there are exceptions, e.g.(Lee et al., 2007))." ></td>
	<td class="line x" title="103:220	Because Daume III (2007) views the adaptation as merely augmenting the feature space, each of his features has the same prior mean and variance, regardless of whether it is domain specific or independent." ></td>
	<td class="line x" title="104:220	He could have set these parameters differently, but he did not.3 In our presentation of the model, we explicitly represent different variances for each domain, as well as the top level parameters." ></td>
	<td class="line x" title="105:220	We found that specifying different values for the domain specific versus domain independent variances significantly improved performance, though we found no gains from using different values for the different domain specific variances." ></td>
	<td class="line x" title="106:220	The values were set based on development data." ></td>
	<td class="line x" title="107:220	3 Named Entity Recognition For our first set of experiments, we used a linearchain, conditional random field (CRF) model, trained for named entity recognition (NER)." ></td>
	<td class="line x" title="108:220	The use of CRFs for sequence modeling has become standard so we will omit the model details; good explanations can be found in a number of places (Lafferty et al., 2001; Sutton and McCallum, 2007)." ></td>
	<td class="line oc" title="109:220	Our features were based on those in (Finkel et al., 2005)." ></td>
	<td class="line x" title="110:220	3.1 Data We used three named entity datasets, from the CoNLL 2003, MUC-6 and MUC-7 shared tasks." ></td>
	<td class="line x" title="111:220	CoNLL is British newswire, while MUC-6 and MUC-7 are both American newswire." ></td>
	<td class="line x" title="112:220	Arguably MUC-6 and MUC-7 should not count as separate domains, but because they were annotated separately, for different shared tasks, we chose to treat them as such, and feel that our experimental results justify the distinction." ></td>
	<td class="line x" title="113:220	We used the standard train and test sets for each domain, which for CoNLL corresponds to the (more difficult) testb set." ></td>
	<td class="line x" title="114:220	For details about the number of training and test words in each dataset, please see Table 1." ></td>
	<td class="line x" title="115:220	One interesting challenge in dealing with both CoNLL and MUC data is that the label sets differ." ></td>
	<td class="line x" title="116:220	3Although he alludes to the potential for something similar in the last section of his paper, when discussing the kernelization interpretation of his approach." ></td>
	<td class="line x" title="117:220	# Train # Test Words Words MUC-6 165,082 15,032 MUC-7 89,644 64,490 CoNLL 203,261 46,435 Table 1: Number of words in the training and test sets for each of the named entity recognition datasets." ></td>
	<td class="line x" title="118:220	CoNLL has four classes: person, organization, location, and misc." ></td>
	<td class="line x" title="119:220	MUC data has seven classes: person, organization, location, percent, date, time, and money." ></td>
	<td class="line x" title="120:220	They overlap in the three core classes (person, organization, and location), but CoNLL has one additional class and MUC has four additional classes." ></td>
	<td class="line x" title="121:220	The differences in the label sets led us to perform two sets of experiments for the baseline and hierarchical Bayesian models." ></td>
	<td class="line x" title="122:220	In the first set of experiments, at training time, the model allows any label from the union of the label sets, regardless of whether that label was legal for the domain." ></td>
	<td class="line x" title="123:220	At test time, we would ignore guesses made by the model which were inconsistent with the allowed labels for that domain.4 In the second set of experiments, we restricted the model at training time to only allow legal labels for each domain." ></td>
	<td class="line x" title="124:220	At test time, the domain was specified, and the model was once again restricted so that words would never be tagged with a label outside of that domains label set." ></td>
	<td class="line x" title="125:220	3.2 Experimental Results and Discussion In our experiments, we compared our model to several strong baselines, and the full set of results is in Table 2." ></td>
	<td class="line x" title="126:220	The models we used were: TARGET ONLY." ></td>
	<td class="line x" title="127:220	Trained and tested on only the data for that domain." ></td>
	<td class="line x" title="128:220	ALL DATA." ></td>
	<td class="line x" title="129:220	Trained and tested on data from all domains, concatenated into one large dataset." ></td>
	<td class="line x" title="130:220	ALL DATA*." ></td>
	<td class="line x" title="131:220	Same as ALL DATA, but restricted possible labels for each word based on domain." ></td>
	<td class="line x" title="132:220	DAUME07." ></td>
	<td class="line x" title="133:220	Trained and tested using the same technique as (Daume III, 2007)." ></td>
	<td class="line x" title="134:220	We note that they present results using per-token label accuracy, while we used the more standard entity precision, recall, and F score (as in the CoNLL 2003 shared task)." ></td>
	<td class="line x" title="135:220	4We treated them identically to the background symbol." ></td>
	<td class="line x" title="136:220	So, for instance, labelling a word a date in the CoNLL data had no effect on the score." ></td>
	<td class="line x" title="137:220	606 Named Entity Recognition Model Precision Recall F1 MUC-6 TARGET ONLY 86.74 80.10 83.29 ALL DATA* 85.04 83.49 84.26 ALL DATA 86.00 82.71 84.32 DAUME07* 87.83 83.41 85.56 DAUME07 87.81 82.23 85.46 HIER BAYES* 88.59 84.97 86.74 HIER BAYES 88.77 85.14 86.92 MUC-7 TARGET ONLY 81.17 70.23 75.30 ALL DATA* 81.66 76.17 78.82 ALL DATA 82.20 70.91 76.14 DAUME07* 83.33 75.42 79.18 DAUME07 83.51 75.63 79.37 HIER BAYES* 82.90 76.95 79.82 HIER BAYES 83.17 77.02 79.98 CoNLL TARGET ONLY 85.55 84.72 85.13 ALL DATA* 86.34 84.45 85.38 ALL DATA 86.58 83.90 85.22 DAUME07* 86.09 85.06 85.57 DAUME07 86.35 85.26 85.80 HIER BAYES* 86.33 85.06 85.69 HIER BAYES 86.51 85.13 85.81 Table 2: Named entity recognition results for each of the models." ></td>
	<td class="line x" title="138:220	With the exception of the TARGET ONLY model, all three datasets were combined when training each of the models." ></td>
	<td class="line x" title="139:220	DAUME07*." ></td>
	<td class="line x" title="140:220	Same as DAUME07, but restricted possible labels for each word based on domain." ></td>
	<td class="line x" title="141:220	HIER BAYES." ></td>
	<td class="line x" title="142:220	Our hierarchical Bayesian domain adaptation model." ></td>
	<td class="line x" title="143:220	HIER BAYES*." ></td>
	<td class="line x" title="144:220	Same as HIER BAYES, but restricted possible labels for each word based on the domain." ></td>
	<td class="line x" title="145:220	For all of the baseline models, and for the top level-parameters in the hierarchical Bayesian model, we used  = 1." ></td>
	<td class="line x" title="146:220	For the domain-specific parameters, we used d = 0.1 for all domains." ></td>
	<td class="line x" title="147:220	The HIER BAYES model outperformed all baselines for both of the MUC datasets, and tied with the DAUME07 for CoNLL." ></td>
	<td class="line x" title="148:220	The largest improvement was on MUC-6, where HIER BAYES outperformed DAUME07*, the second best model, by 1.36%." ></td>
	<td class="line x" title="149:220	This improvement is greater than the improvement made by that model over the ALL DATA* baseline." ></td>
	<td class="line x" title="150:220	To assess significance we used a document-level paired t-test (over all of the data combined), and found that HIER BAYES significantly outperformed all of the baselines (not including HIER BAYES*) with greater than 95% confidence." ></td>
	<td class="line x" title="151:220	For both the HIER BAYES and DAUME07 models, we found that performance was better for the variant which did not restrict possible labels based on the domain, while the ALL DATA model did benefit from the label restriction." ></td>
	<td class="line x" title="152:220	For HIER BAYES and DAUME07, this result may be due to the structure of the models." ></td>
	<td class="line x" title="153:220	Because both models have domainspecific features, the models likely learned that these labels were never actually allowed." ></td>
	<td class="line x" title="154:220	However, when a feature does not occur in the data for a particular domain, then the domain-specific parameter for that feature will have positive weight due to evidence present in the other domains, which at test time can lead to assigning an illegal label to a word." ></td>
	<td class="line x" title="155:220	This information that a word may be of some other (unknown to that domain) entity type may help prevent the model from mislabeling the word." ></td>
	<td class="line x" title="156:220	For example, in CoNLL, nationalities, such as Iraqi and American, are labeled as misc." ></td>
	<td class="line x" title="157:220	If a previously unseen nationality is encountered in the MUC testing data, the MUC model may be tempted to label is as a location, but this evidence from the CoNLL data may prevent that, by causing it to instead be labeled misc, a label which will subsequently be ignored." ></td>
	<td class="line x" title="158:220	In typical domain adaptation work, showing gains is made easier by the fact that the amount of training data in the target domain is comparatively small." ></td>
	<td class="line x" title="159:220	Within the multi-task learning setting, it is more challenging to show gains over the ALL DATA baseline." ></td>
	<td class="line x" title="160:220	Nevertheless, our results show that, so long as the amount of data in each domain is not widely disparate, it is possible to achieve gains on all of the domains simultaneously." ></td>
	<td class="line x" title="161:220	4 Dependency Parsing 4.1 Parsing Model We also tested our model on an untyped dependency parsing task, to see how it performs on a more structurally complex task than sequence modeling." ></td>
	<td class="line x" title="162:220	To our knowledge, the discriminatively trained dependency model we used has not been previously published, but it is very similar to recent work on discriminative constituency parsing (Finkel and Manning, 2008)." ></td>
	<td class="line x" title="163:220	Due to space restrictions, we cannot give a complete treatment of the model, but will give an overview." ></td>
	<td class="line x" title="164:220	607 We built a CRF-based model, optimizing the likelihood of the parse, conditioned on the words and parts of speech of the sentence." ></td>
	<td class="line x" title="165:220	At the heart of our model is the Eisner dependency grammar chartparsing algorithm (Eisner, 1996), which allows for efficient computation of inside and outside scores." ></td>
	<td class="line x" title="166:220	The Eisner algorithm, originally designed for generative parsing, decomposes the probability of a dependency parse into the probabilities of each attachment of a dependent to its parent, and the probabilities of each parent stopping taking dependents." ></td>
	<td class="line x" title="167:220	These probabilities can be conditioned on the child, parent, and direction of the dependency." ></td>
	<td class="line x" title="168:220	We used a slight modification of the algorithm which allows each probability to also be conditioned on whether there is a previous dependent." ></td>
	<td class="line x" title="169:220	While the unmodified version of the algorithm includes stopping probabilities, conditioned on the parent and direction, they have no impact on which parse for a particular sentence is most likely, because all words must eventually stop taking dependents." ></td>
	<td class="line x" title="170:220	However, in the modified version, the stopping probability is also conditioned on whether or not there is a previous dependent, so this probability does make a difference." ></td>
	<td class="line x" title="171:220	While the Eisner algorithm computes locally normalized probabilities for each attachment decision, our model computes unnormalized scores." ></td>
	<td class="line x" title="172:220	From a graphical models perspective, our parsing model is undirected, while the original model is directed.5 The score for a particular tree decomposes the same way in our model as in the original Eisner model, but it is globally normalized instead of locally normalized." ></td>
	<td class="line x" title="173:220	Using the inside and outside scores we can compute partial derivatives for the feature weights, as well as the value of the normalizing constant needed to determine the probability of a particular parse." ></td>
	<td class="line x" title="174:220	This is done in a manner completely analogous to (Finkel and Manning, 2008)." ></td>
	<td class="line x" title="175:220	Partial derivatives and the function value are all that is needed to find the optimal feature weights using L-BFGS.6 Features are computed over each attachment and stopping decision, and can be conditioned on the 5The dependencies themselves are still directed in both cases, it is just the underlying graphical model used to compute the likelihood of a parse which changes from a directed model to an undirected model." ></td>
	<td class="line x" title="176:220	6In (Finkel and Manning, 2008) we used stochastic gradient descent to optimize our weights because our function evaluation was too slow to use L-BFGS." ></td>
	<td class="line x" title="177:220	We did not encounter this problem in this setting." ></td>
	<td class="line x" title="178:220	parent, dependent (or none, if it is a stopping decision), direction of attachment, whether there is a previous dependent in that direction, and the words and parts of speech of the sentence." ></td>
	<td class="line x" title="179:220	We used the same features as (McDonald et al., 2005), augmented with information about whether or not a dependent is the first dependent (information they did not have)." ></td>
	<td class="line x" title="180:220	4.2 Data For our dependency parsing experiments, we used LDC2008T04 OntoNotes Release 2.0 data (Hovy et al., 2006)." ></td>
	<td class="line x" title="181:220	This dataset is still in development, and includes data from seven different domains, labeled for a number of tasks, including PCFG trees." ></td>
	<td class="line x" title="182:220	The domains span both newswire and speech from multiple sources." ></td>
	<td class="line x" title="183:220	We converted the PCFG trees into dependency trees using the Collins head rules (Collins, 2003)." ></td>
	<td class="line x" title="184:220	We also omitted the WSJ portion of the data, because it follows a different annotation scheme from the other domains.7 For each of the remaining six domains, we aimed for an 75/25 data split, but because we divided the data using the provided sections, this split was fairly rough." ></td>
	<td class="line x" title="185:220	The number of training and test sentences for each domain are specified in the Table 3, along with our results." ></td>
	<td class="line x" title="186:220	4.3 Experimental Results and Discussion We compared the same four domain adaptation models for dependency parsing as we did for the named entity experiments, once again setting  = 1.0 and d = 0.1." ></td>
	<td class="line x" title="187:220	Unlike the named entity experiments however, there were no label set discrepencies between the domains, so only one version of each domain adaptation model was necessary, instead of the two versions in that section." ></td>
	<td class="line x" title="188:220	Our full dependency parsing results can be found in Table 3." ></td>
	<td class="line x" title="189:220	Firstly, we found that DAUME07, which had outperformed the ALL DATA baseline for the sequence modeling task, performed worse than the 7Specifically, all the other domains use the new Penn Treebank annotation style, whereas the WSJ data is still in the traditional annotation style, familiar from the past decades work in Penn Treebank parsing." ></td>
	<td class="line x" title="190:220	The major changes are in hyphenation and NP structure." ></td>
	<td class="line x" title="191:220	In the new annotation style, many hyphenated words are separated into multiple tokens, with a new part-of-speech tag given to the hyphens, and leftwardbranching structure inside noun phrases is indicated by use of a new NML phrasal category." ></td>
	<td class="line x" title="192:220	The treatment of hyphenated words, in particular, makes the two annotation styles inconsistent, and so we could not work with all the data together." ></td>
	<td class="line x" title="193:220	608 Dependency Parsing Training Testing TARGET ALL HIER Range # Sent Range # Sent ONLY DATA DAUME07 BAYES ABC 055 1195 5669 199 83.32% 88.97% 87.30% 88.68% CNN 0375 5092 376437 1521 85.53% 87.09% 86.41% 87.26% MNB 017 509 1825 245 77.06% 86.41% 84.70% 86.71% NBC 029 552 3039 149 76.21% 85.82% 85.01% 85.32% PRI 089 1707 90112 394 87.65% 90.28% 89.52% 90.59% VOA 0198 1512 199264 383 89.17% 92.11% 90.67% 92.09% Table 3: Dependency parsing results for each of the domain adaptation models." ></td>
	<td class="line x" title="194:220	Performance is measured as unlabeled attachment accuracy." ></td>
	<td class="line x" title="195:220	baseline here, indicating that the transfer of information between domains in the more structurally complicated task is inherently more difficult." ></td>
	<td class="line x" title="196:220	Our models gains over the ALL DATA baseline are quite small, but we tested their significance using a sentence-level paired t-test (over all of the data combined) and found them to be significant at p < 105." ></td>
	<td class="line x" title="197:220	We are unsure why some domains improved while others did not." ></td>
	<td class="line x" title="198:220	It is not simply a consequence of training set size, but may be due to qualities of the domains themselves." ></td>
	<td class="line x" title="199:220	5 Related Work We already discussed the relation of our work to (Daume III, 2007) in Section 2.4." ></td>
	<td class="line x" title="200:220	Another piece of similar work is (Chelba and Acero, 2004), who also modify their prior." ></td>
	<td class="line x" title="201:220	Their work is limited to two domains, a source and a target, and their algorithm has a two stage process: First, train a classifier on the source data, and then use the learned weights from that classifier as the mean for a Gaussian prior when training a new model on just the target data." ></td>
	<td class="line x" title="202:220	Daume III and Marcu (2006) also took a Bayesian approach to domain adaptation, but structured their model in a very different way." ></td>
	<td class="line x" title="203:220	In their model, it is assumed that each datum within a domain is either a domain-specific datum, or a general datum, and then domain-specific and general weights were learned." ></td>
	<td class="line x" title="204:220	Whether each datum is domain-specific or general is not known, so they developed an EM based algorithm for determining this information while simultaneously learning the feature weights." ></td>
	<td class="line x" title="205:220	Their model had good performance, but came with a 10 to 15 times slowdown at training time." ></td>
	<td class="line x" title="206:220	Our slowest dependency parser took four days to train, making this model close to infeasible for learning on that data." ></td>
	<td class="line x" title="207:220	Outside of the NLP community there has been much similar work making use of hierarchical Bayesian priors to tie parameters across multiple, similar tasks." ></td>
	<td class="line x" title="208:220	Evgeniou et al.(2005) present a similar model, but based on support vector machines, to predict the exam scores of students." ></td>
	<td class="line x" title="210:220	Elidan et al.(2008) make us of an undirected Bayesian transfer hierarchy to jointly model the shapes of different mammals." ></td>
	<td class="line x" title="212:220	The complete literature on related multi-task learning is too large to fully discuss here, but we direct the reader to (Baxter, 1997; Caruana, 1997; Yu et al., 2005; Xue et al., 2007)." ></td>
	<td class="line x" title="213:220	For a more general discussion of hierarchical priors, we recommend Chapter 5 of (Gelman et al., 2003) and Chapter 12 of (Gelman and Hill, 2006)." ></td>
	<td class="line x" title="214:220	6 Conclusion and Future Work In this paper we presented a new model for domain adaptation, based on a hierarchical Bayesian prior, which allows information to be shared between domains when information is sparse, while still allowing the data from a particular domain to override the information from other domains when there is sufficient evidence." ></td>
	<td class="line x" title="215:220	We outperformed previous work on a sequence modeling task, and showed improvements on dependency parsing, a structurally more complex problem, where previous work failed." ></td>
	<td class="line x" title="216:220	Our model is practically useful and does not require significantly more time to train than a baseline model using the same data (though it does require more memory, proportional to the number of domains)." ></td>
	<td class="line x" title="217:220	In the future we would like to see if the model could be adapted to improve performance on data from a new domain, potentially by using the top-level weights which should be less domain-dependent." ></td>
	<td class="line x" title="218:220	Acknowledgements The first author is supported by a Stanford Graduate Fellowship." ></td>
	<td class="line x" title="219:220	We also thank David Vickrey for his helpful comments and observations." ></td>
	<td class="line x" title="220:220	609" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1113
Distant supervision for relation extraction without labeled data
Mintz, Mike;Bills, Steven;Snow, Rion;Jurafsky, Daniel;"></td>
	<td class="line x" title="1:184	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 10031011, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:184	c2009 ACL and AFNLP Distant supervision for relation extraction without labeled data Mike Mintz, Steven Bills, Rion Snow, Dan Jurafsky Stanford University / Stanford, CA 94305 {mikemintz,sbills,rion,jurafsky}@cs.stanford.edu Abstract Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora." ></td>
	<td class="line x" title="3:184	We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACEstyle algorithms, and allowing the use of corpora of any size." ></td>
	<td class="line x" title="4:184	Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision." ></td>
	<td class="line x" title="5:184	For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classifier." ></td>
	<td class="line x" title="6:184	Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain)." ></td>
	<td class="line x" title="7:184	Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%." ></td>
	<td class="line x" title="8:184	We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression." ></td>
	<td class="line x" title="9:184	1 Introduction At least three learning paradigms have been applied to the task of extracting relational facts from text (for example, learning that a person is employed by a particular organization, or that a geographic entity is located in a particular region)." ></td>
	<td class="line x" title="10:184	In supervised approaches, sentences in a corpus are first hand-labeled for the presence of entities and the relations between them." ></td>
	<td class="line x" title="11:184	The NIST Automatic Content Extraction (ACE) RDC 2003 and 2004 corpora, for example, include over 1,000 documents in which pairs of entities have been labeled with 5 to 7 major relation types and 23 to 24 subrelations, totaling 16,771 relation instances." ></td>
	<td class="line x" title="12:184	ACE systems then extract a wide variety of lexical, syntactic, and semantic features, and use supervised classifiers to label the relation mention holding between a given pair of entities in a test set sentence, optionally combining relation mentions (Zhou et al., 2005; Zhou et al., 2007; Surdeanu and Ciaramita, 2007)." ></td>
	<td class="line x" title="13:184	Supervised relation extraction suffers from a number of problems, however." ></td>
	<td class="line x" title="14:184	Labeled training data is expensive to produce and thus limited in quantity." ></td>
	<td class="line x" title="15:184	Also, because the relations are labeled on a particular corpus, the resulting classifiers tend to be biased toward that text domain." ></td>
	<td class="line x" title="16:184	An alternative approach, purely unsupervised information extraction, extracts strings of words between entities in large amounts of text, and clusters and simplifies these word strings to produce relation-strings (Shinyama and Sekine, 2006; Banko et al., 2007)." ></td>
	<td class="line x" title="17:184	Unsupervised approaches can use very large amounts of data and extract very large numbers of relations, but the resulting relations may not be easy to map to relations needed for a particular knowledge base." ></td>
	<td class="line x" title="18:184	A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008)." ></td>
	<td class="line x" title="19:184	These seeds are used with a large corpus to extract a new set of patterns, which are used to extract more instances, which are used to extract more patterns, in an iterative fashion." ></td>
	<td class="line x" title="20:184	The resulting patterns often suffer from low precision and semantic drift." ></td>
	<td class="line x" title="21:184	We propose an alternative paradigm, distant supervision, that combines some of the advantages of each of these approaches." ></td>
	<td class="line x" title="22:184	Distant supervision is an extension of the paradigm used by Snow et al.(2005) for exploiting WordNet to extract hypernym (is-a) relations between entities, and is similar to the use of weakly labeled data in bioinformatics (Craven and Kumlien, 1999; Morgan et al., 1003 Relation name New instance /location/location/contains Paris, Montmartre /location/location/contains Ontario, Fort Erie /music/artist/origin Mighty Wagon, Cincinnati /people/deceased person/place of death Fyodor Kamensky, Clearwater /people/person/nationality Marianne Yvonne Heemskerk, Netherlands /people/person/place of birth Wavell Wayne Hinds, Kingston /book/author/works written Upton Sinclair, Lanny Budd /business/company/founders WWE, Vince McMahon /people/person/profession Thomas Mellon, judge Table 1: Ten relation instances extracted by our system that did not appear in Freebase." ></td>
	<td class="line x" title="24:184	2004)." ></td>
	<td class="line x" title="25:184	Our algorithm uses Freebase (Bollacker et al., 2008), a large semantic database, to provide distant supervision for relation extraction." ></td>
	<td class="line x" title="26:184	Freebase contains 116 million instances of 7,300 relations between 9 million entities." ></td>
	<td class="line x" title="27:184	The intuition of distant supervision is that any sentence that contains a pair of entities that participate in a known Freebase relation is likely to express that relation in some way." ></td>
	<td class="line x" title="28:184	Since there may be many sentences containing a given entity pair, we can extract very large numbers of (potentially noisy) features that are combined in a logistic regression classifier." ></td>
	<td class="line x" title="29:184	Thus whereas the supervised training paradigm uses a small labeled corpus of only 17,000 relation instances as training data, our algorithm can use much larger amounts of data: more text, more relations, and more instances." ></td>
	<td class="line x" title="30:184	We use 1.2 million Wikipedia articles and 1.8 million instances of 102 relations connecting 940,000 entities." ></td>
	<td class="line x" title="31:184	In addition, combining vast numbers of features in a large classifier helps obviate problems with bad features." ></td>
	<td class="line x" title="32:184	Because our algorithm is supervised by a database, rather than by labeled text, it does not suffer from the problems of overfitting and domain-dependence that plague supervised systems." ></td>
	<td class="line x" title="33:184	Supervision by a database also means that, unlike in unsupervised approaches, the output of our classifier uses canonical names for relations." ></td>
	<td class="line x" title="34:184	Our paradigm offers a natural way of integrating data from multiple sentences to decide if a relation holds between two entities." ></td>
	<td class="line x" title="35:184	Because our algorithm can use large amounts of unlabeled data, a pair of entities may occur multiple times in the test set." ></td>
	<td class="line x" title="36:184	For each pair of entities, we aggregate the features from the many different sentences in which that pair appeared into a single feature vector, allowing us to provide our classifier with more information, resulting in more accurate labels." ></td>
	<td class="line x" title="37:184	Table 1 shows examples of relation instances extracted by our system." ></td>
	<td class="line x" title="38:184	We also use this system to investigate the value of syntactic versus lexical (word sequence) features in relation extraction." ></td>
	<td class="line x" title="39:184	While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE." ></td>
	<td class="line x" title="40:184	Most previous research in bootstrapping or unsupervised IE has used only simple lexical features, thereby avoiding the computational expense of parsing (Brin, 1998; Agichtein and Gravano, 2000; Etzioni et al., 2005), and the few systems that have used unsupervised IE have not compared the performance of these two types of feature." ></td>
	<td class="line x" title="41:184	2 Previous work Except for the unsupervised algorithms discussed above, previous supervised or bootstrapping approaches to relation extraction have typically relied on relatively small datasets, or on only a small number of distinct relations." ></td>
	<td class="line x" title="42:184	Approaches based on WordNet have often only looked at the hypernym (is-a) or meronym (part-of) relation (Girju et al., 2003; Snow et al., 2005), while those based on the ACE program (Doddington et al., 2004) have been restricted in their evaluation to a small number of relation instances and corpora of less than a million words." ></td>
	<td class="line x" title="43:184	Many early algorithms for relation extraction used little or no syntactic information." ></td>
	<td class="line x" title="44:184	For example, the DIPRE algorithm by Brin (1998) used string-based regular expressions in order to recognize relations such as author-book, while the SNOWBALL algorithm by Agichtein and Gravano (2000) learned similar regular expression patterns over words and named entity tags." ></td>
	<td class="line x" title="45:184	Hearst (1992) used a small number of regular expressions over words and part-of-speech tags to find examples of the hypernym relation." ></td>
	<td class="line x" title="46:184	The use of these patterns has been widely replicated in successful systems, for example by Etzioni et al.(2005)." ></td>
	<td class="line x" title="48:184	Other work 1004 Relation name Size Example /people/person/nationality 281,107 John Dugard, South Africa /location/location/contains 253,223 Belgium, Nijlen /people/person/profession 208,888 Dusa McDuff, Mathematician /people/person/place of birth 105,799 Edwin Hubble, Marshfield /dining/restaurant/cuisine 86,213 MacAyos Mexican Kitchen, Mexican /business/business chain/location 66,529 Apple Inc., Apple Inc., South Park, NC /biology/organism classification rank 42,806 Scorpaeniformes, Order /film/film/genre 40,658 Where the Sidewalk Ends, Film noir /film/film/language 31,103 Enter the Phoenix, Cantonese /biology/organism higher classification 30,052 Calopteryx, Calopterygidae /film/film/country 27,217 Turtle Diary, United States /film/writer/film 23,856 Irving Shulman, Rebel Without a Cause /film/director/film 23,539 Michael Mann, Collateral /film/producer/film 22,079 Diane Eskenazi, Aladdin /people/deceased person/place of death 18,814 John W. Kern, Asheville /music/artist/origin 18,619 The Octopus Project, Austin /people/person/religion 17,582 Joseph Chartrand, Catholicism /book/author/works written 17,278 Paul Auster, Travels in the Scriptorium /soccer/football position/players 17,244 Midfielder, Chen Tao /people/deceased person/cause of death 16,709 Richard Daintree, Tuberculosis /book/book/genre 16,431 Pony Soldiers, Science fiction /film/film/music 14,070 Stavisky, Stephen Sondheim /business/company/industry 13,805 ATS Medical, Health care Table 2: The 23 largest Freebase relations we use, with their size and an instance of each relation." ></td>
	<td class="line x" title="49:184	such as Ravichandran and Hovy (2002) and Pantel and Pennacchiotti (2006) use the same formalism of learning regular expressions over words and part-of-speech tags to discover patterns indicating a variety of relations." ></td>
	<td class="line x" title="50:184	More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al.(2005), and work in the ACE paradigm such as Zhou et al.(2005) and Zhou et al.(2007)." ></td>
	<td class="line x" title="54:184	Perhaps most similar to our distant supervision algorithm is the effective method of Wu and Weld (2007) who extract relations from a Wikipedia page by using supervision from the pages infobox." ></td>
	<td class="line x" title="55:184	Unlike their corpus-specific method, which is specific to a (single) Wikipedia page, our algorithm allows us to extract evidence for a relation from many different documents, and from any genre." ></td>
	<td class="line x" title="56:184	3 Freebase Following the literature, we use the term relation to refer to an ordered, binary relation between entities." ></td>
	<td class="line x" title="57:184	We refer to individual ordered pairs in this relation as relation instances." ></td>
	<td class="line x" title="58:184	For example, the person-nationality relation holds between the entities named John Steinbeck and United States, so it has John Steinbeck, United States as an instance." ></td>
	<td class="line x" title="59:184	We use relations and relation instances from Freebase, a freely available online database of structured semantic data." ></td>
	<td class="line x" title="60:184	Data in Freebase is collected from a variety of sources." ></td>
	<td class="line x" title="61:184	One major source is text boxes and other tabular data from Wikipedia." ></td>
	<td class="line x" title="62:184	Data is also taken from NNDB (biographical information), MusicBrainz (music), the SEC (financial and corporate data), as well as direct, wiki-style user editing." ></td>
	<td class="line x" title="63:184	After some basic processing of the July 2008 link export to convert Freebases data representation into binary relations, we have 116 million instances of 7,300 relations between 9 million entities." ></td>
	<td class="line x" title="64:184	We next filter out nameless and uninteresting entities such as user profiles and music tracks." ></td>
	<td class="line x" title="65:184	Freebase also contains the reverses of many of its relations (bookauthor v. author-book), and these are merged." ></td>
	<td class="line x" title="66:184	Filtering and removing all but the largest relations leaves us with 1.8 million instances of 102 relations connecting 940,000 entities." ></td>
	<td class="line x" title="67:184	Examples are shown in Table 2." ></td>
	<td class="line x" title="68:184	4 Architecture The intuition of our distant supervision approach is to use Freebase to give us a training set of relations and entity pairs that participate in those relations." ></td>
	<td class="line x" title="69:184	In the training step, all entities are identified 1005 in sentences using a named entity tagger that labels persons, organizations and locations." ></td>
	<td class="line x" title="70:184	If a sentence contains two entities and those entities are an instance of one of our Freebase relations, features are extracted from that sentence and are added to the feature vector for the relation." ></td>
	<td class="line x" title="71:184	The distant supervision assumption is that if two entities participate in a relation, any sentence that contain those two entities might express that relation." ></td>
	<td class="line x" title="72:184	Because any individual sentence may give an incorrect cue, our algorithm trains a multiclass logistic regression classifier, learning weights for each noisy feature." ></td>
	<td class="line x" title="73:184	In training, the features for identical tuples (relation, entity1, entity2) from different sentences are combined, creating a richer feature vector." ></td>
	<td class="line x" title="74:184	In the testing step, entities are again identified using the named entity tagger." ></td>
	<td class="line x" title="75:184	This time, every pair of entities appearing together in a sentence is considered a potential relation instance, and whenever those entities appear together, features are extracted on the sentence and added to a feature vector for that entity pair." ></td>
	<td class="line x" title="76:184	For example, if a pair of entities occurs in 10 sentences in the test set, and each sentence has 3 features extracted from it, the entity pair will have 30 associated features." ></td>
	<td class="line x" title="77:184	Each entity pair in each sentence in the test corpus is run through feature extraction, and the regression classifier predicts a relation name for each entity pair based on the features from all of the sentences in which it appeared." ></td>
	<td class="line x" title="78:184	Consider the location-contains relation, imagining that in Freebase we had two instances of this relation: Virginia, Richmond and France, Nantes." ></td>
	<td class="line x" title="79:184	As we encountered sentences like Richmond, the capital of Virginia and Henrys Edict of Nantes helped the Protestants of France we would extract features from these sentences." ></td>
	<td class="line x" title="80:184	Some features would be very useful, such as the features from the Richmond sentence, and some would be less useful, like those from the Nantes sentence." ></td>
	<td class="line x" title="81:184	In testing, if we came across a sentence like Vienna, the capital of Austria, one or more of its features would match those of the Richmond sentence, providing evidence that Austria, Vienna belongs to the locationcontains relation." ></td>
	<td class="line x" title="82:184	Note that one of the main advantages of our architecture is its ability to combine information from many different mentions of the same relation." ></td>
	<td class="line x" title="83:184	Consider the entity pair Steven Spielberg, Saving Private Ryan from the following two sentences, as evidence for the film-director relation." ></td>
	<td class="line x" title="84:184	[Steven Spielberg]s film [Saving Private Ryan] is loosely based on the brothers story." ></td>
	<td class="line x" title="85:184	Allison co-produced the Academy Awardwinning [Saving Private Ryan], directed by [Steven Spielberg] The first sentence, while providing evidence for film-director, could instead be evidence for filmwriter or film-producer." ></td>
	<td class="line x" title="86:184	The second sentence does not mention that Saving Private Ryan is a film, and so could instead be evidence for the CEO relation (consider Robert Mueller directed the FBI)." ></td>
	<td class="line x" title="87:184	In isolation, neither of these features is conclusive, but in combination, they are." ></td>
	<td class="line x" title="88:184	5 Features Our features are based on standard lexical and syntactic features from the literature." ></td>
	<td class="line x" title="89:184	Each feature describes how two entities are related in a sentence, using either syntactic or non-syntactic information." ></td>
	<td class="line x" title="90:184	5.1 Lexical features Our lexical features describe specific words between and surrounding the two entities in the sentence in which they appear:  The sequence of words between the two entities  The part-of-speech tags of these words  A flag indicating which entity came first in the sentence  A window of k words to the left of Entity 1 and their part-of-speech tags  A window of k words to the right of Entity 2 and their part-of-speech tags Each lexical feature consists of the conjunction of all these components." ></td>
	<td class="line x" title="91:184	We generate a conjunctive feature for each k  {0,1,2}." ></td>
	<td class="line x" title="92:184	Thus each lexical row in Table 3 represents a single lexical feature." ></td>
	<td class="line x" title="93:184	Part-of-speech tags were assigned by a maximum entropy tagger trained on the Penn Treebank, and then simplified into seven categories: nouns, verbs, adverbs, adjectives, numbers, foreign words, and everything else." ></td>
	<td class="line x" title="94:184	In an attempt to approximate syntactic features, we also tested variations on our lexical features: (1) omitting all words that are not verbs and (2) omitting all function words." ></td>
	<td class="line x" title="95:184	In combination with the other lexical features, they gave a small boost to precision, but not large enough to justify the increased demand on our computational resources." ></td>
	<td class="line x" title="96:184	1006 Feature type Left window NE1 Middle NE2 Right window Lexical [] PER [was/VERB born/VERB in/CLOSED] LOC [] Lexical [Astronomer] PER [was/VERB born/VERB in/CLOSED] LOC [,] Lexical [#PAD#, Astronomer] PER [was/VERB born/VERB in/CLOSED] LOC [, Missouri] Syntactic [] PER [s waspred bornmod inpcompn] LOC [] Syntactic [Edwin Hubblelexmod] PER [s waspred bornmod inpcompn] LOC [] Syntactic [Astronomerlexmod] PER [s waspred bornmod inpcompn] LOC [] Syntactic [] PER [s waspred bornmod inpcompn] LOC [lexmod ,] Syntactic [Edwin Hubblelexmod] PER [s waspred bornmod inpcompn] LOC [lexmod ,] Syntactic [Astronomerlexmod] PER [s waspred bornmod inpcompn] LOC [lexmod ,] Syntactic [] PER [s waspred bornmod inpcompn] LOC [inside Missouri] Syntactic [Edwin Hubblelexmod] PER [s waspred bornmod inpcompn] LOC [inside Missouri] Syntactic [Astronomerlexmod] PER [s waspred bornmod inpcompn] LOC [inside Missouri] Table 3: Features for Astronomer Edwin Hubble was born in Marshfield, Missouri." ></td>
	<td class="line x" title="97:184	Astronomer Edwin Hubble was born in Marshfield , Missouri lex-mod s pred mod pcomp-n lex-mod inside Figure 1: Dependency parse with dependency path from Edwin Hubble to Marshfield highlighted in boldface." ></td>
	<td class="line x" title="98:184	5.2 Syntactic features In addition to lexical features we extract a number of features based on syntax." ></td>
	<td class="line x" title="99:184	In order to generate these features we parse each sentence with the broad-coverage dependency parser MINIPAR (Lin, 1998)." ></td>
	<td class="line x" title="100:184	A dependency parse consists of a set of words and chunks (e.g. Edwin Hubble, Missouri, born), linked by directional dependencies (e.g. pred, lex-mod), as in Figure 1." ></td>
	<td class="line x" title="101:184	For each sentence we extract a dependency path between each pair of entities." ></td>
	<td class="line x" title="102:184	A dependency path consists of a series of dependencies, directions and words/chunks representing a traversal of the parse." ></td>
	<td class="line x" title="103:184	Part-of-speech tags are not included in the dependency path." ></td>
	<td class="line x" title="104:184	Our syntactic features are similar to those used in Snow et al.(2005)." ></td>
	<td class="line x" title="106:184	They consist of the conjunction of:  A dependency path between the two entities  For each entity, one window node that is not part of the dependency path A window node is a node connected to one of the two entities and not part of the dependency path." ></td>
	<td class="line x" title="107:184	We generate one conjunctive feature for each pair of left and right window nodes, as well as features which omit one or both of them." ></td>
	<td class="line x" title="108:184	Thus each syntactic row in Table 3 represents a single syntactic feature." ></td>
	<td class="line x" title="109:184	5.3 Named entity tag features Every feature contains, in addition to the content described above, named entity tags for the two entities." ></td>
	<td class="line oc" title="110:184	We perform named entity tagging using the Stanford four-class named entity tagger (Finkel et al., 2005)." ></td>
	<td class="line o" title="111:184	The tagger provides each word with a label from {person, location, organization, miscellaneous, none}." ></td>
	<td class="line x" title="112:184	5.4 Feature conjunction Rather than use each of the above features in the classifier independently, we use only conjunctive features." ></td>
	<td class="line x" title="113:184	Each feature consists of the conjunction of several attributes of the sentence, plus the named entity tags." ></td>
	<td class="line x" title="114:184	For two features to match, all of their conjuncts must match exactly." ></td>
	<td class="line x" title="115:184	This yields low-recall but high-precision features." ></td>
	<td class="line x" title="116:184	With a small amount of data, this approach would be problematic, since most features would only be seen once, rendering them useless to the classifier." ></td>
	<td class="line x" title="117:184	Since we use large amounts of data, even complex features appear multiple times, allowing our highprecision features to work as intended." ></td>
	<td class="line x" title="118:184	Features for a sample sentence are shown in Table 3." ></td>
	<td class="line x" title="119:184	6 Implementation 6.1 Text For unstructured text we use the Freebase Wikipedia Extraction, a dump of the full text of all Wikipedia articles (not including discussion and 1007 Relation Feature type Left window NE1 Middle NE2 Right window /architecture/structure/architect LEXarchleftdown ORG , the designer of the PER SYN designed s ORG s designed bysubj by pcn PER s designed /book/author/works written LEX PER s novel ORG SYN PER pcn by mod story pred is s ORG /book/book edition/author editor LEXarchleftdown ORG s novel PER SYN PER nn series gen PER /business/company/founders LEX ORG co founder PER SYN ORG nn owner person PER /business/company/place founded LEXarchleftdown ORG based LOC SYN ORG s founded mod in pcn LOC /film/film/country LEX PER , released in LOC SYN opened s ORG s opened mod in pcn LOC s opened /geography/river/mouth LEX LOC , which flows into the LOC SYN the det LOC s is pred tributary mod of pcn LOC det the /government/political party/country LEXarchleftdown ORG politician of the LOC SYN candidate nn ORG nn candidate mod for pcn LOC nn candidate /influence/influence node/influenced LEXarchleftdown PER , a student of PER SYN of pcn PER pcn of mod student appo PER pcn of /language/human language/region LEX LOC speaking areas of LOC SYN LOC lexmod speaking areas mod of pcn LOC /music/artist/origin LEXarchleftdown ORG based band LOC SYN is s ORG s is pred band mod from pcn LOC s is /people/deceased person/place of death LEX PER died in LOC SYN hanged s PER s hanged mod in pcn LOC s hanged /people/person/nationality LEX PER is a citizen of LOC SYN PER mod from pcn LOC /people/person/parents LEX PER , son of PER SYN father gen PER gen father person PER gen father /people/person/place of birth LEXarchleftdown PER is the birthplace of PER SYN PER s born mod in pcn LOC /people/person/religion LEX PER embraced LOC SYN convert appo PER appo convert mod to pcn LOC appo convert Table 4: Examples of high-weight features for several relations." ></td>
	<td class="line x" title="120:184	Key: SYN = syntactic feature; LEX = lexical feature;archleftdown= reversed; NE# = named entity tag of entity." ></td>
	<td class="line x" title="121:184	user pages) which has been sentence-tokenized by Metaweb Technologies, the developers of Freebase (Metaweb, 2008)." ></td>
	<td class="line x" title="122:184	This dump consists of approximately 1.8 million articles, with an average of 14.3 sentences per article." ></td>
	<td class="line x" title="123:184	The total number of words (counting punctuation marks) is 601,600,703." ></td>
	<td class="line x" title="124:184	For our experiments we use about half of the articles: 800,000 for training and 400,000 for testing." ></td>
	<td class="line x" title="125:184	We use Wikipedia because it is relatively upto-date, and because its sentences tend to make explicit many facts that might be omitted in newswire." ></td>
	<td class="line x" title="126:184	Much of the information in Freebase is derived from tabular data from Wikipedia, meaning that Freebase relations are more likely to appear in sentences in Wikipedia." ></td>
	<td class="line x" title="127:184	6.2 Parsing and chunking Each sentence of this unstructured text is dependency parsed by MINIPAR to produce a dependency graph." ></td>
	<td class="line x" title="128:184	In preprocessing, consecutive words with the same named entity tag are chunked, so that Edwin/PERSON Hubble/PERSON becomes [Edwin Hubble]/PERSON." ></td>
	<td class="line x" title="129:184	This chunking is restricted by the dependency parse of the sentence, however, in that chunks must be contiguous in the parse (i.e., no chunks across subtrees)." ></td>
	<td class="line x" title="130:184	This ensures that parse tree structure is preserved, since the parses must be updated to reflect the chunking." ></td>
	<td class="line x" title="131:184	6.3 Training and testing For held-out evaluation experiments (see section 7.1), half of the instances of each relation are not used in training, and are later used to compare against newly discovered instances." ></td>
	<td class="line x" title="132:184	This means that 900,000 Freebase relation instances are used in training, and 900,000 are held out." ></td>
	<td class="line x" title="133:184	These experiments used 800,000 Wikipedia articles in the training phase and 400,000 different articles in the testing phase." ></td>
	<td class="line x" title="134:184	For human evaluation experiments, all 1.8 million relation instances are used in training." ></td>
	<td class="line x" title="135:184	Again, we use 800,000 Wikipedia articles in the training phase and 400,000 different articles in the testing phase." ></td>
	<td class="line x" title="136:184	For all our experiments, we only extract relation instances that do not appear in our training data, i.e., instances that are not already in Freebase." ></td>
	<td class="line x" title="137:184	Our system needs negative training data for the purposes of constructing the classifier." ></td>
	<td class="line x" title="138:184	Towards this end, we build a feature vector in the training phase for an unrelated relation by randomly selecting entity pairs that do not appear in any Freebase relation and extracting features for them." ></td>
	<td class="line x" title="139:184	While it is possible that some of these entity pairs 1008 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Pr ecision Oraclerecall Both Syntax Surface Figure 2: Automatic evaluation with 50% of Freebase relation data held out and 50% used in training on the 102 largest relations we use." ></td>
	<td class="line x" title="140:184	Precision for three different feature sets (lexical features, syntactic features, and both) is reported at recall levels from 10 to 100,000." ></td>
	<td class="line x" title="141:184	At the 100,000 recall level, we classify most of the instances into three relations: 60% as location-contains, 13% as person-place-of-birth, and 10% as person-nationality." ></td>
	<td class="line x" title="142:184	are in fact related but are wrongly omitted from the Freebase data, we expect that on average these false negatives will have a small effect on the performance of the classifier." ></td>
	<td class="line x" title="143:184	For performance reasons, we randomly sample 1% of such entity pairs for use as negative training examples." ></td>
	<td class="line x" title="144:184	By contrast, in the actual test data, 98.7% of the entity pairs we extract do not possess any of the top 102 relations we consider in Freebase." ></td>
	<td class="line x" title="145:184	We use a multi-class logistic classifier optimized using L-BFGS with Gaussian regularization." ></td>
	<td class="line x" title="146:184	Our classifier takes as input an entity pair and a feature vector, and returns a relation name and a confidence score based on the probability of the entity pair belonging to that relation." ></td>
	<td class="line x" title="147:184	Once all of the entity pairs discovered during testing have been classified, they can be ranked by confidence score and used to generate a list of the n most likely new relation instances." ></td>
	<td class="line x" title="148:184	Table 4 shows some high-weight features learned by our system." ></td>
	<td class="line x" title="149:184	We discuss the results in the next section." ></td>
	<td class="line x" title="150:184	7 Evaluation We evaluate labels in two ways: automatically, by holding out part of the Freebase relation data during training, and comparing newly discovered relation instances against this held-out data, and manually, having humans who look at each positively labeled entity pair and mark whether the relation indeed holds between the participants." ></td>
	<td class="line x" title="151:184	Both evaluations allow us to calculate the precision of the system for the best N instances." ></td>
	<td class="line x" title="152:184	7.1 Held-out evaluation Figure 2 shows the performance of our classifier on held-out Freebase relation data." ></td>
	<td class="line x" title="153:184	While held-out evaluation suffers from false negatives, it gives a rough measure of precision without requiring expensive human evaluation, making it useful for parameter setting." ></td>
	<td class="line x" title="154:184	At most recall levels, the combination of syntactic and lexical features offers a substantial improvement in precision over either of these feature sets on its own." ></td>
	<td class="line x" title="155:184	7.2 Human evaluation Human evaluation was performed by evaluators on Amazons Mechanical Turk service, shown to be effective for natural language annotation in Snow et al.(2008)." ></td>
	<td class="line x" title="157:184	We ran three experiments: one using only syntactic features; one using only lexical features; and one using both syntactic and lexical features." ></td>
	<td class="line x" title="158:184	For each of the 10 relations that appeared most frequently in our test data (according to our classifier), we took samples from the first 100 and 1000 instances of this relation generated in each experiment, and sent these to Mechanical Turk for 1009 Relation name 100 instances 1000 instancesSyn Lex Both Syn Lex Both /film/director/film 0.49 0.43 0.44 0.49 0.41 0.46 /film/writer/film 0.70 0.60 0.65 0.71 0.61 0.69 /geography/river/basin countries 0.65 0.64 0.67 0.73 0.71 0.64 /location/country/administrative divisions 0.68 0.59 0.70 0.72 0.68 0.72 /location/location/contains 0.81 0.89 0.84 0.85 0.83 0.84 /location/us county/county seat 0.51 0.51 0.53 0.47 0.57 0.42 /music/artist/origin 0.64 0.66 0.71 0.61 0.63 0.60 /people/deceased person/place of death 0.80 0.79 0.81 0.80 0.81 0.78 /people/person/nationality 0.61 0.70 0.72 0.56 0.61 0.63 /people/person/place of birth 0.78 0.77 0.78 0.88 0.85 0.91 Average 0.67 0.66 0.69 0.68 0.67 0.67 Table 5: Estimated precision on human-evaluation experiments of the highest-ranked 100 and 1000 results per relation, using stratified samples." ></td>
	<td class="line x" title="159:184	Average gives the mean precision of the 10 relations." ></td>
	<td class="line x" title="160:184	Key: Syn = syntactic features only." ></td>
	<td class="line x" title="161:184	Lex = lexical features only." ></td>
	<td class="line x" title="162:184	We use stratified samples because of the overabundance of location-contains instances among our high-confidence results." ></td>
	<td class="line x" title="163:184	human evaluation." ></td>
	<td class="line x" title="164:184	Our sample size was 100." ></td>
	<td class="line x" title="165:184	Each predicted relation instance was labeled as true or false by between 1 and 3 labelers on Mechanical Turk." ></td>
	<td class="line x" title="166:184	We assigned the truth or falsehood of each relation according to the majority vote of the labels; in the case of a tie (one vote each way) we assigned the relation as true or false with equal probability." ></td>
	<td class="line x" title="167:184	The evaluation of the syntactic, lexical, and combination of features at a recall of 100 and 1000 instances is presented in Table 5." ></td>
	<td class="line x" title="168:184	At a recall of 100 instances, the combination of lexical and syntactic features has the best performance for a majority of the relations, while at a recall level of 1000 instances the results are mixed." ></td>
	<td class="line x" title="169:184	No feature set strongly outperforms any of the others across all relations." ></td>
	<td class="line x" title="170:184	8 Discussion Our results show that the distant supervision algorithm is able to extract high-precision patterns for a reasonably large number of relations." ></td>
	<td class="line x" title="171:184	The held-out results in Figure 2 suggest that the combination of syntactic and lexical features provides better performance than either feature set on its own." ></td>
	<td class="line x" title="172:184	In order to understand the role of syntactic features, we examine Table 5, the human evaluation of the most frequent 10 relations." ></td>
	<td class="line x" title="173:184	For the topranking 100 instances of each relation, most of the best results use syntactic features, either alone or in combination with lexical features." ></td>
	<td class="line x" title="174:184	For the topranking 1000 instances of each relation, the results are more mixed, but syntactic features still helped in most classifications." ></td>
	<td class="line x" title="175:184	We then examine those relations for which syntactic features seem to help." ></td>
	<td class="line x" title="176:184	For example, syntactic features consistently outperform lexical features for the director-film and writer-film relations." ></td>
	<td class="line x" title="177:184	As discussed in section 4, these two relations are particularly ambiguous, suggesting that syntactic features may help tease apart difficult relations." ></td>
	<td class="line x" title="178:184	Perhaps more telling, we noticed many examples with a long string of words between the director and the film: Back Street is a 1932 film made by Universal Pictures, directed by John M. Stahl, and produced by Carl Laemmle Jr. Sentences like this have very long (and thus rare) lexical features, but relatively short dependency paths." ></td>
	<td class="line x" title="179:184	Syntactic features can more easily abstract from the syntactic modifiers that comprise the extraneous parts of these strings." ></td>
	<td class="line x" title="180:184	Our results thus suggest that syntactic features are indeed useful in distantly supervised information extraction, and that the benefit of syntax occurs in cases where the individual patterns are particularly ambiguous, and where they are nearby in the dependency structure but distant in terms of words." ></td>
	<td class="line x" title="181:184	It remains for future work to see whether simpler, chunk-based syntactic features might be able to capture enough of this gain without the overhead of full parsing, and whether coreference resolution could improve performance." ></td>
	<td class="line x" title="182:184	Acknowledgments We would like to acknowledge Sarah Spikes for her help in developing the relation extraction system, Christopher Manning and Mihai Surdeanu for their invaluable advice, and Fuliang Weng and Baoshi Yan for their guidance." ></td>
	<td class="line x" title="183:184	Our research was partially funded by the NSF via award IIS0811974 and by Robert Bosch LLC." ></td>
	<td class="line x" title="184:184	1010" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-2041
Automatic Satire Detection: Are You Having a Laugh?
Burfoot, Clint;Baldwin, Timothy;"></td>
	<td class="line x" title="1:79	Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 161164, Suntec, Singapore, 4 August 2009." ></td>
	<td class="line x" title="2:79	c 2009 ACL and AFNLP Automatic Satire Detection: Are You Having a Laugh?" ></td>
	<td class="line x" title="3:79	Clint Burfoot CSSE University of Melbourne VIC 3010 Australia cburfoot@csse.unimelb.edu.au Timothy Baldwin CSSE University of Melbourne VIC 3010 Australia tim@csse.unimelb.edu.au Abstract We introduce the novel task of determining whether a newswire article is true or satirical." ></td>
	<td class="line x" title="4:79	We experiment with SVMs, feature scaling, and a number of lexical and semantic feature types, and achieve promising results over the task." ></td>
	<td class="line x" title="5:79	1 Introduction Thispaperdescribesamethodforlteringsatirical news articles from true newswire documents." ></td>
	<td class="line x" title="6:79	We dene a satirical article as one which deliberately exposes real-world individuals, organisations and events to ridicule." ></td>
	<td class="line x" title="7:79	Satirical news articles tend to mimic true newswire articles, incorporating irony and non sequitur in an attempt to provide humorous insight." ></td>
	<td class="line x" title="8:79	An example excerpt is: Bank Of England Governor Mervyn King is a Queen, Says Fed Chairman Ben Bernanke During last nights appearance on the American David Letterman Show, Fed Chairman Ben Bernanke let slip that Bank of England (BOE) Governor, Mervyn King, enjoys wearing womens clothing." ></td>
	<td class="line x" title="9:79	Contrast this with a snippet of a true newswire article: Delegates prepare for Cairo conference amid tight security Delegates from 156 countries began preparatory talks here Saturday ahead of the ofcial opening of the UN World Population Conference amid tight security." ></td>
	<td class="line x" title="10:79	The basis for our claim that the rst document is satiricalissurprisinglysubtleinnature, andrelates to the absurdity of the suggestion that a prominent gure would expose another prominent gure as a cross dresser, the implausibility of this story appearing in a reputable news source, and the pun on the name (King being a Queen)." ></td>
	<td class="line x" title="11:79	Satire classication is a novel task to computational linguistics." ></td>
	<td class="line x" title="12:79	It is somewhat similar to the more widely-researched text classication tasks of spam ltering (Androutsopoulos et al., 2000) and sentiment classication (Pang and Lee, 2008), in that: (a) it is a binary classication task, and (b) it is an intrinsically semantic task, i.e. satire news articles are recognisable as such through interpretation and cross-comparison to world knowledge about the entities involved." ></td>
	<td class="line x" title="13:79	Similarly to spam ltering and sentiment classication, a key question asked in this research is whether it is possible to perform the task on the basis of simple lexical features of various types." ></td>
	<td class="line x" title="14:79	That is, is it possible to automatically detect satire without access to the complex inferencing and real-world knowledge that humans make use of." ></td>
	<td class="line x" title="15:79	Theprimarycontributionsofthisresearchareas follows: (1) we introduce a novel task to the arena ofcomputationallinguisticsandmachinelearning, and make available a standardised dataset for research on satire detection; and (2) we develop a method which is adept at identifying satire based on simple bag-of-words features, and further extend it to include richer features." ></td>
	<td class="line x" title="16:79	2 Corpus Our satire corpus consists of a total of 4000 newswire documents and 233 satire news articles, split into xed training and test sets as detailed in Table 1." ></td>
	<td class="line x" title="17:79	The newswire documents were randomly sampled from the English Gigaword Corpus." ></td>
	<td class="line x" title="18:79	The satire documents were selected to relate closely to at least one of the newswire documents by: (1) randomly selecting a newswire document; (2) hand-picking a key individual, institution or event from the selected document, and using it to formulate a phrasal query (e.g. Bill Clinton); (3) using the query to issue a site-restricted query to the 161 Training Test Total TRUE 2505 1495 4000 SATIRE 133 100 233 Table 1: Corpus statistics Google search engine;1 and (4) manually ltering out non-newsy, irrelevant and overly-offensive documents from the top-10 returned documents (i.e. documents not containing satire news articles, or containing satire articles which were not relevant to the original query)." ></td>
	<td class="line x" title="19:79	All newswire and satire documents were then converted to plain text of consistent format using lynx, and all content other than the title and body of the article was manually removed (including web page menus, andheaderandfooterdata)." ></td>
	<td class="line x" title="20:79	Finally,alldocuments were manually post-edited to remove references to the source (e.g. AP or Onion), formatting quirks specic to a particular source (e.g. all caps in the title), and any textual metadata which was indicative of the document source (e.g. editorial notes, dates and locations)." ></td>
	<td class="line x" title="21:79	This was all in an effort to prevent classiers from accessing supercial features which are reliable indicators of the document sourceandhencetrivialisethesatiredetectionprocess." ></td>
	<td class="line x" title="22:79	It is important to note that the number of satirical news articles in the corpus is signicantly less than the number of true newswire articles." ></td>
	<td class="line x" title="23:79	This reects an impressionistic view of the web: there is far more true news content than satirical news content." ></td>
	<td class="line x" title="24:79	The corpus is novel to this research, and is publicly available for download at http://www.csse.unimelb.edu.au/ research/lt/resources/satire/." ></td>
	<td class="line x" title="25:79	3 Method 3.1 Standard text classication approach We take our starting point from topic-based text classication (Dumais et al., 1998; Joachims, 1998) and sentiment classication (Turney, 2002; Pang and Lee, 2008)." ></td>
	<td class="line x" title="26:79	State-of-the-art results in both elds have been achieved using support vec1The sites queried were satirewire.com, theonion.com, newsgroper.com, thespoof." ></td>
	<td class="line x" title="27:79	com, brokennewz.com, thetoque.com, bbspot.com, neowhig.org, humorfeed.com, satiricalmuslim.com, yunews.com, newsbiscuit.com." ></td>
	<td class="line x" title="28:79	tor machines (SVMs) and bag-of-words features." ></td>
	<td class="line x" title="29:79	We supplement the bag-of-words model with feature weighting, using the two methods described below." ></td>
	<td class="line x" title="30:79	Binary feature weights: Under this scheme all features are given the same weight, regardless of how many times they appear in each article." ></td>
	<td class="line x" title="31:79	The topic and sentiment classication examples cited found binary features gave better performance than other alternatives." ></td>
	<td class="line x" title="32:79	Bi-normal separation feature scaling: BNS (Forman, 2008) has been shown to outperform other established feature representation schemes on a wide range of text classication tasks." ></td>
	<td class="line x" title="33:79	This superiority is especially pronounced for collections with a low proportion of positive class instances." ></td>
	<td class="line x" title="34:79	Under BNS, features are allocated a weight according to the formula: |F1(tpr)F1(fpr)| where F1 is the inverse normal cumulative distribution function, tpr is the true positive rate (P(feature|positive class)) and fpr is the false positive rate (P(feature|negative class))." ></td>
	<td class="line x" title="35:79	BNS produces the highest weights for features that are strongly correlated with either the negative or positive class." ></td>
	<td class="line x" title="36:79	Features that occur evenly across the training instances are given the lowest weight." ></td>
	<td class="line x" title="37:79	This behaviour is particularly helpful for features that correlate with the negative class in a negatively-skewed classication task, so in our caseBNSshouldassisttheclassierinmakinguse of features that identify true articles." ></td>
	<td class="line x" title="38:79	SVM classication is performed with SVMlight (Joachims, 1999) using a linear kernel and the default parameter settings." ></td>
	<td class="line x" title="39:79	Tokens are case folded; currency amounts (e.g. $2.50), abbreviations (e.g. U.S.A.), and punctuation sequences (e.g. a comma, or a closing quote mark followed by a period) are treated as separate features." ></td>
	<td class="line x" title="40:79	3.2 Targeted lexical features This section describe three types of features intended to embody characteristics of satire news documents." ></td>
	<td class="line x" title="41:79	Headline features: Most of the articles in the corpus have a headline as their rst line." ></td>
	<td class="line x" title="42:79	To a human reader, the vast majority of the satire documents in our corpus are immediately recognisable as such from the headline alone, suggesting that ourclassiersmaygetsomethingoutofhavingthe 162 headline contents explicitly identied in the feature vector." ></td>
	<td class="line x" title="43:79	To this end, we add an additional feature for each unigram appearing on the rst line of an article." ></td>
	<td class="line x" title="44:79	In this way the heading tokens are represented twice: once in the overall set of unigrams in the article, and once in the set of heading unigrams." ></td>
	<td class="line x" title="45:79	Profanity: true news articles very occasionally include a verbal quote which contains offensive language, but in practically all other cases it is incumbent on journalists and editors to keep their language clean." ></td>
	<td class="line x" title="46:79	A review of the corpus shows that this is not the case with satirical news, which occasionally uses profanity as a humorous device." ></td>
	<td class="line x" title="47:79	Let P be a binary feature indicating whether or not an article contains profanity, as determined by the Regexp::Common::profanity Perl module.2 Slang: As with profanity, it is intuitively true that true news articles tend to avoid slang." ></td>
	<td class="line x" title="48:79	An impressionistic review of the corpus suggests that informallanguageismuchmorecommontosatirical articles." ></td>
	<td class="line x" title="49:79	We measure the informality of an article as: i def= 1|T| tT s(t) where T is the set of unigram tokens in the article and s is a function taking the value 1 if the token has a dictionary denition marked as slang and 0 if it does not." ></td>
	<td class="line x" title="50:79	It is important to note that this measure of informality is approximate at best." ></td>
	<td class="line x" title="51:79	We do not attempt, e.g., to disambiguate the sense of individual word terms to tell whether the slang sense of a word is the one intended." ></td>
	<td class="line x" title="52:79	Rather, we simply checktoseeifeachwordhasaslangusageinWiktionary.3 A continuous feature is set to the value of i for each article." ></td>
	<td class="line x" title="53:79	Discrete features highi and lowi are set as: highi def= { 1 v >i + 2; 0 lowi def= { 1 v <i2; 0 wherei and  are, respectively, the mean and standard deviation of i across all articles." ></td>
	<td class="line x" title="54:79	2http://search.cpan.org/perldoc?" ></td>
	<td class="line x" title="55:79	Regexp::Common::profanity 3http://www.wiktionary.org 3.3 Semantic validity Lexical approaches are clearly inadequate if we assume that good satirical news articles tend to emulate real news in tone, style, and content." ></td>
	<td class="line x" title="56:79	What is needed is an approach that captures the document semantics." ></td>
	<td class="line x" title="57:79	One common device in satire news articles is absurdity, in terms of describing well-known individuals in unfamiliar settings which parody their viewpoints or public prole." ></td>
	<td class="line x" title="58:79	We attempt to capture this via validity, in the form of the relative frequencyoftheparticularcombinationofkeyparticipants reported in the story." ></td>
	<td class="line x" title="59:79	Our method identies thenamedentitiesinagivendocumentandqueries the web for the conjunction of those entities." ></td>
	<td class="line x" title="60:79	Our expectation is that true news stories will have been reported in various forums, and hence the number of web documents which include the same combination of entities will be higher than with satire documents." ></td>
	<td class="line oc" title="61:79	To implement this method, we rst use the Stanford Named Entity Recognizer4 (Finkel et al., 2005)toidentifythesetofpersonandorganisation entities, E, from each article in the corpus." ></td>
	<td class="line x" title="62:79	From this, we estimate the validity of the combination of entities in the article as: v(E) def= |g(E)| where g is the set of matching documents returned by Google using a conjunctive query." ></td>
	<td class="line x" title="63:79	We anticipate that v will have two potentially useful properties: (1) it will be relatively lower when E includes made-up entity names such as Hitler Commemoration Institute, found in one satirical corpus article; and (2) it will be relatively lower when E contains unusual combinations of entities such as, forexample, thoseinthesatiricalarticlebeginning Missing Brazilian balloonist Padre spotted straddling Pink Floyd ying pig." ></td>
	<td class="line x" title="64:79	We include both a continuous representation of v for each article, in the form of log(v(E)), and discrete variants of the feature, based on the same methodology as for highi and lowi." ></td>
	<td class="line x" title="65:79	4 Results The results for our classiers over the satire corpus are shown in Table 2." ></td>
	<td class="line x" title="66:79	The baseline is a naive classier that assigns all instances to the positive 4http://nlp.stanford.edu/software/ CRF-NER.shtml 163 (articleSATIRE?) P R F all-positive baseline 0.063 1.000 0.118 BIN 0.943 0.500 0.654 BIN+lex 0.945 0.520 0.671 BIN+val 0.943 0.500 0.654 BIN+all 0.945 0.520 0.671 BNS 0.944 0.670 0.784 BNS+lex 0.957 0.660 0.781 BNS+val 0.945 0.690 0.798 BNS+all 0.958 0.680 0.795 Table 2: Results for satire detection (P = precision, R = recall, and F = F-score) for binary unigram features (BIN) and BNS unigram features (BNS),optionallyusinglexical(lex), validity(val) or combined lexical and validity (all) features class(i.e. SATIRE)." ></td>
	<td class="line x" title="67:79	AnSVMclassierwithsimple binary unigram word features provides a standard text classication benchmark." ></td>
	<td class="line x" title="68:79	All of the classiers easily outperform the baseline." ></td>
	<td class="line x" title="69:79	This is to be expected given the low proportion of positive instances in the corpus." ></td>
	<td class="line x" title="70:79	The benchmark classier has very good precision, but recall of only 0.500." ></td>
	<td class="line x" title="71:79	Adding the heading, slang, and profanity features provides a small improvement in both precision and recall." ></td>
	<td class="line x" title="72:79	Moving to BNS feature scaling keeps the very high precision and increases the recall to 0.670." ></td>
	<td class="line x" title="73:79	Adding in the heading, slang and profanity lexical features (+lex) actually decreases the F-score slightly, but adding the validity features (+val) provides a near 2 point F-score increase, resulting in the best overall F-score of 0.798." ></td>
	<td class="line x" title="74:79	All of the BNS scores achieve statistically signicant improvements over the benchmark in terms of F-score (using approximate randomisation, p < 0.05)." ></td>
	<td class="line x" title="75:79	The 1-2% gains given by adding inthevariousfeaturetypesarenotstatisticallysignicant due to the small number of satire instances concerned." ></td>
	<td class="line x" title="76:79	All of the classiers achieve very high precision and considerably lower recall." ></td>
	<td class="line x" title="77:79	Error analysis suggests that the reason for the lower recall is subtler satire articles, which require detailed knowledge of the individuals to be fully appreciated as satire." ></td>
	<td class="line x" title="78:79	While they are not perfect, however, the classiers achieve remarkably high performance given the superciality of the features used." ></td>
	<td class="line x" title="79:79	5 Conclusions and future work Thispaperhasintroducedanoveltasktocomputational linguistics and machine learning: determining whether a newswire article is true or satirical. We found that the combination of SVMs with BNS feature scaling achieves high precision and lower recall, and that the inclusion of the notion of validity achieves the best overall F-score." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-0422
English-Czech MT in 2008
Bojar, Ondej;Mareek, David;Novk, Vclav;Popel, Martin;Ptek, Jan;Rou, Jan;abokrtsk, Zdenk;"></td>
	<td class="line x" title="1:101	Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 125129, Athens, Greece, 30 March  31 March 2009." ></td>
	<td class="line x" title="2:101	c2009 Association for Computational Linguistics English-Czech MT in 2008  Ondrej Bojar, David Marecek, Vaclav Novak, Martin Popel, Jan Ptacek, Jan Rous, Zdenek Zabokrtsky Charles University, Institute of Formal and Applied Linguistics Malostranske nam." ></td>
	<td class="line x" title="3:101	25, Praha 1, CZ-118 00, Czech Republic {bojar,marecek,novak,ptacek,zabokrtsky}@ufal.mff.cuni.cz {popel,jan.rous}@matfyz.cz Abstract We describe two systems for English-toCzech machine translation that took part in the WMT09 translation task." ></td>
	<td class="line x" title="4:101	One of the systems is a tuned phrase-based system and the other one is based on a linguistically motivated analysis-transfer-synthesis approach." ></td>
	<td class="line x" title="5:101	1 Introduction We participated in WMT09 with two very different systems: (1) a phrase-based MT based on Moses (Koehn et al., 2007) and tuned for EnglishCzech translation, and (2) a complex system in the TectoMT platform ( Zabokrtsky et al., 2008)." ></td>
	<td class="line x" title="6:101	2 Data 2.1 Monolingual Data Our Czech monolingual data consist of (1) the Czech National Corpus (CNC, versions SYN200[056], 72.6%, Kocek et al.(2000)), (2) a collection of web pages downloaded by Pavel Pecina (Web, 17.1%), and (3) the Czech monolingual data provided by WMT09 organizers (10.3%)." ></td>
	<td class="line x" title="8:101	Table 1 lists sentence and token counts (see Section 2.3 for the explanation of aand tlayer)." ></td>
	<td class="line x" title="9:101	Sentences 52 M with nonempty t-layer 51 M a-nodes (i.e. tokens) 0.9 G t-nodes 0.6 G Table 1: Czech monolingual training data." ></td>
	<td class="line x" title="10:101	 The work on this project was supported by the grants MSM0021620838, 1ET201120505, 1ET101120503, GAUK 52408/2008, MSMT CR LC536 and FP6-IST-5-034291-STP (EuroMatrix)." ></td>
	<td class="line x" title="11:101	2.2 Parallel Data As the source of parallel data we use an internal release of Czech-English parallel corpus CzEng (Bojar et al., 2008) extended with some additional texts." ></td>
	<td class="line x" title="12:101	One of the added sections was gathered from two major websites containing Czech subtitles to movies and TV series1." ></td>
	<td class="line x" title="13:101	The matching of the Czech and English movies is rather straightforward thanks to the naming conventions." ></td>
	<td class="line x" title="14:101	However, we were unable to reliably determine the series number and the episode number from the file names." ></td>
	<td class="line x" title="15:101	We employed a two-step procedure to automatically pair the TV series subtitle files." ></td>
	<td class="line x" title="16:101	For every TV series: 1." ></td>
	<td class="line x" title="17:101	We clustered the files on both sides to remove duplicates 2." ></td>
	<td class="line x" title="18:101	We found the best matching using a provisional translation dictionary." ></td>
	<td class="line x" title="19:101	This proved to be a successful technique on a small sample of manually paired test data." ></td>
	<td class="line x" title="20:101	The process was facilitated by the fact that the correct pairs of episodes usually share some named entities which the human translator chose to keep in the original English form." ></td>
	<td class="line x" title="21:101	Table 2 lists parallel corpus sizes and the distribution of text domains." ></td>
	<td class="line x" title="22:101	English Czech Sentences 6.91 M with nonempty t-layer 6.89 M a-nodes (i.e. tokens) 61 M 50 M t-nodes 41 M 33 M Distribution: [%] [%] Subtitles 68.2 Novels 3.3 Software Docs 17.0 Commentaries/News 1.5 EU (Legal) Texts 9.5 Volunteer-supplied 0.4 Table 2: Czech-English data sizes and sources." ></td>
	<td class="line x" title="23:101	1www.opensubtitles.organd titulky.com 125 2.3 Data Preprocessing using TectoMT platform: Analysis and Alignment As we believe that various kinds of linguistically relevant information might be helpful in MT, we performed automatic analysis of the data." ></td>
	<td class="line x" title="24:101	The data were analyzed using the layered annotation scheme of the Prague Dependency Treebank 2.0 (PDT 2.0, Hajic and others (2006)), i.e. we used three layers of sentence representation: morphological layer, surface-syntax layer (called analytical (a-) layer), and deep-syntax layer (called tectogrammatical (t-) layer)." ></td>
	<td class="line x" title="25:101	The analysis was implemented using TectoMT, ( Zabokrtsky et al., 2008)." ></td>
	<td class="line x" title="26:101	TectoMT is a highly modular software framework aimed at creating MT systems (focused, but by far not limited to translation using tectogrammatical transfer) and other NLP applications." ></td>
	<td class="line x" title="27:101	Numerous existing NLP tools such as taggers, parsers, and named entity recognizers are already integrated in TectoMT, especially for (but again, not limited to) English and Czech." ></td>
	<td class="line x" title="28:101	During the analysis of the large Czech monolingual data, we used Jan Hajics Czech tagger shipped with PDT 2.0, Maximum Spanning Tree parser (McDonald et al., 2005) with optimized set of features as described in Novak and Zabokrtsky (2007), and a tool for assigning functors (semantic roles) from Klimes (2006), and numerous other components of our own (e.g. for conversion of analytical trees into tectogrammatical ones)." ></td>
	<td class="line x" title="29:101	In the parallel data, we analyzed the Czech side using more or less the same scenario as used for the monolingual data." ></td>
	<td class="line x" title="30:101	English sentences were analyzed using (among other tools) Morce tagger Spoustova et al.(2007) and Maximum Spanning Tree parser.2 The resulting deep syntactic (tectogrammatical) Czech and English trees are then aligned using Talignera feature based greedy algorithm implemented for this purpose (Marecek et al., 2008)." ></td>
	<td class="line x" title="32:101	Taligner finds corresponding nodes between the two given trees and links them." ></td>
	<td class="line x" title="33:101	For deciding whether to link two nodes or not, T-aligner makes use of a bilingual lexicon of tectogrammatical lemmas, morphosyntactic similarities between the two candidate nodes, their positions in the trees and other similarities between their parent/child nodes." ></td>
	<td class="line x" title="34:101	It 2In some previous experiments (e.g. Zabokrtsky et al.(2008)), we used phrase-structure parser Collins (1999) with subsequent constituency-dependency conversion." ></td>
	<td class="line x" title="36:101	also uses word alignment generated from surface shapes of sentences by GIZA++ tool, Och and Ney (2003)." ></td>
	<td class="line x" title="37:101	We use acquired aligned tectogrammatical trees for training some models for the transfer." ></td>
	<td class="line x" title="38:101	As analysis of such amounts of data is obviously computationally very demanding, we run it in parallel using Sun Grid Engine3 cluster of 40 4-CPU computers." ></td>
	<td class="line x" title="39:101	For this purpose, we implemented a rather generic tool that submits any TectoMT pipeline to the cluster." ></td>
	<td class="line x" title="40:101	3 Factored Phrase-Based MT We essentially repeat our experiments from last year (Bojar and Hajic, 2008): GIZA++ alignments4 on a-layer lemmas (a-layer nodes correspond 1-1 to surface tokens), symmetrized using grow-diag-final (no -and) heuristic5." ></td>
	<td class="line x" title="41:101	Probably due to the domain difference (the test set is news), including Subtitles in the parallel data and Web in the monolingual data did not bring any improvement that would justify the additional performance costs." ></td>
	<td class="line x" title="42:101	For most of the phrase-based experiments, we thus used only 2.2M parallel sentences (27M Czech and 32M English tokens) and 43M Czech sentences (694 M tokens)." ></td>
	<td class="line x" title="43:101	In Table 3 below, we report the scores for the following setups selected from about 50 experiments we ran in total: Moses T is a simple phrase-based translation (T) with no additional factors." ></td>
	<td class="line x" title="44:101	The translation is performed on truecased word forms (i.e. sentence capitalization removed unless the first word seems to be a name)." ></td>
	<td class="line x" title="45:101	The 4-gram language model is based on the 43M sentences." ></td>
	<td class="line x" title="46:101	Moses T+C is a factored setup with form-to-form translation (T) and target-side morphological coherence check following Bojar and Hajic (2008)." ></td>
	<td class="line x" title="47:101	The setup uses two language models: 4-grams of word forms and 7-grams of morphological tags." ></td>
	<td class="line x" title="48:101	Moses T+C+C&T+T+G 84k is a setup desirable from the linguistic point of view." ></td>
	<td class="line x" title="49:101	Two independent translation paths are used: (1) formform translation with two target-side checks (lemma and tag generated from the target-side form) as a fine-grained baseline 3http://gridengine.sunsource.net/ 4Default settings, IBM models and iterations: 153343." ></td>
	<td class="line x" title="50:101	5Later, we found out that the grow-diag-final-and heuristic provides insignificantly superior results." ></td>
	<td class="line x" title="51:101	126 with the option to resort to (2) an independent translation of lemmalemma and tagtag finished by a generation step that combines target-side lemma and tag to produce the final target-side form." ></td>
	<td class="line x" title="52:101	We use three language models in this setup (3-grams of forms, 3-grams of lemmas, and 10-grams of tags)." ></td>
	<td class="line x" title="53:101	Due to the increased complexity of the setup, we were able to train this model on 84k parallel sentences only (the Commentaries section) and we use the target-side of this small training data for language models, too." ></td>
	<td class="line x" title="54:101	For all the setups we perform standard MERT training on the provided development set.6 4 Translation Setup Based on Tectogrammatical Transfer In this translation experiment, we follow the traditional analysis-transfer-synthesis approach, using the set of PDT 2.0 layers: we analyze the input English sentence up to the tectogrammatical layer (through the morphological and analytical ones), then perform the tectogrammatical transfer, and then synthesize the target Czech sentence from its tectogrammatical representation." ></td>
	<td class="line x" title="55:101	The whole procedure consists of about 80 steps, so the following description is necessarily very high level." ></td>
	<td class="line x" title="56:101	4.1 Analysis Each sentence is tokenized (roughly according to the Penn Treebank conventions), tagged by the English version of the Morce tagger Spoustova et al.(2007), and lemmatized by our lemmatizer." ></td>
	<td class="line x" title="58:101	Then the dependency parser (McDonald et al., 2005) is applied." ></td>
	<td class="line x" title="59:101	Then the analytical trees resulting from the parser are converted to the tectogrammatical ones (i.e. functional words are removed, only morphologically indispensable categories are left with the nodes using a sequence of heuristic procedures)." ></td>
	<td class="line x" title="60:101	Unlike in PDT 2.0, the information about the original syntactic form is stored with each tnode (values such as v:inf for an infinitive verb form, v:since+fin for the head of a subordinate clause of a certain type, adj:attr for an adjective in attribute position, n:for+X for a given prepositional group are distinguished)." ></td>
	<td class="line x" title="61:101	6We used the full development set of 2k sentences for Moses T and a subset of 1k sentences for the other two setups due to time constraints." ></td>
	<td class="line oc" title="62:101	One of the steps in the analysis of English is named entity recognition using Stanford Named Entity Recognizer (Finkel et al., 2005)." ></td>
	<td class="line x" title="63:101	The nodes in the English t-layer are grouped according to the detected named entities and they are assigned the type of entity (location, person, or organization)." ></td>
	<td class="line x" title="64:101	This information is preserved in the transfer of the deep English trees to the deep Czech trees to allow for the appropriate capitalization of the Czech translation." ></td>
	<td class="line x" title="65:101	4.2 Transfer The transfer phase consists of the following steps:  Initiate the target-side (Czech) t-trees simply by cloning the source-side (English) ttrees." ></td>
	<td class="line x" title="66:101	Subsequent steps usually iterate over all t-nodes." ></td>
	<td class="line x" title="67:101	In the following, we denote a source-side t-node as S and the corresponding target-side node as T.  Translate formemes using two probabilistic dictionaries (p(T.formeme|S.formeme, S.parent.lemma) and p(T.formeme|S.formeme)) and a few manual rules." ></td>
	<td class="line x" title="68:101	The formeme translation probability estimates were extracted from a part of the parallel data mentioned above." ></td>
	<td class="line x" title="69:101	 Translate lemmas using a probabilistic dictionary (p(T.lemma|S.lemma)) and a few rules that ensure compatibility with the previously chosen formeme." ></td>
	<td class="line x" title="70:101	Again, this probabilistic dictionary was obtained using the aligned tectogrammatical trees from the parallel corpus." ></td>
	<td class="line x" title="71:101	 Fill the grammatemes (deep-syntactic equivalent of morphological categories) gender (for denotative nouns) and aspect (for verbs) according to the chosen lemma." ></td>
	<td class="line x" title="72:101	We also fix grammateme values where the EnglishCzech grammateme correspondence is nontrivial (e.g. if an English gerund expression is translated to Czech as a subordinating clause, the tense grammateme has to be filled)." ></td>
	<td class="line x" title="73:101	However, the transfer of grammatemes is definitely much easier task than the transfer of formemes and lemmas." ></td>
	<td class="line x" title="74:101	4.3 Synthesis The transfer step yields an abstract deep syntactico-semantical tree structure." ></td>
	<td class="line x" title="75:101	Firstly, 127 we derive surface morphological categories from their deep counterparts taking care of their agreement where appropriate and we also remove personal pronouns in subject positions (because Czech is a pro-drop language)." ></td>
	<td class="line x" title="76:101	To arrive at the surface tree structure, auxiliary nodes of several types are added, including (1) reflexive particles, (2) prepositions, (3) subordinating conjunctions, (4) modal verbs, (5) verbal auxiliaries, and (6) punctuation nodes." ></td>
	<td class="line x" title="77:101	Also, grammar-based node ordering changes (implemented by rules) are performed: e.g. if an English possessive attribute is translated using Czech genitive, it is shifted into post-modification position." ></td>
	<td class="line x" title="78:101	After finishing the inflection of nouns, verbs, adjectives and adverbs (according to the values of morphological categories derived from agreement etc.), prepositions may need to be vocalized: the vowel -e or -u is attached to the preposition if the pronunciation of prepositional group would be difficult otherwise." ></td>
	<td class="line x" title="79:101	After the capitalization of the beginning of each sentence (and each named entity instance), we obtain the final translation by flattening the surface tree." ></td>
	<td class="line x" title="80:101	4.4 Preliminary Error Analysis According to our observations most errors happen during the transfer of lemmas and formemes." ></td>
	<td class="line x" title="81:101	Usually, there are acceptable translations of lemma and formeme in respective n-best lists but we fail to choose the best one." ></td>
	<td class="line x" title="82:101	The scenario described in Section 4.2 uses quite a primitive transfer algorithm where formemes and lemmas are translated separately in two steps." ></td>
	<td class="line x" title="83:101	We hope that big improvements could be achieved with more sophisticated algorithms (optimizing the probability of the whole tree) and smoothed probabilistic models (such as p(T.lemma|S.lemma, T.parent.lemma) and p(T.formeme|S.formeme, T.lemma, T.parent.lemma))." ></td>
	<td class="line x" title="84:101	Other common errors include:  Analysis: parsing (especially coordinations are problematic with McDonalds parser)." ></td>
	<td class="line x" title="85:101	 Transfer: the translation of idioms and collocations, including named entities." ></td>
	<td class="line x" title="86:101	In these cases, the classical transfer at the t-layer is not appropriate and utilization of some phrase-based MT would help." ></td>
	<td class="line x" title="87:101	 Synthesis: reflexive particles, word order." ></td>
	<td class="line x" title="88:101	5 Experimental Results and Discussion Table 3 reports lowercase BLEU and NIST scores and preliminary manual ranks of our submissions in contrast with other systems participating in EnglishCzech translation, as evaluated on the official WMT09 unseen test set." ></td>
	<td class="line x" title="89:101	Note that automatic metrics are known to correlate quite poorly with human judgements, see the best ranking but lower scoring PC Translator this year and also in Callison-Burch et al.(2008)." ></td>
	<td class="line x" title="91:101	System BLEU NIST Rank Moses T 14.24 5.175 -3.02 (4) Moses T+C 13.86 5.110  Google 13.59 4.964 -2.82 (3) U. of Edinburgh 13.55 5.039 -3.24 (5) Moses T+C+C&T+T+G 84k 10.01 4.360 Eurotran XP 09.51 4.381 -2.81 (2) PC Translator 09.42 4.335 -2.77 (1) TectoMT 07.29 4.173 -3.35 (6) Table 3: Automatic scores and preliminary human rank for EnglishCzech translation." ></td>
	<td class="line x" title="92:101	Systems in italics are provided for comparison only." ></td>
	<td class="line x" title="93:101	Best results in bold." ></td>
	<td class="line x" title="94:101	Unfortunately, this preliminary evaluation suggests that simpler models perform better, partly because it is easier to tune them properly both from computational point of view (e.g. MERT not stable and prone to overfitting with more features7), as well as from software engineering point of view (debugging of complex pipelines of tools is demanding)." ></td>
	<td class="line x" title="95:101	Moreover, simpler models run faster: Moses T with 12 sents/minute is 4.6 times faster than Moses T+C." ></td>
	<td class="line x" title="96:101	(Note that we have not tuned either of the models for speed.)" ></td>
	<td class="line x" title="97:101	While Moses T is probably nearly identical setup as Google and Univ. of Edinburgh use, the knowledge of correct language-dependent tokenization and the use of relatively high quality large language model data seems to bring moderate improvements." ></td>
	<td class="line x" title="98:101	6 Conclusion We described our experiments with a complex linguistically motivated translation system and various (again linguistically-motivated) setups of factored phrase-based translation." ></td>
	<td class="line x" title="99:101	An automatic evaluation seems to suggest that simpler is better, but we are well aware that a reliable judgement comes only from human annotators." ></td>
	<td class="line x" title="100:101	7For Moses T+C+C&T+T+G, we observed BLEU scores on the test set varying by up to five points absolute for various weight settings yielding nearly identical dev set scores." ></td>
	<td class="line x" title="101:101	128" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-1119
Design Challenges and Misconceptions in Named Entity Recognition
Ratinov, Lev;Roth, Dan;"></td>
	<td class="line x" title="1:223	Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 147155, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:223	c 2009 Association for Computational Linguistics Design Challenges and Misconceptions in Named Entity Recognition Lev Ratinov Dan Roth Computer Science Department University of Illinois Urbana, IL 61801 USA {ratinov2,danr}@uiuc.edu Abstract We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system." ></td>
	<td class="line x" title="3:223	In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system." ></td>
	<td class="line x" title="4:223	In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset." ></td>
	<td class="line x" title="5:223	1 Introduction Natural Language Processing applications are characterized by making complex interdependent decisions that require large amounts of prior knowledge." ></td>
	<td class="line x" title="6:223	In this paper we investigate one such application Named Entity Recognition (NER)." ></td>
	<td class="line x" title="7:223	Figure 1 illustrates the necessity of using prior knowledge and non-local decisions in NER." ></td>
	<td class="line x" title="8:223	In the absence of mixed case information it is difficult to understand that The system and the Webpages dataset are available at: http://l2r.cs.uiuc.edu/cogcomp/software.php This work was supported by NSF grant NSF SoD-HCER0613885, by MIAS, a DHS-IDS Center for Multimodal Information Access and Synthesis at UIUC and by an NDIIPP project from the National Library of Congress." ></td>
	<td class="line x" title="9:223	 We thank Nicholas Rizzolo for the baseline LBJ NER system, Xavier Carreras for suggesting the word class models, and multiple reviewers for insightful comments." ></td>
	<td class="line x" title="10:223	SOCCER [PER BLINKER] BAN LIFTED . [LOC LONDON] 1996-12-06 [MISC Dutch] forward [PER Reggie Blinker] had his indefinite suspension lifted by [ORG FIFA] on Friday and was set to make his [ORG Sheffield Wednesday] comeback against [ORG Liverpool] on Saturday . [PER Blinker] missed his clubs last two games after [ORG FIFA] slapped a worldwide ban on him for appearing to sign contracts for both [ORG Wednesday] and [ORG Udinese] while he was playing for [ORG Feyenoord]." ></td>
	<td class="line x" title="11:223	Figure 1: Example illustrating challenges in NER." ></td>
	<td class="line x" title="12:223	BLINKER is a person." ></td>
	<td class="line x" title="13:223	Likewise, it is not obvious that the last mention of Wednesday is an organization (in fact, the first mention of Wednesday can also be understood as a comeback which happens on Wednesday)." ></td>
	<td class="line x" title="14:223	An NER system could take advantage of the fact that blinker is also mentioned later in the text as the easily identifiable Reggie Blinker." ></td>
	<td class="line x" title="15:223	It is also useful to know that Udinese is a soccer club (an entry about this club appears in Wikipedia), and the expression both Wednesday and Udinese implies that Wednesday and Udinese should be assigned the same label." ></td>
	<td class="line x" title="16:223	The above discussion focuses on the need for external knowledge resources (for example, that Udinese can be a soccer club) and the need for nonlocal features to leverage the multiple occurrences of named entities in the text." ></td>
	<td class="line x" title="17:223	While these two needs have motivated some of the research in NER in the last decade, several other fundamental decisions must be made." ></td>
	<td class="line x" title="18:223	These include: what model to use for 147 sequential inference, how to represent text chunks and what inference (decoding) algorithm to use." ></td>
	<td class="line x" title="19:223	Despite the recent progress in NER, the effort has been dispersed in several directions and there are no published attempts to compare or combine the recent advances, leading to some design misconceptions and less than optimal performance." ></td>
	<td class="line x" title="20:223	In this paper we analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system." ></td>
	<td class="line x" title="21:223	We find that BILOU representation of text chunks significantly outperforms the widely adopted BIO." ></td>
	<td class="line x" title="22:223	Surprisingly, naive greedy inference performs comparably to beamsearch or Viterbi, while being considerably more computationally efficient." ></td>
	<td class="line x" title="23:223	We analyze several approaches for modeling non-local dependencies proposed in the literature and find that none of them clearly outperforms the others across several datasets." ></td>
	<td class="line x" title="24:223	However, as we show, these contributions are, to a large extent, independent and, as we show, the approaches can be used together to yield better results." ></td>
	<td class="line x" title="25:223	Our experiments corroborate recently published results indicating that word class models learned on unlabeled text can significantly improve the performance of the system and can be an alternative to the traditional semi-supervised learning paradigm." ></td>
	<td class="line x" title="26:223	Combining recent advances, we develop a publicly available NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset." ></td>
	<td class="line x" title="27:223	Our system is robust  it consistently outperforms all publicly available NER systems (e.g., the Stanford NER system) on all three datasets." ></td>
	<td class="line x" title="28:223	2 Datasets and Evaluation Methodology NER system should be robust across multiple domains, as it is expected to be applied on a diverse set of documents: historical texts, news articles, patent applications, webpages etc. Therefore, we have considered three datasets: CoNLL03 shared task data, MUC7 data and a set of Webpages we have annotated manually." ></td>
	<td class="line x" title="29:223	In the experiments throughout the paper, we test the ability of the tagger to adapt to new test domains." ></td>
	<td class="line x" title="30:223	Throughout this work, we train on the CoNLL03 data and test on the other datasets without retraining." ></td>
	<td class="line x" title="31:223	The differences in annotation schemes across datasets created evaluation challenges." ></td>
	<td class="line x" title="32:223	We discuss the datasets and the evaluation methods below." ></td>
	<td class="line x" title="33:223	The CoNLL03 shared task data is a subset of Reuters 1996 news corpus annotated with 4 entity types: PER,ORG, LOC, MISC." ></td>
	<td class="line x" title="34:223	It is important to notice that both the training and the development datasets are news feeds from August 1996, while the test set contains news feeds from December 1996." ></td>
	<td class="line x" title="35:223	The named entities mentioned in the test dataset are considerably different from those that appear in the training or the development set." ></td>
	<td class="line x" title="36:223	As a result, the test dataset is considerably harder than the development set." ></td>
	<td class="line x" title="37:223	Evaluation: Following the convention, we report phrase-level F1 score." ></td>
	<td class="line x" title="38:223	The MUC7 dataset is a subset of the North American News Text Corpora annotated with a wide variety of entities including people, locations, organizations, temporal events, monetary units, and so on." ></td>
	<td class="line x" title="39:223	Since there was no direct mapping from temporal events, monetary units, and other entities from MUC7 and the MISC label in the CoNLL03 dataset, we measure performance only on PER,ORG and LOC." ></td>
	<td class="line x" title="40:223	Evaluation: There are several sources of inconsistency in annotation between MUC7 and CoNLL03." ></td>
	<td class="line x" title="41:223	For example, since the MUC7 dataset does not contain the MISC label, in the sentence balloon, called the Virgin Global Challenger , the expression Virgin Global Challenger should be labeled as MISC according to CoNLL03 guidelines." ></td>
	<td class="line x" title="42:223	However, the gold annotation in MUC7 is balloon, called the [ORG Virgin] Global Challenger." ></td>
	<td class="line x" title="43:223	These and other annotation inconsistencies have prompted us to relax the requirements of finding the exact phrase boundaries and measure performance using token-level F1." ></td>
	<td class="line x" title="44:223	Webpages we have assembled and manually annotated a collection of 20 webpages, including personal, academic and computer-science conference homepages." ></td>
	<td class="line x" title="45:223	The dataset contains 783 entities (96loc, 223-org, 276-per, 188-misc)." ></td>
	<td class="line x" title="46:223	Evaluation: The named entities in the webpages were highly ambiguous and very different from the named entities seen in the training data." ></td>
	<td class="line x" title="47:223	For example, the data included sentences such as : Hear, O Israel, the Lord our God, the Lord is one. We could not agree on whether O Israel should be labeled as ORG, LOC, or PER." ></td>
	<td class="line x" title="48:223	Similarly, we could not agree on whether God and Lord is an ORG or PER." ></td>
	<td class="line x" title="49:223	These issues 148 led us to report token-level entity-identification F1 score for this dataset." ></td>
	<td class="line x" title="50:223	That is, if a named entity token was identified as such, we counted it as a correct prediction ignoring the named entity type." ></td>
	<td class="line x" title="51:223	3 Design Challenges in NER In this section we introduce the baseline NER system, and raise the fundamental questions underlying robust and efficient design." ></td>
	<td class="line x" title="52:223	These questions define the outline of this paper." ></td>
	<td class="line x" title="53:223	NER is typically viewed as a sequential prediction problem, the typical models include HMM (Rabiner, 1989), CRF (Lafferty et al., 2001), and sequential application of Perceptron or Winnow (Collins, 2002)." ></td>
	<td class="line x" title="54:223	That is, let x = (x1,,xN) be an input sequence and y = (y1,,yN) be the output sequence." ></td>
	<td class="line x" title="55:223	The sequential prediction problem is to estimate the probabilities P(yi|xik xi+l,yimyi1), where k,l and m are small numbers to allow tractable inference and avoid overfitting." ></td>
	<td class="line x" title="56:223	This conditional probability distribution is estimated in NER using the following baseline set of features (Zhang and Johnson, 2003): (1) previous two predictions yi1 and yi2 (2) current word xi (3) xi word type (all-capitalized, is-capitalized, all-digits, alphanumeric, etc.)" ></td>
	<td class="line x" title="57:223	(4) prefixes and suffixes of xi (5) tokens in the window c = (xi2,xi1,xi,xi+1,xi+2) (6) capitalization pattern in the window c (7) conjunction of c and yi1." ></td>
	<td class="line x" title="58:223	Most NER systems use additional features, such as POS tags, shallow parsing information and gazetteers." ></td>
	<td class="line x" title="59:223	We discuss additional features in the following sections." ></td>
	<td class="line x" title="60:223	We note that we normalize dates and numbers, that is 12/3/2008 becomes *Date*, 1980 becomes *DDDD* and 212-325-4751 becomes *DDD*-*DDD*-*DDDD*." ></td>
	<td class="line x" title="61:223	This allows a degree of abstraction to years, phone numbers, etc. Our baseline NER system uses a regularized averaged perceptron (Freund and Schapire, 1999)." ></td>
	<td class="line x" title="62:223	Systems based on perceptron have been shown to be competitive in NER and text chunking (Kazama and Torisawa, 2007b; Punyakanok and Roth, 2001; Carreras et al., 2003) We specify the model and the features with the LBJ (Rizzolo and Roth, 2007) modeling language." ></td>
	<td class="line x" title="63:223	We now state the four fundamental design decisions in NER system which define the structure of this paper." ></td>
	<td class="line x" title="64:223	Algorithm Baseline system Final System Greedy 83.29 90.57 Beam size=10 83.38 90.67 Beam size=100 83.38 90.67 Viterbi 83.71 N/A Table 1: Phrase-level F1 performance of different inference methods on CoNLL03 test data." ></td>
	<td class="line x" title="65:223	Viterbi cannot be used in the end system due to non-local features." ></td>
	<td class="line x" title="66:223	Key design decisions in an NER system." ></td>
	<td class="line x" title="67:223	1) How to represent text chunks in NER system?" ></td>
	<td class="line x" title="68:223	2) What inference algorithm to use?" ></td>
	<td class="line x" title="69:223	3) How to model non-local dependencies?" ></td>
	<td class="line x" title="70:223	4) How to use external knowledge resources in NER?" ></td>
	<td class="line x" title="71:223	4 Inference & Chunk Representation In this section we compare the performance of several inference (decoding) algorithms: greedy leftto-right decoding, Viterbi and beamsearch." ></td>
	<td class="line x" title="72:223	It may appear that beamsearch or Viterbi will perform much better than naive greedy left-to-right decoding, which can be seen as beamsearch of size one." ></td>
	<td class="line x" title="73:223	The Viterbi algorithm has the limitation that it does not allow incorporating some of the non-local features which will be discussed later, therefore, we cannot use it in our end system." ></td>
	<td class="line x" title="74:223	However, it has the appealing quality of finding the most likely assignment to a second-order model, and since the baseline features only have second order dependencies, we have tested it on the baseline configuration." ></td>
	<td class="line x" title="75:223	Table 1 compares between the greedy decoding, beamsearch with varying beam size, and Viterbi, both for the system with baseline features and for the end system (to be presented later)." ></td>
	<td class="line x" title="76:223	Surprisingly, the greedy policy performs well, this phenmenon was also observed in the POS tagging task (Toutanova et al., 2003; Roth and Zelenko, 1998)." ></td>
	<td class="line x" title="77:223	The implications are subtle." ></td>
	<td class="line x" title="78:223	First, due to the second-order of the model, the greedy decoding is over 100 times faster than Viterbi." ></td>
	<td class="line x" title="79:223	The reason is that with the BILOU encoding of four NE types, each token can take 21 states (O, B-PER, I-PER , U-PER, etc.)." ></td>
	<td class="line x" title="80:223	To tag a token, the greedy policy requires 21 comparisons, while the Viterbi requires 213, and this analysis carries over to the number of classifier invocations." ></td>
	<td class="line x" title="81:223	Furthermore, both beamsearch and Viterbi require transforming the predictions of the classi149 Rep. CoNLL03 MUC7 Scheme Test Dev Dev Test BIO 89.15 93.61 86.76 85.15 BILOU 90.57 93.28 88.09 85.62 Table 2: End system performance with BILOU and BIO schemes." ></td>
	<td class="line x" title="82:223	BILOU outperforms the more widely used BIO." ></td>
	<td class="line x" title="83:223	fiers to probabilities as discussed in (NiculescuMizil and Caruana, 2005), incurring additional time overhead." ></td>
	<td class="line x" title="84:223	Second, this result reinforces the intuition that global inference over the second-order HMM features does not capture the non-local properties of the task." ></td>
	<td class="line x" title="85:223	The reason is that the NEs tend to be short chunks separated by multiple outside tokens." ></td>
	<td class="line x" title="86:223	This separation breaks the Viterbi decision process to independent maximization of assignment over short chunks, where the greedy policy performs well." ></td>
	<td class="line x" title="87:223	On the other hand, dependencies between isolated named entity chunks have longer-range dependencies and are not captured by second-order transition features, therefore requiring separate mechanisms, which we discuss in Section 5." ></td>
	<td class="line x" title="88:223	Another important question that has been studied extensively in the context of shallow parsing and was somewhat overlooked in the NER literature is the representation of text segments (Veenstra, 1999)." ></td>
	<td class="line x" title="89:223	Related works include voting between several representation schemes (Shen and Sarkar, 2005), lexicalizing the schemes (Molina and Pla, 2002) and automatically searching for best encoding (Edward, 2007)." ></td>
	<td class="line x" title="90:223	However, we are not aware of similar work in the NER settings." ></td>
	<td class="line x" title="91:223	Due to space limitations, we do not discuss all the representation schemes and combining predictions by voting." ></td>
	<td class="line x" title="92:223	We focus instead on two most popular schemes BIO and BILOU." ></td>
	<td class="line x" title="93:223	The BIO scheme suggests to learn classifiers that identify the Beginning, the Inside and the Outside of the text segments." ></td>
	<td class="line x" title="94:223	The BILOU scheme suggests to learn classifiers that identify the Beginning, the Inside and the Last tokens of multi-token chunks as well as Unit-length chunks." ></td>
	<td class="line x" title="95:223	The BILOU scheme allows to learn a more expressive model with only a small increase in the number of parameters to be learned." ></td>
	<td class="line x" title="96:223	Table 2 compares the end systems performance with BIO and BILOU." ></td>
	<td class="line x" title="97:223	Examining the results, we reach two conclusions: (1) choice of encoding scheme has a big impact on the system performance and (2) the less used BILOU formalism significantly outperforms the widely adopted BIO tagging scheme." ></td>
	<td class="line x" title="98:223	We use the BILOU scheme throughout the paper." ></td>
	<td class="line x" title="99:223	5 Non-Local Features The key intuition behind non-local features in NER has been that identical tokens should have identical label assignments." ></td>
	<td class="line x" title="100:223	The sample text discussed in the introduction shows one such example, where all occurrences of blinker are assigned the PER label." ></td>
	<td class="line x" title="101:223	However, in general, this is not always the case; for example we might see in the same document the word sequences Australia and The bank of Australia." ></td>
	<td class="line x" title="102:223	The first instance should be labeled as LOC, and the second as ORG." ></td>
	<td class="line x" title="103:223	We consider three approaches proposed in the literature in the following sections." ></td>
	<td class="line x" title="104:223	Before continuing the discussion, we note that we found that adjacent documents in the CoNLL03 and the MUC7 datasets often discuss the same entities." ></td>
	<td class="line x" title="105:223	Therefore, we ignore document boundaries and analyze global dependencies in 200 and 1000 token windows." ></td>
	<td class="line x" title="106:223	These constants were selected by hand after trying a small number of values." ></td>
	<td class="line x" title="107:223	We believe that this approach will also make our system more robust in cases when the document boundaries are not given." ></td>
	<td class="line x" title="108:223	5.1 Context aggregation (Chieu and Ng, 2003) used features that aggregate, for each document, the context tokens appear in." ></td>
	<td class="line x" title="109:223	Sample features are: the longest capitilized sequence of words in the document which contains the current token and the token appears before a company marker such as ltd. elsewhere in text." ></td>
	<td class="line x" title="110:223	In this work, we call this type of features context aggregation features." ></td>
	<td class="line x" title="111:223	Manually designed context aggregation features clearly have low coverage, therefore we used the following approach." ></td>
	<td class="line x" title="112:223	Recall that for each token instance xi, we use as features the tokens in the window of size two around it: ci = (xi2,xi1,xi,xi+1,xi+2)." ></td>
	<td class="line x" title="113:223	When the same token type t appears in several locations in the text, say xi1,xi2,,xiN , for each instance xij, in addition to the context features cij, we also aggregate the context across all instances within 200 tokens: C =j=Nj=1 cij." ></td>
	<td class="line x" title="114:223	150 CoNLL03 CoNLL03 MUC7 MUC7 Web Component Test data Dev data Dev Test pages 1) Baseline 83.65 89.25 74.72 71.28 71.41 2) (1) + Context Aggregation 85.40 89.99 79.16 71.53 70.76 3) (1) + Extended Prediction History 85.57 90.97 78.56 74.27 72.19 4) (1)+ Two-stage Prediction Aggregation 85.01 89.97 75.48 72.16 72.72 5) All Non-local Features (1-4) 86.53 90.69 81.41 73.61 71.21 Table 3: The utility of non-local features." ></td>
	<td class="line x" title="115:223	The system was trained on CoNLL03 data and tested on CoNNL03, MUC7 and Webpages." ></td>
	<td class="line x" title="116:223	No single technique outperformed the rest on all domains." ></td>
	<td class="line x" title="117:223	The combination of all techniques is the most robust." ></td>
	<td class="line x" title="118:223	5.2 Two-stage prediction aggregation Context aggregation as done above can lead to excessive number of features." ></td>
	<td class="line x" title="119:223	(Krishnan and Manning, 2006) used the intuition that some instances of a token appear in easily-identifiable contexts." ></td>
	<td class="line x" title="120:223	Therefore they apply a baseline NER system, and use the resulting predictions as features in a second level of inference." ></td>
	<td class="line x" title="121:223	We call the technique two-stage prediction aggregation." ></td>
	<td class="line x" title="122:223	We implemented the token-majority and the entity-majority features discussed in (Krishnan and Manning, 2006); however, instead of document and corpus majority tags, we used relative frequency of the tags in a 1000 token window." ></td>
	<td class="line x" title="123:223	5.3 Extended prediction history Both context aggregation and two-stage prediction aggregation treat all tokens in the text similarly." ></td>
	<td class="line x" title="124:223	However, we observed that the named entities in the beginning of the documents tended to be more easily identifiable and matched gazetteers more often." ></td>
	<td class="line x" title="125:223	This is due to the fact that when a named entity is introduced for the first time in text, a canonical name is used, while in the following discussion abbreviated mentions, pronouns, and other references are used." ></td>
	<td class="line x" title="126:223	To break the symmetry, when using beamsearch or greedy left-to-right decoding, we use the fact that when we are making a prediction for token instance xi, we have already made predictions y1,,yi1 for token instances x1,,xi1." ></td>
	<td class="line x" title="127:223	When making the prediction for token instance xi, we record the label assignment distribution for all token instances for the same token type in the previous 1000 words." ></td>
	<td class="line x" title="128:223	That is, if the token instance is Australia, and in the previous 1000 tokens, the token type Australia was twice assigned the label L-ORG and three times the label U-LOC, then the prediction history feature will be: (LORG : 25;ULOC : 35)." ></td>
	<td class="line x" title="129:223	5.4 Utility of non-local features Table 3 summarizes the results." ></td>
	<td class="line x" title="130:223	Surprisingly, no single technique outperformed the others on all datasets." ></td>
	<td class="line x" title="131:223	The extended prediction history method was the best on CoNLL03 data and MUC7 test set." ></td>
	<td class="line x" title="132:223	Context aggregation was the best method for MUC7 development set and two-stage prediction was the best for Webpages." ></td>
	<td class="line x" title="133:223	Non-local features proved less effective for MUC7 test set and the Webpages." ></td>
	<td class="line x" title="134:223	Since the named entities in Webpages have less context, this result is expected for the Webpages." ></td>
	<td class="line x" title="135:223	However, we are unsure why MUC7 test set benefits from nonlocal features much less than MUC7 development set." ></td>
	<td class="line x" title="136:223	Our key conclusion is that no single approach is better than the rest and that the approaches are complimentarytheir combination is the most stable and best performing." ></td>
	<td class="line x" title="137:223	6 External Knowledge As we have illustrated in the introduction, NER is a knowledge-intensive task." ></td>
	<td class="line x" title="138:223	In this section, we discuss two important knowledge resources gazetteers and unlabeled text." ></td>
	<td class="line x" title="139:223	6.1 Unlabeled Text Recent successful semi-supervised systems (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) have illustrated that unlabeled text can be used to improve the performance of NER systems." ></td>
	<td class="line x" title="140:223	In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing (Koo et al., 2008), Chinese word segmentation (Liang, 2005) and NER (Miller et al., 2004)." ></td>
	<td class="line x" title="141:223	The technique is based on word class models, pioneered by (Brown et al., 1992), which hierarchically 151 CoNLL03 CoNLL03 MUC7 MUC7 Web Component Test data Dev data Dev Test pages 1) Baseline 83.65 89.25 74.72 71.28 71.41 2) (1) + Gazetteer Match 87.22 91.61 85.83 80.43 74.46 3) (1) + Word Class Model 86.82 90.85 80.25 79.88 72.26 4) All External Knowledge 88.55 92.49 84.50 83.23 74.44 Table 4: Utility of external knowledge." ></td>
	<td class="line x" title="142:223	The system was trained on CoNLL03 data and tested on CoNNL03, MUC7 and Webpages." ></td>
	<td class="line x" title="143:223	clusters words, producing a binary tree as in Figure 2." ></td>
	<td class="line x" title="144:223	Figure 2: An extract from word cluster hierarchy." ></td>
	<td class="line x" title="145:223	The approach is related, but not identical, to distributional similarity (for details, see (Brown et al., 1992) and (Liang, 2005))." ></td>
	<td class="line x" title="146:223	For example, since the words Friday and Tuesday appear in similar contexts, the Brown algorithm will assign them to the same cluster." ></td>
	<td class="line x" title="147:223	Successful abstraction of both as a day of the week, addresses the data sparsity problem common in NLP tasks." ></td>
	<td class="line x" title="148:223	In this work, we use the implementation and the clusters obtained in (Liang, 2005) from running the algorithm on the Reuters 1996 dataset, a superset of the CoNLL03 NER dataset." ></td>
	<td class="line x" title="149:223	Within the binary tree produced by the algorithm, each word can be uniquely identified by its path from the root, and this path can be compactly represented with a bit string." ></td>
	<td class="line x" title="150:223	Paths of different depths along the path from the root to the word provide different levels of word abstraction." ></td>
	<td class="line x" title="151:223	For example, paths at depth 4 closely correspond to POS tags." ></td>
	<td class="line x" title="152:223	Since word class models use large amounts of unlabeled data, they are essentially a semi-supervised technique, which we use to considerably improve the performance of our system." ></td>
	<td class="line x" title="153:223	In this work, we used path prefixes of length 4,6,10, and 20." ></td>
	<td class="line x" title="154:223	When Brown clusters are used as features in the following sections, it implies that all features in the system which contain a word form will be duplicated and a new set of features containing the paths of varying length will be introduced." ></td>
	<td class="line x" title="155:223	For example, if the system contains the feature concatenation of the current token and the system prediction on the previous word, four new features will be introduced which are concatenations of the previous prediction and the 4,6,10,20 length path-representations of the current word." ></td>
	<td class="line x" title="156:223	6.2 Gazetteers An important question at the inception of the NER task was whether machine learning techniques are necessary at all, and whether simple dictionary lookup would be sufficient for good performance." ></td>
	<td class="line x" title="157:223	Indeed, the baseline for the CoNLL03 shared task was essentially a dictionary lookup of the entities which appeared in the training data, and it achieves 71.91 F1 score on the test set (Tjong and De Meulder, 2003)." ></td>
	<td class="line x" title="158:223	It turns out that while problems of coverage and ambiguity prevent straightforward lookup, injection of gazetteer matches as features in machine-learning based approaches is critical for good performance (Cohen, 2004; Kazama and Torisawa, 2007a; Toral and Munoz, 2006; Florian et al., 2003)." ></td>
	<td class="line x" title="159:223	Given these findings, several approaches have been proposed to automatically extract comprehensive gazetteers from the web and from large collections of unlabeled text (Etzioni et al., 2005; Riloff and Jones, 1999) with limited impact on NER." ></td>
	<td class="line x" title="160:223	Recently, (Toral and Munoz, 2006; Kazama and Torisawa, 2007a) have successfully constructed high quality and high coverage gazetteers from Wikipedia." ></td>
	<td class="line x" title="161:223	In this work, we use a collection of 14 highprecision, low-recall lists extracted from the web that cover common names, countries, monetary units, temporal expressions, etc. While these gazetteers have excellent accuracy, they do not provide sufficient coverage." ></td>
	<td class="line x" title="162:223	To further improve the coverage, we have extracted 16 gazetteers from Wikipedia, which collectively contain over 1.5M entities." ></td>
	<td class="line x" title="163:223	Overall, we have 30 gazetteers (available for download with the system), and matches against 152 CoNLL03 CoNLL03 MUC7 MUC7 Web Component Test data Dev data Dev Test pages 1) Baseline 83.65 89.25 74.72 71.28 71.41 2) (1) + External Knowledge 88.55 92.49 84.50 83.23 74.44 3) (1) + Non-local 86.53 90.69 81.41 73.61 71.21 4) All Features 90.57 93.50 89.19 86.15 74.53 5) All Features (train with dev) 90.80 N/A 89.19 86.15 74.33 Table 5: End system performance by component." ></td>
	<td class="line x" title="164:223	Results confirm that NER is a knowledge-intensive task." ></td>
	<td class="line x" title="165:223	each one are weighted as a separate feature in the system (this allows us to trust each gazetteer to a different degree)." ></td>
	<td class="line x" title="166:223	We also note that we have developed a technique for injecting non-exact string matching to gazetteers, which has marginally improved the performance, but is not covered in the paper due to space limitations." ></td>
	<td class="line x" title="167:223	In the rest of this section, we discuss the construction of gazetteers from Wikipedia." ></td>
	<td class="line x" title="168:223	Wikipedia is an open, collaborative encyclopedia with several attractive properties." ></td>
	<td class="line x" title="169:223	(1) It is kept updated manually by it collaborators, hence new entities are constantly added to it." ></td>
	<td class="line x" title="170:223	(2) Wikipedia contains redirection pages, mapping several variations of spelling of the same name to one canonical entry." ></td>
	<td class="line x" title="171:223	For example, Suker is redirected to an entry about Davor Suker, the Croatian footballer (3) The entries in Wikipedia are manually tagged with categories." ></td>
	<td class="line x" title="172:223	For example, the entry about the Microsoft in Wikipedia has the following categories: Companies listed on NASDAQ; Cloud computing vendors; etc. Both (Toral and Munoz, 2006) and (Kazama and Torisawa, 2007a) used the free-text description of the Wikipedia entity to reason about the entity type." ></td>
	<td class="line x" title="173:223	We use a simpler method to extract high coverage and high quality gazetteers from Wikipedia." ></td>
	<td class="line x" title="174:223	By inspection of the CoNLL03 shared task annotation guidelines and of the training set, we manually aggregated several categories into a higher-level concept (not necessarily NER type)." ></td>
	<td class="line x" title="175:223	When a Wikipedia entry was tagged by one of the categories in the table, it was added to the corresponding gazetteer." ></td>
	<td class="line x" title="176:223	6.3 Utility of External Knowledge Table 4 summarizes the results of the techniques for injecting external knowledge." ></td>
	<td class="line x" title="177:223	It is important to note that, although the world class model was learned on the superset of CoNLL03 data, and although the Wikipedia gazetteers were constructed Dataset Stanford-NER LBJ-NER MUC7 Test 80.62 85.71 MUC7 Dev 84.67 87.99 Webpages 72.50 74.89 Reuters2003 test 87.04 90.74 Reuters2003 dev 92.36 93.94 Table 6: Comparison: token-based F1 score of LBJ-NER and Stanford NER tagger across several domains based on CoNLL03 annotation guidelines, these features proved extremely good on all datasets." ></td>
	<td class="line x" title="178:223	Word class models discussed in Section 6.1 are computed offline, are available online1, and provide an alternative to traditional semi-supervised learning." ></td>
	<td class="line x" title="179:223	It is important to note that the word class models and the gazetteers and independednt and accumulative." ></td>
	<td class="line x" title="180:223	Furthermore, despite the number and the gigantic size of the extracted gazetteers, the gazeteers alone are not sufficient for adequate performance." ></td>
	<td class="line x" title="181:223	When we modified the CoNLL03 baseline to include gazetteer matches, the performance went up from 71.91 to 82.3 on the CoNLL03 test set, below our baseline systems result of 83.65." ></td>
	<td class="line x" title="182:223	When we have injected the gazetteers into our system, the performance went up to 87.22." ></td>
	<td class="line x" title="183:223	Word class model and nonlocal features further improve the performance to 90.57 (see Table 5), by more than 3 F1 points." ></td>
	<td class="line x" title="184:223	7 Final System Performance Analysis As a final experiment, we have trained our system both on the training and on the development set, which gave us our best F1 score of 90.8 on the CoNLL03 data, yet it failed to improve the performance on other datasets." ></td>
	<td class="line x" title="185:223	Table 5 summarizes the performance of the system." ></td>
	<td class="line x" title="186:223	Next, we have compared the performance of our 1http://people.csail.mit.edu/maestro/papers/bllip-clusters.gz 153 system to that of the Stanford NER tagger, across the datasets discussed above." ></td>
	<td class="line x" title="187:223	We have chosen to compare against the Stanford tagger because to the best of our knowledge, it is the best publicly available system which is trained on the same data." ></td>
	<td class="line x" title="188:223	We have downloaded the Stanford NER tagger and used the strongest provided model trained on the CoNLL03 data with distributional similarity features." ></td>
	<td class="line oc" title="189:223	The results we obtained on the CoNLL03 test set were consistent with what was reported in (Finkel et al., 2005)." ></td>
	<td class="line x" title="190:223	Our goal was to compare the performance of the taggers across several datasets." ></td>
	<td class="line x" title="191:223	For the most realistic comparison, we have presented each system with a raw text, and relied on the systems sentence splitter and tokenizer." ></td>
	<td class="line x" title="192:223	When evaluating the systems, we matched against the gold tokenization ignoring punctuation marks." ></td>
	<td class="line x" title="193:223	Table 6 summarizes the results." ></td>
	<td class="line x" title="194:223	Note that due to differences in sentence splitting, tokenization and evaluation, these results are not identical to those reported in Table 5." ></td>
	<td class="line x" title="195:223	Also note that in this experiment we have used token-level accuracy on the CoNLL dataset as well." ></td>
	<td class="line x" title="196:223	Finally, to complete the comparison to other systems, in Table 7 we summarize the best results reported for the CoNLL03 dataset in literature." ></td>
	<td class="line x" title="197:223	8 Conclusions We have presented a simple model for NER that uses expressive features to achieve new state of the art performance on the Named Entity recognition task." ></td>
	<td class="line x" title="198:223	We explored four fundamental design decisions: text chunks representation, inference algorithm, using non-local features and external knowledge." ></td>
	<td class="line x" title="199:223	We showed that BILOU encoding scheme significantly outperforms BIO and that, surprisingly, a conditional model that does not take into account interactions at the output level performs comparably to beamsearch or Viterbi, while being considerably more efficient computationally." ></td>
	<td class="line x" title="200:223	We analyzed several approaches for modeling non-local dependencies and found that none of them clearly outperforms the others across several datasets." ></td>
	<td class="line x" title="201:223	Our experiments corroborate recently published results indicating that word class models learned on unlabeled text can be an alternative to the traditional semi-supervised learning paradigm." ></td>
	<td class="line oc" title="202:223	NER proves to be a knowledgeintensive task, and it was reassuring to observe that System Resources Used F1 + LBJ-NER Wikipedia, Nonlocal Features, Word-class Model 90.80 (Suzuki and Isozaki, 2008) Semi-supervised on 1Gword unlabeled data 89.92 (Ando and Zhang, 2005) Semi-supervised on 27Mword unlabeled data 89.31 (Kazama and Torisawa, 2007a) Wikipedia 88.02 (Krishnan and Manning, 2006) Non-local Features 87.24 (Kazama and Torisawa, 2007b) Non-local Features 87.17 + (Finkel et al., 2005) Non-local Features 86.86 Table 7: Results for CoNLL03 data reported in the literature." ></td>
	<td class="line x" title="203:223	publicly available systems marked by +." ></td>
	<td class="line x" title="204:223	knowledge-driven techniques adapt well across several domains." ></td>
	<td class="line x" title="205:223	We observed consistent performance gains across several domains, most interestingly in Webpages, where the named entities had less context and were different in nature from the named entities in the training set." ></td>
	<td class="line x" title="206:223	Our system significantly outperforms the current state of the art and is available to download under a research license." ></td>
	<td class="line x" title="207:223	Apendix wikipedia gazetters & categories 1)People: people, births, deaths." ></td>
	<td class="line x" title="208:223	Extracts 494,699 Wikipedia titles and 382,336 redirect links." ></td>
	<td class="line x" title="209:223	2)Organizations: cooperatives, federations, teams, clubs, departments, organizations, organisations, banks, legislatures, record labels, constructors, manufacturers, ministries, ministers, military units, military formations, universities, radio stations, newspapers, broadcasters, political parties, television networks, companies, businesses, agencies." ></td>
	<td class="line x" title="210:223	Extracts 124,403 titles and 130,588 redirects." ></td>
	<td class="line x" title="211:223	3)Locations: airports, districts, regions, countries, areas, lakes, seas, oceans, towns, villages, parks, bays, bases, cities, landmarks, rivers, valleys, deserts, locations, places, neighborhoods." ></td>
	<td class="line x" title="212:223	Extracts 211,872 titles and 194,049 redirects." ></td>
	<td class="line x" title="213:223	4)Named Objects: aircraft, spacecraft, tanks, rifles, weapons, ships, firearms, automobiles, computers, boats." ></td>
	<td class="line x" title="214:223	Extracts 28,739 titles and 31,389 redirects." ></td>
	<td class="line x" title="215:223	5)Art Work: novels, books, paintings, operas, plays." ></td>
	<td class="line x" title="216:223	Extracts 39,800 titles and 34037 redirects." ></td>
	<td class="line x" title="217:223	6)Films: films, telenovelas, shows, musicals." ></td>
	<td class="line x" title="218:223	Extracts 50,454 titles and 49,252 redirects." ></td>
	<td class="line x" title="219:223	7)Songs: songs, singles, albums." ></td>
	<td class="line x" title="220:223	Extracts 109,645 titles and 67,473 redirects." ></td>
	<td class="line x" title="221:223	8)Events: playoffs, championships, races, competitions, battles." ></td>
	<td class="line x" title="222:223	Extracts 20,176 titles and 15,182 redirects." ></td>
	<td class="line x" title="223:223	154" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-1218
Multilingual Syntactic-Semantic Dependency Parsing with Three-Stage Approximate Max-Margin Linear Models
Watanabe, Yotaro;Asahara, Masayuki;Matsumoto, Yuji;"></td>
	<td class="line x" title="1:138	Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 114119, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:138	c 2009 Association for Computational Linguistics Multilingual Syntactic-Semantic Dependency Parsing with Three-Stage Approximate Max-Margin Linear Models Yotaro Watanabe, Masayuki Asahara and Yuji Matsumoto Graduate School of Information Science Nara Institute of Science and Technology 8916-5 Takayama, Ikoma, Nara, Japan, 630-0192 {yotaro-w, masayu-a, matsu}@is.naist.jp Abstract This paper describes a system for syntacticsemantic dependency parsing for multiple languages." ></td>
	<td class="line x" title="3:138	The system consists of three parts: a state-of-the-art higher-order projective dependency parser for syntactic dependency parsing, a predicate classier, and an argument classier for semantic dependency parsing." ></td>
	<td class="line x" title="4:138	For semantic dependency parsing, we explore use of global features." ></td>
	<td class="line x" title="5:138	All components are trained with an approximate max-margin learning algorithm." ></td>
	<td class="line x" title="6:138	In the closed challenge of the CoNLL-2009 Shared Task (Hajic et al., 2009), our system achieved the 3rd best performances for English and Czech, and the 4th best performance for Japanese." ></td>
	<td class="line x" title="7:138	1 Introduction In recent years, joint inference of syntactic and semantic dependencies has attracted attention in NLP communities." ></td>
	<td class="line x" title="8:138	Ideally, we would like to choose the most plausible syntactic-semantic structure among all possible structures in that syntactic dependencies and semantic dependencies are correlated." ></td>
	<td class="line x" title="9:138	However, solving this problem is too difcult because the search space of the problem is extremely large." ></td>
	<td class="line x" title="10:138	Therefore we focus on improving performance for each subproblem: dependency parsing and semantic role labeling." ></td>
	<td class="line x" title="11:138	In the past few years, research investigating higher-order dependency parsing algorithms has found its superiority to rst-order parsing algorithms." ></td>
	<td class="line x" title="12:138	To reap the benets of these advances, we use a higher-order projective dependency parsing algorithm (Carreras, 2007) which is an extension of the span-based parsing algorithm (Eisner, 1996), for syntactic dependency parsing." ></td>
	<td class="line x" title="13:138	In terms of semantic role labeling, we would like to capture global information about predicateargumentstructuresinordertoaccuratelypredictthe correct predicate-argument structure." ></td>
	<td class="line x" title="14:138	Previous research dealt with such information using re-ranking (Toutanova et al., 2005; Johansson and Nugues, 2008)." ></td>
	<td class="line x" title="15:138	We explore a different approach to deal with such information using global features." ></td>
	<td class="line oc" title="16:138	Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling (Finkel et al., 2005; Krishnan and Manning, 2006; Kazama and Torisawa, 2007) and dependency parsing (Nakagawa, 2007) with a great deal of success." ></td>
	<td class="line x" title="17:138	We attempt to use global features for argument classication in which the most plausible semantic role assignment is selected using both local and global information." ></td>
	<td class="line x" title="18:138	We present an approximate max-margin learning algorithm for argument classiers with global features." ></td>
	<td class="line x" title="19:138	2 Dependency Parsing As in previous work, we use a linear model for dependency parsing." ></td>
	<td class="line x" title="20:138	The score function used in our dependency parser is dened as follows." ></td>
	<td class="line x" title="21:138	s(y) =  (h,m)y F(h,m,x) (1) where h and m denote the head and the dependent of the dependency edge in y, and F(h,m,x) is a Factor that species dependency edge scores." ></td>
	<td class="line x" title="22:138	114 We used a second-order factorization as in (Carreras, 2007)." ></td>
	<td class="line x" title="23:138	The second-order factor F is dened as follows." ></td>
	<td class="line x" title="24:138	F(h,m,x) = w(h,m,x)+w(h,m,ch,x) +w(h,m,cmi,x) +w(h,m,cmo,x) (2) where w is a parameter vector,  is a feature vector, ch is the child of h in the span [hm] that is closest to m, cmi is the child of m in the span [hm] that is farthest frommandcmo is the child of moutside the span[hm]thatisfarthestfromm." ></td>
	<td class="line x" title="25:138	Formoredetails ofthesecond-orderparsingalgorithm, see(Carreras, 2007)." ></td>
	<td class="line x" title="26:138	For parser training, we use the Passive Aggressive Algorithm (Crammer et al., 2006), which is an approximate max-margin variant of the perceptron algorithm." ></td>
	<td class="line x" title="27:138	Also, we apply an efcient parameter averaging technique (Daume III, 2006)." ></td>
	<td class="line x" title="28:138	The resulting learning algorithm is shown in Algorithm 1." ></td>
	<td class="line x" title="29:138	Algorithm 1 A Passive Aggressive Algorithm with parameter averaging input Training set T = {xt,yt}Tt=1, Number of iterations N and Parameter C w  0, v  0, c  1 for i  0 to N do for (xt,yt)  T do y = argmaxy w(xt,y)+ (yt, y) t = min  C, w(xt,y)w(xt,yt)+(yt,y)||(xt,yt)(xt,y)||2  w  w+ t((xt,yt)(xt, y)) v  v+ ct((xt,yt)(xt, y)) c  c +1 end for end for return wv/c We set (yt, y) as the number of incorrect head predictions in the y, and C as 1.0." ></td>
	<td class="line x" title="30:138	Among the 7 languages of the task, 4 languages (Czech, English, German and Japanese) contain non-projective edges (13.94 %, 3.74 %, 25.79 % and 0.91 % respectively), therefore we need to deal with non-projectivity." ></td>
	<td class="line x" title="31:138	In order to avoid losing the benets of higher-order parsing, we considered applying pseudo-projective transformation (Nivre and Nilsson, 2005)." ></td>
	<td class="line x" title="32:138	However, growth of the number of dependency labels by pseudo-projective transformation increases the dependency parser training time, so we did not adopt transformations." ></td>
	<td class="line x" title="33:138	Therefore, the parser ignores the presence of non-projective edges in the training and the testing phases." ></td>
	<td class="line x" title="34:138	The features used for our dependency parser are based on those listed in (Johansson, 2008)." ></td>
	<td class="line x" title="35:138	In addition, distance features are used." ></td>
	<td class="line x" title="36:138	We use shorthand notations in order to simplify the feature representations: h, d, c, l, p, 1 and +1 correspond to head, dependent, heads or dependents child, lemma , POS, left position and right position respectively." ></td>
	<td class="line x" title="37:138	First-order Features Token features: hl, hp, hl+hp, dl, dp and dl+dp." ></td>
	<td class="line x" title="38:138	Head-Dependent features: hp+dp, hl+dl, hl+dl, hl+hp+dl, hl+hp+dp, hl+dl+dp, hp+dl+dp and hl+hp+dl+dp." ></td>
	<td class="line x" title="39:138	Context features: hp+hp+1+dp1+dp, hp1+hp+dp1+dp, hp+hp+1+dp+dp+1 and hp1+hp+dp+dp+1." ></td>
	<td class="line x" title="40:138	Distance features: The number of tokens between the head and the dependent." ></td>
	<td class="line x" title="41:138	Second-order Features Head-Dependent-Heads or Dependents Child: hl+cl, hl+cl+cp, hp+cl, hp+cp, hp+dp+cp, dp+cp, dp+cl+cp, dl+cp, dl+cp+cl 3 Semantic Role Labeling Our SRL module consists of two parts: a predicate classier and an argument classier." ></td>
	<td class="line x" title="42:138	First, our system determines the word sense for each predicate with the predicate classier, and then it detects the highest scored argument assignment using the argument classier with global features." ></td>
	<td class="line x" title="43:138	3.1 Predicate Classication The rst phase of SRL in our system is to detect the word sense for each predicate." ></td>
	<td class="line x" title="44:138	WSD can be formalizedasamulti-classclassicationproblemgiven lemmas." ></td>
	<td class="line x" title="45:138	We created a linear model for each lemma and used the Passive Aggressive Algorithm with parameter averaging to train the models." ></td>
	<td class="line x" title="46:138	3.1.1 Features for Predicate Classication Word features: Predicted lemma and the predicted POS of the predicate, predicates head, and its conjunctions." ></td>
	<td class="line x" title="47:138	Dependency label: The dependency label between the predicate and the predicates head." ></td>
	<td class="line x" title="48:138	115 Dependency label sequence: The concatenation of the dependency labels of the predicate dependents." ></td>
	<td class="line x" title="49:138	Since effective features for predicate classication are different for each language, we performed greedy forward feature selection." ></td>
	<td class="line x" title="50:138	3.2 Argument Classication In order to capture global clues of predicateargument structures, we consider introducing global features for linear models." ></td>
	<td class="line x" title="51:138	Let A(p) be a joint assignment of role labels for argument candidates given the predicate p. Then we dene a score function s(A(p)) for argument label assignments A(p)." ></td>
	<td class="line x" title="52:138	s(A(p)) =  k Fk(x,A(p)) (3) We introduce two factors: Local Factor FL and Global Factor FG dened as follows." ></td>
	<td class="line x" title="53:138	FL(x,a(p)) = wL(x,a(p)) (4) FG(x,A(p)) = wG(x,A(p)) (5) where L, G denote feature vectors for the local factor and the global factor respectively." ></td>
	<td class="line x" title="54:138	FL scores a particular role assignment for each argument candidate individually, and FG treats global features that capture what structure the assignment A has." ></td>
	<td class="line x" title="55:138	Resulting scoring function for the assignment A(p) is as follows." ></td>
	<td class="line x" title="56:138	s(A(p)) =  a(p)A(p) wL(x,a(p))+wG(x,A(p)) (6) Use of global features is problematic, because it becomes difcult to nd the highest assignment efciently." ></td>
	<td class="line x" title="57:138	In order to deal with the problem, we use a simple approach, n-best relaxation as in (Kazama and Torisawa, 2007)." ></td>
	<td class="line x" title="58:138	At rst we generate n-best assignments using only the local factor, and then add theglobalfactorscoreforeachn-bestassignment,nally select the best scoring assignment from them." ></td>
	<td class="line x" title="59:138	In order to generate n-best assignments, we used a beam-search algorithm." ></td>
	<td class="line x" title="60:138	3.2.1 Learning the Model As in dependency parser and predicate classier, we train the model using the PA algorithm with parameter averaging." ></td>
	<td class="line x" title="61:138	The learning algorithm is shown in Algorithm 2." ></td>
	<td class="line x" title="62:138	In this algorithm, the weights correspond to local factor features L and global factor features G are updated simultaneously." ></td>
	<td class="line x" title="63:138	Algorithm 2 Learning with Global Features for Argument Classication input Training set T = {xt,At}Tt=1, Number of iterations N and Parameter C w  0, v  0, c  1 for i  0 to N do for (xt,At)  T do let (xt,A) = PaA L(xt,a)+G(xt,A) generate n-best assignments {An} using FL A = argmaxA{An} w(xt,A)+ (At,A) t = min  C, w(xt, A)w(xt,At)+(At, A)||(x t,At)(xt, A)||2  w  w+ t((xt,At)(xt, A)) v  v+ ct((xt,At)(xt, A)) c  c +1 end for end for return wv/c We set the margin value (A, A) as the number of incorrect assignments plus (A, A), and C as 1.0." ></td>
	<td class="line x" title="64:138	The delta function returns 1 if at least one assignment is different from the correct assignment and 0 otherwise." ></td>
	<td class="line x" title="65:138	The model is similar to re-ranking (Toutanova et al., 2005; Johansson and Nugues, 2008)." ></td>
	<td class="line x" title="66:138	However in contrast to re-ranking, we only have to prepare one model." ></td>
	<td class="line x" title="67:138	The re-ranking approach requires other training datasets that are different from the data used in local model training." ></td>
	<td class="line x" title="68:138	3.2.2 Features for Argument Classication The local features used in our system are the same as our previous work (Watanabe et al., 2008) except forlanguagedependentfeatures." ></td>
	<td class="line x" title="69:138	Theglobalfeatures that used in our system are based on (Johansson and Nugues, 2008) that used for re-ranking." ></td>
	<td class="line x" title="70:138	Local Features Word features: Predicted lemma and predicted POS of the predicate, predicates head, argument candidate, argument candidates head, leftmost/rightmost dependent and leftmost/rightmost sibling." ></td>
	<td class="line x" title="71:138	Dependency label: The dependency label of predicate, argument candidate and argument candidates dependent." ></td>
	<td class="line x" title="72:138	Family: The position of the argument candicate with respect to the predicate position in the dependency tree (e.g. child, sibling)." ></td>
	<td class="line x" title="73:138	116 Average Catalan Chinese Czech English German Japanese Spanish Macro F1 Score 78.43 75.91 73.43 81.43 86.40 69.84 84.86 77.12 (78.00*) (74.83*) (73.43*) (81.38*) (86.40*) (68.39*) (84.84*) (76.74*) Semantic Labeled F1 75.65 72.35 74.17 84.69 84.26 63.66 77.93 72.50 (75.17*) (71.05*) (74.17*) (84.66*) (84.26*) (61.94*) (77.91*) (72.25*) Labeled Syntactic Accuracy 81.16 79.48 72.66 78.17 88.54 75.85 91.69 81.74 (80.77*) (78.62*) (72.66*) (78.10*) (88.54*) (74.60*) (91.66*) (81.23*) Macro F1 Score 84.30 84.79 81.63 83.08 87.93 83.25 85.54 83.94 Semantic Labeled F1 81.58 80.99 79.99 86.67 85.09 79.46 79.03 79.85 Labeled Syntactic Accuracy 87.02 88.59 83.27 79.48 90.77 87.03 91.96 88.04 Table 1: Scores of our system." ></td>
	<td class="line x" title="74:138	Position: The position of the head of the dependency relation with respect to the predicate position in the sentence." ></td>
	<td class="line x" title="75:138	Pattern: The left-to-right chain of the predicted POS/dependency labels of the predicates children." ></td>
	<td class="line x" title="76:138	Path features: Predicted lemma, predicted POS and dependency label paths between the predicate and the argument candidate." ></td>
	<td class="line x" title="77:138	Distance: The number of dependency edges between the predicate and the argument candidate." ></td>
	<td class="line x" title="78:138	Global Features Predicate-argument label sequence: The sequence of the predicate sense and argument labels in the predicate-argument strucuture." ></td>
	<td class="line x" title="79:138	Presence of labels dened in frame les: Whether the semantic roles dened in the frame present in the predicate-argument structure (e.g. MISSING:A1 or CONTAINS:A1.)" ></td>
	<td class="line x" title="80:138	3.2.3 Argument Pruning We observe that most arguments tend to be not far from its predicate, so we can prune argument candidates to reduce search space." ></td>
	<td class="line x" title="81:138	Since the characteristics of the languages are slightly different, we apply two types of pruning algorithms." ></td>
	<td class="line x" title="82:138	Pruning Algorithm 1: Let S be an argument candidate set." ></td>
	<td class="line x" title="83:138	Initially set S   and start at predicate node." ></td>
	<td class="line x" title="84:138	Add dependents of the node to S, and move current node to its parent." ></td>
	<td class="line x" title="85:138	Repeat until current node reaches to ROOT." ></td>
	<td class="line x" title="86:138	Pruning Algorithm 2: Same as the Algorithm 1 except that added nodes are its grandchildren as well as its dependents." ></td>
	<td class="line x" title="87:138	The pruning results are shown in Table 2." ></td>
	<td class="line x" title="88:138	Since we could not prune arguments in Japanese accuratelyusingthetwoalgorithms, weprunedargument candidates simply by POS." ></td>
	<td class="line x" title="89:138	algorithm coverage (%) reduction (%) Catalan 1 100 69.1 Chinese 1 98.9 69.1 Czech 2 98.5 49.1 English 1 97.3 63.1 German 1 98.3 64.3 Japanese POS 99.9 41.0 Spanish 1 100 69.7 Table 2: Pruning results." ></td>
	<td class="line x" title="90:138	4 Results The submitted results on the test data are shown in the upper part of Table 1." ></td>
	<td class="line x" title="91:138	Due to a bug, we mistakenly used the gold lemmas in the dependency parser." ></td>
	<td class="line x" title="92:138	Corrected results are shown in the part marked with *." ></td>
	<td class="line x" title="93:138	The lower part shows the post evaluation results with the gold lemmas and POSs." ></td>
	<td class="line x" title="94:138	For some of the 7 languages, since the global model described in Section 3.2 degraded performance compare to a model trained with only FL, we did NOT use the model for all languages." ></td>
	<td class="line x" title="95:138	We used the global model for only three languages: Chinese, English and Japanese." ></td>
	<td class="line x" title="96:138	The remaining languages (Catalan, Czech, German and Spanish) used a model trained with only FL." ></td>
	<td class="line x" title="97:138	4.1 Dependency Parsing Results The parser achieved relatively high accuracies for Czech, EnglishandJapanese, andforeachlanguage, the difference between the performance with correct POS and predicted POS is not so large." ></td>
	<td class="line x" title="98:138	However, in Catalan, Chinese German and Spanish, the parsing accuracies was seriously degraded by replacing correct POSs with predicted POSs (6.3 11.2 %)." ></td>
	<td class="line x" title="99:138	This is likely because these languages have relatively low predicted POS accuracies (92.3 95.5 %) ; Chinese 117 FL FL+FG (P, R) Catalan 85.80 85.68 (+0.01, -0.26) Chinese 86.58 87.39 (+0.24, +1.36) Czech 89.63 89.05 (-0.87, -0.28) English 85.66 85.74 (-0.87, +0.98) German 80.82 77.30 (-7.27, +0.40) Japanese 79.87 81.01 (+0.17, +1.88) Spanish 84.38 83.89 (-0.42, -0.57) Table 3: Effect of global features (semantic labeled F1)." ></td>
	<td class="line x" title="100:138	P and R denote the differentials of labeled precision and labeled recall between FL and FL+FG respectively." ></td>
	<td class="line x" title="101:138	has especially low accuracy (92.3%)." ></td>
	<td class="line x" title="102:138	The POS accuracy may affect the parsing performances." ></td>
	<td class="line x" title="103:138	4.2 SRL Results In order to highlight the effect of the global features, we compared two models." ></td>
	<td class="line x" title="104:138	The rst model is trained with only the local factor FL." ></td>
	<td class="line x" title="105:138	The second model is trained with both the local factor FL and the global factor FG." ></td>
	<td class="line x" title="106:138	The results are shown in Table 3." ></td>
	<td class="line x" title="107:138	In the experiments, we used the development data with gold parse trees." ></td>
	<td class="line x" title="108:138	For Chinese and Japanese, signicant improvements are obtained using the global features (over +1.0% in labeled recall and the slightly better labeled precision)." ></td>
	<td class="line x" title="109:138	However, for Catalan, Czech, German and Spanish, the global features degraded the performance in labeled F1." ></td>
	<td class="line x" title="110:138	Especially, in German, the precision is substantially degraded (-7.27% in labeled F1)." ></td>
	<td class="line x" title="111:138	These results indicatethatitisnecessarytointroducelanguagedependent features." ></td>
	<td class="line x" title="112:138	4.3 Training, Evaluation Time and Memory Requirements Table 4 and 5 shows the training/evaluation times and the memory consumption of the second-order dependency parsers and the global argument classiers respectively." ></td>
	<td class="line x" title="113:138	The training times of the predicate classier were less than one day, and the testing times were mere seconds." ></td>
	<td class="line x" title="114:138	As reported in (Carreras, 2007; Johansson and Nugues, 2008), training and inference of the secondorder parser are very expensive." ></td>
	<td class="line x" title="115:138	For Chinese, we could only complete 2 iterations." ></td>
	<td class="line x" title="116:138	In terms of the argument classier, since N-best generation time account for a substantial proportion of the training time (in this work N = 100), changiter hrs./iter sent./min." ></td>
	<td class="line x" title="117:138	mem." ></td>
	<td class="line x" title="118:138	Catalan 9 14.6 9.0 9.6 GB Chinese 2 56.5 3.7 16.2 GB Czech 8 14.6 20.5 12.6 GB English 7 22.0 13.4 15.1 GB German 4 12.3 59.1 13.1 GB Japanese 7 11.2 21.8 13.0 GB Spanish 7 19.5 7.3 17.9 GB Table 4: Training, evaluation time and memory requirements of the second-order dependency parsers." ></td>
	<td class="line x" title="119:138	The iter column denote the number of iterations of the model used for the evaluations." ></td>
	<td class="line x" title="120:138	Catalan, Czech and English are trained on Xeon 3.0GHz, Chinese and Japanese are trained on Xeon 2.66GHz, German and Spanish are trained on Opteron 2.3GHz machines." ></td>
	<td class="line x" title="121:138	train (hrs.)" ></td>
	<td class="line x" title="122:138	sent./min." ></td>
	<td class="line x" title="123:138	mem." ></td>
	<td class="line x" title="124:138	Chinese 6.5 453.7 2.0 GB English 13.5 449.8 3.2 GB Japanese 3.5 137.6 1.1 GB Table 5: Training, evaluation time and memory requirements of the global argument classiers." ></td>
	<td class="line x" title="125:138	The classiers are all trained on Opteron 2.3GHz machines." ></td>
	<td class="line x" title="126:138	ing N affects the training and evaluation times signicantly." ></td>
	<td class="line x" title="127:138	All modules of our system are implemented in Java." ></td>
	<td class="line x" title="128:138	The required memory spaces shown in Table 4 and 5 are calculated by subtracting free memory size from the total memory size of the Java VM." ></td>
	<td class="line x" title="129:138	Note that we observed that the value uctuated drastically while measuring memory usage, so the value may not indicate precise memory requirements of our system." ></td>
	<td class="line x" title="130:138	5 Conclusion In this paper, we have described our system for syntactic and semantic dependency analysis in multilingual." ></td>
	<td class="line x" title="131:138	Although our system is not a joint approach but a pipeline approach, the system is comparable to the top system for some of the 7 languages." ></td>
	<td class="line x" title="132:138	A further research direction we are investigating is the application of various types of global features." ></td>
	<td class="line x" title="133:138	We believe that there is still room for improvements since we used only two types of global features for the argument classier." ></td>
	<td class="line x" title="134:138	Another research direction is investigating joint approaches." ></td>
	<td class="line x" title="135:138	To the best of our knowledge, three 118 types of joint approaches have been proposed: N-best based approach (Johansson and Nugues, 2008), synchronous joint approach (Henderson et al., 2008), and a joint approach where parsing and SRL are performed simultaneously (Llus and M`arquez, 2008)." ></td>
	<td class="line x" title="136:138	We attempted to perform Nbest based joint approach, however, the expensive computational cost of the 2nd-order projective parser discouraged it." ></td>
	<td class="line x" title="137:138	We would like to investigate syntactic-semantic joint approaches with reasonable time complexities." ></td>
	<td class="line x" title="138:138	Acknowledgments We would like to thank Richard Johansson for his advice on parser implementation, and the CoNLL2009 organizers (Hajic et al., 2009; Taule et al., 2008; Palmer and Xue, 2009; Hajic et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006; Kawahara et al., 2002; Taule et al., 2008)." ></td>
</tr></table>
</div
</body></html>
