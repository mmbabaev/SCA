<html><body><head><link rel="stylesheet" type="text/css" href="style.css" /><script src="map.js"></script><script src="jquery-1.7.1.min.js"></script></head>
<div class="dstPaperData">
A92-1018 <div class="dstPaperTitle">A Practical Part-Of-Speech Tagger</div><div class="dstPaperAuthors">Cutting, Douglass;Kupiec, Julian;Pedersen, Jan;Sibun, Penelope;</div>
</div>
<table cellspacing="0" cellpadding="0"><tr>
	<td class="srcData" >Source Paper</td>
	<td class="pp legend" ><input type="checkbox" id="cbIPositive" checked="true"/><label for="cbIPositive">Informal +<label></td>
	<td class="nn legend" ><input type="checkbox" id="cbINegative" checked="true"/><label for="cbINegative">Informal -<label></td>
	<td class="oo legend" ><input type="checkbox" id="cbIObjective" checked="true"/><label for="cbIObjective">Informal Neutral<label></td>
	<td class="ppc legend" ><input type="checkbox" id="cbEPositive" checked="true"/><label for="cbEPositive">Formal +</label></td>
	<td class="nnc legend" ><input type="checkbox" id="cbENegative" checked="true"/><label for="cbENegative">Formal -</label></td>
	<td class="ooc legend" ><input type="checkbox" id="cbEObjective" checked="true"/><label for="cbEObjective">Formal Neutral</label></td>
	<td class="lb"><input type="checkbox" id="cbSentenceBoundary"/><label for="cbSentenceBoundary">Sentence Boundary</label></td>
</tr></table>
<div class="dstPaper">
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="J93-1002
Generalized Probabilistic LR Parsing Of Natural Language (Corpora) With Unification-Based Grammars
Briscoe, Ted;Carroll, John A.;"></td>
	<td class="line x" title="1:516	Generalized Probabilistic LR Parsing of Natural Language (Corpora) with Unification-Based Grammars Ted Briscoe* University of Cambridge John Carroll* University of Cambridge We describe work toward the construction of a very wide-coverage probabilistic parsing system for natural language (NL), based on LR parsing techniques." ></td>
	<td class="line x" title="2:516	The system is intended to rank the large number of syntactic analyses produced by NL grammars according to the frequency of occurrence of the individual rules deployed in each analysis." ></td>
	<td class="line x" title="3:516	We discuss a fully automatic procedure for constructing an LR parse table from a unification-based grammar formalism, and consider the suitability of alternative LALR(1) parse table construction methods for large grammars." ></td>
	<td class="line x" title="4:516	The parse table is used as the basis for two parsers; a user-driven interactive system that provides a computationally tractable and labor-efficient method of supervised training of the statistical information required to drive the probabilistic parser." ></td>
	<td class="line x" title="5:516	The latter is constructed by associating probabilities with the LR parse table directly." ></td>
	<td class="line x" title="6:516	This technique is superior to parsers based on probabilistic lexical tagging or probabilistic context-free grammar because it allows for a more context-dependent probabilistic language model, as well as use of a more linguistically adequate grammar formalism." ></td>
	<td class="line x" title="7:516	We compare the performance of an optimized variant of Tomita's (1987) generalized LR parsing algorithm to an (efficiently indexed and optimized) chart parser." ></td>
	<td class="line x" title="8:516	We report promising results of a pilot study training on 150 noun definitions from the Longman Dictionary of Contemporary English (LDOCE) and retesting on these plus a further 55 definitions." ></td>
	<td class="line x" title="9:516	Finally, we discuss limitations of the current system and possible extensions to deal with lexical (syntactic and semantic)frequency of occurrence." ></td>
	<td class="line x" title="10:516	1." ></td>
	<td class="line x" title="11:516	Wide-Coverage Parsing of Natural Language The task of syntactically analyzing substantial corpora of naturally occurring text and transcribed speech has become a focus of recent work." ></td>
	<td class="line x" title="12:516	Analyzed corpora would be of great benefit in the gathering of statistical data regarding language use, for example to train speech recognition devices, in more general linguistic research, and as a first step toward robust wide-coverage semantic interpretation." ></td>
	<td class="line x" title="13:516	The Alvey Natural Language Tools (ANLT) system is a wide-coverage lexical, morphological, and syntactic analysis system for English (Briscoe et al. 1987)." ></td>
	<td class="line x" title="14:516	Previous work has demonstrated that the ANLT system is, in principle, able to assign the correct parse to a high proportion of English noun phrases drawn from a variety of corpora." ></td>
	<td class="line x" title="15:516	The goal of the work reported here is to develop a practical parser capable of returning probabilistically highly ranked analyses (from the usually large number of syntactically legitimate possibilities) for material drawn from a specific corpus on the basis of minimal (supervised) training and manual modification." ></td>
	<td class="line x" title="16:516	* University of Cambridge, Computer Laboratory, Pembroke Street, Cambridge, CB2 3QG, UK." ></td>
	<td class="line x" title="17:516	(Tel." ></td>
	<td class="line x" title="18:516	+44-223-334600), (ejb / jac @cl.cam.ac.uk)." ></td>
	<td class="line x" title="19:516	(~) 1993 Association for Computational Linguistics Computational Linguistics Volume 19, Number 1 The first issue to consider is what the analysis will be used for and what constraints this places on its form." ></td>
	<td class="line x" title="20:516	The corpus analysis literature contains a variety of proposals, ranging from part-of-speech tagging to assignment of a unique, sophisticated syntactic analysis." ></td>
	<td class="line x" title="21:516	Our eventual goal is to recover a semantically and pragmatically appropriate syntactic analysis capable of supporting semantic interpretation." ></td>
	<td class="line x" title="22:516	Two stringent requirements follow immediately: firstly, the analyses assigned must determinately represent the syntactic relations that hold between all constituents in the input; secondly, they must be drawn from an a priori defined, well-formed set of possible syntactic analyses (such as the set defined by a generative grammar)." ></td>
	<td class="line x" title="23:516	Otherwise, semantic interpretation of the resultant analyses cannot be guaranteed to be (structurally) unambiguous, and the semantic operations defined (over syntactic configurations) cannot be guaranteed to match and yield an interpretation." ></td>
	<td class="line x" title="24:516	These requirements immediately suggest that approaches that recover only lexical tags (e.g. de Rose 1988) or a syntactic analysis that is the 'closest fit' to some previously defined set of possible analyses (e.g. Sampson, Haigh, and Atwell 1989), are inadequate (taken alone)." ></td>
	<td class="line x" title="25:516	' Pioneering approaches to corpus analysis proceeded on the assumption that computationally tractable generative grammars of sufficiently general coverage could not be developed (see, for example, papers in Garside, Leech, and Sampson 1987)." ></td>
	<td class="line x" title="26:516	However, the development of wide-coverage declarative and computationally tractable grammars makes this assumption questionable." ></td>
	<td class="line x" title="27:516	For example, the ANLT word and sentence grammar (Grover et al. 1989; Carroll and Grover 1989) consists of an English lexicon of approximately 40,000 lexemes and a 'compiled' fixed-arity term unification grammar containing around 700 phrase structure rules." ></td>
	<td class="line x" title="28:516	Taylor, Grover, and Briscoe (1989) demonstrate that an earlier version of this grammar was capable of assigning the correct analysis to 96.8% of a corpus of 10,000 noun phrases extracted (without regard for their internal form) from a variety of corpora." ></td>
	<td class="line x" title="29:516	However, although Taylor, Grover, and Briscoe show that the ANLT grammar has very wide coverage, they abstract away from issues of lexical idiosyncrasy by formimg equivalence classes of noun phrases and parsing a single token of each class, and they do not address the issues of 1) tuning a grammar to a particular corpus or sublanguage 2) selecting the correct analysis from the set licensed by the grammar and 3) providing reliable analyses of input outside the coverage of the grammar." ></td>
	<td class="line x" title="30:516	Firstly, it is clear that vocabulary, idiom, and conventionalized constructions used in, say, legal language and dictionary definitions, will differ both in terms of the range and frequency of words and constructions deployed." ></td>
	<td class="line x" title="31:516	Secondly, Church and Patil (1982) demonstrate that for a realistic grammar parsing realistic input, the set of possible analyses licensed by the grammar can be in the thousands." ></td>
	<td class="line x" title="32:516	Finally, it is extremely unlikely that any generative grammar will ever be capable of correctly analyzing all naturally occurring input, even when tuned for a particular corpus or sublanguage (if only because of the synchronic idealization implicit in the assumption that the set of grammatical sentences of a language is well formed)." ></td>
	<td class="line x" title="33:516	In this paper, we describe our approach to the first and second problems and make some preliminary remarks concerning the third (far harder) problem." ></td>
	<td class="line x" title="34:516	Our approach to grammar tuning is based on a semi-automatic parsing phase during which additions to the grammar are made manually and statistical information concerning the frequency of use of grammar rules is acquired." ></td>
	<td class="line x" title="35:516	Using this statistical information and modified grammar, a breadth-first probabilistic parser is constructed." ></td>
	<td class="line x" title="36:516	The latter is capable of ranking the possible parses identified by the grammar in a useful (and efficient) manner." ></td>
	<td class="line x" title="37:516	However, (unseen) sentences whose correct analysis is outside the coverage of the grammar ren,.~in a problem." ></td>
	<td class="line x" title="38:516	The feasibility and usefulness of our approach has been investigated in a preliminary way by analyzing a ~small corpus of 26 Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing noun definitions drawn from the Longman Dictionary of Contemporary English (LDOCE) (Procter 1978)." ></td>
	<td class="line x" title="39:516	This corpus was chosen because the vocabulary employed is restricted (to approximately 2,000 morphemes), average definition length is about 10 words (with a maximum of around 30), and each definition is independent, allowing us to ignore phenomena such as ellipsis." ></td>
	<td class="line x" title="40:516	In addition, the language of definitions represents a recognizable sublanguage, allowing us to explore the task of tuning a general purpose grammar." ></td>
	<td class="line x" title="41:516	The results reported below suggest that probabilistic information concerning the frequency of occurrence of syntactic rules correlates in a useful (though not absolute) way with the semantically and pragmatically most plausible analysis." ></td>
	<td class="line x" title="42:516	In Section 2, we briefly review extant work on probabilistic approaches to corpus analysis and parsing and argue the need for a more refined probabilistic model to distinguish distinct derivations." ></td>
	<td class="line x" title="43:516	Section 3 discusses work on LR parsing of natural language and presents our technique for automatic construction of LR parsers for unification-based grammars." ></td>
	<td class="line x" title="44:516	Section 4 presents the method and results for constructing a LALR(1) parse table for the ANLT grammar and discusses these in the light of both computational complexity and other empirical results concerning parse table size and construction time." ></td>
	<td class="line x" title="45:516	Section 5 motivates our interactive and incremental approach to semi-automatic production of a disambiguated training corpus and describes the variant of the LR parser used for this task." ></td>
	<td class="line x" title="46:516	Section 6 describes our implementation of a breadth-first LR parser and compares its performance empirically to a highly optimized chart parser for the same grammar, suggesting that (optimized) LR parsing is more efficient in practice for the ANLT grammar despite exponential worst case complexity results." ></td>
	<td class="line x" title="47:516	Section 7 explains the technique we employ for deriving a probabilistic version of the LR parse table from the training corpus, and demonstrates that this leads to a more refined and parse-context-dependent probabilistic model capable of distinguishing derivations that in a probabilistic context-free model would be equally probable." ></td>
	<td class="line x" title="48:516	Section 8 describes and presents the results of our first experiment parsing LDOCE noun definitions, and Section 9 draws some preliminary conclusions and outlines ways in which the work described should be modified and extended." ></td>
	<td class="line x" title="49:516	2." ></td>
	<td class="line x" title="50:516	Probabilistic Approaches to Parsing In the field of speech recognition, statistical techniques based on hidden Markov modeling are well established (see e.g. Holmes 1988:129f for an introduction)." ></td>
	<td class="line x" title="51:516	The two main algorithms utilized are the Viterbi (1967) algorithm and the ABCD Baum-Welch algorithm (Baum 1972)." ></td>
	<td class="line x" title="52:516	These ABCD algorithms provide polynomial solutions to the tasks of finding the most probable derivation for a given input and a stochastic regular grammar, and of performing iterative re-estimation of the parameters of a (hidden) stochastic regular grammar by considering all possible derivations over a corpus of inputs, respectively." ></td>
	<td class="line x" title="53:516	Baker (1982) demonstrates that Baum-Welch re-estimation can be extended to context-free grammars (CFGs) in Chomsky Normal Form (CNF)." ></td>
	<td class="line x" title="54:516	Fujisaki et al.(1989) demonstrate that the Viterbi algorithm can be used in conjunction with the CYK parsing algorithm and a CFG in CNF to efficiently select the most probable derivation of a given input." ></td>
	<td class="line x" title="56:516	Kupiec (1991) extends Baum-Welch re-estimation to arbitrary (nonCNF) CFGs." ></td>
	<td class="line x" title="57:516	Baum-Welch re-estimation can be used with restricted or unrestricted grammars/models in the sense that some of the parameters corresponding to possible productions over a given (non-)terminal category set/set of states can be given an initial probability of zero." ></td>
	<td class="line x" title="58:516	Unrestricted grammars/models quickly become impractical because the number of parameters requiring estimation becomes large and these algorithms are polynomial in the length of the input and number of free parameters." ></td>
	<td class="line x" title="59:516	27 Computational Linguistics Volume 19, Number 1 Typically, in applications of Markov modeling in speech recognition, the derivation used to analyze a given input is not of interest; rather what is sought is the best (most likely) model of the input." ></td>
	<td class="line x" title="60:516	In any application of these or similar techniques to parsing, though, the derivation selected is of prime interest." ></td>
	<td class="line x" title="61:516	Baum (1972) proves that Baum-Welch re-estimation will converge to a local optimum in the sense that the initial probabilities will be modified to increase the likelihood of the corpus given the grammar and 'stabilize' within some threshold after a number of iterations over the training corpus." ></td>
	<td class="line x" title="62:516	However, there is no guarantee that the global optimum will be found, and the a priori initial probabilities chosen are critical for convergence on useful probabilities (e.g. Lari and Young 1990)." ></td>
	<td class="line oc" title="63:516	The main application of these techniques to written input has been in the robust, lexical tagging of corpora with part-of-speech labels (e.g. Garside, Leech, and Sampson 1987; de Rose 1988; Meteer, Schwartz, and Weischedel 1991; Cutting et al. 1992)." ></td>
	<td class="line x" title="64:516	Fujisaki et al.(1989) describe a corpus analysis experiment using a probabilistic CNF CFG containing 7550 rules on a corpus of 4206 sentences (with an average sentence length of approximately 11 words)." ></td>
	<td class="line x" title="66:516	The unsupervised training process involved automatically assigning probabilities to each CF rule on the basis of their frequency of occurrence in all possible analyses of each sentence of the corpus." ></td>
	<td class="line x" title="67:516	These probabilities were iteratively re-estimated using a variant of the Baum-Welch algorithm, and the Viterbi algorithm was used in conjunction with the CYK parsing algorithm to efficiently select the most probable analysis after training." ></td>
	<td class="line x" title="68:516	Thus the model was restricted in that many of the possible parameters (rules) defined over the (non-)terminal category set were initially set to zero and training was used only to estimate new probabilities for a set of predefined rules." ></td>
	<td class="line x" title="69:516	Fujisaki et al. suggest that the stable probabilities will model semantic and pragmatic constraints in the corpus, but this will only be so if these correlate with the frequency of rules in correct analyses, and also if the 'noise' in the training data created by the incorrect parses is effectively factored out." ></td>
	<td class="line x" title="70:516	Whether this is so will depend on the number of 'false positive' examples with only incorrect analyses, the degree of heterogeneity in the training corpus, and so forth." ></td>
	<td class="line x" title="71:516	Fujisaki et al. report some results based on testing the parser on the corpus used for training." ></td>
	<td class="line x" title="72:516	In 72 out of 84 sentences examined, the most probable analysis was also the correct analysis." ></td>
	<td class="line x" title="73:516	Of the remainder, 6 were false positives and did not receive a correct parse, while the other 6 did but it was not the most probable." ></td>
	<td class="line x" title="74:516	A success rate (per sentence) of 85% is apparently impressive, but it is difficult to evaluate properly in the absence of full details concerning the nature of the corpus." ></td>
	<td class="line x" title="75:516	For example, if the corpus contains many simple and similar constructions, unsupervised training is more likely to converge quickly on a useful set of probabilities." ></td>
	<td class="line x" title="76:516	Sharman, Jelinek, and Mercer (1990) conducted a similar experiment with a grammar in ID/LP format (Gazdar et al. 1985; Sharman 1989)." ></td>
	<td class="line x" title="77:516	ID/LP grammars separate the two types of information encoded in CF rules--immediate dominance and immediate precedence--into two rule types that together define a CFG." ></td>
	<td class="line x" title="78:516	This allows probabilities concerning dominance, associated with ID rules, to be factored out from those concerning precedence, associated with LP rules." ></td>
	<td class="line x" title="79:516	In this experiment, a supervised training regime was employed." ></td>
	<td class="line x" title="80:516	A grammar containing 100 terminals and 16 nonterminals and initial probabilities based on the frequency of ID and LP relations was extracted from a manually parsed corpus of about one million words of text." ></td>
	<td class="line x" title="81:516	The resulting probabilistic ID/LP grammar was used to parse 42 sentences of 30 words or less drawn from the same corpus." ></td>
	<td class="line x" title="82:516	In addition, lexical syntactic probabilities were integrated with the probability of the ID/LP relations to rank parses." ></td>
	<td class="line x" title="83:516	Eighteen of the parses were identical to the original manual analyses, while a further 19 were 'similar,' yielding a success rate of 88%." ></td>
	<td class="line x" title="84:516	What is noticeable about this experiment is that the results are no better 28 Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing than Fujisaki et al.'s unsupervised training experiment discussed above, despite the use of supervised training and a more sophisticated grammatical model." ></td>
	<td class="line x" title="85:516	It is likely that these differences derive from the corpus material used for training and testing, and that the results reported by Fujisaki et al. will not be achieved with all corpora." ></td>
	<td class="line x" title="86:516	Pereira and Schabes (1992) report an experiment using Baum-Welch re-estimation to infer a grammar and associated rule probabilities from a category set containing 15 nonterminals and 48 terminals, corresponding to the Penn Treebank lexical tagset (Santorini 1990)." ></td>
	<td class="line x" title="87:516	The training data was 770 sentences, represented as tag sequences, drawn from the treebank." ></td>
	<td class="line x" title="88:516	They trained the system in an unsupervised mode and also in a 'semi-supervised' mode, in which the manually parsed version of the corpus was used to constrain the set of analyses used during re-estimation." ></td>
	<td class="line x" title="89:516	In supervised training analyses were accepted if they produced bracketings consistent but not necessarily identical with those assigned manually." ></td>
	<td class="line x" title="90:516	They demonstrate that in supervised mode, training not only converges faster but also results in a grammar in which the most probable analysis is compatible with the manually assigned analysis of further test sentences drawn from the tree bank in a much greater percentage of cases--78% as opposed to 35%." ></td>
	<td class="line x" title="91:516	This result indicates very clearly the importance of supervised training, particularly in a context where the grammar itself is being inferred in addition to the probability of individual rules." ></td>
	<td class="line x" title="92:516	In our work, we are concerned to utilize the existing wide-coverage ANLT grammar; therefore, we have concentrated initially on exploring how an adequate probabilistic model can be derived for a unification-based grammar and trained in a supervised mode to effectively select useful analyses from the large space of syntactically legitimate possibilities." ></td>
	<td class="line x" title="93:516	There are several inherent problems with probabilistic CFG (including ID/LP)-based systems." ></td>
	<td class="line x" title="94:516	Firstly, although CFG is an adequate model of the majority of constructions occurring in natural language (Gazdar and Mellish 1989), it is clear that wide-coverage CFGs will need to be very large indeed, and this will lead to difficulties of (manual) development of consistent grammars and, possibly, to computational intractability at parse time (particularly during the already computationally expensive training phase)." ></td>
	<td class="line x" title="95:516	Secondly, associating probabilities with CF rules means that information about the probability of a rule applying at a particular point in a parse derivation is lost." ></td>
	<td class="line x" title="96:516	This leads to complications distinguishing the probability of different derivations when the same rule can be applied several times in more than one way." ></td>
	<td class="line x" title="97:516	Grammar 1 below is an example of a probabilistic CFG, in which each production is associated with a probability and the probabilities of all rules expanding a given nonterminal category sum to one." ></td>
	<td class="line x" title="98:516	Grammar 1 i) s' -~ S (i.O) 2) S -~ NP VP (I.0) 3) VP -~ Vt NP (.4) 4) VP -+ Vi (.6) 5) NP -~ ProNP (.4) 6) NP -+ Det N (.3) 7) NP -~ NP PP (.3) 8) N --+ N N (.3) 9) PP --+ P NP (1.0) ~o) N -+ N (.7) 29 Computational Linguistics Volume 19, Number 1 a) s NP VP Det N Vt NP I I N@ ProNP b) d) e) N N N N N c) N N N N N 0 NP NP NP pp NP pp NP P NP NP PP ProNP NP PP P NP Figure 1 Probabilistic context-free derivations." ></td>
	<td class="line x" title="99:516	VP vt NP Det N I The probability of a particular parse is the product of the probabilities of each rule used in the derivation." ></td>
	<td class="line x" title="100:516	Thus the probability of parse a) in Figure 1 is 0.0336." ></td>
	<td class="line x" title="101:516	The probability of parse b) or c) must be identical though (0.09), because the same rule is applied twice in each case." ></td>
	<td class="line x" title="102:516	Similarly, the probability of d) and e) is also identical (0.09) for essentially the same reason." ></td>
	<td class="line x" title="103:516	However, these rules are natural treatments of noun compounding and prepositional phrase (PP) attachment in English, and the different derivations correlate with different interpretations." ></td>
	<td class="line x" title="104:516	For example, b) would be an appropriate analysis for toy coffee grinder, while c) would be appropriate for cat food tin, and each of d) and e) yields one of the two possible interpretations of the man in the park with the telescope." ></td>
	<td class="line x" title="105:516	We want to keep these structural configurations probabilistically distinct in case there are structurally conditioned differences in their frequency of occurrence; as would be predicted, for example, by the theory of parsing strategies (e.g. Frazier 1988)." ></td>
	<td class="line x" title="106:516	Fujisaki et al.(1989) propose a rather inelegant solution for the noun compound case, which involves creating 5582 instances of 4 morphosyntactically identical rules for classes of word forms with distinct bracketing behavior in noun-noun compounds." ></td>
	<td class="line x" title="108:516	However, we would like to avoid enlarging the grammar and eventually to integrate probabilistic lexical information with probabilistic structural information in a more modular fashion." ></td>
	<td class="line x" title="109:516	Probabilistic CFGs also will not model the context dependence of rule use; for example, an NP is more likely to be expanded as a pronoun in subject position than elsewhere (e.g. Magerman and Marcus 1991), but only one global probability can be associated with the relevant CF production." ></td>
	<td class="line x" title="110:516	Thus the probabilistic CFG model predicts (incorrectly) that a) and f) will have the same probability of occurrence." ></td>
	<td class="line x" title="111:516	These considerations suggest that we need a technique that allows use of a more adequate grammatical formalism than CFG and a more context-dependent probabilistic model." ></td>
	<td class="line x" title="112:516	Our approach is to use the LR parsing technique as a natural way to obtain a finitestate representation of a non-finite-state grammar incorporating information about parse context." ></td>
	<td class="line x" title="113:516	In the following sections, we introduce the LR parser and in Section 8 30 Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing we demonstrate that LR parse tables do provide an appropriate amount of contextual information to solve the problems described above." ></td>
	<td class="line x" title="114:516	3." ></td>
	<td class="line x" title="115:516	LR Parsing in a Unification-Based Grammar Framework The heart of the LR parsing technique is the parse table construction algorithm, which is the most complex and computationally expensive aspect of LR parsing." ></td>
	<td class="line x" title="116:516	Much of the attraction of the technique stems from the fact that the real work takes place in a precompilation phase and the run time behavior of the resulting parser is relatively simple and directed." ></td>
	<td class="line x" title="117:516	An LR parser finds the 'rightmost derivation in reverse,' for a given string and CF grammar." ></td>
	<td class="line x" title="118:516	The precompilation process results in a parser control mechanism that enables the parser to identify the 'handle,' or appropriate substring in the input to reduce, and the appropriate rule of the grammar with which to perform the reduction." ></td>
	<td class="line x" title="119:516	The control information is standardly encoded as a parse table with rows representing parse states, and columns terminal and nonterminal symbols of the grammar." ></td>
	<td class="line x" title="120:516	This representation defines a finite-state automaton." ></td>
	<td class="line x" title="121:516	Figure 2 gives the LALR(1) parse table for Grammar 1." ></td>
	<td class="line x" title="122:516	(LALR(1) is the most commonly used variant of LR since it usually provides the best trade-off between directed rule invocation and parse table size)." ></td>
	<td class="line x" title="123:516	If the grammar is in the appropriate LR class (a stronger restriction than being an unambiguous CFG), the automaton will be deterministic; however, some algorithms for parse table construction are also able to build nondeterministic automata containing action conflicts for ambiguous CFGs." ></td>
	<td class="line x" title="124:516	Parse table construction is discussed further in Section 4." ></td>
	<td class="line x" title="125:516	Actions State $ Det N@ P ProNP Vi Vt 0 s3 s2  1 rl  2 r5 r5 r5 r5  3 s4  4 rl0 rl0 r10 rl0 rl0  5 r6 s4 r6 r6 r6  6 r8 rS/s4 r8 r8 r8  7 s8 s13 sll  8 s3 s2  9 r9 rg/s8 r9 r9  i0 r7 r7 r7 r7  ii s3 s2  12 r3 s8  13 r4  14 r2  15 acc Figure 2 LALR(1) parse table for Grammar 1." ></td>
	<td class="line x" title="126:516	Gotos State N NP PP S S' VP  0 7 1 15  1  2  3 5  4  5 6  6 6  7 I0 14  8 9  9 i0  i0  ii 12  12 i0  13  14  15 31 Computational Linguistics Volume 19, Number 1 3.1 Creating LR Parse Tables from Unification Grammars Tomita (1987) describes a system for nondeterministic LR parsing of context-free grammars consisting of atomic categories, in which each CF production may be augmented with a set of tests (which perform similar types of operations to those available in a unification grammar)." ></td>
	<td class="line x" title="127:516	At parse time, whenever a sequence of constituents is about to be reduced into a higher-level constituent using a production, the augmentation associated with the production is invoked to check syntactic or semantic constraints such as agreement, pass attribute values between constituents, and construct a representation of the higher-level constituent." ></td>
	<td class="line x" title="128:516	(This is the standard approach to parsing with attribute grammars)." ></td>
	<td class="line x" title="129:516	The parser is driven by an LR parse table; however, the table is constructed solely from the CF portion of the grammar, and so none of the extra information embodied in the augmentations is taken into account during its construction." ></td>
	<td class="line x" title="130:516	Thus the predictive power of the parser to select the appropriate rule given a specific parse history is limited to the CF portion of the grammar, which must be defined manually by the grammar writer." ></td>
	<td class="line x" title="131:516	This requirement places a greater load on the grammar writer and is inconsistent with most recent unification-based grammar formalisms, which represent grammatical categories entirely as feature bundles (e.g. Gazdar et al. 1985; Pollard and Sag 1987; Zeevat, Calder, and Klein 1987)." ></td>
	<td class="line x" title="132:516	In addition, it violates the principle that grammatical formalisms should be declarative and defined independently of parsing procedure, since different definitions of the CF portion of the grammar will, at least, effect the efficiency of the resulting parser and might, in principle, lead to nontermination on certain inputs in a manner similar to that described by Shieber (1985)." ></td>
	<td class="line x" title="133:516	In what follows, we will assume that the unification-based grammars we are considering are represented in the ANLT object grammar formalism (Briscoe et al. 1987)." ></td>
	<td class="line x" title="134:516	This formalism is a notational variant of Definite Clause Grammar (e.g. Pereira and Warren 1980), in which rules consist of a mother category and one or more daughter categories, defining possible phrase structure configurations." ></td>
	<td class="line x" title="135:516	Categories consist of sets of feature name-value pairs, with the possibility of variable values, which may be bound within a rule, and of category-valued features." ></td>
	<td class="line x" title="136:516	Categories are combined using fixed-arity term unification (Prolog-style)." ></td>
	<td class="line x" title="137:516	The results and techniques we report below should generalize to many other unification-based formalisms." ></td>
	<td class="line x" title="138:516	An example of a possible ANLT object grammar rule is: IN -, V +, BAR 2, PER x, PLU y, VFORM z\] --+ \[N +, V -, BAR 2, PER x, PLU y, CASE Nom\] \[N -, V +, BAR i, PER x, PLU y, VFORM z\] This rule provides a (simple) analysis of the structure of English clauses, corresponding to S --* NP VP, using a feature system based loosely on that of GPSG (Gazdar et al. 1985)." ></td>
	<td class="line x" title="139:516	In Tomita's LR parsing framework, each such rule must be manually converted into a rule of the following form in which some subpart of each category has been replaced by an atomic symbol." ></td>
	<td class="line x" title="140:516	Vb\[BAR 2, PER x, PLU y, VFOKM z\] -~ Nn\[BAR 2, PER x, PLU y, CASE Nom\] Vb\[BAR I, PER x, PLU y, VFORM z\] However, it is not obvious which features should be so replaced--why not include BAR and CASE?" ></td>
	<td class="line x" title="141:516	It will be difficult for the grammar writer to make such substitutions in a consistent way, and still more difficult to make them in an optimal way for the purposes of LR parsing, since both steps involve consideration and comparison of all the categories mentioned in each rule of the grammar." ></td>
	<td class="line x" title="142:516	32 Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing Constructing the LR parse table directly and automatically from a unification grammar would avoid these drawbacks." ></td>
	<td class="line x" title="143:516	In this case, the LR parse table would be based on complex categories, with unification of complex categories taking the place of equality of atomic ones in the standard LR parse table construction algorithm (Osborne 1990; Nakazawa 1991)." ></td>
	<td class="line x" title="144:516	However, this approach is computationally prohibitively expensive: Osborne (1990:26) reports that his implementation (in HP Common Lisp on a Hewlett Packard 9000/350) takes almost 24 hours to construct the LR(0) states for a unification grammar of just 75 productions." ></td>
	<td class="line x" title="145:516	3.2 Constructing a CF Backbone from a Unification Grammar Our approach, described below, not only extracts unification information from complex categories, but is computationally tractable for realistic sized grammars and also safe from inconsistency." ></td>
	<td class="line x" title="146:516	We start with a unification grammar and automatically construct a CF 'backbone' of rules containing categories with atomic names and an associated 'residue' of feature name-value pairs." ></td>
	<td class="line x" title="147:516	Each backbone grammar rule is generally in direct one-to-one correspondence with a single unification grammar rule." ></td>
	<td class="line x" title="148:516	The LR parse table is then constructed from the CF backbone grammar." ></td>
	<td class="line x" title="149:516	The parser is driven by this table, but in addition when reducing a sequence of constituents the parser performs the unifications specified in the relevant unification grammar rule to form the category representing the higher-level constituent, and the derivation fails if one of the unifications fails." ></td>
	<td class="line x" title="150:516	Our parser is thus similar to Tomita's (1987), except that it performs unifications rather than invoking CF rule augmentations; however, the main difference between our approach and Tomita's is the way in which the CF grammar that drives the parser comes into being." ></td>
	<td class="line x" title="151:516	Even though a unification grammar will be, at best, equivalent to a very large (and at worst, if features are employed in recursive or cyclic ways, possibly infinite) set of atomic-category CF productions, in practice we have obtained LR parsers that perform well from backbone grammars containing only about 30% more productions than the original unification grammar." ></td>
	<td class="line x" title="152:516	The construction method ensures that for any given grammar the CF backbone captures at least as much information as the optimal CFG that contains the same number of rules as the unification grammar." ></td>
	<td class="line x" title="153:516	Thus the construction method guarantees that the resulting LR parser will terminate and will be as predictive as the source grammar in principle allows." ></td>
	<td class="line x" title="154:516	Building the backbone grammar is a two-stage process: . Compute the largest maximally specific set (in terms of subsumption) of disjoint categories covering the whole grammar and assign to each category a distinct atomic category name." ></td>
	<td class="line x" title="155:516	That is: initialize disjoint-set to be empty; for each category C in grammar let disjoint-merge be the categories in disjoint-set which unify with C; if disjoint-merge is empty then add C to disjoint-set; else replace all elements of disjoint-merge in disjoint-set with the single most specific category which subsumes C and all categories in disjoint-merge; assign a distinct name to each category in disjoint-set." ></td>
	<td class="line x" title="156:516	33 Computational Linguistics Volume 19, Number 1 IN -, V +, BAR 2, PER x, PLU y, VFORM z\] --> \[N +, V -, BAR 2, PER x, PLU y, CASE Nom\] \[N -, V +, BAR i, PER x, PLU y, VFOKM z\] IN +, V -, BAR 2, PER x, PLU y, CASE c\] --> \[SUBCAT DET\] \[N +, V -, BAR I, PER x, PLU y, CASE c\] \[N -, V +, BAR I, PER x, PLU y, VFORM z\] --> \[N -, V +, BAR O, PER x, PLU y, VFORM z, SUBCAT INTRANS\] IN -, V +, BAR i, PER x, PLU y, VFORM z\] --> \[N -, V +, BAR O, PER x, PLU y, VFOKM z, SUBCAT TRANS\] \[N +, V -, BAR 2, PER x, PLU y, CASE Acc\] Figure 3 ANLT object grammar rules." ></td>
	<td class="line x" title="157:516	{S-l: NP-2: VP-3: DET-4: NI-5: V-6: V-7: \[N -, V +, BAR 2, PER x, PLU y, VFORM z\] \[N +, V -, BAR 2, PER x, PLU y, CASE c\] IN -, V +, BAR i, PER x, PLU y, VFORM z\] \[SUBCAT DET\] IN +, V -, BAR i, PER x, PLU y, CASE c\] \[N -, V +, BAK O, PER x, PLU y, VFOKM z, SUBCAT INTRANS\] \[N -, V +, BAR O, PEK x, PLU y, VFOR/~ z, SUBCAT TKANS\]} Figure 4 Categories in disjoint-set." ></td>
	<td class="line x" title="158:516	S-1 --> NP-2 VP-3 NP-2 --> DET-4 N$-5 VP-3 --> V-6 VP-3 --> V-7 NP-2 Figure 5 Backbone grammar corresponding to object grammar." ></td>
	<td class="line x" title="159:516	For each unification grammar rule, create a backbone grammar rule containing atomic categories, each atomic category being the name assigned to the category in the disjoint category set that unifies with the corresponding category in the unification grammar rule: for each rule K of form C1 -~ C2  Cn in unification grammar add a rule B of form B1 -~ B2  Bn to backbone grammar where Bi is the name assigned to the (single) category in disjoint-set which unifies with Ci, for i=l, n. For example, for the rules in Figure 3 (corresponding loosely to S ---* NP VP, NP -~ Vi and VP --* Vt NP), step 1 would create the disjoint-set shown in Figure 4." ></td>
	<td class="line x" title="160:516	(Note that the value for CASE on the NP categories in the grammar has 'collapsed' down to a 34 Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing N2 either kin N2 -OR N2 -OR+ or lee N2-OR or sandy Figure 6 Backbone parse tree for either kim or lee or sandy using rule N2 --> N2 \[C0NJ EITHER\], N2 \[CONJ OR\] +." ></td>
	<td class="line x" title="161:516	variable, but that the two V categories remain distinct)." ></td>
	<td class="line x" title="162:516	Figure 5 shows the backbone rules that would be built in step 2." ></td>
	<td class="line x" title="163:516	Algorithms for creating LR parse tables assume that the terminal vocabulary of the grammar is distinct from the nonterminal one, so the procedure described above will not deal properly with a unification grammar rule whose mother category is assumed elsewhere in the grammar to be a lexical category." ></td>
	<td class="line x" title="164:516	The modification we make is to automatically associate two different atomic categories, one terminal and one nonterminal, with such categories, and to augment the backbone grammar with a unary rule expanding the nonterminal category to the terminal." ></td>
	<td class="line x" title="165:516	Two other aspects of the ANLT grammar formalism require further minor elaborations to the basic algorithm: firstly, a rule may introduce a gap by including the feature specification \[NULL +\] on the gapped daughter--for each such daughter an extra rule is added to the backbone grammar expanding the gap category to the null string; secondly, the formalism allows Kleene star and plus operators (Gazdar et al. 1985)-in the ANLT grammar these operators are utilized in rules for coordination." ></td>
	<td class="line x" title="166:516	A rule containing Kleene star daughters is treated as two rules: one omitting the daughters concerned and one with the daughters being Kleene plus." ></td>
	<td class="line x" title="167:516	A new nonterminal category is created for each distinct Kleene plus category, and two extra rules are added to the backbone grammar to form a right-branching binary tree structure for it; a parser can easily be modified to flatten this out during processing into the intended flat sequence of categories." ></td>
	<td class="line x" title="168:516	Figure 6 gives an example of what such a backbone tree looks like." ></td>
	<td class="line x" title="169:516	Grammars written in other, more low-level unification grammar formalisms, such as PATR-I1 (Shieber 1984), commonly employ treatments of the type just described to deal with phenomena such as gapping, coordination, and compounding." ></td>
	<td class="line x" title="170:516	However, this method both allows the grammar writer to continue to use the full facilities of the ANLT formalism and allows the algorithmic derivation of an appropriate backbone grammar to support LR parsing." ></td>
	<td class="line x" title="171:516	The major task of the backbone grammar is to encode sufficient information (in the atomic categoried CF rules) from the unification grammar to constrain the application of the latter's rules at parse time." ></td>
	<td class="line x" title="172:516	The nearly one-to-one mapping of unification grammar rules to backbone grammar rules described above works quite well for the ANLT grammar, with only a couple of exceptions that create spurious shift-reduce conflicts during parsing, resulting in an unacceptable degradation in performance." ></td>
	<td class="line x" title="173:516	The 35 Computational Linguistics Volume 19, Number 1 phenomena concerned are coordination and unbounded dependency constructions." ></td>
	<td class="line x" title="174:516	In the ANLT grammar three very general rules are used to form nominal, adjectival, and prepositional phrases following a conjunction; the categories in these rules lead to otherwise disjoint categories for conjuncts being merged, giving rise to a set of overly general backbone grammar rules." ></td>
	<td class="line x" title="175:516	For example, the rule in the ANLT grammar for forming a noun phrase conjunct introduced by a conjunction is N2\[CONJ @con\] --> \[SUBCAT @con, C0NJN +\], H2." ></td>
	<td class="line x" title="176:516	The variable value for the C0NJ feature in the mother means that all N2 categories specified for this feature (e.g. N2 \[C0NJ EITHER\], N2 \[C0NJ NULL\] ) are generalized to the same category." ></td>
	<td class="line x" title="177:516	This results in the backbone rules, when parsing either kim or lee helps, being unable, after forming a N2 \[C0NJ EITHER\] for either kim, to discriminate between the alternatives of preparing to iterate this constituent (as in the phrase kim, lee, or sandy helps where kim would be N2 \[C0NJ NULL\]), or shifting the next word or to start a new constituent." ></td>
	<td class="line x" title="178:516	We solve this problem by declaring C0NJ to be a feature that may not have a variable value in an element of the disjoint category set." ></td>
	<td class="line x" title="179:516	This directs the system to expand out each unification grammar rule that has a category containing this feature with a variable value into a number of rules fully specified for the feature, and to create backbone rules for each of these." ></td>
	<td class="line x" title="180:516	There are eight possible values for C0NJ in the grammar, so the general rule for forming a nominal conjunct given above, for example, ends up being represented by a set of eight specialized backbone grammar rules." ></td>
	<td class="line x" title="181:516	In the grammar, unbounded dependency constructions (UBCs) are analyzed by propagating the preposed constituent through the parse tree as the value of the SLASH feature, to link it with the 'gap' that appears in the constituent's normal position." ></td>
	<td class="line x" title="182:516	All nonlexical major categories contain the feature, rules in the grammar propagating it between mother and a single daughter; other daughters are marked \[SLASH \[NOSLASH +\] \] indicating that the daughter is not 'gapped'." ></td>
	<td class="line x" title="183:516	Backbone grammar construction would normally lose the information in the unification grammar about where gaps are allowed to occur, significantly degrading the performance of a parser." ></td>
	<td class="line x" title="184:516	To carry the information over into the backbone we declare that wherever SLASH occurs with a variable value, the value should be expanded out into two values: \[NOSLASH +\], and a notional value unifying with anything except \[NOSLASH +\]." ></td>
	<td class="line x" title="185:516	We have also experimented with a smaller grammar employing 'gap threading' (e.g. Pereira and Shieber 1987), an alternative treatment of UBCs." ></td>
	<td class="line x" title="186:516	We were able to use the same techniques for expanding out and inference on the values of the (in this case atomic) features used for threading the gaps to produce a backbone grammar (and parse table) that had the same constraining power with respect to gaps as the original grammar." ></td>
	<td class="line x" title="187:516	To date, we have not attempted to compute CF backbones for grammars written in formalisms with minimal phrase structure components and (almost) completely general categories, such as HPSG (Pollard and Sag 1987) and UCG (Zeevat, Calder, and Klein 1987); more extensive inference on patterns of possible unification within nested categories and appropriate expanding-out of the categories concerned would be necessary for an LR parser to work effectively." ></td>
	<td class="line x" title="188:516	This and other areas of complexity in unification-based formalisms need further investigation before we can claim to have developed a system capable of producing a useful LR parse table for any unificationbased grammar." ></td>
	<td class="line x" title="189:516	In particular, declaring certain category-valued features so that they cannot take variable values may lead to nontermination in the backbone construction for some grammars." ></td>
	<td class="line x" title="190:516	However, it should be possible to restrict the set of features that are considered in category-valued features in an analogous way to Shieber's (1985) restrictors for Earley's (1970) algorithm, so that a parse table can still be constructed." ></td>
	<td class="line x" title="191:516	36 Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing 4." ></td>
	<td class="line x" title="192:516	Building LR Parse Tables for Large NL Grammars The backbone grammar generated from the ANLT grammar is large: it contains almost 500 distinct categories and more than 1600 productions." ></td>
	<td class="line x" title="193:516	When we construct the LALR(1) parse table, we therefore require an algorithm with practical time and space requirements." ></td>
	<td class="line x" title="194:516	In the LR parsing literature there are essentially two approaches to constructing LALR(1) parse tables." ></td>
	<td class="line x" title="195:516	One approach is graph-based (DeRemer and Pennello 1982), transforming the parse table construction problem to a set of wellknown directed graph problems, which in turn are solvable by efficient algorithms." ></td>
	<td class="line x" title="196:516	Unfortunately this approach does not work for grammars that are not LR(k) for any k (DeRemer and Pennello 1982:633), for example, ambiguous grammars." ></td>
	<td class="line x" title="197:516	We therefore broadly follow the alternative approach of Aho, Sethi, and Ullman (1986), but with a number of optimizations: . . ." ></td>
	<td class="line x" title="198:516	Constructing the LR(0) sets of items: we compute LR(0) states containing only kernel items (the item IS' --> S\], where S' is the start symbol, and all items that have a symbol to the left of the dot), since nonkernel items can be cached in a table and retrieved only if needed." ></td>
	<td class="line x" title="199:516	Being able to partition the items in this way is especially useful with the ANLT grammar, since the mean number of kernel items in each LR(0) set is about 9, whereas the mean number of nonkernel items per state is more than 400." ></td>
	<td class="line x" title="200:516	Computing the LALR(1) lookaheads for each item: the conventional approach is to compute the LR(1) closure of each kernel item in order to determine the lookaheads that are generated spontaneously and those that propagate from other items." ></td>
	<td class="line x" title="201:516	However, in an initial implementation we found that the LR(1) closure operation as described by Aho et al. was too expensive to be practicable for the number and size of LR(0) states we deal with, even with schemes for caching the closures of nonkernel items once they had been computed." ></td>
	<td class="line x" title="202:516	Instead, we have moved to an algorithm devised by Kristensen and Madsen (1981), which avoids performing the LR(1) closure operation." ></td>
	<td class="line x" title="203:516	The crucial advantage of this algorithm is the ability, at any stage in the computation, to tell whether the calculation of the lookahead set for a particular item has been completed, is underway, or has not yet started." ></td>
	<td class="line x" title="204:516	This means that even partially computed lookahead sets can be cached (with the computation yet to be done explicitly marked), and that items whose lookahead sets are found to subsume those of others are able to just copy the results from the subsumed sets." ></td>
	<td class="line x" title="205:516	Constructing the parse table: the LALR(1) parse table is derived straightforwardly from the lookahead sets, although to keep the size of the parse table within reasonable bounds we chose appropriate data structures to represent the goto entries and shift and reduce actions." ></td>
	<td class="line x" title="206:516	For the ANLT backbone grammar there are approximately 150,000 goto entries (nonterminal--state pairs), 440,000 shift actions (terminal--state pairs), and 670,000 reduce actions (terminal--rule-number pairs); however, of the goto entries only 2,600 are distinct and of the shift actions only 1,100 are distinct; most states contain just reduce or just shift actions, and in any one state very few different rules are involved 37 Computational Linguistics Volume 19, Number 1 in reduce actions." ></td>
	<td class="line x" title="207:516	1 The majority of states contain just reduce or just shift actions, and in any one state very few different rules are involved in reduce actions." ></td>
	<td class="line x" title="208:516	Taking advantage of the characteristics of this distribution, in each state we represent (in Common Lisp) (a) a set of goto entries as a list of (nonterminal--state) conses sorted into a canonical order, list elements and tails of lists shared where possible between states, (b) a set of shift actions as a list containing a single (large) integer (the list shared when possible between states), where if the state shifts to state s on lookahead t, the element indexed by t in an auxiliary array will contain s together with a number n, and bit n in the binary representation of the integer will be 1, (c) a set of reduce actions as, for each rule involved, a cons whose second element is the rule number and whose first is a bit-vector (shared when possible between states) whose nth bit is 1 if the reduce should occur with the nth terminal as lookahead, (d) an accept action as a cons with the first element being the lookahead symbol." ></td>
	<td class="line x" title="209:516	For the grammars we have investigated, this representation achieves a similar order of space saving to the comb vector representation suggested by Aho, Sethi, and Ullman (1986:244ff) for unambiguous grammars (see Klein and Martin \[1989\] for a survey of representation techniques)." ></td>
	<td class="line x" title="210:516	The parse table for the ANLT grammar occupies approximately 360 Kbytes of memory, and so represents each action (shift, reduce, or goto) in an average of less than 2.3 bits." ></td>
	<td class="line x" title="211:516	In contrast to conventional techniques, though, we maintain a faithful representation of the parse table, not replacing error entries with more convenient nonerror ones in order to save extra space." ></td>
	<td class="line x" title="212:516	Our parsers are thus able to detect failures as soon as theoretically possible, an important efficiency feature when parsing nondeterministically with ambiguous grammars, and a time-saving feature when parsing interactively with them (see next section)." ></td>
	<td class="line x" title="213:516	Table 1 compares the size of the LALR(1) parse table for the ANLT grammar with others reported in the literature." ></td>
	<td class="line x" title="214:516	From these figures, the ANLT grammar is more than twice the size of Tomita's (combined morphological and syntactic) grammar for Japanese (Tomita 1987:45)." ></td>
	<td class="line x" title="215:516	The grammar itself is about one order of magnitude bigger than that of a typical programming language, but the LALR(1) parse table, in terms of number of actions, is two orders of magnitude bigger." ></td>
	<td class="line x" title="216:516	Although Tomita (1984:357) anticipates LR parsing techniques being applied to large NL grammars written in formalisms such as GPSG, the sizes of parse tables for such grammars grow more rapidly than he predicts." ></td>
	<td class="line x" title="217:516	However, for large real-world NL grammars such as the ANLT, the table size is still quite manageable despite Johnson's (1989) worst-case complexity result of the number of LR(0) states being exponential on grammar size (leading to a parser with exponentially bad time performance)." ></td>
	<td class="line x" title="218:516	We have, therefore, not found it necessary to use Schabes' (1991a) LR-like tables (with number of states guaranteed to be polynomial even in the worst case)." ></td>
	<td class="line x" title="219:516	1 of the 3,710 states, 2,200 contain at least 1 action conflict, with a median of 34 conflicts per state." ></td>
	<td class="line x" title="220:516	There are a total of 230,000 shift-reduce conflicts and 220,000 reduce-reduce conflicts, fairly uniformly distributed across the terminal lookahead symbols." ></td>
	<td class="line x" title="221:516	In half of the latter conflicts, the rules involved have an identical number of daughters." ></td>
	<td class="line x" title="222:516	One implication of this finding is that an approach to conflict resolution such as that of Shieber (1983) where reduce-reduce conflicts are resolved in favor of the longer reduction may not suffice to select a unique analysis for realistic NL grammars." ></td>
	<td class="line x" title="223:516	38 Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing Table 1 Sizes of grammar and LALR(1) parse tables." ></td>
	<td class="line x" title="224:516	Grammar Number of CFG Number of Number of Total number rules/categories LR(0) states kernel items of actions Pascal 2 158 / 124 275 ? 2883 Modula-23 227 / 194 373 420 3238 Tomita, Japanese 800 / ? ?" ></td>
	<td class="line x" title="225:516	ANLT (689 PS rules) 1641 / 496 3710 34836 1258451 Table 2 Timings for LALR(1) parse table construction (in seconds of CPU time on a Sparc-Server 390 running Sun Common Lisp)." ></td>
	<td class="line x" title="226:516	Grammar Backbone LR(0) state lookahead parse table computation construction computation construction ANLT 150 710 4200 780 As might be expected, and Table 2 illustrates, parse table construction for large grammars is CPU-intensive." ></td>
	<td class="line x" title="227:516	As a rough guide, Grosch (1990) quotes LALR(1) table construction for a grammar for Modula-2 taking from about 5 to 50 seconds, so scaling up two orders of magnitude, our timings for the ANLT grammar fall in the expected region." ></td>
	<td class="line x" title="228:516	5." ></td>
	<td class="line x" title="229:516	Interactive Incremental Deterministic Parsing 5.1 Constructing a Disambiguated Training Corpus The major problem with attempting to employ a disambiguated training corpus is to find a way of constructing this corpus in an error-free and resource-efficient fashion." ></td>
	<td class="line x" title="230:516	Even manual assignment of lexical categories is slow, labor-intensive, and error-prone." ></td>
	<td class="line x" title="231:516	The greater complexity of constructing a complete parse makes the totally manual approach very unattractive, if not impractical, Sampson (1987:83) reports that it took 2 person-years to produce the 'LOB tree bank' of 50,000 words." ></td>
	<td class="line x" title="232:516	Furthermore, in that project, no attempt was made to ensure that the analyses were well formed with respect to a generative grammar." ></td>
	<td class="line x" title="233:516	Attempting to manually construct analyses consistent with a grammar of any size and sophistication would place an enormous additional load on the analyst." ></td>
	<td class="line x" title="234:516	Leech and Garside (1991) discuss the problems that arise in manual parsing of corpora concerning accuracy and consistency of analyses across time and analyst, the labor-intensive nature of producing detailed analyses, and so forth." ></td>
	<td class="line x" title="235:516	They advocate an approach in which simple 'skeleton' parses are produced by hand from previously tagged material, with checking for consistency between analysts." ></td>
	<td class="line x" title="236:516	These skeleton analyses can then be augmented automatically with further information implicit in the lexical tags." ></td>
	<td class="line x" title="237:516	While this approach may well be the best that can be achieved 2 Figures given by Klein and Martin (1989)." ></td>
	<td class="line x" title="238:516	3 Grammar from Spector (1983) with optionality expanded out; statistics taken from a parse table constructed by the second author." ></td>
	<td class="line x" title="239:516	39 Computational Linguistics Volume 19, Number 1 with fully manual techniques, it is still unsatisfactory in several respects." ></td>
	<td class="line x" title="240:516	Firstly, the analyses are crude, while we would like to automatically parse with a grammar capable of assigning sophisticated semantically interpretable ones; but it is not clear how to train an existing grammar with such unrelated analyses." ></td>
	<td class="line x" title="241:516	Secondly, the quality of any grammar obtained automatically from the parsed corpus is likely to be poor because of the lack of any rigorous checks on the form of the skeleton parses." ></td>
	<td class="line x" title="242:516	Such a grammar might, in principle, be trained from the parsed corpus, but there are still likely to be small mismatches between the actual analysis assigned manually and any assigned automatically." ></td>
	<td class="line x" title="243:516	For these reasons, we decided to attempt to produce a training corpus using the grammar that we wished ultimately to train." ></td>
	<td class="line x" title="244:516	As long as the method employed ensured that any analysis assigned was a member of the set defined by the grammar, these problems during training should not arise." ></td>
	<td class="line x" title="245:516	Following our experience of constructing a substantial lexicon for the ANLT grammar from unreliable and indeterminate data (Carroll and Grover 1989), we decided to construct the disambiguated training corpus semi-automatically, restricting manual interaction to selection between alternatives defined by the ANLT grammar." ></td>
	<td class="line x" title="246:516	One obvious technique would be to generate all possible parses with a conventional parser and to have the analyst select the correct parse from the set returned (or reject them all)." ></td>
	<td class="line x" title="247:516	However, this approach places a great load on the analyst, who will routinely need to examine large numbers of parses for given sentences." ></td>
	<td class="line x" title="248:516	In addition, computation of all possible analyses is likely to be expensive and, in the limit, intractable." ></td>
	<td class="line x" title="249:516	Briscoe (1987) demonstrates that the structure of the search space in parse derivations makes a left-to-right, incremental mode of parse selection most efficient." ></td>
	<td class="line x" title="250:516	For example, in noun compounds analyzed using a recursive binary-branching rule (N --* N N) the number of analyses correlates with the Catalan series (Church and Patil, 1982), 4 so a 3-word compound has 2 analyses, 4 has 5, 5 has 14, 9 has 1430, and so forth." ></td>
	<td class="line x" title="251:516	However, Briscoe (1987:154f) shows that with a simple bounded context parser (with one word lookahead) set up to request help whenever a parse indeterminacy arises, it is possible to select any of the 14 analyses of a 5-word compound with a maximum of 5 interactions and any of the 1430 analyses of a 9-word compound with around 13 interactions." ></td>
	<td class="line x" title="252:516	In general, resolution of the first indeterminacy in the input will rule out approximately half the potential analyses, resolution of the next, half of the remaining ones, and so on." ></td>
	<td class="line x" title="253:516	For 'worst case' CF ambiguities (with O(n 3) complexity) this approach to parse selection appears empirically to involve numbers of interactions that increase at little more than linear rate with respect to the length of the input." ></td>
	<td class="line x" title="254:516	It is possible to exploit this insight in two ways." ></td>
	<td class="line x" title="255:516	One method would be to compute all possible analyses represented as a (packed) parse forest and ask the user to select between competing subanalyses that have been incorporated into a successful analysis of the input." ></td>
	<td class="line x" title="256:516	In this way, only genuine global syntactic ambiguities would need to be considered by the user." ></td>
	<td class="line x" title="257:516	However, the disadvantage of this approach is that it relies on a prior (and perhaps CPU-intensive) on-line computation of the full set of analyses." ></td>
	<td class="line x" title="258:516	The second method involves incremental interaction with the parser during the parse to guide it through the search space of possibilities." ></td>
	<td class="line x" title="259:516	This has the advantage of being guaranteed to be computationally tractable but the potential disadvantage of requiring the user to resolve many local syntactic ambiguities that will not be 4 The nth Catalan number is given by Cn=(2n) 1 n n+l' 40 Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing incorporated into a successful analysis." ></td>
	<td class="line x" title="260:516	Nevertheless, using LR techniques this problem can be minimized and, because we do not wish to develop a system that must be able to compute all possible analyses (at some stage) in order to return the most plausible one, we have chosen the latter incremental method." ></td>
	<td class="line x" title="261:516	5.2 The Interactive LR Parsing System The interactive incremental parsing system that we implemented asks the user for a decision at each choice point during the parse." ></td>
	<td class="line x" title="262:516	However, to be usable in practice, such a system must avoid, as far as possible, presenting the user with spurious choices that could be ruled out either by using more of the left context or by looking at words yet to be parsed." ></td>
	<td class="line x" title="263:516	Our approach goes some way to addressing these points, since the parser is as predictive as the backbone grammar and LR technique allow, and the LALR(1) parse table allows one word lookahead to resolve some ambiguities (although, of course, the resolution of a local ambiguity may potentially involve an unlimited amount of lookahead; e.g. Briscoe 1987:125ff)." ></td>
	<td class="line x" title="264:516	In fact, LR parsing is the most effectively predictive parsing technique for which an automatic compilation procedure is known, but this is somewhat undermined by our use of features, which will block some derivations so that the valid prefix property will no longer hold (e.g. Schabes 1991b)." ></td>
	<td class="line x" title="265:516	Extensions to the LR technique, for example those using LR-regular grammars (Culic and Cohen 1973; Bermudez 1991), might be used to further cut down on interactions; however, computation of the parse tables to drive such extended LR parsers may prove intractable for large NL grammars (Hektoen 1991)." ></td>
	<td class="line x" title="266:516	An LR parser faces an indeterminacy when it enters a state in which there is more than one possible action, given the current lookahead." ></td>
	<td class="line x" title="267:516	In a particular state there cannot be more than one shift or accept action, but there can be several reduce actions, each specifying a reduction with a different rule." ></td>
	<td class="line x" title="268:516	When parsing, each shift or reduce choice must lead to a different final structure, and so the indeterminacy represents a point of syntactic ambiguity (although it may not correspond to a genuinely global syntactic ambiguity in the input, on account of the limited amount of lookahead)." ></td>
	<td class="line x" title="269:516	In the ANLT grammar and lexicon, lexical ambiguity is at least as pervasive as structural ambiguity." ></td>
	<td class="line x" title="270:516	A naive implementation of an interactive LR parser would ask the user the correct category for each ambiguous word as it was shifted; many open-class words are assigned upwards of twenty lexical categories by the ANLT lexicon with comparatively fine distinctions between them, so this strategy would be completely impracticable." ></td>
	<td class="line x" title="271:516	To avoid asking the user about lexical ambiguity, we use the technique of preterminal delaying (Shieber 1983), in which the assignment of an atomic preterminal category to a lexical item is not made until the choice is forced by the use of a particular production in a later reduce action." ></td>
	<td class="line x" title="272:516	After shifting an ambiguous lexical item, the parser enters a state corresponding to the union of states that would be entered on shifting the individual lexical categories." ></td>
	<td class="line x" title="273:516	(Each union of states will in practice be small, since it being otherwise would imply that the current context was completely failing to constrain the following input)." ></td>
	<td class="line x" title="274:516	Since, in general, several unification grammar categories for a single word may be subsumed by a single atomic preterminal category, we extend Shieber's technique so that it deals with a grammar containing complex categories by associating a set of alternative analyses with each state (not just one), and letting the choice between them be forced by later reduce actions, just as with atomic preterminal categories." ></td>
	<td class="line x" title="275:516	In order not to overload the user with spurious choices concerning local ambiguities, the parser does not request help immediately after it reaches a parse action conflict." ></td>
	<td class="line x" title="276:516	Instead the parser pursues each option in a limited breadth-first fashion and only requests help with analysis paths that remain active." ></td>
	<td class="line x" title="277:516	In our current system this 41 Computational Linguistics Volume 19, Number 1 Table 3 Amount of user interaction parsing the act of putting an end to something with the ANLT grammar using different amounts of action conflict lookahead." ></td>
	<td class="line x" title="278:516	action conflict number of lookahead choices mean number of options in each choice none 6 2.3 1 choice 5 2.2 2 choices 3 2.0 3 choices 2 2.0 4 choices 1 2.0 type of lookahead is limited to up to four indeterminacies ahead." ></td>
	<td class="line x" title="279:516	Such checking is cheap in terms of machine resources and very effective in cutting down both the number of choice points the user is forced to consider and also the average number of options in each one." ></td>
	<td class="line x" title="280:516	Table 3 shows the reduction in user interaction achieved by increasing the amount of lookahead in our system." ></td>
	<td class="line x" title="281:516	Computation of the backbone grammar generates extra rules (as previously described to deal with lexical categories used as rule mothers and daughters specified to be repeatable an indefinite number of times) that do not correspond directly to single unification grammar rules." ></td>
	<td class="line x" title="282:516	At choice points, reductions involving these rules are not presented to the user; instead the system applies the reductions automatically, proceeding until the next shift action or choice point is reached, including these options in those presented to the user." ></td>
	<td class="line x" title="283:516	The final set of measures taken to reduce the amount of interaction required with the user is to ask if the phrase being parsed contains one or more gaps or instances of coordination before presenting choices involving either of these phenomena, blocking consideration of rules on the basis of the presence of particular feature-value pairs." ></td>
	<td class="line x" title="284:516	Figure 7 shows the system parsing a phrase with a four-choice lookahead." ></td>
	<td class="line x" title="285:516	The resulting parse tree is displayed with category aliases substituted for the actual complex categories." ></td>
	<td class="line x" title="286:516	The requests for manual selection of the analysis path are displayed to the analyst in as terse a manner as possible, and require knowledge of the ANLT grammar and lexicon to be resolved effectively." ></td>
	<td class="line x" title="287:516	Figure 8 summarizes the amount of interaction required in the experiment reported below for parsing a set of 150 LDOCE noun definitions with the ANLT grammar." ></td>
	<td class="line x" title="288:516	To date, the largest number of interactions we have observed for a single phrase is 55 for the (30-word) LDOCE definition for youth hostel: a hostel for usu young people walking around country areas on holiday for which they pay small amounts of money to the youth hostels association or the international yha." ></td>
	<td class="line x" title="289:516	Achieving the correct analysis interactively took the first author about 40 minutes (including the addition of two lexical entries)." ></td>
	<td class="line x" title="290:516	Definitions of this length will often have many hundreds or even thousands of parses; computing just the parse forest for this definition takes of the order of two hours of CPU time (on a DEC 3100 Unix workstation)." ></td>
	<td class="line x" title="291:516	Since in a more general corpus of written material the average sentence length is likely to be 30--40 words, this example illustrates clearly the problems with any approach based on post hoc on-line selection of the correct parse." ></td>
	<td class="line x" title="292:516	However, using 42 Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing Parse>> the act of putting an end to something Are there any gaps in this phrase?" ></td>
	<td class="line x" title="293:516	n Ambiguity in state 27/193 with (of putting an end to something $) remaining in buffer." ></td>
	<td class="line x" title="294:516	Analysis so far is the, act." ></td>
	<td class="line x" title="295:516	I: Shift word 'of' onto stack." ></td>
	<td class="line x" title="296:516	2: Reduce end I analyses with rule NI/N (giving category NI-9)." ></td>
	<td class="line x" title="297:516	Which choice (i 2 / abort / finish)?" ></td>
	<td class="line x" title="298:516	2 2384 msec CPU 2700 unifications, 2025 failures, i parse (N2 the (N2 (NI act (P2 (PI of (VP (V putting) (N2 an (N2 (NI end))) (P2 (PI to (N2 something))))))))) Figure 7 An interactive parse." ></td>
	<td class="line x" title="299:516	20Number of def'mitions i0I I i i 0 l0 20 30 40 Number of interactions Figure 8 Numbers of definitions requiring particular amounts of interaction." ></td>
	<td class="line x" title="300:516	I'1 I 50 43 Computational Linguistics Volume 19, Number 1 the incremental approach to semi-automatic parsing we have been able to demonstrate that the correct analysis is among this set." ></td>
	<td class="line x" title="301:516	Furthermore, a probabilistic parser such as the one described later may well be able to compute this analysis in a tractable fashion by extracting it from the parse forest." ></td>
	<td class="line x" title="302:516	(To date, the largest example for which we have been able to compute all analyses had approximately 2500)." ></td>
	<td class="line x" title="303:516	The parse histories resulting from semi-automatic parsing are automatically stored and can be used to derive the probabilistic information that will guide the parser after training." ></td>
	<td class="line x" title="304:516	We return to a discussion of the manner in which this information is utilized in Section 7." ></td>
	<td class="line x" title="305:516	6. Non-Deterministic LR Parsing with Unification Grammars As well as building an interactive parsing system incorporating the ANLT grammar (described above), we have implemented a breadth-first, nondeterministic LR parser for unification grammars." ></td>
	<td class="line x" title="306:516	This parser is integrated with the Grammar Development Environment (GDE; Carroll et al. 1988) in the ANLT system, and provided as an alternative parser for use with stable grammars for batch parsing of large bodies of text." ></td>
	<td class="line x" title="307:516	The existing chart parser, although slower, has been retained since it is more suited to grammar development, because of the speed with which modifications to the grammar can be compiled and its better debugging facilities (Boguraev et al. 1988)." ></td>
	<td class="line x" title="308:516	Our nondeterministic LR parser is based on Kipps' (1989) reformulation of Tomita's (1987) parsing algorithm and uses a graph-structured stack in the same way." ></td>
	<td class="line x" title="309:516	Our parser is driven by the LALR(1) state table computed from the backbone grammar, but in addition on each reduction the parser performs the unifications appropriate to the unification grammar version of the backbone rule involved." ></td>
	<td class="line x" title="310:516	The analysis being pursued fails if one of the unifications fails." ></td>
	<td class="line x" title="311:516	The parser performs sub-analysis sharing (where if two or more trees have a common sub-analysis, that sub-analysis is represented only once), and local ambiguity packing (in which sub-analyses that have the same top node and cover the same input have their top nodes merged, being treated by higher level structures as a single sub-analysis)." ></td>
	<td class="line x" title="312:516	However, we generalize the technique of atomic category packing described by Tomita, driven by atomic category names, to complex feature-based categories following Alshawi (1992): the packing of sub-analyses is driven by the subsumption relationship between the feature values in their top nodes." ></td>
	<td class="line x" title="313:516	An analysis is only packed into one that has already been found if its top node is subsumed by, or is equal to that of the one already found." ></td>
	<td class="line x" title="314:516	An analysis, once packed, will thus never need to be unpacked during parsing (as in Tomita's system) since the value of each feature will always be uniquely determined." ></td>
	<td class="line x" title="315:516	Our use of local ambiguity packing does not in practice seem to result in exponentially bad performance with respect to sentence length (cf.Johnson 1989) since we have been able to generate packed parse forests for sentences of over 30 words having many thousands of parses." ></td>
	<td class="line x" title="317:516	We have implemented a unification version of Schabes' (1991a) chart-based LR-like parser (which is polynomial in sentence length for CF grammars), but experiments with the ANLT grammar suggest that it offers no practical advantages over our Tomita-style parser, and Schabes' table construction algorithm yields less fine-grained and, therefore, less predictive parse tables." ></td>
	<td class="line x" title="318:516	Nevertheless, searching the parse forest exhaustively to recover each distinct analysis proved computationally intractable for sentences over about 22 words in length." ></td>
	<td class="line x" title="319:516	Wright, Wrigley, and Sharman (1991) describe a Viterbi-like algorithm for unpacking parse forests containing probabilities of (sub-)analyses to find the n-best analyses, but this approach does not generalize (except in a heuristic way) to our approach in which unification failure on the different extensions of packed nodes (resulting from differing superor sub44 Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing Table 4 Chart and LR parse times for the LDOCE definition the state of being away or of not being present with the ANLT grammar (in CPU seconds on a DEC 3100)." ></td>
	<td class="line x" title="320:516	Parser Parse time GDE (bottom-up) chart parser 7.9 LR semi-automatic (with 4-choice lookahead) 6.0 LR nondeterministic 5.8 analyses) cannot be computed 'locally'." ></td>
	<td class="line x" title="321:516	In subsequent work (Carroll and Briscoe 1992) we have developed such a heuristic technique for best-first search of the parse forest which, in practice, makes the recovery of the most probable analyses much more efficient (allowing analysis of sentences containing over 30 words)." ></td>
	<td class="line x" title="322:516	We noticed during preliminary experiments with our unification LR parser that it was often the case that the same unifications were being performed repeatedly, even during the course of a single reduce action." ></td>
	<td class="line x" title="323:516	The duplication was happening in cases where two or more pairs of states in the graph-structured stack had identical complex categories between them (for example due to backbone grammar ambiguity)." ></td>
	<td class="line x" title="324:516	During a reduction with a given rule, the categories between each pair of states in a backwards traversal of the stack are collected and unified with the appropriate daughters of the rule." ></td>
	<td class="line x" title="325:516	Identical categories appearing here between traversed pairs of states leads to duplication of unifications." ></td>
	<td class="line x" title="326:516	By caching unification results we eliminated this wasted effort and improved the initially poor performance of the parser by a factor of about three." ></td>
	<td class="line x" title="327:516	As for actual parse times, Table 4 compares those for the GDE chart parser, the semi-automatic, user-directed LR parser, and the nondeterministic LR parser." ></td>
	<td class="line x" title="328:516	Our general experience is that although the nondeterministic LR parser is only around 30-50% faster than the chart parser, it often generates as little as a third the amount of garbage." ></td>
	<td class="line x" title="329:516	(The relatively modest speed advantage compared with the substantial space saving appears to be due to the larger overheads involved in LR parsing)." ></td>
	<td class="line x" title="330:516	Efficient use of space is obviously an important factor for practical parsing of long and ambiguous texts." ></td>
	<td class="line x" title="331:516	7." ></td>
	<td class="line x" title="332:516	LR Parsing with Probabilistic Disambiguation Several researchers (Wright and Wrigley 1989; Wright 1990; Ng and Tomita 1991; Wright, Wrigley, and Sharman 1991) have proposed using LR parsers as a practical method of parsing with a probabilistic context-free grammar." ></td>
	<td class="line x" title="333:516	This approach assumes that probabilities are already associated with a CFG and describes techniques for distributing those probabilities around the LR parse table in such a way that a probabilistic ranking of alternative analyses can be computed quickly at parse time, and probabilities assigned to analyses will be identical to those defined by the original probabilistic CFG." ></td>
	<td class="line x" title="334:516	However, our method of constructing the training corpus allows us to associate probabilities with an LR parse table directly, rather than simply with rules of the grammar." ></td>
	<td class="line x" title="335:516	An LR parse state encodes information about the left and right context of the current parse." ></td>
	<td class="line x" title="336:516	Deriving probabilities relative to the parse context will allow the probabilistic parser to distinguish situations in which identical rules reapply 45 Computational Linguistics Volume 19, Number 1 in different ways across different derivations or apply with differing probabilities in different contexts." ></td>
	<td class="line x" title="337:516	Semi-automatic parsing of the training corpus yields a set of LR parse histories that are used to construct the probabilistic version of the LALR(1) parse table." ></td>
	<td class="line x" title="338:516	The parse table is a nondeterministic finite-state automaton so it is possible to apply Markov modeling techniques to the parse table (in a way analogous to their application to lexical tagging or CFGs)." ></td>
	<td class="line x" title="339:516	Each row of the parse table corresponds to the possible transitions out of the state represented by that row, and each transition is associated with a particular lookahead item and a parse action." ></td>
	<td class="line x" title="340:516	Nondeterminism arises when more than one action, and hence transition, is possible given a particular lookahead item." ></td>
	<td class="line x" title="341:516	The most straightforward technique for associating probabilities with the parse table is to assign a probability to each action in the action part of the table (e.g. Wright 1990)." ></td>
	<td class="line x" title="342:516	5 If probabilities are associated directly with the parse table rather than derived from a probabilistic CFG or equivalent global pairing of probabilities to rules, then the resulting probabilistic model will be more sensitive to parse context." ></td>
	<td class="line x" title="343:516	For example, in a derivation for the sentence he loves her using Grammar 1, the distinction between reducing the first pronoun and second pronoun to NP--using rule 5 (NP --> ProNP)-can be maintained in terms of the different lookahead items paired with the reduce actions relating to this rule (in state 5 of the parse table in Figure 2); in the first case, the lookahead item will be Vi, and in the second $ (the end of sentence marker)." ></td>
	<td class="line x" title="344:516	However, this approach does not make maximal use of the context encoded into a transition in the parse table, and it is possible to devise situations in which the reduction of a pronoun in subject position and elsewhere would be indistinguishable in terms of lookahead alone; for example, if we added appropriate rules for adverbs to Grammar 1, then this reduction would be possible with lookahead Adv in sentences such as he passionately loves her and he loves her passionately." ></td>
	<td class="line x" title="345:516	A slightly less obvious approach is to further subdivide reduce actions according to the state reached after the reduce action has applied." ></td>
	<td class="line x" title="346:516	This state is used together with the resultant nonterminal to define the state transition in the goto part of the parse table." ></td>
	<td class="line x" title="347:516	Thus, this move corresponds to associating probabilities with transitions in the automaton rather than with actions in the action part of the table." ></td>
	<td class="line x" title="348:516	For example, a reduction of pronoun to NP in subject position in the parse table for Grammar 1 in Figure 2 always results in the parser returning to state 0 (from which the goto table deterministically prescribes a transition to state 7 with nonterminal RP)." ></td>
	<td class="line x" title="349:516	Reduction to NP of a pronoun in object position always results in the parser returning to state 11." ></td>
	<td class="line x" title="350:516	Thus training on a corpus with more subject than nonsubject pronominal NPs will now result in a probabilistic preference for reductions that return to 'pre-subject' states with 'post-subject' lookaheads." ></td>
	<td class="line x" title="351:516	Of course, this does not mean that it will be impossible to devise grammars in which reductions cannot be kept distinct that might, in principle, have different frequencies of occurrence." ></td>
	<td class="line x" title="352:516	However, this approach appears to be the natural stochastic, probabilistic model that emerges when using a LALR(1) table." ></td>
	<td class="line x" title="353:516	Any further sensitivity to context would require sensitivity to patterns in larger sections of a parse derivation than can be defined in terms of such a table." ></td>
	<td class="line x" title="354:516	The probabilities required to create the probabilistic version of the parse table can be derived from the set of parse histories resulting from the training phase described in Section 5, by computing the frequency with which each transition from a particular state has been taken and converting these to probabilities such that the probabilities 5 In our implementation, the probabilities are actually stored separately from the parse table to ensure that otherwise-sharable transitions in the table can still be represented compactly even if their probabilities differ." ></td>
	<td class="line x" title="355:516	46 Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing State $ Det N@ p ProNP Vi Vt  0 s3 s2 (.50) (.50)  1 rl (0 .83)  2 r5 r5 r5 r5 (8 .33) (0 .50)  3 s4 (i. 00)  4 rl0 rl0 rl0 rl0 rl0 (3 .Ii 6 .ii) (3 .17 5 .22) (3 .ii) (3 .ii 5 .ii)  5 r6 84 r6 r6 r6 (8 .13 Ii .13) (.33) (ii .13) (0 .20)  6 r8 r8 r8 r8 r8 (3 .17 5 .17) (3 .25) (3 .17) s4 (.17)  7 88 s13 sll (.43) (.43)  8 83 s2 (.50) (.50)  9 r9 r9 r9 r9 (12 .40) (12 .40) 88  i0 r7 r7 r7 r7 (11 .40) (11 .40)  ii 83 82 (.75)  12 r3 88 (7 .43) (.43)  13 r4 (7 .75)  14 r2 (0 .84)  15 acc (l.00) Figure 9 A probabilistic version of the parse table for Grammar 1." ></td>
	<td class="line x" title="356:516	assigned to each transition from a given state sum to one." ></td>
	<td class="line x" title="357:516	In Figure 9 we show a probabilistic LALR(1) parse table for Grammar 1 derived from a simple, partial (and artificial) training phase." ></td>
	<td class="line x" title="358:516	In this version of the table a probability is associated with each shift action in the standard way, but separate probabilities are associated with reduce 47 Computational Linguistics Volume 19, Number 1 o) o i) 0 Det 3 2) 0 Det 3 N@ 4 3) 0 Det 3 N 4) 0 Det 3 N 5 5) 0 Det 3 N 5 N@ 4 6) 0 Det 3 N 5 N 7) 0 Det 3 N 5 N 6 (Det) (N@I) .50 (N@2) 1.00 (N@2) (N@2) .17 (N@3) .33 (N@3) (N@3) .22 8a) 0 Det 3 N 5 N 6 N@ 4 (Vi) .17 9a) 0 Det 3 N 5 N 6 N (Vi) 10a) 0 Det 3 N 5 N 6 N 6 (Vi) .06 lla) 0 Det 3 N 5 N (Vi) 12a) 0 Det 3 N 5 N 6 (Vi) .08 13a) 0 Det 3 N (Vi) 8b) 0 Det 3 N (N@3) 9b) 0 Det 3 N 5 (N@3) .25 10b) 0 Det 3 N 5 N@ 4 (Vi) .33 llb) 0 Det 3 N 5 N (Vi) 12b) 0 Det 3 N 5 N 6 (Vi) .ii 13b) 0 Det 3 N (Vi) 14) 0 Det 3 N 5 (Vi) .17 15) 0 NP (Vi) 16) 0 NP 7 (Vi) .20 17) 0 NP 7 Vi 13 ($) .43 18) 0 NP 7 VP ($) 19) 0 NP 7 VP 14 ($) .75 2o) o s ($) 21) 0 S 1 ($) .83 22) 0 S' ($) 23) 0 S' 15 ($) 1.00 Figure 10 Parse derivations for the winter holiday camp closed." ></td>
	<td class="line x" title="359:516	actions, depending on the state reached after the action; for example, in state 4 with lookahead N~ the probability of reducing with rule 10 is 0.17 if the state reached is 3 and 0.22 if the state reached is 5." ></td>
	<td class="line x" title="360:516	The actions that have no associated probabilities are ones that have not been utilized during the training phase; each is assigned a smoothed probability that is the reciprocal of the result of adding one to the total number of observations of actions actually taken in that state." ></td>
	<td class="line x" title="361:516	Differential probabilities are thus assigned to unseen events in a manner analogous to the Good-Turing technique." ></td>
	<td class="line x" title="362:516	For this reason, the explicit probabilities for each row add up to less than one." ></td>
	<td class="line x" title="363:516	The goto part of the table is not shown because it is always deterministic and, therefore, we do not associate probabilities with goto transitions." ></td>
	<td class="line x" title="364:516	The difference between our approach and one based on probabilistic CFG can be brought out by considering various probabilistic derivations using the probabilistic parse table for Grammar 1." ></td>
	<td class="line x" title="365:516	Assuming that we are using probabilities simply to rank parses, we can compute the total probability of an analysis by multiplying together the probabilities of each transition we take during its derivation." ></td>
	<td class="line x" title="366:516	In Figure 10, we give the two possible complete derivations for a sentence such as the winter holiday camp closed consisting of a determiner, three nouns, and an intransitive verb." ></td>
	<td class="line x" title="367:516	The ambiguity concerns whether the noun compound is leftor right-branching, and, as we saw in Section 2, a probabilistic CFG cannot distinguish these two derivations." ></td>
	<td class="line x" title="368:516	The probability of each step can be read off the action table and is shown after the lookahead item in the figure." ></td>
	<td class="line x" title="369:516	In step 8 a shift-reduce conflict occurs so the stack 'splits' while the leftand rightbranching analyses of the noun compound are constructed." ></td>
	<td class="line x" title="370:516	The a) branch corresponds 48 Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing to the right-branching derivation and the product of the probabilities is 4.6 x 10 -8, while the product for the left-branching b) derivation is 5.1 x 10 -7." ></td>
	<td class="line x" title="371:516	Since the table was constructed from parse histories with a preponderance of left-branching structures this is the desired result." ></td>
	<td class="line x" title="372:516	In practice, this technique is able to distinguish and train accurately on 3 of the 5 possible structures for a 4-word noun-noun compound; but it inaccurately prefers a completely left-branching analysis over structures of the form ((n n)(n n)) and ((n (nn)) n)." ></td>
	<td class="line x" title="373:516	Once we move to 5-word noun-noun compounds, performance degrades further." ></td>
	<td class="line x" title="374:516	However, this level of performance on such structural configurations is likely to be adequate, because correct resolution of most ambiguity in such constructions is likely to be dominated by the actual lexical items that occur in individual texts." ></td>
	<td class="line x" title="375:516	Nevertheless, if there are systematic structural tendencies evident in corpora (for example, Frazier's \[1988\] parsing strategies predict a preference for left-branching analyses of such compounds), then the probabilistic model is sensitive enough to discriminate them." ></td>
	<td class="line x" title="376:516	6 In practice, we take the geometric mean of the probabilities rather than their product to rank parse derivations." ></td>
	<td class="line x" title="377:516	Otherwise, it would be difficult to prevent the system from always developing a bias in favor of analyses involving fewer rules or equivalently 'smaller' trees, almost regardless of the training material." ></td>
	<td class="line x" title="378:516	Of course, the need for this step reflects the fact that, although the model is more context-dependent than probabilistic CFG, it is by no means a perfect probabilistic model of NL." ></td>
	<td class="line x" title="379:516	7 For example, the stochastic nature of the model and the fact that the entire left context of a parse derivation is not encoded in LR state information means that the probabilistic model cannot take account of, say, the pattern of resolution of earlier conflicts in the current derivation." ></td>
	<td class="line x" title="380:516	Another respect in which the model is approximate is that we are associating probabilities with the context-free backbone of the unification grammar." ></td>
	<td class="line x" title="381:516	Successful unification of features at parse time does not affect the probability of a (partial) analysis, while unification failure, in effect, sets the probability of any such analysis to zero." ></td>
	<td class="line x" title="382:516	As long as we only use the probabilistic model to rank successful analyses, this is not particularly problematic." ></td>
	<td class="line x" title="383:516	However, parser control regimes that attempt some form of best-first search using probabilistic information associated with transitions might not yield the desired result given this property." ></td>
	<td class="line x" title="384:516	For example, it is not possible to use Viterbi-style optimization of search for the maximally probable parse because this derivation may contain a sub-analysis that will be pruned locally before a subsequent unification failure renders the current most probable analysis impossible." ></td>
	<td class="line x" title="385:516	In general, the current breadth-first probabilistic parser is more efficient than its nonprobabilistic counterpart described in the previous section." ></td>
	<td class="line x" title="386:516	In contrast to the parser described by Ng and Tomita (1991), our probabilistic parser is able to merge (state and stack) configurations and in all cases still maintain a full record of all the probabilities computed up to that point, since it associates probabilities with partial analyses of the input so far rather than with nodes in the graph-structured stack." ></td>
	<td class="line x" title="387:516	We are currently 6 Although we define our probabilistic model relative to the LR parsing technique, it is likely that there is an equivalent encoding in purely grammatical terms." ></td>
	<td class="line x" title="388:516	In general our approach corresponds to making the probability of rule application conditional on other rules having applied during the parse derivation (e.g. Magerman and Marcus 1991) and the lexical category of the next word; for example, it would be possible to create a grammatical representation of the probabilistic model that emerges from a LR(0) table by assigning a set of probabilities associated with rule numbers to each right-hand side category in each rule of a CFG that would encode the probability of a rule being used to expand that category in that context." ></td>
	<td class="line x" title="389:516	7 Magerrnan and Marcus (1991) argue that it is reasonable to use the geometric mean when computing the probability of two or more sub-analyses because the independence assumptions that motivate using products do not hold for such an approximate model." ></td>
	<td class="line x" title="390:516	In Carroll and Briscoe (1992) we present a more motivated technique for normalizing the probability of competing sub-analyses in the parse forest." ></td>
	<td class="line x" title="391:516	49 Computational Linguistics Volume 19, Number 1 experimenting with techniques for probabilistically unpacking the packed parse forest to recover the first few most probable derivations without the need for exhaustive search or full expansion." ></td>
	<td class="line x" title="392:516	8." ></td>
	<td class="line x" title="393:516	Parsing LDOCE Noun Definitions In order to test the techniques and ideas described in previous sections, we undertook a preliminary experiment using a subset of LDOCE noun definitions as our test corpus." ></td>
	<td class="line x" title="394:516	(The reasons for choosing this corpus are discussed in the introduction)." ></td>
	<td class="line x" title="395:516	A corpus of approximately 32,000 noun definitions was created from LDOCE by extracting the definition fields and normalizing the definitions to remove punctuation, font control information, and so forth, s A lexicon was created for this corpus by extracting the appropriate lemmas and matching these against entries in the ANLT lexicon." ></td>
	<td class="line x" title="396:516	The 10,600 resultant entries were loaded into the ANLT morphological system (Ritchie et al. 1987) and this sublexicon and the full ANLT grammar formed the starting point for the training process." ></td>
	<td class="line x" title="397:516	A total of 246 definitions, selected without regard for their syntactic form, were parsed semi-automatically using the parser described in Section 5." ></td>
	<td class="line x" title="398:516	During this process, further rules and lexical entries were created for some definitions that failed to parse." ></td>
	<td class="line x" title="399:516	Of the total number, 150 were successfully parsed and 63 lexical entries and 14 rules were added." ></td>
	<td class="line x" title="400:516	Some of the rules required reflected general inadequacies in the ANLT grammar; for example, we added rules to deal with new partitives and prepositional phrase and verb complementation." ></td>
	<td class="line x" title="401:516	However, 7 of these rules cover relatively idiosyncratic properties of the definition sublanguage; for example, the postmodification of pronouns by relative clause and prepositional phrase in definitions beginning something that  that of, parenthetical phrases headed by adverbs, such as the period esp the period, and coordinations without explicit conjunctions ending with etc. , and so forth." ></td>
	<td class="line x" title="402:516	Further special rules will be required to deal with brackets in definitions to cover conventions such as a man (monk) or woman (nun) who lives in a monastery, which we ignored for this test." ></td>
	<td class="line x" title="403:516	Nevertheless, the number of new rules required is not great and the need for most was identified very early in the training process." ></td>
	<td class="line x" title="404:516	Lexical entries are more problematic, since there is little sign that the number of new entries required will tail off." ></td>
	<td class="line x" title="405:516	However, many of the entries required reflect systematic inadequacies in the ANLT lexicon rather than idiosyncrasies of the corpus." ></td>
	<td class="line x" title="406:516	It took approximately one person-month to produce this training corpus." ></td>
	<td class="line x" title="407:516	As a rough guide, it takes an average of 15 seconds to resolve a single interaction with the parser." ></td>
	<td class="line x" title="408:516	However, the time a parse takes can often be lengthened by incorrect choices (and the consequent need to back up manually) and by the process of adding lexical entries and occasional rules." ></td>
	<td class="line x" title="409:516	The resultant parse histories were used to construct the probabilistic parser (as described in the previous section)." ></td>
	<td class="line x" title="410:516	This parser was then used to reparse the training corpus, and the most highly ranked analyses were automatically compared with the original parse histories." ></td>
	<td class="line x" title="411:516	We have been able to reparse in a breadth-first fashion all but 3 of the 150 definitions that were parsed manually." ></td>
	<td class="line x" title="412:516	9 (These three are each over 8 The corpus contains about 17,000 unique headwords and 13,500 distinct word forms in the definitions." ></td>
	<td class="line x" title="413:516	Its perplexity (PP) measures based on bigram and trigram word models and an estimate of an infinite model were PP(2) = 104, PP(3) = 41, and PP(inf) = 8 (Sharman 1991)." ></td>
	<td class="line x" title="414:516	9 The results we report here are from using the latest versions of the ANLT grammar and LR parsing system." ></td>
	<td class="line x" title="415:516	Briscoe and Carroll (1991) report an earlier version of this experiment using different versions of the grammar and parser in which results differed in minor ways." ></td>
	<td class="line x" title="416:516	Carroll and Briscoe (1992) report a third version of the experiment in which results were improved slightly through the use of a better normalization and parse forest unpacking technique." ></td>
	<td class="line x" title="417:516	50 i~i~!ii!iiii@!" ></td>
	<td class="line x" title="418:516	20Number of definitions 100 Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing Figure 11 h i0 20 Definition length I-1 1st ranked analysis correct 2nd ranked analysis correct 3rd ranked analysis correct correct analysis not ranked either 1st, 2nd or 3rd Correctness of results for reparsed definitions with respect to length." ></td>
	<td class="line x" title="419:516	I 30 25 words in length)." ></td>
	<td class="line x" title="420:516	There are 22 definitions one word in length: all of these trivially receive correct analyses." ></td>
	<td class="line x" title="421:516	There are 89 definitions between two and ten words in length inclusive (mean length 6.2)." ></td>
	<td class="line x" title="422:516	Of these, in 68 cases the correct analysis (as defined by the training corpus) is also the most highly ranked." ></td>
	<td class="line x" title="423:516	In 13 of the 21 remaining cases the correct analysis is the second or third most highly ranked analysis." ></td>
	<td class="line x" title="424:516	Looking at these 21 cases in more detail, in 8 there is an inappropriate structural preference for 'low' or 'local' attachment (see Kimball 1973), in 4, an inappropriate preference for compounds, and in 6 of the remaining 9 cases, the highest ranked result contains a misanalysis of a single constituent two or three words in length." ></td>
	<td class="line x" title="425:516	If these results are interpreted in terms of a goodness of fit measure such as that of Sampson, Haigh, and Atwell (1989), the measure would be better ttian 96%." ></td>
	<td class="line x" title="426:516	If we take correct parse/sentence as our measure then the result is 76%." ></td>
	<td class="line x" title="427:516	For definitions longer than 10 words this latter figure tails off, mainly due to misapplication of such statistically induced, but nevertheless structural, attachment preferences." ></td>
	<td class="line x" title="428:516	Figure 11 summarizes these results." ></td>
	<td class="line x" title="429:516	We also parsed a further 55 LDOCE noun definitions not drawn from the training corpus, each containing up to 10 words (mean length 5.7)." ></td>
	<td class="line x" title="430:516	Of these, in 41 cases the correct parse is the most highly ranked, in 6 cases it is the second or third most highly ranked, and in the remaining 8 cases it is not in the first three analyses." ></td>
	<td class="line x" title="431:516	This yields a correct parse/sentence measure of 75%." ></td>
	<td class="line x" title="432:516	Examination of the failures again reveals that a preference for local attachment of postmodifiers accounts for 5 cases, a preference for compounds for 1, and the misanalysis of a single constituent for 2." ></td>
	<td class="line x" title="433:516	The others are mostly caused by the lack of lexical entries with appropriate SUBCAT features." ></td>
	<td class="line x" title="434:516	In Figure 12 we show the analysis for the unseen definition of affectation, which has 20 parses of which the most highly ranked is correct." ></td>
	<td class="line x" title="435:516	51 Computational Linguistics Volume 19, Number 1 N2 N2 i N1 N1 N1 CONJNI N2 VP feelinc or N1 that is VP I i manner pretende Figure 12 Parse tree for afeelingormanner that is pretended." ></td>
	<td class="line x" title="436:516	N1 N1/N CONJNI person or N1 N2 I I thing that N2 \ N2 I N1 S I S VP VP CONJVP support~ E or VP helps E Figure 13 Parse tree for a person or thing that supports or helps." ></td>
	<td class="line x" title="437:516	Figure 13 shows the highest-ranked analysis assigned to one definition of aid." ></td>
	<td class="line x" title="438:516	This is an example of a false positive which, in this case, is caused by the lack of a lexical entry for support as an intransitive verb." ></td>
	<td class="line x" title="439:516	Consequently, the parser finds, and ranks highest, an analysis in which supports and helps are treated as transitive verbs forming verb phrases with object NP gaps, and that supports or helps as a zero relative clause with that analyzed as a prenominal subject--compare a person or thing that that supports or helps." ></td>
	<td class="line x" title="440:516	It is difficult to fault this analysis and the same is true for the other false positives we have looked at." ></td>
	<td class="line x" title="441:516	Such false positives present the biggest challenge to the type of system we are attempting to develop." ></td>
	<td class="line x" title="442:516	One hopeful sign is that the analyses assigned such examples appear to have low probabilities relative to most probable correct analyses of other examples." ></td>
	<td class="line x" title="443:516	However, considerably more data will be required before we can decide whether this trend is robust enough to provide the basis for automatic identification of false positives." ></td>
	<td class="line x" title="444:516	52 Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing Using a manually disambiguated training corpus and manually tuned grammar appears feasible with the definitions sublanguage." ></td>
	<td class="line x" title="445:516	Results comparable to those obtained by Fujisaki et al.(1989) and Sharman, Jelinek, and Mercer (1990) are possible on the basis of a quite modest amount of manual effort and a very much smaller training corpus, because the parse histories contain little 'noise' and usefully reflect the semantically and pragmatically appropriate analysis in the training corpus, and because the number of failures of coverage were reduced to some extent by adding the rules specifically motivated by the training corpus." ></td>
	<td class="line x" title="447:516	Unlike Fujisaki et al. or Sharman, Jelinek, and Mercer, we did not integrate information about lexemes into the rule probabilities or make use of lexical syntactic probability." ></td>
	<td class="line x" title="448:516	It seems likely that the structural preference for local attachment might be overruled in appropriate contexts if lexeme (or better, word sense) information were taken into account." ></td>
	<td class="line x" title="449:516	The slightly worse results (relative to mean definition length) obtained for the unseen data appear to be caused more by the nonexistence of a correct analysis in a number of cases, rather than by a marked decline in the usefulness of the rule probabilities." ></td>
	<td class="line x" title="450:516	This again highlights the need to deal effectively with examples outside the coverage of the grammar." ></td>
	<td class="line x" title="451:516	9." ></td>
	<td class="line x" title="452:516	Conclusions and Further Work The system that we have developed offers partial and practical solutions to two of the three problems of corpus analysis we identified in the introduction." ></td>
	<td class="line x" title="453:516	The problem of tuning an existing grammar to a particular corpus or sublanguage is addressed partly by manual extensions to the grammar and lexicon during the semi-automatic training phase and partly by use of statistical information regarding frequency of rule use gathered during this phase." ></td>
	<td class="line x" title="454:516	The results of the experiment reported in the last section suggest that syntactic peculiarities of a sublanguage or corpus surface quite rapidly, so that manual additions to the grammar during the training phase are practical." ></td>
	<td class="line x" title="455:516	However, lexical idiosyncrasies are far less likely to be exhausted during the training phase, suggesting that it will be necessary to develop an automatic method of dealing with them." ></td>
	<td class="line x" title="456:516	In addition, the current system does not take account of differing frequencies of occurrence of lexical entries; for example, in the LOB corpus the verb believe occurs with a finite sentential complement in 90% of citations, although it is grammatical with at least five further patterns of complementation." ></td>
	<td class="line x" title="457:516	This type of lexical information, which will very likely vary between sublanguages, should be integrated into the probabilistic model." ></td>
	<td class="line x" title="458:516	This will be straightforward in terms of the model, since it merely involves associating probabilities with each distinct lexical entry for a lexeme and carrying these forward in the computation of the likelihood of each parse." ></td>
	<td class="line x" title="459:516	However, the acquisition of the statistical information from which these probabilities can be derived is more problematic." ></td>
	<td class="line x" title="460:516	Existing lexical taggers are unable to assign tags that reliably encode subcategorization information." ></td>
	<td class="line x" title="461:516	It seems likely that automatic acquisition of such information must await successful techniques for robust parsing of, at least, phrases in corpora (though Brent \[1991\] claims to be able to recognize some subcategorization patterns using large quantities of untagged text)." ></td>
	<td class="line x" title="462:516	The task of selecting the correct analysis from the set licensed by the grammar is also partially solved by the system." ></td>
	<td class="line x" title="463:516	It is clear from the results of the preliminary experiment reported in the previous section that it is possible to make the semantically and pragmatically correct analysis highly ranked, and even most highly ranked in many cases, just by exploiting the frequency of occurrence of the syntactic rules in the training data." ></td>
	<td class="line x" title="464:516	However, it is also clear that this approach will not succeed in all cases; for example, in the experiment the system appears to have developed a preference for local attachment of prepositional phrases (PPs), which is inappropriate in a 53 Computational Linguistics Volume 19, Number 1 significant number of cases." ></td>
	<td class="line x" title="465:516	It is not surprising that probabilities based solely on the frequency of syntactic rules are not capable of resolving this type of ambiguity; in an example such as John saw the man on Monday again it is the temporal interpretation of Monday that favors the adverbial interpretation (and thus nonlocal attachment)." ></td>
	<td class="line x" title="466:516	Such examples are syntactically identical to ones such as John saw the man on the bus again, in which the possibility of a locative interpretation creates a mild preference for the adjectival reading and local attachment." ></td>
	<td class="line x" title="467:516	To select the correct analysis in such cases it will be necessary to integrate information concerning word sense collocations into the probabilistic analysis." ></td>
	<td class="line x" title="468:516	In this case, we are interested in collocations between the head of a PP complement, a preposition and the head of the phrase being postmodified." ></td>
	<td class="line x" title="469:516	In general, these words will not be adjacent in the text, so it will not be possible to use existing approaches unmodified (e.g. Church and Hanks 1989), because these apply to adjacent words in unanalyzed text." ></td>
	<td class="line x" title="470:516	Hindle and Rooth (1991) report good results using a mutual information measure of collocation applied within such a structurally defined context, and their approach should carry over to our framework straightforwardly." ></td>
	<td class="line x" title="471:516	One way of integrating 'structural' collocational information into the system presented above would be to make use of the semantic component of the (ANLT) grammar. This component pairs logical forms with each distinct syntactic analysis that represent, among other things, the predicate-argument structure of the input." ></td>
	<td class="line x" title="472:516	In the resolution of PP attachment and similar ambiguities, it is 'collocation' at this level of representation that appears to be most relevant." ></td>
	<td class="line x" title="473:516	Integrating a probabilistic ranking of the resultant logical forms with the probabilistic ranking of the distinct syntactic analyses presents no problems, in principle." ></td>
	<td class="line x" title="474:516	However, once again, the acquisition of the relevant statistical information will be difficult, because it will require considerable quantities of analyzed text as training material." ></td>
	<td class="line x" title="475:516	One way to ameliorate the problem might be to reduce the size of the 'vocabulary' for which statistics need to be gathered by replacing lexical items with their superordinate terms (or a disjunction of such terms in the case of ambiguity)." ></td>
	<td class="line x" title="476:516	Copestake (1990, 1992) describes a program capable of extracting the genus term of a definition from an LDOCE definition, resolving the sense of such terms, and constructing hierarchical taxonomies of the resulting word senses." ></td>
	<td class="line x" title="477:516	Taxonomies of this form might be used to replace PP complement heads and postmodified heads in corpus data with a smaller number of superordinate concepts." ></td>
	<td class="line x" title="478:516	This would make the statistical data concerning trigrams of head-preposition-head less sparse (cf.Gale and Church 1990) and easier to gather from a corpus." ></td>
	<td class="line x" title="480:516	Nevertheless, it will only be possible to gather such data from determinately syntactically analyzed material." ></td>
	<td class="line x" title="481:516	The third problem of dealing usefully with examples outside the coverage of the grammar even after training is not addressed by the system we have developed." ></td>
	<td class="line x" title="482:516	Nevertheless, the results of the preliminary experiment for unseen examples indicate that it is a significant problem, at least with respect to lexical entries." ></td>
	<td class="line x" title="483:516	A large part of the problem with such examples is identifying them automatically." ></td>
	<td class="line x" title="484:516	Some such examples will not receive any parse and will, therefore, be easy to spot." ></td>
	<td class="line x" title="485:516	Many, though, will receive incorrect parses (one of which will be automatically ranked as the most probable) and can, therefore, only be identified manually (or perhaps on the basis of relative improbability)." ></td>
	<td class="line x" title="486:516	Jensen et al.(1983) describe an approach to parsing such examples based on parse 'fitting' or rule 'relaxation' to deal with 'ill-formed' input." ></td>
	<td class="line x" title="488:516	An approach of this type might work with input that receives no parse, but cannot help with the identification of those that only receive an incorrect one." ></td>
	<td class="line x" title="489:516	In addition, it involves annotating each grammar rule about what should be relaxed and requires that semantic interpretation can be extended to 'fitted' or partial parses (e.g. Pollack and Pereira 1988)." ></td>
	<td class="line x" title="490:516	54 Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing Sampson, Haigh, and Atwell (1989) propose a more thorough-going probabilistic approach in which the parser uses a statistically defined measure of 'closest fit' to the set of analyses contained in a 'tree bank' of training data to assign an analysis." ></td>
	<td class="line x" title="491:516	This approach attempts to ensure that analyses of new data will conform as closely as possible to existing ones, but does not require that analyses assigned are well formed with respect to any given generative grammar implicit in the tree bank analyses." ></td>
	<td class="line x" title="492:516	Sampson, Haigh, and Atwell report some preliminary results for a parser of this type that uses the technique of simulated annealing to assign the closest fitting analysis on the basis of initial training on the LOB treebank and automatic updating of its statistical data on the basis of further parsed examples." ></td>
	<td class="line x" title="493:516	Sampson, Haigh, and Atwell give their results in terms of a similarity measure with respect to correct analyses assigned by hand." ></td>
	<td class="line x" title="494:516	For a 13-sentence sample the mean similarity measure was 80%, and only one example received a fully correct analysis." ></td>
	<td class="line x" title="495:516	These results suggest that the technique is not reliable enough for practical corpus analysis, to date." ></td>
	<td class="line x" title="496:516	In addition, the analyses assigned, on the basis of the LOB treebank scheme, are not syntactically determinate (for example, syntactic relations in unbounded dependency constructions are not represented)." ></td>
	<td class="line x" title="497:516	A more promising approach with similar potential robustness would be to infer a probabilistic grammar using Baum-Welch re-estimation from a given training corpus and predefined category set, following Lari and Young (1990) and Pereira and Schabes (1992)." ></td>
	<td class="line x" title="498:516	This approach has the advantage that the resulting grammar defines a well-defined set of analyses for which rules of compositional interpretation might be developed." ></td>
	<td class="line x" title="499:516	However, the technique is limited in several ways; firstly, such grammars are restricted to small (maximum about 15 nonterminal) CNF CFGs because of the computational cost of iterative re-estimation with an algorithm polynomial in sentence length and nonterminal category size; and secondly, because some form of supervised training will be essential if the analyses assigned by the grammar are to be linguistically motivated." ></td>
	<td class="line x" title="500:516	Immediate prospects for applying such techniques to realistic NL grammars do not seem promising--the ANLT backbone grammar discussed in Section 4 contains almost 500 categories." ></td>
	<td class="line x" title="501:516	However, Briscoe and Waegner (1992) describe an experiment in which, firstly, Baum-Welch re-estimation was used in conjunction with other more linguistically motivated constraints on the class of grammars that could be inferred, such as 'headedness'; and secondly, initial probabilities were heavily biased in favor of manually coded, linguistically highly plausible rules." ></td>
	<td class="line x" title="502:516	This approach resulted in a simple tag sequence grammar often able to assign coherent and semantically/pragmatically plausible analyses to tag sequences drawn from the Spoken English Corpus." ></td>
	<td class="line x" title="503:516	By combining such techniques and relaxing the CNF constraint, for example, by adopting the trellis algorithm version of Baum-Welch re-estimation (Kupiec 1991), it might be possible to create a computationally tractable system operating with a realistic NL grammar that would only infer a new rule from a finite space of linguistically motivated possibilities in the face of parse failure or improbability." ></td>
	<td class="line x" title="504:516	In the shorter term, such techniques combined with simple tag sequence grammars might yield robust phrase-level 'skeleton' parsers that could be used as corpus analysis tools." ></td>
	<td class="line x" title="505:516	The utility of the system reported here would be considerably improved by a more tractable approach to probabilistically unpacking the packed parse forest than exhaustive search." ></td>
	<td class="line x" title="506:516	Finding the n-best analyses would allow us to recover analyses for longer sentences where a parse forest is constructed and would make the approach generally more efficient." ></td>
	<td class="line x" title="507:516	Carroll and Briscoe (1992) present a heuristic algorithm for parse forest unpacking that interleaves normalization of competing sub-analyses with best-first extraction of the n most probable analyses." ></td>
	<td class="line x" title="508:516	Normalization of competing sub-analyses with respect to the longest derivation both allows us to prune the search probabilisti55 Computational Linguistics Volume 19, Number 1 cally and to treat the probability of analyses as the product of the probability of their sub-analyses, without biasing the system in favor of shorter derivations." ></td>
	<td class="line x" title="509:516	This modified version of the system presented here is able to return analyses for sentences over 31 words in length, yields slightly better results on a replication of the experiment reported in Section 8, and the resultant parser is approximately three times faster at returning the three highest-ranked parsers than that presented here." ></td>
	<td class="line x" title="510:516	In conclusion, the main positive points of the paper are that 1) LR parse tables can be used to define a more context-dependent and adequate probabilistic model of NL, 2) predictive LR parse tables can be constructed automatically from unification-based grammars in standard notation, 3) effective parse table construction and representation techniques can be defined for realistically sized ambiguous NL grammars, 4) semiautomatic LR based parse techniques can be used to efficiently construct training corpora, and 5) the LR parser and ANLT grammar jointly define a useful probabilistic model into which probabilities concerning lexical subcategorization and structurally defined word sense collocations could be integrated." ></td>
	<td class="line x" title="511:516	Acknowledgments This research is supported by SERC/DTI-IED project 4/1/1261 'Extensions to the Alvey Natural Language Tools' and by ESPRIT BRA 3030 'Acquisition of Lexical Information from Machine-Readable Dictionaries'." ></td>
	<td class="line x" title="512:516	We would like to thank Longman Group Ltd. for allowing us access to the LDOCE MRD and Ann Copestake and Antonio Sanfilippo for considerable help in the analysis of the LDOCE noun definition corpus." ></td>
	<td class="line x" title="513:516	Richard Sharman kindly calculated the perplexity measures for this corpus." ></td>
	<td class="line x" title="514:516	In addition, Hiyan Alshawi, David Weir, and Steve Young have helped clarify our thinking and made several suggestions that have influenced the way this research has developed." ></td>
	<td class="line x" title="515:516	Alex Lascarides and four anonymous reviewers' comments on earlier drafts were very helpful to us in preparing the final version." ></td>
	<td class="line x" title="516:516	All errors and mistakes remain our responsibility." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P93-1003
An Algorithm For Finding Noun Phrase Correspondences In Bilingual Corpora
Kupiec, Julian;"></td>
	<td class="line x" title="1:132	AN ALGORITHM FOR FINDING NOUN PHRASE CORRESPONDENCES IN BILINGUAL CORPORA Julian Kupiec Xerox Palo Alto Research Center 3333 Coyote Hill Road, Palo Alto, CA kupiec@parc.xerox.com 94304 Abstract The paper describes an algorithm that employs English and French text taggers to associate noun phrases in an aligned bilingual corpus." ></td>
	<td class="line x" title="2:132	The taggets provide part-of-speech categories which are used by finite-state recognizers to extract simple noun phrases for both languages." ></td>
	<td class="line x" title="3:132	Noun phrases are then mapped to each other using an iterative re-estimation algorithm that bears similarities to the Baum-Welch algorithm which is used for training the taggers." ></td>
	<td class="line x" title="4:132	The algorithm provides an alternative to other approaches for finding word correspondences, with the advantage that linguistic structure is incorporated." ></td>
	<td class="line x" title="5:132	Improvements to the basic algorithm are described, which enable context to be accounted for when constructing the noun phrase mappings." ></td>
	<td class="line x" title="6:132	INTRODUCTION Areas of investigation using bilingual corpora have included the following:  Automatic sentence alignment \[Kay and RSscheisen, 1988, Brown eL al. , 1991a, Gale and Church, 1991b\]." ></td>
	<td class="line x" title="7:132	 Word-sense disambiguation \[Dagan el al. , 1991, Brown et ai., 1991b, Church and Gale, 1991\]." ></td>
	<td class="line x" title="9:132	 Extracting word correspondences \[Gale and Church, 1991a\]." ></td>
	<td class="line x" title="10:132	 Finding bilingual collocations \[Smadja, 1992\]." ></td>
	<td class="line x" title="11:132	 Estimating parameters for statistically-based machine translation \[Brown et al. , 1992\]." ></td>
	<td class="line x" title="12:132	The work described here makes use of the aligned Canadian Hansards \[Gale and Church, 1991b\] to obtain noun phrase correspondences between the English and French text." ></td>
	<td class="line x" title="13:132	The term 'correspondence' is used here to signify a mapping between words in two aligned sentences." ></td>
	<td class="line x" title="14:132	Consider an English sentence Ei and a French sentence Fi which are assumed to be approximate translations of each other." ></td>
	<td class="line x" title="15:132	The subscript i denotes the i'th alignment of sentences in both languages." ></td>
	<td class="line x" title="16:132	A word sequence in E/is defined here as the correspondence of another sequence in Fi if the words of one sequence are considered to represent the words in the other." ></td>
	<td class="line x" title="17:132	Single word correspondences have been investigated \[Gale and Church, 1991a\] using a statistic operating on contingency tables." ></td>
	<td class="line x" title="18:132	An algorithm for producing collocational correspondences has also been described \[Smadja, 1992\]." ></td>
	<td class="line x" title="19:132	The algorithm involves several steps." ></td>
	<td class="line x" title="20:132	English collocations are first extracted from the English side of the corpus." ></td>
	<td class="line x" title="21:132	Instances of the English collocation are found and the mutual information is calculated between the instances and various single word candidates in aligned French sentences." ></td>
	<td class="line x" title="22:132	The highest ranking candidates are then extended by another word and the procedure is repeated until a corresponding French collocation having the highest mutual information is found." ></td>
	<td class="line x" title="23:132	An alternative approach is described here, which employs simple iterative re-estimation." ></td>
	<td class="line x" title="24:132	It is used to make correspondences between simple noun phrases that have been isolated in corresponding sentences of each language using finitestate recognizers." ></td>
	<td class="line x" title="25:132	The algorithm is applicable for finding single or multiple word correspondences and can accommodate additional kinds of phrases." ></td>
	<td class="line x" title="26:132	In contrast to the other methods that have been mentioned, the algorithm can be extended in a straightforward way to enable correct correspondences to be made in circumstances where numerous low frequency phrases are involved." ></td>
	<td class="line x" title="27:132	This is important consideration because in large text corpora roughly a third of the word types only occur once." ></td>
	<td class="line x" title="28:132	Several applications for bilingual correspondence information have been suggested." ></td>
	<td class="line x" title="29:132	They can be used in bilingual concordances, for automatically constructing bilingual lexicons, and probabilistically quantified correspondences may be useful for statistical translation methods." ></td>
	<td class="line x" title="30:132	COMPONENTS Figure 1 illustrates how the corpus is analyzed." ></td>
	<td class="line x" title="31:132	The words in sentences are first tagged with their 17 corresponding part-of-speech categories." ></td>
	<td class="line x" title="32:132	Each tagger contains a hidden Markov model (HMM), which is trained using samples of raw text from the Hansards for each language." ></td>
	<td class="line x" title="33:132	The taggers are robust and operate with a low error rate \[Kupiec, 1992\]." ></td>
	<td class="line x" title="34:132	Simple noun phrases (excluding pronouns and digits) are then extracted from the sentences by finite-state recognizers that are specified by regular expressions defined in terms of part-ofspeech categories." ></td>
	<td class="line x" title="35:132	Simple noun phrases are identified because they are most reliably recognized; it is also assumed that they can be identified unambiguously." ></td>
	<td class="line x" title="36:132	The only embedding that is allowed is by prepositional phrases involving 'of' in English and 'de' in French, as noun phrases involving them can be identified with relatively low error (revisions to this restriction are considered later)." ></td>
	<td class="line x" title="37:132	Noun phrases are placed in an index to associate a unique identifier with each one." ></td>
	<td class="line x" title="38:132	A noun phrase is defined by its word sequence, excluding any leading determiners." ></td>
	<td class="line x" title="39:132	Singular and plural forms of common nouns are thus distinct and assigned different positions in the index." ></td>
	<td class="line x" title="40:132	For each sentence corresponding to an alignment, the index positions of all noun phrases in the sentence are recorded in a separate data structure, providing a compact representation of the corpus." ></td>
	<td class="line x" title="41:132	So far it has been assumed (for the sake of simplicity) that there is always a one-to-one mapping between English and French sentences." ></td>
	<td class="line x" title="42:132	In practice, if an alignment program produces blocks of several sentences in one or both languages, this can be accommodated by treating the block instead as a single bigger 'compound sentence' in which noun phrases have a higher number of possible correspondences." ></td>
	<td class="line x" title="43:132	THE MAPPING ALGORITHM Some terminology is necessary to describe the algorithm concisely." ></td>
	<td class="line x" title="44:132	Let there be L total alignments in the corpus; then Ei is the English sentence for alignment i. Let the function (Ei) be the number of noun phrases identified in the sentence." ></td>
	<td class="line x" title="45:132	If there are k of them, k = (Ei), and they can be referenced by j = 1k." ></td>
	<td class="line x" title="46:132	Considering the j'th noun phrase in sentence Ei, the function I~(Ei, j) produces an identifier for the phrase, which is the position of the phrase in the English index." ></td>
	<td class="line x" title="47:132	If this phrase is at position s, then I~(Ei,j) = s. In turn, the French sentence Fi will contain (Fi) noun phrases and given the p'th one, its position in the French index will be given by/~(Fi, p)." ></td>
	<td class="line x" title="48:132	It will also be assumed that there are a total of VE and Vr phrases in the English and French indexes respectively." ></td>
	<td class="line x" title="49:132	Finally, the indicator function I 0 has the value unity if its argument is true, and zero otherwise." ></td>
	<td class="line x" title="50:132	Assuming these definitions, the algorithm is I English sentence E i 1 I English Tagger I I English NP Recognizer I I n0.sh'o ex I I Bilingual Corpus I rth alignment I French FTntence I French Tagger I I French I NP Recognizer I Frenchlndex I Figure 1: Component Layout stated in Figure 2." ></td>
	<td class="line x" title="51:132	The equations assume a directionality: finding French 'target' correspondences for English 'source' phrases." ></td>
	<td class="line x" title="52:132	The algorithm is reversible, by swapping E with F. The model for correspondence is that a source noun phrase in Ei is responsible for producing the various different target noun phrases in Fi with correspondingly different probabilities." ></td>
	<td class="line x" title="53:132	Two quantities are calculated; Cr(s, t) and Pr(s, t)." ></td>
	<td class="line x" title="54:132	Computation proceeds by evaluating Equation (1), Equation (2) and then iteratively applying Equations (3) and (2); r increasing with each successive iteration." ></td>
	<td class="line x" title="55:132	The argument s refers to the English noun phrase nps(s) having position s in the English index, and the argument t refers to the French noun phrase npF(t) at position t in the French index." ></td>
	<td class="line x" title="56:132	Equation (1) assumes that each English noun phrase in Ei is initially equally likely to correspond to each French noun phrase in Fi." ></td>
	<td class="line x" title="57:132	All correspondences are thus equally weighted, reflecting a state of ignorance." ></td>
	<td class="line x" title="58:132	Weights are summed over the corpus, so noun phrases that co-occur in several sentences will have larger sums." ></td>
	<td class="line x" title="59:132	The weights C0(s, t) can be interpreted as the mean number of times that npF(t) corresponds to apE(s) given the corpus and the initial assumption of equiprobable correspondences." ></td>
	<td class="line x" title="60:132	These weights can be used to form a new estimate of the probability that npF(t) corresponds to npE(s), by considering the mean number of times npF(t) corresponds to apE(s) as a fraction of the total mean number of correspondences for apE(s), as in Equation (2)." ></td>
	<td class="line x" title="61:132	The procedure is then iterated using Equations (3), and (2) to obtain successively refined, convergent estimates of the prob18 Co(,t) = = cr(,t) = r>O VE>s>I Vv>t>l L (E~) (F0 1 E E E I(tt(Ei' J) = s)l(tt(Fi' k) = t) (F,) i=1 j=l k=l Cr-l(S,t) vF Eq=l Cr-l(s, q) L (E0 (F0 E E E I(#(Ei,j) = s)I(tt(Fi,k) = t)Pr_l(s,t) i=I j=l k=l (1) (2) (3) Figure 2: The Algorithm ability that ripE(t) corresponds to ripE(s)." ></td>
	<td class="line x" title="62:132	The probability of correspondences can be used as a method of ranking them (occurrence counts can be taken into account as an indication of the reliability of a correspondence)." ></td>
	<td class="line x" title="63:132	Although Figure 2 defines the coefficients simply, the algorithm is not implemented literally from it." ></td>
	<td class="line x" title="64:132	The algorithm employs a compact representation of the correspondences for efficient operation." ></td>
	<td class="line x" title="65:132	An arbitrarily large corpus can be accommodated by segmenting it appropriately." ></td>
	<td class="line x" title="66:132	The algorithm described here is an instance of a general approach to statistical estimation, represented by the EM algorithm \[Dempster et al. , 1977\]." ></td>
	<td class="line x" title="67:132	In contrast to reservations that have been expressed \[Gale and Church, 1991a\] about using the EM algorithm to provide word correspondences, there have been no indications that prohibitive amounts of memory might be required, or that the approach lacks robustness." ></td>
	<td class="line x" title="68:132	Unlike the other methods that have been mentioned, the approach has the capability to accommodate more context to improve performance." ></td>
	<td class="line x" title="69:132	RESULTS A sample of the aligned corpus comprising 2,600 alignments was used for testing the algorithm (not all of the alignments contained sentences)." ></td>
	<td class="line x" title="70:132	4,900 distinct English noun phrases and 5,100 distinct French noun phrases were extracted from the sample." ></td>
	<td class="line x" title="71:132	When forming correspondences involving long sentences with many clauses, it was observed that the position at which a noun phrase occurred in El was very roughly proportional to the corresponding noun phrase in Fi." ></td>
	<td class="line x" title="72:132	In such cases it was not necessary to form correspondences with all noun phrases in Fi for each noun phrase in Ei." ></td>
	<td class="line x" title="73:132	Instead, the location of a phrase in Ei was mapped linearly to a position in Fi and correspondences were formed for noun phrases occurring in a window around that position." ></td>
	<td class="line x" title="74:132	This resulted in a total of 34,000 correspondences." ></td>
	<td class="line x" title="75:132	The mappings are stable within a few (2-4) iterations." ></td>
	<td class="line x" title="76:132	In discussing results, a selection of examples will be presented that demonstrates the strengths and weaknesses of the algorithm." ></td>
	<td class="line x" title="77:132	To give an indication of noun phrase frequency counts in the sample, the highest ranking correspondences are shown in Table 1." ></td>
	<td class="line x" title="78:132	The figures in columns (1) and (3) indicate the number of instances of the noun phrase to their right." ></td>
	<td class="line x" title="79:132	185 Mr. Speaker 187 M. Le PrSsident 128 Government 141 gouvernement 60 Prime Minister 65 Premier Ministre 63 Hon." ></td>
	<td class="line x" title="80:132	Member 66 d6put6 67 House 68 Chambre Table 1: Common correspondences To give an informal impression of overall performance, the hundred highest ranking correspondences were inspected and of these, ninety were completely correct." ></td>
	<td class="line x" title="81:132	Less frequently occurring noun phrases are also of interest for purposes of evaluation; some of these are shown in Table 2." ></td>
	<td class="line x" title="82:132	32 Atlantic Canada Opportunities Agency 5 DREE 1 late spring 1 whole issue of free trade 23 Agence de promotion 6conomique du Canada atlantique 4 MEER 1 fin du printemps 1 question du libre-~change Table 2: Other correspondences The table also illustrates an unembedded English noun phrase having multiple prepositional 19 phrases in its French correspondent." ></td>
	<td class="line x" title="83:132	Organizational acronyms (which may be not be available in general-purpose dictionaries) are also extracted, as the taggers are robust." ></td>
	<td class="line x" title="84:132	Even when a noun phrase only occurs once, a correct correspondence can be found if there are only single noun phrases in each sentence of the alignment." ></td>
	<td class="line x" title="85:132	This is demonstrated in the last row of Table 2, which is the result of the following alignment: Ei: 'The whole issue of free trade has been mentioned'." ></td>
	<td class="line x" title="86:132	Fi: 'On a mentionn~ la question du libre~change'." ></td>
	<td class="line x" title="87:132	Table 3 shows some incorrect correspondences produced by the algorithm (in the table, 'usine' means 'factory')." ></td>
	<td class="line x" title="88:132	11 r  tho obtraining I 01 asia0 I 1 mix of on-the-job 6 usine Table 3 The sentences that are responsible for these correspondences illustrate some of the problems associated with the correspondence model: Ei: 'They use what is known as the dual system in which there is a mix of on-the-job and offthe-job training'." ></td>
	<td class="line x" title="89:132	Fi: 'Ils ont recours  une formation mixte, partie en usine et partie hors usine'." ></td>
	<td class="line x" title="90:132	The first problem is that the conjunctive modifiers in the English sentence cannot be accommodated by the noun phrase recognizer." ></td>
	<td class="line x" title="91:132	The tagger also assigned 'on-the-job' as a noun when adjectival use would be preferred." ></td>
	<td class="line x" title="92:132	If verb correspondences were included, there is a mismatch between the three that exist in the English sentence and the single one in the French." ></td>
	<td class="line x" title="93:132	If the English were to reflect the French for the correspondence model to be appropriate, the noun phrases would perhaps be 'part in the factory' and 'part out of the factory'." ></td>
	<td class="line x" title="94:132	Considered as a translation, this is lame." ></td>
	<td class="line x" title="95:132	The majority of errors that occur are not the result of incorrect tagging or noun phrase recognition, but are the result of the approximate nature of the correspondence model." ></td>
	<td class="line x" title="96:132	The correspondences in Table 4 are likewise flawed (in the table, 'souris' means 'mouse' and 'tigre de papier' means 'paper tiger'): 1 toothless tiger 1 souris 1 toothless tiger 1 tigre de papier 1 roaring rabbit 1 souris 1 roaring rabbit 1 tigre de papier Table 4 These correspondences are the result of the following sentences: Ei: 'It is a roaring rabbit, a toothless tiger'." ></td>
	<td class="line x" title="97:132	Fi: 'C' est un tigre de papier, un souris qui rugit'." ></td>
	<td class="line x" title="98:132	In the case of the alliterative English phrase 'roaring rabbit', the (presumably) rhetorical aspect is preserved as a rhyme in 'souris qui rugit'; the result being that 'rabbit' corresponds to 'souris' (mouse)." ></td>
	<td class="line x" title="99:132	Here again, even if the best correspondence were made the result would be wrong because of the relatively sophisticated considerations involved in the translation." ></td>
	<td class="line x" title="100:132	EXTENSIONS As regards future possibilities, the algorithm lends itself to a range of improvements and applications, which are outlined next." ></td>
	<td class="line x" title="101:132	Finding Word Correspondences: The algorithm finds corresponding noun phrases but provides no information about word-level correspondences within them." ></td>
	<td class="line x" title="102:132	One possibility is simply to eliminate the tagger and noun phrase recognizer (treating all words as individual phrases of length unity and having a larger number of correspondences)." ></td>
	<td class="line x" title="103:132	Alternatively, the following strategy can be adopted, which involves fewer total correspondences." ></td>
	<td class="line x" title="104:132	First, the algorithm is used to build noun phrase correspondences, then the phrase pairs that are produced are themselves treated as a bilingual noun phrase corpus." ></td>
	<td class="line x" title="105:132	The algorithm is then employed again on this corpus, treating all words as individual phrases." ></td>
	<td class="line x" title="106:132	This results in a set of single word correspondences for the internal words in noun phrases." ></td>
	<td class="line x" title="107:132	Reducing Ambiguity: The basic algorithm assumes that noun phrases can be uniquely identified in both languages, which is only true for simple noun phrases." ></td>
	<td class="line x" title="108:132	The problem of prepositional phrase attachment is exemplified by the following corresp on den ces: 16 Secretary 20 secrdtaire d' Etat of State 16 Secretary 19 Affaires extdrieures of State 16 External Affairs 19 Affaires extdrieures 16 External Affairs 20 secrdtaire d' Etat Table 5 The correct English and French noun phrases are 'Secretary of State for External Affairs' and 'secr~taire d' Etat aux Affaires ext~rieures'." ></td>
	<td class="line x" title="109:132	If prepositional phrases involving 'for' and '~' were also permitted, these phrases would be correctly 20 identified; however many other adverbial prepositional phrases would also be incorrectly attached to noun phrases." ></td>
	<td class="line x" title="110:132	If all embedded prepositional phrases were permitted by the noun phrase recognizer, the algorithm could be used to reduce the degree of ambiguity between alternatives." ></td>
	<td class="line x" title="111:132	Consider a sequence np~ppe of an unembedded English noun phrase npe followed by a prepositional phrase PPe, and likewise a corresponding French sequence nplpp I. Possible interpretations of this are: 1." ></td>
	<td class="line x" title="112:132	The prepositional phrase attaches to the noun phrase in both languages." ></td>
	<td class="line x" title="113:132	2." ></td>
	<td class="line x" title="114:132	The prepositional phrase attaches to the noun phrase in one language and does not in the other." ></td>
	<td class="line x" title="115:132	3." ></td>
	<td class="line x" title="116:132	The prepositional phrase does not attach to the noun phrase in either language." ></td>
	<td class="line x" title="117:132	If the prepositional phrases attach to the noun phrases in both languages, they are likely to be repeated in most instances of the noun phrase; it is less likely that the same prepositional phrase will be used adverbially with each instance of the noun phrase." ></td>
	<td class="line x" title="118:132	This provides a heuristic method for reducing ambiguity in noun phrases that occur several times." ></td>
	<td class="line x" title="119:132	The only modifications required to the algorithm are that the additional possible noun phrases and correspondences between them must be included." ></td>
	<td class="line x" title="120:132	Given thresholds on the number of occurrences and the probability of the correspondence, the most likely correspondence can be predicted." ></td>
	<td class="line x" title="121:132	Including Context: In the algorithm, correspondences between source and target noun phrases are considered irrespectively of other correspondences in an alignment." ></td>
	<td class="line x" title="122:132	This does not make the best use of the information available, and can be improved upon." ></td>
	<td class="line x" title="123:132	For example, consider the following alignment: El: 'The Bill was introduced just before Christmas'." ></td>
	<td class="line x" title="124:132	Fi: 'Le projet de lot a ~t~ present~ juste avant le cong~ des F~tes'." ></td>
	<td class="line x" title="125:132	Here it is assumed that there are many instances of the correspondence 'Bill' and 'projet de lot', but only one instance of 'Christmas' and 'cong~ des F~tes'." ></td>
	<td class="line x" title="126:132	This suggests that 'Bill' corresponds to 'projet de lot' with a high probability and that 'Christmas' likewise corresponds strongly to 'cong~ des F~tes'." ></td>
	<td class="line x" title="127:132	However, the model will assert that 'Christmas' corresponds to 'projet de lot' and to 'cong~ des F~tes' with equal probability, no matter how likely the correspondence between 'Bill' and 'projet de lot'." ></td>
	<td class="line x" title="128:132	The model can be refined to reflect this situation by considering the joint probability that a target npr(t) corresponds to a source ripE(s) and all the other possible correspondences in the alignment are produced." ></td>
	<td class="line oc" title="129:132	This situation is very similar to that involved in training HMM text taggers, where joint probabilities are computed that a particular word corresponds to a particular part-ofspeech, and the rest of the words in the sentence are also generated (e.g. \[Cutting et al. , 1992\])." ></td>
	<td class="line x" title="130:132	CONCLUSION The algorithm described in this paper provides a practical means for obtaining correspondences between noun phrases in a bilingual corpus." ></td>
	<td class="line x" title="131:132	Linguistic structure is used in the form of noun phrase recognizers to select phrases for a stochastic model which serves as a means of minimizing errors due to the approximations inherent in the correspondence model." ></td>
	<td class="line x" title="132:132	The algorithm is robust, and extensible in several ways." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="A94-1008
Tagging Accurately - Don't Guess If You Know
Tapanainen, Pasi;Voutilainen, Atro;"></td>
	<td class="line x" title="1:186	Tagging accuratelyDon't guess if you know Pasi Tapanainen Rank Xerox Research Centre Grenoble Laboratory 6, chemin de Maupertuis 38240 Meylan, France Pasi.Tapanainen@xerox.fr Atro Voutilainen Research Unit for Computational Linguistics University of Helsinki P.O. Box 4 00014 University of Helsinki, Finland Atro.Voutilainen@helsinki.fi Abstract We discuss combining knowledge-based (or rule-based) and statistical part-of-speech taggers." ></td>
	<td class="line x" title="2:186	We use two mature taggers, ENGCG and Xerox Tagger, to independently tag the same text and combine the results to produce a fully disambiguated text." ></td>
	<td class="line x" title="3:186	In a 27000 word test sample taken from a previously unseen corpus we achieve 98.5 % accuracy." ></td>
	<td class="line x" title="4:186	This paper presents the data in detail." ></td>
	<td class="line x" title="5:186	We describe the problems we encountered in the course of combining the two taggers and discuss the problem of evaluating taggers." ></td>
	<td class="line x" title="6:186	1 Introduction This paper combines knowledge-based and statistical methods for part-of-speech disambiguation, taking advantage of the best features of both approaches." ></td>
	<td class="line x" title="7:186	The resulting output is fully and accurately disambiguated." ></td>
	<td class="line x" title="8:186	We demonstrate a system that accurately resolves most part-of-speech ambiguities by means of syntactic rules and employs a stochastic tagger to eliminate the remaining ambiguity." ></td>
	<td class="line x" title="9:186	The overall results are clearly superior to the reported results for stateof-the-art stochastic systems." ></td>
	<td class="line x" title="10:186	The input to our part-of-speech disambiguator consists of lexically analysed sentences." ></td>
	<td class="line x" title="11:186	Many words have more than one analysis." ></td>
	<td class="line x" title="12:186	The task of the disambiguator is to select the contextually appropriate alternative by discarding the improper ones." ></td>
	<td class="line x" title="13:186	Some of the inappropriate alternatives can be discarded reliably by linguistic rules." ></td>
	<td class="line x" title="14:186	For example, we can safely exclude a finite-verb reading if the previous word is an unambiguous determiner." ></td>
	<td class="line x" title="15:186	The application of such rules does not always result in a fully disambiguated output (e.g. adjective-noun ambiguities may be left pending) but the amount of ambiguity is reduced with next to no errors." ></td>
	<td class="line x" title="16:186	Using a large collection of linguistic rules, a lot of ambiguity can be resolved, though some cases remain unresolved." ></td>
	<td class="line x" title="17:186	47 The rule system may also exploit the fact that certain linguistically possible configurations have such a low frequency in certain types of text that they can be ignored." ></td>
	<td class="line x" title="18:186	A rule that assumes that a preposition is followed by a noun phrase may be a useful heuristic rule in a practical system, considering that dangling prepositions occur relatively infrequently." ></td>
	<td class="line x" title="19:186	Such heuristic rules can be applied to resolve some of the ambiguities that survive the more reliable grammar rules." ></td>
	<td class="line x" title="20:186	A stochastic disambiguator selects the most likely tag for a word by consulting the neighbouring tags or words, typically in a two or three word window." ></td>
	<td class="line x" title="21:186	Because of the limited size of the window, the choices made by a stochastic disambiguator are often quite naive from the linguistic point of view." ></td>
	<td class="line x" title="22:186	For instance, the correct resolution of a preposition vs. subordinating conjunction ambiguity in a small window is often impossible because both morphological categories can have identical local contexts (for instance, both can be followed by a noun phrase)." ></td>
	<td class="line x" title="23:186	Some of the errors made by a stochastic system can be avoided in a knowledge-based system because the rules can refer to words and tags in the scope of the entire sentence." ></td>
	<td class="line x" title="24:186	We use both types of disambiguators." ></td>
	<td class="line x" title="25:186	The knowledge-based disambiguator does not resolve all ambiguities but the choices it makes are nearly always correct." ></td>
	<td class="line x" title="26:186	The statistical disambiguator resolves all ambiguities but its decisions are not very reliable." ></td>
	<td class="line x" title="27:186	We combine these two disambiguators; here this means that the text is analysed with both systems." ></td>
	<td class="line x" title="28:186	Whenever there is a conflict between the systems, we trust the analysis proposed by the knowledgebased system." ></td>
	<td class="line x" title="29:186	Whenever the knowledge-based system leaves an ambiguity unresolved, we select that alternative which is closest to the selection made by the statistical system." ></td>
	<td class="line oc" title="30:186	The two systems we use are ENGCG (Karlsson et al. , 1994) and the Xerox Tagger (Cutting et al. , 1992)." ></td>
	<td class="line n" title="31:186	We discuss problems caused by the fact that these taggers use different tag sets, and present the results obtained by applying the combined taggers to a previously unseen sample of text." ></td>
	<td class="line x" title="32:186	2 The taggers in outline 2.1 English Constraint Grammar Parser The English Constraint Grammar Parser, ENGCG (Voutilainen et al. , 1992; Karlsson el al. , 1994), is based on Constraint Grammar, a parsing framework proposed by Fred Karlsson (1990)." ></td>
	<td class="line x" title="33:186	It was developed 1989-1993 at the Research Unit for Computational Linguistics, University of Helsinki, by Atro Voutilainen, Juha Heikkil~i and Arto Anttila; later on, Timo Jrvinen has extended the syntactic description, and Pasi Tapanainen has made a new fast implementation of the CG parsing program." ></td>
	<td class="line x" title="34:186	ENGCG is primarily designed for the analysis of standard written English of the British and American varieties." ></td>
	<td class="line x" title="35:186	In the development and testing of the system, over 100 million words of running text have been used." ></td>
	<td class="line x" title="36:186	The ENGTWOL lexicon is based on the two-level model (Koskenniemi, 1983)." ></td>
	<td class="line x" title="37:186	The lexicon contains over 80,000 lexical entries, each of which represents all inflected and central derived forms of the lexemes." ></td>
	<td class="line x" title="38:186	The lexicon also employs a collection of tags for part of speech, inflection, derivation and even syntactic category (e.g. verb classification)." ></td>
	<td class="line x" title="39:186	Usually less than 5 % of all word-form tokens in running text are not recognised by the morphological analyser." ></td>
	<td class="line x" title="40:186	Therefore the system employs a rule-based heuristic module that provides all unknown words with one or more readings." ></td>
	<td class="line x" title="41:186	About 99.5 % of words not recognised by the ENGTWOL analyser itself get a correct analysis from the heuristic module." ></td>
	<td class="line x" title="42:186	The module contains a list of prefixes and suffixes, and possible analyses for matching words." ></td>
	<td class="line x" title="43:186	For instance, words beginning with un and ending inal are marked as adjectives." ></td>
	<td class="line x" title="44:186	The grammar for morphological disambiguation (Voutilainen, 1994) is based on 23 linguistic generalisations about the form and function of essentially syntactic constructions, e.g. the form of the noun phrase, prepositional phrase, and finite verb chain." ></td>
	<td class="line x" title="45:186	These generalisations are expressed as 1,100 highly reliable 'grammar-based' and some 200 less reliable add-on 'heuristic' constraints, usually in a partial and negative fashion." ></td>
	<td class="line x" title="46:186	Using the 1,100 best constraints results in a somewhat ambiguous output." ></td>
	<td class="line x" title="47:186	Usually there are about 1.04-1.07 morphological analyses per word." ></td>
	<td class="line x" title="48:186	Usually at least 997 words out of every thousand retain the contextually appropriate morphological reading, i.e. the recall usually is at least 99.7 %." ></td>
	<td class="line x" title="49:186	If the heuristic constraints are also used, the ambiguity rate falls to 1.02-1.04 readings per word, with an overall recall of about 99.5 %." ></td>
	<td class="line x" title="50:186	This accuracy compares very favourably with results reported in (de Marcken, 1990; Weisehedel et al. , 1993; Kempe, 1994) for instance, to reach the recall of 99.3 %, the system by (Weischedel et al. , 1993) has to leave as many as three readings per word in its output." ></td>
	<td class="line oc" title="51:186	2.2 Xerox Tagger The Xerox Tagger 1, XT, (Cutting et al. , 1992) is a statistical tagger made by Doug Cutting, Julian Kupiec, Jan Pedersen and Penelope Sibun in Xerox PARC." ></td>
	<td class="line o" title="52:186	It was trained on the untagged Brown Corpus (Francis and Kubera, 1982)." ></td>
	<td class="line x" title="53:186	The lexicon is a word-list of 50,000 words with alternative tags." ></td>
	<td class="line x" title="54:186	Unknown words are analysed according to their suffixes." ></td>
	<td class="line x" title="55:186	The lexicon and suffix tables are implemented as tries." ></td>
	<td class="line x" title="56:186	For instance, for the word live there are the following alternative analyses: JJ (adjective) and VB (uninflected verb)." ></td>
	<td class="line x" title="57:186	Unknown words not recognised by suffix tables get all tags from a specific set (called open-class)." ></td>
	<td class="line o" title="58:186	The tagger itself is based on the Hidden Markov Model (Baum, 1972) and word equivalence classes (Kupiec, 1989)." ></td>
	<td class="line o" title="59:186	Although the tagger is trained with the untagged Brown corpus, there are several ways to 'force' it to learn." ></td>
	<td class="line x" title="60:186	 The symbol biases represent a kind of lexical probabilities for given word equivalence classes." ></td>
	<td class="line x" title="61:186	 The transition biases can be used for saying that it is likely or unlikely that a tag is followed by some specific tag." ></td>
	<td class="line x" title="62:186	The biases serve as default values for the Hidden Markov Model before the training." ></td>
	<td class="line x" title="63:186	 Some rare readings may be removed from the lexicon to prevent the tagger from selecting them." ></td>
	<td class="line x" title="64:186	 There are some training parameters, like the number of iterations (how many times the same block of text is used in training) and the size of the block of the text used for training." ></td>
	<td class="line x" title="65:186	 The choice of the training corpus affects the result." ></td>
	<td class="line oc" title="66:186	The tagger is reported (Cutting el al. , 1992) to have a better than 96 % accuracy in the analysis of parts of the Brown Corpus." ></td>
	<td class="line x" title="67:186	The accuracy is similar to other probabilistic taggers." ></td>
	<td class="line x" title="68:186	3 Grammatical representations of the taggers A major difference between a knowledge-based and a probabilistic tagger is that the knowledge-based tagger needs as much information as possible while the probabilistic tagger requires some compact set of tags that does not make too many distinctions between similar words." ></td>
	<td class="line x" title="69:186	The difference can be seen by comparing the Brown Corpus tag set (used by XT) with the ENGCG tag set." ></td>
	<td class="line x" title="70:186	The ENGTWOL morphological analyser employs 139 tags." ></td>
	<td class="line x" title="71:186	Each word usually receives several tags (see Figure 1)." ></td>
	<td class="line x" title="72:186	There are also 'auxiliary' tags for derivational and syntactic information that do not 1 We use version 1." ></td>
	<td class="line x" title="73:186	48 ENGCG has V PRES SG3 VEIN have V PRES -SG3 VEIN V INF V IMP VFIN V SUBJUNCTIVE VFIN was V PAST SG1,3 VEIN do V PRES -SG3 VEIN V INF V IMP VEIN V SUBJUNCTIVE VEIN done PCP2 cook cool V PRES -SG3 VEIN V INF V IMP VEIN V SUBJUNCTIVE VEIN N NOM SG V PRES -SG3 VFIN V INF V IMP VFIN V SUBJUNCTIVE VEIN A ABS cooled PCP2 V PAST VEIN cooling PCP1 JXT\[ hvz hv bedz do vbn vb nn vb JJ nn rb vbn vbd vbg nn Figure 1: Some morphological ambiguities for verbs." ></td>
	<td class="line x" title="74:186	increase morphological ambiguity but serve as additional information for rules." ></td>
	<td class="line x" title="75:186	If these auxiliary tags are ignored, the morphological analyser produces about 180 different tag combinations." ></td>
	<td class="line x" title="76:186	The XT lexicon contains 94 tags for words; 15 of them are assigned unambiguously to only one word." ></td>
	<td class="line x" title="77:186	There are 32 verb tags: 8 tags for have, 13 for be, 6 for do and 5 tags for other verbs." ></td>
	<td class="line x" title="78:186	ENGCG does not make a distinction in the tagset between words have, be, do and the other verbs." ></td>
	<td class="line x" title="79:186	To see the difference with ENGCG, see Figure 1." ></td>
	<td class="line x" title="80:186	The ENGCG description differs from the Brown Corpus tag set in the following respects." ></td>
	<td class="line x" title="81:186	ENGCG is more distinctive in that a part of speech distinction is spelled out (see Figure 2) in the description of  determiner-pronoun homographs,  preposition-conjunction homographs,  determiner-adverb-pronoun homographs, and  uninflected verb forms (see Figure 1), which are represented as ambiguous due to the subjunctive, imperative, infinitive and present tense readings." ></td>
	<td class="line x" title="82:186	On the other hand, ENGCG does not spell out partof-speech ambiguity in the description of  -ing and nonfinite -ed forms, ~ Two most probable ENGCG tags (%) cs cs (70 %) PREP (28 %) DT DET DEM SG (48 %) PRON DEM SG (27 %) DTI DET SG/PL (68 %) PRON SG/PL (28 %) IN PI~EP (99 %) ADV (0.5 %) JJ A ABS (93 %) N NOMSG (3 %) NN N NOM SG (88 %) N NOM SG/PL (7 %) NP N NOM SG (80 %) NNOMPL(7%) VB VINF (84 %) V PRES -SG3 VEIN (12 %) * NEG-PART (100 %) Figure 2: Some mappings from the Brown Corpus to the ENGCG tagset." ></td>
	<td class="line x" title="83:186	 noun-adjective homographs when the core meanings of the adjective and noun readings are similar,  ambiguities due to proper nouns, common nouns and abbreviations." ></td>
	<td class="line x" title="84:186	4 Combining the taggers In our approach we apply ENGCG and XT independently." ></td>
	<td class="line x" title="85:186	Combining the taggers means aligning the outputs of the taggers and transforming the result of one tagger to that of the other." ></td>
	<td class="line x" title="86:186	Aligning the output is straightforward: we only need to match the word forms in the output of the taggers." ></td>
	<td class="line x" title="87:186	Some minor problems occur when tokenisation is done differently." ></td>
	<td class="line x" title="88:186	For instance, XT handles words like aren't as a single token, when ENGCG divides it to two tokens, are and not." ></td>
	<td class="line x" title="89:186	Also ENGCG recognises some multiple word phrases like in spite of as one token, while XT handles it as three tokens." ></td>
	<td class="line x" title="90:186	We do not need to map both Brown tags to ENGCG and vice versa." ></td>
	<td class="line x" title="91:186	It is enough to transform ENGCG tags to Brown tags and select the tag that XT has produced, or transform the tag of XT into ENGCG tags." ></td>
	<td class="line x" title="92:186	We do the latter because the ENGCG tags contain more information." ></td>
	<td class="line x" title="93:186	This is likely to be desirable in the design of potential applications." ></td>
	<td class="line x" title="94:186	There are a couple of problems in mapping:  Difference in distinctiveness." ></td>
	<td class="line x" title="95:186	Sometimes ENGTWOL makes a distinction not made by the Brown tagset; sometimes the Brown tagset makes a distinction not made by ENGTWOL (see Figure 2)." ></td>
	<td class="line x" title="96:186	 Sometimes tags are used in a different way." ></td>
	<td class="line x" title="97:186	A 49 case in point is the word as." ></td>
	<td class="line x" title="98:186	In a sample of 76 instances of as from the tagged Brown corpus, 73 are analysed as CS; two as QL and one as IN, while in the ENGCG description the same instances of as were analysed 15 times as CS, four times as ADV, and 57 times as PREP." ></td>
	<td class="line x" title="99:186	In ENGCG, the tag CS represents subordinating conjunctions." ></td>
	<td class="line x" title="100:186	In the following sentences the correct analysis for word as in ENGCG is PREP, not CS, which the Brown corpus suggests." ></td>
	<td class="line x" title="101:186	The city purchasing department, the jury said, is lacking in experienced clerical personnel as(CS) a result of city personnel policies." ></td>
	<td class="line x" title="102:186	-The petition listed the mayor's occupation as(CS) attorney and his age as(CS) 71." ></td>
	<td class="line x" title="103:186	It listed his wife's age as(CS) 74 and place of birth as(CS) Opelika, Ala. The sentences are the three first sentences where word as appears in Brown corpus." ></td>
	<td class="line x" title="104:186	In the Brown Corpus as appears over 7000 times and it is the fourteenth most common word." ></td>
	<td class="line x" title="105:186	Because XT is trained according to the Brown Corpus, this is likely to cause problems." ></td>
	<td class="line x" title="106:186	XT is applied independently to the text, and the tagger's prediction is consulted in the analysis of those words where ENGCG is unable to make a unique prediction." ></td>
	<td class="line x" title="107:186	The system selects the ENGCG morphological reading that most closely corresponds to the tag proposed by XT." ></td>
	<td class="line x" title="108:186	The mapping scheme is the following." ></td>
	<td class="line x" title="109:186	For each Brown Corpus tag, there is a decision list for possible ENGCG tags, the most probable one first." ></td>
	<td class="line x" title="110:186	We have computed the decision list from the part of Brown Corpus that is also manually tagged according to the ENGCG grammatical representation." ></td>
	<td class="line x" title="111:186	The mapping can be used in two different ways." ></td>
	<td class="line x" title="112:186	 Careful mode: An ambiguous reading in the output of ENGCG may be removed only when it is not in the decision list." ></td>
	<td class="line x" title="113:186	In practise this leaves quite much ambiguity." ></td>
	<td class="line x" title="114:186	 Unambiguous mode: Select the reading in the output of ENGCG that comes first in the decision list 2." ></td>
	<td class="line x" title="115:186	5 Performance test 5.1 Test data The system was tested against 26,711 words of newspaper text from The Wall Street Journal, The Economist and Today, all taken from the 200-million word Bank of English corpus by the COBUILD team at the University of Birmingham, England (see also (J/irvinen, 1994))." ></td>
	<td class="line x" title="116:186	None of these texts have been 2In some cases a word may still remain ambiguous." ></td>
	<td class="line x" title="117:186	used in the development of the system or the description, i.e. no training effects are to be expected." ></td>
	<td class="line x" title="118:186	5.2 Creation of benchmark corpus Before the test, a benchmark version of the test corpus was created." ></td>
	<td class="line x" title="119:186	The texts were first analysed using the preprocessor, the morphological analyser, and the module for morphological heuristics." ></td>
	<td class="line x" title="120:186	This ambiguous data was then manually disambiguated by judges, each having a thorough understanding of the ENGCG grammatical representation." ></td>
	<td class="line x" title="121:186	The corpus was independently disambiguated by two judges." ></td>
	<td class="line x" title="122:186	In the instructions to the experts, special emphasis was given to the quality of the work (there was no time pressure)." ></td>
	<td class="line x" title="123:186	The two disambiguated versions of the corpus were compared using the Unix sdiff program." ></td>
	<td class="line x" title="124:186	At this stage, slightly above 99 % of all analyses agreed." ></td>
	<td class="line x" title="125:186	The differences were jointly examined by the judges to see whether they were caused by inattention or by a genuine difference of opinion that could not be resolved by consulting the documentation that outlines the principles adopted for this grammatical representation (for the most part documented in (Karlsson et al. , 1994))." ></td>
	<td class="line x" title="126:186	It turned out that almost all of these differences were due to inattention." ></td>
	<td class="line x" title="127:186	Only in the analysis of a few words it was agreed that a multiple choice was appropriate because of different meaning-level interpretations of the utterance (these were actually headings where some of the grammatical information was omitted)." ></td>
	<td class="line x" title="128:186	Overall, these results agree with our previous experiences (Karlsson et al. , 1994): if the analysis is done by experts in the adopted grammatical representation, with emphasis on the quality of the work, a consensus of virtually 100 % is possible, at least at the level of morphological analysis (for a less optimistic view, see (Church, 1992))." ></td>
	<td class="line x" title="129:186	5.3 Morphological analysis The preprocessed text was submitted to the ENGTWOL morphological analyser, which assigns to 25,831 words of the total 26,711 (96.7 %) at least one morphological analysis." ></td>
	<td class="line x" title="130:186	The remaining 880 word-form tokens were analysed with the rule-based heuristic module." ></td>
	<td class="line x" title="131:186	After the combined effect of these modules, there were 47,269 morphological analyses, i.e. 1.77 morphological analyses for each word on an average." ></td>
	<td class="line x" title="132:186	At this stage, 23 words missed a contextually appropriate analysis, i.e. the error rate of the system after morphological analysis was about 0.1%." ></td>
	<td class="line x" title="133:186	5.4 Morphological disambiguation The morphologically analysed text was submitted to five disambiguators (see Figure 3)." ></td>
	<td class="line x" title="134:186	The first one, D1, is the grammar-based ENGCG disambiguator." ></td>
	<td class="line x" title="135:186	In the next step (D2) we have used also heuristic ENGCG constraints." ></td>
	<td class="line x" title="136:186	The probabilistic information 50 is used in D3, where the ambiguities of D2 are resolved by XT." ></td>
	<td class="line x" title="137:186	We also tested the usefulness of the heuristic component of ENGCG by omitting it in D4." ></td>
	<td class="line x" title="138:186	The last test, D5, is XT alone, i.e. only probabilistic techniques are used here for resolving ENGTWOL ambiguities." ></td>
	<td class="line x" title="139:186	The ENGCG disambiguator performed somewhat less well than usually." ></td>
	<td class="line x" title="140:186	With heuristic constraints, the error rate was as high as 0.63 %, with 1.04 morphological readings per word on an average." ></td>
	<td class="line x" title="141:186	However, most (57 %) of the total errors were made after ENGCG analysis (i.e. in the analysis of no more than 3.6 % of all words)." ></td>
	<td class="line x" title="142:186	In a way, this is not very surprising because ENGCG is supposed to tackle all the 'easy' cases and leave the structurally hardest cases pending." ></td>
	<td class="line x" title="143:186	But it is quite revealing that as much as three fourths of the probabilistic tagger's errors occur in the analysis of the structurally 'easy' cases; obviously, many of the probabilistic system's decisions are structurally somewhat naive." ></td>
	<td class="line x" title="144:186	Overall, the hybrid (D3#) reached an accuracy of about 98.5 % significantly better than the 95-97 % accuracy which state-of-the-art probabilistic taggers reach alone." ></td>
	<td class="line x" title="145:186	The hybrid D3~ is like hybrid D3~, but we have used careful mapping." ></td>
	<td class="line x" title="146:186	There some problematic ambiguity (see Figure 2) is left pending." ></td>
	<td class="line x" title="147:186	For instance, ambiguities between preposition and infinitive marker (word to), or between subordinator and preposition (word as), are resolved as far as ENGCG disambiguates them, the prediction of XT is not consulted." ></td>
	<td class="line x" title="148:186	Also, when XT proposes tags like JJ (adjective), AP (post-determiner) or VB (verb base-form) very little further disambiguation is done." ></td>
	<td class="line x" title="149:186	This hybrid does not contain any mapping errors, and on the other hand, not all the XT errors either." ></td>
	<td class="line x" title="150:186	The test without the heuristic component of ENGCG (D4) suggests that ambiguity should be resolved as far as possible with rules." ></td>
	<td class="line x" title="151:186	An open question is, how far we can go using only linguistic information (e.g. by writing more heuristic constraints to be applied after the more reliable ones, in this way avoiding many linguistically naive errors)." ></td>
	<td class="line x" title="152:186	The last test gives further evidence for the usefulness of a carefully designed linguistic rule component." ></td>
	<td class="line x" title="153:186	Without such a rule component, the decrease in accuracy is quite dramatic although a part of the errors come from the mapping between tag sets 3." ></td>
	<td class="line x" title="154:186	6 Conclusion In this paper we have demonstrated how knowledgebased and statistical techniques can be combined to improve the accuracy of a part of speech tagger." ></td>
	<td class="line x" title="155:186	Our system reaches a better than 98 % accuracy using a relatively fine-grained grammatical representation." ></td>
	<td class="line x" title="156:186	Some concluding remarks are in order." ></td>
	<td class="line x" title="157:186	3Even without the mapping errors, the reported 4 % error rate of XT is considerably higher than that of our hybrid." ></td>
	<td class="line x" title="158:186	 Using linguistic information before a statistical module provides a better result than using a statistical module alone." ></td>
	<td class="line x" title="159:186	 ENGCG leaves some 'hard' ambiguities unresolved (about 3-7 % of all words)." ></td>
	<td class="line x" title="160:186	This amount is characteristic of the ENGCG rule-formMism, tagset and disambiguation grammar." ></td>
	<td class="line x" title="161:186	It does not necessarily hold for other knowledge-based systems." ></td>
	<td class="line x" title="162:186	 Only about 20-25 % of errors made by the statistical component occur in the analysis of these 'hard' ambiguities." ></td>
	<td class="line x" title="163:186	That means, 75-80 % of the errors made by the statistical tagger were resolved correctly using linguistic rules." ></td>
	<td class="line x" title="164:186	 Certain kinds of ambiguity left pending by ENGCG, e.g. CS vs. PREP, are resolved rather unreliably by XT." ></td>
	<td class="line x" title="165:186	 The overall result is better than other state-ofthe-art part-of-speech disambiguators." ></td>
	<td class="line x" title="166:186	In our 27000 word test sample from previously unseen corpus, 98.5 % of words received a correct analysis." ></td>
	<td class="line x" title="167:186	In other words, the error rate is reduced at least by half." ></td>
	<td class="line x" title="168:186	Although the result is better than provided by any other tagger that produces fully disambiguated output, we believe that the result could still be improved." ></td>
	<td class="line x" title="169:186	Some possibilities:  We could use partly disambiguated text (e.g. the output of parsers D1, D2 or D3~) and disambiguate the result using a knowledgebased syntactic parser (see experiments in (Voutilainen and Tapanainen, 1993))." ></td>
	<td class="line x" title="170:186	 We could leave the text partly disambiguated, and use a syntactic parser that uses both linguistic knowledge and corpus-based heuristics (see (Tapanainen and J//rvinen, 1994))." ></td>
	<td class="line x" title="171:186	 Some ambiguities are very difficult to resolve in a small window that statistical taggers currently use (e.g. CS vs. PREP ambiguity when a noun phrase follows)." ></td>
	<td class="line x" title="172:186	A better way to resolve them would probably be to write (heuristic) rules." ></td>
	<td class="line x" title="173:186	 We could train the statistical tagger on the output of a knowledge-based tagger." ></td>
	<td class="line x" title="174:186	That is problematic because generally statistical methods seem to require some compact set of tags, while a knowledge-based system needs more informative tags." ></td>
	<td class="line x" title="175:186	The tag set of a knowledge-based system should be reduced down to some subset." ></td>
	<td class="line x" title="176:186	That might prevent some mapping errors but there is no quarantee that the statistical tagger would work any better." ></td>
	<td class="line x" title="177:186	 We could try the components in a different order: using statistics before heuristical knowledge etc. However, currently the heuristic component makes less errors than the statistical tagger." ></td>
	<td class="line x" title="178:186	51 DO (Morphological analysis) D1 (DO + ENGCG) D2 (D1 + ENGCG heuristics) D3~ (D2 + XT + C-mapping) D3Z (D2 + XT + mapping) D4 (D1 + XT + mapping) D5 (DO + XT + mapping) \[ Amb." ></td>
	<td class="line x" title="179:186	words 37.6 % 6.4 % 3.6 % 2.2 % 0.0 % 0.0 % 0.7 % Figure 3: Performance of the Readings Readings / 47269 1.77 28815 1.08 27681 1.04 27358 1.02 26744 1.00 26794 1.00 26977 1.01 word\]Errors \[Error rate (%) 23 0.09 % 94 0.35 % 169 0.63 % 220 0.82 % 391 1.46 % 6.38 % taggers on a 26,711-word corpus." ></td>
	<td class="line x" title="180:186	 We could use a better statistical tagger." ></td>
	<td class="line x" title="181:186	But the accuracy of XT is almost the same as the accuracy of any other statistical tagger." ></td>
	<td class="line x" title="182:186	What is more, the accuracy of the purely statistical taggers has not been greatly increased since the first of its kind, CLAWS1, (Marshall, 1983) was published over ten years ago." ></td>
	<td class="line x" title="183:186	We believe that the best way to boost the accuracy of a tagger is to employ even more linguistic knowledge." ></td>
	<td class="line x" title="184:186	The knowledge should, in addition, contain more syntactic information so that we could refer to real (syntactic) objects of the language, not just a sequence of words or parts of speech." ></td>
	<td class="line x" title="185:186	Statistical information should be used only when one does not know how to resolve the remaing ambiguity, and there is a definite need to get fully unambiguous output." ></td>
	<td class="line x" title="186:186	7 Acknowledgements We would like to thank Timo Jrvinen, Lauri Karttunen, Jussi Piitulainen and anonymous referees for useful comments on earlier versions of this paper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="A94-1009
Does Baum-Welch Re-Estimation Help Taggers?
Elworthy, David;"></td>
	<td class="line x" title="1:191	Does Baum-Welch Re-estimation :Help Taggers?" ></td>
	<td class="line x" title="2:191	David Elworthy Sharp Laboratories of Europe Ltd. Edmund Halley Road Oxford Science Park Oxford OX4 4GA United Kingdom dahe@sharp, co. uk Abstract In part of speech tagging by Hidden Markov Model, a statistical model is used to assign grammatical categories to words in a text." ></td>
	<td class="line x" title="3:191	Early work in the field relied on a corpus which had been tagged by a human annotator to train the model." ></td>
	<td class="line oc" title="4:191	More recently, Cutting et al.(1992) suggest that training can be achieved with a minimal lexicon and a limited amount of a priori information about probabilities, by using an Baum-Welch re-estimation to automatically refine the model." ></td>
	<td class="line x" title="6:191	In this paper, I report two experiments designed to determine how much manual training information is needed." ></td>
	<td class="line x" title="7:191	The first experiment suggests that initial biasing of either lexical or transition probabilities is essential to achieve a good accuracy." ></td>
	<td class="line x" title="8:191	The second experiment reveals that there are three distinct patterns of Baum-Welch reestimation." ></td>
	<td class="line x" title="9:191	In two of the patterns, the re-estimation ultimately reduces the accuracy of the tagging rather than improving it." ></td>
	<td class="line x" title="10:191	The pattern which is applicable can be predicted from the quality of the initial model and the similarity between the tagged training corpus (if any) and the corpus to be tagged." ></td>
	<td class="line x" title="11:191	Heuristics for deciding how to use re-estimation in an effective manner are given." ></td>
	<td class="line x" title="12:191	The conclusions are broadly in agreement with those of Merialdo (1994), but give greater detail about the contributions of different parts of the model." ></td>
	<td class="line x" title="13:191	1 Background Part-of-speech tagging is the process of assigning grammatical categories to individual words in a corpus." ></td>
	<td class="line x" title="14:191	One widely used approach makes use of a statistical technique called a Hidden Markov Model (HMM)." ></td>
	<td class="line x" title="15:191	The model is defined by two collections of parameters: the transition probabilities, which express the probability that a tag follows the preceding one (or two for a second order model); and the lexical probabilities, giving the probability that a word has a given tag without regard to words on either side of it." ></td>
	<td class="line x" title="16:191	To tag a text, the tags with non-zero probability are hypothesised for each word, and the most probable sequence of tags given tbe sequence of words is determined from the probabilities." ></td>
	<td class="line x" title="17:191	Two algorithms are commonly used, known as the Forward-Backward (FB) and Viterbi algorithms." ></td>
	<td class="line x" title="18:191	FB assigns a probability to every tag on every word." ></td>
	<td class="line x" title="19:191	while Viterbi prunes tags which cannot be chosen because their probability is lower than the ones of competing hypotheses, with a corresponding gain in computational efficiency." ></td>
	<td class="line oc" title="20:191	For an introduction to the algorithms, see Cutting et al.(1992), or the lucid description by Sharman (1990)." ></td>
	<td class="line x" title="22:191	There are two principal sources for the parameters of the model." ></td>
	<td class="line x" title="23:191	If a tagged corpus prepared by a human annotator is available, the transition and lexical probabilities can be estimated from the frequencies of pairs of tags and of tags associated with words." ></td>
	<td class="line x" title="24:191	Alternatively~ a procedure called BaumWelch (BW) re-estimation may be used, in which an untagged corpus is passed through the FB algorithm with some initial ruodel, and the resulting probabilities used to determine new values for the lexical and transition probabilities." ></td>
	<td class="line x" title="25:191	By iterating the algorithm with the same corpus, the parameters of the model can be made to converge on values which are locally optimal for the given text." ></td>
	<td class="line x" title="26:191	The degree of convergence can be measured using a perplexity measure, the sum of plog2p for hypothesis probabilities p, which gives an estimate of the degree of disorder in the model." ></td>
	<td class="line o" title="27:191	The algorithm is again described by Cutting et al. and by Sharman, and a mathematical justification for it can be tbund in Huang et al.(1990)." ></td>
	<td class="line x" title="29:191	The first major use of HMMs for part of speech tagging was in CLAWS (Garside et al. , 1987) in the 1970s." ></td>
	<td class="line x" title="30:191	With the availability of large corpora and fast computers, there has been a recent resurgence of interest, and a number of variations on and alter53 natives to the FB, Viterbi and BW algorithms have been tried; see the work of, for example, Church (Church, 1988), Brill (Brill and Marcus, 1992; Brill, 1992), DeRose (DeRose, 1988) and gupiec (Kupiec, 1992)." ></td>
	<td class="line pc" title="31:191	One of the most effective taggers based on a pure HMM is that developed at Xerox (Cutting et al. , 1992)." ></td>
	<td class="line p" title="32:191	An important aspect of this tagger is that it will give good accuracy with a minimal amount of manually tagged training data." ></td>
	<td class="line x" title="33:191	96% accuracy correct assignment of tags to word token, compared with a human annotator, is quoted, over a 500000 word corpus." ></td>
	<td class="line p" title="34:191	The Xerox tagger attempts to avoid the need for a hand-tagged training corpus as far as possible." ></td>
	<td class="line x" title="35:191	Instead, an approximate model is constructed by hand, which is then improved by BW re-estimation on an untagged training corpus." ></td>
	<td class="line x" title="36:191	In the above example, 8 iterations were sufficient." ></td>
	<td class="line x" title="37:191	The initial model set up so that some transitions and some tags in the lexicon are favoured, and hence having a higher initial probability." ></td>
	<td class="line x" title="38:191	Convergence of the model is improved by keeping the number of parameters in the model down." ></td>
	<td class="line x" title="39:191	To assist in this, low frequency items in the lexicon are grouped together into equivalence classes, such that all words in a given equivalence class have the same tags and lexical probabilities, and whenever one of the words is looked up, then the data common to all of them is used." ></td>
	<td class="line x" title="40:191	Re-estimation on any of the words in a class therefore counts towards re-estimation for all of them 1." ></td>
	<td class="line p" title="41:191	The results of the Xerox experiment appear very encouraging." ></td>
	<td class="line x" title="42:191	Preparing tagged corpora either by hand is labour-intensive and potentially error-prone, and although a semi-automatic approach can be used (Marcus et al. , 1993), it is a good thing to reduce the human involvement as much as possible." ></td>
	<td class="line x" title="43:191	However, some careful examination of the experiment is needed." ></td>
	<td class="line n" title="44:191	In the first place, Cutting et al. do not compare the success rate in their work with that achieved from a hand-tagged training text with no re-estimation." ></td>
	<td class="line n" title="45:191	Secondly, it is unclear how much the initial biasing contributes the success rate." ></td>
	<td class="line x" title="46:191	If significant human intervention is needed to provide the biasing, then the advantages of automatic training become rather weaker, especially if such intervention is needed on each new text domain." ></td>
	<td class="line p" title="47:191	The kind of biasing Cutting et al. describe reflects linguistic insights combined with an understanding of the predictions a tagger could reasonably be expected to make and the ones it could not." ></td>
	<td class="line x" title="48:191	The aim of this paper is to examine the role that training plays in the tagging process, by an experimental evaluation of how the accuracy of the tagger varies with the initial conditions." ></td>
	<td class="line x" title="49:191	The results suggest that a completely unconstrained initial model does not produce good quality results, and that one 1The technique was originally developed by Kupiec (Kupiec, 1989)." ></td>
	<td class="line x" title="50:191	54 accurately trained from a hand-tagged corpus will generally do better than using an approach based on re-estimation, even when the training comes from a different source." ></td>
	<td class="line x" title="51:191	A second experiment shows that there are different patterns of re-estimation, and that these patterns vary more or less regularly with a broad characterisation of the initial conditions." ></td>
	<td class="line x" title="52:191	The outcome of the two experiments together points to heuristics for making effective use of training and reestimation, together with some directions for further research." ></td>
	<td class="line x" title="53:191	Work similar to that described here has been carried out by Merialdo (1994), with broadly similar conclusions." ></td>
	<td class="line x" title="54:191	We will discuss this work below." ></td>
	<td class="line x" title="55:191	The principal contribution of this work is to separate the effect of the lexical and transition parameters of the model, and to show how the results vary with different degree of similarity between the training and test data." ></td>
	<td class="line x" title="56:191	2 The tagger and corpora The experiments were conducted using two taggers, one written in C at Cambridge University Computer Laboratory, and the other in C-t-+ at Sharp Laboratories." ></td>
	<td class="line x" title="57:191	Both taggers implement the FB, Viterbi and BW algorithms." ></td>
	<td class="line x" title="58:191	For training from a hand-tagged corpus, the model is estimated by counting the number of transitions from each tag i to each tag j, the total occurrence of each tag i, and the total occurrence of word w with tag i. Writing these as f(i,j), f(i) and f(i,w) respectively, the transition probability from tag i to tag j is estimated as f(i,j)/f(i) and the lexical probability as f(i, w)/f(i)." ></td>
	<td class="line x" title="59:191	Other estimation formulae have been used in the past." ></td>
	<td class="line x" title="60:191	For example, CLAWS (Garside ct al. , 1987) normalises the lexical probabilities by the total frequency of the word rather than of the tag." ></td>
	<td class="line x" title="61:191	Consulting the BaumWelch re-estimation formulae suggests that the approach described is more appropriate, and this is confirmed by slightly greater tagging accuracy." ></td>
	<td class="line x" title="62:191	Any transitions not seen in the training corpus are given a small, non-zero probability The lexicon lists, for each word, all of tags seen in the training corpus with their probabilities." ></td>
	<td class="line x" title="63:191	For words not found in the lexicon, all open-class tags are hypothesised, with equal probabilities." ></td>
	<td class="line x" title="64:191	These words are added to the lexicon at the end of first iteration when re-estimation is being used, so that the probabilities of their hypotheses subsequently diverge from being uniform." ></td>
	<td class="line x" title="65:191	To measure the accuracy of the tagger, we compare the chosen tag with one provided by a human annotator." ></td>
	<td class="line x" title="66:191	Various methods of quoting accuracy have been used in the literature, the most common being the proport ion of words (tokens) receiving the correct tag." ></td>
	<td class="line x" title="67:191	A better measure is the proportion of ambiguous words which are given the correct tag, where by ambiguous we mean that more than one tag was hypothesised." ></td>
	<td class="line x" title="68:191	The former figure looks more impressive, but the latter gives a better measure of how well the tagger is doing, since it factors out the trivial assignment of tags to non-ambiguous words." ></td>
	<td class="line x" title="69:191	For a corpus in which a fraction a of the words are ambiguous, and p is the accuracy on ambiguous words, the overall accuracy can be recovered from 1 a + pa. All of the accuracy figures quoted below are for ambiguous words only." ></td>
	<td class="line x" title="70:191	The training and test corpora were drawn from the LOB corpus and the Penn treebank." ></td>
	<td class="line x" title="71:191	The hand tagging of these corpora is quite different." ></td>
	<td class="line x" title="72:191	For example, the LOB tagset used 134 tags, while the Penn treebank tagset has 48." ></td>
	<td class="line x" title="73:191	The general pattern of the results presented does not vary greatly with the corpus and tagset used." ></td>
	<td class="line x" title="74:191	3 The effect of the initial conditions The first experiment concerned the effect of the initial conditions on the accuracy using Baum-Welch re-estimation." ></td>
	<td class="line x" title="75:191	A model was trained from a handtagged corpus in the manner described above, and then degraded in various ways to simulate the effect of poorer training, as follows: Lexicon DO Un-degraded lexical probabilities, calculated from f(i, w)/f(i)." ></td>
	<td class="line x" title="76:191	D1 Lexical probabilities are correctly ordered, so that the most frequent tag has the highest lexical probability and so on, but the absolute values are otherwise unreliable." ></td>
	<td class="line x" title="77:191	D2 Lexical probabilities are proportional to the overall tag frequencies, and are hence independent of the actual occurrence of the word in the training corpus." ></td>
	<td class="line x" title="78:191	D3 All lexical probabilities have the same value, so that the lexicon contains no information other than the possible tags for each word." ></td>
	<td class="line x" title="79:191	Transitions TO Un-degraded transition probabilities, calculated from f(i, j)/f(i)." ></td>
	<td class="line x" title="80:191	T1 All transition probabilities have the same value." ></td>
	<td class="line x" title="81:191	We could expect to achieve D1 from, say, a printed dictionary listing parts of speech in order of frequency." ></td>
	<td class="line x" title="82:191	Perfect training is represented by case D0+T0." ></td>
	<td class="line oc" title="83:191	The Xerox experiments (Cutting et al. , 1992) correspond to something between D1 and D2, and between TO and T1, in that there is some initial biasing of the probabilities." ></td>
	<td class="line x" title="84:191	For the test, four corpora were constructed from the LOB corpus: LOB-B from part B, LOB-L from part L, LOB-B-G from parts B to G inclusive and LOB-B-J from parts B to J inclusive." ></td>
	<td class="line x" title="85:191	Corpus LOBB-J was used to train the model, and LOB-B." ></td>
	<td class="line x" title="86:191	LOB55 L and LOB-B-G were passed through thirty iterations of the BW algorithm as untagged data." ></td>
	<td class="line x" title="87:191	In each case, the best accuracy (on ambiguous words, as usual) from the FB algorithm was noted." ></td>
	<td class="line x" title="88:191	As an additional test, we tried assigning the most probable tag from the DO lexicon, completely ignoring tag-tag transitions." ></td>
	<td class="line x" title="89:191	The results are summarised in table 1, for various corpora, where F denotes the 'most frequent tag' test." ></td>
	<td class="line x" title="90:191	As an example of how these figures relate to overall accuracies, LOB-B contains 32.35% ambiguous tokens with respect to the lexicon from LOB-B-J, and the overall accuracy in the D0+T0 case is hence 98.69%." ></td>
	<td class="line x" title="91:191	The general pattern of the results is similar across the three test corpora, with the only difference of interest being that case D3+T0 does better for LOB-L than tbr the other two cases, and in particular does better than cases D0+T1 and DI+T1." ></td>
	<td class="line x" title="92:191	A possible explanation is that in this case the test data does not overlap with the training data, and hence the good quality lexicons (DO and D1) have less of an influence." ></td>
	<td class="line x" title="93:191	It is also interesting that D3+T1 does better than D2+T1." ></td>
	<td class="line x" title="94:191	The reasons for this are unclear, and the results are not always the same with other corpora, which suggests that they are not statistically significant." ></td>
	<td class="line x" title="95:191	Several follow-up experiments were used to confirm the results: using corpora from the Penn treebank, using equivalence classes to ensure that all lexical entries have a total relative frequency of at least 0.01, and using larger corpora." ></td>
	<td class="line x" title="96:191	The specific accuracies were different in the various tests, but the overall patterns remained much the same, suggesting that they are not an artifact of the tagset or of details of the text." ></td>
	<td class="line x" title="97:191	The observations we can make about these results are as follows." ></td>
	<td class="line x" title="98:191	Firstly, two of the tests, D2+T1 and D3+T1, give very poor performance." ></td>
	<td class="line x" title="99:191	Their accuracy is not even as good as that achieved by picking the most frequent tag (although this of course implies a lexicon of DO or D1 quality)." ></td>
	<td class="line x" title="100:191	It follows that ifBaumWelch re-estimation is to be an effective technique, the initial data must have either biasing in the transitions (the TO cases) or in the lexical probabilities (cases D0+T1 and DI+T1), but it is not necessary to have both (D2/D3+T0 and D0/DI+T1)." ></td>
	<td class="line x" title="101:191	Secondly, training from a hand-tagged corpus (case D0+T0) always does best, even when the test data is from a different source to the training data, as it is for LOB-L." ></td>
	<td class="line x" title="102:191	So perhaps it is worth investing effort in hand-tagging training corpora after all, rather than just building a lexicon and letting reestimation sort out the probabilities." ></td>
	<td class="line x" title="103:191	But how can we ensure that re-estimation will produce a good quality model?" ></td>
	<td class="line x" title="104:191	We look further at this issue in the next section." ></td>
	<td class="line x" title="105:191	Table 1: Accuracy using Baum-Welch re-estimation with various initial conditions Dict Trans LOB-B (%) DO TO 95.96 D1 TO 95.40 D2 TO 90.52 D3 TO 92.96 DO T1 94.06 D1 T1 94.06 D2 T1 66.51 D3 T1 75.49 F 89.22 LOB-L (%) 94.77 94.44 91.82 92.80 92.27 92.27 72.48 80.87 85.32 LOB-B-G (%) 96.17 95.40 92.36 93.48 94.51 94.51 55.88 79.12 88.71 4 Patterns of re-estimation During the first experiment, it became apparent that Baum-Welch re-estimation sometimes decreases the accuracy as the iteration progresses." ></td>
	<td class="line x" title="106:191	A second experiment was conducted to decide when it is appropriate to use Baum-Welch re-estimation at all." ></td>
	<td class="line x" title="107:191	There seem to be three patterns of behaviour: Classical A general trend of rising accuracy on each iteration, with any falls in accuracy being local. It indicates that the model is converging towards an optimum which is better than its starting point." ></td>
	<td class="line x" title="108:191	Initial maximum Highest accuracy on the first iteration, and falling thereafter." ></td>
	<td class="line x" title="109:191	In this case the initial model is of better quality than BW can achieve." ></td>
	<td class="line x" title="110:191	That is, while BW will converge on an optimum, the notion of optimality is with respect to the HMM rather than to the linguistic judgements about correct tagging." ></td>
	<td class="line x" title="111:191	Early maximum Rising accuracy for a small number of iterations (2-4), and then falling as in initial maximum." ></td>
	<td class="line x" title="112:191	An example of each of the three behaviours is shown in figure 1." ></td>
	<td class="line x" title="113:191	The values of the accuracies and the test conditions are unimportant here; all we want to show is the general patterns." ></td>
	<td class="line x" title="114:191	The second experiment had the aim of trying to discover which pattern applies under which circumstances, in order to help decide how to train the model." ></td>
	<td class="line x" title="115:191	Clearly, if the expected pattern is initial maximum, we should not use BW at all, if early maximum, we should halt the process after a few iterations, and if classical, we should halt the process in a 'standard' way, such as comparing the perplexity of successive models." ></td>
	<td class="line x" title="116:191	The tests were conducted in a similar manner to those of the first experiment, by building a lexicon and transitions from a hand tagged training corpus, and then applying them to a test corpus with varying degrees of degradation." ></td>
	<td class="line x" title="117:191	Firstly, four different degrees of degradation were used: no degradation at all, D2 degradation of the lexicon, T1 degradation of the transitions, and the two together." ></td>
	<td class="line x" title="118:191	Secondly, we selected test corpora with varying degrees of similarity to the training corpus: the same text, text from a similar domain, and text which is significantly different." ></td>
	<td class="line x" title="119:191	Two tests were conducted with each combination of the degradation and similarity, using different corpora (from the Penn treebank) ranging in size from approximately 50000 words to 500000 words." ></td>
	<td class="line x" title="120:191	The re-estimation wa.s allowed to run for ten iterations." ></td>
	<td class="line x" title="121:191	The results appear ill table 2, showing the best accuracy achieved (on ambiguous words)." ></td>
	<td class="line x" title="122:191	the iteration at which it occurred, and the pattern of re-estimation (I = initial maximum, E = early maximum, C = classical)." ></td>
	<td class="line x" title="123:191	The patterns are summarised in table 3, each entry in the table showing the patterns for the two tests under the given conditions." ></td>
	<td class="line x" title="124:191	Although there is some variations in the readings, for example ill the 'similar/D0+T0' case, we can draw some general conclusions about the patterns obtained from different sorts of data." ></td>
	<td class="line x" title="125:191	When the lexicon is degraded (D2), the pattern is always classical." ></td>
	<td class="line x" title="126:191	With a good lexicon but either degraded transitions or a test corpus differing from the training corpus, the pattern tends to be early maximum." ></td>
	<td class="line x" title="127:191	When the test corpus is very similar to the model, then the pattern is initial maximum." ></td>
	<td class="line x" title="128:191	Furthermore, examining the accuracies in table 2, in the cases of initial maximum and early maximum, the accuracy tends to be significantly higher than with classical behaviour." ></td>
	<td class="line x" title="129:191	It seems likely that what is going on is that the model is converging to towards something of similar 'quality' in each case, but when the pattern is classical, the convergence starts from a lower quality model and improves, and in the other cases, it starts from a higher quality one and deteriorates." ></td>
	<td class="line x" title="130:191	In the case of early maximum, the few iterations where the accuracy is improving correspond to the creation of entries for unknown words and th~ ~, fine tuning of ones for known ones, and these changes outweigh those produced by the re-estimation." ></td>
	<td class="line x" title="131:191	5 Discussion From the obserw~tions in the previous section, we propose the following guidelines for how to train a 56 94 92 90 88 Accuracy 86 (%) 84 82 80 78 0 I I I \ _~ -I I 5 10 ." ></td>
	<td class="line x" title="132:191	) Initial Early  Classical . . .,  . . ." ></td>
	<td class="line x" title="133:191	I I I 15 20 25 Iteration Figure 1: Example Baum-Welch behaviour 30 HMM for use in tagging:  If a hand-tagged training corpus is available, use it . If the test and training corpora are nearidentical, do not use BW re-estimation; otherwise use for a small number of iterations." ></td>
	<td class="line x" title="134:191	 If no such training corpus is available, but a lexicon with at least relative frequency data is available, use BW re-estimation for a small number of iterations." ></td>
	<td class="line x" title="135:191	 If neither training corpus nor lexicon are available, use BW re-estimation with standard convergence tests such as perplexity." ></td>
	<td class="line x" title="136:191	Without a lexicon, some initial biasing of the transitions is needed if good results are to be obtained." ></td>
	<td class="line x" title="137:191	Similar results are presented by Merialdo (1994), who describes experiments to compare the effect of training from a hand-tagged corpora and using the Baum-Welch algorithm with various initial conditions." ></td>
	<td class="line x" title="138:191	As in the experiments above, BW reestimation gave a decrease in accuracy when the starting point was derived from a significant amount of hand-tagged text." ></td>
	<td class="line x" title="139:191	In addition, although Merialdo does not highlight the point, BW re-estimation starting from less than 5000 words of hand-tagged text shows early maximum behaviour." ></td>
	<td class="line x" title="140:191	Merialdo's conclusion is that taggers should be trained using as much hand-tagged text as possible to begin with, and only then applying BW re-estimation with untagged text." ></td>
	<td class="line x" title="141:191	The step forward taken in the work here is to show that there are three patterns of reestimation behaviour, with differing guidelines for how to use BW effectively, and that to obtain a good starting point when a hand-tagged corpus is 57 not available or is too small, either the lexicon or the transitions must be biased." ></td>
	<td class="line x" title="142:191	While these may be useful heuristics from a practical point of view, the next step forward is to look for an automatic way of predicting the accuracy of the tagging process given a corpus and a model." ></td>
	<td class="line x" title="143:191	Some preliminary experiments with using measures such as perplexity and the average probability of hypotheses show that, while they do give an indication of convergence during re-estimation, neither shows a strong correlation with the accuracy." ></td>
	<td class="line x" title="144:191	Perhaps what is needed is a 'similarity measure' between two models M and M ~, such that if a corpus were tagged with model M, M ~ is the model obtained by training from the output corpus from the tagger as if it were a hand-tagged corpus." ></td>
	<td class="line x" title="145:191	However, preliminary experiments using such measures as the Kullback-Liebler distance between the initial and new models have again showed that it does not give good predictions of accuracy." ></td>
	<td class="line x" title="146:191	In the end it may turn out there is simply no way of making the prediction without a source of intbrmation extrinsic to both model and corpus." ></td>
	<td class="line x" title="147:191	Acknowledgements The work described here was carried out at the Cambridge University Computer Laboratory as part of Esprit BR Project 7315 'The Acquisition of Lexical Knowledge' (Acquilex-II)." ></td>
	<td class="line x" title="148:191	The results were confirmed and extended at Sharp Laboratories of Europe." ></td>
	<td class="line x" title="149:191	I thank Ted Briscoe for his guidance and advice, and the ANLP referees for their comments." ></td>
	<td class="line x" title="150:191	Corpus relation Same Similar Different Same Similar Different Same Similar Different Same Similar Different * These tests gave an Table 2: Baum-Welch patterns (data) Degradation D0+T0 D0+T0 D0+T0 D0+T1 D0+T1 D0+T1 D2+T0 D2+T0 D2+T0 D2+T1 D2+T1 D2+T1 Test 1 Best (%) at 93.11 1 89.95 1 84.59 2 91.71 2 87.93 2 8O.87 3 84.87 10 81.07 9 78.54 5 72.58 9 68.35 10 65.64 10 pattern I I E E E E C C C* C C C Test 2 Best (%) at 92.83 1 75.03 2 86.00 2 90.52 2 70.63 3 82.68 3 87.31 8 71.40 4 80.81 9 80.53 10 62.76 10 68.95 10 pattern I E E E E E C C* C C C C early peak, but the graphs of accuracy against number of iterations show the pattern to be classical rather than early maximum." ></td>
	<td class="line x" title="151:191	Table 3: Baum-Welch patterns (summary) Degradation D0+T0 D0+T1 D2+T0 D2+T1 Corpus relation Same I, I E, E Similar I, E E, E Different E, E E, E C, C C, C C, C C, C C, C C, C References Eric Brill and Mitch Marcus (1992)." ></td>
	<td class="line x" title="152:191	Tagging an Unfamiliar Text With Minimal Human Supervision." ></td>
	<td class="line x" title="153:191	In AAAI Fall Symposium on Probabilistic Approaches to Natural Language, pages 10-16." ></td>
	<td class="line x" title="154:191	Eric Brill (1992)." ></td>
	<td class="line x" title="155:191	A Simple Rule-Based Part of Speech Tagger." ></td>
	<td class="line x" title="156:191	In Third Conference on Applied Natural Language Processing." ></td>
	<td class="line x" title="157:191	Proceedings of the Conference." ></td>
	<td class="line x" title="158:191	Trento, Italy, pages 152-155, Association for Computational Linguistics." ></td>
	<td class="line x" title="159:191	Kenneth Ward Church (1988)." ></td>
	<td class="line x" title="160:191	A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text." ></td>
	<td class="line x" title="161:191	In Second Conference on Applied Natural Language Processing." ></td>
	<td class="line x" title="162:191	Proceedings of the Conference, pages 136-143, Association for Computational Linguistics." ></td>
	<td class="line xc" title="163:191	Doug Cutting, Julian Kupiec, Jan Pedersen, and Penelope Sibun (1992)." ></td>
	<td class="line x" title="164:191	A Practical Part-of-Speech Tagger." ></td>
	<td class="line x" title="165:191	In Third Conference on Applied Natural Language Processing." ></td>
	<td class="line x" title="166:191	Proceedings of the Conference." ></td>
	<td class="line x" title="167:191	Trento, Italy, pages 133-140, Association for Computational Linguistics." ></td>
	<td class="line x" title="168:191	Steven J. DeRose (1988)." ></td>
	<td class="line x" title="169:191	Grammatical Category Disambiguation by Statistical Optimization." ></td>
	<td class="line x" title="170:191	Computational Linguistics, 14(1) :31-39." ></td>
	<td class="line x" title="171:191	Roger Garside, Geoffrey Leech, and Geoffrey Sampson (1987)." ></td>
	<td class="line x" title="172:191	The Computational Analysis of English: A Corpus-based Approach." ></td>
	<td class="line x" title="173:191	Longman, London." ></td>
	<td class="line x" title="174:191	X. D. Huang, Y. Ariki, and M. A. Jack (1990)." ></td>
	<td class="line x" title="175:191	Hidden Markov Models for Speech Recognition." ></td>
	<td class="line x" title="176:191	Edinburgh University Press." ></td>
	<td class="line x" title="177:191	J. M. Kupiec {1989)." ></td>
	<td class="line x" title="178:191	Probabilistic Models of Short and Long Distance Word Dependencies in Running Text." ></td>
	<td class="line x" title="179:191	In Proceedings of the 1989 DARPA Speech and Natural Language Workshop, pages 290-295." ></td>
	<td class="line x" title="180:191	Julian Kupiec (1992)." ></td>
	<td class="line x" title="181:191	Robust Part-of-speech Tagging Using a Hidden Markov Model." ></td>
	<td class="line x" title="182:191	Computer Speech and Language, 6." ></td>
	<td class="line x" title="183:191	Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz (1993)." ></td>
	<td class="line x" title="184:191	Building a Large Annotated Corpus of English: The Penn Treebank." ></td>
	<td class="line x" title="185:191	Computational Linguistics, 19(2):313-330." ></td>
	<td class="line x" title="186:191	Bernard Merialdo (1994)." ></td>
	<td class="line x" title="187:191	Tagging English Text with a Probabilistic Model." ></td>
	<td class="line x" title="188:191	Computational Linguistics, 20(2): t55-171." ></td>
	<td class="line x" title="189:191	R. A. Sharman (1990)." ></td>
	<td class="line x" title="190:191	Hidden Markov Model Methods for Word Tagging." ></td>
	<td class="line x" title="191:191	Technical Report UKSC 214, IBM UK Scientific Centre ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="A94-1027
A Probabilistic Model For Text Categorization: Based On A Single Random Variable With Multiple Values
Iwayama, Makoto;Tokunaga, Takenobu;"></td>
	<td class="line x" title="1:190	A Probabilistic Model for Text Categorization: Based on a Single Random Variable with Multiple Values Makoto IWAYAMA Advanced Research Laboratory Hitachi Ltd. HATOYAMA SAITAMA 350-03, JAPAN iwayamaharl, hit achi." ></td>
	<td class="line x" title="2:190	co. j p Abstract Text categorization is the classification of documents with respect to a set of predefined categories." ></td>
	<td class="line x" title="3:190	In this paper, we propose a new probabilistic model for text categorization, that is based on a Single random Variable with Multiple Values (SVMV)." ></td>
	<td class="line x" title="4:190	Compared to previous probabilistic models, our model has the following advantages; 1) it considers within-document term frequencies, 2) considers term weighting for target documents, and 3) is less affected by having insufficient training cases." ></td>
	<td class="line x" title="5:190	We verify our model's superiority over the others in the task of categorizing news articles from the 'Wall Street Journal'." ></td>
	<td class="line x" title="6:190	1 Introduction Text categorization is the classification of documents with respect to a set of predefined categories." ></td>
	<td class="line x" title="7:190	As an example, let us take a look at the following article from the 'Wall Street Journal' (1989/11/2)." ></td>
	<td class="line x" title="8:190	McDermott International, Inc. said its Babcock & Wilcox unit completed the sale of its Bailey Controls Operations to Finmeccanica S.p.A for $295 million." ></td>
	<td class="line x" title="9:190	Finmeccanica is an Italian state-owned holding company with interests in the mechanical engineering industry." ></td>
	<td class="line x" title="10:190	Bailey Controls, based in Wickliffe, Ohio, makes computerized industrial controls systems." ></td>
	<td class="line x" title="11:190	It employs 2,700 people and has annual revenue of about $370 million." ></td>
	<td class="line x" title="12:190	Two categories (topics) are manually assigned to this article; 'TENDER OFFERS, MERGERS, ACQUISITIONS (TNM)' and 'COMPUTERS AND INFORMATION TECHNOLOGY (CPK)'." ></td>
	<td class="line x" title="13:190	While there may be certain rules or standards for categorization, it is very difficult for human experts to assign categories consistently and efficiently to large numbers of daily incoming documents." ></td>
	<td class="line x" title="14:190	The purpose of this paper is to propose a new probabilistic model for automatic text categorization." ></td>
	<td class="line x" title="15:190	Takenobu TOKUNAGA Department of Computer Science Tokyo Institute of Technology 2-12-1, OOKAYAMA, MEGURO-KU TOKYO 152, JAPAN t ake@cs, tit ech." ></td>
	<td class="line x" title="16:190	ac." ></td>
	<td class="line x" title="17:190	jp While many text categorization models have been proposed so far, in this paper, we concentrate on the probabilistic models (Robertson and Sparck Jones, 1976; Kwok, 1990; Fuhr, 1989; Lewis, 1992; Croft, 1981; Wong and Yao, 1989; Yu et al. , 1989) because these models have solid formal grounding in probability theory." ></td>
	<td class="line x" title="18:190	Section 2 quickly reviews the probabilistic models and lists their individual problems." ></td>
	<td class="line x" title="19:190	In section 3, we propose a new probabilistic model based on a Single random Variable with Multiple Values (SVMV)." ></td>
	<td class="line x" title="20:190	Our model is very simple, but solves some problems of the previous models." ></td>
	<td class="line x" title="21:190	In section 4, we verify our model's superiority over the others through experiments in which we categorize 'Wall Street Journal' articles." ></td>
	<td class="line x" title="22:190	2 A Brief Survey of Probabilistic Text Categorization In this section, we will briefly review three major probabilistic models for text categorization." ></td>
	<td class="line x" title="23:190	Originally, these models have been exploited for information retrieval, but the adaptation to text categorization is straightforward." ></td>
	<td class="line x" title="24:190	In a model of probabilistic text categorization, P(cld ) = 'the probability that a document d is categorized into a category c' (1) is calculated." ></td>
	<td class="line x" title="25:190	Usually, a set of categories is defined beforehand." ></td>
	<td class="line x" title="26:190	For every document di, probability P(cldi ) is calculated and all the documents are ranked in decreasing order according to their probabilities." ></td>
	<td class="line x" title="27:190	The larger P(cldi) a document di has, the more probably it will be categorized into category c. This is called the Probabilistic Ranking Principle (PRP) (Robertson, 1977)." ></td>
	<td class="line x" title="28:190	Several strategies can be used to assign categories to a document based on PRP (Lewis, 1992)." ></td>
	<td class="line x" title="29:190	There are several ways to calculate P(c\[d)." ></td>
	<td class="line x" title="30:190	Three representatives are (Robertson and Sparck Jones, 1976), (Kwok, 1990), and (Fuhr, 1989)." ></td>
	<td class="line x" title="31:190	2.1 Probabilistlc Relevance Weighting (PRW) Robertson and Sparck Jones (1976) make use of the well-known logistic (or log-odds) transformation of the 162 probability P(c\]d)." ></td>
	<td class="line x" title="32:190	P(cld) g(cld ) = log P(~ld) (2) where ~ means 'not c', that is 'a document is not categorized into c'." ></td>
	<td class="line x" title="33:190	Since this is a monotonic transformation of P(cld), PRP is still satisfied after transformation." ></td>
	<td class="line x" title="34:190	Using Bayes' theorem, Eq." ></td>
	<td class="line x" title="35:190	(2) becomes P(d\]c) P(c) g(cld ) = log ~ + log p(~)." ></td>
	<td class="line x" title="36:190	(3) Here, P(c) is the prior probability that a document is categorized into c. This is estimated from given training data, i.e., the number of documents assigned to the category c. P(dl c) is calculated as follows." ></td>
	<td class="line x" title="37:190	If we assume that a document consists of a set of terms (usually nouns are used for the first approximation) and each term appears independently in a document, P(dlc ) is decomposed to P(dlc) = l-~ P(T/ = lie) l'~ P(~ = 01e) (4) tied tjEe-d where 'cd' is a set of terms that do not appear in d but appear in the training cases assigned to c. 'ti' represents the name of a term and 'T/ = 1, 0' represents whether or not the corresponding term '2i' appears in a document." ></td>
	<td class="line x" title="38:190	Therefore, P(T/= 1, 0\[c) is the probability that a document does or does not contain the term ti, given that the document is categorized into c. This probability is estimated from the training data; the number of documents that are categorized into c and have the term tl." ></td>
	<td class="line x" title="39:190	Substituting Eq." ></td>
	<td class="line x" title="40:190	(4) into Eq." ></td>
	<td class="line x" title="41:190	(3) yields P(~ = lie ) g(cld) = log P(T, 1re) tied P(e) + log +,--, P(Tj = 01c ) \ log tieZe-'Ce_d P(~ = 0l~ ) (5) We refer to Robertson and Sparck Jones' formulation as Probabilistic Relevance Weighting (PRW)." ></td>
	<td class="line x" title="42:190	While PRW is the first attempt to formalize wellknown relevance weighting (Sparck Jones, 1972; Salton and McGill, 1983) by probability theory, there are several drawbacks in PRW." ></td>
	<td class="line x" title="43:190	\[Problem 1\] no within-document term frequencies PRW does not make use of within-document term frequencies." ></td>
	<td class="line x" title="44:190	P(T = 1, 01c) in Eq." ></td>
	<td class="line x" title="45:190	(5) takes into account only the existence/absence of the term t in a document." ></td>
	<td class="line x" title="46:190	In general, frequently appearing terms in a document play an important role in information retrieval (Salton and McGill, 1983)." ></td>
	<td class="line x" title="47:190	Salton and Yang experimentally verified the importance of within-document term frequencies in their vector model (Salton and Yang, 1973)." ></td>
	<td class="line x" title="48:190	\[Problem 2\] no term weighting for target documents In the PRW formulation, there is no factor of term weighting for target documents (i.e. , P(.Id))." ></td>
	<td class="line x" title="49:190	According to Eq." ></td>
	<td class="line x" title="50:190	(5), even if a term exists in a target document, only the importance of the term in a category (i.e. , P(T = lie)) is considered for overall probability." ></td>
	<td class="line x" title="51:190	Term weighting for target documents would also be necessary for sophisticated information retrieval (Fuhr, 1989; Kwok, 1990)." ></td>
	<td class="line x" title="52:190	\[Problem 3\] affected by having insufficient training cases In practical situations, the estimation of P(T = 1, 01c ) is not always straightforward." ></td>
	<td class="line x" title="53:190	Let us consider the following case." ></td>
	<td class="line x" title="54:190	In the training data, we are given R documents that are assigned to c. Among them, r documents have the term t. In this example, the straightforward estimate of P(T -llc ) is 'r/R'." ></td>
	<td class="line x" title="55:190	If 'r = 0' (i.e. , none of the documents in c has t) and the target document d contains the term t, g(c\[d) becomes -0% which means that d is never categorized into c. Robertson and Sparck Jones mentioned other special cases like the above example (Robertson and Sparck Jones, 1976)." ></td>
	<td class="line x" title="56:190	A well-known remedy for this problem is to use '(r + 0.5)/(R + 1)' as the estimate of P(T = lie ) (Robertson and Sparck Jones, 1976)." ></td>
	<td class="line x" title="57:190	While various smoothing methods (Church and Gale, 1991; Jelinek, 1990) are also applicable to these situations and would be expected to work better, we used the simple 'add one' remedy in the following experiments." ></td>
	<td class="line x" title="58:190	2.2 Component Theory (CT) To solve problems 1 and 2 of PRW, Kwok (1990) stresses the assumption that a document consists of terms." ></td>
	<td class="line x" title="59:190	This theory is called the Component Theory (CT)." ></td>
	<td class="line x" title="60:190	To introduce within-document term frequencies (i.e. , to solve problem 1), CT assumes that a document is completely decomposed into its constituting terms." ></td>
	<td class="line x" title="61:190	Therefore, rather than counting the number of documents, as in PRW, CT counts the number of terms in a document for probability estimation." ></td>
	<td class="line x" title="62:190	This leads to within-document term frequencies." ></td>
	<td class="line x" title="63:190	Moreover, to incorporate term weighting for target documents (i.e. , to solve problem 2), CT defines g(cld ) as the geometric mean probabilities over components of the target document d; P(dlc) = \[YI P(TIc)\]~ ~ P(Tl'c) J ' Following Kwok's derivation, g(cld) becomes g(cld) = E P(T = tld)(log P(T tic ) P(T  tic) ted P(T ~ tl'd)~ P(c) log P(T -~-~' + log p(~)." ></td>
	<td class="line x" title="64:190	(6) + (7) 163 For precise derivation, refer to (Kwok, 1990)." ></td>
	<td class="line x" title="65:190	Here, note that P(T = rid ) and P(T = tic ) represent the frequency of a term t within a target document d and that within a category c respectively." ></td>
	<td class="line x" title="66:190	Therefore, CT is not subject to problems 1 and 2." ></td>
	<td class="line x" title="67:190	However, problem 3 still affects CT. Furthermore, Fuhr (1989) pointed out that transformation, as in Eq." ></td>
	<td class="line x" title="68:190	(6), is not monotonic of P(cld )." ></td>
	<td class="line x" title="69:190	It follows then, that CT does not satisfy the probabilistic ranking principle (PRP) any more." ></td>
	<td class="line x" title="70:190	2.3 Retrieval with Probabilistic Indexing (RPI) Fuhr (1989) solves problem 2 by assuming that a document is probabilistically indexed by its term vectors." ></td>
	<td class="line x" title="71:190	This model is called Retrieval with Probabilistie Indexing (RPI)." ></td>
	<td class="line x" title="72:190	In RPI, a document d has a binary vector -(T1,, Tn) where each component corresponds to a term." ></td>
	<td class="line x" title="73:190	7} = 1 means that the document d contains the term ti." ></td>
	<td class="line x" title="74:190	X is defined as the set of all possible indexings, where IX I = 2'." ></td>
	<td class="line x" title="75:190	Conditioning P(cld ) for each possible indexing gives P(c\[d) = Z P(cld'x)P(:rld)' (8) ~EX By assuming conditional independence between c and d given a~ 1, and using Bayes' theorem, Eq." ></td>
	<td class="line x" title="76:190	(8) becomes, P(~lc)P(zld) P(cld ) = P(c) Z P(z) (9) :~EX Assuming that each term appears independently in a target document d and in a document assigned to c, Eq." ></td>
	<td class="line x" title="77:190	(9) is rewritten as 1-I(P(7} = llc)P(7} = lid ) P(c\]d) = P(c) P(7} = 1) i -t P(7} = 0Ic)P(T~ = 01d))." ></td>
	<td class="line x" title="78:190	P(7} -0) (lo) Here, all the probabilities are estimated from the training data using the same method described in Section 2.1." ></td>
	<td class="line x" title="79:190	Since Eq." ></td>
	<td class="line x" title="80:190	(10) includes the factor P(T = 1,01d) as well as P(T = 1,01c), RPI takes into account term weighting for target documents." ></td>
	<td class="line x" title="81:190	While this in principle solves problem 2, if we use a simple estimation method counting the number of documents which have a term, P(T = 1,0\]d) reduces to 1 or 0 (i.e, binary, not weighted)." ></td>
	<td class="line x" title="82:190	For example, when a target document d has a term t, P(t = 1\]d) = 1 and when not, P(T = lid ) = 0." ></td>
	<td class="line x" title="83:190	In the following experiments we used this binary estimation method, but non binary estimates could be used as in (Fuhr, 1989)." ></td>
	<td class="line x" title="84:190	1More precisely, P( cld, x) = P( c\[x ) which assumes that if we know x, information for c is independent of that for d. This assumption sounds valid because x is a kind of representation of d. As far as other problems are concerned, RPI still problematic." ></td>
	<td class="line x" title="85:190	In particular, because of problem 3, P(cld) would become an illegitimate value." ></td>
	<td class="line x" title="86:190	In our experiments, as well as in Lewis' experiments (1992), P(cld ) ranges from 0 to more than 101." ></td>
	<td class="line x" title="87:190	3 A Probabilistic Model Based on a Single Random Variable with Multiple Values (SVMV) In this section, we propose a new probabilistic model for text categorization, and compare it to the previous three models from several viewpoints." ></td>
	<td class="line x" title="88:190	Our model is very simple, but yet solves problems 1, 2, and 3 in PRW." ></td>
	<td class="line x" title="89:190	Document representation of our model is basically the same as CT, that is a document is a set of its constituting terms." ></td>
	<td class="line x" title="90:190	The major difference between our model and others is the way of document characterization through probabilities." ></td>
	<td class="line x" title="91:190	While almost all previous models assume that an event space for a document is whether the document is indexed or not by a term 2, our model characterizes a document as random sampling of a term from the term set that represents the document." ></td>
	<td class="line x" title="92:190	For example, an event 'T = ti' means that a randomly selected term from a document is ti." ></td>
	<td class="line x" title="93:190	If we want to emphasis indexing process like other models, it is possible to interpret 'T = ti' as a randomly selected element from a document being indexed by the term ti." ></td>
	<td class="line x" title="94:190	Formally, our model can be seen as modifying Fuhr's derivation of P(cld ) by replacing an index vector with a single random variable whose value is one of possible terms." ></td>
	<td class="line x" title="95:190	Conditioning P(cld ) for each possible event gives P(cld) = ~_~ P(c\]d, T = ti)f(T = tild)." ></td>
	<td class="line x" title="96:190	(11) ti If we assume conditional independence between c and d, given T = ti, that is P(cid, T = ti) = P(c\]T = tl), we obtain P(cld) = Z P(cIT = ti)P(T = tild)." ></td>
	<td class="line x" title="97:190	(12) ti Using Bayes' theorem, this becomes Z P(T = tilc)P(T = ti Id) P(cld) P(c) t, P(T = ti) (13) All the probabilities in Eq." ></td>
	<td class="line x" title="98:190	(13) can be estimated from given training data based on the following definitions." ></td>
	<td class="line x" title="99:190	 P(T =tilc) is the probability that a randomly selected term in a document is ti, given that the document is assigned to c. We used ~c as the estimator." ></td>
	<td class="line x" title="100:190	NCi is the frequency of the term ti in the category c, and NC is the total frequency of terms in c. 2In section 2 explaining previous models, we simplified 'a document is indexed by a term' as 'a document contains a term' for ease of explanation." ></td>
	<td class="line x" title="101:190	164,, P(T = tild) is the probability that a randomly selected term in a target document d is ti." ></td>
	<td class="line x" title="102:190	We used ~D as the estimator." ></td>
	<td class="line x" title="103:190	NDi is the frequency of the term ti in the document d, and ND is the total frequency of terms in d. * P(T = ti) is the prior probability that a randomly selected term in a randomly selected document is ti." ></td>
	<td class="line x" title="104:190	We used ~t as the estimator." ></td>
	<td class="line x" title="105:190	Ni is the frequency of the term ti in the given training documents, and N is the total frequency of terms in the training documents." ></td>
	<td class="line x" title="106:190	 P(c) is the prior probability that a randomly selected document is categorized into c. We used D_~ as the estimator." ></td>
	<td class="line x" title="107:190	Dc is the frequency of docuD meats that is categorized to c in the given training documents, and D is the frequency of documents in the training documents." ></td>
	<td class="line x" title="108:190	Here, let us recall the three problems of PRW." ></td>
	<td class="line x" title="109:190	Since SVMV's primitive probabilities are based on withindocument term frequencies, SVMV does not have problem 1." ></td>
	<td class="line x" title="110:190	Furthermore, SVMV does not have problem 2 either because Eq." ></td>
	<td class="line x" title="111:190	(13) includes a factor P(T = tld), which accomplishes term weighting for a target document d. For problem 3, let us reconsider the previous example; R documents in the training data are categorized into a category c, none of the R documents has term ti, but a target document d does." ></td>
	<td class="line x" title="112:190	If the straightforward estimate of P(T/= llc ) = 0 or P(T = tilc) = 0 is adopted, the document d would never be categorized into c in the previous models (PRW, CT, and RPI)." ></td>
	<td class="line x" title="113:190	In SVMV, the probability P(c\[d) is much less affected by such estimates." ></td>
	<td class="line x" title="114:190	This is because P(cld ) in Eq." ></td>
	<td class="line x" title="115:190	(13) takes the sum of each term's weight." ></td>
	<td class="line x" title="116:190	In this example, the weight for ti is estimated to be 0 as in the other models, but this little affect the total value of P(c\[d)." ></td>
	<td class="line x" title="117:190	A similar argument applies to all other problems in (Robertson and Sparck Jones, 1976) that are caused by having insufficient training cases." ></td>
	<td class="line x" title="118:190	SVMV is formally proven not to suffer from the serious effects (like never being assigned to a category or always being assigned to a category) by having insufficient training cases." ></td>
	<td class="line x" title="119:190	In other words, SVMV can directly use the straightforward estimates." ></td>
	<td class="line x" title="120:190	In addition, we experimentally verified that the value of P(dlc ) in SVMV is always a legitimate value (i.e. , 0 to 1) unlike in RPI." ></td>
	<td class="line x" title="121:190	Table 1 summarizes the characteristics of the four probabilistic models." ></td>
	<td class="line x" title="122:190	Table 1 Summary of the four probabilistic models PRW CT RPI SVMV Problem 1 considered no yes no yes Problem 2 considered no yes (yes) yes Problem 3 considered no no no yes PRP satisfied yes no ye s yes As illustrated in the table, SVMV has better characteristics for text categorization compared to the previous models." ></td>
	<td class="line x" title="123:190	In the next section, we will experimentally verify SVMV's superiority." ></td>
	<td class="line x" title="124:190	4 Experiments This section describes experiments conducted to evaluate the performance of our model (SVMV) compared to the other three (PRW, CT, and RPI)." ></td>
	<td class="line x" title="125:190	4.1 Data and Preprocessing A collection of Wall Street Journal (WSJ) full-text news stories (Liberman, 1991) 3 was used in the experiments." ></td>
	<td class="line x" title="126:190	We extracted all 12,380 articles from 1989/7/25 to 1989/11/2." ></td>
	<td class="line x" title="127:190	The WSJ articles from 1989 are indexed with 78 categories (topics)." ></td>
	<td class="line x" title="128:190	Articles having no category were excluded." ></td>
	<td class="line x" title="129:190	8,907 articles remained; each having 1.94 categories on the average." ></td>
	<td class="line x" title="130:190	The largest category is 'TENDER OFFERS, MERGERS, ACQUISITIONS (TNM)' which encompassed 2,475 articles; the smallest one is 'RUBBER (RUB)', assigned to only 2 articles." ></td>
	<td class="line x" title="131:190	On the average, one category is assigned to 443 articles." ></td>
	<td class="line oc" title="132:190	All 8,907 articles were tagged by the Xerox Part-ofSpeech Tagger (Cutting et al. , 1992) 4." ></td>
	<td class="line x" title="133:190	From the tagged articles, we extracted the root words of nouns using the 'ispell' program 5." ></td>
	<td class="line x" title="134:190	As a result, each article has a set of root words representing it, and each element in the set (i.e. root word of a noun) corresponds to a term." ></td>
	<td class="line x" title="135:190	We did not reduce the number of terms by using stop words list or feature selection method, etc. The number of terms amounts to 32,975." ></td>
	<td class="line x" title="136:190	Before the experiments, we divided 8,907 articles into two sets; one for training (i.e. , for probability estimation), and the other for testing." ></td>
	<td class="line x" title="137:190	The division was made according to chronology." ></td>
	<td class="line x" title="138:190	All articles that appeared from 1989/7/25 to 1989/9/29 went into a training set of 5,820 documents, and all articles from 1989/10/2 to 1989/11/2 went into a test set of 3,087 documents." ></td>
	<td class="line x" title="139:190	4.2 Category Assignment Strategies In the experiments, the probabilities, P(c), P(Ti = llc), P(T =tilc), and so forth, were estimated from the 5,820 training documents, as described in the previous sections." ></td>
	<td class="line x" title="140:190	Using these estimates, we calculated the posterior probability (P(cld)) for each document (d) of the 3,087 test documents and each of the 78 categories 3We used 'ACL/DCI (September 1991)' CD-ROM which is distributed from the Linguistic Data Consortium (LDC)." ></td>
	<td class="line x" title="141:190	For more details, please contact Mark Liberman (myl@lmagi." ></td>
	<td class="line x" title="142:190	cis." ></td>
	<td class="line x" title="143:190	upena, edu)." ></td>
	<td class="line x" title="144:190	4The xerox part-of-speech tagger version 1.0 is available via anonymous FTP from the host parc~tp.xerox.cora in the directory pub/tagger." ></td>
	<td class="line x" title="145:190	5Ispell is a program for correcting English spelling." ></td>
	<td class="line x" title="146:190	We used the 'ispell version 3.0' which is available via anonymous FTP from the host ftp." ></td>
	<td class="line x" title="147:190	as.ucla." ></td>
	<td class="line x" title="148:190	edu in the directory pub/ispell." ></td>
	<td class="line x" title="149:190	165 (c)." ></td>
	<td class="line x" title="150:190	The four probabilistic models are compared in this calculation." ></td>
	<td class="line x" title="151:190	There are several strategies for assigning categories to a document based on the probability P(cld )." ></td>
	<td class="line x" title="152:190	The simplest one is the k-per-doc strategy (Field, 1975) that assigns the top k categories to each document." ></td>
	<td class="line x" title="153:190	A more sophisticated one is the probability threshold strategy, in which all the categories above a user-defined threshold are assigned to a document." ></td>
	<td class="line x" title="154:190	Lewis proposed the proportional assignment strategy based on the probabilistic ranking principle (Lewis, 1992)." ></td>
	<td class="line x" title="155:190	Each category is assigned to its top scoring documents in proportion to the number of times the category was assigned in the training data." ></td>
	<td class="line x" title="156:190	For example, a category assigned to 2% of the training documents would be assigned to the top scoring 0.2% of the test documents if the proportionality constant was 0.1, or to 10% of the test documents if the proportionality constant was 5.0." ></td>
	<td class="line x" title="157:190	4.3 Results and Discussions By using a category assignment strategy, several categories are assigned to each test document." ></td>
	<td class="line x" title="158:190	The best known measures for evaluating text categorization models are recall and precision, calculated by the following equations (Lewis, 1992): the number of categories that are Recall : correctly assigned to documents the number of categories that should be' assigned to documents the number of categories that are Precision = correctly assigned to documents the number of categories that are' assigned to documents Note that recall and precision have somewhat mutually exclusive characteristics." ></td>
	<td class="line x" title="159:190	To raise the recall value, one can simply assign many categories to each document." ></td>
	<td class="line x" title="160:190	However, this leads to a degradation in precision; i.e., almost all the assigned categories are false." ></td>
	<td class="line x" title="161:190	A breakeven point might be used to summarize the balance between recall and precision, the point at which they are equal." ></td>
	<td class="line x" title="162:190	For each strategy, we calculated breakeven points by using the four probabilistic models." ></td>
	<td class="line x" title="163:190	Table 2 shows the best breakeven points identified for the three strategies along with the used models." ></td>
	<td class="line x" title="164:190	Table 2 Best breakeven points for three category assignment strategies Breakeven Pts." ></td>
	<td class="line x" title="165:190	Prop." ></td>
	<td class="line x" title="166:190	assignment 0.63 (by SVMV) Prob." ></td>
	<td class="line x" title="167:190	thresholding 0.47 (by SVMV) k-per-doe 0.43 (by SVMV) From Table 2, we find that SVMV with proportional assignment gives the best result (0.63)." ></td>
	<td class="line x" title="168:190	The superiority of proportional assignment over the other strategies has already been reported by Lewis (1992)." ></td>
	<td class="line x" title="169:190	Our experiment verified Lewis' assumption." ></td>
	<td class="line x" title="170:190	In addition, for any of the three strategies, SVMV gives the highest breakeven point among the four probabilistic models." ></td>
	<td class="line x" title="171:190	Figure 1 shows the recall/precision trade off for the four probabilistic models with proportional assignment strategy." ></td>
	<td class="line x" title="172:190	As a reference, the recall/precision curve of a well-known vector model (Salton and Yang, 1973) ('TF.IDF')6 is also presented." ></td>
	<td class="line x" title="173:190	Table 3 lists the breakeven point for each model." ></td>
	<td class="line x" title="174:190	All the breakeven points were obtained when proportionality constant was about 1.0." ></td>
	<td class="line x" title="175:190	Fig." ></td>
	<td class="line x" title="176:190	1 OJ~Recall/precision with proportional assignment strategy : _o t 0." ></td>
	<td class="line x" title="177:190	0A0.70.1L 0.60A0.3 / 0.20.1 0.1 *'~ Breakeve~  RM ' L '." ></td>
	<td class="line x" title="178:190	~.-, PmlV TFJOF I I I 0., oi,, oI, olo o. , 0'.0 0.0 Recall Table 3 Breakeven points with proportional assignment strategy SVMV CT RPI PRW TF.IDF Breakeven Pts." ></td>
	<td class="line x" title="180:190	0.63 0.60 0.51 0.53 0.48 From Figure 1 and Table 3, we can see that:  as far as this dataset is concerned, SVMV with proportional assignment strategy gives the best result among the four probabilistic models,  the models that consider within-document term frequencies (SVMV, CT) are better than those that do not (PRW, RPI), Sin the model we used, each element of document vector is the 'term frequency' multiplied by the 'inverted document frequency'." ></td>
	<td class="line x" title="181:190	Similarity between every pair of vectors is measured by cosine." ></td>
	<td class="line x" title="182:190	Note that this is the simplest version of TF.IDF model, and there has been many improvements which we did not consider in the experiments." ></td>
	<td class="line x" title="183:190	166  the models that consider term weighting for target documents (SVMV, CT) are better than those that do not (PRW, (RPI)), and  the models that are less affected by having insufficient training cases (SVMV) are better than those that are (CT, RPI, PRW)." ></td>
	<td class="line x" title="184:190	5 Conclusion We have proposed a new probabilistic model for text categorization." ></td>
	<td class="line x" title="185:190	Compared to previous models, our model has the following advantages; 1) it considers within document term frequencies, 2) considers term weighting for target documents, and 3) is less affected by having insufficient training cases." ></td>
	<td class="line x" title="186:190	We have also provided empirical results verifying our model's superiority over the others in the task of categorizing news articles from the 'Wall Street Journal'." ></td>
	<td class="line x" title="187:190	There are several directions along which this work could be extended." ></td>
	<td class="line x" title="188:190	 We have to compare our probabilistic model to other non probabilistic models like decision tree/rule based models, one of which has recently been reported to be promising (Apt4 et al. , 1994)." ></td>
	<td class="line x" title="189:190	 While we used simple document representation in which a document is defined as a set of nouns, there could be considered several improvements, such as using phrasal information (Lewis, 1992), clustering terms (Sparck Jones, 1973), reducing the number of features by using local dictionary (Apt4 et al. , 1994), etc.  We are incorporating our probabilistic model into cluster-based text categorization that offers an efficient and effective search strategy." ></td>
	<td class="line x" title="190:190	Acknowledgments The authors are grateful to Hiroshi Motoda for beneficial discussions, and would like to thank the anonymous reviewers for their useful comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C94-1027
Part-Of-Speech Tagging With Neural Networks
Schmid, Helmut;"></td>
	<td class="line x" title="1:202	PART-OF-SPEECH TAGGING WITH NEURAL NETWORKS Hehnut Schmid Institute for Computational Linguistics, Azenbergstr.12, 70174 Stuttgart, Germany, schmid@ims.uni-stuttgart.de Topic area: large text corpora, part-of-speech tagging, neural networks 1 ABSTRACT Text corpora which are tagged with part-of-speech information are useful in many areas of linguistic research." ></td>
	<td class="line oc" title="2:202	In this paper, a new part-of-speech tagging method hased on neural networks (Net-Tagger) is presented and its performance is compared to that of a llMM-tagger (Cutting et al. , 1992) and a trigrambased tagger (Kempe, 1993)." ></td>
	<td class="line n" title="3:202	It is shown that the Net-Tagger performs as well as the trigram-based tagger and better than the iIMM-tagger." ></td>
	<td class="line x" title="4:202	2 INTRODUCTION Words are often ambiguous in their part of speech." ></td>
	<td class="line x" title="5:202	The English word store for example can be either a noun, a finite verb or an infinitive." ></td>
	<td class="line x" title="6:202	In an utterance, this ambiguity is normally resolved by the context of a word: e.g. in the seutence 'The 1977 P6's could store two pages of data." ></td>
	<td class="line x" title="7:202	', store can only be an intluitive." ></td>
	<td class="line x" title="8:202	A part-of-speech tagger is a system which automatically assigns the part of speech to words using contextual information." ></td>
	<td class="line x" title="9:202	Potential applications for partof-speech taggers exist in many areas inclnding speech recognition, speech synthesis, machine translation and information retrieval." ></td>
	<td class="line x" title="10:202	l)ifi'ereut methods have been used for the im plemenration of part-of-speech taggers." ></td>
	<td class="line x" title="11:202	TAGGIT (Greene, Rnbin, 1971), an early system, which was used for the initial tagging of the Brown corpus was rule-based." ></td>
	<td class="line x" title="12:202	It was able to assign the correct part-of-speech to about 77 % of the words in the Brown corpus." ></td>
	<td class="line x" title="13:202	In another approach contextual dependencies are modelled statistically." ></td>
	<td class="line x" title="14:202	Churcb (1988) and Kempe (1993) use second order Markov Models and train their systems on large handtagged corpora." ></td>
	<td class="line x" title="15:202	Using this metbod, they are able to tag more than 96 % of their test words with the correct part-of-speech." ></td>
	<td class="line x" title="16:202	The need for reliably tagged training data, however, is a problem for languages, where such data is not available in sufficient quantities." ></td>
	<td class="line oc" title="17:202	Jelinek (1985) and Cutting et al.(1992) circumvent this problem by training their taggers on untagged data using tile Itaum-Welch algorithm (also know as the forward-backward algorithm)." ></td>
	<td class="line o" title="19:202	They report rates of correctly tagged words which are comparable to that presented by Church (1988) and Kempe (1993)." ></td>
	<td class="line x" title="20:202	A third and rather new approach is tagging with artificial neural networks." ></td>
	<td class="line x" title="21:202	In the area of speech recognition neural networks have been used for a decade r, ow." ></td>
	<td class="line x" title="22:202	They have shown performances comparable to that of IIidden Ivlarkov model systems or even better (Lippmann, 1989)." ></td>
	<td class="line x" title="23:202	Part-of-speech prediction is another area, closer to POS tagging, where neural networks have been applied successfidly." ></td>
	<td class="line x" title="24:202	Nakamura el; al." ></td>
	<td class="line x" title="25:202	(1990) trained a d-layer feed-forward network with up to three preceding part-of-speech tags,as input to predict the word category of the next word." ></td>
	<td class="line x" title="26:202	The prediction accuracy was similar to that of a trigram-b,~sed predictor." ></td>
	<td class="line x" title="27:202	Using tile predictor, Nakamura et al. were able to improve the recognition rate of their speech recognition system from 81.0 % to 86.9 %." ></td>
	<td class="line x" title="28:202	Federici and Pirrelli (199a) developed a part-ofspeech tagger which is based on a special type of neural network." ></td>
	<td class="line x" title="29:202	It disambiguates between alternative morphosyntactic tags which are generated by a roof phological analyzer." ></td>
	<td class="line x" title="30:202	The tagger is trained with an analogy-driven learning procedure." ></td>
	<td class="line x" title="31:202	Only preliminary results are presented, so that a comparison with other methods is difficult." ></td>
	<td class="line x" title="32:202	Ill this paper, a part-of-speech tagger based on a multilayer perceptrou network is presented." ></td>
	<td class="line x" title="33:202	It is similar to tile network of Nakamura et al.(1990) in so far as the same training procedure (Backpropagation) is used; but it differs in the structure of tile network and also in its purpose (disambignation vs. prediction)." ></td>
	<td class="line oc" title="35:202	The performance of tl,e presented tagger is measured and compared to that of two other taggers (Cutting et al. , 1992; Kempe, 1993)." ></td>
	<td class="line x" title="36:202	3 NEURAL NETWORKS Artificial neural networks consist of a large number of simple processing units." ></td>
	<td class="line x" title="37:202	These units are highly interconnected by directed weighted links." ></td>
	<td class="line x" title="38:202	Associated with each unit is an activation value." ></td>
	<td class="line x" title="39:202	Through tile connections, this activation is propagated to other units." ></td>
	<td class="line x" title="40:202	In mnltilayer perceptron networks (MLP-networks), tile most popular network type, the processing units are arranged vertically in several layers (fig." ></td>
	<td class="line x" title="41:202	I)." ></td>
	<td class="line x" title="42:202	Connections exist only between units in adjacent layers." ></td>
	<td class="line x" title="43:202	The bottom layer is called input layer', because the activations of the units in this layer represent the input of tile network." ></td>
	<td class="line x" title="44:202	Correspondingly, the top layer is called output layer." ></td>
	<td class="line x" title="45:202	Any layers between input layer 772 Figure 1: A 3-layer perceptron network ( output units hidden units b input units and outlmt layer are called hidden layers." ></td>
	<td class="line x" title="46:202	Their actiwttions are not visible externally." ></td>
	<td class="line x" title="47:202	During the processing in a MLP-network, actiwttions are propagated from inlmt units through hidden units to output units." ></td>
	<td class="line x" title="48:202	At each unit j, the weighted inlmt activations aiwij are summed and a bias parameter Oj is added." ></td>
	<td class="line x" title="49:202	net i = ~ aiwlj + Oj (1) t The resulting network input,telj is then l)~uqsed through a sigmoid fimction (the logistic funclion) in order to restrict the value range of the resulting activation aj to the interval \[0,i\]." ></td>
	<td class="line x" title="50:202	1 a~ t + e ~, (:~) The network learns by adapting the weights of the connections between units, tmtil the correct output is t~rocluced." ></td>
	<td class="line x" title="51:202	One widely used method is the backl.'o p~gation algorithm which performs a gradient descent search on the error surface, The weight update ~XlOij, i.e. the difference between the old and the new value of weight wij, is here defined,~s: AWij -rlapi6pj, where {,,pj(1 --,,,)(t,,j 'pJ), if j is an output unit a,,~ =,,vj(l _avs)~vk,oik, (a) k if j is a hidden unit Ilere, Zp is the target output vector which the network lnnst learn t . 'Daining the MLP-network with the backpropagao tion rule guarantees that a local minimum of the error surface is found, thougl, this is not necessarily the global one." ></td>
	<td class="line x" title="52:202	In order to speed up the trahfiug process, a momentum term is often introduced into the update rormula: kWij(t -~' 1) '~ Oapi~pj '+ (:~lt)ij(l) (4) 1We assume here that the hia.s parameter Oj is realized ms a weight to an additional unit whidt has always the activation va}.ue 1 (cp." ></td>
	<td class="line x" title="53:202	(B.umelhart, McChdland, t98,1))." ></td>
	<td class="line x" title="54:202	For a de.tailed introduction to MLP networks see e.g.(lunaelhart, McClellan(l, 1984)." ></td>
	<td class="line x" title="56:202	r 4 TtIG I~_AGGER NIi',TWO1I( The Net-Tagger consists of a Ml, P-network and a lexicon (see tlg." ></td>
	<td class="line x" title="57:202	2)." ></td>
	<td class="line x" title="58:202	l;'igu,'e 2: Structure." ></td>
	<td class="line x" title="59:202	of I.he Net-Tagger without hidden layer; tile arrow symbolizes the connections between the layers." ></td>
	<td class="line x" title="60:202	11 f \ @ @  @    .-. @  @ @ @@ @ @    @  @  @@ p f In the output layer of the MLP network, each unit corresponds to one of the tags in the tagset." ></td>
	<td class="line x" title="61:202	The network learns during the training to activate that output unit which represents the correct tag and to deactivate all other output units, llence, in the trained network, the output unit with the higlu.'st activation indicates, which tag shouhl be attached to the word that is currently processed." ></td>
	<td class="line x" title="62:202	The input of the network comprises all the information whicii the systeni ti;Ls about the parts of speech of the current word, the p precedhig words al,d the f following words." ></td>
	<td class="line x" title="63:202	More precisely, for each part-of-speech tag posj and each of the p-t1-kf words in the context, there is an input unit whose activation in U represents the probability that wordl h~Ls part of speech posj." ></td>
	<td class="line x" title="64:202	For the word which is being tagged and the following words, the lezical part-of-speech probability l'(posj\]wordi) is all we know about the part of speech ~, This probability does not take into account arty contextual influences." ></td>
	<td class="line x" title="65:202	So, we get the following input representation for the currently tagged word and the following words: i,,,j : v(vo.,v I,,,o,.d,), ir i > o (s) 2 Lexical probabilities are estimated hy dividing, the number of times a word occurs with a giw:n tag by the own'all numher of times the word occurs." ></td>
	<td class="line x" title="67:202	This method is known as the Ma.vimum Likelihood Principle." ></td>
	<td class="line x" title="68:202	IZ'~ For tile preceding words, there is more information available, because they have already bccn tagged." ></td>
	<td class="line x" title="69:202	The activation values of the output units at the time of processing are here used instead of the lexieal part-ofspeech probabilitiesa: i,,;/t) = o,,t/t + O, if ; < 0 (6) Copying output activations of tile network into the input units introduces recurrence into the network." ></td>
	<td class="line x" title="70:202	This complicates the training process, because the output of the network is not correct, when the training starts and therefore, it cannot be fed back directly, when the training starts." ></td>
	<td class="line x" title="71:202	Instead a weighted average of the actual output and the target output is used." ></td>
	<td class="line x" title="72:202	It resembles more the output of the trained network which is similar (or at least shouhl be similar) to the target output." ></td>
	<td class="line x" title="73:202	At tile beginning of the training, the weighting of the target output is high." ></td>
	<td class="line x" title="74:202	It fails to zero during the training." ></td>
	<td class="line x" title="75:202	The network is trained on a tagged corpus." ></td>
	<td class="line x" title="76:202	Target activations are 0 for all output units, excepted for the unit which corresponds to the correct tag, for which it is 1." ></td>
	<td class="line x" title="77:202	A slightly modified version of the backpropagation algorithm with momentum term which has been presented in the last section is used: if the difference between the activation of an output unit j and the corresponding target output is below a predefined threshold (we used 0.1), the error signal ~pJ is set to zero." ></td>
	<td class="line x" title="78:202	In this way the network is forced to pay more attention to larger error signals." ></td>
	<td class="line x" title="79:202	This resulted in an improvement of the tagging accuracy by more than 1 percent." ></td>
	<td class="line x" title="80:202	Network architectures with and without hidden layers have been trained and tested." ></td>
	<td class="line x" title="81:202	In general, MLPnetworks with hidden layers are more powerful than networks without one, but they also need more training and there is a higher risk of overlearning 4." ></td>
	<td class="line x" title="82:202	As will be shown in the next section, the Net-Tagger did not profit from a hidden layer." ></td>
	<td class="line x" title="83:202	In both network types, the tagging of a single word is performed by copying the tag probabilities of the current word and its neighbours into the input units, propagating the activations through the network to the output units and determining the output unit which has the highest activation." ></td>
	<td class="line x" title="84:202	The tag corresponding to this unit is then attached to the current word." ></td>
	<td class="line x" title="85:202	If the second strongest activation in the output layer is close to the strongest one, tile tag corresponding to the second strongest activation may be given as an alternative output." ></td>
	<td class="line x" title="86:202	No additional computation is required for this." ></td>
	<td class="line x" title="87:202	Further, it is possible to give a scored list of all tags as output." ></td>
	<td class="line x" title="88:202	aThe output activations of the network do not necessarily sum to 1." ></td>
	<td class="line x" title="89:202	Therefore, they should not he interpreted as probabilities." ></td>
	<td class="line x" title="90:202	40verlearning means that irrelevant features of the training set are learned." ></td>
	<td class="line x" title="91:202	As a result, the uetwork is unable to generalize." ></td>
	<td class="line oc" title="92:202	5 TIIE LEXICON The lexicon which contains the a priori tag probabilities for each word is similar to the lexicon which was used by Cutting et al.(1992)." ></td>
	<td class="line x" title="94:202	it has three parts: a fullform lexicon, a suffix lexicon and a default enlry." ></td>
	<td class="line nc" title="95:202	No documentation of tile construction algorithm of the su\[lix lexicon in (Cutting et al. , 1992) was available." ></td>
	<td class="line x" title="96:202	Thus, a new method based on information theoretic principles was developed." ></td>
	<td class="line x" title="97:202	During the lookup of a word in the lexicon of the Net-Tagger, the fifllform lexicon is searched first." ></td>
	<td class="line x" title="98:202	If the word is found there, the corresponding tag probability vector is returned." ></td>
	<td class="line x" title="99:202	Otherwise, the uppercase letters of the word are turned to lowercase, and the search in the fullform lexicon is repeated." ></td>
	<td class="line x" title="100:202	If it fails again, the suIfix lexicon is searched next." ></td>
	<td class="line x" title="101:202	If none of the previous steps has been snccessfull, tile default entry of the lexicon is returned." ></td>
	<td class="line x" title="102:202	The fullform lexicon was created from a tagged training corpus (some 2 million words of the Penn Treebank Corpus)." ></td>
	<td class="line x" title="103:202	First, the number of occurrences of each word/tag pair was counted." ></td>
	<td class="line x" title="104:202	Afterwards, those tags of each word with an estimated probability of less than 1 percent were removed, because they were in most eases the result of tagging errors in the original corpus." ></td>
	<td class="line x" title="105:202	Figure 3: A sample suffix tree of length 3 ies Oils ous scd old ble lie nee ive ing ion SOIl ton man ity The second part of the lexicon, the suflix lexicon, forms a tree." ></td>
	<td class="line x" title="106:202	Each node of tile tree (excepted tile root node) is labeled with a character." ></td>
	<td class="line x" title="107:202	At tile leaves, tag probability vectors are attached." ></td>
	<td class="line x" title="108:202	During a lookup, tile suffix tree is searched from the root." ></td>
	<td class="line x" title="109:202	In each step, tile branch which is labeled with the next character from tile end of the word suffix, is followed." ></td>
	<td class="line x" title="110:202	Assume e.g., wc want to look for tile word taggiu 9 in the suflqx lexicon which is shown in fig." ></td>
	<td class="line x" title="111:202	3." ></td>
	<td class="line x" title="112:202	We start at the root (labeled #) and follow the branch which leads to the node labeled g. From there, we move to the node labeled n, and finally we end up in tile node 174 Table 1: Sample frequencies at a tree node and its two child nodes." ></td>
	<td class="line x" title="113:202	suffix ess 10 I gp/ <iS l_5 __1 2 t~a~ 143 sufllx ness suffix less 1 85 2 8 45 0 0 2 48 95 labeled i. This node is a leaf and the attached tag probability vector (which is not shown in lib." ></td>
	<td class="line x" title="114:202	3) is returned." ></td>
	<td class="line x" title="115:202	The suffix lexicon was automatically built from the training corpus." ></td>
	<td class="line x" title="116:202	First, a sujJiz tree wits constructed from the suffices of length 5 of sill words wliich were annotated with an open class l)art-of-speecli s. Then tag frequencies were cotlnted for all suffices and stored at the corresponding tree nodes." ></td>
	<td class="line x" title="117:202	In the next step, an information measure I(S) was calculated for each node of the tree: I(S) = ~ P(posiS ) tomd'(p,>,qS) (7) po* IIere, S is the suffix which corresponds to the current node and P(poslS ) is the probability of tag pos given a word with suffix S. Using this information measure, the suffix tree has been pruned." ></td>
	<td class="line x" title="118:202	For each leaf, the weighted information gain G(aS) was calculated: a(aS) = V(aS) (S(S) S(<,S)), (8) where S is the suffix of the parent node, aS is the suffix of the current node and F(aS) is the frequency of suffix nS." ></td>
	<td class="line x" title="119:202	If the information gain at some leaf of the suffix tree is below a given threshoht ~, it is removed." ></td>
	<td class="line x" title="120:202	The tag frequencies of all deleted subnodes of a parent node are collected at the defi, ult node of the parent node." ></td>
	<td class="line x" title="121:202	If the default node is the only renlaining subnodc, it is deleted too." ></td>
	<td class="line x" title="122:202	In this case, the parent node becomes a leaf and is also checked, whether it is deletable." ></td>
	<td class="line x" title="123:202	To illustrate this process consider the following example, where ess is the suffix of the parent node, less is tim suffix of one child node and hess is the suffix of the other child node." ></td>
	<td class="line x" title="124:202	The tag frequencies of these nodes are given in table 1." ></td>
	<td class="line x" title="125:202	Tim information measure for the parent node is: 86 86 10 10 S(ess)  Iog~  lo,a~- ~ 1.32 (9) 143 143 143 14'3 '\].'lie corresponding values for the chihl nodes are 0.39 for hess and 0.56 for less." ></td>
	<td class="line x" title="126:202	Now, we can determine the welghted information gain at each of the ehihl nodes." ></td>
	<td class="line x" title="127:202	We get: G(ness) = 48(1.32 0.39) = 44.64 (10) 5Opell class parts-of-speech are those, width allow for the production of new words {e.g. noun, verb, adjective)." ></td>
	<td class="line x" title="128:202	6We used a gain threshohl of 10." ></td>
	<td class="line x" title="129:202	Table 2: Comparison of recognition rates method accuracy t Net-Tagger 96.22 % trigrarn tagger 96.06 % IIMM tagger 94.24 % G(less) = 95(1.320.56) = 72,20 (11) Both wdues are well above a threshohl of 10, and therefore none of them should be deleted." ></td>
	<td class="line x" title="130:202	As explained before, the suflix tree is walked during a lookup along the l)ath, where the nodes are annotated with the letters of the word snflix in reversed order." ></td>
	<td class="line x" title="131:202	If at some node on the path, no matching subnode can be found, and there is a default subitode, then the default node is followed." ></td>
	<td class="line x" title="132:202	If a leaf is reached at the end of the path, the corresponding tag probability vector is returned." ></td>
	<td class="line x" title="133:202	Otherwise, the search fails and the default entry is returned." ></td>
	<td class="line x" title="134:202	The defaull entry is constructed by subtracting the tag frequencies at all leaves of the pruned suffix tree from the tag frequencies of the root node and normalizing the resulting frequencies." ></td>
	<td class="line x" title="135:202	Thereby, relative frequencies are obtained which sum to one." ></td>
	<td class="line x" title="136:202	6 Rl,~suurs The 2-layer version of the Net-Tagger w,~s trained on a 2 million word subpart of the Pe.nn-Treebank corpus." ></td>
	<td class="line x" title="137:202	Its performance was tested on a 100,000 word subpart which was not part of the training corlms." ></td>
	<td class="line x" title="138:202	The settings of the network parameters were as follows: the number of preceding words in the context p w,~s 3, the number of following words f was 2 and the number of training cycles was 4 millions." ></td>
	<td class="line x" title="139:202	The training of the tagger took one day on a Sparcl0 workstation and the tagging of 100,000 words took 12 minutes on the same machine." ></td>
	<td class="line oc" title="140:202	In tabh; 2, the accuracy rate of the Net-Tagger is cOrolLated to that of a trigram l)msed tagger (Kempe, 1993) and a lIidden Markov Model tagger (Cutting et al. , 1992) which were." ></td>
	<td class="line o" title="141:202	trained and tested on the same data." ></td>
	<td class="line o" title="142:202	In order to determine the influence of tim size of the training sample, the taggers were also trained on corpora of different sizes and tested again r. The resulting percentages of correctly tagged words are shown in figure 4." ></td>
	<td class="line n" title="143:202	These experiments demonstrate that the performance of the Net-Tagger is comparable to that of the trigram tagger and better than that of the IIMM tagger." ></td>
	<td class="line x" title="144:202	They further show tl,at the performance of the Net-Tagger is less affected by a small amount of training data than that of tim trigram tagger." ></td>
	<td class="line x" title="145:202	This may be due to a much smaller number of paraineters in the Net-Tagger: while the trigram tagger must accurately ~l:or this test, a slightly simpler netwm'k structure with two preceding and one following word in the input context was used." ></td>
	<td class="line x" title="146:202	775 Figure 4: Recognition rates for varying sizes of the training corpus." ></td>
	<td class="line x" title="147:202	100 '', ''    ' ,'   , 95 .=_ 90 ~  .jr '' :~ 85 Net-Tagger -'~o= Xer0x-Tagger  o J 80 -~ Trigram Tagger -='75  ,I  I  I 10000 100000 1 e+06 size of training corpus estimate 110,592 trigrams, the Net-Tagger only has to train 13,824 network parameters." ></td>
	<td class="line x" title="148:202	It was fitrther tested, whether an additional hidden layer in the network with 50 units would improve the accuracy of the tagging." ></td>
	<td class="line x" title="149:202	It turned out that the accuracy actually deteriorated slightly, although the number of training cycles had been increased to 50 millions s. Also, tire influence of the size of the input context was determined." ></td>
	<td class="line x" title="150:202	Shrinking the context from three preceding and two following words to two preceding and one following word reduced the accuracy only by 0.1%." ></td>
	<td class="line x" title="151:202	Enlarging the context gave no improvement." ></td>
	<td class="line x" title="152:202	A context of three preceding and two following words seems to he optimal." ></td>
	<td class="line x" title="153:202	As mentioned previously, the tagger can produce an alternative tag, if the decision between two tags is difficult." ></td>
	<td class="line x" title="154:202	In that way, the accuracy can be raised to 97.79 % at the expense of 4.6 % ambiguously tagged words." ></td>
	<td class="line x" title="155:202	An analysis of tire errors of the Net-Tagger and the trigram tagger shows that both have problems with the same words, althot, gh the individual errors are often different 9 . 7 CONCLUSIONS In this paper, the Net-Tagger was presented, a partof-speech tagger which is based on a MLP-network." ></td>
	<td class="line o" title="156:202	A comparison of the tagging results with those of a trigram tagger and a IIMM tagger showed that the accuracy is as high as that of the trigram tagger and the robustness on small training corpora is as good as that of the HMM tagger." ></td>
	<td class="line x" title="157:202	Thus, the Net-Tagger combines advantages of both of these methods." ></td>
	<td class="line x" title="158:202	The Net-Tagger has the additional advantage that problematic decisions between tags are easy to detect, aDue to the large training times needed to train the 3-layernetwork, no further tests have been conducted." ></td>
	<td class="line x" title="159:202	o Less than 60 % of the tagging errors were made in common by both taggers." ></td>
	<td class="line x" title="160:202	so that in these cases an additional tag can be given in the output." ></td>
	<td class="line x" title="161:202	In this way, the final decision can be delayed to a later processing stage, e.g. a parser." ></td>
	<td class="line x" title="162:202	A disadvantage of the presented method may be its lower processing speed compared to statistical methods." ></td>
	<td class="line x" title="163:202	In the light of the high speed of present computer hardware, however, this does not seem to be a serious drawback." ></td>
	<td class="line x" title="164:202	8 REFERENCES Church, K. W." ></td>
	<td class="line x" title="165:202	(1985)." ></td>
	<td class="line x" title="166:202	A stochastic parts program and noun phrase parser for unrestricted text." ></td>
	<td class="line x" title="167:202	Proceedings of the Second Conference on Applied Natvral Language Processing, p. 136-143." ></td>
	<td class="line xc" title="168:202	Cutting, D. , a. Kupiec, a. Pedersen and P. Sibun (1992)." ></td>
	<td class="line x" title="169:202	A practical part-of-speech tagger." ></td>
	<td class="line x" title="170:202	Proceedings of the Third Conference on Applied Nalural Laguage Processing, r1ento, Italy (ACL), pages 133-140, 1992." ></td>
	<td class="line x" title="171:202	Also awtilable as Xerox technical report SSL-92-01." ></td>
	<td class="line x" title="172:202	Federici, S. and V. Pirrelli (1993)." ></td>
	<td class="line x" title="173:202	Analogical modelling of text tagging, unpublished report, Istituto di Linguistica Computazionale, Pisa, Italy." ></td>
	<td class="line x" title="174:202	Greene, 1t." ></td>
	<td class="line x" title="175:202	B and G. M. R.ubin (1971)." ></td>
	<td class="line x" title="176:202	Automatic grammatical tagging of English." ></td>
	<td class="line x" title="177:202	technical report, Department of Linguistics, Brown University, Providence, Rhode Island." ></td>
	<td class="line x" title="178:202	aelinek, F." ></td>
	<td class="line x" title="179:202	(1985)." ></td>
	<td class="line x" title="180:202	Markov Source modeling of text generation'." ></td>
	<td class="line x" title="181:202	In J.K. Skwirzinski Ed., Impact of Processing Techniques on Communication, Nijhoff, Dordrecht." ></td>
	<td class="line x" title="183:202	Kempe, A." ></td>
	<td class="line x" title="184:202	(1993)." ></td>
	<td class="line x" title="185:202	A stochastic Tagger and an Analysis of Tagging Errors." ></td>
	<td class="line x" title="186:202	Internal paper." ></td>
	<td class="line x" title="187:202	Institute for Computational Linguistics, University of Stuttgart." ></td>
	<td class="line x" title="188:202	Lippmann, R P." ></td>
	<td class="line x" title="189:202	(1989)." ></td>
	<td class="line x" title="190:202	Review of Neural Networks for Speech Recognition." ></td>
	<td class="line x" title="191:202	Neural Computation, Vol." ></td>
	<td class="line x" title="192:202	i, p. 1-38." ></td>
	<td class="line x" title="193:202	Nakamura, M. , I(." ></td>
	<td class="line x" title="194:202	Marnyama, T. Kawabata and K. Shikano (1990)." ></td>
	<td class="line x" title="195:202	Neural network approach to word category prediction for Englis}i texts." ></td>
	<td class="line x" title="196:202	In iI." ></td>
	<td class="line x" title="197:202	l(arlgren Ed., COLING-90, lIelslnki University, p. 213-218." ></td>
	<td class="line x" title="199:202	Rumelhart, D. E. and J. L. McClelland (1984)." ></td>
	<td class="line x" title="200:202	Parallel Distributed Processing." ></td>
	<td class="line x" title="201:202	MIT-Press, Cambridge, MA." ></td>
	<td class="line x" title="202:202	176" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="J94-2001
Tagging English Text With A Probabilistic Model
Merialdo, Bernard;"></td>
	<td class="line x" title="1:179	Tagging English Text with a Probabilistic Model Bernard Merialdo 't Institut EURECOM In this paper we present some experiments on the use of a probabilistic model to tag English text, i.e. to assign to each word the correct tag (part of speech) in the context of the sentence." ></td>
	<td class="line x" title="2:179	The main novelty of these experiments is the use of untagged text in the training of the model." ></td>
	<td class="line x" title="3:179	We have used a simple triclass Marlcov model and are looking for the best way to estimate the parameters of this model, depending on the kind and amount of training data provided." ></td>
	<td class="line x" title="4:179	Two approaches in particular are compared and combined:  using text that has been tagged by hand and computing relative frequency counts,  using text without tags and training the model as a hidden Markov process, according to a Maximum Likelihood principle." ></td>
	<td class="line x" title="5:179	Experiments show that the best training is obtained by using as much tagged text as possible." ></td>
	<td class="line x" title="6:179	They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy." ></td>
	<td class="line x" title="7:179	In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available." ></td>
	<td class="line x" title="8:179	1." ></td>
	<td class="line x" title="9:179	Introduction A lot of effort has been devoted in the past to the problem of tagging text, i.e. assigning to each word the correct tag (part of speech) in the context of the sentence." ></td>
	<td class="line oc" title="10:179	Two main approaches have generally been considered: rule-based (Klein and Simmons 1963; Brodda 1982; Paulussen and Martin 1992; Brill et al. 1990) probabilistic (Bahl and Mercer 1976; Debili 1977; Stolz, Tannenbaum, and Carstensen 1965; Marshall 1983; Leech, Garside, and Atwell 1983; Derouault and Merialdo 1986; DeRose 1988; Church 1989; Beale 1988; Marcken 1990; Merialdo 1991; Cutting et al. 1992)." ></td>
	<td class="line x" title="11:179	More recently, some work has been proposed using neural networks (Benello, Mackie, and Anderson 1989; Nakamura and Shikano 1989)." ></td>
	<td class="line x" title="12:179	Multimedia Communications Department, Institut EURECOM, 2229 Route des Cretes, B.P. 193, 06904 Valbonne Cedex France; merialdo@eurecom.fr." ></td>
	<td class="line x" title="13:179	t This work was carried out while the author was a visitor of the Continuous Speech Recognition group, IBM T. J. Watson Research Center, Yorktown Heights, NY (USA)." ></td>
	<td class="line x" title="14:179	Part of the material included in this work has been presented at the IEEE International Conference on Acoustics, Speech and Signal Processing, Toronto (Canada), May 1991." ></td>
	<td class="line x" title="15:179	f~) 1994 Association for Computational Linguistics Computational Linguistics Volume 20, Number 2 Through these different approaches, some common points have emerged: For any given word, only a few tags are possible, a list of which can be found either in the dictionary or through a morphological analysis of the word." ></td>
	<td class="line x" title="16:179	When a word has several possible tags, the correct tag can generally be chosen from the local context, using contextual rules that define the valid sequences of tags." ></td>
	<td class="line x" title="17:179	These rules may be given priorities so that a selection can be made even when several rules apply." ></td>
	<td class="line x" title="18:179	These kinds of considerations fit nicely inside a probabilistic formulation of the problem (Beale 1985; Garside and Leech 1985), which offers the following advantages:  a sound theoretical framework is provided  the approximations are clear  the probabilities provide a straightforward way to disambiguate  the probabilities can be estimated automatically from data." ></td>
	<td class="line x" title="19:179	In this paper we present a particular probabilistic model, the triclass model, and results from experiments involving different ways to estimate its parameters, with the intention of maximizing the ability of the model to tag text accurately." ></td>
	<td class="line x" title="20:179	In particular, we are interested in a way to make the best use of untagged text in the training of the model." ></td>
	<td class="line x" title="21:179	2." ></td>
	<td class="line x" title="22:179	The Problem of Tagging We suppose that the user has defined a set of tags (attached to words)." ></td>
	<td class="line x" title="23:179	Consider a sentence W = WlW2 w,, and a sequence of tags T -ht2 , t,, of the same length." ></td>
	<td class="line x" title="24:179	We call the pair (W, T) an alignment." ></td>
	<td class="line x" title="25:179	We say that word wi has been assigned the tag ti in this alignment." ></td>
	<td class="line x" title="26:179	We assume that the tags have some linguistic meaning for the user, so that among all possible alignments for a sentence there is a single one that is correct from a grammatical point of view." ></td>
	<td class="line x" title="27:179	A tagging procedure is a procedure ~ that selects a sequence of tags (and so defines an alignment) for each sentence." ></td>
	<td class="line x" title="28:179	~: W ~ T = ~(W) There are (at least) two measures for the quality of a tagging procedure:  at sentence level perfs(~) -percentage of sentences correctly tagged  at word level perfw(~) = percentage of words correctly tagged 156 Bernard Merialdo Tagging English Text with a Probabilistic Model In practice, performance at sentence level is generally lower than performance at word level, since all the words have to be tagged correctly for the sentence to be tagged correctly." ></td>
	<td class="line x" title="29:179	The standard measure used in the literature is performance at word level, and this is the one considered here." ></td>
	<td class="line x" title="30:179	3." ></td>
	<td class="line x" title="31:179	Probabilistic Formulation In the probabilistic formulation of the tagging problem we assume that the alignments are generated by a probabilistic model according to a probability distribution: p(W,T) In this case, depending on the criterion that we choose for evaluation, the optimal tagging procedure is as follows:  for evaluation at sentence level, choose the most probable sequence of tags for the sentence argmax argmax T p(T/W)= T p(W,T) We call this procedure Viterbi tagging." ></td>
	<td class="line x" title="32:179	It is achieved using a dynamic programming scheme." ></td>
	<td class="line x" title="33:179	for evaluation at word level, choose the most probable tag for each word in the sentence argmax argmax ~(W)i = t p(ti = t/W) = t ~ p(W, T) T:ti=t where ~(W)i is the tag assigned to word wi by the tagging procedure ~b in the context of the sentence W, We call this procedure Maximum Likelihood (ML) tagging." ></td>
	<td class="line x" title="34:179	It is interesting to note that the most commonly used method is Viterbi tagging (see DeRose 1988; Church 1989) although it is not the optimal method for evaluation at word level." ></td>
	<td class="line x" title="35:179	The reasons for this preference are presumably that:  Viterbi tagging is simpler to implement than ML tagging and requires less computation (although they both have the same asymptotic complexity)  Viterbi tagging provides the best interpretation for the sentence, which is linguistically appealing  ML tagging may produce sequences of tags that are linguistically impossible (because the choice of a tag depends on all contexts taken together)." ></td>
	<td class="line x" title="36:179	However, in our experiments, we will show that Viterbi and ML tagging result in very similar performance." ></td>
	<td class="line x" title="37:179	157 Computational Linguistics Volume 20, Number 2 Of course, the real tags have not been generated by a probabilistic model and, even if they had been, we would not be able to determine this model exactly because of practical limitations." ></td>
	<td class="line x" title="38:179	Therefore the models that we construct will only be approximations of an ideal model that does not exist." ></td>
	<td class="line x" title="39:179	It so happens that despite these assumptions and approximations, these models are still able to perform reasonably well." ></td>
	<td class="line x" title="40:179	4." ></td>
	<td class="line x" title="41:179	The Triclass Model We have the mathematical expression: H p(W~ T) = II P(Wi/Wltl." ></td>
	<td class="line x" title="42:179	wi-lti-lti).p(ti/wltl  . Wi-lti-1) i=1 The triclass (or tri-POS \[Derouault 1986\], or tri-Ggram \[Codogno et al. 1987\], or HK) model is based on the following approximations:  The probability of the tag given the past depends only on the last two tags p(ti/wltl . . ." ></td>
	<td class="line x" title="43:179	wi-lti-1) = h(ti/ti_ati_l)  The probability of the word given the past depends only on its tag p(wi/wltl . . ." ></td>
	<td class="line x" title="44:179	Wi-lti-lti) = k(wi/ti) (the name HK model comes from the notation chosen for these probabilities)." ></td>
	<td class="line x" title="45:179	In order to define the model completely we have to specify the values of all h and k probabilities." ></td>
	<td class="line x" title="46:179	If Nw is the size of the vocabulary and NT the number of different tags, then there are:  NT.NT.NT values for the h probabilities  Nw.NT values for the k probabilities." ></td>
	<td class="line x" title="47:179	Also, since all probability distributions have to sum to one, there are:  NT.NT equations to constrain the values for the h probabilities  NT equations to constrain the values for the k probabilities." ></td>
	<td class="line x" title="48:179	The total number of free parameters is then: (Nw 1).NT + (NT -1).NT.NT." ></td>
	<td class="line x" title="49:179	Note that this number grows only linearly with respect to the size of the vocabulary, which makes this model attractive for vocabularies of a very large size." ></td>
	<td class="line x" title="50:179	The triclass model by itself allows any word to have any tag." ></td>
	<td class="line x" title="51:179	However, if we have a dictionary that specifies the list of possible tags for each word, we can use this information to constrain the model: if t is not a valid tag for the word w, then we are sure that k(w/t) = O. There are thus at most as many nonzero values for the k probabilities as there are possible pairs (word, tag) allowed in the dictionary." ></td>
	<td class="line x" title="52:179	158 Bernard Merialdo Tagging English Text with a Probabilistic Model 5." ></td>
	<td class="line x" title="53:179	Training the Triclass Model We consider two different types of training:  Relative Frequency (RF) training  Maximum Likelihood (ML) training which is done via the Forward-Backward (FB) algorithm." ></td>
	<td class="line x" title="54:179	5.1 Relative Frequency Training If we have some tagged text available we can compute the number of times N(w, t) a given word w appears with the tag t, and the number of times N(h, t2~ t3) the sequence (tl~ t2~ t3) appears in this text." ></td>
	<td class="line x" title="55:179	We can then estimate the probabilities h and k by computing the relative frequencies of the corresponding events on this data: N(h, t2~ t3) hrf(tB/tl~ t2) =f(tg/tl, t2) N(tl, t2) N(w,t) krf(W/t)=f(w/t)N(t) These estimates assign a probability of zero to any sequence of tags that did not occur in the training data." ></td>
	<td class="line x" title="56:179	But such sequences may occur if we consider other texts." ></td>
	<td class="line x" title="57:179	A probability of zero for a sequence creates problems because any alignment that contains this sequence will get a probability of zero." ></td>
	<td class="line x" title="58:179	Therefore, it may happen that, for some sequences of words, all alignments get a probability of zero and the model becomes useless for such sentences." ></td>
	<td class="line x" title="59:179	To avoid this, we interpolate these distributions with uniform distributions, i.e. we consider the interpolated model defined by: where hinter(t3/tl~ t2) = ~.hrf(t3/tl~ t2) q(1 ),).hunid(t3/h, t2) kinter(W/t) =/~.krf(W/t) q(1 -/~).kunif(w/t ) 1 hunif(t3/tl, t2) = ~TT 1 ku,if(w/t) = number of words that have the tag t The interpolation coefficient,~ is computed using the deleted interpolation algorithm (Jelinek and Mercer 1980) (it would also be possible to use two coefficients, one for the interpolation on h, one for the interpolation on k)." ></td>
	<td class="line x" title="60:179	The value of this coefficient is expected to increase if we increase the size of the training text, since the relative frequencies should be more reliable." ></td>
	<td class="line x" title="61:179	This interpolation procedure is also called 'smoothing'." ></td>
	<td class="line x" title="62:179	Smoothing is performed as follows: Some quantity of tagged text from the training data is not used in the computation of the relative frequencies." ></td>
	<td class="line x" title="63:179	It is called the 'held-out' data." ></td>
	<td class="line x" title="64:179	The coefficient & is chosen to maximize the probability of emission of the held-out data by the interpolated model." ></td>
	<td class="line x" title="65:179	159 Computational Linguistics Volume 20, Number 2 This maximization can be performed by the standard Forward-Backward (FB) or Baum-Welch algorithm (Baum and Eagon 1967; Jelinek 1976; Bahl, Jelinek, and Mercer 1983; Poritz 1988), by considering ~ and 1 as the transition probabilities of a Markov model." ></td>
	<td class="line x" title="66:179	It can be noted that more complicated interpolation schemes are possible." ></td>
	<td class="line x" title="67:179	For example, different coefficients can be used depending on the count of (h, t2), with the intuition that relative frequencies can be trusted more when this count is high." ></td>
	<td class="line x" title="68:179	Another possibilitity is to interpolate also with models of different orders, such as hrf(t3/t2) or hrf(t3)." ></td>
	<td class="line x" title="69:179	Smoothing can also be achieved with procedures other than interpolation." ></td>
	<td class="line x" title="70:179	One example is the 'backing-off' strategy proposed by Katz (1987)." ></td>
	<td class="line x" title="71:179	5.2 Maximum Likelihood Training Using a triclass model M it is possible to compute the probability of any sequence of words W according to this model: = Zp (w, T) T where the sum is taken over all possible alignments." ></td>
	<td class="line x" title="72:179	The Maximum Likelihood (ML) training finds the model M that maximizes the probability of the training text: max II pM(W) M W where the product is taken over all the sentences W in the training text." ></td>
	<td class="line x" title="73:179	This is the problem of training a hidden Markov model (it is hidden because the sequence of tags is hidden)." ></td>
	<td class="line x" title="74:179	A well-known solution to this problem is the Forward-Backward (FB) or Baum-Welch algorithm (Baum and Eagon 1967; Jelinek 1976; Bahl, Jelinek, and Mercer 1983), which iteratively constructs a sequence of models that improve the probability of the training data." ></td>
	<td class="line x" title="75:179	The advantage of this approach is that it does not require any tagging of the text, but makes the assumption that the correct model is the one in which tags are used to best predict the word sequence." ></td>
	<td class="line x" title="76:179	6." ></td>
	<td class="line x" title="77:179	Tagging Algorithms The Viterbi algorithm is easily implemented using a dynamic programming scheme (Bellman 1957)." ></td>
	<td class="line x" title="78:179	The Maximum Likelihood algorithm appears more complex at first glance, because it involves computing the sum of the probabilities of a large number of alignments." ></td>
	<td class="line x" title="79:179	However, in the case of a hidden Markov model, these computations can be arranged in a way similar to the one used during the FB algorithm, so that the overall amount of computation needed becomes linear in the length of the sentence (Baum and Eagon 1967)." ></td>
	<td class="line x" title="80:179	7." ></td>
	<td class="line x" title="81:179	Experiments The main objective of this paper is to compare RF and ML training." ></td>
	<td class="line x" title="82:179	This is done in Section 7.2." ></td>
	<td class="line x" title="83:179	We also take advantage of the environment that we have set up to perform other experiments, described in Section 7.3, that have some theoretical interest, but did 160 Bernard Merialdo Tagging English Text with a Probabilistic Model Table 1 RF training on N sentences, Viterbi tagging." ></td>
	<td class="line x" title="84:179	Training data Interpolation Nb of errors % correct (sentences) coefficient )~ (words) tags 0 .0 10498 77.0 100 .48 4568 90.0 2000 .77 2110 95.4 5000 .85 1744 96.2 10000 .90 1555 96.6 20000 .92 1419 96.9 all .94 1365 97.0 not bring any improvement in practice." ></td>
	<td class="line x" title="85:179	One concerns the difference between Viterbi and ML tagging, and the other concerns the use of constraints during training." ></td>
	<td class="line x" title="86:179	We shall begin by describing the textual data that we are using, before presenting the different tagging experiments using these various training and tagging methods." ></td>
	<td class="line x" title="87:179	7.1 Text Data We use the 'treebank' data described in Beale (1988)." ></td>
	<td class="line x" title="88:179	It contains 42,186 sentences (about one million words) from the Associated Press." ></td>
	<td class="line x" title="89:179	These sentences have been tagged manually at the Unit for Computer Research on the English Language (University of Lancaster, U.K.), in collaboration with IBM U.K." ></td>
	<td class="line x" title="90:179	(Winchester) and the IBM Speech Recognition group in Yorktown Heights (USA)." ></td>
	<td class="line x" title="91:179	In fact, these sentences are not only tagged but also parsed." ></td>
	<td class="line x" title="92:179	However, we do not use the information contained in the parse." ></td>
	<td class="line x" title="93:179	In the treebank 159 different tags are used." ></td>
	<td class="line x" title="94:179	These tags were projected on a smaller system of 76 tags designed by Evelyne Tzoukermann and Peter Brown (see Appendix)." ></td>
	<td class="line x" title="95:179	The results quoted in this paper all refer to this smaller system." ></td>
	<td class="line x" title="96:179	We built a dictionary that indicates the list of possible tags for each word, by taking all the words that occur in this text and, for each word, all the tags that are assigned to it somewhere in the text." ></td>
	<td class="line x" title="97:179	In some sense, this is an optimal dictionary for this data, since a word will not have all its possible tags (in the language), but only the tags that it actually had within the text." ></td>
	<td class="line x" title="98:179	We separated this data into two parts: a set of 40,186 tagged sentences, the training data, which is used to build the models a set of 2,000 tagged sentences (45,583 words), the test data, which is used to test the quality of the models." ></td>
	<td class="line x" title="99:179	7.2 Basic Experiments RF training, Viterbi tagging In this experiment, we extracted N tagged sentences from the training data." ></td>
	<td class="line x" title="100:179	We then computed the relative frequencies on these sentences and built a 'smoothed' model using the procedure previously described." ></td>
	<td class="line x" title="101:179	This model was then used to tag the 2,000 test sentences." ></td>
	<td class="line x" title="102:179	We experimented with different values of N, for each of which we indicate the value of the interpolation coefficient and the number and percentage of correctly tagged words." ></td>
	<td class="line x" title="103:179	Results are indicated in Table 1." ></td>
	<td class="line x" title="104:179	161 Computational Linguistics Volume 20, Number 2 As expected, as the size of the training increases, the interpolation coefficient increases and the quality of the tagging improves." ></td>
	<td class="line x" title="105:179	When N = 0, the model is made up of uniform distributions." ></td>
	<td class="line x" title="106:179	In this case, all alignments for a sentence are equally probable, so that the choice of the correct tag is just a choice at random." ></td>
	<td class="line x" title="107:179	However, the percentage of correct tags is relatively high (more than three out of four) because:  almost half of the words of the text have a single possible tag, so that no mistake can be made on these words  about a quarter of the words of the text have only two possible tags so that, on the average, a random choice is correct every other time." ></td>
	<td class="line x" title="108:179	Note that this behavior is obviously very dependent on the system of tags that is used." ></td>
	<td class="line x" title="109:179	It can be noted that reasonable results are obtained quite rapidly." ></td>
	<td class="line x" title="110:179	Using 2,000 tagged sentences (less than 50,000 words), the tagging error rate is already less than 5%." ></td>
	<td class="line x" title="111:179	Using 10 times as much data (20,000 tagged sentences) provides an improvement of only 1.5%." ></td>
	<td class="line x" title="112:179	ML training, Viterbi tagging In ML training we take all the training data available (40,186 sentences) but we only use the word sequences, not the associated tags (except to compute the initial model, as will be described later)." ></td>
	<td class="line x" title="113:179	This is possible since the FB algorithm is able to train the model using the word sequence only." ></td>
	<td class="line x" title="114:179	In the first experiment we took the model made up of uniform distributions as the initial one." ></td>
	<td class="line x" title="115:179	The only constraints in this model came from the values k(w/t) that were set to zero when the tag t was not possible for the word w (as found in the dictionary)." ></td>
	<td class="line x" title="116:179	We then ran the FB algorithm and evaluated the quality of the tagging." ></td>
	<td class="line x" title="117:179	The results are shown in Figure 1." ></td>
	<td class="line x" title="118:179	(Perplexity is a measure of the average branching factor for probabilistic models)." ></td>
	<td class="line x" title="119:179	This figure shows that ML training both improves the perplexity of the model and reduces the tagging error rate." ></td>
	<td class="line x" title="120:179	However, this error rate remains at a relatively high level--higher than that obtained with a RF training on 100 tagged sentences." ></td>
	<td class="line x" title="121:179	Having shown that ML training is able to improve the uniform model, we then wanted to know if it was also able to improve more accurate models." ></td>
	<td class="line x" title="122:179	We therefore took as the initial model each of the models obtained previously by RF training and, for each one, performed ML training using all of the training word sequences." ></td>
	<td class="line x" title="123:179	The results are shown graphically in Figure 2 and numerically in Table 2." ></td>
	<td class="line x" title="124:179	These results show that, when we use few tagged data, the model obtained by relative frequency is not very good and Maximum Likelihood training is able to improve it." ></td>
	<td class="line x" title="125:179	However, as the amount of tagged data increases, the models obtained by Relative Frequency are more accurate and Maximum Likelihood training improves on the initial iterations only, but after deteriorates." ></td>
	<td class="line x" title="126:179	If we use more than 5,000 tagged sentences, even the first iteration of ML training degrades the tagging." ></td>
	<td class="line x" title="127:179	(This number is of course dependent on both the particular system of tags and the kind of text used in this experiment)." ></td>
	<td class="line x" title="128:179	These results call for some comments." ></td>
	<td class="line x" title="129:179	ML training is a theoretically sound procedure, and one that is routinely and successfully used in speech recognition to estimate the parameters of hidden Markov models that describe the relations between sequences of phonemes and the speech signal." ></td>
	<td class="line x" title="130:179	Although ML training is guaranteed to improve perplexity, perplexity is not necessarily related to tagging accuracy, and it is possible to improve one while degrading the other." ></td>
	<td class="line x" title="131:179	Also, in the case of tagging, 162 Bernard Merialdo Tagging English Text with a Probabilistic Model 24 22 20 18 16 14 12 i0 0 6O0 580 56O 540 52O 5OO 480 460 440 I I I I 5 10 15 20 o o o .~ : I I 315 I 415 25 30 40 Iterations i, ii0 I t t S 15 20 25 Iterations 0 0 t I 30 35 4O 45 50 Figure 1 ML training from uniform distributions." ></td>
	<td class="line x" title="132:179	Table 2 ML training from various initial points." ></td>
	<td class="line x" title="133:179	Number of tagged sentences used for the initial model 0 100 2000 5000 10000 20000 all Iter Correct tags (% words) after ML on 1M words 0 77.0 90.0 95.4 96.2 96.6 96.9 97.0 1 80.5 92.6 95.8 96.3 96.6 96.7 96.8 2 81.8 93.0 95.7 96.1 96.3 96.4 96.4 3 83.0 93.1 95.4 95.8 96.1 96.2 96.2 4 84.0 93.0 95.2 95.5 95.8 96.0 96.0 5 84.8 92.9 95.1 95.4 95.6 95.8 95.8 6 85.3 92.8 94.9 95.2 95.5 95.6 95.7 7 85.8 92.8 94.7 95.1 95.3 95.5 95.5 8 86.1 92.7 94.6 95.0 95.2 95.4 95.4 9 86.3 92.6 94.5 94.9 95.1 95.3 95.3 10 86.6 92.6 94.4 94.8 95.0 95.2 95.2 163 Computational Linguistics Volume 20, Number 2 12 10' I I I I A  ~  -~  y.T.~_T  ~.-.-----'~.~:':::-:-.-~:z:-:-:x e  '' '' x  ~,':2C'?'.-:-:~_-.-zZ-'.'~:,~-':m'-'Z--m-'-- I I I I 2 4 6 8 i0 Iterations Figure 2 ML training from various initial points (top line corresponds to N=IO0, bottom line to N=all)." ></td>
	<td class="line x" title="134:179	the relations between words and tags are much more precise than the relations between phonemes and speech signals (where the correct correspondence is harder to define precisely)." ></td>
	<td class="line x" title="135:179	Some characteristics of ML training, such as the effect of smoothing probabilities, are probably more suited to speech than to tagging." ></td>
	<td class="line x" title="136:179	7.3 Extra Experiments Viterbi versus ML tagging For this experiment we considered the initial model built by RF training over the whole training data and all the successive models created by the iterations of ML training." ></td>
	<td class="line x" title="137:179	For each of these models we performed Viterbi tagging and ML tagging on the same test data, then evaluated and compared the number of tagging errors produced by these two methods." ></td>
	<td class="line x" title="138:179	The results are shown in Table 3." ></td>
	<td class="line x" title="139:179	The models obtained at different iterations are related, so one should not draw strong conclusions about the definite superiority of one tagging procedure." ></td>
	<td class="line x" title="140:179	However, the difference in error rate is very small, and shows that the choice of the tagging procedure is not as critical as the kind of training material." ></td>
	<td class="line x" title="141:179	Constrained ML training Following a suggestion made by E Jelinek, we investigated the effect of constraining the ML training by imposing constraints on the probabilities." ></td>
	<td class="line x" title="142:179	This idea comes from the observation that the amount of training data needed to properly estimate the model increases with the number of free parameters of the model." ></td>
	<td class="line x" title="143:179	In the case of little training data, adding reasonable constraints on the shape of the models that are looked for reduces the number of free parameters and should improve the quality of the estimates." ></td>
	<td class="line x" title="144:179	164 Bernard Merialdo Tagging English Text with a Probabilistic Model Table 3 Viterbi vs. ML tagging." ></td>
	<td class="line x" title="145:179	Tagging errors out of 45,583 words Iter." ></td>
	<td class="line x" title="146:179	Viterbi ML Vit." ></td>
	<td class="line x" title="147:179	ML 0 % nb % nb nb 0 97.01 1365 97.01 1362 3 1 96.76 1477 96.75 1480 3 2 96.44 1623 96.47 1607 16 3 96.23 1718 96.23 1719 1 4 96.00 1824 96.02 1812 12 5 95.82 1906 95.85 1892 14 6 95.66 1978 95.68 1970 8 7 95.51 2046 95.54 2031 15 8 95.39 2100 95.42 2087 13 9 95.30 2144 95.31 2140 4 10 95.21 2183 95.22 2177 6 Table 4 Standard ML vs. tw-constrained ML training." ></td>
	<td class="line x" title="148:179	Tagging errors out of 45,583 words Iter." ></td>
	<td class="line x" title="149:179	ML tw-c." ></td>
	<td class="line x" title="150:179	ML 0 % nb % nb 0 97.01 1365 97.01 1365 1 96.76 1477 96.87 1427 2 96.44 1623 96.71 1501 3 96.23 1718 96.57 1562 4 96.00 1824 96.43 1626 5 95.82 1906 96.36 1661 6 95.66 1978 96.29 1690 7 95.51 2046 96.22 1723 8 95.39 2100 96.18 1741 9 95.30 2144 96.12 1768 10 95.21 2183 96.09 1784 We tried two different constraints:  The first one keeps p(t/w) fixed if w is a frequent word, in our case one of the 1,000 most frequent words." ></td>
	<td class="line x" title="151:179	We call it tw-constraint." ></td>
	<td class="line x" title="152:179	The rationale is that if w is frequent, the relative frequency provides a good estimate for p(t/w) and the training should not change it." ></td>
	<td class="line x" title="153:179	 The second one keeps the marginal distribution p(t) constant and is based on a similar reasoning." ></td>
	<td class="line x" title="154:179	We call it t-constraint." ></td>
	<td class="line x" title="155:179	tw-constraint The tw-constrained ML training is similar to the standard ML training, except that the probabilities p(t/w) are not changed at the end of an iteration." ></td>
	<td class="line x" title="156:179	The results in Table 4 show the number of tagging errors when the model is trained with the standard or tw-constrained ML training." ></td>
	<td class="line x" title="157:179	They show that the tw-constrained ML training still degrades the RF training, but not as quickly as the standard ML." ></td>
	<td class="line x" title="158:179	We 165 Computational Linguistics Volume 20, Number 2 Table 5 Standard ML vs. constrained ML training." ></td>
	<td class="line x" title="159:179	Tagging errors out of 45,583 words (biclass model) Iter." ></td>
	<td class="line x" title="160:179	ML t-c." ></td>
	<td class="line x" title="161:179	ML 0 % nb % nb 0 96.87 1429 96.87 1429 1 96.51 1592 96.54 1576 2 96.18 1743 96.23 1718 3 96.00 1824 96.03 1810 4 95.84 1896 95.90 1871 5 95.67 1972 95.77 1928 6 95.52 2044 95.59 2009 7 95.42 2087 95.50 2051 8 95.33 2129 95.42 2087 9 95.24 2171 95.34 2126 10 95.18 2196 95.30 2141 have not tested what happens when smaller training data is used to build the initial model." ></td>
	<td class="line x" title="162:179	t-constraint This constraint is more difficult to implement than the previous one because the probabilities p(t) are not the parameters of the model, but a combination of these parameters." ></td>
	<td class="line x" title="163:179	With the help of R. Polyak we have designed an iterative procedure that allows the likelihood to be improved while preserving the values of p(t)." ></td>
	<td class="line x" title="164:179	We do not have sufficient space to describe this procedure here." ></td>
	<td class="line x" title="165:179	Because of its greater computational complexity, we have only applied it to a biclass model, i.e. a model where p(ti/wltl . . ." ></td>
	<td class="line x" title="166:179	Wi-lti-1) = h(ti/ti-1)." ></td>
	<td class="line x" title="167:179	The initial model is estimated by relative frequency on the whole training data and Viterbi tagging is used." ></td>
	<td class="line x" title="168:179	As in the previous experiment, the results in Table 5 show the number of tagging errors when the model is trained with the standard or t-constrained ML training." ></td>
	<td class="line x" title="169:179	They show that the t-constrained ML training still degrades the RF training, but not as quickly as the standard ML." ></td>
	<td class="line x" title="170:179	Again, we have not tested what happens when smaller training data is used to build the initial model." ></td>
	<td class="line x" title="171:179	8." ></td>
	<td class="line x" title="172:179	Conclusion The results presented in this paper show that estimating the parameters of the model by counting relative frequencies over a very large amount of hand-tagged text lead to the best tagging accuracy." ></td>
	<td class="line x" title="173:179	Maximum Likelihood training is guaranteed to improve perplexity, but will not necessarily improve tagging accuracy." ></td>
	<td class="line x" title="174:179	In our experiments, ML training degrades the performance unless the initial model is already very bad." ></td>
	<td class="line x" title="175:179	The preceding results suggest that the optimal strategy to build the best possible model for tagging is the following:  get as much tagged (by hand) text as you can afford 166 Bernard Merialdo Tagging English Text with a Probabilistic Model compute the relative frequencies from this data to build an initial model M0 get as much untagged text as you can afford starting from M0, perform the Forward-Backward iterations." ></td>
	<td class="line x" title="176:179	At each iteration, evaluate the tagging quality of the new model Mi on some held-out tagged text." ></td>
	<td class="line x" title="177:179	Stop either when you have reached a preset number of iterations or the model Mi performs worse than Mi-1, whichever occurs first." ></td>
	<td class="line x" title="178:179	Acknowledgments I would like to thank Peter Brown, Fred Jelinek, John Lafferty, Robert Mercer, Salim Roukos, and other members of the Continuous Speech Recognition group for the fruitful discussions I had with them throughout this work." ></td>
	<td class="line x" title="179:179	I also want to thank one of the referees for his judicious comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W94-0111
Exploring The Statistical Derivation Of Transformational Rule Sequences For Part-Of-Speech Tagging
Ramshaw, Lance A.;Marcus, Mitchell P.;"></td>
	<td class="line x" title="1:188	Exploring the Statistical Derivation of Transformational Rule Sequences for Part-of-Speech Tagging Lance A. Ramshaw Inst." ></td>
	<td class="line x" title="2:188	for Research in Cognitive Science University of Pennsylvania 3401 Walnut Street #412-C Philadelphia, PA 19104-6228 USA ramshaw@linc, cis." ></td>
	<td class="line x" title="3:188	upenn, edu and Dept. of Computer Science Bowdoin College Brunswick, ME 04011 USA Mitchell P. Marcus Computer and Information Science Dept. University of Pennsylvania 558 Moore Building Philadelphia, PA 19104-6389 USA mit oh@line, cis." ></td>
	<td class="line x" title="4:188	upenn, edu Introduction Eric Brill in his recent thesis (1993b) proposed an approach called 'transformation-based error-driven learning' that can statistically derive linguistic models from corpora, and he has applied the approach in various domains including part-of-speech tagging (Brill, 1992; Brill, 1994) and building phrase structure trees (Brill, 1993a)." ></td>
	<td class="line x" title="5:188	The method learns a sequence of symbolic rules that characterize important contextual factors and use them to predict a most likely value." ></td>
	<td class="line x" title="6:188	The search for such factors only requires counting various sets of events that actually occur in a training corpus, and the method is thus able to survey a larger space of possible contextual factors than could be practically captured by a statistical model that required explicit probability estimates for every possible combination of factors." ></td>
	<td class="line x" title="7:188	Brill's results on part-of-speech tagging show that the method can outperform the HMM techniques widely used for that task, while also providing more compact and perspicuo.s models." ></td>
	<td class="line x" title="8:188	Decision trees are an established learning technique that is also based on surveying a wide space of possible factors and repeatedly selecting a most significant factor or combination of factors." ></td>
	<td class="line x" title="9:188	After briefly describing Brill's approach and noting a fast implementation of it, this paper analyzes it in relation to decision trees." ></td>
	<td class="line x" title="10:188	The contrast highlights the kinds of applications to which rule sequence learning is especially suited." ></td>
	<td class="line x" title="11:188	We point out how it, ma.ages to largely avoid difficulties with overtraining, and show a way of recording the dependencies bt.tween rules in the learned sequence." ></td>
	<td class="line x" title="12:188	The analysis throughout is based on part-of-speech tagging experiments using the tagged Brown Corpus (Francis and K.eera, 1979) and a tagged Septuagint Greek version of the first five books of the Bible (CATSS, 1991)." ></td>
	<td class="line x" title="13:188	Brill's Approach This learning approach starts with a supervised training corpus and a baseline heuristic for assigning initial values." ></td>
	<td class="line x" title="14:188	In the part-of-speech tagging application, for example, the baseline heuristic might be to assign each known but ambiguous word whatever tag is most often correct for it in the training corpus, and to assign all unknown words an initial tag as nouns." ></td>
	<td class="line x" title="15:188	(Brill's results point out that performance on unknown words is a crucial factor for part-of-speech tagging systems." ></td>
	<td class="line x" title="16:188	His system is organized in two separate rule sequence training passes, with an important purpose of the first pass being exactly to predict the part-of-speech of unknown words." ></td>
	<td class="line x" title="17:188	However, because the focus in these experiments is on understanding the mechanism, rather than on comparative performance, the simple but unrealistic assumption of a closed vocabulary is made)." ></td>
	<td class="line x" title="18:188	The learner then works from those baseline tag assignments using a set of templates that define classes of transformational rules, where each rule changes some assigned values based on characteristics of the neighborhood." ></td>
	<td class="line x" title="19:188	Again, for tagging, the rule templates typically involve either the actual words or the tags currently assigned to words within a few positions on each side of the value to be changed." ></td>
	<td class="line x" title="20:188	The rule templates used in these experiments involve up to two of the currentlyassigned tags on each side of the tag being changed; they include \[ -C A/B \] (change tag A to tag B if the previous tag is C) and \[ --A/B C D \] (change A to B if the following two tags are C and D)." ></td>
	<td class="line x" title="21:188	During training, instantiated rules like \[ -DET V/N --\] are built by matching these templates against the training corpus." ></td>
	<td class="line x" title="22:188	A set of such templates combined with the given partof-speech tagset (and vocabulary, if the rule patterns 86 also refer directly to the words) defines a large space of possible rules, and the training process operates by us." ></td>
	<td class="line x" title="23:188	ing some ranking function to select at each step a rule judged likely to improve the current tag assignment." ></td>
	<td class="line x" title="24:188	Brill suggests the simple ranking function of choosing (one of) the rule(s) that makes the largest net improvement in the current training set tag assignments." ></td>
	<td class="line x" title="25:188	Note that applying a rule at a location can have a positive effect (changing the current tag assignment from incorrect to correct), a negative one (from correct to some incorrect value), or can he a neutral move (from one incorrect tag to another)." ></td>
	<td class="line x" title="26:188	Rules with the largest positive minus negative score cause the largest net benefit." ></td>
	<td class="line x" title="27:188	In each training cycle, one such rule is selected and applied to the training corpus and then the scoring and selection process is repeated on the newly-transformed corpus." ></td>
	<td class="line x" title="28:188	This process is continued either until no beneficial rule can be found, or until the degree of improvement becomes less than some threshold." ></td>
	<td class="line x" title="29:188	The scoring process is tractable in spite of the huge space of possible rules because rules that never apply positively can be ignored." ></td>
	<td class="line x" title="30:188	The final model is thus an ordered sequence of pattern-action rules." ></td>
	<td class="line x" title="31:188	It is used for prediction on a test corpus by beginning with the predictions of the baseline heuristic and then applying the transformational rules in order." ></td>
	<td class="line x" title="32:188	In our test runs, seven templates were used, three templates testing the tags of the immediate, next, and both neighbors to the left, three similar templates looking to the right, and a seventh template that tests the tags of the immediate left and right neighbors." ></td>
	<td class="line x" title="33:188	The first ten rules learned from a training run across a 50Kword sample of the Brown Corpus are listed in Fig." ></td>
	<td class="line x" title="34:188	1; they closely replicate Brill's results (1993b, page 96), allowing for the fact that his tests used more templates, including templates like 'if one of the three previous tags is A'." ></td>
	<td class="line nc" title="35:188	Brill's results demonstrate that this approach can outperform the Hidden Markov Model approaches that are frequently used for part-of-speech tagging (Jelinek, 1985; Church, 1988; DeRose, 1988; Cutting et al. , 1992; Weischedel et al. , 1993), as well as showing promise for other applications." ></td>
	<td class="line x" title="36:188	The resulting model, encoded as a list of rules, is also typically more compact and for some purposes more easily interpretable than a table of HMM probabilities." ></td>
	<td class="line x" title="37:188	An Incremental Algorithm It is worthwhile noting first that it is possible in some circumstances to significantly speed up the straightforward algorithm described above." ></td>
	<td class="line x" title="38:188	An improvement in our experiments of almost two orders of magnitude (from four days to under an hour) was achieved by using an incremental approach that maintains lists of pointers to link rules with the sites in the training corpus where they apply, rather than scanning the corpus from scratch each time." ></td>
	<td class="line x" title="39:188	The improvement is particularly noticeable in the later stages of training, when the rules being learned typically affect only one or two sites isl the training corpus." ></td>
	<td class="line x" title="40:188	Note, however, that the linked lists ill this incremental approach require a significant amount of storage space." ></td>
	<td class="line x" title="41:188	Depending on the number of possible rules generated by a particular combination of rul,." ></td>
	<td class="line x" title="42:188	templates and training corpus, space constraints may not permit this optimization." ></td>
	<td class="line x" title="43:188	Incrementalizing the algorithm requires maintaining a list for each rule generated of those sites in the corpus where it applies, and a list for each site of the rules that apply there." ></td>
	<td class="line x" title="44:188	Once one of the highest-scoring rules is selected, its list of site pointers is first used to make the appropriate changes in the current tag values in the corpus." ></td>
	<td class="line x" title="45:188	After making the changes, that list is used again in order to update other rule pointers that may have been affected by them." ></td>
	<td class="line x" title="46:188	It suffices to check each site within the span of the largest defined rule template from each changed site, testing to see whether all of its old rule links are still active, and whether any new rules now apply at that site." ></td>
	<td class="line x" title="47:188	Our current algorithm is shown in Fig." ></td>
	<td class="line x" title="48:188	2." ></td>
	<td class="line x" title="49:188	Note that, after the initial setup, it is necessary to rescan the corpus only when updating uncovers a rule that has not previously had any positive effect." ></td>
	<td class="line x" title="50:188	Rule Sequences and Decision Trees To understand the success of Brill's new method, it is useful to compare it with the decision tree approach (Breiman et al. , 1984; Quinlan, 1993), which is an established method for inducing compact and interpretable models." ></td>
	<td class="line x" title="51:188	The key difference is that decision trees are applied to a population of non-interacting problems that are solved independently, while rule sequence learning is applied to a sequence of interrelated problems that are solved in parallel, by applying rules to the entire corpus." ></td>
	<td class="line x" title="52:188	The following sections discuss how this parallel approach allows leveraging of partial solutions between neighboring instances, but also requires that the rules themselves be largely independent." ></td>
	<td class="line x" title="53:188	While decision trees can synthesize complex rules from simple tests, rule sequence learning requires those combinations to be built into the templates." ></td>
	<td class="line x" title="54:188	Leveraged Learning Decision trees are traditionally applied to independent problem instances encoded as vectors of measurements for the various possibly-relevant factors." ></td>
	<td class="line x" title="55:188	In predicting the part of speech of a word in a corpus, such." ></td>
	<td class="line x" title="56:188	factors would include the identities of the neighboring words within some window." ></td>
	<td class="line x" title="57:188	However, it would also be useful to know the currently predicted tags for those words, since the tag-assignment problems for neighboring words in a corpus are not independent." ></td>
	<td class="line x" title="58:188	The rule sequence learning technique is particularly well adapted to a corpus that is inherently a sequence of such interrelated problem instances." ></td>
	<td class="line x" title="59:188	Because the rule patterns in a part-of-speech system do depend in part on tile unknown part-of-speech values at neighboring locations, 87 Pass 1." ></td>
	<td class="line x" title="60:188	2. 3." ></td>
	<td class="line x" title="61:188	4. 5." ></td>
	<td class="line x" title="62:188	6. 7." ></td>
	<td class="line x" title="63:188	8. 9." ></td>
	<td class="line x" title="64:188	10." ></td>
	<td class="line x" title="65:188	Rule Pos." ></td>
	<td class="line x" title="66:188	Ne 9." ></td>
	<td class="line x" title="67:188	Neu|." ></td>
	<td class="line x" title="68:188	m __ TO/IN AT u 227 0 0 TO NN/VB -u 113 13 0 -TO/IN NN -49 0 0 -IN PPS/PPO -51 4 0 --TO/IN NP ~ 46 0 0 ~ TO/IN PP$ -46 1 0 m CS/DT NN -52 Ii l HVD VBD/VBN ~ -38 0 0 ~ CS/QL ~ CS 41 7 0 MD NN/VB m ~ 32 0 0 Figure 1: First 10 Rules Learned on Brown Corpus Sample //Records for locations in the corpus, called 'sites', //include a linked list of the rules that apply at that site." ></td>
	<td class="line x" title="69:188	//Records for rules include score components (positive, negative, and neutral) //and a linked list of the sites at which the rule applies." ></td>
	<td class="line x" title="70:188	//A hash table stores all rules that apply positively anywhere in the training." ></td>
	<td class="line x" title="71:188	scan corpus using templates, making hash table entries for positive rules scan corpus again to identify negative and neutral sites for those rules loop high_rule := some rule with maximum score if high_rule.score <= 0 then exit loop output rule trace for each change_site on high.xule.site_list do apply high_rule at change_site by changing current tag unseen_rules := 0 for each change_site on high_rule.site.list do for each test_site in the neighborhood of change_site do new_rules_list := NIL for each template do if template applies at test_site then add resulting rule to new_rules.list for each rule in test_site.rules.list new_rules_list do remove connection between rule and test_site for each rule in new_rules_list test_site.rules_list do if rule in hash table then make new connection between rule and test_site else unseen_rules := unseen.rules O {rule} if unseen_rules # 0 then add unseen_rules to hash table for each site in corpus do for each rule in unseen_rules do if rule applies at site then make connection between rule and site adjust appropriate rule score (positive, negative, or neutral) end loop Figure 2: Incremental Version of Rule Sequence Learning Algorithm 88 it seems useful to allow those patterns to be based at each point on the system's best current guess for those values." ></td>
	<td class="line x" title="72:188	It is difficult to take account of that kind of dependence in a traditional decision tree, since changes in neighboring tag predictions can force the recomputation of predicate splits higher in the tree." ></td>
	<td class="line x" title="73:188	Breaking the tag prediction process up into a series of rules that  can each be applied immediately to the entire corpus is a simple scheme that allows for that kind of leverage." ></td>
	<td class="line x" title="74:188	Much as when a bank compounds interest, this allows the system to base its future learning on the improved estimates of neighborhood tags resulting from the operation of earlier rules." ></td>
	<td class="line x" title="75:188	A non-leveraged learner would have to build rules or trees based only on the unchanging features of the neighboring words and perhaps the baseline guesses of their tags." ></td>
	<td class="line x" title="76:188	In effect, such a learner would be forced to try to resolve the ambiguity at the neighboring location as part of the rule for the primary site, using as evidence only cases where the two occur together." ></td>
	<td class="line x" title="77:188	The leveraging approach allows the system to factor the best current guess for the neighboring site in terms of all the evidence into the choice for the primary site." ></td>
	<td class="line x" title="78:188	It is to allow for leveraging that the model is formulated as a sequence of individual rules." ></td>
	<td class="line x" title="79:188	Largely Independent Rules This breaking up of the rule sequence model into largely independent rules also results in another important difference between rule sequence learning and decision trees." ></td>
	<td class="line x" title="80:188	In the building of a decision tree, an elementary predicate is selected at each step to split a single leaf node, meaning that it is applied only to those training instances associated with that particular branch of the tree." ></td>
	<td class="line x" title="81:188	The two new leaves thus created effectively represent two new classification rules, each one selecting exactly the instances that classify to it, and thus each including all of the predicates inherited down that branch of the tree." ></td>
	<td class="line x" title="82:188	In the rule sequence method, on the other hand, the rules are generated from the templates as they are applied to the whole corpus in a largely independent manner; there is no corresponding inheritance of earlier predicates down the branches of a tree." ></td>
	<td class="line x" title="83:188	Note that one could simulate the decision tree style in a sequence learner by adding to the pattern for each rule template a variable-length field that records the complete history of rules which have affected that location." ></td>
	<td class="line x" title="84:188	Then, as in a decision tree, a rule generated at one site in the training set would be scored only against sites whose previous rule history exactly matched its own." ></td>
	<td class="line x" title="85:188	But rule sequence learning as defined here is not sensitive in that way to the previous rule history." ></td>
	<td class="line x" title="86:188	The 'largely independent' rules in the sequence would be fully independent if the system were not doing leveraging; if all rule patterns were tested each time against the original baseline tag predictions, then there would be no way for earlier rules to affect later ones in the sequence." ></td>
	<td class="line x" title="87:188	Leveraging does make later rules dependent on the results of earlier ones, but it does so to a strictly limited degree, which is generally much weaker than the direct inheritance of rules down decision tree branches." ></td>
	<td class="line x" title="88:188	To see the limitation, suppose that templates could test the current tag of the word to be changed, but could only consult the baseline tags for the rest of the pattern." ></td>
	<td class="line x" title="89:188	Earlier rule firings could then affect what rules might later apply at a particular location only by changing the current tag assignment for that location itself to one of the other possible tag values." ></td>
	<td class="line x" title="90:188	Each rule firing would make potentially applicable at the locations affected all rules whose central pattern element specify that new tag value, while disabling those rules whose patterns specify the old value." ></td>
	<td class="line x" title="91:188	The training set at any time during training would thus in effect be partitioned for purposes of rule application into at most as many classes as there are tags." ></td>
	<td class="line x" title="92:188	Such a system can be pictured as a lattice with one column for each tag assignment and with a single slanting arc at each generation that moves some corpus locations from one column to another." ></td>
	<td class="line x" title="93:188	While a decision tree path can encode an arbitrary amount of information in its branching, this system is forced to merge as often as it branches, which requires the rules to be more independent." ></td>
	<td class="line x" title="94:188	Furthermore, the system's ability to use even the available partitioning in order to construct dependent rule sequences is further limited, since tag changes are only made when some subset of the data is identified for which the new tag is more representative of the training corpus; tile learner is not free to use tag assignments to encode arbitrary rule dependencies." ></td>
	<td class="line x" title="95:188	Even in the actual system, where the leveraging can include changes in the neighborhood as well as at the location itself, the rule sequence mechanism still appears to have much less power to create complex combined rules than do decision trees." ></td>
	<td class="line x" title="96:188	Because rule sequence learners are more limited in terms of the connections between rules that they can construct during training, they must begin with more complex predicates built into their rule templates." ></td>
	<td class="line x" title="97:188	If the templates in a rule sequence run are not strong enough to distinguish the important patterns in tile data, performance will naturally suffer." ></td>
	<td class="line x" title="98:188	But if the rule templates that are likely to be useful can be predicted ill advance, the rule sequence approach can benefit both from leveraging and, as shown later, from decreased fragmentation of the training set." ></td>
	<td class="line x" title="99:188	Scoring Metrics This difference in organization between rule sequence learning and decision trees carries through naturally to the scoring methods used to select the next rule to apply." ></td>
	<td class="line x" title="100:188	Decision trees often select the split which most reduces either a diversity index or some measure based on the conditional entropy of the truth given the tree's predictions (Breiman et al. , 1984; Quinlan and Rivest, 1989; Quinlan, 1993)." ></td>
	<td class="line x" title="101:188	Note that these metrics may select a split that does not change the score of the current 89 predictions against the truth, for instance by splitting a node in such a way that both children still have the same plurality class as the parent." ></td>
	<td class="line x" title="102:188	Such a split may still make sense in entropy terms if the distributions of the other tags in the two new nodes are substantially different, thus suggesting that later rules will have an easier time isolating particular tag values." ></td>
	<td class="line x" title="103:188	In a rule sequence learner, however, there is less likely to be any advantage to such a split, since the instances whose tags are changed by that rule will then be mixed with others that were already assigned the new tag for other reasons." ></td>
	<td class="line x" title="104:188	The net benefit metric that is actually used in rule sequence learning is equivalent in decision tree terms to using the resubstitution estimate of the misclassificatiou rate." ></td>
	<td class="line x" title="105:188	While that metric is not ideal for decision trees, it appears to work well for rule sequence learning, where the mechanism is strictly limited in terms of the connections between rules that it can construct." ></td>
	<td class="line x" title="106:188	Overtraining It is particularly interesting to compare rule sequences with decision trees in terms of the risk of overtraining (or 'overfitting')." ></td>
	<td class="line x" title="107:188	One of the intriguing features of rule sequence learning is its apparent resistance to overtrai,ing." ></td>
	<td class="line x" title="108:188	For example, Fig." ></td>
	<td class="line x" title="109:188	3 shows the graph of percent correct on both training set (solid line) and test set (dotted line) as a function of the number of rules applied for a typical part-of-speech training run on 120K words of Greek text." ></td>
	<td class="line x" title="110:188	The training set performance naturally improves monotonically, given the nature of the algoriti~m, but the surprising feature of that graph is that the test set performance also improves monotonically, except for minor noise, and this seems to be true for the great majority of our rule sequence training runs." ></td>
	<td class="line x" title="111:188	This is in marked contrast to similar graphs for decision trees or neural net classifiers or for the iterative EM training of HMM tuggers on unsupervised data, where performance on the test set initially improves, but later significantly degrades." ></td>
	<td class="line x" title="112:188	Experiments suggest that part of the difference is due to knowledge embodied in the templates." ></td>
	<td class="line x" title="113:188	When a partof-speech training run is supplied with relevant templates, as in Fig." ></td>
	<td class="line x" title="114:188	3, one gets an 'improve to plateau' test-set curve." ></td>
	<td class="line x" title="115:188	Irrelevant templates, however, can lead to overtraining." ></td>
	<td class="line x" title="116:188	Fig." ></td>
	<td class="line x" title="117:188	4 shows that noticeable overtraining results from using just a single irrelevant template, in this case, one that tested the tags of the words five positions to the left and right, which seem likely to be largely uncorrelated with the tag at the central location." ></td>
	<td class="line x" title="118:188	l'ig." ></td>
	<td class="line x" title="119:188	5, where the single irrelevant template is combia,ed with the seven normal templates, shows that in such cases, most of the overtraining happens late in the training process, when most of the useful relevant templates have already been applied." ></td>
	<td class="line x" title="120:188	At that stage, as always, the templates are applied to each remaini,g incorrectly-tagged site, generating candidate rules." ></td>
	<td class="line x" title="121:188	I,~ach r,h, imturally succeeds at the site that proposed it, h,t most are now effectively random changes, which are thus likely to do more harm than good when tried elsewhere, especially since most of the assigned tags at this stage are correct." ></td>
	<td class="line x" title="122:188	Thus if the rule's pattern matches elsewhere in the training set, it is quite likely that the change there will be negative, so that the unhelpful rule will not be learned." ></td>
	<td class="line x" title="123:188	Thus the presence of relevant templates supplies an important degree of protection against overtraining from any irrelevant templates, both by reducing the number of incorrect sites that are left late in training and by raising the percentage already correct, which makes it more likely that bad rules will be filtered out." ></td>
	<td class="line x" title="124:188	The same applies, of course, to relevant and irrelevant instances of mixed templates, which is the usual case." ></td>
	<td class="line x" title="125:188	Most of the overtraining will thus come from patterns that match only once in the training set (to their generating instance)." ></td>
	<td class="line x" title="126:188	Under these assumptions, note that applying a score threshold > 1 can significantly reduce the overtraining risk, just as decision trees sometimes control that risk by applying a threshold to the entropy gain required before splitting a node." ></td>
	<td class="line x" title="127:188	Brill's system uses a score threshold of 2 as the default, thus gaining additional protection against overtraining, while our experimental runs have been exhaustive, in order to better understand the mechanism." ></td>
	<td class="line x" title="128:188	Using test runs like those plotted above for irrelevant templates of various degrees of complexity, we also found a connection in terms of overtraining risk between the inherent matching probability of the templates used and the size of the training set." ></td>
	<td class="line x" title="129:188	A large training set means a larger number of incorrect sites that might engender overtrained rules, but also a better chance of finding other instances of those rule patterns and thus filtering them out." ></td>
	<td class="line x" title="130:188	The combination of those factors appears to cause the risk of overtraining for a particular irrelevant template to first rise and then fall with increasing training set size, as the initial effect of increased exposure is later overcome by that of increased filtering from further occurrences of the patterns." ></td>
	<td class="line x" title="131:188	In comparing this with decision trees, the key contrust is that the filtering effect there decreases as training proceeds." ></td>
	<td class="line x" title="132:188	The splitting predicates there are applied to increasingly small fragments of the training set, so that the chance of filtering counterexamples also decreases." ></td>
	<td class="line x" title="133:188	(To put it in decision tree terms, with few points left in the rectangle being split, it becomes more likely that an irrelevant predicate will incorrectly appear to provide a useful split)." ></td>
	<td class="line x" title="134:188	But since rule sequence learning continues to score its essentially independent rules against the entire training set, the protection of filtering against overtraining remains stronger." ></td>
	<td class="line x" title="135:188	Giving up the power to synthesize new rules thus provides an overtraining payoff as well as a leverage one." ></td>
	<td class="line x" title="136:188	Rule Interdependence While the connections between rules in a rule sequence are more limited than the inheritance of rule ancestors found in decision trees, it is still interesting to be able 90 o o 03 03 03 r,,, 03 03 i ! !" ></td>
	<td class="line x" title="137:188	0 200 400 600 800 1000 Figure 3: Training Set (solid line) and Test Set (dotted line) Performance on Greek Corpus 0 0 'ql'' 03 03 03 03 .0 03 ! !" ></td>
	<td class="line x" title="138:188	i 0 200 400 600 800 1000 Figure 4: Training with 1 Irrelevant Template on Greek Corpus 91 o o 03 03 0 03 r,, 03 03 I ; i i ! i i 0 200 400 600 800 1000 1200 Figure 5: Training with 7 Relevant and 1 Irrelevant Templates to characterize and quantify the rule dependencies that are present." ></td>
	<td class="line x" title="139:188	We have therefore added code that keeps track, whenever a rule is applied at a site, of a dependency tree showing the earlier rule applications that that rule depends on." ></td>
	<td class="line x" title="140:188	For example, the dependency tree from the Brown Corpus data in Fig." ></td>
	<td class="line x" title="141:188	6 shows a case where the last rule that applied at this particular site (the bottom line in the figure, representing the root of the tree), which changed JJ to RB, depended on earlier rules that changed the previous site (relative position -1) to VBN and the following one (position +1) to DT." ></td>
	<td class="line x" title="142:188	(The final number on each line tells during what pass that rule was learned." ></td>
	<td class="line x" title="143:188	While recorded internally as trees, these structures actually represent dependency DAGs, since one rule application may be an ancestor of another along more than one path)." ></td>
	<td class="line x" title="144:188	All sites start out ~qsigned a null dependency tree representing the baseline heuristic choice." ></td>
	<td class="line x" title="145:188	The application of a rule causes ;t new tree to be built, with a new root node, whose children are the dependency trees for those neighboring locations referenced by the rule pattern." ></td>
	<td class="line x" title="146:188	At the end of the training run, the final dependency trees are sorted, sl.ructurally similar trees are grouped together, and the cla.~s~.s are then sorted by frequency and output along with the list of rules learned." ></td>
	<td class="line x" title="147:188	(','rtain common classes of dependency can be noted iu t.ht, r,'sulting trees." ></td>
	<td class="line x" title="148:188	('.orrectiou rules result when one rnh, inak~,s an; overly gem~ral change, which affects not -,ly apl~rol~riate sites, but also inappropriate ones, so that a later rule in the sequence undoes part of the earlier effect." ></td>
	<td class="line x" title="149:188	One dependency of this type from our Brown Corpus run can be seen in Fig." ></td>
	<td class="line x" title="150:188	7." ></td>
	<td class="line x" title="151:188	Here the first rule was the more general one that changed PP$ to PPO whenever it follows VBD." ></td>
	<td class="line x" title="152:188	While that rule was generally useful, it overshot in some cases, causing the later learning of a correction rule that changed PPO back to PP$ after RB VBD." ></td>
	<td class="line x" title="153:188	Chaining rules occur in cases where a change ripples across a context, as in Fig." ></td>
	<td class="line x" title="154:188	8." ></td>
	<td class="line x" title="155:188	The first rule to apply here (21) changed QL to AP in relative position +2." ></td>
	<td class="line x" title="156:188	That change enabled the RB to QL rule (181) at position +1, and together those two changes enabled the root rule (781)." ></td>
	<td class="line x" title="157:188	Note that this two-step rule chain has allowed this rule to depend indirectly on a current tag value that is further away than could be sensed in a single rule, given the current maximum template width." ></td>
	<td class="line x" title="158:188	The dependency tree output also shows something of the overall degree and nature of rule interdependence." ></td>
	<td class="line x" title="159:188	The trees for a run on 50K words of the Brown Corpus bear out that rule dependencies, at least in the partof-speech tagging application, are limited." ></td>
	<td class="line x" title="160:188	Of a total of 3395 sites changed during training, only 396 had dependency trees with more than one node, with the most frequent such tree appearing only 4 times." ></td>
	<td class="line x" title="161:188	Thus the great majority of the learning in this case came from templates that applied in one step directly to the baseline tags, with leveraging being involved in only about 12% of the changes." ></td>
	<td class="line x" title="162:188	The relatively small amount of interaction found between the rules also suggests that the order in which 92 (7) (8) (649) o Figure 6: Sample Dependency Tree from Brown Corpus Data 0: -VBD PP$/PPO (30) 0: RB VBD PPO/PP$ (174) Figure 7: Sample Correction Class Dependency Tree from Brown Corpus Data +2: --QL/AP CS -(21) +I: RB/QL AP CS (181) 0: NNS/VBZ QL AP (781) Figure 8: Sample Chaining Class Dependency Tree from Brown Corpus Data o 03 03 0o 03 r, 03 ,D 03 i i ! !" ></td>
	<td class="line x" title="163:188	0 500 1000 1500 +1: ~ -CD/DT NN --1: -HVD VBD/VBN -0:,-VBN JJ/RB DT -Figure 9: Training and Test Set Performance on Greek, Random Rule Choice 93 the rules are applied is not likely to be a major factor in the success of the method for this particular application, and initial experiments tend to bear this out." ></td>
	<td class="line x" title="164:188	\['ig." ></td>
	<td class="line x" title="165:188	3 earlier showed a training run on Greek text using the largest net benefit choice rule that Brill proposes." ></td>
	<td class="line x" title="166:188	Note that, on this Greek corpus, the initial baseline level of choosing the most frequent training set tag for each word is already quite good; performance on both sets further improves during training, with most of the improvement occurring in the first few passes." ></td>
	<td class="line x" title="167:188	In comparison, Fig." ></td>
	<td class="line x" title="168:188	9 gives the results for a training run where the next rule at each step was randomly selected from amoug all rules that had a net positive effect of any size." ></td>
	<td class="line x" title="169:188	While tim progress is more gradual, both the training and test curves reach very close to the same maxima under these conditions as they do when the largest net I)enefit rule is chosen at each step." ></td>
	<td class="line x" title="170:188	Note that it does take more rules to reach those levels, since the randora training frequently chooses more specific rules that would have been subsumed by more general ones chosen later." ></td>
	<td class="line x" title="171:188	Thus the largest net benefit ranking criterion is a useful one, particularly if one wants to find a short initial subsequence of rules which achieves the bulk of the good effect." ></td>
	<td class="line x" title="172:188	But at least for this task, where there is little interdependence, choice of search order does not nm('h affect the final performance achieved." ></td>
	<td class="line x" title="173:188	Future Work The general analysis of rule sequences in relation to decision trees presented here is based on experiments primarily in the part-of-speech tagging domain." ></td>
	<td class="line x" title="174:188	Within that domain, it would be useful to quantify more clearly whether or not rule sequence learning is more effective than tiaditional decision tree methods when applied to the same corpora and making use of the same factors." ></td>
	<td class="line x" title="175:188	Such experiments would better illuminate the tradeoils between the ability to combine predicates into more complex rules on the one hand and the ability to leverage partial results and resist overtraining on the other." ></td>
	<td class="line x" title="176:188	It would also be usefu\[to test the data presented here on overtraining risk and on rule interdependence in other domains, particularly ones where the degree of rule interdependence could be expected to be greater." ></td>
	<td class="line x" title="177:188	Further exploration of the connections between rule sequences and decision trees may also suggest other approaches, perhaps blends of the two, that would work better in solne circumstances." ></td>
	<td class="line x" title="178:188	Within rule sequence learning itself, other ranking schemes for selecting the next rule to apply might be ahh." ></td>
	<td class="line x" title="179:188	to improve on the simple maximum net benefit heuristic." ></td>
	<td class="line x" title="180:188	We are currently exploring the use of likelihood ratios for this purpose." ></td>
	<td class="line x" title="181:188	It may also be possible to control for the remaining risk of overtraining in a more sensitive way than with a simple threshold." ></td>
	<td class="line x" title="182:188	Decision trees often use selective pruning to control overtraining, and deleted estimation (Jelinek and Mercer, 19~()) or other cross-validation techniques arc also natural .~u14gi,'sti~ms for this purpose, but if, is difficult to see how to apply any of these techniques to bare rule sequences because they contain hidden dependencies between rules, so that there is no obvious way to delete selected rules or to interpolate between two different rule sequences." ></td>
	<td class="line x" title="183:188	One goal for collecting the dependency tree data is to make it possible to prune or restructure rule sequences, using the recorded dependencies to maintain consistency among the remaining rules." ></td>
	<td class="line x" title="184:188	Conclusions Transformational rule sequence learning is a simple and powerful mechanism for capturing the patterns in linguistic data, which makes it an attractive alternative well worth further exploration." ></td>
	<td class="line x" title="185:188	Brill has showed that its performance for part-of-speech tagging can surpass that of the HMM models most frequently used, while producing a more compact and perhaps more interpretable model." ></td>
	<td class="line x" title="186:188	While its results can be compared with those of HMM models, the rule sequence technique itself seems to have more in common with decision trees, especially in its ability to automatically select at each stage from a large space of possible factors the predicate or rule that appears to be most useful." ></td>
	<td class="line x" title="187:188	Decision trees synthesize complex rules from elementary predicates by inheritance; rule sequence learning, on the other hand, prespecities in the templates essentially the full space of possible rules, with each rule acting largely independently." ></td>
	<td class="line x" title="188:188	This restriction in power turns out not to be crippling as long the template set can be made rich enough to cover the patterns likely to be found in the data, and it brings two important benefits in return: first, breaking the learning process into independent rules means that they can be applied to the whole corpus as they are learned, so that where neighboring patterns in the data are interrelated, the rules can leverage off the best estimates regarding their surroundings; and second, since the independent rules continue to be scored against the whole training corpus, a substantial measure of protection against overtraining compared to decision trees is gained." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E95-1014
Corpus-Based Method For Automatic Identification Of Support Verbs For Nominalizations
Teufel, Simone;Grefenstette, Gregory;"></td>
	<td class="line x" title="1:142	Corpus-based Method for Automatic Identification of Support Verbs for Nominalizations Gregory Grefenstette Simone Teufel Rank Xerox Research Centre Universitt Stuttgart 38240 Meylan, France Institut ffir maschinelle Sprachverarbeitung grefenxerox.fr D 70174 Stuttgart 1 s imone~ ims." ></td>
	<td class="line x" title="2:142	unistuttgart, de Abstract Nominalization is a highly productive phenomena in most languages." ></td>
	<td class="line x" title="3:142	The process of nominalization ejects a verb from its syntactic role into a nominal position." ></td>
	<td class="line x" title="4:142	The original verb is often replaced by a semantically emptied support verb (e.g. , make a proposal)." ></td>
	<td class="line x" title="5:142	The choice of a support verb for a given nominalization is unpredictable, causing a problem for language learners as well as for natural language processing systems." ></td>
	<td class="line x" title="6:142	We present here a method of discovering support verbs from an untagged corpus via low-level syntactic processing and comparison of arguments attached to verbal forms and potential nominalized forms." ></td>
	<td class="line x" title="7:142	The result of the process is a list of potential support verbs for the nominalized form of a given predicate." ></td>
	<td class="line x" title="8:142	1 Introduction Nominalization, the transformation of a verbal phrase into a nominal form, is possible in most languages (Comrie and Thompson, 1990)." ></td>
	<td class="line x" title="9:142	Nominalizations are used for a variety of stylistic reasons: to avoid repetitions of a verb, to avoid awkward intransitive uses of transitive verbs, in technical descriptions where passive is commonly used, etc. Though, as a result of nominalization, the original verb is ejected from its syntactic position, it often retains many of its thematic roles." ></td>
	<td class="line x" title="10:142	The original agents and patients can reappear as genitival or adjectival modifiers of the nominalized predicate." ></td>
	<td class="line x" title="11:142	In the syntactic place of the original verb can appear a semantically impoverished verb." ></td>
	<td class="line x" title="12:142	The semantically impoverished verb, often called a support verb, to be used with a nominalized predicate structure is unpredictable." ></td>
	<td class="line x" title="13:142	Allerton (1982)\[p. 76\] writes: Perhaps the most serious problem for these structures is that there is no constant selection of the empty verb: sometimes we find have, sometimes take, sometimes give, and rarely pay; sometimes we have a choice between two or more empty verbs e.g. have/take a look We have little choice but to record such irregularities in the lexicon  For this reason, the collocational choice of a support verb for a given nominalization is a difficult problem for language learners, as well as for natural language processor implementation." ></td>
	<td class="line x" title="14:142	We present here a method of deriving probable support verbs for nominalized predicates from corpora using low-level syntactic analysis and simple frequency statistics over its results." ></td>
	<td class="line x" title="15:142	This automatic procedure may be looked upon as an aid to lexicographers, as an independent extraction tool, or as a verification of lexical collocation information stored in a machine-based lexicon." ></td>
	<td class="line x" title="16:142	Since nominalized predicates can semantically drift over time to become concrete nouns having lost all their thematic role, our method first attempts to distinguish true nominalizations from concrete uses of the nominalized surface form." ></td>
	<td class="line x" title="17:142	This is done by comparing approximations to the argument/adjunct structures of verbal predicates to those of candidate nominalized forms in a large corpus." ></td>
	<td class="line x" title="18:142	For each selected nominalized form, syntactically supporting verb information is extracted from the corpus and then collated, providing the candidates for support verbs." ></td>
	<td class="line x" title="19:142	98 1.1 The rqominalization Cline The phenomenon of nominalization in English happens when a verb is replaced by a noun construction using a gerundive or nominal form of the verb." ></td>
	<td class="line x" title="20:142	The original subject and objects of the verb can reappear as Saxon or Norman genitives modifying the nominalized form." ></td>
	<td class="line x" title="21:142	Quirk et al.(Quirk et al. , 1985) distinguish nominalizations between deverbal and verbal nouns." ></td>
	<td class="line x" title="23:142	Examples of these are advice vs. killing." ></td>
	<td class="line x" title="24:142	Deverbal nouns are defined as records of the action having taken place rather than as description of the action itself." ></td>
	<td class="line x" title="25:142	This accounts for the contrast between their arriving for a month and .their arrival for a month." ></td>
	<td class="line x" title="26:142	Deverbal nouns can be replaced by regular count nouns in any context, for example painting as a deverbal noun in Brown's paintings of his daughter can be replaced by photograph whereas this is not the case with the verbal noun painting in The painting of Brown is as skillful as that of Gainsborough which describes the action of painting itself (Quirk et al. , 1985)\[p.1291\]." ></td>
	<td class="line x" title="27:142	As the following evidence shows, the semantics of the verb and much of its syntactic structure can be retained by either of its nominalized forms: She was surprised that the enemy destroyed the city." ></td>
	<td class="line x" title="28:142	She was surprised by the enemy('s) destroying the city." ></td>
	<td class="line x" title="29:142	She was surprised by the enemy's destruction of the city." ></td>
	<td class="line x" title="30:142	The cline of nominalization can be seen in the morphological changes 1 that some predicate undergo as they move from an inflected verb (e.g. destroyed) to non-inflected verbal noun (destroying) to a deverbal noun (destruction)." ></td>
	<td class="line x" title="31:142	In this article we shall consider only deverbal nouns 2 since these are the nominalizations involving the collocational phenomena of support verbs." ></td>
	<td class="line x" title="32:142	A remaining problem with the deverbal nouns is that the meaning of such nouns can become concretized over time, by a aThe morphological processes involved in transforming verbs into nominalizations are described in (Quirk et at., 1985), Sections 1.43 (conversion) and 1.30 (suffixation)." ></td>
	<td class="line x" title="34:142	See also 17.52 for discussion of this cline." ></td>
	<td class="line x" title="35:142	2On a practical level, we will also accept as deverbal nouns those forms ending in -ing which are marked as nouns in our lexicon, e.g. warning." ></td>
	<td class="line x" title="36:142	metonymic association." ></td>
	<td class="line x" title="37:142	Compare the uses of proposal in : He made his formal proposal to the full committee." ></td>
	<td class="line x" title="38:142	He put the proposal in the drawer." ></td>
	<td class="line x" title="39:142	The concrete uses of deverbal nouns are not involved in support verb constructions since they have lost the semantics of actions, and their attending thematic roles." ></td>
	<td class="line x" title="40:142	2 A very simple approach and its problems Looking for support verbs for nominalizatious might seem an easy problem at first, given that these support verbs are always the main verb for which the nominalization is the direct object." ></td>
	<td class="line x" title="41:142	What is needed is a low-level parser that extracts verb-object relations from corpora." ></td>
	<td class="line x" title="42:142	Given such a parser, one might be tempted to extract all the main verbs for a given nominalized form and consider tile most frequent of these verbs the expected support verbs." ></td>
	<td class="line x" title="43:142	As will be seen below, this approach is too simple." ></td>
	<td class="line x" title="44:142	The examples given above for proposal show that a given word form may be used with a meaning anywhere along the cline from true nominalization to concrete nouns." ></td>
	<td class="line x" title="45:142	Counting verbs of which these concrete nouns are direct objects will create noise hiding the true support verbs." ></td>
	<td class="line x" title="46:142	Since real nominalizations are those that still have verbal character, i.e. they have retained the semantic roles from the verb, we will try to recognize true nominalized uses by comparing the most frequent argument/adjunct structures found in the corpus around verbal uses of a given predicate to those syntactic structures found around tile candidate nominalized forms 3." ></td>
	<td class="line x" title="47:142	We will define true nominalizations as those which have a parallel syntactic structure to the original verb." ></td>
	<td class="line x" title="48:142	This is also in keeping with the definition of nominalizations given in (Quirk et al. , 1985)\[p. 1288\]: A noun phrase which has a systematic correspondence with a clause structure will be termed a 3Since these argument/adjunct structures are difficult to recognize precisely without an elaborate parser incorporating semantic analysis, we decided to identify heuristically the structures fulfilling these roles, for example, taking the most frequently occurring prepositional phrases after verbal sequences as adjuncts or arguments." ></td>
	<td class="line x" title="49:142	99 Nominalization." ></td>
	<td class="line x" title="50:142	The noun head of such a phrase is normally related morphologically to a verb, or to an adjective (i.e. , a deverbal or deadjectival noun)." ></td>
	<td class="line x" title="51:142	In this article we have been restricting the term nominalization to the noun heading the noun phrase." ></td>
	<td class="line x" title="52:142	3 Method In this section we explain our method for extracting support verbs for nominalizations." ></td>
	<td class="line x" title="53:142	We suppose that we are given a pair of words: ayerb and its nominalized form." ></td>
	<td class="line x" title="54:142	As explained in the previous section, we are interested in extracting only nominalized forms which have not become concrete nouns, and that this will be done by comparing syntactic structures attached to the verb and noun forms." ></td>
	<td class="line x" title="55:142	In order to extract corpus evidence related to these phenomena, we proceed as follows: 1." ></td>
	<td class="line x" title="56:142	We generate all the morphologically related forms of the word pair using a lexical transducer for English (Karttunen et al. , 1992)." ></td>
	<td class="line x" title="57:142	This list of words will be used as corpus filter." ></td>
	<td class="line x" title="58:142	2." ></td>
	<td class="line x" title="59:142	The lines of the corpus are tokenized (Grefenstette and Tapanainen, 1994), and only sentences containing one of the word forms in the filter are retained." ></td>
	<td class="line x" title="60:142	3." ></td>
	<td class="line oc" title="61:142	The corpus lines retained are part-of-speech tagged (Cutting et al. , 1992)." ></td>
	<td class="line x" title="62:142	This allows us to divide the corpus evidence into verb evidence and noun evidence." ></td>
	<td class="line x" title="63:142	4." ></td>
	<td class="line x" title="64:142	Using a robust surface parser (Grefenstette, 1994), we derive the local syntactic patterns involving the verbal form and the nominalized form." ></td>
	<td class="line x" title="65:142	5." ></td>
	<td class="line x" title="66:142	Considering that nominalized forms retain some of the verbal characteristics of the underlying predicate, we want to extract the most common argument/adjunct structures found around verbal uses of the predicate." ></td>
	<td class="line x" title="67:142	As an approximation, we extract here all the prepositional phrases found after the verb." ></td>
	<td class="line x" title="68:142	6." ></td>
	<td class="line x" title="69:142	For nominal forms, we select only those uses which involve argument/adjunct structures similar to phrases extracted in the previous step." ></td>
	<td class="line x" title="70:142	For these selected nominalized forms, we extract the verbs of which these forms are the direct frequency 458 million 438 billion 296 accord 260 increase 239 call 201 year 198 change 178 support 154 proposal 154 percent 143 money 142 plan 139 cut 130 aid 124 program 122 people Figure 1: The most common nouns preceding the most common prepositions following 'propose', and appearing in the same environment." ></td>
	<td class="line x" title="71:142	object." ></td>
	<td class="line x" title="72:142	We sort these verbs by frequency." ></td>
	<td class="line x" title="73:142	7." ></td>
	<td class="line x" title="74:142	This sorted list is the list of candidate support verbs for the nominalization." ></td>
	<td class="line x" title="75:142	This method assumes that the verb and the nominalized form of the verb are given." ></td>
	<td class="line x" title="76:142	We have experimented with automatically extracting the nominalized form by using the prepositional patterns extracted for the verb in step 5." ></td>
	<td class="line x" title="77:142	We extracted 6 megabytes of newspaper articles containing a form of the verb propose: propose, proposes, proposed, proposing." ></td>
	<td class="line x" title="78:142	Since one use of nominalization is to avoid repetition of the verb form, we suppose that the nominalization of propose is likely to appear in the same articles." ></td>
	<td class="line x" title="79:142	We extracted the three most common prepositions following a form of propose (step 5)." ></td>
	<td class="line x" title="80:142	We then extracted the nouns appearing in these same artigles and which preceded these prepositions.~oThe results 4 appear in figure 1." ></td>
	<td class="line x" title="81:142	Since a nomilmlized form is normally morphologically related to the verb form, almost any morphological comparison method will pick proposal from this list." ></td>
	<td class="line x" title="82:142	4Further experimentation has confirmed these results, but indicate that it may sufficient to simply tag a text, and perform morphological comparison with the most commonly cooccurring nouns in order to extract the nominalized forms of verbs." ></td>
	<td class="line x" title="83:142	100 frequency 167 reject 127 hear 114 make 81 file Figure 2: Most common verbs of which 'appeal' is marked as direct object." ></td>
	<td class="line x" title="84:142	4 Experiment with appeal-appeal We have taken for example the case of the verb appeal which was interesting since its corresponding deverbal noun shares the same surface form appeal." ></td>
	<td class="line x" title="85:142	In order to extract corpus evidence, we used a lexical transducer of English that, given the surface word appeal, produced all the inflected forms appeal, appeal's, appealing, appealed, appeals and appeals '." ></td>
	<td class="line x" title="86:142	Using these surface forms as a filter, we scanned 134 Megabytes of tokenized Associated Press newswire stories from the year 19895." ></td>
	<td class="line x" title="87:142	As a result of filtering, 6704 sentences (1 Mbyte of text) were extracted." ></td>
	<td class="line oc" title="88:142	This text was part-of-speech tagged using the Xerox HMM tagger (Cutting et al. , 1992)." ></td>
	<td class="line x" title="89:142	The lexical entries corresponding to appeal were tagged with the following tags: as a noun (3910 times), as an active or infinitival verb (1417), as a progressive verb (292), and as a past participle (400)." ></td>
	<td class="line x" title="90:142	This tagged text was then parsed by a low-level dependency parser (Grefenstette, 1994)\[Chap 3\]." ></td>
	<td class="line x" title="91:142	From the output of the dependency parser we extracted all the lexically normalized verbs of which appeal was tagged as a direct object." ></td>
	<td class="line x" title="92:142	The most common of these verbs are shown in Figure 2." ></td>
	<td class="line x" title="93:142	Our speaker's intuition tells us that the support verb for the nominalized use of appeal is make." ></td>
	<td class="line x" title="94:142	But this data does not give us enough information to make this judgement, since concrete versions as a separate entity are not distinguishable from nomlnalizations of the verb." ></td>
	<td class="line x" title="95:142	In order to separate nominalized uses of the predicate appeal from concrete uses, we will refer to the linguistic discussion presented in the introduction that says that nominalizations retain some of the argument/adjunct structure of the verbal predicate." ></td>
	<td class="line x" title="96:142	This is verified in the corpus since we find many parallel SThis corresponds to 20 million words of text." ></td>
	<td class="line x" title="97:142	structures involving appeal both as a verb and as a noun, such as: Vice President Salvador Laurel said today that an ailing Ferdinand Marcos may not survive the year and appealed to President Corazon Aquino to allow her ousted predecessor to die in his homeland." ></td>
	<td class="line x" title="98:142	Mrs. Marcos made a public appeal to President Corazon Aqulno to allow Marcos to return to his homeland to die." ></td>
	<td class="line x" title="99:142	Indeed, if we examine a common nominalization transformation, i.e. that of transforming the direct object of a verb into a Norman genitive of the nominalized form, we find a great overlap in the lexical arguments 6." ></td>
	<td class="line x" title="100:142	VERB FORM NOMINALIZATION 22 appeal of conviction 67 appeal decision 61 appeal ruling 35 appeal conviction 29 appeal verdict 23 appeal case 22 appeal sentence 21 appeal order 7 appeal judgment 10 appeal of ruling 9 appeal of order 6 appeal of decision 4 appeal of verdict 4 appeal of sentence 4 appeal of plan 4 appeal of.inmate The parser's output allowed us to extract patterns involving prepositional phrases following noun phrases headed by appeal as well as those following verb sequences headed by appeal." ></td>
	<td class="line x" title="101:142	The most common prepositional phrases found after appeal as a verb began with the prepositions7: Lo (466 times), for (145), in (18), on (12), wilh (5), etc. The prepositional phrases following appeal as a noun are headed by to (321 times), for (253), in (200), of (134), from (78), on (34), etc. The correspondence between the most frequent prepositions allowed us to consider that the patterns of a noun phrase headed by appeal followed by one these prepositional phrases (i.e. , begun with to, for, and in) constituted true nominalizations s. There were 6We decided not to use this type of data in our experiments because matching lexical arguments requires much larger corpora than the ones we had extracted for the other verbs tested." ></td>
	<td class="line x" title="102:142	rWe ignored prepositionM phrases headed by by as being probable passivizations, since our parser does not recognize passive patterns involving by." ></td>
	<td class="line x" title="103:142	SHere we used only part of the corpus evidence that was available." ></td>
	<td class="line x" title="104:142	Other patterns of nominMizations of appeal, e.g. Saxon genitives like the criminal's appeal, may well exist in the corpus." ></td>
	<td class="line x" title="105:142	101 frequency 63 make 16 have 15 issue Figure 3: Most common verbs supporting the structure NP PP where 'appeal' heads the NP and where one of {to, for, in} begins the PP." ></td>
	<td class="line x" title="106:142	774 instances of these patterns." ></td>
	<td class="line x" title="107:142	The parser's output further allowed us to extract the verbs for which these nominalizations were considered as the direct objects." ></td>
	<td class="line x" title="108:142	318 of these nominal syntactic patterns including to, for and in were found." ></td>
	<td class="line x" title="109:142	Of these patterns, the main verb supporting the objective nominalizations are shown in Figure 3." ></td>
	<td class="line x" title="110:142	These results suggest that the support verb for the nominalization of appeal is make." ></td>
	<td class="line x" title="111:142	5 Other Predicate Examples and Discussion When the same filtering technique is applied to subcorpora derived for other nominalization pairs, we obtain the results given in Figure 4." ></td>
	<td class="line x" title="112:142	For each verb-noun pair all sentences containing any form of the words were extracted from the AP corpus." ></td>
	<td class="line x" title="113:142	The sentences were processed as explained in section 3." ></td>
	<td class="line x" title="114:142	For each verbal use, the most frequent prepositional phrases following the verbs were tabulated and the three most frequent prepositions were retained." ></td>
	<td class="line x" title="115:142	For example, the most frequent prepositions beginning prepositional phrases following verb uses of the lemma offer were for, in and to." ></td>
	<td class="line x" title="116:142	These prepositions were used to select probable nominalizations by extracting noun uses of the predicates that were immediately followed by prepositional phrases headed by one of the three most frequent verbal prepositions." ></td>
	<td class="line x" title="117:142	For these extracted noun phrases, when they were found in a direct object position, the main verbs were tabulated which gives the results in Figure 4." ></td>
	<td class="line x" title="118:142	Some of the results in Figure 4 correspond to our naive intuitions of collocational support verbs, such as make an offer." ></td>
	<td class="line x" title="119:142	For discussion, both have and hold appear equally frequently." ></td>
	<td class="line x" title="120:142	But other words show the limitations of this method, we would expect make a demand where we find meet a demand." ></td>
	<td class="line x" title="121:142	In the same subcorpus, although we find make a demand 77 times, meet a demand is twoand-a-half times more common." ></td>
	<td class="line x" title="122:142	Could this be because, in a newswire corpus, meeting a demand is more newsworthy than making one?" ></td>
	<td class="line x" title="123:142	If we just look at the cases where demand is modified by the indefinite article; which might correspond to the more generic nominalizations one spontaneously creates when generating examples, we find that in the corpus make a demand occurs slightly more often than meet a demand, ten times vs. six times, but this is too rare to use as a criterion." ></td>
	<td class="line x" title="124:142	In other cases, such as with proposal and assertion we find make and reject with almost equal frequency, and though make might well be considered a support verb, it is hard to accept reject as semantically empty." ></td>
	<td class="line x" title="125:142	Though reject is more a consequence than an antonym of make a proposal, this raises the question, to which we have no answer, of whether support verbs have an equally empty antonym." ></td>
	<td class="line x" title="126:142	A more interesting case is the appearance of issue for order and warning where we would expect give." ></td>
	<td class="line x" title="127:142	Looking into the corpus evidence, we find issue a restraining order 46 times, and give any type of order only 16 times." ></td>
	<td class="line x" title="128:142	This evidence suggest a limitation of our word-based approach." ></td>
	<td class="line x" title="129:142	Multi-word phrases, such as the nominalized phrase restraining order, might take a different support verb than the simple unqualified word forms, such as order." ></td>
	<td class="line x" title="130:142	6 Conclusions Nominalization is a very productive process." ></td>
	<td class="line x" title="131:142	The proper choice of collocational support verbs for nominalizations in English is a difficult task for language learners given the unpredictability of the semantically emptied verb that fulfills the syntactic role." ></td>
	<td class="line x" title="132:142	Given a robust parser and large corpus, the simple technique of extracting the most common verbs for which the nominalized form is the direct object is not always sufficient, since completely deverbal concrete noun uses share the same lexical surface form." ></td>
	<td class="line x" title="133:142	Comparing argument/adjunct structures involving the verbal uses of the predicate and using the most common of these structures as filters on the surface forms possibly corresponding to nominalizations captures the linguistic fact that nominalizations retain the syntactic structures of their underlying predicate." ></td>
	<td class="line x" title="134:142	When these filters are applied, the most common supporting verb in the corpus for the recognized nominalized patterns seems to correspond to native speakers' intuition of the support verb associated with the nominalization." ></td>
	<td class="line x" title="135:142	The experiment described here on a 134 102 nominalization offer-offer discuss-discussion demand-demand propose-proposal order-order complain-complaint warn-warning confirm-confirmation assert-assertion suggest-suggestion preps for(ll6), in(100), to(98) with(127), in(85), at(54) for(37), in(28), of(22) in(103), for(77), to(46) of(91), to(50), in(33) about(183), of(155), to(91) of(140), against(46), in(44) in(30), of(28), to(10) in(12), at(3), to(2) to(60), in(57), of(27) most common main verbs make (116 cases), begin(37), launch(36) have (42), hold(42), begin(9) meet(58), press(34), increase(22) make(28), reject(26), submit(19) issue (24), give(8), bring(7) receive (20), file(12), have(10) issue (17), receive(5), make(4) win (6), recommend(5), have(4) make (3), repeat(l), dispute(l) make(5), reject(5), offer(2) Figure 4: Most common verbs supporting the structure found for other nominalization pairs using the syntactic structure filtering mechanism." ></td>
	<td class="line x" title="136:142	megabyte corpus of newspaper text from which was extracted evidence for appeal and other predicates shows how this automated procedure can be applied to any verbnominalization pair given a large corpus and a robust parser." ></td>
	<td class="line x" title="137:142	Other work in automated support verb discovery using bilingual dictionaries as a source has been reported in Fontenelle (1993)." ></td>
	<td class="line x" title="138:142	It remains to be seen whether these statistical results are more useful to lexicographers than their more traditional tools of key-wordin-context files and T-score measures." ></td>
	<td class="line x" title="139:142	Human experiments would be necessary to demonstrate this." ></td>
	<td class="line x" title="140:142	Another useful test of the results would be to compare the results given by this technique against machine-readable dictionary-derived data." ></td>
	<td class="line x" title="141:142	In conclusion, the interest of this technique is its general approach to corpus linguistics as one of multiple passes over the same corpus material, using results of previous passes to filter and refine data extracted on subsequent passes." ></td>
	<td class="line x" title="142:142	We believe that this approach, coupled with lexical resources and robust parsers, offers much promise for the future of corpus exploitation." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E95-1020
Distributional Part-Of-Speech Tagging
Schtze, Hinrich;"></td>
	<td class="line x" title="1:193	Distributional Part-of-Speech Tagging Hinrich Schfitze CSLI, Ventura Hall Stanford, CA 94305-4115, USA emMl: schuetze~csli.stanford.edu URL: ftp://csli.stanford.edu/pub/prosit/DisPosTag.ps Abstract This paper presents an algorithm for tagging words whose part-of-speech properties are unknown." ></td>
	<td class="line x" title="2:193	Unlike previous work, the algorithm categorizes word tokens in con$ezt instead of word ~ypes." ></td>
	<td class="line x" title="3:193	The algorithm is evaluated on the Brown Corpus." ></td>
	<td class="line x" title="4:193	1 Introduction Since online text becomes available in ever increasing volumes and an ever increasing number of languages, there is a growing need for robust processing techniques that can analyze text without expensive and time-consuming adaptation to new domains and genres." ></td>
	<td class="line x" title="5:193	This need motivates research on fully automatic text processing that may rely on general principles of linguistics and computation, but does not depend on knowledge about individual words." ></td>
	<td class="line x" title="6:193	In this paper, we describe an experiment on fully automatic derivation of the knowledge necessary for part-of-speech tagging." ></td>
	<td class="line x" title="7:193	Part-of-speech tagging is of interest for a number of applications, for example access to text data bases (Kupiec, 1993), robust parsing (Abney, 1991), and general parsing (deMarcken, 1990; Charniak et al. , 1994)." ></td>
	<td class="line x" title="8:193	The goal is to find an unsupervised method for tagging that relies on general distributional properties of text, properties that are invariant across languages and sublanguages." ></td>
	<td class="line x" title="9:193	While the proposed algorithm is not successful for all grammatical categories, it does show that fully automatic tagging is possible when demands on accuracy are modest." ></td>
	<td class="line x" title="10:193	The following sections discuss related work, describe the learning procedure and evaluate it on the Brown Corpus (Francis and Ku~era, 1982)." ></td>
	<td class="line x" title="11:193	2 Related Work The simplest part-of-speech taggers are bigram or trigram models (Church, 1989; Charniak et al. , 1993)." ></td>
	<td class="line x" title="12:193	They require a relatively large tagged training text." ></td>
	<td class="line x" title="13:193	Transformation-based tagging as introduced by Brill (1993) also requires a handtagged text for training." ></td>
	<td class="line oc" title="14:193	No pretagged text is necessary for Hidden Markov Models (Jelinek, 1985; Cutting et al. , 1991; Kupiec, 1992)." ></td>
	<td class="line x" title="15:193	Still, a lexicon is needed that specifies the possible parts of speech for every word." ></td>
	<td class="line x" title="16:193	Brill and Marcus (1992a) have shown that the effort necessary to construct the part-of-speech lexicon can be considerably reduced by combining learning procedures and a partial part-of-speech categorization elicited from an informant." ></td>
	<td class="line x" title="17:193	The present paper is concerned with tagging languages and sublanguages for which no a priori knowledge about grammatical categories is available, a situation that occurs often in practice (Brill and Marcus, 1992a)." ></td>
	<td class="line x" title="18:193	Several researchers have worked on learning grammatical properties of words." ></td>
	<td class="line x" title="19:193	Elman (1990) trains a connectionist net to predict words, a process that generates internal representations that reflect grammatical category." ></td>
	<td class="line x" title="20:193	Brill et al.(1990) try to infer grammatical category from bigram statistics." ></td>
	<td class="line x" title="22:193	Finch and Chater (1992) and Finch (1993) use vector models in which words are clustered according to the similarity of their close neighbors in a corpus." ></td>
	<td class="line x" title="23:193	Kneser and Ney (1993) present a probabilistic model for entropy maximization that also relies on the immediate neighbors of words in a corpus." ></td>
	<td class="line x" title="24:193	Biber (1993) applies factor analysis to collocations of two target words ('certain' and 'right') with their immediate neighbors." ></td>
	<td class="line x" title="25:193	What these approaches have in common is that they classify words instead of individual occurrences." ></td>
	<td class="line x" title="26:193	Given the widespread part-of-speech ambiguity of words this is problematicJ How should a word like 'plant' be categorized if it has uses both as a verb and as a noun?" ></td>
	<td class="line x" title="27:193	How can a categorization be considered meaningful if the infinitive marker 'to' is not distinguished from the homophonous preposition?" ></td>
	<td class="line x" title="28:193	In a previous paper (Schfitze, 1993), we trained a neural network to disambiguate part-of-speech *Although Biber (1993) classifies collocations, these can also be ambiguous." ></td>
	<td class="line x" title="29:193	For example, 'for certain' has both senses of 'certain': 'particular' and 'sure'." ></td>
	<td class="line x" title="30:193	141 word side nearest neighbors onto left onto right seemed left seemed right into toward away off together against beside around down reduce among regarding against towards plus toward using unlike appeared might would remained had became could must should seem seems wanted want going meant tried expect likely Table h Words with most similar left and right neighbors for 'onto' and 'seemed'." ></td>
	<td class="line x" title="31:193	using context; however, no information about the word that is to be categorized was used." ></td>
	<td class="line x" title="32:193	This scheme fails for cases like 'The soldiers rarely come home'." ></td>
	<td class="line x" title="33:193	vs. 'The soldiers will come home'." ></td>
	<td class="line x" title="34:193	where the context is identical and information about the lexical item in question ('rarely' vs. 'will') is needed in combination with context for correct classification." ></td>
	<td class="line x" title="35:193	In this paper, we will compare two tagging algorithms, one based on classifying word types, and one based on classifying words-plus-context." ></td>
	<td class="line x" title="36:193	3 Tag induction We start by constructing representations of the syntactic behavior of a word with respect to its left and right context." ></td>
	<td class="line x" title="37:193	Our working hypothesis is that syntactic behavior is reflected in cooccurrence patterns." ></td>
	<td class="line x" title="38:193	Therefore, we will measure the similarity between two words with respect to their syntactic behavior to, say, their left side by the degree to which they share the same neighbors on the left." ></td>
	<td class="line x" title="39:193	If the counts of neighbors are assembled into a vector (with one dimension for each neighbor), the cosine can be employed to measure similarity." ></td>
	<td class="line x" title="40:193	It will assign a value close to 1.0 if two words share many neighbors, and 0.0 if they share none." ></td>
	<td class="line x" title="41:193	We refer to the vector of left neighbors of a word as its left contezt vector, and to the vector of right neighbors as its right contezt vector." ></td>
	<td class="line x" title="42:193	The unreduced context vectors in the experiment described here have 250 entries, corresponding to the 250 most frequent words in the Brown corpus." ></td>
	<td class="line x" title="43:193	This basic idea of measuring distributional similarity in terms of shared neighbors must be modified because of the sparseness of the data." ></td>
	<td class="line x" title="44:193	Consider two infrequent adjectives that happen to modify different nouns in the corpus." ></td>
	<td class="line x" title="45:193	Their right similarity according to the cosine measure would be zero." ></td>
	<td class="line x" title="46:193	This is clearly undesirable." ></td>
	<td class="line x" title="47:193	But even with high-frequency words, the simple vector model can yield misleading similarity measurements." ></td>
	<td class="line x" title="48:193	A case in point is 'a' vs. 'an'." ></td>
	<td class="line x" title="49:193	These two articles do not share any right neighbors since the former is only used before consonants and the latter only before vowels." ></td>
	<td class="line x" title="50:193	Yet intuitively, they are similar with respect to their right syntactic context despite the lack of common right neighbors." ></td>
	<td class="line x" title="51:193	Our solution to these problems is the application of a singular value decomposition." ></td>
	<td class="line x" title="52:193	We can represent the left vectors of all words in the corpus as a matrix C with n rows, one for each word whose left neighbors are to be represented, and k columns, one for each of the possible neighbors." ></td>
	<td class="line x" title="53:193	SVD can be used to approximate the row and column vectors of C in a low-dimensional space." ></td>
	<td class="line x" title="54:193	In more detail, SVD decomposes a matrix C, the matrix of left vectors in our case, into three matrices To, So, and Do such that: C = ToSoD' o So is a diagonal k-by-k matrix that contains the singular values of C in descending order." ></td>
	<td class="line x" title="55:193	The ith singular value can be interpreted as indicating the strength of the ith principal component of C. To and Do are orthonormal matrices that approximate the rows and columns of C, respectively." ></td>
	<td class="line x" title="56:193	By restricting the matrices To, So, and Do to their first m < k columns (= principal components) one obtains the matrices T, S, and D. Their product C is the best least square approximation of C by a matrix of rank m: C = TSD'." ></td>
	<td class="line x" title="57:193	We chose m = 50 (reduction to a 50-dimensional space) for the SVD's described in this paper." ></td>
	<td class="line x" title="58:193	SVD addresses the problems of generalization and sparseness because broad and stable generalizations are represented on dimensions with large values which will be retained in the dimensionality reduction." ></td>
	<td class="line x" title="59:193	In contrast, dimensions corresponding to small singular values represent idiosyncrasies, like the phonological constraint on the usage of 'an' vs. 'a', and will be dropped." ></td>
	<td class="line x" title="60:193	We also gain efficiency since we can manipulate smaller vectors, reduced to 50 dimensions." ></td>
	<td class="line x" title="61:193	We used SVDPACK to compute the singular value decompositions described in this paper (Berry, 1992)." ></td>
	<td class="line x" title="62:193	Table 1 shows the nearest neighbors of two words (ordered according to closeness to the head word) after the dimensionality reduction." ></td>
	<td class="line x" title="63:193	Neighbors with highest similarity according to both left and right context are listed." ></td>
	<td class="line x" title="64:193	One can see clear differences between the nearest neighbors in the two spaces." ></td>
	<td class="line x" title="65:193	The right-context neighbors of 'onto' contain verbs because both prepositions and verbs govern noun phrases to their right." ></td>
	<td class="line x" title="66:193	The left-context neighborhood of 'onto' reflects the fact that prepositional phrases are used in the same position as adverbs like 'away' and 'together', thus making their left context similar." ></td>
	<td class="line x" title="67:193	For 'seemed', left-context neighbors are words that have similar types of noun phrases in subject position (mainly auxiliaries)." ></td>
	<td class="line x" title="68:193	The rightcontext neighbors all take 'to'-infinitives as complements." ></td>
	<td class="line x" title="69:193	An adjective like 'likely' is very sim142 ilar to 'seemed' in this respect although its left context is quite different from that of 'seemed'." ></td>
	<td class="line x" title="70:193	Similarly, the generalization that prepositions and transitive verbs are very similar if not identical in the way they govern noun phrases would be lost if 'left' and 'right' properties of words were lumped together in one representation." ></td>
	<td class="line x" title="71:193	These examples demonstrate the importance of representing generalizations about left and right context separately." ></td>
	<td class="line x" title="72:193	The left and right context vectors are the basis for four different tag induction experiments, which are described in detail below:  induction based on word type only  induction based on word type and context  induction based on word type and context, restricted to 'natural' contexts  induction based on word type and context, using generalized left and right context vectors 3.1 Induction based on word type only The two context vectors of a word characterize the distribution of neighboring words to its left an.d right." ></td>
	<td class="line x" title="73:193	The concatenation of left and right context vector can therefore serve as a representation of a word's distributional behavior (Finch and Chater, 1992; Sch/itze, 1993)." ></td>
	<td class="line x" title="74:193	We formed such concatenated vectors for all 47,025 words (surface forms) in the Brown corpus." ></td>
	<td class="line x" title="75:193	Here, we use the raw 250dimensional context vectors and apply the SVD to the 47,025-by-500 matrix (47,025 words with two 250-dimensional context vectors each)." ></td>
	<td class="line oc" title="76:193	We obtained 47,025 50-dimensional reduced vectors from the SVD and clustered them into 200 classes using the fast clustering algorithm Buckshot (Cutting et al. , 1992) (group average agglomeration applied to a sample)." ></td>
	<td class="line x" title="77:193	This classification constitutes the baseline performance for distributional partof-speech tagging." ></td>
	<td class="line x" title="78:193	All occurrences of a word are assigned to one class." ></td>
	<td class="line x" title="79:193	As pointed out above, such a procedure is problematic for ambiguous words." ></td>
	<td class="line x" title="80:193	3.2 Induction based on word type and context In order to exploit contextual information in the classification of a token, we simply use context vectors of the two words occurring next to the token." ></td>
	<td class="line x" title="81:193	An occurrence of word w is represented by a concatenation of four context vectors:  The right context vector of the preceding word." ></td>
	<td class="line x" title="82:193	 The left context vector of w.  The right context vector of w.  The left context vector of the following word." ></td>
	<td class="line x" title="83:193	The motivation is that a word's syntactic role depends both on the syntactic properties of its neighbors and on its own potential for entering into syntactic relationships with these neighbors." ></td>
	<td class="line x" title="84:193	The only properties of context that we consider are the right-context vector of the preceding word and the left-context vector of the following word because they seem to represent the contextual information most important for the categorization of w. For example, for the disambiguation of 'work' in 'her work seemed to be important', only the fact that 'seemed' expects noun phrases to its left is important, the right context vector of 'seemed' does not contribute to disambiguation." ></td>
	<td class="line x" title="85:193	That only the immediate neighbors are crucial for categorization is clearly a simplification, but as the results presented below show it seems to work surprisingly well." ></td>
	<td class="line x" title="86:193	Again, an SVD is applied to address the problems of sparseness and generalization." ></td>
	<td class="line x" title="87:193	We randomly selected 20,000 word triplets from the corpus and formed concatenations of four context vectors as described above." ></td>
	<td class="line x" title="88:193	The singular value decomposition of the resulting 20,000-by-l,000 matrix defines a mapping from the 1,000-dimensional space of concatenated context vectors to a 50dimensional reduced space." ></td>
	<td class="line x" title="89:193	Our tag set was then induced by clustering the reduced vectors of the 20,000 selected occurrences into 200 classes." ></td>
	<td class="line x" title="90:193	Each of the 200 tags is defined by the centroid of the corresponding class (the sum of its members)." ></td>
	<td class="line x" title="91:193	Distributional tagging of an occurrence of a word w proceeds then by retrieving the four relevant context vectors (right context vector of previous word, left context vector of following word, both context vectors of w) concatenating them to one 1000-component vector, mapping this vector to 50 dimensions, computing the correlations with the 200 cluster centroids and, finally, assigning the occurrence to the closest cluster." ></td>
	<td class="line x" title="92:193	This procedure was applied to all tokens of the Brown corpus." ></td>
	<td class="line x" title="93:193	We will see below that this method of distributional tagging, although partially successful, fails for many tokens whose neighbors are punctuation marks." ></td>
	<td class="line x" title="94:193	The context vectors of punctuation marks contribute little information about syntactic categorization since there are no grammatical dependencies between words and punctuation marks, in contrast to strong dependencies between neighboring words." ></td>
	<td class="line x" title="95:193	For this reason, a second induction on the basis of word type and context was performed, but only for those tokens with informative contexts." ></td>
	<td class="line x" title="96:193	Tokens next to punctuation marks and tokens with rare words as neighbors were not included." ></td>
	<td class="line x" title="97:193	Contexts with rare words (less than ten occurrences) were also excluded for similar reasons: If a word only occurs nine or fewer times its left and right context vectors capture little information for syntactic categorization." ></td>
	<td class="line x" title="98:193	In the experiment, 20,000 natural contexts were randomly selected, processed by the SVD and clustered into 143 tag ADN CC CD DT IN ING MD N description adnominal modifier conjunction cardinal determiner preposition '-ing' forms modal nominal Table 2: Evaluation tag Penn Treebank tags ADN* $ CC CD DT PDT PRP$ IN VBG MD NNP(S) NN(S) tag POS PRP RB TO VB VBD VBN WDT description Penn Treebank tags possessive marker POS pronoun PRP adverbial RB RP RBR RBS infinitive marker TO infinitive VB inflected verb form VBD VBZ VBP predicative VBN PRD  wh-word WP($) WRB WDT set." ></td>
	<td class="line x" title="99:193	Structural tags derived from parse trees are marked with 200 classes." ></td>
	<td class="line x" title="100:193	The classification was then applied to all natural contexts of the Brown corpus." ></td>
	<td class="line x" title="101:193	3.3 Generalized context vectors The context vectors used so far only capture information about distributional interactions with the 250 most frequent words." ></td>
	<td class="line x" title="102:193	Intuitively, it should be possible to gain accuracy in tag induction by using information from more words." ></td>
	<td class="line x" title="103:193	One way to do this is to let the right context vector record which classes of left conte~t vectors occur to the right of a word." ></td>
	<td class="line x" title="104:193	The rationale is that words with similar left context characterize words to their right in a similar way." ></td>
	<td class="line x" title="105:193	For example, 'seemed' and 'would' have similar left contexts, and they characterize the right contexts of 'he' and 'the firefighter' as potentially containing an inflected verb form." ></td>
	<td class="line x" title="106:193	Rather than having separate entries in its right context vector for 'seemed', 'would', and 'likes', a word like 'he' can now be characterized by a generalized entry for 'inflected verb form occurs frequently to my right'." ></td>
	<td class="line x" title="107:193	This proposal was implemented by applying a singular value decomposition to the 47025-by-250 matrix of left context vectors and clustering the resulting context vectors into 250 classes." ></td>
	<td class="line x" title="108:193	A generalized right context vector v for word w was then formed by counting how often words from these 250 classes occurred to the right of w. Entry vi counts the number of times that a word from class i occurs to the right of w in the corpus (as opposed to the number of times that the word with frequency rank i occurs to the right of w)." ></td>
	<td class="line x" title="109:193	Generalized left context vectors were derived by an analogous procedure using word-based right context vectors." ></td>
	<td class="line x" title="110:193	Note that the information about left and right is kept separate in this computation." ></td>
	<td class="line x" title="111:193	This differs from previous approaches (Finch and Chater, 1992; Schfitze, 1993) in which left and right context vectors of a word are always used in one concatenated vector." ></td>
	<td class="line x" title="112:193	There are arguably fewer different types of right syntactic contexts than types of syntactic categories." ></td>
	<td class="line x" title="113:193	For example, transitive verbs and prepositions belong to different syntactic categories, but their right contexts are virtually identical in that they require a noun phrase." ></td>
	<td class="line x" title="114:193	This generalization could not be exploited if left and right context were not treated separately." ></td>
	<td class="line x" title="115:193	Another argument for the two-step derivation is that many words don't have any of the 250 most frequent words as their left or right neighbor." ></td>
	<td class="line x" title="116:193	Hence, their vector would be zero in the wordbased scheme." ></td>
	<td class="line x" title="117:193	The class-based scheme makes it more likely that meaningful representations are formed for all words in the vocabulary." ></td>
	<td class="line x" title="118:193	The generalized context vectors were input to the tag induction procedure described above for word-based context vectors: 20,000 word triplets were selected from the corpus, encoded as 1,000dimensional vectors (consisting of four generalized context vectors), decomposed by a singular value decomposition and clustered into 200 classes." ></td>
	<td class="line x" title="119:193	The resulting classification was applied to all tokens in the Brown corpus." ></td>
	<td class="line x" title="120:193	4 Results The results of the four experiments were evaluated by forming 16 classes of tags from the Penn Treebank as shown in Table 2." ></td>
	<td class="line x" title="121:193	Preliminary experiments showed that distributional methods distinguish adnominal and predicative uses of adjectives (e.g. 'the black cat' vs. 'the cat is black')." ></td>
	<td class="line x" title="122:193	Therefore the tag 'ADN' was introduced for uses of adjectives, nouns, and participles as adnominal modifiers." ></td>
	<td class="line x" title="123:193	The tag 'PRD' stands for predicative uses of adjectives." ></td>
	<td class="line x" title="124:193	The Penn Treebank parses of the Brown corpus were used to determine whether a token functions as an adnominal modifier." ></td>
	<td class="line x" title="125:193	Punctuation marks, special symbols, interjections, foreign words and tags with fewer than 100 instances were excluded from the evaluation." ></td>
	<td class="line x" title="126:193	Tables 3 and 4 present results for word typebased induction and induction based on word type and context." ></td>
	<td class="line x" title="127:193	For each tag t, the table lists the frequency of t in the corpus ('frequency') 2, the number of induced tags i0, il,  ., iz, that were assigned to it ('# classes'); the number of times an occurrence of t was correctly labeled as belonging to one of i0, Q,,iz ('correct'); the number of times that a token of a different tag t ~ was 2The small difference in overall frequency in the tables is due to the fact that some word-based context vectors consist entirely of zeros." ></td>
	<td class="line x" title="129:193	There were about a hundred word triplets whose four context vectors did not have non-zero entries and could not be assigned a cluster." ></td>
	<td class="line x" title="130:193	144 tag J~ frequency 108586 CC 36808 CD 15085 DT 129626 IN 132079 ING 14753 MD 13498 N 231434 POS 5086 PRP 47686 RB 54525 TO 25196 VB 35342 VBD 80058 VBN 41146 WDT 14093 avg." ></td>
	<td class="line x" title="131:193	# classes }correct 0 4 \[ 3376 2 \[ 125540 3 \[118726 5 \[ 2111 2 \[ 13383 98 \[ 193838 1 \[ 4641 3 \[ 43839 0 8 I 29138 12 I 36653 0 incorrect precision 19528 0.66 0 0.00 1431 0.70 31783 0.80 75829 0.61 1016 0.68 13016 0.51 79652 0.71 1213 0.79 21723 0.67 56505 0.38 0 0.00 17945 0.62 3855 0.90 8841 0.47 0 0.00 0.53 0T-- 35 m \[ 0.00 \[ 0.22 \[ 0.97 \[ 0.90 \[ 0.14 \[ 0.99 \[ 0.84 \[ 0.91 I 0.92 \[ 0.65 \[ 0.00 \[ 0.82 \[ 0.46 \[ 0.19 j 0.52 Table 3: Precision and recall for induction based on word type." ></td>
	<td class="line x" title="132:193	F 0.46 0.00 0.34 0.87 0.73 0.24 0.67 0.77 0.85 0.77 0.48 0.00 0.71 0.61 0.27 0.00 0.49 miscategorized as being an instance of i0, il,  , il ('incorrect'); and precision and recall of the categorization of t. Precision is the number of correct tokens divided by the sum of correct and incorrect tokens." ></td>
	<td class="line x" title="133:193	Recall is the number of correct tokens divided by the total number of tokens of t (in the first column)." ></td>
	<td class="line x" title="134:193	The last column gives van Rijsbergen's F measure which computes an aggregate score from precision and recall: (van Rijsbergen, 1 1979) F = ~-~+(1-~)~' We chose c~ = 0.5 to give equal weight to precision and recall." ></td>
	<td class="line x" title="135:193	It is clear from the tables that incorporating context improves performance considerably." ></td>
	<td class="line x" title="136:193	The F score increases for all tags except CD, with an average improvement of more than 0.20." ></td>
	<td class="line x" title="137:193	The tag CD is probably better thought of as describing a word class." ></td>
	<td class="line x" title="138:193	There is a wide range of heterogeneous syntactic functions of cardinals in particular contexts: quantificational and adnominal uses, bare NP's ('is one of'), dates and ages ('Jan 1', 'gave his age as 25'), and enumerations." ></td>
	<td class="line x" title="139:193	In this light, it is not surprising that the word-type method does better on cardinals." ></td>
	<td class="line x" title="140:193	Table 5 shows that performance for generalized context vectors is better than for word-based context vectors (0.74 vs. 0.72)." ></td>
	<td class="line x" title="141:193	However, since the number of tags with better and worse performance is about the same (7 and 5), one cannot conclude with certainty that generalized context vectors induce tags of higher quality." ></td>
	<td class="line x" title="142:193	Apparently, the 250 most frequent words capture most of the relevant distributional information so that the additional information from less frequent words available from generalized vectors only has a small effect." ></td>
	<td class="line x" title="143:193	Table 6 looks at results for 'natural' contexts, i.e. those not containing punctuation marks and rare words." ></td>
	<td class="line x" title="144:193	Performance is consistently better than for the evaluation on all contexts, indicating that the low quality of the distributional information about punctuation marks and rare words is a difficulty for successful tag induction." ></td>
	<td class="line x" title="145:193	Even for 'natural' contexts, performance varies considerably." ></td>
	<td class="line x" title="146:193	It is fairly good for prepositions, determiners, pronouns, conjunctions, the infinitive marker, modals, and the possessive marker." ></td>
	<td class="line x" title="147:193	Tag induction fails for cardinals (for the reasons mentioned above) and for '-ing' forms." ></td>
	<td class="line x" title="148:193	Present participles and gerunds are difficult because they exhibit both verbal and nominal properties and occur in a wide variety of different contexts whereas other parts of speech have a few typical and frequent contexts." ></td>
	<td class="line x" title="149:193	It may seem worrying that some of the tags are assigned a high number of clusters (e.g. , 49 for N, 36 for ADN)." ></td>
	<td class="line x" title="150:193	A closer look reveals that many clusters embody finer distinctions." ></td>
	<td class="line x" title="151:193	Some exampies: Nouns in cluster 0 are heads of larger noun phrases, whereas the nouns in cluster 1 are fullfledged NPs." ></td>
	<td class="line x" title="152:193	The members of classes 29 and 111 function as subjects." ></td>
	<td class="line x" title="153:193	Class 49 consists of proper nouns." ></td>
	<td class="line x" title="154:193	However, there are many pairs or triples of clusters that should be collapsed into one on linguistic grounds." ></td>
	<td class="line x" title="155:193	They were separated on distributional criteria that don't have linguistic correlates." ></td>
	<td class="line x" title="156:193	An analysis of the divergence between our classification and the manually assigned tags revealed three main sources of errors: rare words and rare syntactic phenomena, indistinguishable distribution, and non-local dependencies." ></td>
	<td class="line x" title="157:193	Rare words are difficult because of lack of distributional evidence." ></td>
	<td class="line x" title="158:193	For example, 'ties' is used as a verb only 2 times (out of 15 occurrences in the corpus)." ></td>
	<td class="line x" title="159:193	Both occurrences are miscategorized, since its context vectors do not provide enough evidence for the verbal use." ></td>
	<td class="line x" title="160:193	Rare syntactic constructions pose a related problem: There are not enough instances to justify the creation of a separate cluster." ></td>
	<td class="line x" title="161:193	For example, verbs taking bare in145 recall ~ CC CD DT IN ING MD N POS PRP RB TO VB VBD VBN WDT avg." ></td>
	<td class="line x" title="162:193	108532 36808 15084 129626 132079 14753 13498 231424 5086 47686 54524 25196 35342 80058 41145 14093 42 2 1 6 11 4 2 68 2 7 16 1 8 17 11 2 tag frequency ~ classes precision 24743 0.78 1501 0.95 809 0.48 6178 0.95 25316 0.83 4876 0.39 936 0.93 51695 0.80 533 0.90 12759 0.78 17403 0.64 61 1.00 6152 0.83 8663 0.88 11972 0.68 1017 0.61 0.78 F 0.79 0.86 0.09 0.94 0.89 0.27 0.95 0.85 0.90 0.85 0.60 0.96 0.83 0.84 0.65 0.19 0.72 Table 4: Precision and recall for induction based on word type and context." ></td>
	<td class="line x" title="163:193	tag ADN CC CD DT IN ING MD N POS PRP RB TO VB VBD VBN WDT avg." ></td>
	<td class="line x" title="164:193	~equency 108586 36808 15085 129626 132079 14753 13498 231434 5086 47686 54524 25196 35342 80058 41145 14093 classes 50 4 3 10 8 2 3 70 2 5 9 1 7 15 10 1 ~incorrect 3707 I 120968 I 123516 I 3798 I 13175 I 201890 I 4932 \] 37535 \] 29892 I 25181 I 28879 I 66457 I 26960 I precision 26790 0.77 6430 0.84 1530 0.71 5780 0.95 22070 0.85 7161 0.35 1059 0.93 33206 0.86 1636 0.75 9221 0.80 18398 0.62 27 1.00 6560 0.81 12079 0.85 17356 0.61 563 0.80 0.78 ~F 0  8-------~ 0.88 0.36 0.94 0.89 0.30 0.95 0.87 0.85 0.79 0.58 1.00 I 1.00 0.82 I 0.82 0.83 I 0.84 0.66 \[ 0.63 0.26 o.73 I0.74 Table 5: Precision and recall for induction based on generalized context vectors." ></td>
	<td class="line x" title="165:193	tag ADN 63771 CC 16148 CD 7011 DT 87914 IN 91950 ING 7268 MD 11244 N 111368 POS 3202 PRP 23946 RB 32331 TO 19859 VB 26714 VBD 56540 VBN 24804 WDT 8329 avg." ></td>
	<td class="line x" title="166:193	frequency ~ classes 36 4 1 9 9 2 3 49 i 7 16 2 11 33 14 3 __~ incorrect 12203 1798 918 2664 6842 1412 476 14452 precision ~ __ 0.82 ~--0-~-~ -0.90 I 0.97 0.67 I 0.26 0.97 I 0.94 093 I 094 0.47 I 0.17 0.96 I 0.92 0.87 I 0.90 I 0.91 I 0.96 I 0.65 Io.98 I 0.90 \] 0.90 I 0.76 Io.78 255 0.92 4062 0.85 9922 0.68 53 1.00 4119 0.85 8488 0.86 7448 0.72 670 0.85 0.83 Table 6: Precision and recall for induction for natural contexts." ></td>
	<td class="line x" title="167:193	F 0.83 0.93 0.38 0.95 0.94 0.25 0.94 0.89 0.91 0.90 0.66 0.99 0.88 0.88 0.74 0.58 0.79 146 finitives were classified as adverbs since this is too rare a phenomenon to provide strong distributional evidence ('we do not DARE speak of', 'legislation could HELP remove')." ></td>
	<td class="line x" title="168:193	The case of the tags 'VBN' and 'PRD' (past participles and predicative adjectives) demonstrates the difficulties of word classes with indistinguishable distributions." ></td>
	<td class="line x" title="169:193	There are hardly any distributional clues for distinguishing 'VBN' and 'PRD' since both are mainly used as complements of 'to be'.s A common tag class was created for 'VBN' and 'PRD' to show that they are reasonably well distinguished from other parts of speech, even if not from each other." ></td>
	<td class="line x" title="170:193	Semantic understanding is necessary to distinguish between the states described by phrases of the form 'to be adjective' and the processes described by phrases of the form 'to be past participle'." ></td>
	<td class="line x" title="171:193	Finally, the method fails if there are no local dependencies that could be used for categorization and only non-local dependencies are informative." ></td>
	<td class="line x" title="172:193	For example, the adverb in 'Mc*N. Hester, CURRENTLY Dean of' and the conjunction in 'to add that, IF United States policies ' have similar immediate neighbors (comma, NP)." ></td>
	<td class="line x" title="173:193	The decision to consider only immediate neighbors is responsible for this type of error since taking a wider context into account would disambiguate the parts of speech in question." ></td>
	<td class="line x" title="174:193	5 Future Work There are three avenues of future research we are interested in pursuing." ></td>
	<td class="line x" title="175:193	First, we are planning to apply the algorithm to an as yet untagged language." ></td>
	<td class="line x" title="176:193	Languages with a rich morphology may be more difficult than English since with fewer tokens per type, there is less data on which to base a categorization decision." ></td>
	<td class="line x" title="177:193	Secondly, the error analysis suggests that considering non-local dependencies would improve results." ></td>
	<td class="line x" title="178:193	Categories that can be induced well (those characterized by local dependencies) could be input into procedures that learn phrase structure (e.g.(Brill and Marcus, 19925; Finch, 1993))." ></td>
	<td class="line x" title="180:193	These phrase constraints could then be incorporated into the distributional tagger to characterize non-local dependencies." ></td>
	<td class="line x" title="181:193	Finally, our procedure induces a 'hard' part-ofspeech classification of occurrences in context, i.e., each occurrence is assigned to only one category." ></td>
	<td class="line x" title="182:193	It is by no means generally accepted that such a classification is linguistically adequate." ></td>
	<td class="line x" title="183:193	There is both synchronic (Ross, 1972) and diachronic (Tabor, 1994) evidence suggesting that words and their uses can inherit properties from several prototypical syntactic categories." ></td>
	<td class="line x" title="184:193	For example, 'fun' SBecause of phrases like 'I had sweet potatoes', forms of 'have' cannot serve as a reliable discriminator either." ></td>
	<td class="line x" title="185:193	in 'It's a fun thing to do'." ></td>
	<td class="line x" title="186:193	has properties of both a noun and an adjective (superlative 'funnest' possible)." ></td>
	<td class="line x" title="187:193	We are planning to explore 'soft' classification algorithms that can account for these phenomena." ></td>
	<td class="line x" title="188:193	6 Conclusion In this paper, we have attempted to construct an algorithm for fully automatic distributional tagging, using unannotated corpora as the sole source of information." ></td>
	<td class="line x" title="189:193	The main innovation is that the algorithm is able to deal with part-of-speech ambiguity, a pervasive phenomenon in natural language that was unaccounted for in previous work on learning categories from corpora." ></td>
	<td class="line x" title="190:193	The method was systematically evaluated on the Brown corpus." ></td>
	<td class="line x" title="191:193	Even if no automatic procedure can rival the accuracy of human tagging, we hope that the algorithm will facilitate the initial tagging of texts in new languages and sublanguages." ></td>
	<td class="line x" title="192:193	7 Acknowledgments I am grateful for helpful comments to Steve Finch, Jan Pedersen and two anonymous reviewers (from ACL and EACL)." ></td>
	<td class="line x" title="193:193	I'm also indebted to Michael Berry for SVDPACK and to the Penn Treebank Project for the parsed Brown corpus." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E95-1021
Tagging French - Comparing A Statistical And A Constraint-Based Method
Chanod, Jean-Pierre;Tapanainen, Pasi;"></td>
	<td class="line x" title="1:227	Tagging Frenchcomparing a statistical and a constraint-based method Jean-Pierre Chanod and Pasi Tapanainen Rank Xerox Research Centre, Grenoble Laboratory 6, chemin de Maupertuis, 38240 Meylan, France Jean." ></td>
	<td class="line x" title="2:227	Pierre." ></td>
	<td class="line x" title="3:227	Chanod, Pasi." ></td>
	<td class="line x" title="4:227	TapanainenOxerox." ></td>
	<td class="line x" title="5:227	fr Abstract In this paper we compare two competing approaches to part-of-speech tagging, statistical and constraint-based disambiguation, using French as our test language." ></td>
	<td class="line x" title="6:227	We imposed a time limit on our experiment: the amount of time spent on the design of our constraint system was about the same as the time we used to train and test the easy-to-implement statistical model." ></td>
	<td class="line x" title="7:227	We describe the two systems and compare the results." ></td>
	<td class="line x" title="8:227	The accuracy of the statistical method is reasonably good, comparable to taggers for English." ></td>
	<td class="line x" title="9:227	But the constraint-based tagger seems to be superior even with the limited time we allowed ourselves for rule development." ></td>
	<td class="line x" title="10:227	1 Overview In this paper 1 we compare two competing approaches to part-of-speech tagging, statistical and constraint-based disambiguation, using French as our test language." ></td>
	<td class="line x" title="11:227	The process of tagging consists of three stages: tokenisation, morphological analysis and disambiguation." ></td>
	<td class="line x" title="12:227	The two taggers include the same tokeniser and morphological analyser." ></td>
	<td class="line x" title="13:227	The tokeniser uses a finite-state transducer that reads the input and outputs a token whenever it has read far enough to be sure that a token is detected." ></td>
	<td class="line x" title="14:227	The morphological analYser contains a transducer lexicon." ></td>
	<td class="line x" title="15:227	It produces all the legitimate tags for words that appear in the lexicon." ></td>
	<td class="line x" title="16:227	If a word is not in the lexicon, a guesser is consulted." ></td>
	<td class="line x" title="17:227	The guesser employs another finite-state transducer." ></td>
	<td class="line x" title="18:227	It reads a token and prints out a set of tags depending on prefixes, inflectional information and productive endings that it finds." ></td>
	<td class="line x" title="19:227	We make even more use of transducers in the constraint-based tagger." ></td>
	<td class="line x" title="20:227	The tagger reads one sentence at a time, a string of words and alternative tags, feeds them to the grammatical transduc1There is a \]onger version (17 pages) of this paper in (Chanod and Tapanainen, 1994) ers that remove all but one alternative tag from all the words on the basis of contextual information." ></td>
	<td class="line x" title="21:227	If all the transducers described above (tokeniser, morphological analyser and disambiguatot) could be composed together, we would get one single transducer that transforms a raw input text to a fully disambiguated output." ></td>
	<td class="line x" title="22:227	The statistical method contains the same tokeniser and morphological analyser." ></td>
	<td class="line x" title="23:227	The disambiguation method is a conventional one: a hidden Markov model." ></td>
	<td class="line x" title="24:227	2 Morphological analysis and guessing The morphological analyser is based on a lexical transducer (Karttunen et al. , 1992)." ></td>
	<td class="line x" title="25:227	The transducer maps each inflected surface form of a word to its canonical lexical form followed by the appropriate morphological tags." ></td>
	<td class="line x" title="26:227	Words not found in the lexicon are analysed by a separate finite-state transducer, the guesser." ></td>
	<td class="line x" title="27:227	We developed a simple, extremely compact and efficient guesser for French." ></td>
	<td class="line x" title="28:227	It is based on the general assumption that neologisms and uncommon words tend to follow regular inflectional patterns." ></td>
	<td class="line x" title="29:227	The guesser is thus based on productive endings (like merit for adverbs, ible for adjectives, er for verbs)." ></td>
	<td class="line x" title="30:227	A given ending may of course point to various categories, e.g. er identifies nouns as well as verbs due to possible borrowings from English." ></td>
	<td class="line oc" title="31:227	3 The statistical model We use the Xerox part-of-speech tagger (Cutting et al. , 1992), a statistical tagger made at the Xerox Palo Alto Research Center." ></td>
	<td class="line pc" title="32:227	3.1 Training The Xerox tagger is claimed (Cutting el al. , 1992) to be adaptable and easily trained; only a lexicon and suitable amount of untagged text is required." ></td>
	<td class="line x" title="33:227	A new language-specific tagger can therefore be built with a minimal amount of work." ></td>
	<td class="line x" title="34:227	We started our project by doing so." ></td>
	<td class="line x" title="35:227	We took our lexicon with the new tagset, a corpus of French text, and 149 trained the tagger." ></td>
	<td class="line o" title="36:227	We ran the tagger on another text and counted the errors." ></td>
	<td class="line n" title="37:227	The result was not good; 13 % of the words were tagged incorrectly." ></td>
	<td class="line o" title="38:227	The tagger does not require a tagged corpus for training, but two types of biases can be set to tell the tagger what is correct and what is not: symbol biases and transition biases." ></td>
	<td class="line x" title="39:227	The symbol biases describe what is likely in a given ambiguity class." ></td>
	<td class="line x" title="40:227	They represent kinds of lexical probabilities." ></td>
	<td class="line x" title="41:227	The transition biases describe the likelihood of various tag pairs occurring in succession." ></td>
	<td class="line x" title="42:227	The biases serve as initial values before training." ></td>
	<td class="line o" title="43:227	We spent approximately one man-month writing biases and tuning the tagger." ></td>
	<td class="line x" title="44:227	Our training corpus was rather small, because the training had to be repeated frequently." ></td>
	<td class="line o" title="45:227	When it seemed that the results could not be further improved, we tested the tagger on a new corpus." ></td>
	<td class="line x" title="46:227	The eventual result was that 96.8 % of the words in the corpus were tagged correctly." ></td>
	<td class="line x" title="47:227	This result is about the same as for statistical tuggers of English." ></td>
	<td class="line x" title="48:227	3.2 Modifying the biases A 4 % error rate is not generally considered a negative result for a statistical tagger, but some of the errors are serious." ></td>
	<td class="line x" title="49:227	For example, a sequence of determiner , noun , noun/verbpreposition is frequently disambiguated in the wrong way, e.g. Le ~rain part ~t cinq heures (The ~rain leaves a~ 5 o'clock)." ></td>
	<td class="line x" title="50:227	The word part is ambiguous between a noun and a verb (singular, third person), and it is disambiguated incorrectly." ></td>
	<td class="line x" title="51:227	The tagger seems to prefer the noun reading between a singular noun and a preposition." ></td>
	<td class="line x" title="52:227	One way to resolve this is to write new biases." ></td>
	<td class="line x" title="53:227	We added two new ones." ></td>
	<td class="line x" title="54:227	The first one says that a singular noun is not likely to be followed by a noun (this is not always true but we could call this a tendency)." ></td>
	<td class="line x" title="55:227	The second states that a singular noun is likely to be followed by a singular, third-person verb." ></td>
	<td class="line x" title="56:227	The result was that the problematic sentence was disambiguated correctly, but the changes had a bad side effect." ></td>
	<td class="line x" title="57:227	The overall error rate of the tagger increased by over 50 %." ></td>
	<td class="line x" title="58:227	This illustrates how difficult it is to write good biases." ></td>
	<td class="line x" title="59:227	Getting a correct result for a particular sentence does not necessarily increase the overall success rate." ></td>
	<td class="line x" title="60:227	4 The constraint-based model 4.1 A two-level model for tagging In the constraint-based tagger, the rules are represented as finite-state transducers." ></td>
	<td class="line x" title="61:227	The transducers are composed with the sentence in a sequence." ></td>
	<td class="line x" title="62:227	Each transducer may remove, or in principle it may also change, one or more readings of the words." ></td>
	<td class="line x" title="63:227	After all the transducers have been applied, each word in the sentence has only one analysis." ></td>
	<td class="line x" title="64:227	Our constraint-based tagger is based on techniques that were originally developed for morphological analysis." ></td>
	<td class="line x" title="65:227	The disambiguation rules are similar to phonological rewrite rules (Kaplan and Kay, 1994), and the parsing algorithm is similar to the algorithm for combining the morphological rules with the lexicon (Karttunen, 1994)." ></td>
	<td class="line x" title="66:227	The tagger has a close relative in (Koskenniemi, 1990; Koskenniemi et al. , 1992; Voutilalnen and Tapanainen, 1993) where the rules are represented as finite-state machines that are conceptually intersected with each other." ></td>
	<td class="line x" title="67:227	In this tagger the disambiguation rules are applied in the same manner as the morphological rules in (Koskenniemi, 1983)." ></td>
	<td class="line x" title="68:227	Another relative is represented in (Roche and Schabes, 1994) which uses a single finitestate transducer to transform one tag into another." ></td>
	<td class="line x" title="69:227	A constraint-based system is also presented in (Karlsson, 1990; Karlsson et al. , 1995)." ></td>
	<td class="line x" title="70:227	Related work using finite-state machines has been done using local grammars (Roche, 1992; Silberztein, 1993; Laporte, 1994)'." ></td>
	<td class="line x" title="71:227	4.2 Writing the rules 4.2.1 Studying ambiguities One quick experiment that motivated the building of the constraint-based model was the following: we took a million words of newspaper text and ranked ambiguous words by frequency." ></td>
	<td class="line x" title="72:227	We found that a very limited set of word forms covers a large part of the total ambiguity." ></td>
	<td class="line x" title="73:227	The 16 most frequent ambiguous word forms 2 account for 50 % of all ambiguity." ></td>
	<td class="line x" title="74:227	Two thirds of the ambiguity are due to the 97 most frequent ambiguous words 3." ></td>
	<td class="line x" title="75:227	Another interesting observation is that the most frequent ambiguous words are usually words which are in general corpus-independent, i.e. words that belong to closed classes (determiners, prepositions, pronouns, conjunctions), auxiliaries, common adverbials or common verbs, like faire (to do, to make)." ></td>
	<td class="line x" title="76:227	The first corpus-specific word is in the 41st position." ></td>
	<td class="line x" title="77:227	4.2.2 Principled rules For the most frequent ambiguous word forms, one may safely define principled contextual restrictions to resolve ambiguities." ></td>
	<td class="line x" title="78:227	This is in particular the case for clitic/determiner ambiguities attached to words like le or la. Our rule says that clitic pronouns are attached to a verb and determiners to a noun with possibly an unrestricted number of premodifiers." ></td>
	<td class="line x" title="79:227	This is a good starting point although some ambiguity remains as in la 2Namely de, la, le, les, des, en, du, un, a, duns, une, pus, est, plus, Le, son 3 A similar experiment shows that in the Brown corpus 63 word forms cover 50 % of all the ambiguity, and two thirds of the ambiguity is covered by 220 word forms." ></td>
	<td class="line x" title="80:227	150 place, which can be read as a determiner-noun or clitic-verb sequence." ></td>
	<td class="line x" title="81:227	Some of the very frequent words have categories that are rare, for instance the auxiliary forms a and est can also be nouns and the pronoun cela is also a very rare verb form." ></td>
	<td class="line x" title="82:227	In such a case, we restrict the use of the rarest categories to contexts where the most frequent reading is not at all possible, otherwise the most frequent reading is preferred." ></td>
	<td class="line x" title="83:227	For instance, the word avions may be a noun or an auxiliary verb." ></td>
	<td class="line x" title="84:227	We prefer the noun reading and accept the verb reading only when the first-person pronoun nous appears in the left cor/text, e.g. as in nous ne les avions pas (we did not have them)." ></td>
	<td class="line x" title="85:227	This means that the tagger errs only when a rare reading should be chosen in a context where the most common reading is still acceptable." ></td>
	<td class="line x" title="86:227	This may never actually occur, depending on how accurate the contextual restrictions are." ></td>
	<td class="line x" title="87:227	It can even be the case that discarding the rare readings would not induce a detectable loss in accuracy, e.g. in the conflict between cela as a pronoun and as a verb." ></td>
	<td class="line x" title="88:227	The latter is a rarely used tense of a rather literary verb." ></td>
	<td class="line x" title="89:227	The principled rules do not require any tagged corpus, and should be thus corpus-independent." ></td>
	<td class="line x" title="90:227	The rules are based on a short list of extremely common words (fewer than 100 words)." ></td>
	<td class="line x" title="91:227	4.2.3 Heuristics The rules described above are certainly not sufficient to provide full disambiguation, even if one considers only the most ambiguous word forms." ></td>
	<td class="line x" title="92:227	We need more rules for cases that the principled rules do not disambiguate." ></td>
	<td class="line x" title="93:227	Some ambiguity is extremely difficult to resolve using the information available." ></td>
	<td class="line x" title="94:227	A very problematic case is the word des, which can either be a determiner, Jean mange des pommes (Jean eats apples) or an amalgamated preposition-determiner, as in Jean aime le bruit des vagues (Jean likes the sound of waves)." ></td>
	<td class="line x" title="95:227	Proper treatment of such an ambiguity would require verb subcategorisation and a description of complex coordinations of noun and prepositional phrases." ></td>
	<td class="line x" title="96:227	This goes beyond the scope of both the statistical and the constraint-based taggers." ></td>
	<td class="line x" title="97:227	For such cases we introduce ad-hoc heuristics." ></td>
	<td class="line x" title="98:227	Some are quite reasonable, e.g. the determiner reading of des is preferred at the begining of a sentence." ></td>
	<td class="line x" title="99:227	Some are more or less arguable, e.g. the prepositional reading is preferred after a noun." ></td>
	<td class="line x" title="100:227	One may identify various contexts in which either the noun or the adjective can be preferred." ></td>
	<td class="line x" title="101:227	Such contextual restrictions (Chanod, 1993) are not always true, but may be considered reasonable for resolving the ambiguity." ></td>
	<td class="line x" title="102:227	For instance, in the case of two successive noun/adjective ambiguities like le franc fort (the strong franc or the frank fort), we favour the noun-adjective sequence except when the first word is a common prenominal adjective such as bon, petit, grand, premier, as in le petit fort (the small fort) or even le bon petit (the good little one)." ></td>
	<td class="line x" title="103:227	4.2.4 Non-contextual rules Our heuristics do not resolve all the ambiguity." ></td>
	<td class="line x" title="104:227	To obtain the fully unambiguous result we make use of non-contextual heuristics." ></td>
	<td class="line x" title="105:227	The noncontextual rules may be thought of as lexical probabilities." ></td>
	<td class="line x" title="106:227	We guess what the most probable tag is in the remaining ambiguities." ></td>
	<td class="line x" title="107:227	For instance, preposition is preferred to adjective, pronoun is preferred to past participle, etc. The rules are obviously not very reliable, but they are needed only when the previous rules fail to fully disambiguate." ></td>
	<td class="line x" title="108:227	4.2.5 Current rules The current system contains 75 rules, consisting of:  39 reliable contextual rules dealing mostly with frequent ambiguous words." ></td>
	<td class="line x" title="109:227	 25 rules describing heuristics with various degrees of linguistic generality." ></td>
	<td class="line x" title="110:227	 11 non-contextual rules for the remaining ambiguities." ></td>
	<td class="line x" title="111:227	The rules were constructed in less than one month, on the basis of 50 newspaper sentences." ></td>
	<td class="line x" title="112:227	All the rules are currently represented by 11 transducers." ></td>
	<td class="line x" title="113:227	5 The results 5.1 Test A For evaluation, we used a corpus totally unrelated to the development corpus." ></td>
	<td class="line x" title="114:227	It contains 255 sentences (5752 words) randomly selected from a corpus of economic reports." ></td>
	<td class="line x" title="115:227	About 54 % of the words are ambiguous." ></td>
	<td class="line x" title="116:227	The text is first tagged manually without using the disambiguators, and the output of the tagger is then compared to the hand-tagged result." ></td>
	<td class="line x" title="117:227	If we apply all the rules, we get a fully disambiguated result with an error rate of only 1.3 %." ></td>
	<td class="line x" title="118:227	This error rate is much lower than the one we get using the hidden Markov model (3.2 %)." ></td>
	<td class="line x" title="119:227	See Figure 1." ></td>
	<td class="line x" title="120:227	We can also restrict the tagger to using only the most reliable rules." ></td>
	<td class="line x" title="121:227	Only 10 words lose the correct tag when almost 2000 out of 3085 ambiguous words are disambiguated." ></td>
	<td class="line x" title="122:227	Among the remaining 1136 ambiguous words about 25 % of the ambiguity is due to determiner/preposition ambiguities (words like dn and des), 30 % are adjective/noun ambiguities and 18 % are noun/verb ambiguities." ></td>
	<td class="line x" title="123:227	If we use both the principled and heuristic rules, the error rate is 0.52 % while 423 words remain ambiguous." ></td>
	<td class="line x" title="124:227	The non-contextual rules that eliminate the remaining 423 ambiguities produce an 151 error rate (correctness) Lexicon + Guesser 0.03 % (99.97 %) 54 % Hidden Markov model 3.2 % (96.8 %) 0 % Principled rules 0.17 % (99.83 %) 20 % Principled and heuristic rules 0.52 % (99.48 %) I 7 % All the rules I 1.3% (98.7 %) I 0% remaining ambiguity tag / word 1.64 1.00 1.24 1.09 1.00 Figure 1: The result in the test sample additional 43 errors." ></td>
	<td class="line x" title="125:227	Overall, 98.7 % of the words receive the correct tag." ></td>
	<td class="line x" title="126:227	5.2 Test B We also tested the tuggers with more difficult text." ></td>
	<td class="line x" title="127:227	The 12 000 word sample of newspaper text has typos and proper names 4 that match an existing word in the lexicon." ></td>
	<td class="line x" title="128:227	Problems of the latter type are relatively rare but this sample was exceptional." ></td>
	<td class="line x" title="129:227	Altogether the lexicon mismatches produced 0.5 % errors to the input of the tuggers." ></td>
	<td class="line x" title="130:227	The results are shown in Figure 2." ></td>
	<td class="line x" title="131:227	This text also seems to be generally more difficult to parse than the first one." ></td>
	<td class="line x" title="132:227	5.3 Combination of the tuggers We also tried combining the tuggers, using first the rules and then the statistics (a similar approach was also used in (Tapanainen and Voutilainen, 1994))." ></td>
	<td class="line x" title="133:227	We evaluated the results obtained by the following sequence of operations: 1) Running the constraint-based tagger without the final, non-contextual rules." ></td>
	<td class="line x" title="134:227	2) Using the statistical disambiguator independently." ></td>
	<td class="line x" title="135:227	We select the tag proposed by the statistical disambiguator if it is not removed during step 1." ></td>
	<td class="line x" title="136:227	3) Solving the remaining ambiguities by running the final non-contextual rules of the constraint-based tagger." ></td>
	<td class="line x" title="137:227	This last step ensures that one gets a fully disambiguated text." ></td>
	<td class="line x" title="138:227	Actually only about 0.5 % of words were not fully disambiguated after step 2." ></td>
	<td class="line x" title="139:227	We used the test sample B. After the first step, 1400 words out of 12 000 remain ambiguous." ></td>
	<td class="line x" title="140:227	The process of combining the three steps described above eventually leads to more errors than running the constraint-based tagger alone." ></td>
	<td class="line x" title="141:227	The statistical tagger introduces 220 errors on the 1400 words that remain ambiguous after step 1." ></td>
	<td class="line x" title="142:227	In comparison, the final set of non-contextual rules introduces around 150 errors on the same set of 1400 words." ></td>
	<td class="line x" title="143:227	We did not expect this result." ></td>
	<td class="line x" title="144:227	One possible explanation for the superior performance of the final non-contextual rules is that they are meant to apply after the previous rules failed to disambiguate the word." ></td>
	<td class="line x" title="145:227	This is in itself useful 4like Bats, Botta, Ddrnis, Ferrasse, Hersant,  information." ></td>
	<td class="line x" title="146:227	The final heuristics favour tags that have survived all conditions that restrict their use." ></td>
	<td class="line x" title="147:227	For instance, the contextual rules define various contexts where the preposition tag for des is preferred." ></td>
	<td class="line x" title="148:227	Therefore, the final heuristics favours the determiner reading for des." ></td>
	<td class="line x" title="149:227	6 Analysis of errors 6.1 Errors of principled and heuristic rules Let us now consider what kind of errors the constraint-based tagger produced." ></td>
	<td class="line x" title="150:227	We do not deal with errors produced by the last set of rules, the non-contextual rules, because it is already known that they are not very accurate." ></td>
	<td class="line x" title="151:227	To make the tagger better, they should be replaced by writing more accurate heuristic rules." ></td>
	<td class="line x" title="152:227	We divide the errors into three categories: (1) errors due to multi-word expressions, (2) errors that should/could be resolved and (3) errors that are hard to resolve by using the information that is available." ></td>
	<td class="line x" title="153:227	Thefirst group (15 errors), the multi-word expressions, are difficult for the syntax-based rules because in many cases the expression does not follow any conventional syntactic structure, or the structure may be very rare." ></td>
	<td class="line x" title="154:227	In multi-word expressions some words also have categories that may not appear anywhere else." ></td>
	<td class="line x" title="155:227	The best way to handle them is to lexicalise these expressions." ></td>
	<td class="line x" title="156:227	When a possible expression is recognised we can either collapse it into one unit or leave it otherwise intact except that the most 'likely' interpretation is marked." ></td>
	<td class="line x" title="157:227	The biggest group (41 errors) contains errors that could have been resolved correctly but were not." ></td>
	<td class="line x" title="158:227	The reason for this is obvious: only a relatively small amount of time was allowed for writing the rules." ></td>
	<td class="line x" title="159:227	In addition, the rules were constructed on the basis of a rather small set of example sentences." ></td>
	<td class="line x" title="160:227	Therefore, it would be very surprising if such errors did not appear in the test sample taken from a different source." ></td>
	<td class="line x" title="161:227	The errors are the following:  The biggest subgroup has 19 errors that require modifications to existing rules." ></td>
	<td class="line x" title="162:227	Our rules were meant to handle such cases but fail 152 ' error rate remaining tag / word (correctness) ambiguity Lexicon + Guesser 0.5 % (99.5 %) 48 % 1.59 Hidden Markov model 5.0 % (95.0 %) 0 % 1.00 Principled rules I 0.8 % (99.2 %) 23 % 1.29 Principled and heuristic rules \] 1.3 % (98.7 %) 12 % 1.14 All the rules \[ 2.5 % (97.5 %) 0 % 1.00 Figure 2: The result in a difficult test sample with many lexicon mismatches to do so correctly in some sentences." ></td>
	<td class="line x" title="163:227	Often only a minor correction is needed." ></td>
	<td class="line x" title="164:227	 Some syntactic constructions, or word sequences, were omitted." ></td>
	<td class="line x" title="165:227	This caused 7 errors which could easily be avoided by writing more rules." ></td>
	<td class="line x" title="166:227	For instance, a construction like 'preposition  clitic + finite verb' was not forbidden." ></td>
	<td class="line x" title="167:227	The phrase h l'est was analysed in this way while the correct analysis is 'preposition  determiner + noun'." ></td>
	<td class="line x" title="168:227	 Sometimes a little bit of extra lexical information is required." ></td>
	<td class="line x" title="169:227	Six errors would require more information or the kind of refinement in the tag inventory that would not have been appropriate for the statistical tagger." ></td>
	<td class="line x" title="170:227	 Nine errors could be avoided by refining existing heuristics, especially by taking into account exceptions for specific words like point, pendant and devant." ></td>
	<td class="line x" title="171:227	The remaining errors (28 errors) constitute the price we pay for using the heuristics." ></td>
	<td class="line x" title="172:227	Removing the rules which fail would cause a lot of ambiguity to remain." ></td>
	<td class="line x" title="173:227	The errors are the following:  Fifteen errors are due to the heuristics for de and des." ></td>
	<td class="line x" title="174:227	There is little room for improvement at this level of description (see Chapter 4.2.3)." ></td>
	<td class="line x" title="175:227	However, the current, simple heuristics fully disambiguate 850 instances of de and des out of 914 i.e. 92 % of all the occurrences were parsed with less than a 2 % error rate." ></td>
	<td class="line x" title="176:227	 Six errors involve noun-adjective ambiguities that are difficult to solve, for instance, in a subject or object predicate position." ></td>
	<td class="line x" title="177:227	 Seven errors seem to be beyond reach for various reasons: long coordination, rare constructions, etc. An example is les boltes (the boxes) where les is wrongly tagged in the test sample because the noun form is misspelled as boites, which is identified only as a verb by the lexicon." ></td>
	<td class="line x" title="178:227	6.2 Difference between the taggers We also investigated how the errors compare between the two taggers." ></td>
	<td class="line x" title="179:227	Here we used the fully disambiguated outputs of the taggers." ></td>
	<td class="line x" title="180:227	The errors belong mainly to three classes: * Some errors appear predominantly with the statistical tagger and almost never with the constraint-based tagger." ></td>
	<td class="line x" title="181:227	This is particularly the case with the ambiguity between past participles and adjectives." ></td>
	<td class="line x" title="182:227	 Some errors are common to both taggers, the constraint-based tagger generally being more accurate (often with a ratio of I to 2)." ></td>
	<td class="line x" title="183:227	These errors cover ambiguities that are known to be difficult to handle in general, such as the already mentioned determiner/preposition ambiguity." ></td>
	<td class="line x" title="184:227	 Finally, there are errors that are specific to the constraint-based tagger." ></td>
	<td class="line x" title="185:227	They are often related to errors that could be corrected with some extra work." ></td>
	<td class="line x" title="186:227	They are relatively infrequent, thus the global accuracy of the constraint-based tagger remains higher." ></td>
	<td class="line x" title="187:227	The first two classes of errors are generally difficult to correct." ></td>
	<td class="line x" title="188:227	The easiest way to improve the constraint-based tagger is to concentrate on the final class." ></td>
	<td class="line x" title="189:227	As we mentioned earlier, it is not very easy to change the behaviour of the statistical tagger in one place without some side-effects elsewhere." ></td>
	<td class="line x" title="190:227	This means that the errors of the first class are probably easiest to resolve by means other than statistics." ></td>
	<td class="line x" title="191:227	The first class is quite annoying for the statistical parser because it contains errors that are intuitively very clear and resolvable, but which are far beyond the limits of the current statistical tagger." ></td>
	<td class="line x" title="192:227	We can take an easy sentence to demonstrate this: Je ne le pense pas." ></td>
	<td class="line x" title="193:227	I do not think so." ></td>
	<td class="line x" title="194:227	Tune le penses pas." ></td>
	<td class="line x" title="195:227	You do not think so." ></td>
	<td class="line x" title="196:227	Il ne le pense pas." ></td>
	<td class="line x" title="197:227	He does not think so." ></td>
	<td class="line x" title="198:227	The verb pense is ambiguous 5 in the first person or in the third person." ></td>
	<td class="line x" title="199:227	It is usually easy to determine the person just by checking the personal pronoun nearby." ></td>
	<td class="line x" title="200:227	For a human or a constraint-based tagger this is an easy task, for a statistical tagger it is not." ></td>
	<td class="line x" title="201:227	There are two words between the pronoun and the verb that do not carry any information about the person." ></td>
	<td class="line x" title="202:227	The personal pronoun may thus be too far from the verb because bi-gram models can see backward no farther than le, and tri-gram models SThat is not case with all the French verbs, e.g. Je crois and //croit." ></td>
	<td class="line x" title="203:227	153 no farther than ne le." ></td>
	<td class="line x" title="204:227	Also, as mentioned earlier, resolving the adjective vs. past participle ambiguity is much harder, if the tagger does not know whether there is an auxiliary verb in the sentence or not." ></td>
	<td class="line x" title="205:227	7 Conclusion We have presented two taggers for french: a statistical one and a constraint-based one." ></td>
	<td class="line x" title="206:227	There are two ways to train the statistical tagger: from a tagged corpus or using a selforganising method that does not need a tagged corpus." ></td>
	<td class="line x" title="207:227	We had a strict time limit of one month for doing the tagger and no tagged corpus was available." ></td>
	<td class="line x" title="208:227	This is a short time for the manual tagging of a corpus and for the training of the tagger." ></td>
	<td class="line x" title="209:227	It would be risky to spend, say, three weeks for writing a corpus, and only one week for training." ></td>
	<td class="line x" title="210:227	The size of corpus would have to be limited, because it should be also checked." ></td>
	<td class="line o" title="211:227	We selected the Xerox tagger that learns from an untagged corpuS." ></td>
	<td class="line x" title="212:227	The task was not as straigthforward as we thought." ></td>
	<td class="line x" title="213:227	Without human assistance in the training the result was not impressive, and we had to spend much time tuning the tagger and guiding the learning process." ></td>
	<td class="line x" title="214:227	In a month we achieved 95-97 % accuracy." ></td>
	<td class="line x" title="215:227	The training process of a statistical tagger requires some time because the linguistic information has to be incorporated into the tagger one way or another, it cannot be obtained for free starting from null." ></td>
	<td class="line x" title="216:227	Because the linguistic information is needed, we decided to encode the information in a more straightforward way, as explicit linguistic disambiguation rules." ></td>
	<td class="line x" title="217:227	It has been argued that statistical taggers are superior to rulebased/hand-coded ones because of better accuracy and better adaptability (easy to train)." ></td>
	<td class="line x" title="218:227	In our experiment, both claims turned out to be wrong." ></td>
	<td class="line x" title="219:227	For the constraint-based tagger we set one month time limit for writing the constraints by hand." ></td>
	<td class="line x" title="220:227	We used only linguistic intuition and a very limited set of sentences to write the 75 constraints." ></td>
	<td class="line x" title="221:227	We formulated constraints of different accuracy." ></td>
	<td class="line x" title="222:227	Some of the constraints are almost 100 % accurate, some of them just describe tendencies." ></td>
	<td class="line x" title="223:227	Finally, when we thought that the rules were good enough, we took two text samples from different sources and tested both the taggers." ></td>
	<td class="line x" title="224:227	The constraint-based tagger made several naive errors because we had forgotten, miscoded or ignored some linguistic phenomena, but still, it made only half of the errors that the statistical one made." ></td>
	<td class="line x" title="225:227	A big difference between the taggers is that the tuning of the statistical tagger is very subtle i.e. it is hard to predict the effect of tuning the parameters of the system, whereas the constraint-based tagger is very straightforward to correct." ></td>
	<td class="line x" title="226:227	Our general conclusion is that the hand-coded constraints perform better than the statistical tagger and that we can still refine them." ></td>
	<td class="line x" title="227:227	The most important of our findings is that writing constraints that contain more linguistic information than the current statisticM model does not take much time." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E95-1022
A Syntax-Based Part-Of-Speech Analyser
Voutilainen, Atro;"></td>
	<td class="line x" title="1:171	A syntax-based part-of-speech analyser Atro Voutilainen Research Unit for Multilingual Language Technology P.O. Box 4 FIN-00014 University of Helsinki Finland Atro.Voutilainen@Helsinki.FI Abstract There are two main methodologies for constructing the knowledge base of a natural language analyser: the linguistic and the data-driven." ></td>
	<td class="line x" title="2:171	Recent state-ofthe-art part-of-speech taggers are based on the data-driven approach." ></td>
	<td class="line x" title="3:171	Because of the known feasibility of the linguistic rule-based approach at related levels of description, the success of the datadriven approach in part-of-speech analysis may appear surprising." ></td>
	<td class="line x" title="4:171	In this paper, a case is made for the syntactic nature of part-of-speech tagging." ></td>
	<td class="line x" title="5:171	A new tagger of English that uses only linguistic distributional rules is outlined and empirically evaluated." ></td>
	<td class="line x" title="6:171	Tested against a benchmark corpus of 38,000 words of previously unseen text, this syntax-based system reaches an accuracy of above 99%." ></td>
	<td class="line x" title="7:171	Compared to the 95-97% accuracy of its best competitors, this result suggests the feasibility of the linguistic approach also in part-of-speech analysis." ></td>
	<td class="line x" title="8:171	1 Introduction Part-of-speech analysis usually consists of (i) introduction of ambiguity (lexical analysis) and (ii) disambiguation (elimination of illegitimate alternatives)." ></td>
	<td class="line x" title="9:171	While introducing ambiguity is regarded as relatively straightforward, disambiguation is known to be a difficult and controversial problem." ></td>
	<td class="line x" title="10:171	There are two main methodologies: the linguistic and the data-driven." ></td>
	<td class="line x" title="11:171	 In the linguistic approach, the generalisations are based on the linguist's (potentially corpus-based) abstractions about the paradigms and syntagms of the language." ></td>
	<td class="line x" title="12:171	Distributional generalisations are manually coded as a grammar, a system of constraint rules used for discarding contextually illegitimate analyses." ></td>
	<td class="line x" title="13:171	The linguistic approach is labour-intensive: skill and effort is needed for writing an exhaustive grammar." ></td>
	<td class="line x" title="14:171	 In the data-driven approach, frequency-based information is automatically derived from corpora." ></td>
	<td class="line x" title="15:171	The learning corpus can consist of plain text, but the best results seem achievable with annotated corpora (Merialdo 1994; Elworthy 1994)." ></td>
	<td class="line oc" title="16:171	This corpus-based information typically concerns sequences of 1-3 tags or words (with some well-known exceptions, e.g. Cutting et al. 1992)." ></td>
	<td class="line x" title="17:171	Corpus-based information can be represented e.g. as neural networks (Eineborg and Gamb/~c k 1994; Schmid 1994), local rules (Brill 1992), or collocational matrices (Garside 1987)." ></td>
	<td class="line x" title="18:171	In the data-driven approach, no human effort is needed for rulewriting." ></td>
	<td class="line x" title="19:171	However, considerable effort may be needed for determining a workable tag set (cf.Cutting 1994) and annotating the training corpus." ></td>
	<td class="line x" title="21:171	At the first flush, the linguistic approach may seem an obvious choice." ></td>
	<td class="line x" title="22:171	A part-of-speech tagger's task is often illustrated with a noun-verb ambiguous word directly preceded by an unambiguous determiner (e.g. table in the table)." ></td>
	<td class="line x" title="23:171	This ambiguity can reliably be resolved with a simple and obvious grammar rule that disallows verbs after determiners." ></td>
	<td class="line x" title="24:171	Indeed, few contest the fact that reliable linguistic rules can be written for resolving some partof-speech ambiguities." ></td>
	<td class="line x" title="25:171	The main problem with this approach seems to be that resolving part-ofspeech ambiguities on a large scale, without introducing a considerable error margin, is very difficult at best." ></td>
	<td class="line x" title="26:171	At least, no rule-based system with a convincing accuracy has been reported so far." ></td>
	<td class="line x" title="27:171	1 As a rule, data-driven systems rely on statistical generalisations about short sequences of words or tags." ></td>
	<td class="line x" title="28:171	Though these systems do not usually employ information about long-distance phenom1There is one potential exception: the rule-based morphological disambiguator used in the English Constraint Grammar Parser ENGCG (Voutilainen, Heikkil and Anttila 1992)." ></td>
	<td class="line x" title="29:171	Its recall is very high (99.7% of all words receive the correct morphological analysis), but this system leaves 3-7% of all words ambiguous, trading precision for recall." ></td>
	<td class="line oc" title="30:171	157 ena or the linguist's abstraction capabilities (e.g. knowledge about what is relevant in the context), they tend to reach a 95-97% accuracy in the analysis of several languages, in particular English (Marshall 1983; Black et aL 1992; Church 1988; Cutting et al. 1992; de Marcken 1990; DeRose 1988; Hindle 1989; Merialdo 1994; Weischedel et al. 1993; Brill 1992; Samuelsson 1994; Eineborg and Gamb~ick 1994, etc.)." ></td>
	<td class="line x" title="31:171	Interestingly, no significant improvement beyond the 97% 'barrier' by means of purely data-driven systems has been reported so far." ></td>
	<td class="line x" title="32:171	In terms of the accuracy of known systems, the data-driven approach seems then to provide the best model of part-of-speech distribution." ></td>
	<td class="line x" title="33:171	This should appear a little curious because very competitive results have been achieved using the linguistic approach at related levels of description." ></td>
	<td class="line x" title="34:171	With respect to computational morphology, witness for instance the success of the Two-Level paradigm introduced by Koskenniemi (1983): extensive morphological descriptions have been made of more than 15 typologically different languages (Kimmo Koskenniemi, personal communication)." ></td>
	<td class="line x" title="35:171	With regard t.o computational syntax, see for instance (GiingSrdii and Oflazer 1994; Hindle 1983; Jensen, Heidorn and Richardson (eds)." ></td>
	<td class="line x" title="36:171	1993; McCord 1990; Sleator and Temperley 1991; Alshawi (ed)." ></td>
	<td class="line x" title="37:171	1992; Strzalkowski 1992)." ></td>
	<td class="line x" title="38:171	The present success of the statistical approach in part-of-speech analysis seems then to form an exception to the general feasibility of the rule-based linguistic approach." ></td>
	<td class="line x" title="39:171	Is the level of parts of speech somehow different, perhaps less rulegoverned, than related levels?" ></td>
	<td class="line x" title="40:171	2 We do not need to assume this idiosyncratic status entirely." ></td>
	<td class="line x" title="41:171	The rest of this paper argues that also parts of speech can be viewed as a rule-governed phenomenon, possible to model using the linguistic approach." ></td>
	<td class="line x" title="42:171	However, it will also be argued that though the distribution of parts of speech can to some extent be described with rules specific to this level of representation, a more natural account could be given using rules overtly about the form and function of essentially syntactic categories." ></td>
	<td class="line x" title="43:171	A syntactic grammar appears to predict the distribution of parts of speech as a 'side effect'." ></td>
	<td class="line x" title="44:171	In this sense parts of speech seem to differ from morphology and syntax: their status as an independent level of linguistic description appears doubtful." ></td>
	<td class="line x" title="45:171	Before proceeding further with the main argument, consider three very recent hybrids systems that employ linguistic rules for resolving some of the ambiguities before using automatically generated corpus-based information: collocation matrices (Leech, Garside and Bryant 1994), Hidden Markov Models (Tapanainen and Voutilainen 1994), or syntactic patterns (Tapanainen and 2For related discussion, cf.Sampson (1987) and Church (1992)." ></td>
	<td class="line x" title="47:171	J~irvinen 1994)." ></td>
	<td class="line x" title="48:171	What is interesting in these hybrids is that they, unlike purely data-driven taggers, seem capable of exceeding the 97% barrier: all three report an accuracy of about 98.5%." ></td>
	<td class="line x" title="49:171	3 The success of these hybrids could be regarded as evidence for the syntactic aspects of parts of speech." ></td>
	<td class="line x" title="50:171	However, the above hybrids still contain a datadriven component, i.e. it remains an open question whether a tagger entirely based on the linguistic approach can compare with a data-driven system." ></td>
	<td class="line x" title="51:171	Next, a new system with the following properties is outlined and evaluated:  The tagger uses only linguistic distributional rules." ></td>
	<td class="line x" title="52:171	 Tested agMnst a 38,000-word corpus of previously unseen text, the tagger reaches a better accuracy than previous systems (over 99%)." ></td>
	<td class="line x" title="53:171	 At the level of linguistic abstraction, the grammar rules are essentially syntactic." ></td>
	<td class="line x" title="54:171	Ideally, part-of-speech disambiguation should fall out as a 'side effect' of syntactic analysis." ></td>
	<td class="line x" title="55:171	Section 2 outlines a rule-based system consisting of the ENGCG tagger followed by a finitestate syntactic parser (Voutilainen and Tapanainen 1993; Voutilainen 1994) that resolves remaining part-of-speech ambiguities as a side effect." ></td>
	<td class="line x" title="56:171	In Section 3, this rule-based system is tested against a 38,000-word corpus of previously unseen text." ></td>
	<td class="line x" title="57:171	Currently tagger evaluation is only becoming standardised; the evaluation method is accordingly reported in detail." ></td>
	<td class="line x" title="58:171	2 System description The tagger consists of the following sequential components:  Tokeniser  ENGCG morphological analyser Lexicon Morphological heuristics  ENGCG morphological disambiguator  Lookup of alternative syntactic tags  Finite state syntactic disambiguator 2.1 Morphological analysis The tokeniser is a rule-based system for identifying words, punctuation marks, document markers, and fixed syntagms (multiword prepositions, certain compounds etc.)." ></td>
	<td class="line x" title="59:171	The morphological description consists of two rule components: (i) the lexicon and (ii) heuristic rules for analysing unrecognised words." ></td>
	<td class="line x" title="60:171	The English Koskenniemi-style lexicon contains over 80,000 lexical entries, each of which represents all inflected and some derived surface forms." ></td>
	<td class="line x" title="61:171	3However, CLAWS4 (Leech, Gazside and Bryant 1994) leaves some ambiguities unresolved; it uses portmanteau tags for representing them." ></td>
	<td class="line x" title="62:171	158 The lexicon employs 139 tags mainly for part of speech, inflection and derivation; for instance: '<,hat>' 'that' <**CLB> CS 'that' DET CENTRAL DEM SG 'that' ADV 'that' PRON DEM SG 'that' <Rel> PRON SG/PL The morphological analyser produces about 180 different tag combinations." ></td>
	<td class="line x" title="63:171	To contrast the ENGCG morphological description with the well-known Brown Corpus tags: ENGCG is more distinctive in that a part-of-speech distinction is spelled out in the description of (i) determiner-pronoun, (ii) prepositionconjunction, (iii) determiner-adverb-pronoun, and (iv) subjunctive-imperative-infinitive-present tense homographs." ></td>
	<td class="line x" title="64:171	On the other hand, ENGCG does not spell out part-of-speech ambiguity in the description of (i) -ing and nonfinite -ed forms, (ii) noun-adjective homographs with similar core meanings, or (iii) abbreviation-proper noun-common noun homographs." ></td>
	<td class="line x" title="65:171	'Morphological heuristics' is a rule-based module for the analysis of those 1-5% of input words." ></td>
	<td class="line x" title="66:171	not represented in the lexicon." ></td>
	<td class="line x" title="67:171	This module employs ordered hand-grafted rules that base their analyses on word shape." ></td>
	<td class="line x" title="68:171	If none of the pattern rules apply, a nominal reading is assigned as a default." ></td>
	<td class="line x" title="69:171	2.2 ENGCG disambiguator A Constraint Grammar can be viewed as a collection 4 of pattern-action rules, no more than one for each ambiguity-forming tag." ></td>
	<td class="line x" title="70:171	Each rule specifies one or more context patterns, or 'constraints', where the tag is illegitimate." ></td>
	<td class="line x" title="71:171	If any of these context patterns are satisfied during disambiguation, the tag is deleted; otherwise it is left intact." ></td>
	<td class="line x" title="72:171	The context patterns can be local or global, and they can refer to ambiguous or unambiguous analyses." ></td>
	<td class="line x" title="73:171	During disambiguatiop, the context can become less ambiguous." ></td>
	<td class="line x" title="74:171	To help a pattern defining an unambiguous context match, several passes are made over the sentence during disambiguation." ></td>
	<td class="line x" title="75:171	The current English grammar contains 1,185 linguistic constraints on the linear order of morphological tags." ></td>
	<td class="line x" title="76:171	Of these, 844 specify a context that extends beyond the neighboring word; in this limited sense, 71% of the constraints are global." ></td>
	<td class="line x" title="77:171	Interestingly, the constraints are partial and often negative paraphrases of 23 general, essentially syntactic generalisations about the form of the noun phrase, the prepositional phrase, the finite verb chain etc.(Voutilainen 1994)." ></td>
	<td class="line x" title="79:171	4Actually, it is possible to define additional heuristic rule collections that can optionally be applied after the more reliable ones for resolving remahdng ambiguities." ></td>
	<td class="line x" title="80:171	The grammar avoids risky'predictions, therefore 3-7% of all words remain ambiguous (an average 1.04-1.08 alternative analyses per output word)." ></td>
	<td class="line x" title="81:171	On the other hand, at least 99.7% of all words retain the correct morphological analysis." ></td>
	<td class="line x" title="82:171	Note in passing that the ratio 1.04-1.08/99.7% compares very favourably with other systems; c.f. 3.0/99.3% by POST (Weischedel et al. 1993) and 1.04/97.6% or 1.09/98.6% by de Marcken (1990)." ></td>
	<td class="line x" title="83:171	There is an additional collection of 200 optionally applicable heuristic constraints that are based on simplified linguistic generalisations." ></td>
	<td class="line x" title="84:171	They resolve about half of the remaining ambiguities, increasing the overall error rate to about 0.5%." ></td>
	<td class="line x" title="85:171	Most of even the remaining ambiguities are structurally resolvable." ></td>
	<td class="line x" title="86:171	ENGCG leaves them pending mainly because it is prohibitively difficult to express certain kinds of structural generalisation using the available rule formalism and grammatical representation." ></td>
	<td class="line x" title="87:171	2.3 Syntactic analysis 2.3.1 Finite-State Intersection Grammar Syntactic analysis is carried out in another reductionistic parsing framework known as FiniteState Intersection Grammar (Koskenniemi 1990; Koskenniemi, Tapanainen and Voutilainen 1992; Tapanainen 1992; Voutilainen and Tapanainen 1993; Voutilainen 1994)." ></td>
	<td class="line x" title="88:171	A short introduction:  Also here syntactic analysis means resolution of structural ambiguities." ></td>
	<td class="line x" title="89:171	Morphological, syntactic and clause boundary descriptors are introduced as ambiguities with simple mappings; these ambiguities are then resolved in parallel." ></td>
	<td class="line x" title="90:171	 The formalism does not distinguish between various types of ambiguity; nor are ambiguity class specific rule sets needed." ></td>
	<td class="line x" title="91:171	A single rule often resolves all types of ambiguity, though superficially it may look e.g. like a rule about syntactic functions." ></td>
	<td class="line x" title="92:171	 The grammarian can define constants and predicates using regular expressions." ></td>
	<td class="line x" title="93:171	For instance, the constants ''." ></td>
	<td class="line x" title="94:171	and '' accept any features within a morphological reading and a finite clause (that may even contain centreembedded clauses), respectively." ></td>
	<td class="line x" title="95:171	Constants and predicates can be used in rules, e.g. implication rules that are of the form X => LC1 _ RC1, LC2 _ RC2,,,, LCn _ RCn; Here X, LC1, RC1, LC2 etc. are regular expressions." ></td>
	<td class="line x" title="96:171	The rule reads: 'X is legitimate only if it occurs in context LC1 _ RC1 or in context LC2 _ RC2 or in context LCn _ RCn'." ></td>
	<td class="line x" title="97:171	159  Also the ambiguous sentences are represented as regular expressions." ></td>
	<td class="line x" title="98:171	 Before parsing, rules and sentences are compiled into deterministic finite-state automata." ></td>
	<td class="line x" title="99:171	 Parsing means intersecting the (ambiguous) sentence automaton with each rule automaton." ></td>
	<td class="line x" title="100:171	Those sentence readings accepted by all rule automata are proposed as parses." ></td>
	<td class="line x" title="101:171	 In addition, heuristic rules can be used for ranking alternative analyses accepted by the strict rules." ></td>
	<td class="line x" title="102:171	2.3.2 Grammatical representation The grammatical representation used in the Finite State framework is an extension of the ENGCG syntax." ></td>
	<td class="line x" title="103:171	Surface-syntactic grammatical relations are encoded with dependency-oriented functional tags." ></td>
	<td class="line x" title="104:171	Functional representation of phrases and clauses has been introduced to facilitate expressing syntactic generMisations." ></td>
	<td class="line x" title="105:171	The representation is introduced in (Voutilainen and Tapanainen 1993; Voutilainen 1994); here, only the main characteristics are given:  Each word boundary is explicitly represented as one of five alternatives: the sentence boundary '@@' the boundary separating juxtaposed finite clauses '@/' -centre-embedded (sequences of) finite clauses are flanked with '@<' and '@>' the plain word boundary '@'  Each word is furnished with a tag indicating a surface-syntactic function (subject, premoditier, auxiliary, main verb, adverbial, etc.)." ></td>
	<td class="line x" title="106:171	All main verbs are furnished with two syntactic tags, one indicating its main verb status, the other indicating the function of the clause." ></td>
	<td class="line x" title="107:171	 An explicit difference is made between finite and nonfinite clauses." ></td>
	<td class="line x" title="108:171	Members in nonfinite clauses are indicated with lower case tags; the rest with upper case." ></td>
	<td class="line x" title="109:171	 In addition to syntactic tags, also morphological, e.g. part-of-speech tags are provided for each word." ></td>
	<td class="line x" title="110:171	Let us illustrate with a simplified example." ></td>
	<td class="line x" title="111:171	@@ Mary N @SUB3 @ told V @MV MC@ @ the DET @>N @ fat A @>N @ butcher's N @>N @ ~ife N @IOBJ @ and CC @CC @ daughters N @IOBJ @/ that CS @CS @ she PKON @SUBJ @ remembers V @MV 0BJ@ @ seeing V @my OBJ@ @ a DET @>N @ dream N @obj @ last DET @>N @ night N @ADVL @ @fullstop @@ Here Mary is a subject in a finite clause (hence the upper case); told is a main verb in a main clause; ghe, fag and bugcher's are premodifiers; wife and daughgers are indirect objects; that is a subordinating conjunction; remembers is a main verb in a finite clause that serves the Object role in a finite clause (the regent being gold); seeing is a main verb in a nonfinite clause (hence the lower case) that also serves the Object role in a finite clause; dream is an object in a nonfinite clause; night is an adverbial." ></td>
	<td class="line x" title="112:171	Because only boundaries separating finite clauses are indicated, there is only one sentence-internal clause boundary, '@/' between daughters and that." ></td>
	<td class="line x" title="113:171	This kind of representation seeks to be (i) sufficiently expressive for stating grammatical generMisations in an economical and transparent fashion and (ii) sufficiently underspecific to make for a structurally resolvable grammatical representation." ></td>
	<td class="line x" title="114:171	For example, the present way of functionally accounting for clauses enables the grammarian to . express rules about the coordination of formally different but functionally similar entities." ></td>
	<td class="line x" title="115:171	Regarding the resolvability requirement, certain kinds of structurMly unresolvable distinctions are never introduced." ></td>
	<td class="line x" title="116:171	For instance, the premodifier tag @>N only indicates that its head is a nominal in the right hand context." ></td>
	<td class="line x" title="117:171	2.3.3 A sample rule Here is a realistic implication rule that partially defines the form of prepositional phrases: PREP => . @ Coord, _ . .PrepComp, PassVChain <Deferred> . _, PostModiCl <Deferred> . _, WH-Question <Deferred> . _ ; A preposition is followed by a coordination or a preposition complement (here hidden in the constant PrepComp that accepts e.g. noun phrases, nonfinite clauses and nominal clauses), or it (as a 'deferred' preposition) is preceded by a passive verb chain Pass VChain or a postmodifying clause PostModiCl (the main verb in a postmodifying clause is furnished with the postmodifier tag N< @) or of a WH-question (i.e. in the same clause, there is a WH-word)." ></td>
	<td class="line x" title="118:171	If the tag PREP occurs in none of the specified contexts, the sentence reading containing it is discarded." ></td>
	<td class="line x" title="119:171	A comprehensive parsing grammar is under development." ></td>
	<td class="line x" title="120:171	Currently it accounts for all major syntactic structures of English, but in a somewhat underspecific fashion." ></td>
	<td class="line x" title="121:171	Though the accuracy of the 160 grammar at the level of syntactic analysis can still be considerably improved, the syntactic grammar is already capable of resolving morphological ambiguities left pending by ENGCG." ></td>
	<td class="line x" title="122:171	3 An experiment with part-of-speech disambiguation The system was tested against a 38,202-word test corpus consisting of previously unseen journalistic, scientific and manual texts." ></td>
	<td class="line x" title="123:171	The finite-state parser, the last module in the system, can in principle be 'forced' to produce an unambiguous analysis for each input sentence, even for ungrammatical ones." ></td>
	<td class="line x" title="124:171	In practice, the present implementation sometimes fails to give an analysis to heavily ambiguous inputs, regardless of their grammaticality." ></td>
	<td class="line x" title="125:171	5 Therefore two kinds of output were accepted for the evaluation: (i) the unambiguous analyses actually proposed by the finite-state parser, and (ii) the ENGCG analysis of those sentences for which the finite-state parser gave no analyses." ></td>
	<td class="line x" title="126:171	From this nearly unambiguous combined output, the success of the hybrid was measured, by automatically comparing it with a benchmark version of the test corpus at the level." ></td>
	<td class="line x" title="127:171	of morphological (including part-of-speech) analysis (i.e. the syntax tags were ignored)." ></td>
	<td class="line x" title="128:171	3.1 Creation of benchmark corpus The benchmark corpus was created by first applying the preprocessor and morphological analyser to the test text." ></td>
	<td class="line x" title="129:171	This morphologically analysed ambiguous text was then independently disambiguated by two experts whose task also was to detect any errors potentially produced by the previously applied components." ></td>
	<td class="line x" title="130:171	They worked independently, consulting written documentation of the grammatical representation when necessary." ></td>
	<td class="line x" title="131:171	Then these manually disambiguated versions were automatically compared." ></td>
	<td class="line x" title="132:171	At this stage, slightly over 99% of all analyses were identical." ></td>
	<td class="line x" title="133:171	When the differences were collectively examined, it was agreed that virtually all were due to inattention." ></td>
	<td class="line x" title="134:171	6 One of these two corpus versions was modified to represent the consensus, and this 'consensus corpus' was used as the benchmark in the evaluation." ></td>
	<td class="line x" title="135:171	7 3.2 Results The results are given in Figure 1 (next page)." ></td>
	<td class="line x" title="136:171	Let us examine the results." ></td>
	<td class="line x" title="137:171	ENGCG accuracy was close to normal, except that the heuristic con5During the intersection, the sentence automaton sometimes becomes prohibitively large." ></td>
	<td class="line x" title="138:171	6Only in the analysis of a few headings, different (meaning-level) interpretations arose, and even here it was agreed by both judges that this ambiguity was genuine." ></td>
	<td class="line x" title="139:171	7If this high consensus level appears surprising, see Voutilainen and Jrvinen (this volume)." ></td>
	<td class="line x" title="140:171	stralnts (tagger D2) performed somewhat poorer than usual." ></td>
	<td class="line x" title="141:171	The finite-state parser gave an analysis to about 80% of all words." ></td>
	<td class="line x" title="142:171	Overall, 0.6% of all words remained ambiguous (due to the failure of the Finite State parser; c.f. Section 3)." ></td>
	<td class="line x" title="143:171	Parsing speed varied greatly (0.1-150 words/see)." ></td>
	<td class="line x" title="144:171	-refinement of the Finite State software is still underway." ></td>
	<td class="line x" title="145:171	The overall success of the system is very encouraging 99.26% of all words retained the correct morphological analysis." ></td>
	<td class="line x" title="146:171	Compared to the 95-97% accuracy of the best competing probabilistic partof-speech taggers, this accuracy, achieved with an entirely rule-based description, suggests that partof-speech disambiguation is a syntactic problem." ></td>
	<td class="line x" title="147:171	The misanalyses have not been studied in detail, but some general observations can be made:  Many misanalyses made by the Finite State parser were due to ENGCG misanalyses (the 'domino effect')." ></td>
	<td class="line x" title="148:171	 The choice between adverbs and other categories was sometimes difficult." ></td>
	<td class="line x" title="149:171	The distributions of adverbs and certain other categories overlaps; this may explain this error type." ></td>
	<td class="line x" title="150:171	Lexeme-oriented constraints could be formulated for some of these cases." ></td>
	<td class="line x" title="151:171	* Some ambiguities, e.g. noun-verb and participle-past tense, were problematic." ></td>
	<td class="line x" title="152:171	This is probably due to the fact that while the parsing grammar always requires a regent for a dependent, it is much more permissive on dependentless regents." ></td>
	<td class="line x" title="153:171	Clause boundaries, and hence the internal structure of clauses, could probably be determined more accurately if the heuristic part of the grammar also contained rules for preferring e.g. verbs with typical complements over verbs without complements." ></td>
	<td class="line x" title="154:171	4 Conclusion Part-of-speech disambiguation has recently been tackled best with data-driven techniques." ></td>
	<td class="line x" title="155:171	Linguistic techniques have done well at related levels (morphology, syntax) but not here." ></td>
	<td class="line x" title="156:171	Is there something in parts of speech that makes them less accessible to the rule-based linguistic approach?" ></td>
	<td class="line x" title="157:171	This paper outlines and evaluates a new partof-speech tagger." ></td>
	<td class="line x" title="158:171	It uses only linguistic distributional rules, yet reaches an accuracy clearly better than any competing system." ></td>
	<td class="line x" title="159:171	This suggests that also parts of speech are a rule-governed distributional phenomenon." ></td>
	<td class="line x" title="160:171	The tagger has two rule components." ></td>
	<td class="line x" title="161:171	One is a grammar specifically developed for resolution of part-of-speech ambiguities." ></td>
	<td class="line x" title="162:171	Though much effort was given to its development, it leaves many ambiguities unresolved." ></td>
	<td class="line x" title="163:171	These rules, superficially about parts of speech, actually express essentially syntactic generalisations, though indirectly and 161 II ambiguous words readings Do (Morph." ></td>
	<td class="line x" title="164:171	analysis) D1 (DO + ENGCG) D2 (D1 + ENGCG heur)." ></td>
	<td class="line x" title="165:171	.D3 (D2 + FS parser) 39.0% 6.2% 3.2% 0.6% 67,737 40,450 38,949 38,342 I readings/wrd I errors \] error rate 1.77 31 0.08% 1.06 124 0.32% 1.02 226 0.59% 1.00 281 0.74% Figure 1: Results from a tagging test on a 38,202-word corpus." ></td>
	<td class="line x" title="166:171	partially." ></td>
	<td class="line x" title="167:171	The other rule component is a syntactic grammar." ></td>
	<td class="line x" title="168:171	This syntactic grammar is able to resolve the pending part-of-speech ambiguities as a side effect." ></td>
	<td class="line x" title="169:171	In short: like morphology and syntax, parts of speech seem to be a rule-governed phenomenon." ></td>
	<td class="line x" title="170:171	However, the best distributional account of parts of speech appears achievable by means of a syntactic grammar, s Acknowledgements I would like to thank Timo Jrvinen, Jussi Piitulainen, Past Tapanainen and two EACL referees for useful comments on an earlier version of this paper." ></td>
	<td class="line x" title="171:171	The usual disclaimers hold." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="J95-2001
Automatic Stochastic Tagging Of Natural Language Texts
Dermatas, Evangelos;Kokkinakis, George K.;"></td>
	<td class="line x" title="1:410	Automatic Stochastic Tagging of Natural Language Texts Evangelos Dermatas  University of Patras George Kokkinakis* University of Patras Five language and tagset independent stochastic taggers, handling morphological and contextual information, are presented and tested in corpora of seven European languages (Dutch, English, French, German, Greek, Italian and Spanish), using two sets of grammatical tags; a small set containing the eleven main grammatical classes and a large set of grammatical categories common to all languages." ></td>
	<td class="line x" title="2:410	The unknown words are tagged using an experimentally proven stochastic hypothesis that links the stochastic behavior of the unknown words with that of the less probable known words." ></td>
	<td class="line x" title="3:410	A fully automatic training and tagging program has been implemented on an IBM PC-compatible 80386-based computer." ></td>
	<td class="line x" title="4:410	Measurements of error rate, time response, and memory requirements have shown that the taggers' performance is satisfactory, even though a small training text is available." ></td>
	<td class="line x" title="5:410	The error rate is improved when new texts are used to update the stochastic model parameters." ></td>
	<td class="line x" title="6:410	1." ></td>
	<td class="line x" title="7:410	Introduction In the natural language processing community, there has been a growing awareness of the key importance that lexical and corpora resources, especially annotated corpora, have to play, both in the advancement of research in this area and in the development of relevant products." ></td>
	<td class="line x" title="8:410	In order to reduce the huge cost of manually creating such corpora, the development of automatic taggers is of paramount importance." ></td>
	<td class="line x" title="9:410	In this respect, the ability of a tagger to handle both known and unknown words, to improve its performance by training, and to achieve a high rate of correctly tagged words, is the criterion for assessing its usability in practical cases." ></td>
	<td class="line x" title="10:410	Several taggers based on rules, stochastic models, neural networks, and hybrid systems have already been presented for Part-of-speech (POS) tagging." ></td>
	<td class="line x" title="11:410	Rule-based taggers (Brill 1992; Elenius 1990; Jacobs and Zernik 1988; Karlsson 1990; Karlsson et al. 1991; Voutilainen, Heikkila, and Antitila 1992; Voutilainen and Tapanainen 1993) use POS-dependent constraints defined by experienced linguists." ></td>
	<td class="line x" title="12:410	A small error rate has been achieved by such systems when a restricted, application-dependent POS set is used; e.g., an error rate of 2-6 percent has been reported by Marcus, Santorini, and Marcinkiewicz (1993) using the Penn Treebank corpus." ></td>
	<td class="line x" title="13:410	Nevertheless, if a large POS set is specified, the number of rules increases significantly and rule definition becomes highly costly and cumbersome." ></td>
	<td class="line oc" title="14:410	Stochastic taggers use both contextual and morphological information, and the model parameters are usually defined or updated automatically from tagged texts (Cerf-Danon and E1-Beze 1991; Church 1988; Cutting et al. 1992; Dermatas and Kokkinakis 1988, 1990, 1993, 1994; Garside, Leech, and Sampson 1987; Kupiec 1992; Maltese * Department of Electrical Engineering, Wire Communications Laboratory (WCL), University of Patras, 265 00 Patras, Greece." ></td>
	<td class="line x" title="15:410	E-mail: dermatas@wcl.ee.upatras.gr." ></td>
	<td class="line x" title="16:410	(~) 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 2 and Mancini 1991; Meteer, Schwartz, and Weischedel 1991; Merialdo 1991; Pelillo, Moro, and Refice 1992; Weischedel et al. 1993; Wothke et al. 1993)." ></td>
	<td class="line x" title="17:410	These taggers are preferred when tagged texts are available for training, and large tagsets and multilingual applications are involved." ></td>
	<td class="line x" title="18:410	In the case where additionally raw untagged text is available, the Maximum Likelihood training can be used to reestimate the parameters of HMM taggers (Merialdo 1994)." ></td>
	<td class="line x" title="19:410	Connectionist models have been used successfully for lexical acquisition (Eineborg and Gamback 1993; Elenius 1990; Elenius and Carlson 1989; Nakamura et al. 1990)." ></td>
	<td class="line x" title="20:410	Correct classification rates up to 96.4 percent have been achieved in the latter case by testing on the Teleman Swedish corpus." ></td>
	<td class="line x" title="21:410	On the other hand, a time-consuming training process has been reported." ></td>
	<td class="line x" title="22:410	Recently, several solutions to the problem of tagging unknown words have been presented (Charniak et al. 1993; Meteer, Schwartz, and Weischedel 1991)." ></td>
	<td class="line x" title="23:410	Hypotheses for unknown words, both stochastic (Dermatas and Kokkinakis 1993, 1994; Maltese and Mancini 1991; Weischedel et al. 1993), and connectionist (Eineborg and Gamback 1993; Elenius 1990) have been applied to unlimited vocabulary taggers." ></td>
	<td class="line x" title="24:410	In taggers that are based on hidden Markov models (HMM), parameters of the unknown words are estimated by taking into account morphological information from the last part of the word (Dermatas and Kokkinakis 1994; Maltese and Mancini 1991)." ></td>
	<td class="line x" title="25:410	Accurate tagging of seven European languages has been achieved in the first case (error rates of 3-13 percent for a detailed POS set), but an enormous amount of training text is required for the estimation of the parameters for unknown words." ></td>
	<td class="line x" title="26:410	Similar results have been reported by Maltese and Mancini (1991) for the Italian language." ></td>
	<td class="line x" title="27:410	Weischedel et al.(1993) have used four categories of word morphology, such as inflectional endings, derivational endings, hyphenation, and capitalization." ></td>
	<td class="line x" title="29:410	For the case in which only a restricted training text is available, a simple, languageand tagset-independent HMM tagger has been presented by Dermatas and Kokkinakis (1993), where the HMM parameters for the unknown words are estimated by assuming that the POS probability distribution of the unknown words and the POS probability distribution of the less probable words in the small training text are identical." ></td>
	<td class="line x" title="30:410	In this paper, five natural language stochastic taggers that are able to predict POS of unknown words are presented and tested following the process of developing annotated corpora (the most recently fully tagged and corrected text is used to update the model parameters)." ></td>
	<td class="line x" title="31:410	Three stochastic optimization criteria and seven European languages (Dutch, English, French, German, Greek, Italian and Spanish) and two POS sets are used in the tests." ></td>
	<td class="line x" title="32:410	The set of main grammatical classes and an extended set of detailed grammatical categories is the same in all languages." ></td>
	<td class="line x" title="33:410	The testing material consists of newspaper texts with 60,000-180,000 words for each language and an English EEC-law text with 110,000 words." ></td>
	<td class="line x" title="34:410	This material was assembled and annotated in the framework of the ESPRIT-291/860 project 'Linguistic Analysis of the European Languages'." ></td>
	<td class="line x" title="35:410	In addition, we present transformations of the taggers' calculations to a fixed-point arithmetic system, which are useful for machines without floating-point hardware." ></td>
	<td class="line x" title="36:410	The taggers handle both lexical and tag transition information, and without performing morphological analysis can be used to annotate corpora when small training texts are available." ></td>
	<td class="line x" title="37:410	Thus, they are preferred when a new language or a new tagset is used." ></td>
	<td class="line x" title="38:410	When the training text is adequate to estimate the tagger parameters, more efficient stochastic taggers (Dermatas and Kokkinakis 1994; Maltese and Mancini 1991; Weischedel et al. 1993) and training methods can be implemented (Merialdo 1994)." ></td>
	<td class="line x" title="39:410	The structure of this paper is as follows: in Section 2 the stochastic tagging models are presented in detail." ></td>
	<td class="line x" title="40:410	In Section 3 the influence of the training text errors and the 138 Dermatas and Kokkinakis Stochastic Tagging sources of stochastic tagger errors are discussed, followed, in Section 4, by a short presentation of the implementation." ></td>
	<td class="line x" title="41:410	In Section 5, statistical measurements on the corpora and a short description of the taggers' performance is given." ></td>
	<td class="line x" title="42:410	Detailed experimental results are included in Appendices A and B. 2." ></td>
	<td class="line x" title="43:410	Stochastic Tagging Models A stochastic optimal sequence of tags T, to be assigned to the words of a sentence W, can be expressed as a function of both lexical P(W \[ T) and language model P(T) probabilities using Bayes' rule: To -argmaxP(T \[ W) = argmax P(W \[ T)  P(T) = argmaxP(W \[ T)  P(T) (1) P(W) T T T Several assumptions and approximations on the probabilities P(W \[ T) and P(T) lead to good comprises concerning memory and computational complexity." ></td>
	<td class="line x" title="44:410	2.1 Hidden Markov Model (HMM) Approach The tagging process can be modeled by an HMM by assuming that each hidden tag state produces a word in the sentence, each word wi is uncorrelated with neighboring words and their tags, and each tag is probabilistic dependent on the N previous tags only." ></td>
	<td class="line x" title="45:410	2.1.1 Most probable tag sequence (HMM-TS)." ></td>
	<td class="line x" title="46:410	The optimal tag sequence for a given observation sequence of words is given by the following equation: N M M Z~ HMM-TS) -argmaxP(h)H P(ti \[ ti-1 h) H P(ti \] ti-1,, ti-N) H P(wi \[ ti) tl,,tM i=2 i=N+I i=1 (2) where M is the number of words in the sentence W. The optimal solution is estimated by the well-known Viterbi algorithm." ></td>
	<td class="line x" title="47:410	The first(Rabiner 1989) and second(He 1988) order Viterbi algorithms have been presented elsewhere." ></td>
	<td class="line x" title="48:410	Recently, Tao (1992) described the Viterbi algorithm for generalized HMMs." ></td>
	<td class="line x" title="49:410	2.1.2 Most probable tags (HMM-T)." ></td>
	<td class="line x" title="50:410	The optimal criterion is to choose the tags that are most likely to be computed independently at each word event: To HMM-T) = {tio, tio -----argmaxP(ti\[W)}, ti i = 1,M (3) The optimum tag tio is estimated using the probabilities of the forward-backward algorithm (Rabiner 1989): rio -argmax P(ti, W) = argmax P(ti, wl,." ></td>
	<td class="line x" title="51:410	., wi)P(wi+l,, WM \[ ti) (4) ti ti The probabilities in equation 4 are estimated recursively for the first(Rabiner 1989) and second-order HMM (Watson and Chung 1992)." ></td>
	<td class="line x" title="52:410	The main difference between the optimization criteria in 2.1.1 and that in 2.1.2 results from the definition of the expected correct tagging rate; the HMM-TS model maximizes the correctly tagged sentences, while the HMM-T model maximizes the correctly tagged words." ></td>
	<td class="line x" title="53:410	139 Computational Linguistics Volume 21, Number 2 2.1.3 Stochastic hypothesis for the unknown words." ></td>
	<td class="line x" title="54:410	When a new text is processed, some words are unknown to the tagger lexicon (i.e. they are not included in the training text)." ></td>
	<td class="line x" title="55:410	In this case, in order to use the forward-backward and the Viterbi algorithm we must estimate the unknown word's conditional probabilities P(w I t)." ></td>
	<td class="line x" title="56:410	Methods for the estimation of these probabilities have already been proposed (e.g. the use of word endings morphology)." ></td>
	<td class="line x" title="57:410	Nevertheless, these methods fail if only a small training text is available because of the huge number of events not occurring in this text, such as pairs of tags and word endings." ></td>
	<td class="line x" title="58:410	To address the above problem we have approximated the conditional probabilities of the unknown word tags by the conditional probabilities of the less probable word tags, i.e. tags of the words occurring only once." ></td>
	<td class="line x" title="59:410	In the following we demonstrate experimentally that this approximation is valid and independent of the training text size." ></td>
	<td class="line x" title="60:410	Figures 1 and 2 show the probability distributions of the tags in the training text (known words) and that of the words occurring only once in this text for the English and French language, respectively." ></td>
	<td class="line x" title="61:410	Furthermore, the tags' probability distribution of the words that are not included in the training text and are characterized as unknown words is shown." ></td>
	<td class="line x" title="62:410	This distribution is measured in a different open testing text, i.e. a text that may include both known and unknown words." ></td>
	<td class="line x" title="63:410	The measurements were carried out on newspaper text and split into two parts of the same size--the training and the open testing text." ></td>
	<td class="line x" title="64:410	Each part contained 90,000 words for the English text and 50,000 words for the French text." ></td>
	<td class="line x" title="65:410	In this experiment, a tagset comprising the main grammatical categories was used: Verb (Vet), Noun (Nou), Adjective (Adj), Adverb (Adv), Pronoun (Pro), Preposition (Pre), Article/Determiner (A-D), Conjunction (Con), Particle (Par), Interjection (Int), Miscellaneous (Mis; i.e., tags that cannot be classified in the previous categories)." ></td>
	<td class="line x" title="66:410	This experiment has two significant results: a. The probability distribution of the tags of unknown words is significantly different from the distribution for known words, while it is very close to the probability distribution of the tags of the less probable known words both in the English and French text." ></td>
	<td class="line x" title="67:410	b. A number of closed and functional grammatical classes has very low probability for both unknown and words occurring only once, e.g., the tags article, determiner, conjunction, pronoun, miscellaneous in English text, and article, determiner, conjunction, pronoun, interjection and miscellaneous in French text." ></td>
	<td class="line x" title="68:410	In the English text, verbs, adjectives and conjunctions are more frequent than in the French text." ></td>
	<td class="line x" title="69:410	On the other hand, prepositions in the French text have a 0.05 greater probability, which is also the most significant difference between the distributions of the two languages." ></td>
	<td class="line x" title="70:410	Prepositions in the words occurring only once and in unknown words are minimal in the English text, while in the French text one out of ten unknown words is a preposition." ></td>
	<td class="line x" title="71:410	The text coverage by prepositions is 11.2 percent for the English and 16.2 percent for the French corpus." ></td>
	<td class="line x" title="72:410	This difference increases significantly in the lexicon coverage: 0.47 percent for the English and 1.54 percent for the French lexicon." ></td>
	<td class="line x" title="73:410	In Figures 3 and 4, the results of chi-square tests that measure the difference between the probability distribution of the tags of the less probable words and that of the unknown words are shown." ></td>
	<td class="line x" title="74:410	Various sizes of training text and two sets of grammatical categories, the main set (11 classes) and an extended set (described in detail in Section 5) were used." ></td>
	<td class="line x" title="75:410	140 Dermatas and Kokkinakis Stochastic Tagging 0,7 0,6 0,5 .~ 0,4 J~ o 0,3 Q. 0,2 0,1 0,." ></td>
	<td class="line x" title="76:410	Nou Int Known words Unknown words occurring only Words elm uB am BII im Ver Mis Pre A-D Adj Pro Adv Con Par Grammatical class Figure 1 Distribution of the main grammatical classes of the known and unknown words and the words occurring only once in English text." ></td>
	<td class="line x" title="77:410	0,7 0,6 0,5 >, 0,4 o 0,3 0,2 0,1 m 0,." ></td>
	<td class="line x" title="78:410	Nou Int  Known words Unknown words  Words occurring only once Ver Mis Pre A-D Adj Pro Adv Con Par Grammatical class Figure 2 Distribution of the main grammatical classes of the known and unknown words and the words occurring only once in French text." ></td>
	<td class="line x" title="79:410	141 Computational Linguistics Volume 21, Number 2 Specifically, the grammatically labeled text of 180,000 word entries of the English language was separated into two parts: the training text, where the tag probabilities distribution of the less probable words was estimated, and the open testing text, where the tag probabilities distribution of the unknown words was measured." ></td>
	<td class="line x" title="80:410	Multiple chisquare experiments were carried out by transferring successively a portion of 30,000 words from the open testing text to the training text and by modifying the word occurrence threshold from 1 to 15 in order to determine the experimentally optimal threshold." ></td>
	<td class="line x" title="81:410	Words having an occurrence below or equal to this threshold in the training text are counted as less probable words." ></td>
	<td class="line x" title="82:410	The results of the tests shown in Figures 3 and 4 include threshold values up to 15 because the difference between the distributions for values greater than 15 increases significantly." ></td>
	<td class="line x" title="83:410	As shown in the above figures, the close relation between the tested probability distributions is evident for all sizes of training and testing text." ></td>
	<td class="line x" title="84:410	Furthermore, we observe that: a. b. C. d. e. The chi-square distance between the tag probability distributions is minimized for low values of the word occurrence threshold." ></td>
	<td class="line x" title="85:410	In the tagset of main grammatical classes, this distance is minimized for threshold values less than three, four, or five, depending on the training text size." ></td>
	<td class="line x" title="86:410	In the extended set of grammatical classes the distance is minimized in all cases for the threshold value one; i.e., when only the words occurring once in the training text are regarded as less probable words." ></td>
	<td class="line x" title="87:410	In the English text the chi-square distance between the tag." ></td>
	<td class="line x" title="88:410	probability distributions is minimized for 120,000 words training text for the set of main grammatical classes and for 60,000 words for the extended set." ></td>
	<td class="line x" title="89:410	The same results are measured in the French text." ></td>
	<td class="line x" title="90:410	There is no significant variation in the chi-square test results for additional training text." ></td>
	<td class="line x" title="91:410	The closed and functional grammatical classes can be estimated automatically as the less probable grammatical classes of the less probable words in the tagged text." ></td>
	<td class="line x" title="92:410	(The manual definition process is time-consuming when a set of detailed grammatical classes is used)." ></td>
	<td class="line x" title="93:410	The probability distribution of some grammatical classes of the unknown words changes significantly when the size of the training text is increased." ></td>
	<td class="line x" title="94:410	These changes can be measured in the training text from the tags' distribution of the less probable words." ></td>
	<td class="line x" title="95:410	Similar results have been achieved by testing the Dutch, German, Greek, Italian, and Spanish texts, both with the tagset of the main grammatical categories and with the common extended set of grammatical categories." ></td>
	<td class="line x" title="96:410	Based on the above we can complete both optimization criteria of the HMM formulation, given in 2.1.1 and 2.1.2, by calculating the conditional probability of the unknown word tags using Bayes' rule: P(Unknown word \[ ti) = P(ti\[ Unknown word)P(Unknown word) P(ti) P(ti \] Less probable word)P(Unknown word) (5) -P(ti) 142 Dermatas and Kokkinakis Stochastic Tagging Tagset of Main Grammatical Classes 0,035 --~ 30K .~ I ~Bl~ 60K 0,03 ~k~ )< 120K 0,025 0,02 0,015 0,01 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Word Occurrence Threshold Figure 3 Chi-square test for the main grammatical classes' distribution of the unknown and the less probable words in the English text for various training text sizes." ></td>
	<td class="line x" title="97:410	Extended tagset of Grammatical classes  30K 0,14 t ~60K . 0,13 .1~ '\[ ~." ></td>
	<td class="line x" title="98:410	90K ~ 0,12 / X 120K 0,11 0,1 0,09 0,08 0,07 0,06 0,05 0,04 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Word Occurrence Threshold Figure 4 Chi-square test for the distribution of the grammatical tags of the unknown words and the less probable words in the English text, for the extended tagset of grammatical classes and various training text sizes." ></td>
	<td class="line x" title="99:410	143 Computational Linguistics Volume 21, Number 2 The probability P(Unknown word) is approximated in open testing texts by measuring the unknown word frequency." ></td>
	<td class="line x" title="100:410	Therefore the model parameters are adapted each time an open testing text is being tagged." ></td>
	<td class="line x" title="101:410	The probability P(t I Less probable word) and the tags probability P(t) are measured in the training text." ></td>
	<td class="line x" title="102:410	Finally, each tag-conditional probability of the unknown word tags is normalized: L ~_, P(wj \[ ti) + P(Unknown word I ti) = 1, j=l Vi = 1, T (6) where L is the number of the known words and T is the number of tags." ></td>
	<td class="line x" title="103:410	2.2 Tagging without Lexical Probabilities When the corresponding lexical probabilities p(w I t) are not available in the dictionary that specifies the possible tags for each word, a simple tagger can be implemented by assuming that each word wi in a sentence is uncorrelated with the assigned tag ti; e.g., p(wi l ti) = p(wi)." ></td>
	<td class="line x" title="104:410	In this case the most probable tag sequence, according to equation 2, is given by: N M T~MLM) = argmaxP(h)1-IP(ti \[ ti-b,h) 1-I P(ti \[ ti-1,,ti-N) tl,,tM i=2 i=N+I (7) which is a Nth-order Markovian chain for the language model (MLM)." ></td>
	<td class="line x" title="105:410	Taggers based on MLM require the training process to store each tag assigned to every lexicon entry and to define the unknown word tagset." ></td>
	<td class="line x" title="106:410	2.2.1 Stochastic hypothesis for the unknown words." ></td>
	<td class="line x" title="107:410	The unknown word tagset is defined by the selection of the most probable tags that have been assigned to the less probable words of the training text." ></td>
	<td class="line x" title="108:410	In this way the unknown words' ambiguity is decreased significantly." ></td>
	<td class="line x" title="109:410	The word occurrence threshold used to define the less probable words and a tag probability threshold used to isolate the less probable tags are estimated experimentally." ></td>
	<td class="line x" title="110:410	Extensive experiments have shown insignificant differences in the tagging error rate when alternative word occurrence thresholds have been tested." ></td>
	<td class="line x" title="111:410	The best results are obtained when values less than 10 are used." ></td>
	<td class="line x" title="112:410	In this paper the word occurrence threshold has been set to one in all experiments." ></td>
	<td class="line x" title="113:410	3." ></td>
	<td class="line x" title="114:410	Tagger Errors 3.1 Errors in the Training Text Taggers based on the HMM technique compensate for some serious training problems inherent in the MLM approach." ></td>
	<td class="line x" title="115:410	The most important one is the presence of errors in the training text." ></td>
	<td class="line x" title="116:410	This situation appears when uncorrected tags or analysts' mistakes remain in the text used to estimate the stochastic model parameters." ></td>
	<td class="line x" title="117:410	These errors generate tag assignments that are not valid." ></td>
	<td class="line x" title="118:410	In MLM taggers these tags are equally weighted to the correct ones." ></td>
	<td class="line x" title="119:410	In contrast, in HMM taggers invalid assignments are biased by the very low value of the corresponding conditional probability of the tags (the wrong tag rarely appears in the specific word environment), which decreases the overall probability for incorrect tag assignments." ></td>
	<td class="line x" title="120:410	144 Dermatas and Kokkinakis Stochastic Tagging Another important issue concerns the HMM ability to handle lexicon information, e.g., to find how frequently the tags have been assigned to each lexicon entry." ></td>
	<td class="line x" title="121:410	In some languages, taggers based on HMMs almost reduce the prediction error to the half compared to the MLM approach." ></td>
	<td class="line x" title="122:410	3.2 Tagger prediction errors Generally, tagger errors can be classified into three categories: a. b. C. Errors due to inadequate training data." ></td>
	<td class="line x" title="123:410	When the model parameters are estimated from a limited amount of training data, tagging errors appear because of unknown or inaccurately estimated conditional probabilities." ></td>
	<td class="line x" title="124:410	Various interpolation techniques have been proposed for the estimation of the model parameters for unseen events or to smooth the model parameters (Church and Gale 1991; Essen and Steinbiss 1992; Jardino and Adda 1993; Katz 1987; McInnes 1992)." ></td>
	<td class="line x" title="125:410	Errors due to the syntactical or grammatical style of the testing text." ></td>
	<td class="line x" title="126:410	This type of error appears when the testing text has a style unknown to the model (i.e. , a style used in the open testing text, not included in the training text)." ></td>
	<td class="line x" title="127:410	It can be reduced by using multiple models that have been previously trained in different text styles." ></td>
	<td class="line x" title="128:410	Errors due to insufficient model hypotheses." ></td>
	<td class="line x" title="129:410	In this case the model hypotheses are not satisfied; e.g., there are strong intra-tag relations in distances greater than the model order, idiomatic expressions, language dependent exceptions, etc. A general solution to the variable length and depth of dependency for HMM has been already proposed (Tao 1992), but has not been implemented in taggers." ></td>
	<td class="line x" title="130:410	4." ></td>
	<td class="line x" title="131:410	Implementation In this section we present techniques to speed up the tagging process and avoid underflow or overflow phenomena during the estimation of the optimum solution." ></td>
	<td class="line x" title="132:410	These techniques do not increase the prediction error rate or have only minimal influence on it, as proven in the experiments." ></td>
	<td class="line x" title="133:410	Two modules consume the majority of the tagger computational time." ></td>
	<td class="line x" title="134:410	The first module extracts from the model parameters the intra-tag and the word-tag conditional probabilities requested by the second module, which computes the optimum solution by multiplying the corresponding conditional probabilities." ></td>
	<td class="line x" title="135:410	Binary search maximizes the searching speed of the first module, while the following three transformation techniques decrease the computing time of the second module, avoid underflow or overflow phenomena, and use the faster and low-cost fixed-point arithmetic system." ></td>
	<td class="line x" title="136:410	4.1 Logarithmic Transformation The stochastic solutions described by equations 2 and 7 are computed by multiplying several conditional probabilities." ></td>
	<td class="line x" title="137:410	The floating-point multiplications of these probabilities are transformed into an equal number of floating-point additions, by computing the logarithm of the optimum criterion probability." ></td>
	<td class="line x" title="138:410	This technique solves the underflow problem which arises when many small probabilities are multiplied, and accelerates the tagger response time." ></td>
	<td class="line x" title="139:410	145 Computational Linguistics Volume 21, Number 2 4.2 Fixed-Point Transformation The fixed-point transformation converts the floating-point logarithmic additions into an equal number of fixed-point additions." ></td>
	<td class="line x" title="140:410	It is realized by the following quantization process: \[/max (ln(Pmin)-ln(Px))\] (8) Ix ---Round Mw ln(Pmin) where: Px is a conditional probability, Pmin is the minimum conditional probability in the model parameter set,/max is the maximum integer of the fixed-point arithmetic system, Mw is the maximum number of words in a sentence and Round\[.\] is a quantization function mapping real numbers into the nearest integer." ></td>
	<td class="line x" title="141:410	After the logarithmic and the fixed-point transformation, equations 2 and 7 become: N I (HMM-Ts) -argmaxI(tl) + ~_,I(tilti_l,,h) tl,,tM i=2 M M + ~_, I(ti \] ti-1,,ti-N) + ~I(wi I ti) (9) i=N+l i=1 N M I~ MLM) = argmaxI(tl) + ~__I(ti I ti_, ,tl) + ~ I(ti I ti_,,,ti-N) (10) tl  tM i=2 i=N+I The quantization function approximates the computations, producing theoretically differing solutions." ></td>
	<td class="line x" title="142:410	In practice the prediction error differences measured for all languages, taggers, and tagsets were less than 0.02 percent." ></td>
	<td class="line x" title="143:410	4.3 Scaling The solution obtained by the forward-backward algorithm cannot be logarithmically transformed because of the presence of summations." ></td>
	<td class="line x" title="144:410	It is well known that for HMMs the forward and backward probabilities tend exponentially to zero." ></td>
	<td class="line x" title="145:410	The scaling process introduced in this case multiplies the forward and backward probabilities by a scaling factor at selective word events in order to keep the computations within the floatingpoint dynamic range of the computer (Rabiner 1989)." ></td>
	<td class="line x" title="146:410	4.4 Hardware--Software The taggers have been realized under MS-DOS using a 32-bit C compiler." ></td>
	<td class="line x" title="147:410	The lexicon size is limited by the available RAM." ></td>
	<td class="line x" title="148:410	A mean value of 35 bytes per word is allocated." ></td>
	<td class="line x" title="149:410	The tagger speed exceeds the rate of 500 word/sec in a 80386 (33MHz) for all languages and tagsets in text with known words." ></td>
	<td class="line x" title="150:410	A maximum memory requirement of 930Kb has been measured in the experiments described in this paper." ></td>
	<td class="line x" title="151:410	A set of symbols and keywords (a sentence separators set) and the maximum length of a sentence are the only manually defined parameters when the HMM taggers are applied." ></td>
	<td class="line x" title="152:410	In the MLM taggers, the word occurrence threshold that isolates the less probable words and the tag probability threshold used to reject the less probable tags from the unknown words tagset are the manually defined parameters." ></td>
	<td class="line x" title="153:410	The training process has been designed to estimate or update the model parameters from fully tagged text without any manual intervention." ></td>
	<td class="line x" title="154:410	Therefore, frequency measurements are defined or updated as model parameters instead of conditional 146 Dermatas and Kokkinakis Stochastic Tagging Table 1 Size of the corpora." ></td>
	<td class="line x" title="155:410	Text Dutch English French German Greek Italian Spanish Newspaper 110,000 180,000 100,000 100,000 120,000 160,000 60,000 EEC-Law -110,000  Table 2 ESPRIT 291/860: Project partners." ></td>
	<td class="line x" title="156:410	Country Partner England France Germany Greece Italy Italy Netherlands Spain Acorn Computers Limited Centre National de la Recherche Scientifique (CNRS), LIMSI Division Ruhr Universitaet Bochum, Lehrstuhl fur Allgemeine Elektrotechnik und Akustik University of Patras, Wire Communications Laboratory (WCL), Speech and Language Group Ing." ></td>
	<td class="line x" title="157:410	C. Olivetti & C. , S.p.A. Centro Studi Applicazioni in Tecnologie Avanzate CSATA Katholieke Universiteit Nijmegen, Dienst A-Faculteiten Universidad National de Educacion a Distancia (UNED), Madrid probabilities that are computed afterwards by using the corresponding relative frequencies." ></td>
	<td class="line x" title="158:410	5." ></td>
	<td class="line x" title="159:410	Performance of the Systems 5.1 Taggers Five taggers have been realized and tested using bi-POS and tri-POS transition probabilities." ></td>
	<td class="line x" title="160:410	Specifically, the firstand the second-order MLM (MLM1 and MLM2, respectively), the firstand the second-order HMM of the most probable tag sequence criterion (HMM-TS1 and HMM-TS2, respectively), and the first-order HMM of the most probable tag criterion (HMM-T1) have been realized." ></td>
	<td class="line x" title="161:410	5.2 Corpora The tagger performance has been measured in extensive experiments carried out on corpora of seven languages, English, Dutch, German, French, Greek, Italian and Spanish, annotated according to detailed grammatical categories." ></td>
	<td class="line x" title="162:410	In Table 1, the type and the size of these corpora is shown." ></td>
	<td class="line x" title="163:410	They are part of corpora selected in the framework of the ESPRIT-I project 291/860: 'Linguistic Analysis of the European Languages' (1985-1989) by the project partners (Table 2) and annotated by using semi-automatic taggers." ></td>
	<td class="line x" title="164:410	Manual correction was performed by experienced, native analysts for each language separately." ></td>
	<td class="line x" title="165:410	In all languages the entries were tagged as they appeared in the text." ></td>
	<td class="line x" title="166:410	In the German corpus, for example, where multiple words are concatenated, the words were not separated." ></td>
	<td class="line x" title="167:410	5.3 Tagsets Two sets of grammatical tags were isolated from a unified set of grammatical categories defined in the ESPRIT I project 291/860 (ESPRIT-860, Internal report, 1986): 147 Computational Linguistics Volume 21, Number 2 Table 3 Extended set of grammatical categories." ></td>
	<td class="line x" title="168:410	Main grammatical categories Detailed grammatical information Adjective, Noun, Pronoun Adverb Article, Determiner, Preposition Verb Regular base comparative superlative interrogative person number case Regular base comparative superlative interrogative Person number case Tense voice mood person number case Table 4 Number of grammatical tags." ></td>
	<td class="line x" title="169:410	Text Dutch English French German Greek Italian Spanish Main set 9 News: 10, Law: 10 10 11 11 10 10 Extended set 50 News: 43, Low: 36 14 116 443 121 121 Table 5 Word ambiguity in the newspaper corpus." ></td>
	<td class="line x" title="170:410	Tagset English Dutch German French Greek Italian Spanish Main set 1.336 1.111 1.3 1.69 1.209 1.62 1.197 Extended set 1.417 1.291 1.878 1.705 1.855 1.729 1.25 a. b. A common tagset of 11 main grammatical categories for each language, as described in 2.1.3." ></td>
	<td class="line x" title="171:410	An extended set including common categorization of the grammatical information for all languages, as shown in Table 3." ></td>
	<td class="line x" title="172:410	In some languages a number of grammatical categories is not applicable." ></td>
	<td class="line x" title="173:410	The depth of grammatical analysis and the grammatical structure of each language produce a different number of POS tags." ></td>
	<td class="line x" title="174:410	In Table 4 the number of POS tags used for each language and each set of grammatical categories is shown." ></td>
	<td class="line x" title="175:410	5.4 Corpus Ambiguity The corpus ambiguity was measured by the mean number of possible tags for each word of the corpus for both sets of grammatical tags (Table 5)." ></td>
	<td class="line x" title="176:410	The most ambiguous texts are the French, Italian, and English in the tagset of main grammatical classes and the German, Greek, Italian, and French in the extended set of grammatical categories." ></td>
	<td class="line x" title="177:410	In Figure 5 the percent occurrence of unknown words in an open testing text of 10,000 words is shown versus the size of the training text." ></td>
	<td class="line x" title="178:410	The Italian and Greek corpora have the greatest number of unknown words followed by the Spanish corpus (for the available results with restricted training text)." ></td>
	<td class="line x" title="179:410	Taking into account the word ambiguity in the training text (Table 5), the occurrence of unknown words in the open testing text (Figure 5), and the hypothesis that the unknown word tagset and the application tagset are the same, the ambiguity of the open testing corpus for both sets of grammatical categories was computed for a 50,000-word training corpus (Table 6)." ></td>
	<td class="line x" title="180:410	148 Dermatas and Kokkinakis Stochastic Tagging 35 25 % 15 Dutch -B-English \ --AFrench --XGerman ~ -~-Greek ~(~ -OItalian I I I I I I I I I I I I I I 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Size of training text ('1 OK words) Figure 5 Percentage of unknown words in open testing text of 10,000 words for various sizes of the training text." ></td>
	<td class="line x" title="181:410	Table 6 Corpus ambiguity in newspaper open testing text." ></td>
	<td class="line x" title="182:410	Tagset English Dutch German French Greek Italian Spanish Main set 8.75 7.83 9.9 9.19 9.32 8.5 8.5 Extended set 37.03 42.78 103.07 12.8 367.25 99.86 100.69 For the set of main grammatical classes the ambiguity of the open testing corpus is more or less the same for all languages, varying from a minimum of 7.83 tags per word in the Dutch text to a maximum of 9.32 in the Greek corpus." ></td>
	<td class="line x" title="183:410	For the extended set of grammatical categories three types of corpora can be distinguished: a. The most ambiguous is the corpus of the Greek language, because of the great number of grammatical tags (443) and the strong presence of unknown words in the open testing text." ></td>
	<td class="line x" title="184:410	b. In the German, Spanish, and Italian texts the same ambiguity is measured." ></td>
	<td class="line x" title="185:410	c. The least ambiguous are the Dutch and French texts." ></td>
	<td class="line x" title="186:410	Taking into account the previous results, it is important to note that the great differences between languages in text ambiguity, in the presence of unknown words and in the statistics of the grammatical categories, e.g. the different occurrence of prepositions in English and French corpora, prevent a direct comparison of languages from the taggers' error rate." ></td>
	<td class="line x" title="187:410	Apart from a few obvious observations given in Section 5.7, such a comparison would require a detailed examination of the corpora and the taggers' errors by experienced linguists." ></td>
	<td class="line x" title="188:410	Therefore, the prediction error rates presented in 149 Computational Linguistics Volume 21, Number 2 Table 7 Lexicon size for 100,000-word training text." ></td>
	<td class="line x" title="189:410	Language Dutch English French German Greek Italian Lexicon size 13,700 12,200 13,500 8,900 17,400 15,300 this paper should be regarded only as indication of the probabilistic taggers' efficiency in each separate language when small training texts are available." ></td>
	<td class="line x" title="190:410	5.5 Experiments The corpora were divided into 10,000-word entries." ></td>
	<td class="line x" title="191:410	All parts except the last one were used to create (initially) and update the model parameters successively." ></td>
	<td class="line x" title="192:410	The last part was tagged each time after the model parameters were updated, giving results of the tagger performance on open testing text." ></td>
	<td class="line x" title="193:410	The influence of the application tagset on the tagger performance was measured by testing the two totally different tagsets described in Section 5.3." ></td>
	<td class="line x" title="194:410	The experimental process was repeated for each language, tagset and tagger." ></td>
	<td class="line x" title="195:410	Thus a total number of 2 (tagsets)  5 (taggers) ~ \[7 (languages) + 1 (Test on English EEC-law text)\] = 80 experiments was carried out." ></td>
	<td class="line x" title="196:410	5.6 Tagger Speed and Memory Requirements In Figures 6 and 7 the tagger speed and the memory requirements after the last memory adaptation process are presented for all taggers and languages, and for the extended tagset." ></td>
	<td class="line x" title="197:410	The Greek and Italian corpora have a great number of lexical entries (different word forms) for the same amount of 100,000-word training text, as shown in Table 7." ></td>
	<td class="line x" title="198:410	As a result these taggers require more memory (Figure 7)." ></td>
	<td class="line x" title="199:410	In contrast, the small size of the German lexicon decreases the required memory." ></td>
	<td class="line x" title="200:410	Tagger speed is closely related to the corpus ambiguity (Table 6)." ></td>
	<td class="line x" title="201:410	The ambiguity of the Greek corpus is more than three times greater than the next one, the German corpus." ></td>
	<td class="line x" title="202:410	The significant influence of the training text size on tagger speed is proven by comparing the experimental results in the English corpus (newspaper and EEC-Law)." ></td>
	<td class="line x" title="203:410	When the taggers are trained using the 170,000 words of the English newspaper corpus, a greater number of lexicon entries and a greater number of transition probabilities (Figure 7) is measured than in the case of the EEC-law corpus (100K words training text)." ></td>
	<td class="line x" title="204:410	The model becomes more complex, but tagger speed is slightly higher because of the greater size of the training text, which reduces the presence of unknown words in the testing text." ></td>
	<td class="line x" title="205:410	Generally, tagger speed increases when the training text is increased." ></td>
	<td class="line x" title="206:410	5.7 Tagger Error Rate The actual tagger error rates for all experiments are given in Appendices A and B. In this section we present a discussion of these error rates." ></td>
	<td class="line x" title="207:410	The error rate depends strongly on the test text and language, and the type and size of the tagset." ></td>
	<td class="line x" title="208:410	The worst results have been obtained for the Greek language because of its significantly greater ambiguity, the number of tags (requiring significantly greater training text), and its freer syntax." ></td>
	<td class="line x" title="209:410	In the main category of tagset experiments, the model parameters for the MLM systems are estimated accurately when the training text exceeds 50,000-90,000 words, 150 O u) 0 1000 100 10 800 700 0,1 Dut \/Tiii $1 $2 HMM-T1 I I I I I I I Eng EngFre Get Gre Ira Spa Law Language Figure 6 Tagger speed after the last adaptation process for the extended set of grammatical categories." ></td>
	<td class="line x" title="210:410	600 == 500 400 Gre Ita Spa 300 2OO Dut --4P-MLM1 MLM2,L HMM-TS1 X HMM-TS2 Eng Eng Fre Ger Law Language Dermatas and Kokkinakis Stochastic Tagging Figure 7 Tagger memory requirements for the extended set of grammatical categories." ></td>
	<td class="line x" title="211:410	151 Computational Linguistics Volume 21, Number 2 45  35 25 15 Dutch -BEnglish French -XGerman -~Greek -~ Italian ~-Spanish I I I I I J I \] I I \] I I I \[ I I 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Size of training text ('1 OK words) Figure 8 Unknown word error rate for the HMM-TS2 tagger and the set of main grammatical categories." ></td>
	<td class="line x" title="212:410	in contrast to the extended tagset experiments, where a greater-size training text for the German, Greek, and Spanish languages is required." ></td>
	<td class="line x" title="213:410	This phenomenon becomes stronger in taggers based on the HMM where the accuracy of the P(w J t) estimation is proportional to the word and the tag frequency of occurrence in the training text." ></td>
	<td class="line x" title="214:410	Thus, for all tagsets and languages a larger training text is required in order to minimize the error rate." ></td>
	<td class="line x" title="215:410	The taggers based on the HMM reduce the prediction error almost to half in comparison to the same order taggers based on MLM." ></td>
	<td class="line x" title="216:410	Strong dependencies on the language and the estimation accuracy of the model parameters influence this reduction." ></td>
	<td class="line x" title="217:410	The alternative HMM solutions give trivial performance differences, confirming recent results obtained in the Treebank corpus by using an HMM tagger (Merialdo 1991)." ></td>
	<td class="line x" title="218:410	Concerning the performance of the taggers in unknown words, we present in Figure 8 as an example the HMM-TS2 error rate for the tagset of the main grammatical categories, which is also the worst case for this set of grammatical categories." ></td>
	<td class="line x" title="219:410	Generally the error rate decreases when the training text is increased." ></td>
	<td class="line x" title="220:410	The stochastic model is successful for only half of the unknown words for the Italian text and for approximately two out of three unknown words for the English text." ></td>
	<td class="line x" title="221:410	In all other languages the HMM-TS2 tagger gives the correct solution for three out of four unknown words." ></td>
	<td class="line x" title="222:410	Similar results are achieved when the extended set of grammatical categories is tested." ></td>
	<td class="line x" title="223:410	In this case the unknown word error rate increases about 10-20 percent for all the languages except the Greek language." ></td>
	<td class="line x" title="224:410	In the Greek text the error rate reaches approximately 65 percent when 100,000-word text is used to define the parameters of the HMM." ></td>
	<td class="line x" title="225:410	The unknown words, which initially cover about 25-35 percent of the text, are reduced to 8-15 percent when all the available text is used as training data." ></td>
	<td class="line x" title="226:410	In the majority of the experiments, the tagger error rate decreases when new text updates the 152 Dermatas and Kokkinakis Stochastic Tagging model parameters." ></td>
	<td class="line x" title="227:410	Trivial differences of the tagger learning rates between languages and tagsets show the efficiency of the training method in estimating the model transition probabilities for the tested languages and the validity of the stochastic hypothesis for the unknown words." ></td>
	<td class="line x" title="228:410	6." ></td>
	<td class="line x" title="229:410	Conclusion In this paper five automatic, stochastic taggers that are able to tag unknown words have been presented." ></td>
	<td class="line x" title="230:410	The taggers have been tested in newspaper corpora of seven European languages and an EEC-law text of the English language using two sets of grammatical categories." ></td>
	<td class="line x" title="231:410	When new training text updates the model parameters, the tagging error rate changes as expected: in text with unknown words a lower error rate is measured, proving the efficiency of the relative frequencies learning method and the validity of the hypothesis for the unknown words' stochastic behavior." ></td>
	<td class="line x" title="232:410	153 Computational Linguistics Volume 21, Number 2 Appendix A: Tests in the Main Grammatical Categories Set Error rate (%) Prediction Error for the Dutch language Main grammatical classes 14 13 12 11 10 9876 5 4 3  e._. ------13-_ '  ' 'O. , -.e  o Tagging system, MLM1 --MLM2  HMM-TS1  HMM-TS2  HMM-T1 Size of the training text (* 10 Kwords) Error rate (%) 19 18 17 1615141312111098 ~ 7Prediction Error for the English language Main grammatical classes I,, ', ~,~ ~ -0." ></td>
	<td class="line x" title="233:410	~ ~ ~_, ~, ~x o,\ o'~ ',8--.~O _, _ Q 65 -i ~, ~,~ 5 6 -;' 8 o 1'o 1'11', 1'31'41'51'61~ Size of the training text ('10 Kwords) Tagging system MLM1 MLM2 HMM-TS1 H M M -TS 2 HMM-T1 154 Dermatas and Kokkinakis Stochastic Tagging Error rate (%) 11 Prediction Error for the English EEC-law text 10 9876 5 4 3 2 Main grammatical classes  e  -~ ~  -= 'o -.o~ _o i 2 3 4 6 6 ~ 6 9 f0 Size of the training text ('10 Kwords) Tagging system, MLM1 MLM2  HMM-TS1  HMM-TS2  HMM-T1 Prediction Error for the French language Error rate (%) 161514131211 10 98765i = 4 i 2 3 4 Size of the training text ('10 Kwords) Main grammatical classes  \ \  -." ></td>
	<td class="line x" title="234:410	.~ \\x,:, Tagging system, MLM1  MLM2  HMM-TS1  HMM-TS2  HMM-T1 155 Computational Linguistics Volume 21, Number 2 Error rate (%) 1615141312 11 10 982 76s: 4321 Prediction Error for the German language Main grammatical classes \ 'NN\ N \~-' x Size of the training text ('10 Kwords) Tagging system, MLM1 MLM2  HMM-TS1  HMM-TS2  HMM-T1 Error rate (%) 24Prediction Error for the Greek language 22 20 18 16 14 12 10 8 6 4 2 Main grammatical classes -~ .0 L. :',~_  @~  o~ _ _ I 2 3 4 5 6 7 8 9 1'0 11 Size of the training text ('10 Kwords) Tagging system, MLM1 --MLM2  HMM-TS1  HMM-TS2 --~-HMM-T1 156 Dermatas and Kokkinakis Stochastic Tagging Error rate (%) Prediction Error for the Italian language Main grammatical classes \ o~.,\ \ \ 3836 343230282624222018 16 14 12 lo i 2 3 ;, s 6 -; 8 9 1'o 1'1 1'2 1'3 ;4 l's Size of the training text ('10 Kwords) Tagging system, MLM1 --MLM2  HMM-TS1  H M M -TS2  HMM-T1 Error rate (%) 1817161514." ></td>
	<td class="line x" title="236:410	131210 Prediction Error for the Spanish language Main grammatical classes Tagging system, MLM1 --M LM2  HMM-TS1 ~--HMM-TS2 --~---HMM-T1 \\ o~' . ~,  o Size of the training text ('10 Kwords) 157 Computational Linguistics Volume 21, Number 2 Appendix B: Tests in the Extended Grammatical Categories Set Error rate (%) Prediction Error for the Dutch language Extended grammatical classes 19 18 17161514131211109876,,, \ Tagging system, MLM1 MLM2  HMM-TS1  HMM-TS2 --~--." ></td>
	<td class="line x" title="237:410	HMM-T1 Size of the training text ('10 Kwords) Error rate (%) 22 21 20 1918171615141312 Prediction Error for the English language Extended grammatical set 10 I  i '' i~ ',~ ~ \~''~ ~   ~, ~ ~ ''0 '~  >~.~." ></td>
	<td class="line x" title="238:410	--e." ></td>
	<td class="line x" title="239:410	-e." ></td>
	<td class="line x" title="240:410	_(3 _. _o Size of the training text ('10 Kwords) Tagging system MLM1 MLM2 HMM-TS1 HMM-TS2 HMM-T1 158 Dermatas and Kokkinakis Stochastic Tagging Error rate (%) 13 Prediction Error for the English EEC-law text 12 11 10 9 8 i 7~ 6542 Extended grammatical classes C,,x. '- % '~ ~-----~ ,\  ~'~." ></td>
	<td class="line x" title="241:410	i ~, 3 ;." ></td>
	<td class="line x" title="242:410	s 6 -~ 8 9 1'o Size of the training text ('10 Kwords) Tagging system, MLM1  MLM2  HMM-TS1 -~ HMM-TS2  HMM-T1 Error rate (%) 16 Prediction Error for the French language Extended grammatical classes 15 14 13 12 11 10 9 8 7 6 5 4 ~++ ~_~ Size of the training text ('10 Kwords) Tagging system, MLM1 --MLM2  HMM-TS1  H M M -TS2 --~--." ></td>
	<td class="line x" title="243:410	HMMoT1 159 Computational Linguistics Volume 21, Number 2 Error rate (%) 28 26 24 22 2018161412108 Prediction Error for the German language Extended grammatical classes '\ '%,, ',',\ \ . . ." ></td>
	<td class="line x" title="244:410	_v:~-~v=.__,=~ ~   --:,=~='  ---:a --r--i  r  r  T  '1 2 3 4 5 6 7 8 9 Size of the training text ('10 Kwords) Tagging system, MLM1 --MLM2  HMM-TS1  HMM-TS2  HMM-T1 Error rate (%) :t 35302520 15 Prediction Error for the Greek language Extended grammatical classes R 4,,,  'x\ ' \ ' ',3. e ~e -O' --43-Size of the training text ('10 Kwords) Tagging system MLM1 MLM2 HMM-TS1 HMM-TS2 HMM-T1 160 Dermatas and Kokkinakis Stochastic Tagging Error rate (%) 45Prediction Error for the Italian language 40 35 30 25 20 15 10 5 Extended grammatical set ~, i i i  '~<, x~ h 3 4 6 6 7 8 9 10 11 12 la 14 is Size of the training text ('10 Kwords) Tagging system, MLM1 --MLM2  HMM-TS1  HMM-TS2  HMM-T1 Error rate (%) 26Prediction Error for the Spanish language 24 222018161412 10 8 Extended grammatical classes o \ \  'N  .  ' ' = o h a, s Size of the training text ('10 Kwords) Tagging system, MLM1 --MLM2  HMM-TS1  HMM-TS2  HMM-T1 161 Computational Linguistics Volume 21, Number 2 References Brill, E." ></td>
	<td class="line x" title="245:410	(1992)." ></td>
	<td class="line x" title="246:410	'A simple rule-based part of speech tagger'." ></td>
	<td class="line x" title="247:410	In Proceedings, Third Conference on Applied Natural Language Processing." ></td>
	<td class="line x" title="248:410	Trento, Italy, 152-155." ></td>
	<td class="line x" title="249:410	Cerf-Danon, H. , and EI-Beze, M." ></td>
	<td class="line x" title="250:410	(1991)." ></td>
	<td class="line x" title="251:410	'Three different probabilistic language models: Comparison and combination'." ></td>
	<td class="line x" title="252:410	In Proceedings, International Conference on Acoustics Speech and Signal Processing, 297-300." ></td>
	<td class="line x" title="253:410	Charniak, E. Hendrickson, C. Jacobson, N. and Perkowitz, M." ></td>
	<td class="line x" title="254:410	(1993)." ></td>
	<td class="line x" title="255:410	'Equations for part-of-speech tagging'." ></td>
	<td class="line x" title="256:410	In Proceedings, National Conference on Artificial Intelligence." ></td>
	<td class="line x" title="257:410	Church, K." ></td>
	<td class="line x" title="258:410	(1988)." ></td>
	<td class="line x" title="259:410	'A stochastic parts program and noun phrase parser for unrestricted text'." ></td>
	<td class="line x" title="260:410	In Proceedings, Second Conference on Applied Natural Language Processing." ></td>
	<td class="line x" title="261:410	Austin, Texas, 136-143." ></td>
	<td class="line x" title="262:410	Church, K. , and Gale, W." ></td>
	<td class="line x" title="263:410	(1991)." ></td>
	<td class="line x" title="264:410	'A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams'." ></td>
	<td class="line x" title="265:410	Computer Speech and Language 5, 19-24." ></td>
	<td class="line x" title="266:410	Cutting, D. Kupiec, J. Pederson, J. and Sibun, P." ></td>
	<td class="line x" title="267:410	(1992)." ></td>
	<td class="line x" title="268:410	'A practical part-of-speech tagger'." ></td>
	<td class="line x" title="269:410	In Proceedings, Third Conference on Applied Natural Language Processing." ></td>
	<td class="line x" title="270:410	Trento, Italy, 133-140." ></td>
	<td class="line x" title="271:410	Dermatas, E. , and Kokkinakis, G." ></td>
	<td class="line x" title="272:410	(1988)." ></td>
	<td class="line x" title="273:410	'Semi automatic labelling of Greek texts'." ></td>
	<td class="line x" title="274:410	In Proceedings, Seventh FASE Symposium SPEECH '88." ></td>
	<td class="line x" title="275:410	Edinburgh, 239-245." ></td>
	<td class="line x" title="276:410	Dermatas, E. , and Kokkinakis, G." ></td>
	<td class="line x" title="277:410	(1993)." ></td>
	<td class="line x" title="278:410	'A system for automatic text labelling'." ></td>
	<td class="line x" title="279:410	In Proceedings, Eurospeech-90." ></td>
	<td class="line x" title="280:410	Paris, 382-385." ></td>
	<td class="line x" title="281:410	Dermatas, E. , and Kokkinakis, G." ></td>
	<td class="line x" title="282:410	(1993)." ></td>
	<td class="line x" title="283:410	'A fast multilingual probabilistic tagger'." ></td>
	<td class="line x" title="284:410	In Proceedings, Eurospeech-93." ></td>
	<td class="line x" title="285:410	Berlin, 1323-1326 (presented also in the Eurospeech-93 exhibition)." ></td>
	<td class="line x" title="286:410	Dermatas, E. , and Kokkinakis, G." ></td>
	<td class="line x" title="287:410	(1994)." ></td>
	<td class="line x" title="288:410	'A multilingual unlimited vocabulary stochastic tagger'." ></td>
	<td class="line x" title="289:410	In Advanced Speech Applications--European Commission ESPRIT (1), edited by K. Varghese, S. Pfleger, and J. Lefevre, 98-106." ></td>
	<td class="line x" title="290:410	Springer-Verlag." ></td>
	<td class="line x" title="291:410	Eineborg, M. , and Gamback, B." ></td>
	<td class="line x" title="292:410	(1993)." ></td>
	<td class="line x" title="293:410	'Back-propagation based lexical acquisition experiments'." ></td>
	<td class="line x" title="294:410	In Proceedings, NeuroNimes: Neural Networks and their Industrial & Cognitive Applications." ></td>
	<td class="line x" title="295:410	Nimes, 169-178." ></td>
	<td class="line x" title="296:410	Elenius, K." ></td>
	<td class="line x" title="297:410	(1990).'Comparing a connectionist and rule based model for assignment parts-of-speech'." ></td>
	<td class="line x" title="298:410	In Proceedings, International Conference on Acoustics, Speech and Signal Processing, 597-600." ></td>
	<td class="line x" title="299:410	Elenius, K. , and Carlson, R." ></td>
	<td class="line x" title="300:410	(1989)." ></td>
	<td class="line x" title="301:410	'Assigning parts-of-speech of words from their orthography using a connectionist model'." ></td>
	<td class="line x" title="302:410	In Proceedings, European Conference on Speech Communication and Technology." ></td>
	<td class="line x" title="303:410	Paris, 534-537." ></td>
	<td class="line x" title="304:410	Partners of ESPRIT-291/860 (1986)." ></td>
	<td class="line x" title="305:410	'Unification of the word classes of the ESPRIT Project 860'." ></td>
	<td class="line x" title="306:410	BU-WKL-0376." ></td>
	<td class="line x" title="307:410	Internal Report." ></td>
	<td class="line x" title="308:410	Essen, U. , and Steinbiss, V." ></td>
	<td class="line x" title="309:410	(1992)." ></td>
	<td class="line x" title="310:410	'Cooccurrence smoothing for statistical language modelling'." ></td>
	<td class="line x" title="311:410	In Proceedings, International Conference on Acoustics, Speech and Signal Processing, 161-164." ></td>
	<td class="line x" title="312:410	Garside, R. Leech, G. and Sampson, G." ></td>
	<td class="line x" title="313:410	(1987)." ></td>
	<td class="line x" title="314:410	The Computational Analysis of English: A Corpus-Based Approach." ></td>
	<td class="line x" title="315:410	Longman." ></td>
	<td class="line x" title="316:410	He, Y." ></td>
	<td class="line x" title="317:410	(1988)." ></td>
	<td class="line x" title="318:410	'Extended Viterbi algorithm for second order hidden Markov process'." ></td>
	<td class="line x" title="319:410	In Proceedings, International Conference on Acoustics, Speech and Signal Processing, 718-720." ></td>
	<td class="line x" title="320:410	Jacobs, P. , and Zernik, U." ></td>
	<td class="line x" title="321:410	(1988)." ></td>
	<td class="line x" title="322:410	'Acquiring lexical knowledge from text: A case study'." ></td>
	<td class="line x" title="323:410	In Proceedings, Seventh National Conference on Artificial Intelligence." ></td>
	<td class="line x" title="324:410	Saint Paul, Minnesota, 739-744." ></td>
	<td class="line x" title="325:410	Jardino, M. , and Adda, G." ></td>
	<td class="line x" title="326:410	(1993)." ></td>
	<td class="line x" title="327:410	'Automatic word classification using simulated annealing'." ></td>
	<td class="line x" title="328:410	In Proceedings, International Conference on Acoustics, Speech and Signal Processing, 41-44." ></td>
	<td class="line x" title="329:410	Karlsson, E (1990)." ></td>
	<td class="line x" title="330:410	'Constraint grammar as a framework for parsing running text'." ></td>
	<td class="line x" title="331:410	In Proceedings, Thirteenth International Conference on Computational Linguistics." ></td>
	<td class="line x" title="332:410	Helsinki, Finland, 168-173." ></td>
	<td class="line x" title="333:410	Karlsson, F. Voutilainen, A. Anttila, A. and Heikkila, J." ></td>
	<td class="line x" title="334:410	(1991)." ></td>
	<td class="line x" title="335:410	'Constraint grammar: A language-independent system for parsing unrestricted text, with an application to English'." ></td>
	<td class="line x" title="336:410	In Workshop Notes from the Ninth National Conference on Artificial Intelligence." ></td>
	<td class="line x" title="337:410	Anaheim, California." ></td>
	<td class="line x" title="338:410	Katz, S." ></td>
	<td class="line x" title="339:410	(1987)." ></td>
	<td class="line x" title="340:410	'Estimation of probabilities from sparse data for the language model component of a speech recognizer'." ></td>
	<td class="line x" title="341:410	IEEE Trans." ></td>
	<td class="line x" title="342:410	on Acoustics, Speech, and Language Processing, 35(3), 400-401." ></td>
	<td class="line x" title="343:410	Kupiec, J." ></td>
	<td class="line x" title="344:410	(1992)." ></td>
	<td class="line x" title="345:410	'Robust part-of-speech tagging using a Hidden Markov Model'." ></td>
	<td class="line x" title="346:410	Computer Speech & Language, 6(3), 225-242." ></td>
	<td class="line x" title="347:410	Maltese, G. , and Mancini, F." ></td>
	<td class="line x" title="348:410	(1991)." ></td>
	<td class="line x" title="349:410	'A technique to automatically assign parts-of-speech to words taking into 162 Dermatas and Kokkinakis Stochastic Tagging account word-ending information through a probabilistic model'." ></td>
	<td class="line x" title="350:410	In Proceedings, Eurospeech-91, 753-756." ></td>
	<td class="line x" title="351:410	Marcus, M. Santorini, B. and Marcinkiewicz, M." ></td>
	<td class="line x" title="352:410	(1993)." ></td>
	<td class="line x" title="353:410	'Building a large annotated corpus of English: The Penn Treebank'." ></td>
	<td class="line x" title="354:410	Computational Linguistics, 19(2), 315-330." ></td>
	<td class="line x" title="355:410	McInnes, E (1992)." ></td>
	<td class="line x" title="356:410	'An enhanced interpolation technique for context-specific probability estimation in speech and language modelling'." ></td>
	<td class="line x" title="357:410	In Proceedings, International Conference on Spoken Language Processing, 1491-1494." ></td>
	<td class="line x" title="358:410	Merialdo, B." ></td>
	<td class="line x" title="359:410	(1991)." ></td>
	<td class="line x" title="360:410	'Tagging text with a probabilistic model'." ></td>
	<td class="line x" title="361:410	In International Conference on Acoustics, Speech and Signal Processing, 809-812." ></td>
	<td class="line x" title="362:410	Merialdo, B." ></td>
	<td class="line x" title="363:410	(1994)." ></td>
	<td class="line x" title="364:410	'Tagging English text with a probabilistic model'." ></td>
	<td class="line x" title="365:410	Computational Linguistics, 20(2), 155-171." ></td>
	<td class="line x" title="366:410	Meteer, M. Schwartz, R. and Weischedel, R." ></td>
	<td class="line x" title="367:410	(1991)." ></td>
	<td class="line x" title="368:410	'Empirical studies in part of speech labelling'." ></td>
	<td class="line x" title="369:410	In Proceedings, Fourth DARPA Workshop on Speech and Natural Language." ></td>
	<td class="line x" title="370:410	Morgan Kaufman." ></td>
	<td class="line x" title="371:410	Nakamura, M. Maruyama, K. Kawabata, T. and Shikano, K." ></td>
	<td class="line x" title="372:410	(1990)." ></td>
	<td class="line x" title="373:410	'Neural network approach to word category prediction for English texts'." ></td>
	<td class="line x" title="374:410	In Proceedings, Thirteenth International Conference on Computational Linguistics." ></td>
	<td class="line x" title="375:410	Helinski, Finland, 213-218." ></td>
	<td class="line x" title="376:410	Pelillo, W. Moro, E; and Refice, M." ></td>
	<td class="line x" title="377:410	(1992)." ></td>
	<td class="line x" title="378:410	'Probabilistic prediction of parts-of-speech from spelling using decision trees'." ></td>
	<td class="line x" title="379:410	In Proceedings, International Conference on Spoken Language Processing, 1343-1346." ></td>
	<td class="line x" title="380:410	Rabiner, L." ></td>
	<td class="line x" title="381:410	(1989)." ></td>
	<td class="line x" title="382:410	'A tutorial on Hidden Markov Models and selected applications in speech recognition'." ></td>
	<td class="line x" title="383:410	In Proceedings, IEEE 77(2), 257-285." ></td>
	<td class="line x" title="384:410	Tao, C." ></td>
	<td class="line x" title="385:410	(1992)." ></td>
	<td class="line x" title="386:410	'A generalisation of discrete Hidden Markov Model and of Viterbi algorithm'." ></td>
	<td class="line x" title="387:410	Pattern Recognition, 25(11), 1381-1397." ></td>
	<td class="line x" title="388:410	Voutilainen, A. , and Tapanainen, P." ></td>
	<td class="line x" title="389:410	(1993)." ></td>
	<td class="line x" title="390:410	'Ambiguity resolution in a reductionistic parser'." ></td>
	<td class="line x" title="391:410	In Proceedings, Sixth Conference of the European Chapter of the Association for Computational Linguistics." ></td>
	<td class="line x" title="392:410	Utrecht, Netherlands, 394-403." ></td>
	<td class="line x" title="393:410	Voutilainen, A. Heikkila, J. and Antitila, A." ></td>
	<td class="line x" title="394:410	(1992)." ></td>
	<td class="line x" title="395:410	'Constraint grammar of English'." ></td>
	<td class="line x" title="396:410	Publication 21, Department of General Linguistics, University of Helinski, Helinski, Finland." ></td>
	<td class="line x" title="397:410	Watson, B. , and Chung Tsoi, A." ></td>
	<td class="line x" title="398:410	(1992)." ></td>
	<td class="line x" title="399:410	'Second order Hidden Markov Models for speech recognition'." ></td>
	<td class="line x" title="400:410	In Proceedings, Fourth Australian International Conference on Speech Science and Technology, 146-151." ></td>
	<td class="line x" title="401:410	Weischedel, R. Meteer, M. Schwartz, R. Ramshaw, L. and Palmucci, J." ></td>
	<td class="line x" title="402:410	(1993)." ></td>
	<td class="line x" title="403:410	'Coping with ambiguity and unknown words through probabilistic models'." ></td>
	<td class="line x" title="404:410	Computational Linguistics, 19(2), 359-382." ></td>
	<td class="line x" title="405:410	Wothke, K. Weck-Ulm, I. Heinecke, J. Mertineit, O. and Pachunke, T." ></td>
	<td class="line x" title="406:410	(1993)." ></td>
	<td class="line x" title="407:410	'Statistically based automatic tagging of German text corpora with parts-of-speech--some experiments'." ></td>
	<td class="line x" title="408:410	TR75.93.02-IBM." ></td>
	<td class="line x" title="409:410	IBM Germany, Heidelberg Scientific Center." ></td>
	<td class="line x" title="410:410	163" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="J95-2004
Deterministic Part-Of-Speech Tagging With Finite-State Transducers
Roche, Emmanuel;Schabes, Yves;"></td>
	<td class="line x" title="1:471	Deterministic Part-of-Speech Tagging with Finite-State Transducers Emmanuel Roche* MERL Yves Schabes* MERL Stochastic approaches to natural language processing have often been preferred to rule-based approaches because of their robustness and their automatic training capabilities." ></td>
	<td class="line x" title="2:471	This was the case for part-of-speech tagging until Brill showed how state-of-the-art part-of-speech tagging can be achieved with a rule-based tagger by inferring rules from a training corpus." ></td>
	<td class="line x" title="3:471	However, current implementations of the rule-based tagger run more slowly than previous approaches." ></td>
	<td class="line x" title="4:471	In this paper, we present a finite-state tagger, inspired by the rule-based tagger, that operates in optimal time in the sense that the time to assign tags to a sentence corresponds to the time required to follow a single path in a deterministic finite-state machine." ></td>
	<td class="line x" title="5:471	This result is achieved by encoding the application of the rules found in the tagger as a nondeterministic finite-state transducer and then turning it into a deterministic transducer." ></td>
	<td class="line x" title="6:471	The resulting deterministic transducer yields a part-of-speech tagger whose speed is dominated by the access time of mass storage devices." ></td>
	<td class="line x" title="7:471	We then generalize the techniques to the class of transformation-based systems." ></td>
	<td class="line x" title="8:471	1." ></td>
	<td class="line x" title="9:471	Introduction Finite-state devices have important applications to many areas of computer science, including pattern matching, databases, and compiler technology." ></td>
	<td class="line x" title="10:471	Although their linguistic adequacy to natural language processing has been questioned in the past (Chomsky, 1964), there has recently been a dramatic renewal of interest in the application of finitestate devices to several aspects of natural language processing." ></td>
	<td class="line x" title="11:471	This renewal of interest is due to the speed and compactness of finite-state representations." ></td>
	<td class="line x" title="12:471	This efficiency is explained by two properties: finite-state devices can be made deterministic, and they can be turned into a minimal form." ></td>
	<td class="line x" title="13:471	Such representations have been successfully applied to different aspects of natural language processing, such as morphological analysis and generation (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (Roche 1993; Tapanainen and Voutilainen 1993), phonology (Laporte 1993; Kaplan and Kay 1994) and speech recognition (Pereira, Riley, and Sproat 1994)." ></td>
	<td class="line x" title="14:471	Although finite-state machines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993; Silberztein 1993), none of these approaches has the same flexibility as stochastic techniques." ></td>
	<td class="line oc" title="15:471	Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993), up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired." ></td>
	<td class="line x" title="16:471	Recently, Brill (1992) described a rule-based tagger that performs as well as taggers based upon probabilistic models and overcomes the limitations common in rule-based approaches to language processing: it is robust and the rules are automatically ac* Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA 02139." ></td>
	<td class="line x" title="17:471	E-mail: rocbe/schabes@merl.com." ></td>
	<td class="line x" title="18:471	(~) 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 2 quired." ></td>
	<td class="line x" title="19:471	In addition, the tagger requires drastically less space than stochastic taggers." ></td>
	<td class="line x" title="20:471	However, current implementations of Brill's tagger are considerably slower than the ones based on probabilistic models since it may require RKn elementary steps to tag an input of n words with R rules requiring at most K tokens of context." ></td>
	<td class="line x" title="21:471	Although the speed of current part-of-speech taggers is acceptable for interactive systems where a sentence at a time is being processed, it is not adequate for applications where large bodies of text need to be tagged, such as in information retrieval, indexing applications, and grammar-checking systems." ></td>
	<td class="line x" title="22:471	Furthermore, the space required for part-of-speech taggers is also an issue in commercial personal computer applications such as grammar-checking systems." ></td>
	<td class="line x" title="23:471	In addition, part-of-speech taggers are often being coupled with a syntactic analysis module." ></td>
	<td class="line x" title="24:471	Usually these two modules are written in different frameworks, making it very difficult to integrate interactions between the two modules." ></td>
	<td class="line x" title="25:471	In this paper, we design a tagger that requires n steps to tag a sentence of length n, independently of the number of rules and the length of the context they require." ></td>
	<td class="line x" title="26:471	The tagger is represented by a finite-state transducer, a framework that can also be the basis for syntactic analysis." ></td>
	<td class="line x" title="27:471	This finite-state tagger will also be found useful when combined with other language components, since it can be naturally extended by composing it with finite-state transducers that could encode other aspects of natural language syntax." ></td>
	<td class="line x" title="28:471	Relying on algorithms and formal characterizations described in later sections, we explain how each rule in Brill's tagger can be viewed as a nondeterministic finite-state transducer." ></td>
	<td class="line x" title="29:471	We also show how the application of all rules in Brill's tagger is achieved by composing each of these nondeterministic transducers and why nondeterminism arises in this transducer." ></td>
	<td class="line x" title="30:471	We then prove the correctness of the general algorithm for determinizing (whenever possible) finite-state transducers, and we successfully apply this algorithm to the previously obtained nondeterministic transducer." ></td>
	<td class="line x" title="31:471	The resulting deterministic transducer yields a part-of-speech tagger that operates in optimal time in the sense that the time to assign tags to a sentence corresponds to the time required to follow a single path in this deterministic finite-state machine." ></td>
	<td class="line x" title="32:471	We also show how the lexicon used by the tagger can be optimally encoded using a finite-state machine." ></td>
	<td class="line x" title="33:471	The techniques used for the construction of the finite-state tagger are then formalized and mathematically proven correct." ></td>
	<td class="line x" title="34:471	We introduce a proof of soundness and completeness with a worst-case complexity analysis for the algorithm for determinizing finite-state transducers." ></td>
	<td class="line x" title="35:471	We conclude by proving that the method can be applied to the class of transformationbased error-driven systems." ></td>
	<td class="line x" title="36:471	2." ></td>
	<td class="line x" title="37:471	Overview of Brill's Tagger Brill's tagger is comprised of three parts, each of which is inferred from a training corpus: a lexical tagger, an unknown word tagger, and a contextual tagger." ></td>
	<td class="line x" title="38:471	For purposes of exposition, we will postpone the discussion of the unknown word tagger and focus mainly on the contextual rule tagger, which is the core of the tagger." ></td>
	<td class="line x" title="39:471	The lexical tagger initially tags each word with its most likely tag, estimated by examining a large tagged corpus, without regard to context." ></td>
	<td class="line x" title="40:471	For example, assuming that vbn is the most likely tag for the word 'killed' and vbd for 'shot,' the lexical tagger might assign the following part-of-speech tags: 1 1 The notation for part-of-speech tags is adapted from the one used in the Brown Corpus (Francis and 228 Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging Figure 1 Sample rules." ></td>
	<td class="line x" title="41:471	1." ></td>
	<td class="line x" title="42:471	vbn vbd PREVTAG np 2." ></td>
	<td class="line x" title="43:471	vbd vbn NEXTTAG by (1) (2) (3) Chapman/np killed/vbn John/np Lennon/np John/np Lennon/np was/bedz shot/vbd by~by Chapman/np He/pps witnessed/vbd Lennon/np killed/vbn by~by Chapman/np Since the lexical tagger does not use any contextual information, many words can be tagged incorrectly." ></td>
	<td class="line x" title="44:471	For example, in (1), the word 'killed' is erroneously tagged as a verb in past participle form, and in (2), 'shot' is incorrectly tagged as a verb in past tense." ></td>
	<td class="line x" title="45:471	Given the initial tagging obtained by the lexical tagger, the contextual tagger applies a sequence of rules in order and attempts to remedy the errors made by the initial tagging." ></td>
	<td class="line x" title="46:471	For example, the rules in Figure 1 might be found in a contextual tagger." ></td>
	<td class="line x" title="47:471	The first rule says to change tag vbn to vbd if the previous tag is np." ></td>
	<td class="line x" title="48:471	The second rule says to change vbd to tag vbn if the next tag is by." ></td>
	<td class="line x" title="49:471	Once the first rule is applied, the tag for 'killed' in (1) and (3) is changed from vbn to vbd and the following tagged sentences are obtained: (4) (5) (6) Chapman/np killed/vbd John/np Lennon/np John/np Lennon/np was/bedz shot/vbd by~by Chapman/np He/pps witnessed/vbd Lennon/np killed/vbd by~by Chapman/np And once the second rule is applied, the tag for 'shot' in (5) is changed from vbd to vbn, resulting in (8), and the tag for 'killed' in (6) is changed back from vbd to vbn, resulting in (9): (7) (8) (9) Chapman/np killed/vbd John/np Lennon/np John/np Lennon/np was~be& shot/vbn by~by Chapman/np He/pps witnessed/vbd Lennon/np killed/vbn by~by Chapman/np It is relevant to our following discussion to note that the application of the NEXTTAG rule must look ahead one token in the sentence before it can be applied, and that the application of two rules may perform a series of operations resulting in no net change." ></td>
	<td class="line x" title="50:471	As we will see in the next section, these two aspects are the source of local nondeterminism in Brill's tagger." ></td>
	<td class="line x" title="51:471	The sequence of contextual rules is automatically inferred from a training corpus." ></td>
	<td class="line x" title="52:471	A list of tagging errors (with their counts) is compiled by comparing the output of the lexical tagger to the correct part-of-speech assignment." ></td>
	<td class="line x" title="53:471	Then, for each error, it is determined which instantiation of a set of rule templates results in the greatest error reduction." ></td>
	<td class="line x" title="54:471	Then the set of new errors caused by applying the rule is computed and the process is repeated until the error reduction drops below a given threshold." ></td>
	<td class="line x" title="55:471	Ku~era 1982): pps stands for singular nominative pronoun in third person, vbd for verb in past tense, np for proper noun, vbn for verb in past participle form, by for the word 'by,' at for determiner, nn for singular noun, and bedz for the word 'was'." ></td>
	<td class="line x" title="56:471	229 Computational Linguistics Volume 21, Number 2 A B PREVTAG C A B PREVIOR2OR3TAG C A B PREVIOR2TAG C A B NEXTIOR2TAG C A B NEXTTAG C A B SURROUNDTAG C D A B NEXTBIGRAM C D A B PREVBIGRAM C D change A to B if previous tag is C change A to B if previous one or two or three tag is C change A to B if previous one or two tag is C change A to B if next one or two tag is C change A to B if next tag is C change A to B if surrounding tags are C and D change A to B if next bigram tag is C D change A to B if previous bigram tag is C D Figure 2 Contextual rule templates." ></td>
	<td class="line x" title="57:471	iii iilD \] C \[C IA \[ IclclAI ICIDIClClAI I C ID lii iiI ii iiiil IClCIAI IClClAI (1) (2) Figure 3 Partial matches of A B PREVBIGRAM C C on the input C D C C A." ></td>
	<td class="line x" title="58:471	(3) Using the set of contextual rule templates shown in Figure 2, after training on the Brown Corpus, 280 contextual rules are obtained." ></td>
	<td class="line x" title="59:471	The resulting rule-based tagger performs as well as state-of-the-art taggers based upon probabilistic models." ></td>
	<td class="line x" title="60:471	It also overcomes the limitations common in rule-based approaches to language processing: it is robust, and the rules are automatically acquired." ></td>
	<td class="line x" title="61:471	In addition, the tagger requires drastically less space than stochastic taggers." ></td>
	<td class="line x" title="62:471	However, as we will see in the next section, Brill's tagger is inherently slow." ></td>
	<td class="line x" title="63:471	3." ></td>
	<td class="line x" title="64:471	Complexity of Brill's Tagger Once the lexical assignment is performed, in Brill's algorithm, each contextual rule acquired during the training phase is applied to each sentence to be tagged." ></td>
	<td class="line x" title="65:471	For each individual rule, the algorithm scans the input from left to right while attempting to match the rule." ></td>
	<td class="line x" title="66:471	This simple algorithm is computationally inefficient for two reasons." ></td>
	<td class="line x" title="67:471	The first reason for inefficiency is the fact that an individual rule is compared at each token of the input, regardless of the fact that some of the current tokens may have been previously examined when matching the same rule at a previous position." ></td>
	<td class="line x" title="68:471	The algorithm treats each rule as a template of tags and slides it along the input, one word at a time." ></td>
	<td class="line x" title="69:471	Consider, for example, the rule A B PREVBIGRAM C C that changes tag A to tag B if the previous two tags are C. When applied to the input CDCCA, the pattern CCA is compared three times to the input, as shown in Figure 3." ></td>
	<td class="line x" title="70:471	At each step no record of previous partial matches or mismatches is remembered." ></td>
	<td class="line x" title="71:471	In this example, C is compared with the second input token D during the first and second steps, and therefore, the second step could have been skipped by remembering the comparisons from the first step." ></td>
	<td class="line x" title="72:471	This method is similar to a naive pattern-matching algorithm." ></td>
	<td class="line x" title="73:471	The second reason for inefficiency is the potential interaction between rules." ></td>
	<td class="line x" title="74:471	For example, when the rules in Figure 1 are applied to sentence (3), the first rule results 230 Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging in a change (6) that is undone by the second rule as shown in (9)." ></td>
	<td class="line x" title="75:471	The algorithm may therefore perform unnecessary computation." ></td>
	<td class="line x" title="76:471	In summary, Brill's algorithm for implementing the contextual tagger may require RKn elementary steps to tag an input of n words with R contextual rules requiring at most K tokens of context." ></td>
	<td class="line x" title="77:471	4." ></td>
	<td class="line x" title="78:471	Construction of the Finite-State Tagger We show how the function represented by each contextual rule can be represented as a nondeterministic finite-state transducer and how the sequential application of each contextual rule also corresponds to a nondeterministic finite-state transducer being the result of the composition of each individual transducer." ></td>
	<td class="line x" title="79:471	We will then turn the nondeterministic transducer into a deterministic transducer." ></td>
	<td class="line x" title="80:471	The resulting partof-speech tagger operates in linear time independent of the number of rules and the length of the context." ></td>
	<td class="line x" title="81:471	The new tagger operates in optimal time in the sense that the time to assign tags to a sentence corresponds to the time required to follow a single path in the resulting deterministic finite-state machine." ></td>
	<td class="line x" title="82:471	Our work relies on two central notions: the notion of a finite-state transducer and the notion of a subsequential transducer." ></td>
	<td class="line x" title="83:471	Informally speaking, a finite-state transducer is a finite-state automaton whose transitions are labeled by pairs of symbols." ></td>
	<td class="line x" title="84:471	The first symbol is the input and the second is the output." ></td>
	<td class="line x" title="85:471	Applying a finite-state transducer to an input consists of following a path according to the input symbols while storing the output symbols, the result being the sequence of output symbols stored." ></td>
	<td class="line x" title="86:471	Section 8.1 formally defines the notion of transducer." ></td>
	<td class="line x" title="87:471	Finite-state transducers can be composed, intersected, merged with the union operation and sometimes determinized." ></td>
	<td class="line x" title="88:471	Basically, one can manipulate finite-state transducers as easily as finite-state automata." ></td>
	<td class="line x" title="89:471	However, whereas every finite-state automaton is equivalent to some deterministic finite-state automaton, there are finite-state transducers that are not equivalent to any deterministic finite-state transducer." ></td>
	<td class="line x" title="90:471	Transductions that can be computed by some deterministic finite-state transducer are called subsequential functions." ></td>
	<td class="line x" title="91:471	We will see that the final step of the compilation of our tagger consists of transforming a finite-state transducer into an equivalent subsequential transducer." ></td>
	<td class="line x" title="92:471	We will use the following notation when pictorially describing a finite-state transducer: final states are depicted with two concentric circles; e represents the empty string; on a transition from state i to state j, a/b indicates a transition on input symbol a and output symbol(s) b; a a question mark ()?" ></td>
	<td class="line x" title="93:471	on an input transition (for example labeled ?/b) originating at state i stands for any input symbol that does not appear as input symbol on any other outgoing arc from i. In this document, each depicted finitestate transducer will be assumed to have a single initial state, namely the leftmost state (usually labeled 0)." ></td>
	<td class="line x" title="94:471	We are now ready to construct the tagger." ></td>
	<td class="line x" title="95:471	Given a set of rules, the tagger is constructed in four steps." ></td>
	<td class="line x" title="96:471	The first step consists of turning each contextual rule found in Brill's tagger into a finite-state transducer." ></td>
	<td class="line x" title="97:471	Following the example discussed in Section 2, the functionality of the rule vbn vbd PREVTAG np is represented by the transducer shown on the left of Figure 4." ></td>
	<td class="line x" title="98:471	2 When multiple output symbols are emitted, a comma symbolizes the concatenation of the output symbols." ></td>
	<td class="line x" title="99:471	231 Computational Linguistics Volume 21, Number 2 np/np vbn/vbd ?/?" ></td>
	<td class="line x" title="100:471	(.~p/np Figure 4 Left: Transducer T1 representing the contextual rule vbn vbd PREVTAG np." ></td>
	<td class="line x" title="101:471	Right: Local extension LocExt(T1) of T1." ></td>
	<td class="line x" title="102:471	bn Figure 5 Left: Transducer T2 representing vbd vbn NEXTTAG by." ></td>
	<td class="line x" title="103:471	Right: Local extension LocExt(T2) of T2." ></td>
	<td class="line x" title="104:471	Each contextual rule is defined locally; that is, the transformation it describes must be applied at each position of the input sequence." ></td>
	<td class="line x" title="105:471	For instance, the rule A B PREVIOR2TAG C, which changes A into B if the previous tag or the one before is C, must be applied twice on C A A (resulting in the output C B B)." ></td>
	<td class="line x" title="106:471	As we have seen in the previous section, this method is not efficient." ></td>
	<td class="line x" title="107:471	The second step consists of turning the transducers produced by the preceding step into transducers that operate globally on the input in one pass." ></td>
	<td class="line x" title="108:471	This transformation is performed for each transducer associated with each rule." ></td>
	<td class="line x" title="109:471	Given a function fl that transforms, say, a into b (i.e. fl(a) = b), we want to extend it to a function f2 such that f2(w) = w / where w' is the word built from the word w where each occurrence of a has been replaced by b. We say that f2 is the local extension 3 of fl, and we write f2 = LocExt(fl)." ></td>
	<td class="line x" title="110:471	Section 8.2 formally defines this notion and gives an algorithm for computing the local extension." ></td>
	<td class="line x" title="111:471	Referring to the example of Section 2, the local extension of the transducer for the rule vbn vbd PREVTAG np is shown to the right of Figure 4." ></td>
	<td class="line x" title="112:471	Similarly, the transducer for the contextual rule vbd vbn NEXTTAG by and its local extension are shown in Figure 5." ></td>
	<td class="line x" title="113:471	The transducers obtained in the previous step still need to be applied one after the other." ></td>
	<td class="line x" title="114:471	3 This notion was introduced by Roche (1993)." ></td>
	<td class="line x" title="115:471	232 Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging vbd/vbn ~~~4 Figure 6 Composition T3 = LocExt(T1) o LocExt(T2)." ></td>
	<td class="line x" title="116:471	a:a Figure 7 Example of a transducer not equivalent to any subsequential transducer." ></td>
	<td class="line x" title="117:471	The third step combines all transducers into one single transducer." ></td>
	<td class="line x" title="118:471	This corresponds to the formal operation of composition defined on transducers." ></td>
	<td class="line x" title="119:471	The formalization of this notion and an algorithm for computing the composed transducer are well known and are described originally by Elgot and Mezei (1965)." ></td>
	<td class="line x" title="120:471	Returning to our running example of Section 2, the transducer obtained by composing the local extension of T2 (right in Figure 5) with the local extension of T1 (right in Figure 4) is shown in Figure 6." ></td>
	<td class="line x" title="121:471	The fourth and final step consists of transforming the finite-state transducer obtained in the previous step into an equivalent subsequential (deterministic) transducer." ></td>
	<td class="line x" title="122:471	The transducer obtained in the previous step may contain some nondeterminism." ></td>
	<td class="line x" title="123:471	The fourth step tries to turn it into a deterministic machine." ></td>
	<td class="line x" title="124:471	This determinization is not always possible for any given finite-state transducer." ></td>
	<td class="line x" title="125:471	For example, the transducer shown in Figure 7 is not equivalent to any subsequential transducer." ></td>
	<td class="line x" title="126:471	Intuitively speaking, this transducer has to look ahead an unbounded distance in order to correctly generate the output." ></td>
	<td class="line x" title="127:471	This intuition will be formalized in Section 9.2." ></td>
	<td class="line x" title="128:471	However, as proven in Section 10, the rules inferred in Brill's tagger can always be turned into a deterministic machine." ></td>
	<td class="line x" title="129:471	Section 9.1 describes an algorithm for determinizing finite-state transducers." ></td>
	<td class="line x" title="130:471	This algorithm will not terminate when applied to transducers representing nonsubsequential functions." ></td>
	<td class="line x" title="131:471	In our running example, the transducer in Figure 6 has some nondeterministic paths." ></td>
	<td class="line x" title="132:471	For example, from state 0 on input symbol vbd, two possible emissions are possible: vbn (from 0 to 2) and vbd (from 0 to 3)." ></td>
	<td class="line x" title="133:471	This nondeterminism is due to the rule vbd vbn NEXTTAG by, since this rule has to read the second symbol before it can know which symbol must be emitted." ></td>
	<td class="line x" title="134:471	The deterministic version of the transducer T3 is shown in Figure 8." ></td>
	<td class="line x" title="135:471	Whenever nondeterminism arises in T3, the deterministic machine 233 Computational Linguistics Volume 21, Number 2 Figure 8 Subsequential form for T3." ></td>
	<td class="line x" title="136:471	?/vbd,?" ></td>
	<td class="line x" title="137:471	emits the empty symbol , and postpones the emission of the output symbol." ></td>
	<td class="line x" title="138:471	For example, from the start state 0, the empty string is emitted on input vbd, while the current state is set to 2." ></td>
	<td class="line x" title="139:471	If the following word is by, the two token string vbn by is emitted (from 2 to 0), otherwise vbd is emitted (depending on the input from 2 to 2 or from 2 to 0)." ></td>
	<td class="line x" title="140:471	Using an appropriate implementation for finite-state transducers (see Section 11), the resulting part-of-speech tagger operates in linear time, independently of the number of rules and the length of the context." ></td>
	<td class="line x" title="141:471	The new tagger therefore operates in optimal time." ></td>
	<td class="line x" title="142:471	We have shown how the contextual rules can be implemented very efficiently." ></td>
	<td class="line x" title="143:471	We now turn our attention to lexical assignment, the step that precedes the application of the contextual transducer." ></td>
	<td class="line x" title="144:471	This step can also be made very efficient." ></td>
	<td class="line x" title="145:471	5." ></td>
	<td class="line x" title="146:471	Lexical Tagger The first step of the tagging process consists of looking up each word in a dictionary." ></td>
	<td class="line x" title="147:471	Since the dictionary is the largest part of the tagger in terms of space, a compact representation is crucial." ></td>
	<td class="line x" title="148:471	Moreover, the lookup process has to be very fast too---otherwise the improvement in speed of the contextual manipulations would be of little practical interest." ></td>
	<td class="line x" title="149:471	To achieve high speed for this procedure, the dictionary is represented by a deterministic finite-state automaton with both fast access and small storage space." ></td>
	<td class="line x" title="150:471	Suppose one wants to encode the sample dictionary of Figure 9." ></td>
	<td class="line x" title="151:471	The algorithm, as described by Revuz (1991), consists of first building a tree whose branches are labeled by letters and whose leaves are labeled by a list of tags (such as nn vb), and then minimizing it into a directed acyclic graph (DAG)." ></td>
	<td class="line x" title="152:471	The result of applying this procedure to the sample dictionary of Figure 9 is the DAG of Figure 10." ></td>
	<td class="line x" title="153:471	When a dictionary is represented as a DAG, looking up a word in it consists simply of following one path in the DAG." ></td>
	<td class="line x" title="154:471	The complexity of the lookup procedure depends only on the length of the word; in particular, it is independent of the size of the dictionary." ></td>
	<td class="line x" title="155:471	The lexicon used in our system encodes 54, 000 words." ></td>
	<td class="line x" title="156:471	The corresponding DAG takes 360Kb of space and provides an access time of 12, 000 words per second." ></td>
	<td class="line x" title="157:471	4 4 The size of the dictionary in plain text (ASCII form) is 742KB." ></td>
	<td class="line x" title="158:471	234 Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging ads nns bag nn vb bagged vbn vbd bayed vbn vbd bids nns Figure 9 Sample dictionary." ></td>
	<td class="line x" title="159:471	a ' ~ / d ~,O s ~-~ (nns) _ ~,/~ 7." ></td>
	<td class="line x" title="160:471	~ (nn,vb) ~-~O----~ ~) (vbd,vbn) Figure 10 DAG representation of the dictionary of Figure 9." ></td>
	<td class="line x" title="161:471	6. Tagging Unknown Words The rule-based system described by Brill (1992) contains a module that operates after all known words--that is, words listed in the dictionary--have been tagged with their most frequent tag, and before contextual rules are applied." ></td>
	<td class="line x" title="162:471	This module guesses a tag for a word according to its suffix (e.g. a word with an 'ing' suffix is likely to be a verb), its prefix (e.g. a word starting with an uppercase character is likely to be a proper noun), and other relevant properties." ></td>
	<td class="line x" title="163:471	This module basically follows the same techniques as the ones used to implement the lexicon." ></td>
	<td class="line x" title="164:471	Because of the similarity of the methods used, we do not provide further details about this module." ></td>
	<td class="line x" title="165:471	7." ></td>
	<td class="line x" title="166:471	Empirical Evaluation The tagger we constructed has an accuracy identical s to Brill's tagger and comparable to statistical-based methods." ></td>
	<td class="line x" title="167:471	However, it runs at a much higher speed." ></td>
	<td class="line x" title="168:471	The tagger runs nearly ten times faster than the fastest of the other systems." ></td>
	<td class="line x" title="169:471	Moreover, the finitestate tagger inherits from the rule-based system its compactness compared with a stochastic tagger." ></td>
	<td class="line x" title="170:471	In fact, whereas stochastic taggers have to store word-tag, bigram, and trigram probabilities, the rule-based tagger and therefore the finite-state one only have to encode a small number of rules (between 200 and 300)." ></td>
	<td class="line x" title="171:471	We empirically compared our tagger with Eric Brill's implementation of his tagger, and with our implementation of a trigram tagger adapted from the work of Church (1988) that we previously implemented for another purpose." ></td>
	<td class="line x" title="172:471	We ran the three programs on large files and piped their output into a file." ></td>
	<td class="line x" title="173:471	In the times reported, we included the time spent reading the input and writing the output." ></td>
	<td class="line x" title="174:471	Figure 11 summarizes the results." ></td>
	<td class="line x" title="175:471	All taggers were trained on a portion of the Brown corpus." ></td>
	<td class="line x" title="176:471	The experiments were run on an HP720 with 32MB of memory." ></td>
	<td class="line x" title="177:471	In order to conduct a fair comparison, the dictionary lookup part of the stochastic tagger has also been implemented using the techniques described in Section 5." ></td>
	<td class="line x" title="178:471	All three taggers have approximately the same 5 Our current implementation is functionally equivalent to the tagger as described by Brill (1992)." ></td>
	<td class="line x" title="179:471	However, the tagger could be extended to include recent improvements described in more recent papers (Brill 1994)." ></td>
	<td class="line x" title="180:471	235 Computational Linguistics Volume 21, Number 2 Stochastic Tagger Speed 1,200 w/s Space 2,158KB Rule-Based Tagger 500 w/s 379KB Finite-State Tagger 10,800 w/s 815KB Figure 11 Overall performance comparison." ></td>
	<td class="line x" title="181:471	dictionary lookup unknown words Speed 12,800 w/s 16,600 w/s Percent of the time 85% 6,5% contextual 125,100 w/s 8.5% Figure 12 Speeds of the different parts of the program." ></td>
	<td class="line x" title="182:471	precision (95% of the tags are correct)." ></td>
	<td class="line x" title="183:471	6 By design, the finite-state tagger produces the same output as the rule-based tagger." ></td>
	<td class="line x" title="184:471	The rule-based tagger--and the finite-state tagger--do not always produce the exact same tagging as the stochastic tagger (they do not make the same errors); however, no significant difference in performance between the systems was detected." ></td>
	<td class="line oc" title="185:471	7 Independently, Cutting et aL (1992) quote a performance of 800 words per second for their part-of-speech tagger based on hidden Markov models." ></td>
	<td class="line x" title="186:471	The space required by the finite-state tagger (815KB) is distributed as follows: 363KB for the dictionary, 440KB for the subsequential transducer and 12KB for the module for unknown words." ></td>
	<td class="line x" title="187:471	The speeds of the different parts of our system are shown in Figure 12." ></td>
	<td class="line x" title="188:471	8 Our system reaches a performance level in speed for which other, very low-level factors (such as storage access) may dominate the computation." ></td>
	<td class="line x" title="189:471	At such speeds, the time spent reading the input file, breaking the file into sentences, breaking the sentences into words, and writing the result into a file is no longer negligible." ></td>
	<td class="line x" title="190:471	8." ></td>
	<td class="line x" title="191:471	Finite-State Transducers The methods used in the construction of the finite-state tagger described in the previous sections were described informally." ></td>
	<td class="line x" title="192:471	In the following section, the notion of finitestate transducer and the notion of local extension are defined." ></td>
	<td class="line x" title="193:471	We also provide an algorithm for computing the local extension of a finite-state transducer." ></td>
	<td class="line x" title="194:471	Issues related to the determinization of finite-state transducers are discussed in the section following this one." ></td>
	<td class="line x" title="195:471	8.1 Definition of Finite-State Transducers A finite-state transducer T is a five-tuple (~, Q, i,F, E) where: G is a fnite alphabet; Q is a finite set of states or vertices; i c Q is the initial state; F C Q is the set of final states; E c Q x (y, u {c}) x ~,* x Q is the set of edges or transitions." ></td>
	<td class="line x" title="196:471	6 For evaluation purposes, we randomly selected 90% of the Brown corpus for training purposes and 10% for testing." ></td>
	<td class="line x" title="197:471	7 An extended discussion of the precision of the rule-based tagger can be found in Brill (1992)." ></td>
	<td class="line x" title="198:471	8 In Figure 12, the dictionary lookup includes reading the file, splitting it into sentences, looking up each word in the dictionary, and writing the final result to a file." ></td>
	<td class="line x" title="199:471	The dictionary lookup and the tagging of unknown words take roughly the same amount of time, but since the second procedure only applies on unknown words (around 10% in our experiments), the percentage of time it takes is much smaller." ></td>
	<td class="line x" title="200:471	236 Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging 1 Figure 13 T4: Example of a finite-state transducer." ></td>
	<td class="line x" title="201:471	For instance, Figure 13 is the graphical representation of the transducer: T4 = (Ca, b,c,h,e}, C 0,1, 2,3}, o, {3}, C(0,a, b, 1), (0,a, c, 2), (1, h, h, 3), (2, e, e, 3)})." ></td>
	<td class="line x" title="202:471	A finite-state transducer T also defines a function on words in the following way: the extended set of edges F. , the transitive closure of E, is defined by the following recursive relation:  ifeEEtheneE/~  if (q,a,b,q'), (q',a',b',q') E E then (q, aa',bb',q') E E. Then the function f from G* to ~* defined byf(w) = w' iff 3q E F such that (i,w,w',q) E /~ is the function defined by T. One says that T represents f and writes f = ITI." ></td>
	<td class="line x" title="203:471	The functions on words that are represented by finite-state transducers are called rational functions." ></td>
	<td class="line x" title="204:471	If, for some input w, more than one output is allowed (e.g. f(w) = {Wl, w2  }) then f is called a rational transduction." ></td>
	<td class="line x" title="205:471	In the example of Figure 13, IT41 is defined by IT4i(ah) = bh and IT4i(ae) = ce." ></td>
	<td class="line x" title="206:471	Given a finite-state transducer T = (~, Q, i,F, E), the following additional notions are useful: its state transition function d that maps Q x (G u {}) into 2 Q defined by d(q,a) = Cq' E Q I 3w' E G* and (q,a,w',q') E E}; and its emission function ~ that maps Q x (G u {~}) x Q into 2 ~' defined by 6(q,a,q') = {w' E G* I (q,a,w,',q') E E}." ></td>
	<td class="line x" title="207:471	A finite-state transducer could be seen as a finite-state automaton, where each transition label is a pair." ></td>
	<td class="line x" title="208:471	In this respect, T4 would be deterministic; however, since transducers are generally used to compute a function, a more relevant definition of determinism consists of saying that both the transition function d and the emission function ~ lead to sets containing at most one element, that is, Id(q,a)I < 1 and I~(q, a, qt)l < 1 (and that these sets are empty for a = ~)." ></td>
	<td class="line x" title="209:471	With this notion, if a finite-state transducer is deterministic, one can apply the function to a given word by deterministically following a single path in the transducer." ></td>
	<td class="line x" title="210:471	Deterministic transducers are called subsequential transducers (Schfitzenberger 1977)." ></td>
	<td class="line x" title="211:471	9 Given a deterministic transducer, we can define the partial functions qa = q' iff d(q,a) ~ {q~} and q,a = w ~ iff 3q' E Q such that q @ a = q~ and 6(q, a, q~) = Cw~}." ></td>
	<td class="line x" title="212:471	This leads to the definition of subsequential transducers: a subsequential transducer T' is a seven-tuple (G, Q,/, F, , *, p) where: ~, Q, i, F are defined as above;  is the deterministic state transition function that maps Q x on Q, one writes qa = q~; * is the deterministic emission function that maps Q x ~ on Y,*, one writes q  a = w~; and the final emission function p maps F on G*, one writes,(q) = w. For instance, T4 is not deterministic because d(0,a) = C1,2}, but it is equivalent to T5 represented Figure 14 in the sense that they represent the same function, i.e. 9 A sequential transducer is a deterministic transducer for which all states are final." ></td>
	<td class="line x" title="213:471	Sequential transducers are also called generalized sequential machines (Eilenberg 1974)." ></td>
	<td class="line x" title="214:471	237 Computational Linguistics Volume 21, Number 2 Figure 14 Subsequential transducer T5." ></td>
	<td class="line x" title="215:471	h/bh 0 a& 1,/''-'~ 2 b,c Figure 15 T6: a finite-state transducer to be extended." ></td>
	<td class="line x" title="216:471	a a b c a b a a b c a b b c b c a a b c a b d c a Figure 16 Top: Input." ></td>
	<td class="line x" title="217:471	Middle: First factorization." ></td>
	<td class="line x" title="218:471	Bottom: Second factorization." ></td>
	<td class="line x" title="219:471	IT4\] =\]Ts\[." ></td>
	<td class="line x" title="220:471	T5 is defined by T5 = ({a,b,c,h,e},(O, 1,2},O,{2},,,,p) where 0a = 1, 0,a = , 1 h = 2, 1,h = bh, 1@e = 2, 1,e = ce, and p(2) = ~." ></td>
	<td class="line x" title="221:471	8.2 Local Extension In this section, we will see how a function that needs to be applied at all input positions can be transformed into a global function that needs to be applied once on the input." ></td>
	<td class="line x" title="222:471	For instance, consider T6 of Figure 15." ></td>
	<td class="line x" title="223:471	It represents the function f6 = \]T6\[ such that f6(ab) = bc and f6(bca) = dca." ></td>
	<td class="line x" title="224:471	We want to build the function that, given a word w, each time w contains ab (i.e. ab is a factor of the word) (resp." ></td>
	<td class="line x" title="225:471	bca), this factor is transformed into its image bc (resp." ></td>
	<td class="line x" title="226:471	dca)." ></td>
	<td class="line x" title="227:471	Suppose, for instance, that the input word is w = aabcab, as shown in Figure 16, and that the factors that are in dom(f6) 1 can be found according to two different factorizations: i.e. w I = a.w2." ></td>
	<td class="line x" title="228:471	c-W211, where w2 -ab, and wl = aa  w3  b, where w3 = bca." ></td>
	<td class="line x" title="229:471	The local extension of f6 will be the transduction that takes each possible factorization and transforms each factor according to f6, i.e. f6(w2) = bc and f6(w3) -= dca, and leaves the other parts unchanged; here this leads to two outputs: abccbc according to the first factorization, and aadcab according to the second factorization." ></td>
	<td class="line x" title="230:471	The notion of local extension is formalized through the following definition." ></td>
	<td class="line x" title="231:471	Definition If f is a rational transduction from G* to G*, the local extension F = LocExt(f) is the rational transduction from G* on G* defined in the following way: if u =  ' '." ></td>
	<td class="line x" title="232:471	' F(u) if E ~*  albla2b2  'anbnan+l E G* then v = albla2b 2  'anbnan+l E ai (G* dom(f) . ~*), bi c dom(f) and b I c f(bi)." ></td>
	<td class="line x" title="233:471	10 dom(f) denotes the domain of f, that is, the set of words that have at least one output through f. 11 If wi, w2 C ~*, Wl W2 denotes the concatenation of Wl and w 2." ></td>
	<td class="line x" title="234:471	It may also be written WlW 2, 238 Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging Local Extension ( T' = (G, Q', i', F', E' ), T = (~., Q, i, F, E ) ) 1 C'\[0\] = ({i}, identity); q = 0; i' = 0; F' = O; E' = 0; Q' = 0; C'\[1\] = (0, transduction); n = 2; 2 do 3 (S, type)= C'\[q\];Q' = Q'u {q}; 4 if (type == identity) 5 F' = F'U {q};E' = E' u {(q, ?, ?, i')}; 6 for each w E (~." ></td>
	<td class="line x" title="236:471	U {}) s.t. 3x E S, d(x,w) # 0 and Vy E S, d(y,w) NF = O 7 if 3r E \[0,n 1\] such that C'\[r\] == ({i} U Ud(x,w),identity) xES 8 e=r; 9 else 10 C'\[e = n + +\] = ({i} U Ud(x,w),identity); xES 11 E' = E' U {(q,w,w,e)}; 12 for each (i, w, w', x) E E 13 if 3r E \[0, n 1\] such that C'\[r\] == ({x}, transduction) 14 e=r; 15 else 16 C'\[e = n + +\] = ({x}, transduction); 17 E' = E' U {(q,w,w',e)}; 18 for each w E (G U {c}) s.t. 3x E S d(x,w) MF # 0 then E' = E' U {(q,w,w, 1)}; 19 else if (type == transduction) 20 if 3Xl E Q s.t. S == {Xl} 21 if (xi E F) then E' = E' U {(q,~,c,0)}; 22 for each (xl, w, w', y) E E 23 if 3r E \[0, n -1\] such that C'\[r\] == ({y}, transduction) 24 e = r; 25 else 26 C'\[e = n + +\] = ({y}, transduction); 27 E' = E' U {(q,w,w',e)}; 28 q++; 29 }while(q < n); Figure 17 Local extension algorithm." ></td>
	<td class="line x" title="237:471	Intuitively, if F = LocExt(f) and w E ~*, each factor of w in dom(f) is transformed into its image by f and the remaining part of w is left unchanged." ></td>
	<td class="line x" title="238:471	If f is represented by a finite-state transducer T and LocExt(f) is represented by a finite-state transducer T', one writes T' = LocExt(T)." ></td>
	<td class="line x" title="239:471	It could also be seen that if 'YT is the identity function on * (~*  dom(T)  ~*), then LocExt(T) = 'Tr ' (T. 'yw)*." ></td>
	<td class="line x" title="240:471	12 Figure 17 gives an algorithm that computes the local extension directly." ></td>
	<td class="line x" title="241:471	The idea is that an input word is processed nondeterministically from left to right." ></td>
	<td class="line x" title="242:471	Suppose, for instance, that we have the initial transducer T7 of Figure 18 and that we want to build its local extension, Ts of Figure 19." ></td>
	<td class="line x" title="243:471	When the input is read, if a current input letter cannot be transformed at the initial state of T7 (the letter c for instance), it is left unchanged: this is expressed by the looping transition on the initial state 0 of Ts labeled ?/?.13 On the other hand, 12 In this last formula, the concatenation  stands for the concatenation of the graphs of each function; that is, for the concatenation of the transducers viewed as automata whose labels are of the form a/b. 13 As explained before, an input transition labeled by the symbol ? stands for all transitions labeled with a letter that doesn't appear as input on any outgoing arc from this state." ></td>
	<td class="line x" title="244:471	A transition labeled ?/?" ></td>
	<td class="line x" title="245:471	stands 239 b,c Figure 18 Sample transducer T7." ></td>
	<td class="line x" title="246:471	F.dE ?/?" ></td>
	<td class="line x" title="247:471	Computational Linguistics Volume 21, Number 2 Figure 19 Local extension Ts of TT: T8 = LocExt(T7)." ></td>
	<td class="line x" title="248:471	if the input symbol, say a, can be processed at the initial state of T7, one doesn't know yet whether a will be the beginning of a word that can be transformed (e.g. ab) or whether it will be followed by a sequence that makes it impossible to apply the transformation (e.g. ac)." ></td>
	<td class="line x" title="249:471	Hence one has to entertain two possibilities, namely (1) we are processing the input according to T7 and the transitions should be a/b; or (2) we are within the identity and the transition should be a/a. This leads to two kind of states: the transduction states (marked transduction in the algorithm) and the identity states (marked identity in the algorithm)." ></td>
	<td class="line x" title="250:471	It can be seen in Figure 19 that this leads to a transducer that has a copy of the initial transducer and an additional part that processes the identity while making sure it could not have been transformed." ></td>
	<td class="line x" title="251:471	In other words, the algorithm consists of building a copy of the original transducer and at the same time the identity function that operates on ~* ~*  dom(T)  Y,*." ></td>
	<td class="line x" title="252:471	Let us now see how the algorithm of Figure 17 applies step by step to the transducer T7 of Figure 18, producing the transducer T8 of Figure 19." ></td>
	<td class="line x" title="253:471	In Figure 17, C'\[0\] = ({i}, identity) of line 1 states that state 0 of the transducer to be built is of type identity and refers to the initial state i = 0 of T7." ></td>
	<td class="line x" title="254:471	q represents the current state and n the current number of states." ></td>
	<td class="line x" title="255:471	In the loop do} while (q < n), one builds the transitions of each state one after the other: if the transition points to a state not already built, a new state is added, thus incrementing n. The program stops when all states have been inspected and when no additional state is created." ></td>
	<td class="line x" title="256:471	The number of iterations is bounded by 2 Ilz\]l*2, where \[\[T\[I = \[Q\[ is the number of states of the original transducer." ></td>
	<td class="line x" title="257:471	14 Line 3 says that the current state within the loop is q and that this state for all the diagonal pairs a/a s.t. a is not an input symbol on any outgoing arc from this state." ></td>
	<td class="line x" title="258:471	14 In fact, Qr c 2 Qx {transduction,identity}." ></td>
	<td class="line x" title="259:471	Thus, q ~ 2 2\[Q\[." ></td>
	<td class="line x" title="260:471	240 Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging 1 ?/?" ></td>
	<td class="line x" title="261:471	Figure 20 Local extension T9 of T6:T9 = LocExt(T6)." ></td>
	<td class="line x" title="262:471	refers to the set of states S and is marked by the type type." ></td>
	<td class="line x" title="263:471	In our example, at the first occurrence of this line, S is instantiated to {0} and type = identity." ></td>
	<td class="line x" title="264:471	Line 5 adds the current identity state to the set of final states and a transition to the initial state for all letters that do not appear on any outgoing arc from this state." ></td>
	<td class="line x" title="265:471	Lines 6-11 build the transitions from and to the identity states, keeping track of where this leads in the original transducer." ></td>
	<td class="line x" title="266:471	For instance, a is a label that verifies the conditions of line 6." ></td>
	<td class="line x" title="267:471	Thus a transition a/a is to be added to the identity state 2, which refers to 1 (because of the transition a/b of T7) and to i = 0 (because it is possible to start the transduction T7 from any identity state)." ></td>
	<td class="line x" title="268:471	Line 7 checks that this state doesn't already exist and adds it if necessary, e = n + + means that the arrival state for this transition, i.e. d(q, w), will be the last added state and that the number of states being built has to be incremented." ></td>
	<td class="line x" title="269:471	Line 11 actually builds the transition between 0 and e = 2 labeled a/a. Lines 12-17 describe the fact that it is possible to start a transduction from any identity state." ></td>
	<td class="line x" title="270:471	Here a transition is added to a new state, i.e. a/b to 3." ></td>
	<td class="line x" title="271:471	The next state to be considered is 2 and it is built like state 0, except that the symbol b should block the current output." ></td>
	<td class="line x" title="272:471	In fact, state 1 means that we already read a with a as output; thus, if one reads b, ab is at the current point, and since ab should be transformed into bc, the current identity transformation (that is a ~ a) should be blocked: this is expressed by the transition b/b that leads to state 1 (this state is a 'trash' state; that is, it has no outgoing transition and it is not final)." ></td>
	<td class="line x" title="273:471	The following state is 3, which is marked as being of type transduction, which means that lines 19-27 should be applied." ></td>
	<td class="line x" title="274:471	This consists simply of copying the transitions of the original transducer." ></td>
	<td class="line x" title="275:471	If the original state was final, as for 4 = ({2}, transduction), an ~/~ transition to the initial state is added (to get the behavior of T+)." ></td>
	<td class="line x" title="276:471	The transducer T9 = LocExt(T6) of Figure 20 gives a more complete (and slightly more complex) example of this algorithm." ></td>
	<td class="line x" title="277:471	241 Computational Linguistics Volume 21, Number 2 9." ></td>
	<td class="line x" title="278:471	Determinization The basic idea behind the determinization algorithm comes from Mehryar Mohri." ></td>
	<td class="line x" title="279:471	is In this section, after giving a formalization of the algorithm, we introduce a proof of soundness and completeness, and we study its worst-case complexity." ></td>
	<td class="line x" title="280:471	9.1 Determinization Algorithm In the following, for Wl, w 2 E Y~,*, Wl /~ W2 denotes the longest common prefix of wl and w2." ></td>
	<td class="line x" title="281:471	The finite-state transducers we use in our system have the property that they can be made deterministic; that is, there exists a subsequential transducer that represents the same function." ></td>
	<td class="line x" title="282:471	16 If T = (~, Q, i, F, E) is such a finite-state transducer, the subsequential transducer T' = (E, Q', i', F', ,,, p) defined as follows will be later proved equivalent to T: Q~ c 2 QxE* . In fact, the determinization of the transducer is related to the determinization of FSAs in the sense that it also involves a power set construction." ></td>
	<td class="line x" title="283:471	The difference is that one has to keep track of the set of states of the original transducer, one might be in and also of the words whose emission have been postponed." ></td>
	<td class="line x" title="284:471	For instance, a state {(ql, Wl), (q2,w2)} means that this state corresponds to a path that leads to q~ and q2 in the original transducer and that the emission of wl (resp." ></td>
	<td class="line x" title="285:471	w2) was delayed for ql (resp." ></td>
	<td class="line x" title="286:471	q2)." ></td>
	<td class="line x" title="287:471	i' = {(i, ~)}." ></td>
	<td class="line x" title="288:471	There is no postponed emission at the initial state." ></td>
	<td class="line x" title="289:471	the emission function is defined by: S,a= A A u.6(q,a,q') (q,u)~S q' Ed(q,a) This means that, for a given symbol, the set of possible emissions is obtained by concatenating the postponed emissions with the emission at the current state." ></td>
	<td class="line x" title="290:471	Since one wants the transition to be deterministic, the actual emission is the longest common prefix of this set." ></td>
	<td class="line x" title="291:471	the state transition function is defined by: Sa= U U {(q',(S*a)-l'u'6(q,a,q'))} (q,u)cS q,~d(q,a) Given u, v E E*, u v denotes the concatenation of u and v and u -1  v -w, if w is such that u w -v, u -I  v = 0 if no such w exists." ></td>
	<td class="line x" title="292:471	F'={SEQ'I3(q,u) ESandqCF} if S E F t, p(S) = u s.t. 3q E F, (q, u) C S. We will see in the proof of correctness that p is properly defined." ></td>
	<td class="line x" title="293:471	15 Mohri (1994b) also gives a formalization of the algorithm." ></td>
	<td class="line x" title="294:471	16 As opposed to automata, a large class of finite-state transducers do not have any deterministic representation; they cannot be determinized." ></td>
	<td class="line x" title="295:471	242 Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging The determinization algorithm of Figure 21 computes the above subsequential transducer." ></td>
	<td class="line x" title="296:471	Let us now apply the determinization algorithm of Figure 21 on the finite-state transducer T4 of Figure 13 and show how it builds the subsequential transducer T10 of Figure 22." ></td>
	<td class="line x" title="297:471	Line 1 of the algorithm builds the first state and instantiates it with the pair {(0, e)}." ></td>
	<td class="line x" title="298:471	q and n respectively denote the current state and the number of states having been built so far." ></td>
	<td class="line x" title="299:471	At line 5, one takes all the possible input symbols w; here only a is possible, w' of line 6 is the output symbol, w'= e." ></td>
	<td class="line x" title="300:471	( A a(0,a,~')), ~'E1,2} thus w' = a(0,a, 1) A 6(0,a,2) = b A c = e. Line 8 is then computed as follows: s'= U U ~ff0} ~'E1,2} thus S' = { (1, a (0, a, 1 )) } U { (2, 6 (0, a, 2) } = { (1, b), (2, c) }." ></td>
	<td class="line x" title="301:471	Since no r verifies the condition on line 9, a new state e is created to which the transition labeled a/w = a/e points and n is incremented." ></td>
	<td class="line x" title="302:471	On line 15, the program goes to the construction of the transitions of state 1." ></td>
	<td class="line x" title="303:471	On line 5, d and e are then two possible symbols." ></td>
	<td class="line x" title="304:471	The first symbol, h, at line 6, is such that w' is w' = A b. 6(1,h,~')) = bh." ></td>
	<td class="line x" title="305:471	F/'cd(1,h)={2} Henceforth, the computation of line 8 leads to S'= U U {(q ''(bh)-l'b'h)}={(2'e)}' qE1} ~'E2} State 2 labeled {(2, e)} is thus added, and a transition labeled h/bh that points to state 2 is also added." ></td>
	<td class="line x" title="306:471	The transition for the input symbol e is computed the same way." ></td>
	<td class="line x" title="307:471	The subsequential transducer generated by this algorithm could in turn be minimized by an~'algorithm described in Mohri (1994a)." ></td>
	<td class="line x" title="308:471	However, in our case, the transducer is nearly minimal." ></td>
	<td class="line x" title="309:471	9.2 Proof of Correctness Although it is decidable whether a function is subsequential or not (Choffrut 1977), the determinization algorithm described in the previous section does not terminate when run on a nonsubsequential function." ></td>
	<td class="line x" title="310:471	Two issues are addressed in this section." ></td>
	<td class="line x" title="311:471	First, the proof of soundness: the fact that if the algorithm terminates, then the output transducer is deterministic and represents the same function." ></td>
	<td class="line x" title="312:471	Second, the proof of completeness: the algorithm terminates in the case of subsequential functions." ></td>
	<td class="line x" title="313:471	Soundness and completeness are a consequence of the main proposition, which states that if a transducer T represents a subsequential function f, then the algorithm DeterminizeTransducer described in the previous section applied on T computes a subsequential transducer representing the same function." ></td>
	<td class="line x" title="314:471	In order to simplify the proofs, we will only consider transducers that do not have e input transitions, that is E C Q x ~ x ~* x Q, and also without loss of generality, 243 Computational Linguistics Volume 21, Number 2 DeterminizeTransducer(T' = (G, Q', i', F', ,,, p), T = (~I, Q, i, F, E)) 9 10 11 12 13 14 15 16 i'= 0;q = 0;n = 1;C'\[0\] = {(0,~)};F' = 0;Q'= 0; do { S = C'\[q\];Q' = Q'u {q}; if 3(~, u)  S s.t. ~  F then F' = F' U {q} and p(q) = u; foreach w such that 3(~,u) E S and d(~,w) 0 { w,= A A u 61,,w,,'l G,)es ~'edGw) q*w=w'; s'= U U (~,u) es 7' edGw) if 3r E \[0,n -1\] such that C'\[r\] == S' e=r; else C'\[e = n + +\] = S'; q@w=e; } q++; }while(q < n); Figure 21 Determinization algorithm." ></td>
	<td class="line x" title="315:471	h/bh Figure 22 Subsequential transducer T10 such that IT10I = IT4I . transducers that are reduced and that are deterministic in the sense of finite-state automata." ></td>
	<td class="line x" title="316:471	17 In order to prove this proposition, we need to establish some preliminary notations and lemmas." ></td>
	<td class="line x" title="317:471	First we extend the definition of the transition function d, the emission function 6, the deterministic transition function @, and the deterministic emission function * on words in the classical way." ></td>
	<td class="line x" title="318:471	We then have the following properties: ab) = U a(q',b) 6(ql,ab, q2) = U 6(ql, a, q')." ></td>
	<td class="line x" title="319:471	6(q', b, q2) {q' Cd(ql,a) \[q2 Cd(q',b ) } qab = (qa)b 17 A transducer defines an automaton whose labels are the pairs 'input/output'; this automaton is assumed to be deterministic." ></td>
	<td class="line x" title="320:471	244 Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging q,ab = (q,a).(qa),b For the following, it is useful to note that if IT I is a function, then 6 is a function too." ></td>
	<td class="line x" title="321:471	The following lemma states an invariant that holds for each state S built within the algorithm." ></td>
	<td class="line x" title="322:471	The lemma will later be used for the proof of soundness." ></td>
	<td class="line x" title="323:471	Lemma 1 Let I = C'\[0\] be the initial state." ></td>
	<td class="line x" title="324:471	At each iteration of the 'do' loop in DeterminizeTransducer, for each S --C'\[q\] and for each w E ~* such that I  w = S, the following holds: (i) I,w= /~ 6(i,w,q) qEd(i,w) (ii) S=Iw={(q,u) lqEd(i,w)andu=(I*w)-l.6(i,w,q)} Proof (i) and (ii) are obviously true for S = I (since d(i, ~) = i and ~(i, c, i) = c), and we will show that given some w E ~* if it is true for S = I  w, then it is also true for $1 = S @ a = I Q wa for all a E Y Assuming that (i) and (ii) hold for S and w, then for each a E ~: A ~(i,w,q)." ></td>
	<td class="line x" title="325:471	~(q,a,q') qEd(i,w),q' Ed(q,a) = (I,w)." ></td>
	<td class="line x" title="326:471	A ') qEd( i,w),q' Ed(q,a ) = A (q,u ) ES=Iw,q' Ed(q,a) = (I,w).(S,a) = I,w.(Iw),a = I,wa This proves (i)." ></td>
	<td class="line x" title="327:471	We now turn to (ii)." ></td>
	<td class="line x" title="328:471	Assuming that (i) and (ii) hold for S and w, then for each a E ~, let $1 = S  a; the algorithm (line 8) is such that $1 = ( (q',u') \] 3(q,u) E S,q' E d(q,a) and u'= (S,a) -1 . u . 6(q,a,q') } Let $2 -{(q',u') I q' E d(i, wa) and u' = (I  wa) -1." ></td>
	<td class="line x" title="329:471	6(i, wa, q')} We show that $1 c $2." ></td>
	<td class="line x" title="330:471	Let (q',u') E $1, then 3(q,u) E S s.t. q' E d(q,a) and u' = (S * a)-l." ></td>
	<td class="line x" title="331:471	u. 6(q, a, q')." ></td>
	<td class="line x" title="332:471	Since u = (I  w)-I . 6(i, w, q), then u' = (S * a)-I ." ></td>
	<td class="line x" title="333:471	(I * w)-I . 6(i,w,q)." ></td>
	<td class="line x" title="334:471	6(q,a,q'); that is, u' = (I*wa) -1." ></td>
	<td class="line x" title="335:471	6(i, wa, q')." ></td>
	<td class="line x" title="336:471	Thus (q',u') E $2." ></td>
	<td class="line x" title="337:471	Hence $1 c $2." ></td>
	<td class="line x" title="338:471	We now show that $2 c $1." ></td>
	<td class="line x" title="339:471	Let (q',u') E $2, and let q E d(i,w) be s.t. q' E d(q,a) and u = (I, w) -1 . 6(i,w,q) then (q,u) E S and since u' = (I* wa) -1  6(i, wa, q') = (s,a) -1  u ." ></td>
	<td class="line x" title="340:471	(q',u') E sl This concludes the proof of (ii)." ></td>
	<td class="line x" title="341:471	\[\] 245 Computational Linguistics Volume 21, Number 2 The following lemma states a common property of the state S, which will be used in the complexity analysis of the algorithm." ></td>
	<td class="line x" title="342:471	Lemma 2 Each S = C'\[q\] built within the 'do' loop is s.t. Vq E Q, there is at most one pair (q, w) c S with q as first element." ></td>
	<td class="line x" title="343:471	Proof Suppose (q, wl) c S and (q, w2) c S, and let w be s.t. Iw = S. Then Wl = (I W) -1 ' fi(i, w, q) and w2 = (I  w)-I . 6(i, w, q)." ></td>
	<td class="line x" title="344:471	Thus W 1 = W 2." ></td>
	<td class="line x" title="345:471	\[\] The following lemma will also be used for soundness." ></td>
	<td class="line x" title="346:471	It states that the final state emission function is indeed a function." ></td>
	<td class="line x" title="347:471	Lemma 3 For each S built in the algorithm, if (q, u), (q', u') c S, then q, q' E F ~ u = u' Proof Let S be one state set built in line 8 of the algorithm." ></td>
	<td class="line x" title="348:471	Suppose (q, u), (q', u') E S and q, q' E F. According to (ii) of lemma 1, u = (I,w) -1 .6(i,w,q) and u' = (I,w) -1.6(i,w,q')." ></td>
	<td class="line x" title="349:471	Since IT\[ is a function and {6(i,w,q),6(i,w,q')} E ITl(w) then 6(i,w,q) = 6(i,w,q'), therefore u = uq \[\] The following lemma will be used for completeness." ></td>
	<td class="line x" title="350:471	Lemma 4 Given a transducer T representing a subsequential function, there exists a bound M s.t. for each S built at line 8, for each (q,u) E S, lu\[ < M. We rely on the following theorem proven by Choffrut (1978): Theorem 1 A function f on G* is subsequential iff it has bounded variations and for any rational language L C ~*, f-1 (L) is also rational." ></td>
	<td class="line x" title="351:471	with the following two definitions: Definition The left distance between two strings u and v is I\[u,v\[I = \[u\[ + Iv\[ 2\[u/~ v\[." ></td>
	<td class="line x" title="352:471	Definition A function f on G* has bounded variations iff for all k ~ 0, there exists K > 0 s.t. u,v C dom(f), \[\[u,v\[\[ <_ k ~ \]\[f(u),f(v)\[\[ <_ K. Proof of Lemma 4 Let f = IT\[." ></td>
	<td class="line x" title="353:471	For each q E Q, let c(q) be a string w s.t. d(q,w) N F ~ 0 and s.t. \[w\[ is minimal among such strings." ></td>
	<td class="line x" title="354:471	Note that \[c(q)\[ _< \[IT\[\] where \[IT\[\[ is the number of states 246 Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging in T. For each q c Q let s(q) E Q be a state s.t. s(q) c d(q,c(q)) AF." ></td>
	<td class="line x" title="355:471	Let us further define M1 = maxl6(q,c(q),s(q))\] qEQ M2 = max Ic(q)l qEQ i %l~ l Since f is subsequential, it is of bounded variations, therefore there exists K s.t. if \]\[u, vi\] ~ aM 2 then I\[f(u),f(v)\] I G K. Let M = K + 2M1." ></td>
	<td class="line x" title="356:471	Let S be a state set built at line 8, let w be s.t. Iw = S and A = I,w. Let (ql, u) E S. Let (q2, v) C S be s.t. u A v = c. Such a pair always exists, since if not thus \[A. \] A u'\] > 0 (q',u')ES A u'l = I A .x'u'l>l,',l (q',u')~s (q',u')cs Thus, because of (ii) in Lemma 1, I A 6(i,w,q')\] > II,wl q' Ed(i,w) which contradicts (i) in Lemma 1." ></td>
	<td class="line x" title="357:471	Let w = ~(ql, c(ql), s(ql)) and a;' = 6(q2, c(q2), s(q2))." ></td>
	<td class="line x" title="358:471	Moreover, for any a,b,c,d E ~*, Iia, ciI <_ \]lab, cd\[I + Ibl + \[d I. In fact, Ilab, cdiI = \[ab\[ + IcdI2Iab A cd I = lal + I c\] + IbI + IdI2Iab A cd I = II a, c\]I + 21a A c I + \[b I + \]d I -2lab A cd I but labAcd\] <_ laAcI +Ib\[+\]d\[ and since \]Iab, cd\[I = Ila, cI\[-2(\[abAcd I -\[aAc I -\[b I -IdI) IbIIdI one has Iia, cil < I\]ab, cdll + Ib\[ + Idl." ></td>
	<td class="line x" title="359:471	Therefore, in particular, luI < \]\[Au, AvI\[ < JiAua;,Avw'\]\[ + \]0; I + Iw'I, thus I u\] < Iif(w  c(ql)),f(w, c(q2))I\] q2M1." ></td>
	<td class="line x" title="360:471	But \]\[w. c(ql),W' c(q2)ll G \]c(ql)\[ + Ic(q2)I ~ 2M2, thus Iif(w  c(ql)),f(w' c(q2))\[\] < K and therefore I u\] < K + 2M 1 = M. \[\] The time is now ripe for the main proposition, which proves soundness and completeness." ></td>
	<td class="line x" title="361:471	Proposition If a transducer T represents a subsequential function f, then the algorithm DeterminizeTransducer described in the previous section applied on T computes a subsequential transducer ~representing the same function." ></td>
	<td class="line x" title="362:471	Proof Lemma 4 shows that the algorithm always terminates if IT\] is subsequential." ></td>
	<td class="line x" title="363:471	Let us show that dom(iTI) c dom(iTI)." ></td>
	<td class="line x" title="364:471	Let w E ~* s.t. w is not in dom(iTI), then d(i, w) M F = 0." ></td>
	<td class="line x" title="365:471	Thus, according to (ii) of Lemma 1, for all (q, u) c I  w, q is not in F, thus I  w is not terminal and therefore w is not in dom(~-)." ></td>
	<td class="line x" title="366:471	Conversely, let w E dom(iT\[)." ></td>
	<td class="line x" title="367:471	There exists a qf C F s.t. IT\](w) = 6(i,w, qf) and s.t. qf C d(i,w)." ></td>
	<td class="line x" title="368:471	Therefore \]Zi(w ) = (I, w)." ></td>
	<td class="line x" title="369:471	((I* w) -16(i,w, qf)) and according to (ii) of Lemma 1 (qf, (I * w) -I  6(i,w, qf)) c I  w and since qf E F, Lemma 3 shows that p(I w) = (I,w) -1." ></td>
	<td class="line x" title="370:471	~(i,w, qf), thus ITI(w) = (I,w)." ></td>
	<td class="line x" title="371:471	p(I w) = ITi(w)." ></td>
	<td class="line x" title="372:471	\[\] 247 Computational Linguistics Volume 21, Number 2 9.3 Worst-Case Complexity In this section we give a worst-case upper bound of the size of the subsequential transducer in terms of the size of the input transducer." ></td>
	<td class="line x" title="373:471	Let L = {w E G' s.t. Iw\[ <__ M}, where M is the bound defined in the proof of Lemma 4." ></td>
	<td class="line x" title="374:471	Since, according to Lemma 2, for each state set Q~, for each q E Q, Q' contains at most one pair (q, w), the maximal number N of states built in the algorithm is smaller than the sum of the number of functions from states to strings in L for each state set, that is N < ILl IQ't Q' E2Q we thus have N _< 2 IQI x ILl iQI -2 IQI x 2 \[Qlxlg2 iLl and therefore N _< 2 IQl(l+lglLI)." ></td>
	<td class="line x" title="375:471	Moreover, M+' 1 ILl = 1 + lye\] + + ISl M ISl 1 if Is\] > 1 and ILl = M+I if = 1." ></td>
	<td class="line x" title="376:471	In this last formula, M = K+2M1, as described in Lemma 4." ></td>
	<td class="line x" title="377:471	Note that if P = MAXa~sl6(q,a, q')l is the maximal length of the simple transitions emissions, M1 ~ IQI x P, thus M _< K + 2 x IQI x P. Therefore, if \[E I > 1, the number of states N is bounded: i:gl(K+2 x IQI xP+1-1 ) N <_ 2 IQIx(l+lg i~l-, and if lee = 1, N ~ 2 \[QIx(l+lg(K+2xiQLxP+l)) 10." ></td>
	<td class="line x" title="378:471	Subsequentiality of Transformation-Based Systems The proof of correctness of the determinization algorithm and the fact that the algorithm terminates on the transducer encoding Brill's tagger show that the final function is subsequential and equivalent to Brill's original tagger." ></td>
	<td class="line x" title="379:471	In this section, we prove in general that any transformation-based system, such as those used by Brill, is a subsequential function." ></td>
	<td class="line x" title="380:471	In other words, any transformationbased system can be turned into a deterministic finite-state transducer." ></td>
	<td class="line x" title="381:471	We define transformation-based systems as follows." ></td>
	<td class="line x" title="382:471	Definition A transformation-based system is a finite sequence (f\],,fn) of subsequential functions whose domains are bounded." ></td>
	<td class="line x" title="383:471	Applying a transformation-based system consists of applying each function fi one after the other." ></td>
	<td class="line x" title="384:471	Applying one function consists of looking for the first position in the input at which the function can be triggered." ></td>
	<td class="line x" title="385:471	When the function is triggered, the longest possible string starting at that position is transformed according to this function." ></td>
	<td class="line x" title="386:471	After the string is transformed, the process is iterated starting at the end of the previously transformed string." ></td>
	<td class="line x" title="387:471	Then, the next function is applied." ></td>
	<td class="line x" title="388:471	The program ends when all functions have been applied." ></td>
	<td class="line x" title="389:471	It is not true that, in general, the local extension of a subsequential function is subsequential." ></td>
	<td class="line x" title="390:471	TM For instance, consider the function fa of Figure 23." ></td>
	<td class="line x" title="391:471	18 However, the local extensions of the functions we had to compute were subsequentiaL 248 Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging Figure 23 Function fa." ></td>
	<td class="line x" title="392:471	a:b a:b a:b The local extension of the function fa is not a function." ></td>
	<td class="line x" title="393:471	In fact, consider the input string daaaad; it can be decomposed either into d  aaa." ></td>
	<td class="line x" title="394:471	ad or into da  aaa." ></td>
	<td class="line x" title="395:471	d. The first decomposition leads to the output dbbbad, and the second one to the output dabbbd." ></td>
	<td class="line x" title="396:471	The intended use of the rules in the tagger defined by Brill is to apply each function from left to right." ></td>
	<td class="line x" title="397:471	In addition, if several decompositions are possible, the one that occurs first is the one chosen." ></td>
	<td class="line x" title="398:471	In our previous example, it means that only the output dbbbad is generated." ></td>
	<td class="line x" title="399:471	This notion is now defined precisely." ></td>
	<td class="line x" title="400:471	Let a be the rational function defined by a(a) = a for a c ~, a(\[) = a(\]) = ~ on the additional symbols '\[' and '\]', with a such that a(u. v) = a(u)." ></td>
	<td class="line x" title="401:471	a(v)." ></td>
	<td class="line x" title="402:471	Definition Let Y c ~+ and X = ~* ~*." ></td>
	<td class="line x" title="403:471	Y. ~*, a Y-decomposition of x is a string y E X." ></td>
	<td class="line x" title="404:471	Y. \]." ></td>
	<td class="line x" title="405:471	X)* s.t. a(y) = x For instance, if Y = dom(fa) -{aaa}, the set of Y-decompositions of x = daaad is { d \[aaa \]ad, da \[aaa \] d }." ></td>
	<td class="line x" title="406:471	Definition Let < be a total order on P, and let ~ = ~ U {\[,\]} be the al _phabet ~ with the two additional symbols '\[' and '\]'." ></td>
	<td class="line x" title="407:471	Let extend the order > to N by Va E ~, '\['< a and a < '\]'." ></td>
	<td class="line x" title="408:471	< defines a lexicographic order on ~* that we also denote <." ></td>
	<td class="line x" title="409:471	Let Y c 2 + and x c N*, the minimal Y-decomposition of x is the Y-decomposition which is minimal in (~*, <)." ></td>
	<td class="line x" title="410:471	For instance, the minimal dom(fa)-decomposition of daaaad is d\[aaa\]ad." ></td>
	<td class="line x" title="411:471	In fact, d\[aaaJad < da\[aaa\]d. Proposition Given Y C ~+ finite, the function mdy that to each x c G* associates its minimal Y-decomposition, is subsequential and total." ></td>
	<td class="line x" title="412:471	Proof Let dec be defined by dec(w) = u. \[." ></td>
	<td class="line x" title="413:471	v. 1." ></td>
	<td class="line x" title="414:471	dec((uv) -1 . w), where u, v E P~* are s.t. v E Y, 3v' c ~* with w = uvv' and lul is minimal among such strings and dec(w) -w if no such u, v exists." ></td>
	<td class="line x" title="415:471	The function mdy is total because the function dec always returns an output that is a Y-decomposition of w. We shall now prove that the function is rational and then that it has bounded variations; this will prove according to Theorem 1 that the function is subsequential." ></td>
	<td class="line x" title="416:471	In the following X = ~* P,*  YP,*." ></td>
	<td class="line x" title="417:471	The transduction Ty that generates the set of Y-decompositions is defined by Ty = Idx." ></td>
	<td class="line x" title="418:471	(eft." ></td>
	<td class="line x" title="419:471	Idyc/\]." ></td>
	<td class="line x" title="420:471	Idx)* where Idx (resp." ></td>
	<td class="line x" title="421:471	Idy) stands for the identity function on X (resp." ></td>
	<td class="line x" title="422:471	Y)." ></td>
	<td class="line x" title="423:471	Furthermore, 249 Computational Linguistics Volume 21, Number 2 Figure 24 Transduction T~,>." ></td>
	<td class="line x" title="424:471	C D the transduction TU,> that to each string w E ~* associates the set of strings strictly greater than w, that is T~,>(w) = {w' E ~*I w < w'}, is defined by the transducer of ---2 -Figure 24, in which A = {(x,x)ix E G}, B = {(x,y) E ~2\[x < y}, C = G, D = {} x and E = G x {c}." ></td>
	<td class="line x" title="425:471	19 Therefore, the right-minimal Y-decomposition function mdy is defined by mdy -Ty (Tu,> o Ty), which proves that mdy is rational." ></td>
	<td class="line x" title="426:471	Letk > 0." ></td>
	<td class="line x" title="427:471	LetK = 6xk+6xM, whereM-maxx~yix I. Let u,v E G* bes.t. Iiu, vII _< k. Let us consider two cases: (i) I u A v I _< M and (ii) lu A v I > M." ></td>
	<td class="line x" title="428:471	(i): I u Av I _< M, thus \[uHv I ~ I u Av I + Iiu, vI\[< M+k. Moreover, for each w E Y~*, for each Y-decomposition w' of w, Iw'\[ _< 3 x \]w I. In fact, Y doesn't contain ~, thus the number of \[ (resp." ></td>
	<td class="line x" title="429:471	l) in w' is smaller than Iw\[." ></td>
	<td class="line x" title="430:471	Therefore, Imdy(u) I, Imdy(v)l <_ 3 x (M+k) thus \[Imdy(u),mdy(v)lI < K." ></td>
	<td class="line x" title="431:471	(ii): u A v = ~  a; with \[a; I = M. Let #, v be s.t. u = &w# and v = )~a;~." ></td>
	<td class="line x" title="432:471	Let )~', w', #', .~', a;' and v' be s.t. mdy(u) = )~'J#', mdy(v) -)~'~;'~,', c~(~') = ~(,V') = ~, c~(a;') = c~(~o') = w, o~(#') = # and ~(~,') = ~,." ></td>
	<td class="line x" title="433:471	Suppose that &' # &', for instance ), < )i,." ></td>
	<td class="line x" title="434:471	Let i be the first indice s.t." ></td>
	<td class="line x" title="435:471	(;f)i < (,VI)i. 20 We have two possible situations: (ii.1) ()~r)i = \[ and ;~' E ~ or (~')i ---\]." ></td>
	<td class="line x" title="436:471	In that case, since the length of the elements in Y is smaller than M = 14, one has &'~;' = .~1\[.~2\],~3 with \[~ll = i, ;~2 ~ Y and '~3 E . We also have ),'w' ' ' = /~1/~2/~ 3 with c~()~) = c~(&2) and the first letter of '~2 is different from \[." ></td>
	<td class="line x" title="437:471	Let )~4 be a Y-decomposition of ~ 3 y, then &1\[~2\]/~4 is a Y-decomposition of v strictly smaller than ~1 &~)~L,' = mdy (v), which contradicts the minimality of mdy (v)." ></td>
	<td class="line x" title="438:471	The second situation is (ii.2): (&~)i E ~." ></td>
	<td class="line x" title="439:471	and (&')i = \], then we have )~GJ = ~1\[,~2,~3\],~4 s.t. I,~1\[/~2I = i and ),'M' = .,~1\[,~2\],,~&~ s.t. C~(&~) = C~()~3) and c~(&~) = c~(&4)." ></td>
	<td class="line x" title="440:471	Let As be a Y-decomposition of ~', then ;~ \[/~2/~3\]/~5 is a Y-decomposition of v strictly smaller than,V'w'~,', which leads to the same contradiction." ></td>
	<td class="line x" title="441:471	Therefore, &' = )~' and since I#'\[+1~,'1 _< 3x (l#l+l~,l)--3x Ilu, vll _< 3x/, IImdyCu),mdy(v)ll <_ la;'l+la;'l+l#'l+l~,r'l < 2 x M + 3 x k _< K. This proves that mdy has bounded variations and therefore that it is subsequential." ></td>
	<td class="line x" title="442:471	\[\] N We can now define precisely what is the effect of a function when one applies it from left to right, as was done in the original tagger." ></td>
	<td class="line x" title="443:471	19 This construction is similar to the transduction built within the proof of Eilenberg's cross section theorem (Eilenberg 1974)." ></td>
	<td class="line x" title="444:471	20 (w)i refers to the i th letter in w. 250 Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging Definition If f is a rational function with bounded domain, Y = dom(f) c ~.+, the right-minimal local extension of f, denoted RmLocExt(f), is the composition of a right-minimal Y-decomposition mdy with Ida,  (\[/~." ></td>
	<td class="line x" title="445:471	f. \]/~." ></td>
	<td class="line x" title="446:471	Ida,)*." ></td>
	<td class="line x" title="447:471	RmLocExt being the composition of two subsequential functions, it is itself subsequential; this proves the following final proposition, which states that given a rulebased system similar to Brill's system, one can build a subsequential transducer that represents it: Proposition If (fl ,fn) is a sequence of subsequential functions with bounded domains and such that fi(~) = 0, then RmLocExt(h ) o o RmLocExt(fn) is subsequential." ></td>
	<td class="line x" title="448:471	We have proven in this section that our techniques apply to the class of transformationbased systems." ></td>
	<td class="line x" title="449:471	We now turn our attention to the implementation of finite-state transducers." ></td>
	<td class="line x" title="450:471	11." ></td>
	<td class="line x" title="451:471	Implementation of Finite-State Transducers Once the final finite-state transducer is computed, applying it to an input is straightforward: it consists of following the unique sequence of transitions whose left labels correspond to the input." ></td>
	<td class="line x" title="452:471	However, in order to have a complexity fully independent of the size of the grammar and in particular independent of the number of transitions at each state, one should carefully choose an appropriate representation for the transducer." ></td>
	<td class="line x" title="453:471	In our implementation, transitions can be accessed randomly." ></td>
	<td class="line x" title="454:471	The transducer is first represented by a two-dimensional table whose rows are indexed by states and whose columns are indexed by the alphabet of all possible input letters." ></td>
	<td class="line x" title="455:471	The content of the table at line q and at column a is the word w such that the transition from q with the input label a outputs w. Since only a few transitions are allowed from many states, this table is very sparse and can be compressed." ></td>
	<td class="line x" title="456:471	This compression is achieved while maintaining random access using a procedure for sparse data tables following the method given by Tarjan and Yao (1979)." ></td>
	<td class="line x" title="457:471	12." ></td>
	<td class="line x" title="458:471	Conclusion The techniques described in this paper are more general than the problem of part-ofspeech tagging and are applicable to the class of problems dealing with local transformation rules." ></td>
	<td class="line x" title="459:471	We showed that any transformation-based program can be transformed into a deterministic finite-state transducer." ></td>
	<td class="line x" title="460:471	This yields to optimal time implementations of transformation based programs." ></td>
	<td class="line x" title="461:471	As a case study, we applied these techniques to the problem of part-of-speech tagging and presented a finite-state tagger that requires n steps to tag a sentence of length n, independently of the number of rules and the length of the context they require." ></td>
	<td class="line x" title="462:471	We achieved this result by representing the rules acquired for Brill's tagger as nondeterministic finite-state transducers." ></td>
	<td class="line x" title="463:471	We composed each of these nondeterministic transducers and turned the resulting transducer into a deterministic transducer." ></td>
	<td class="line x" title="464:471	The resulting deterministic transducer yields a part-of-speech tagger that operates in optimal time in the sense that the time to assign tags to a sentence corresponds to the time required to follow a single path in this deterministic finite-state machine." ></td>
	<td class="line x" title="465:471	The 251 Computational Linguistics Volume 21, Number 2 tagger outperforms in speed both Brill's tagger and stochastic taggers." ></td>
	<td class="line x" title="466:471	Moreover, the finite-state tagger inherits from the rule-based system its compactness compared with stochastic taggers." ></td>
	<td class="line x" title="467:471	We also proved the correctness and the generality of the methods." ></td>
	<td class="line x" title="468:471	We believe that this finite-state tagger will also be found useful when combined with other language components, since it can be naturally extended by composing it with finite-state transducers that could encode other aspects of natural language syntax." ></td>
	<td class="line x" title="469:471	Acknowledgments We thank Eric Brill for providing us with the code of his tagger and for many useful discussions." ></td>
	<td class="line x" title="470:471	We also thank Aravind K. Joshi, Mark Liberman, and Mehryar Mohri for valuable discussions." ></td>
	<td class="line x" title="471:471	We thank the anonymous reviewers for many helpful comments that led to improvements in both the content and the presentation of this paper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="J95-3004
Learning Morpho-Lexical Probabilities From An Untagged Corpus With An Application To Hebrew
Levinger, Moshe;Itai, Alon;Ornan, Uzzi;"></td>
	<td class="line x" title="1:411	Learning Morpho-Lexical Probabilities from an Untagged Corpus with an Application to Hebrew Moshe Levinger* Haifa Research Laboratory Alon Itait Technion Uzzi Ornan t Technion This paper proposes a new approach for acquiring morpho-lexical probabilities from an untagged corpus." ></td>
	<td class="line x" title="2:411	This approach demonstrates a way to extract very useful and nontrivial information from an untagged corpus, which otherwise would require laborious tagging of large corpora." ></td>
	<td class="line x" title="3:411	The paper describes the use of these morpho-lexical probabilities as an information source for morphological disambiguation in Hebrew." ></td>
	<td class="line x" title="4:411	The suggested method depends primarily on the following property: a lexical entry in Hebrew may have many different word forms, some of which are ambiguous and some of which are not." ></td>
	<td class="line x" title="5:411	Thus, the disambiguation of a given word can be achieved using other word forms of the same lexical entry." ></td>
	<td class="line x" title="6:411	Even though it was originally devised and implemented for  dealing with the morphological ambiguity problem in Hebrew, the basic idea can be extended and used to handle similar problems in other languages with rich morphology." ></td>
	<td class="line x" title="7:411	1." ></td>
	<td class="line x" title="8:411	Introduction This paper addresses the problem of morphological disambiguation in Hebrew by extracting statistical information from an untagged corpus." ></td>
	<td class="line x" title="9:411	Yet, the primary point is not to propose a method for morphological disambiguation per se, but rather to suggest a method to compute morpho-lexical probabilities to be used as a linguistic source for morphological disambiguation." ></td>
	<td class="line x" title="10:411	Let us start with a few definitions and terminology that will be used throughout this paper." ></td>
	<td class="line x" title="11:411	We consider written languages, and for the purpose of this paper, a word is a string of letters delimited by spaces or punctuation." ></td>
	<td class="line x" title="12:411	Given a language L, and a word w E L, we can find (manually or automatically by a morphological analyzer for L) all the possible morphological analyses of the word w. Suppose a word w has k different analyses, then A1 Ak, will be used to denote these k analyses." ></td>
	<td class="line x" title="13:411	A word is morphologically ambiguous if k > 2." ></td>
	<td class="line x" title="14:411	The number and character of the analyses depend on the language model." ></td>
	<td class="line x" title="15:411	We have used the definitions of the automatic morphological analyzer developed at the IBM Scientific Center, Haifa, Israel (Bentur, Angel', and Segev 1992)." ></td>
	<td class="line x" title="16:411	Given a text T with n words: Wl,, wn, for each morphologically ambiguous word Wi E T, with k analyses: A1  Ak, there is one analysis, 1 AF E {A1  Ak} that is the * Haifa Research Laboratory, IBM Science & Technology, Haifa, Israel." ></td>
	<td class="line x" title="17:411	jComputer Science Department, Technion, Haifa, Israel." ></td>
	<td class="line x" title="18:411	1 We will assume that there is only one right analysis, although, in rare cases, there might be more than one." ></td>
	<td class="line x" title="19:411	(~) 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 3 right analysis, while all the other k 1 analyses of w are wrong analyses." ></td>
	<td class="line x" title="20:411	The same word wi in a different text, may have, of course, a different right analysis, thus, right and wrong in this case are meaningful only with respect to the context in which wi appears." ></td>
	<td class="line x" title="21:411	Morphological disambiguation of a text T is done by indicating for each ambiguous word in T--which of its different analyses is the right one." ></td>
	<td class="line x" title="22:411	At present, this can be done manually by a speaker of the language, and hopefully in the future it will be done automatically by a computer program." ></td>
	<td class="line x" title="23:411	When dealing with automatic disambiguation of a text it is sometimes useful to reduce its ambiguity level." ></td>
	<td class="line x" title="24:411	A reduction of the ambiguity level of an ambiguous word w, with k morphological analyses: A1  Ak, occurs when it is possible to select from A1  Ak, a proper subset of I analyses 1 G 1 < k, such that the right analysis of w is one of these 1 analyses." ></td>
	<td class="line x" title="25:411	In the case where l = 1, we say that the word w is fully disambiguated." ></td>
	<td class="line x" title="26:411	Since this paper suggests a method for morphological disambiguation using probabilities, the notion of morpho-lexical probabilities is also required." ></td>
	<td class="line x" title="27:411	Our model of the language is based on a large fixed Hebrew corpus." ></td>
	<td class="line x" title="28:411	For a word w with k analyses, A1,, Ak, the morpho-lexical probability of Ai is the estimate of the conditional probability P(Ai \[ w) from the given corpus, i.e., Pi = P(ai \] w) = no." ></td>
	<td class="line x" title="29:411	of times Ai was the right analysis of w no." ></td>
	<td class="line x" title="30:411	of occurrences of w Note that Pi is the probability that Ai is the right analysis of w independently of the context in which w appears." ></td>
	<td class="line x" title="31:411	Since the word w has exactly k different analyses: k E~ 1P(Ai w) Ei=I Pi = = \[ = 1." ></td>
	<td class="line x" title="32:411	For reasons that will be elaborated in Section 2, our problem is most acute in Hebrew and some other languages (e.g. , Arabic), though ambiguity problems of a similar nature occur in other languages." ></td>
	<td class="line x" title="33:411	One such problem is sense disambiguation." ></td>
	<td class="line x" title="34:411	In the context of machine translation, Dagan and Itai (Dagan, Itai, and Schwall 1991; Dagan and Itai 1994) used corpora in the target language to resolve ambiguities in the source language." ></td>
	<td class="line x" title="35:411	Yarowsky (1992) proposed a method for sense disambiguation using wide contexts." ></td>
	<td class="line x" title="36:411	Part-of-speech tagging--deciding the correct part of speech in the current context of the sentence--has received major attention." ></td>
	<td class="line x" title="37:411	Most successful methods have followed speech recognition systems (Jelinek, Mercer, and Roukos 1992) and used large corpora to deduce the probability of each part of speech in the current context (usually the two previous words--trigrams)." ></td>
	<td class="line oc" title="38:411	These methods have reported performance in the range of 95-99% 'correct' by word (DeRose 1988; Cutting et al. 1992; Jelinek, Mercer, and Roukos 1992; Kupiec 1992)." ></td>
	<td class="line x" title="39:411	(The difference in performance is due to different evaluation methods, different tag sets, and different corpora)." ></td>
	<td class="line x" title="40:411	See Church (1992) for a survey." ></td>
	<td class="line x" title="41:411	Our work did not use the trigram model, since because of the relatively free word order in Hebrew it was less promising, and also, in some cases the different choices are among words of the same part-of-speech category." ></td>
	<td class="line x" title="42:411	Thus tagging for part of speech alone would not solve our problems." ></td>
	<td class="line x" title="43:411	Note that a single morphological analysis may correspond to several senses." ></td>
	<td class="line x" title="44:411	Even though each sense may have different behavior patterns, in practice this did not present a problem for our program." ></td>
	<td class="line x" title="45:411	The rest of this paper is organized as follows." ></td>
	<td class="line x" title="46:411	Sections 2 through 4 include a description of the morphological ambiguity problem in Hebrew, followed by the claim that knowing the morpho-lexical probabilities of an ambiguous word can be very effective for automatic morphological disambiguation in Hebrew." ></td>
	<td class="line x" title="47:411	384 Moshe Levinger et al. Table 1 The dimension of morphological ambiguity in Hebrew." ></td>
	<td class="line x" title="48:411	Learning Morpho-Lexical Probabilities no." ></td>
	<td class="line x" title="49:411	of Analyses 1 2 3 4 5 6 no." ></td>
	<td class="line x" title="50:411	of Word-Tokens 17,551 9,876 6,401 2,760 1,309 493 % 45.1 25.4 16.5 7.1 3.37 1.27 no." ></td>
	<td class="line x" title="51:411	of Analyses 7 8 9 10 11 12 13 no." ></td>
	<td class="line x" title="52:411	of Word-Tokens 337 134 10 18 1 3 5 % 0.87 0.34 0.02 0.05 0.002 0.007 0.01 Then, in Sections 5 and 6, we present the key idea of this paper: How to acquire a good approximation for the morpho-lexical probabilities from an untagged corpus." ></td>
	<td class="line x" title="53:411	Using this method we can find for each ambiguous word w with k analyses: A1  Ak, probabilities P1 , P---k that are an approximation to the morpho-lexical probabilities: P1 , Pk." ></td>
	<td class="line x" title="54:411	In Section 7 we clarify some subtle aspects of the algorithm presented in Section 6 by looking at its application to several ambiguous words in Hebrew." ></td>
	<td class="line x" title="55:411	A description of an experiment that serves to evaluate the approximated morpho-lexical probabilities calculated using an untagged corpus will be given in Section 8." ></td>
	<td class="line x" title="56:411	Finally, in Section 9, a simple strategy for morphological disambiguation in Hebrew using morpho-lexical probabilities will be described." ></td>
	<td class="line x" title="57:411	This simple strategy was used in an experiment conducted in order to test the significance of the morpho-lexical probabilities as a basis for morphological disambiguation in Hebrew." ></td>
	<td class="line x" title="58:411	The experiment shows that using our method we can significantly reduce the level of ambiguity in a Hebrew text." ></td>
	<td class="line x" title="59:411	2." ></td>
	<td class="line x" title="60:411	Morphological Ambiguity in Hebrew Morphological ambiguity is a severe problem in modern Hebrew." ></td>
	<td class="line x" title="61:411	Thus, finding methods to reduce the morphological ambiguity in the language is a great challenge for researchers in the field and for people who wish to develop natural language applications for Hebrew." ></td>
	<td class="line x" title="62:411	Table 1 demonstrates the dimension of the morphological ambiguity in Hebrew." ></td>
	<td class="line x" title="63:411	The data was obtained by analyzing large texts, randomly chosen from the Hebrew press, consisting of nearly 40,000 word-tokens." ></td>
	<td class="line x" title="64:411	According to this table, the average number of possible analyses per word-token was 2.1, while 55% of the word-tokens were morphologically ambiguous." ></td>
	<td class="line x" title="65:411	The main reason for this amount of ambiguity is the standard writing system used in modern Hebrew (unpointed script)." ></td>
	<td class="line x" title="66:411	In this writing system not all the vowels are represented, several letters represent both consonants and different vowels, and gemination is not represented at all (Ornan 1986, 1991)." ></td>
	<td class="line x" title="67:411	The rich morphology of the language and the fact that many particles are attached to the word, forming a single string, further contribute to the morphological ambiguity." ></td>
	<td class="line x" title="68:411	In order to demonstrate the complexity of the problem, we should take a closer look at Hebrew morphology." ></td>
	<td class="line x" title="69:411	A morphological analysis of a word in Hebrew should extract the following information:  lexical entry 385 Computational Linguistics Volume 21, Number 3  category  tense (for verbs only)  attached particles (i.e. , prepositions, connectives, determiners)  status--a flag indicating whether a noun is in its construct or absolute form  gender, number, and person (for nouns, adjectives, verbs etc)." ></td>
	<td class="line x" title="70:411	 gender, number, and person of pronoun suffixes For example, the morphological analysis of the Hebrew string 1)D~I'~'lVd~I (written in a Latin transliteration 2 WK$RAYTYW) is as follows:  lexical entry: R^H (nN'l)--the verb 'to see'  category: verb  tense: past  attached particles: W + K$ (~ + ~) = 'and when'  gender: feminine/masculine, number: singular, person: first person  object pronoun: masculine, singular, third person Thus, WK$R^YTYW should be translated into English as: 'and when I saw him'." ></td>
	<td class="line x" title="71:411	To see the nature of the morphological ambiguity in Hebrew, consider, for example, the string HQPH (ngpn), which has three possible analyses: 1." ></td>
	<td class="line x" title="72:411	The determiner H + the noun QPH (ngp + D, 'the coffee')." ></td>
	<td class="line x" title="73:411	2." ></td>
	<td class="line x" title="74:411	The noun HQPH (ngpn, 'encirclement')." ></td>
	<td class="line x" title="75:411	3." ></td>
	<td class="line x" title="76:411	The noun HQP + the feminine possessive suffix H (n + qpn, 'her perimeter')." ></td>
	<td class="line x" title="77:411	The use of computers for morphological analysis of Hebrew words is nowadays well studied and understood." ></td>
	<td class="line x" title="78:411	Several high-quality morphological analyzers for Hebrew have been developed in the last decade." ></td>
	<td class="line x" title="79:411	One such morphological analyzer 3 was used to supply the input for the morphological disambiguation project described in this paper." ></td>
	<td class="line x" title="80:411	3." ></td>
	<td class="line x" title="81:411	Former Approaches Eliminating or reducing the ambiguity at this early stage of automatic processing of Hebrew is crucial for the efficiency and the success rate of parsers and other natural language applications." ></td>
	<td class="line x" title="82:411	It should be noted that the morphological ambiguity in Hebrew makes even 'simple' applications--as is often considered when dealing with other languages--complicated." ></td>
	<td class="line x" title="83:411	2 See Appendix A for the Hebrew-Latin transliteration." ></td>
	<td class="line x" title="84:411	3 The morphological analyzer was developed at the IBM Scientific Center, Haifa, Israel (Bentur, Angel, and Segev 1992)." ></td>
	<td class="line x" title="85:411	We would like to thank the center for letting us use it for research purposes." ></td>
	<td class="line x" title="86:411	386 Moshe Levinger et al. Learning Morpho-Lexical Probabilities One good example for this is full-text retrieval systems (Choueka 1980)." ></td>
	<td class="line x" title="87:411	Such systems must handle the morphological ambiguity problem." ></td>
	<td class="line x" title="88:411	To see that, consider, for example, the case where we look for all the texts with the word HQPH ('encirclement')." ></td>
	<td class="line x" title="89:411	Without morphological disambiguation, we get many texts which really include the word H+QPH ('the coffee'), or even HQP+H ('her perimeter') (Ornan 1987)." ></td>
	<td class="line x" title="90:411	Another application which is more difficult in Hebrew than in other languages is text-to-speech systems, which cannot be implemented in Hebrew without first solving the morphological ambiguity, since in many cases different analyses of a word imply different pronunciations." ></td>
	<td class="line x" title="91:411	A much simpler problem occurs in English, where for some words the correct syntactic tag is necessary for pronunciation (Church 1988)." ></td>
	<td class="line x" title="92:411	The notion that this ambiguity problem in Hebrew is very complicated and that it can be dealt with only by using vast syntactic and semantic knowledge has led researchers to look for solutions involving a considerable amount of human interaction." ></td>
	<td class="line x" title="93:411	Ornan (1986) for instance, developed a new writing system for Hebrew, called 'The Phonemic Script'." ></td>
	<td class="line x" title="94:411	This script enables the user to write Hebrew texts that are morphologically unambiguous, in order to use them later as an input for various kinds of natural language applications." ></td>
	<td class="line x" title="95:411	However, since regular Hebrew texts are not written in this script, they first must be transcribed to phonemic texts." ></td>
	<td class="line x" title="96:411	Choueka and Lusignan (1985) presented a system for the morphological tagging of large texts that is based on the short context of the word but also depends heavily on human interaction." ></td>
	<td class="line x" title="97:411	Methods using the short context of a word in order to resolve ambiguity (usually categorical ambiguity) are very common in English and other languages (DeRose 1988; Church 1988; Karlsson 1990)." ></td>
	<td class="line x" title="98:411	A system using this approach was developed by Levinger and Ornan in order to serve as a component in their project of morphological disambiguation in Hebrew (Levinger 1992)." ></td>
	<td class="line x" title="99:411	The main resource, used by this system for disambiguation, is a set of syntactic constraints that were defined manually by the authors and followed two theoretical works that defined short context rules for Hebrew (Pines 1975; Albeck 1992)." ></td>
	<td class="line x" title="100:411	The syntactic constraints approach, which is an extension of the short context approach, was found to be useful and reliable, but its applicability (based on the proportion of ambiguous words that were fully disambiguated) was very poor." ></td>
	<td class="line x" title="101:411	Hence, the overall performance of this system is much less promising in Hebrew than in other languages." ></td>
	<td class="line x" title="102:411	These results can be explained by the following properties of the ambiguity problem in Hebrew: 1." ></td>
	<td class="line x" title="103:411	(a) (b) In many cases two or more alternative analyses share the same category, and hence these alternatives satisfy the same syntactic constraints." ></td>
	<td class="line x" title="104:411	Moreover, there are cases where two or even more analyses share exactly the same morphological attributes and differ only in their lexical entry." ></td>
	<td class="line x" title="105:411	For instance, the word XLW (~n) has two such morphological analyses: The verb XLH (n~n), fem./masc., plural, third person, past tense ('they became ill')." ></td>
	<td class="line x" title="107:411	The verb XL (Vn), fem./masc., plural, third person, past tense ('they occurred')." ></td>
	<td class="line x" title="109:411	The short context constraints use unambiguous anchors that are often function words such as determiners and prepositions." ></td>
	<td class="line x" title="110:411	In English most such function words are unambiguous." ></td>
	<td class="line x" title="111:411	In Hebrew, these words are almost always morphologically ambiguous." ></td>
	<td class="line x" title="112:411	Moreover, many of them appear as prefixes of the word to be analyzed, and their identification is part of the morphological analysis." ></td>
	<td class="line x" title="113:411	We thus have a circularity problem: In order to perform the morphological analysis, we need the short context, 387 Computational Linguistics Volume 21, Number 3 to identify the short context, we have to find anchors, but in order to find such words, we need first to perform the morphological analysis." ></td>
	<td class="line x" title="114:411	3." ></td>
	<td class="line x" title="115:411	The word order in Hebrew is rather free." ></td>
	<td class="line x" title="116:411	4." ></td>
	<td class="line x" title="117:411	Our Approach The purpose of this paper is to suggest a new approach to deal with the abovementioned problem." ></td>
	<td class="line x" title="118:411	This approach provides highly useful data that can be used by systems for automatic, unsupervised morphological tagging of Hebrew texts." ></td>
	<td class="line x" title="119:411	In order to justify and motivate our approach, we must first make the following conjecture: Although the Hebrew language is highly ambiguous morphologically, it seems that in many cases a native speaker of the language can accurately 'guess' the right analysis of a word, without even being exposed to the concrete context in which it appears." ></td>
	<td class="line x" title="120:411	The accuracy can even be enhanced if the native speaker is told from which sublanguage the ambiguous word was taken." ></td>
	<td class="line x" title="121:411	If this conjecture is true, we can now suggest a simple strategy for automatic tagging of Hebrew texts: For each ambiguous word, find the morpho-lexical probabilities of each possible analysis." ></td>
	<td class="line x" title="122:411	If any of these analyses is substantially more frequent than the others, choose it as the right analysis." ></td>
	<td class="line x" title="123:411	As we have already noted, by saying morpho-lexical probabilities, we mean the probability of a given analysis to be the right analysis of a word, independently of the context in which it appears." ></td>
	<td class="line x" title="124:411	It should be emphasized that having these morpholexical probabilities enables us not only to use them rather naively in the abovementioned strategy, but also to incorporate these probabilities into other systems that exploit higher level knowledge (syntactic, semantic etc.)." ></td>
	<td class="line x" title="125:411	Such a system that uses the morpho-lexical probabilities together with a syntactic knowledge is described in Levinger (1992)." ></td>
	<td class="line x" title="126:411	5." ></td>
	<td class="line x" title="127:411	Acquiring the Probabilities Adopting this approach leaves us with the problem of finding the morpho-lexical probabilities for the different analyses of every ambiguous word in the language." ></td>
	<td class="line x" title="128:411	Since we use a large corpus for this purpose, the morpho-lexical probabilities we acquire must be considered relative to this specific training corpus." ></td>
	<td class="line x" title="129:411	One way to acquire morpho-lexical probabilities from a corpus is to use a large tagged corpus." ></td>
	<td class="line x" title="130:411	Given a corpus in which every word is tagged with its right analysis, we can find the morpho-lexical probabilities as reflected in the corpus." ></td>
	<td class="line x" title="131:411	This is done by simply counting for each analysis the number of times that it was the right analysis, and using these counters to calculate the probability of each analysis being the right one." ></td>
	<td class="line x" title="132:411	The main drawback of this solution is the need for a very large tagged corpus." ></td>
	<td class="line x" title="133:411	No such corpus exists for modern Hebrew." ></td>
	<td class="line x" title="134:411	Moreover, for such a solution a separate tagged corpus is required for each domain." ></td>
	<td class="line x" title="135:411	The method we are about to present saves us the laborious effort of tagging a large corpus, and enables us to find a good approximation to the morpho-lexical probabilities by learning about them from an untagged corpus." ></td>
	<td class="line x" title="136:411	Using this method, one can easily move to a new domain by applying the method to a new untagged corpus suited to this new domain." ></td>
	<td class="line x" title="137:411	388 Moshe Levinger et al. Learning Morpho-Lexical Probabilities This might seem, at first sight, an impossible mission." ></td>
	<td class="line x" title="138:411	When we see the word HQPH in an untagged corpus we cannot automatically decide which of its possible readings is the right one." ></td>
	<td class="line x" title="139:411	The key idea is to shift each of the analyses of an ambiguous word in such a way that they all become distinguishable." ></td>
	<td class="line x" title="140:411	To be more specific, for each possible analysis (lexical entry + the morphological information), we define a set of words that we call Similar Words (SW)." ></td>
	<td class="line x" title="141:411	An element in this set is another word form of the same lexical entry that has similar morphological attributes to the given analysis." ></td>
	<td class="line x" title="142:411	These words are assumed similar to the analysis in the sense that we expect them to have approximately the same frequency in the language as the analysis they belong to." ></td>
	<td class="line x" title="143:411	A reasonable assumption of this kind would be, for instance, to say that the masculine form of a verb in a certain tense in Hebrew is expected to have approximately the same frequency as the feminine form of the same verb, in the same tense." ></td>
	<td class="line x" title="144:411	This assumption holds for most of the Hebrew verbs, since all Hebrew nouns (and not only animate ones) have the gender attribute." ></td>
	<td class="line x" title="145:411	4 To see a concrete example, consider the word R^H (nt~7) and one of its analyses: the verb 'to see', masculine, singular, third person, past tense." ></td>
	<td class="line x" title="146:411	A similar word for this analysis is the following one:  RATH (n~t~7), feminine, singular, third person, past tense." ></td>
	<td class="line x" title="147:411	The choice of which words should be included in the SW set of a given analysis is determined by a set of pre-defined rules based on the intuition of a native speaker." ></td>
	<td class="line x" title="148:411	Nevertheless, the elements in the SW sets are not determined for each analysis separately, but rather are generated automatically, for each analysis, by changing the contents of one or several morphological attributes in the morphological analysis." ></td>
	<td class="line x" title="149:411	In the previous example the elements are generated by changing the contents of the gender attribute in the morphological analysis, while keeping all the other attributes unchanged." ></td>
	<td class="line x" title="150:411	The set of rules used by the algorithm for automatic generation of SW sets for each analysis in the language are of a heuristic nature." ></td>
	<td class="line x" title="151:411	For the problem in Hebrew, a set of ten rules 5 was sufficient for the generation of SW sets for all the possible morphological analyses in Hebrew." ></td>
	<td class="line x" title="152:411	In case we wish to move to some other domain in Hebrew, we should be able to use the same set of rules, but with a suitable training corpus." ></td>
	<td class="line x" title="153:411	Hence, the set of rules are language-dependent but not domain-dependent." ></td>
	<td class="line x" title="154:411	To clarify this point, consider the word MCBY& (~'~Xr2), which has the following two morphological analyses: . . The verb HCBY& (~2Xn), masculine, singular, present tense ('indicates' or 'votes')." ></td>
	<td class="line x" title="155:411	The noun MCBY& (~2xr~, 'a pointer')." ></td>
	<td class="line x" title="156:411	The set of rules defined for Hebrew would enable us to observe that in the domain of daily newspaper articles, the first analysis probably has a high morpho-lexical probability while the second analysis has a very low probability." ></td>
	<td class="line x" title="157:411	Using the same set of rules, we should be able to deduce for a domain of articles dealing with computer languages that the second analysis is probably much more frequent than the first one." ></td>
	<td class="line x" title="158:411	Whenever we wish to apply our method to some other language that has a similar 4 This assumption does not hold for a small number of verbs that take as a subject only animate nouns with a specific gender, such as YLDH (n'f~ ~, 'she gave birth')." ></td>
	<td class="line x" title="159:411	5 See Appendix B for the list of the rules used for Hebrew." ></td>
	<td class="line x" title="160:411	389 Computational Linguistics Volume 21, Number 3 ambiguity problem, all we need to do is define a new set of rules for generation of SW sets in that other language." ></td>
	<td class="line x" title="161:411	By choosing the elements in the SW set carefully so that they meet the requirement of similarity, we can study the frequency of an analysis from the frequencies of the elements in its SW set." ></td>
	<td class="line x" title="162:411	Note that we should choose the words for the SW sets such that they are morphologically unambiguous." ></td>
	<td class="line x" title="163:411	We assume that this is the case in the following examples, and will return to this issue in the next two sections." ></td>
	<td class="line x" title="164:411	To illustrate the whole process, let us reconsider the ambiguous word HQPH (~flpn) and its three different analyses." ></td>
	<td class="line x" title="165:411	The SW sets for each analysis is as follows:  HQPH (n~pn, 'encirclement') SW1 = { HHQPH (n~pnn, 'the encirclement') }  H + QPH ( n~p + n, 'the coffee') SW2 = { QPH (hillY, 'coffee') }  HQP + H (n + qpn, 'her perimeter') SW3 = { HQPW (~flpn, 'his perimeter'), HQPM (O~pn, masculine 'their perimeter'), HQPN (lflpn, feminine 'their perimeter') }." ></td>
	<td class="line x" title="166:411	Given the SW set of each analysis we can now find in the corpus how many times each word appears, calculate the expected frequency of each analysis, and get the desired probabilities by normalizing the frequency distribution." ></td>
	<td class="line x" title="167:411	Had our similarity assumption been totally correct, namely, that each word in the SW set appears exactly the same number of times as the related analysis, we would have expected to get a neat situation such as the following (assuming that the ambiguous word HQPH appears 200 times in the corpus): 6  SW 1 = { HHQPH = 18 } ' SW2={QPH=180}  SW3 = { HQPW = 2, HQPM = 2, HQPN = 2 }." ></td>
	<td class="line x" title="168:411	These counters suggest that if we manually tagged the 200 occurrences of the string HQPH in the corpus, we would find that the first analysis of HQPH is the right one 18 times out of the 200 times that the word appears in the corpus, that the second analysis is the right one 180 times, and that the third analysis is the right analysis only twice." ></td>
	<td class="line x" title="169:411	Using these counters we can relate the following morpho-lexical probabilities to the three analyses of HQPH: 0.09, 0.90, 0.01, respectively." ></td>
	<td class="line x" title="170:411	These probabilities must be considered an approximation to the real morpho-lexical probabilities, because of the following reasons: . . The words in the SW set are only expected to appear approximately the same number of times as the analysis they represent." ></td>
	<td class="line x" title="171:411	The reliability of the probabilities we acquire using our method depends on the number of times the ambiguous word appears in the corpus 6 The numbers in this example are fictitious." ></td>
	<td class="line x" title="172:411	They were chosen in order to clarify our point." ></td>
	<td class="line x" title="173:411	390 Moshe Levinger et al. Learning Morpho-Lexical Probabilities (which is really the size of the sample we use to calculate the morpho-lexical probabilities)." ></td>
	<td class="line x" title="174:411	In the corpus we worked with, the word HQPH appeared 202 times, and the number of occurrences of the words in its SW sets were as follows:  SW1 ={HHQPH=3}  SW2={QPH=368}  SW3 = { HQPW = 0, HQPM = 0, HQPN = 0 }." ></td>
	<td class="line x" title="175:411	By applying now the algorithm of the next section on these counters, we can calculate the desired probabilities." ></td>
	<td class="line x" title="176:411	6." ></td>
	<td class="line x" title="177:411	The Algorithm Our algorithm has to handle the frequently occurring case in which a certain word appears in more than one SW set." ></td>
	<td class="line x" title="178:411	In that case, we would like to consider the counter of such a word appropriately." ></td>
	<td class="line x" title="179:411	The algorithm takes care of this problem and works as follows: Initially we assume that the proportions between the different analyses are equal." ></td>
	<td class="line x" title="180:411	For each analysis we compute its average number of occurrences, by summing up all the counters for each word in the SW set and dividing this sum by the SW size." ></td>
	<td class="line x" title="181:411	Note that in this stage we also include the ambiguous word in each of the SW sets." ></td>
	<td class="line x" title="182:411	7 If a word appears in several SW sets, we calculate its contribution to the total sum according to the proportions between all those sets, using the proportions calculated in the previous iteration." ></td>
	<td class="line x" title="183:411	Calculate the new proportions between the different analyses by computing the proportions between the average number of occurrences of each analysis." ></td>
	<td class="line x" title="184:411	This process is iterated until the new proportions calculated are sufficiently close to the proportions calculated in the previous iteration." ></td>
	<td class="line x" title="185:411	Finally, the proportions are normalized to obtain probabilities." ></td>
	<td class="line x" title="186:411	A formal description of the algorithm written in a pseudo-code is given in Figure 1." ></td>
	<td class="line x" title="187:411	7 This is done mainly in order to handle cases where a certain analysis has an empty SW set, since it does not have naturally similar words." ></td>
	<td class="line x" title="188:411	The third example in the next section serves to clarify this point." ></td>
	<td class="line x" title="189:411	391 Computational Linguistics Volume 21, Number 3 Input: w A word with k analyses: A1  Ak." ></td>
	<td class="line x" title="190:411	SW1  SWk The similar words sets of analyses A1,,Ak." ></td>
	<td class="line x" title="191:411	sw A word in some SW set." ></td>
	<td class="line x" title="192:411	C(sw) The number of occurrences of sw in the training corpus." ></td>
	<td class="line x" title="193:411	Inc (sw) A set of indexes representing the analyses for which sw is a member in their SW set, i.e., Inc(sw) = {1:1 < / < k, sw E SWl} e A prespecified threshold indicating the convergence of the algorithm." ></td>
	<td class="line x" title="194:411	Internal Variables: /:j The approximated morphoqexical probability of Aj in the i-th iteration." ></td>
	<td class="line x" title="195:411	SumAnalj The sum over the contribution of all the words in SWj." ></td>
	<td class="line x" title="196:411	AvgAnalj The average contribution of a single word in SWj to SumAnalj." ></td>
	<td class="line x" title="197:411	The Algorithm: p0 := pO2 := pk o := 1/k; i := O; repeat i:=i+l; for j := 1 to k do begin K-~ pi-1,~ . SumAnalj = ~sw~SWj C(sw) x (F} -1 /,' 4~1nc(sw) l '' AvgAnalj := SumAnalj / size(SWj) end; for j := 1 to k do F~ := AvgAnalj / (AvgAnall + + AvgAnalk) until ( maxj \[ P~ p~-I \]< ~.)." ></td>
	<td class="line x" title="198:411	Figure 1 Calculating the approximated morpho-lexical probabilities." ></td>
	<td class="line x" title="199:411	Applying this algorithm to the sets and the counters extracted from the corpus (our previous example) yields the following probabilities: 8  HQPH = 0.0113  H + QPH = 0.9870 8 Because of the finite nature of our algorithm, we assign non-zero probabilities even to events that do not occur in the training corpus." ></td>
	<td class="line x" title="200:411	This property agrees with common statistical practice (Agresti 1990)." ></td>
	<td class="line x" title="201:411	392 Moshe Levinger et al. Learning Morpho-Lexical Probabilities  HQP + H = 0.0017." ></td>
	<td class="line x" title="202:411	Although this method for acquiring morpho-lexical probabilities gives very good results for many ambiguous words, as will be shown in Section 8, we detected two types of inherently problematic cases: . . Because of the high degree of morphological ambiguity in Hebrew, some of the words in the SW sets may also be ambiguous." ></td>
	<td class="line x" title="203:411	As long as the other possible analyses of such a word are not too frequent, it only slightly affects the final probabilities." ></td>
	<td class="line x" title="204:411	Otherwise, we might get wrong results by erroneously crediting the high number of occurrences of such a word 9 to one of the analyses." ></td>
	<td class="line x" title="205:411	For this reason, we try to construct the SW sets from as many suitable elements as possible, in order to be able to detect 'misleading' words of this sort." ></td>
	<td class="line x" title="206:411	Occasionally, the SW sets defined for two different analyses are actually the same." ></td>
	<td class="line x" title="207:411	Thus, a differentiation between those two analyses cannot be done using our method." ></td>
	<td class="line x" title="208:411	Another potentially problematic case is the coverage problem, that arises whenever we do not have enough data in the corpus for disambiguation of a certain word (see a discussion on this problem in Dagan, Itai, and Schwall \[1991\])." ></td>
	<td class="line x" title="209:411	This problem was found to occur very rarely--for only 3% of the ambiguous words in our test texts the counters found in the corpus were smaller than 20." ></td>
	<td class="line x" title="210:411	We expect this percentage would be even smaller had we used a larger training corpus." ></td>
	<td class="line x" title="211:411	For such words, we simply ignored the data and arbitrarily gave a uniform probability to all their analyses." ></td>
	<td class="line x" title="212:411	7." ></td>
	<td class="line x" title="213:411	Examples Several aspects of the algorithm described in the previous section can be better understood by looking at some clarifying examples." ></td>
	<td class="line x" title="214:411	To see an example for the convergence of the algorithm, consider the neat situation described in Section 5 for the word HQPH:  SWI= { HQPH = 200, HHQPH = 18 }  SW2 = { HQPH = 200, QPH = 180 }  SW3 = { HQPH = 200, HQPW = 2, HQPM = 2, HQPN = 2 }." ></td>
	<td class="line x" title="215:411	For these sets and counters and for ~ = 0.001, the algorithm converges after 10 iterations." ></td>
	<td class="line x" title="216:411	The probabilities for each iteration are given below:  Iteration no. 1: P1 = 0.333, P2 = 0.333, P3 = 0.333  Iteration no. 2: P1 = 0.230, P2 -= 0.671, P3 = 0.099  Iteration no. 3: P1 = 0.164, P2 --0.803, P3 = 0.033  Iteration no. 4: P1 = 0.128, P2 -= 0.857, P3 = 0.015  Iteration no. 5: P1 = 0.110, P2 = 0.880, P3 = 0.010  Iteration no. 6: P1 = 0.100, P2 = 0.890, P3 = 0.010  Iteration no. 7: P1 = 0.095, P2 = 0.895, P3 = 0.010 9 Because of technical reasons, we cannot decide whether a given word is ambiguous or not when we automatically generate the words for the SW sets." ></td>
	<td class="line x" title="217:411	See Section 7 for more details." ></td>
	<td class="line x" title="218:411	393 Computational Linguistics Volume 21, Number 3  Iteration no. 8: P1 = 0.092, P2 = 0.898, P3 = 0.010  Iteration no. 9: Pt = 0.091, P2 = 0.899, P3 = 0.010  Iteration no. 10: P1 = 0.091, P2 = 0.899, P3 = 0.010 In this example the similarity assumption holds, and the words in the SW sets (excluding the word HQPH itself) are also unambiguous." ></td>
	<td class="line x" title="219:411	This need not hold in other situations." ></td>
	<td class="line x" title="220:411	As we have pointed out already, because of technical reasons we have not been able to apply the morphological analyzer to the words in the SW sets, and thus we have not been able to automatically observe that a given similar word is ambiguous by itself." ></td>
	<td class="line x" title="221:411	The problem stems from the fact that we have been able to use the morphological analyzer on personal computers only, while both the corpus and the program that automatically generates the SW sets for each analysis could have been used only on our mainframe computer." ></td>
	<td class="line x" title="222:411	Given this, the morphological analyzer was only used in order to obtain the input files for the disambiguation project." ></td>
	<td class="line x" title="223:411	Nonetheless, the fact that ambiguous words in the SW sets cannot be automatically identified does not affect the quality of the probabilities obtained by our method for most ambiguous words." ></td>
	<td class="line x" title="224:411	1 To see the reason for this, consider the word XWD$ (vd'nn) and its two analyses: . . The noun XWD$ (Vd'I~r~, 'a month'): SW1 = { XWD$ = 2079, HXWD$ = 970 (~'nnn, 'the month') } The verb XWD$, masculine, singular, third person, past tense ('he/it was resumed')." ></td>
	<td class="line x" title="225:411	SW2 = { XWD$ = 2079, XWD$H = 41 (n~'rln, 'she/it was resumed'), XWD$W = 57 (Wd'f~r~, 'they were resumed') } Both XWD$H and XWD$W (SW2) are ambiguous words." ></td>
	<td class="line x" title="226:411	Still, since the counters for these two words are substantially smaller than the counter for the word HXWD$ (SW1), the probabilities calculated according to these counters can be considered as a reasonable approximation for the real morpho-lexical probabilities." ></td>
	<td class="line x" title="227:411	The algorithm, applied to these sets and counters, yielded the following probabilities: P1 = 0.961, P2 = 0.039." ></td>
	<td class="line x" title="228:411	This kind of situation is not unique for the word XWD$." ></td>
	<td class="line x" title="229:411	Similar situations occur in many other ambiguous words in Hebrew." ></td>
	<td class="line x" title="230:411	Hence, not having the ability to identify ambiguous words in the SW sets has a meaningful effect on the quality of the probabilities only in cases where some similar word is ambiguous and its other analysis is frequent in the language." ></td>
	<td class="line x" title="231:411	In such cases the analysis that this word belongs to is assigned a higher probability than its real morpho-lexical probability." ></td>
	<td class="line x" title="232:411	We use the term misleading words for such ambiguous similar words." ></td>
	<td class="line x" title="233:411	A partial solution for such cases was implemented in the revised algorithm we used for morpho-lexical probabilities calculation." ></td>
	<td class="line x" title="234:411	In this revised version we automatically identified similar words as misleading words by looking at the counters of all the similar words in a given SW set." ></td>
	<td class="line x" title="235:411	A word was considered misleading if its counter was at least five times greater than that of any other word in the set." ></td>
	<td class="line x" title="236:411	This solution was not applicable in cases where all the similar words in a given SW set were misleading words." ></td>
	<td class="line x" title="237:411	10 In our test sample of 53 words, the probabilities were significantly affected by this phenomenon in only three cases." ></td>
	<td class="line x" title="238:411	394 Moshe Levinger et al. Learning Morpho-Lexical Probabilities The need to add the original ambiguous word to all the SW sets of its analyses can be made clear by the following example." ></td>
	<td class="line x" title="239:411	Consider the word AT (~t~) and its sets and counters, as found in our training corpus: 1." ></td>
	<td class="line x" title="240:411	The direct object particle for definite nouns, AT." ></td>
	<td class="line x" title="241:411	SW 1 = { AT = 197,501 } 2." ></td>
	<td class="line x" title="242:411	The feminine, singular, second person, nominal personal pronoun AT (feminine 'you')." ></td>
	<td class="line x" title="243:411	SW2 = { AT = 197,501, ATH (n~I'() = 1689, ATM (t3nt~) = 891, ATN Qnl'0 = 105 } 3." ></td>
	<td class="line x" title="244:411	The noun AT ('a spade')." ></td>
	<td class="line x" title="245:411	SW3 = { AT = 197,501, HAT (nnn) = 0 } The key point here is that the particle AT has no natural similar word." ></td>
	<td class="line x" title="246:411	u Yet, from the above counters we should be able to deduce that the first analysis has a very high morpho-lexical probability." ></td>
	<td class="line x" title="247:411	This is since the ambiguous word AT is very frequent in the corpus, while the counters in the SW sets for the second and third analyses indicate that these analyses are not the 'reason' for the high frequency of AT in the corpus." ></td>
	<td class="line x" title="248:411	Adding the ambiguous word to all the SW sets allows the algorithm to take this fact into account." ></td>
	<td class="line x" title="249:411	Applying the algorithm on the above sets and counters yields the following morpho-lexical probabilities: P1 = 0.9954, P2 = 0.0045, P3 = 0.0001." ></td>
	<td class="line x" title="250:411	8." ></td>
	<td class="line x" title="251:411	Evaluating the Probabilities Before we evaluate the quality of the approximated probabilities that can be acquired using our method, we would like to start with a definition of three terms that will be used in this section: Morpho-Lexical Probabilities Estimated from a Training Corpus Given a large corpus in Hebrew the morpho-lexical probabilities of a given word are the probabilities of its analyses as calculated by manually tagging all the occurrences of the given word in the corpus." ></td>
	<td class="line x" title="252:411	We will use the abbreviation morpho-lexical probabilities to denote this term." ></td>
	<td class="line x" title="253:411	Morpho-Lexical Probabilities Estimated over a Test-Corpus In order to avoid the laborious effort needed for the manual tagging of all the occurrences of an ambiguous word in a large corpus, we estimate the morpho-lexical probabilities by calculating them from a relatively small corpus." ></td>
	<td class="line x" title="254:411	The abbreviation test-corpus probabilities will be used for this term." ></td>
	<td class="line x" title="255:411	Approximated Probabilities Given an ambiguous word, the approximated probabilities of the word are the probabilities calculated using the method described in this paper." ></td>
	<td class="line x" title="256:411	The approximated probabilities obtained by our method were evaluated by comparing these probabilities with test-corpus probabilities obtained by manual tagging of a relatively small corpus." ></td>
	<td class="line x" title="257:411	Since the approximation we acquire depends on the corpus we have been using--texts taken from the Hebrew newspaper Ha'aretz12--we have to 11 In fact, all the prepositions in the language lack natural similar words." ></td>
	<td class="line x" title="258:411	12 We would like to thank Ha'aretz for the permission to use magnetic tapes from its archives." ></td>
	<td class="line x" title="259:411	395 Computational Linguistics Volume 21, Number 3 calculate the test-corpus probabilities from texts taken from the same source." ></td>
	<td class="line x" title="260:411	For this purpose we used a small corpus consisting of more than 500,000 word-tokens taken from the same newspaper." ></td>
	<td class="line x" title="261:411	For our experiment we picked from this small corpus two kinds of test groups." ></td>
	<td class="line x" title="262:411	Test-group1 consisted of 30 ambiguous word-types chosen randomly from all the ambiguous word types appearing more than 100 times in the corpus." ></td>
	<td class="line x" title="263:411	For the second test group, test-group2, we randomly picked a short text from the corpus from which we extracted all the ambiguous word-tokens appearing at least 30 times in the small corpus." ></td>
	<td class="line x" title="264:411	This test group consisted of 23 words." ></td>
	<td class="line x" title="265:411	These two test groups are of a different nature." ></td>
	<td class="line x" title="266:411	Test-group1 consists only of very frequent word types in Hebrew, but the test-corpus probabilities for these word types can be viewed as a reliable estimate of the morpho-lexical probabilities." ></td>
	<td class="line x" title="267:411	The wordtokens in test-group2 better represent the typical ambiguous word in the language, but their test-corpus probabilities were calculated from a relatively small sample of tagged words." ></td>
	<td class="line x" title="268:411	For each word in these test groups, we extracted from the small corpus all the sentences in which the ambiguous word appears." ></td>
	<td class="line x" title="269:411	We then manually tagged each ambiguous word and found for each one of its analyses how many times it was the right analysis." ></td>
	<td class="line x" title="270:411	For example, the word AWLM (O~1b0 (taken from test-group1) has the following two morphological analyses: 1." ></td>
	<td class="line x" title="271:411	The particle AWLM ('but')." ></td>
	<td class="line x" title="272:411	2." ></td>
	<td class="line x" title="273:411	The noun ^WLM ('a hall')." ></td>
	<td class="line x" title="274:411	The word AWLM appeared 236 times in the small corpus." ></td>
	<td class="line x" title="275:411	By manually tagging all the relevant sentences we found that the first analysis, 'but,' was the right analysis 232 times, and the second analysis, 'a hall,' was the right analysis only 4 times." ></td>
	<td class="line x" title="276:411	Given these numbers we can calculate the relative weights of these two analyses: 232/236, 4/236 and the test-corpus probabilities: 0.983, 0.017, respectively." ></td>
	<td class="line x" title="277:411	In the same way, using the small corpus we found the test-corpus probability, Ptest, for each of the analyses in the test groups." ></td>
	<td class="line x" title="278:411	Table 2 shows the test-corpus probabilities and the approximated probabilities for five representative ambiguous words from our test groups." ></td>
	<td class="line x" title="279:411	In this table the approximation for the probabilities of the first three words is very good while the approximation for the fourth word is quantitatively poor, but still succeeds in identifying the first analysis of LPNY (~3~V, 'before') as the dominant analysis." ></td>
	<td class="line x" title="280:411	As for the fifth word, here the approximation we got is totally incorrect." ></td>
	<td class="line x" title="281:411	At the end of this section we shall identify some cases for which our method fails to find a reasonable approximation for the morpho-lexical probabilities of an ambiguous word." ></td>
	<td class="line x" title="282:411	In order to evaluate the quality of the approximation we got by our method, we should compare the approximated probabilities for the words in these test groups with the test-corpus probabilities we found." ></td>
	<td class="line x" title="283:411	When we tried to make a quantitative comparison using statistical methods we found that for many analyses Papp 'looks' like a good approximation for Ptest, but from a statistical point of view the approximation is not satisfying." ></td>
	<td class="line x" title="284:411	The main reason for this is that the words in the SW set of a given analysis can be considered similar in their frequency to the analysis only from a qualitative point of view, and not from a quantitative one." ></td>
	<td class="line x" title="285:411	Thus, the comparison we describe in what follows serves for evaluation of the quality of the approximated probabilities." ></td>
	<td class="line x" title="286:411	Motivated by the way we use the morpho-lexical probabilities for morphological disambiguation, we can divide the probability of an analysis into three categories: 396 Moshe Levinger et al. Learning Morpho-Lexical Probabilities Table 2 Approximated and test-corpus probabilities for five ambiguous words from the two test groups." ></td>
	<td class="line x" title="287:411	Ambiguous Approximated Relative Test-Corpus Word Probability Weight Probability ^WLM 0.968 232/236 0.983 (~N) 0.032 4/236 0.017 AT 0.995 300/300 1.000 (~) 0.001 0/300 0.000 0.004 0/300 0.000 XWD$ 0.976 75/78 0.962 (?A~ID) 0.024 3/78 0.038 LPNY 0.725 100/100 1.000 (~) 0.274 0/100 0.000 0.001 0/100 0.000 ^LH 0.141 112/168 0.667 (n~N) 0.005 0/168 0.000 0.001 0/168 0.000 0.849 56/168 0.333 0.001 0/168 0.000 . . ." ></td>
	<td class="line x" title="288:411	Very high probability An analysis with a probability from this category is the dominant analysis of the ambiguous word and thus, given that we cannot use any other source of information to disambiguate the given word, we would like to select the dominant analysis as the right analysis." ></td>
	<td class="line x" title="289:411	Very low probability Given no other information, an analysis with a very low probability should be treated as a wrong analysis." ></td>
	<td class="line x" title="290:411	All other probabilities An analysis with probability of this sort should not be selected as wrong/right analysis solely according to its morpho-lexical probability." ></td>
	<td class="line x" title="291:411	Formally, the mapping from the probability of an analysis to its category is done using two thresholds, upper threshold and lower threshold, as follows: 1 prob ~ upper threshold CAT(prob) = 2 prob ~ lower threshold 3 otherwise The quality of the approximated probabilities we acquire using our method is now measured by examining the proportion of words for which the estimated category for each of their analyses agrees with the category defined by the approximated probabilities." ></td>
	<td class="line x" title="292:411	The results of this comparison for the two test groups we used are shown in Table 3 and Table 4." ></td>
	<td class="line x" title="293:411	In these tables we divide the words into three groups according to the quality of the approximation found for them: 1." ></td>
	<td class="line x" title="294:411	Words with good approximation--words for which CAT(Ptest) = CAT(Papp) holds for all their analyses, using: lower 397 Computational Linguistics Table 3 The quality of the approximation for test-group1." ></td>
	<td class="line x" title="295:411	Volume 21, Number 3 Total Good Reasonable Incorrect Approximation Approximation Approximation Number of Words 30 29 0 1 % 100 97 0 3 Table 4 The quality of the approximation for test-group2." ></td>
	<td class="line x" title="296:411	Total Good Reasonable Incorrect Approximation Approximation Approximation Number of Words 23 17 2 4 % 100 74 9 17 . . threshold = 0.20, and upper threshold = 0.80." ></td>
	<td class="line x" title="297:411	(The first three words in Table 2 belong to this category)." ></td>
	<td class="line x" title="298:411	Words with reasonable approximation--words that do not fall into the previous category, but CAT(Ptest) = CAT(Papp) holds for all their analyses, using: lower threshold = 0.35, and upper threshold = 0.65 (The fourth word in Table 2 belongs to this category)." ></td>
	<td class="line x" title="299:411	Words with incorrect approximation--the words whose approximation is neither good nor reasonable." ></td>
	<td class="line x" title="300:411	(The fifth word in Table 2 belongs to this category)." ></td>
	<td class="line x" title="301:411	From these tables we can see that our method yielded incorrect approximation for only 5 words out of the 53 words in the test groups (9.5%)." ></td>
	<td class="line x" title="302:411	By closely looking at these words, we can identify two reasons for failure: . . Ambiguity of a word in the SW set of a given analysis." ></td>
	<td class="line x" title="303:411	This may affect the probabilities calculated for this analysis." ></td>
	<td class="line x" title="304:411	To see that, consider the word MWNH (~\]1r2) (test-group2), one analysis of which is the noun MWNH ('a counter')." ></td>
	<td class="line x" title="305:411	By manually tagging all the occurrences of MWNH in our small corpus, we found that the above-mentioned analysis is extremely rare--its relative weight is 0/44." ></td>
	<td class="line x" title="306:411	As for the approximated probability of this analysis, its SW set contains a single word: HMWNH (n~lr~n, 'the counter'), the definite form of the same noun." ></td>
	<td class="line x" title="307:411	The word HMWNH is very frequent in our corpus and for that reason the approximated probability found for this analysis is very high: 0.894." ></td>
	<td class="line x" title="308:411	The mismatch between Ptest and Papp in this case is due to the fact that HMWNH is a misleading word--an ambiguous word one analysis of which H + present form of MNH (~\]r~, 'numbered'), is a frequent idiom in Hebrew ('which numbers')." ></td>
	<td class="line x" title="309:411	Our method may also yield an incorrect approximation for analyses where the similarity assumption we use between the frequency of an 398 Moshe Levinger et al. Learning Morpho-Lexical Probabilities analysis and the frequency of the words in its SW set does not hold." ></td>
	<td class="line x" title="310:411	An example for this is the word $&H (n~Vd) (test-group2), and one of its analyses the noun $&H ('an hour')." ></td>
	<td class="line x" title="311:411	The approximated probability for this analysis is calculated by looking at the frequency of the similar word H$&H (~Vdn, 'the hour')." ></td>
	<td class="line x" title="312:411	Unfortunately, the similarity assumption does not hold in this case, since the indefinite form of $&H is much more frequent in Hebrew than the definite form of the word." ></td>
	<td class="line x" title="313:411	For this reason, 13 the approximated probability for this analysis (0.376) is substantially lower than its test-corpus probability (0.847)." ></td>
	<td class="line x" title="314:411	9." ></td>
	<td class="line x" title="315:411	Morphological Disambiguation In the previous section we compared the approximated probabilities obtained by our method to the probabilities found by manually tagging a small corpus." ></td>
	<td class="line x" title="316:411	We found that the acquired probabilities are truly a good approximation for the morpho-lexical probabilities." ></td>
	<td class="line x" title="317:411	In this section we describe an experiment that was conducted in order to test the effectiveness of the morpho-lexical probabilities for morphological disambiguation in Hebrew." ></td>
	<td class="line x" title="318:411	Following are the main components in our project that were used in order to conduct the experiment: 1." ></td>
	<td class="line x" title="319:411	A robust morphological analyzer for Hebrew that gives for each word in the language all its possible analyses." ></td>
	<td class="line x" title="320:411	The input for our project is supplied by this module." ></td>
	<td class="line x" title="321:411	2." ></td>
	<td class="line x" title="322:411	An interactive program for manually tagging Hebrew texts." ></td>
	<td class="line x" title="323:411	It was created in order to rapidly tag large texts and was used to mark the right analysis for each ambiguous word in order to be used later to evaluate the performance of our method." ></td>
	<td class="line x" title="324:411	3." ></td>
	<td class="line x" title="325:411	Untagged Hebrew corpus." ></td>
	<td class="line x" title="326:411	Because of the fact that Hebrew corpora (untagged and tagged as well) are not available in the public domain, we had to build a Hebrew corpus especially for this project." ></td>
	<td class="line x" title="327:411	This corpus consists of 11 million word-tokens taken from the daily newspaper Ha'aretz." ></td>
	<td class="line x" title="328:411	4." ></td>
	<td class="line x" title="329:411	A hash table that stores all the words in the corpus." ></td>
	<td class="line x" title="330:411	Each word is accompanied by a counter indicating how many times it appears in the corpus." ></td>
	<td class="line x" title="331:411	Since this is the only information we extract from the corpus, our algorithm needs only this hash table and is therefore very efficient." ></td>
	<td class="line x" title="332:411	5." ></td>
	<td class="line x" title="333:411	A morphological generator for Hebrew that was written especially for this project." ></td>
	<td class="line x" title="334:411	The SW sets for every analysis are generated using this module." ></td>
	<td class="line x" title="335:411	Because of technical reasons, we were not able to use the morphological analyzer at this stage, and thus we could not identify ambiguous words in the SW sets." ></td>
	<td class="line x" title="336:411	6." ></td>
	<td class="line x" title="337:411	An implementation of the iterative algorithm that calculates the probabilities." ></td>
	<td class="line x" title="338:411	13 The indefinite form of $&H appears in many Hebrew idioms, e.g., LPY $&H (n~)9~, 'for the time being'), B^WTH $&H (D~ ~llR~l, 'at the same time') etc. 399 Computational Linguistics Volume 21, Number 3 . A simple selection algorithm that reduces the level of morphological ambiguity using the probabilities obtained from the corpus." ></td>
	<td class="line x" title="339:411	The algorithm uses two thresholds, an upper threshold and a lower threshold, which serve to choose the right analysis or to rule out wrong analyses, respectively." ></td>
	<td class="line x" title="340:411	A set of 21 articles was selected in order to test the performance of the method." ></td>
	<td class="line x" title="341:411	Since the morpho-lexical probabilities we use are calculated from a large Hebrew corpus (representing a certain Hebrew sublanguage), these 21 texts were randomly selected from texts belonging to the same sublanguage." ></td>
	<td class="line x" title="342:411	The total number of wordtokens in these test texts was 3,400, out of which nearly 50% were morphologically ambiguous." ></td>
	<td class="line x" title="343:411	The reason for testing the method only on a relatively small set of test texts is that no tagged Hebrew corpus is currently available for a more powerful evaluation." ></td>
	<td class="line x" title="344:411	The need to manually tag the texts used for evaluation limited the number of words in the test texts we used." ></td>
	<td class="line x" title="345:411	Nevertheless, we believe that the results obtained for this restricted set of texts gives a fairly good indication for the success of the method on large texts as well." ></td>
	<td class="line x" title="346:411	We tested the performance of the method on the test texts from two different perspectives." ></td>
	<td class="line x" title="347:411	First, we used the probabilities only for ambiguous words that can be fully disambiguated." ></td>
	<td class="line x" title="348:411	In this case a single analysis can be selected as the right analysis." ></td>
	<td class="line x" title="349:411	The performance of the method for full-disambiguation is measured by the recall parameter, which is defined as follows: no." ></td>
	<td class="line x" title="350:411	of correctly assigned words Recall = no." ></td>
	<td class="line x" title="351:411	of ambiguous words In addition to this parameter we present two additional performance parameters: applicability and precision." ></td>
	<td class="line x" title="352:411	We believe that these parameters are relevant for the particular naive method described in the current section." ></td>
	<td class="line x" title="353:411	This is due to the fact that the morpho-lexical probabilities are not supposed to be used alone for disambiguation, but rather are meant to serve as one information source in a system that combines several linguistic sources for disambiguation." ></td>
	<td class="line x" title="354:411	The above-mentioned parameters are defined as follows: no." ></td>
	<td class="line x" title="355:411	of correctly assigned words Precision = no." ></td>
	<td class="line x" title="356:411	of fully disambiguated words no." ></td>
	<td class="line x" title="357:411	of fully disambiguated words Applicability = no." ></td>
	<td class="line x" title="358:411	of ambiguous words The results obtained for full disambiguation are shown in Table 5." ></td>
	<td class="line x" title="359:411	However, the morpho-lexical probabilities can also be used in order to reduce the ambiguity level in the text." ></td>
	<td class="line x" title="360:411	The performance of the method in this sense is much more interesting and important since it examines, more accurately, the quality of the probabilities as data for other, more sophisticated, systems that use higher levels of information." ></td>
	<td class="line x" title="361:411	In this experiment we test the performance of the morpho-lexical probabilities on the task of analysis assignment." ></td>
	<td class="line x" title="362:411	Here one or more analyses of an ambiguous word are recognized as wrong and hence are rejected." ></td>
	<td class="line x" title="363:411	The right analysis should be one of the remaining analyses." ></td>
	<td class="line x" title="364:411	The three parameters used for evaluation are as follows: no." ></td>
	<td class="line x" title="365:411	of correct right assignments Recall = no." ></td>
	<td class="line x" title="366:411	of ambiguous words 400 Moshe Levinger et al. Table 5 The performance for full disambiguation." ></td>
	<td class="line x" title="367:411	Learning Morpho-Lexical Probabilities Ambiguous Disambiguated Correct Recall Applicability Precision Words Words Assignments 1613 1315 1160 72% 82% 88% Table 6 The performance for analysis assignment." ></td>
	<td class="line x" title="368:411	Ambiguous Wrong Remaining Correct Incorrect Recall Precision Fallout Words Analyses Analyses Assignments Assignments 1613 3260 1802 1444 358 90% 80% 11% Table 7 Reducing the degree of ambiguity." ></td>
	<td class="line x" title="369:411	Number of Remaining Analyses Analyses Total 1 2 3 4 5 6 7 8 9 10 3 487 411 62 14 4 218 170 36 11 1 5 90 72 12 4 2 0 6 52 40 12 0 0 0 7 28 22 6 0 0 0 8 11 8 2 1 0 0 9 1 1 0 0 0 0 10 4 4 0 0 0 0 total 891 728 130 30 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Precision = no." ></td>
	<td class="line x" title="370:411	of correct right assignments no." ></td>
	<td class="line x" title="371:411	of remaining analyses Fallout -no." ></td>
	<td class="line x" title="372:411	of incorrect assignments no." ></td>
	<td class="line x" title="373:411	o/wrong analyses The results are shown in Table 6." ></td>
	<td class="line x" title="374:411	In another experiment we examined 891 words with more than two analyses." ></td>
	<td class="line x" title="375:411	Table 7 shows how our algorithm reduced the ambiguity of these words." ></td>
	<td class="line x" title="376:411	These results demonstrate the effectiveness of morpho-lexical probabilities in reducing the ambiguity level in a Hebrew text, and it seems that by using such information combined with other approaches for morphological disambiguation in Hebrew, we come very close to a practical solution for this problem." ></td>
	<td class="line x" title="377:411	10." ></td>
	<td class="line x" title="378:411	Conclusions A method to acquire morpho-lexical probabilities from an untagged corpus has been described." ></td>
	<td class="line x" title="379:411	The main idea was to use the rich morphology of the language to learn the 401 Computational Linguistics Volume 21, Number 3 frequency of a certain analysis from the frequency of other word forms of the same lexical entry." ></td>
	<td class="line x" title="380:411	The results of the experiment confirm the conjecture we made about the nature of the morphological ambiguity problem in Hebrew." ></td>
	<td class="line x" title="381:411	It can be argued, therefore, that the computer with its complete morphological knowledge is facing a much more complex problem than that of a human who may be ignorant of some rare analyses reading a Hebrew text." ></td>
	<td class="line x" title="382:411	This observation is also supported by the fact that humans are very often surprised to see the amount of possible analyses of a given ambiguous word." ></td>
	<td class="line x" title="383:411	It may even have a significance from a psycholinguistic point of view, by suggesting that these kind of probabilities are also used by a human reader of Hebrew." ></td>
	<td class="line x" title="384:411	However, this conjecture should be tested empirically." ></td>
	<td class="line x" title="385:411	An experiment to test the usefulness of the morpho-lexical probabilities for morphological disambiguation in Hebrew yielded the following results: a recall of 70% for full disambiguation, and a recall of 90% for analysis assignment." ></td>
	<td class="line x" title="386:411	However, the morpho-lexical probabilities cannot serve as the only source of information for morphological disambiguation, since they are imperfect by definition--they always choose the same analysis as the right one, regardless of the context in which the ambiguous word appears." ></td>
	<td class="line x" title="387:411	Thus, as has been already mentioned, we have incorporated these probabilities into an existing system for morphological disambiguation." ></td>
	<td class="line x" title="388:411	The combined system tackles the disambiguation problem by combining two kinds of linguistic information sources: Morpho-Lexical Probabilities and Syntactic Constraints (a full description of this system can be found in Levinger \[1992\])." ></td>
	<td class="line x" title="389:411	Appendix A Given below is the Latin-Hebrew transliteration used throughout the paper." ></td>
	<td class="line x" title="390:411	Note that accepted transcriptions for Hebrew (Academy of The Hebrew Language 1957; Ornan 1994) include indication for the vowels that are missing in the modern Hebrew writing system." ></td>
	<td class="line x" title="391:411	For this reason, these transcriptions are not suitable for demonstrating the morphological ambiguity problem in the language." ></td>
	<td class="line x" title="392:411	Instead, we use the following transliteration, which is based on the phonemic script (Ornan 1994); see Table 8." ></td>
	<td class="line x" title="393:411	Appendix B Following is the set of rules used for Hebrew in order to automatically generate the SW set for every morphological analysis in Hebrew." ></td>
	<td class="line x" title="394:411	Note that in case an analysis includes a particular attached particle, this particle is also attached to each of its similar words." ></td>
	<td class="line x" title="395:411	Table 8 The Hebrew-Latin transliteration." ></td>
	<td class="line x" title="396:411	Latin Hebrew Latin Hebrew Latin Hebrew P %9 @ k3 A N C R,~ Y ) B 2 Q ~ K 1,~ G R 7 L ~ D 't $ Vd M 0,~ H D T n N '1,\] W '1 S t3 Z & ~ X B 402 Moshe Levinger et al. Learning Morpho-Lexical Probabilities .  3." ></td>
	<td class="line x" title="397:411	9." ></td>
	<td class="line x" title="398:411	10." ></td>
	<td class="line x" title="399:411	A definite form of a noun--the SW set includes the indefinite form of the same noun." ></td>
	<td class="line x" title="400:411	An indefinite form of a noun--the definite form of the same noun." ></td>
	<td class="line x" title="401:411	A noun with a possessive pronoun--the same noun with all the other possessive pronouns with the same person attribute." ></td>
	<td class="line x" title="402:411	An adjective---the other forms of the same adjective (changing the gender and number attributes)." ></td>
	<td class="line x" title="403:411	A verb without an object pronoun--the same verb in the same tense and person (changing the gender and number attributes only)." ></td>
	<td class="line x" title="404:411	A verb with an object pronoun--the same verb form with all the other object pronouns forms (preserving the person attribute while changing the gender and number ones)." ></td>
	<td class="line x" title="405:411	Nominal personal pronoun--the other nominal personal pronouns of the same person." ></td>
	<td class="line x" title="406:411	A masculine form of a number--the feminine form of the same number." ></td>
	<td class="line x" title="407:411	A feminine form of a number--the masculine form of the same number." ></td>
	<td class="line x" title="408:411	A proper noun, a particle (preposition, connective, etc.)--the empty SW set." ></td>
	<td class="line x" title="409:411	Acknowledgments This research was partially supported by grant number 1380031 of the Israel Council for Research and Development." ></td>
	<td class="line x" title="410:411	We would like to thank Ayal Shiran and Ohad Zeliger for programming support of this project and Ido Dagan for useful discussions concerning this paper." ></td>
	<td class="line x" title="411:411	We would also like to thank IBM, which enabled us to complete this paper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="J95-4004
Transformation-Based-Error-Driven Learning And Natural Language Processing: A Case Study In Part-Of-Speech Tagging
Brill, Eric;"></td>
	<td class="line x" title="1:404	Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part-of-Speech Tagging Eric Brill* The Johns Hopkins University Recently, there has been a rebirth of empiricism in the field of natural language processing." ></td>
	<td class="line x" title="2:404	Manual encoding of linguistic information is being challenged by automated corpus-based learning as a method of providing a natural language processing system with linguistic knowledge." ></td>
	<td class="line x" title="3:404	Although corpus-based approaches have been successful in many different areas of natural language processing, it is often the case that these methods capture the linguistic information they are modelling indirectly in large opaque tables of statistics." ></td>
	<td class="line x" title="4:404	This can make it difficult to analyze, understand and improve the ability of these approaches to model underlying linguistic behavior." ></td>
	<td class="line x" title="5:404	In this paper, we will describe a simple rule-based approach to automated learning of linguistic knowledge." ></td>
	<td class="line x" title="6:404	This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance." ></td>
	<td class="line x" title="7:404	We present a detailed case study of this learning method applied to part-of-speech tagging." ></td>
	<td class="line x" title="8:404	1." ></td>
	<td class="line x" title="9:404	Introduction It has recently become clear that automatically extracting linguistic information from a sample text corpus can be an extremely powerful method of overcoming the linguistic knowledge acquisition bottleneck inhibiting the creation of robust and accurate natural language processing systems." ></td>
	<td class="line pc" title="10:404	A number of part-of-speech taggers are readily available and widely used, all trained and retrainable on text corpora (Church 1988; Cutting et al. 1992; Brill 1992; Weischedel et al. 1993)." ></td>
	<td class="line x" title="11:404	Endemic structural ambiguity, which can lead to such difficulties as trying to cope with the many thousands of possible parses that a grammar can assign to a sentence, can be greatly reduced by adding empirically derived probabilities to grammar rules (Fujisaki et al. 1989; Sharman, Jelinek, and Mercer 1990; Black et al. 1993) and by computing statistical measures of lexical association (Hindle and Rooth 1993)." ></td>
	<td class="line x" title="12:404	Word-sense disambiguation, a problem that once seemed out of reach for systems without a great deal of handcrafted linguistic and world knowledge, can now in some cases be done with high accuracy when all information is derived automatically from corpora (Brown, Lai, and Mercer 1991; Yarowsky 1992; Gale, Church, and Yarowsky 1992; Bruce and Wiebe 1994)." ></td>
	<td class="line x" title="13:404	An effort has recently been undertaken to create automated machine translation systems in which the linguistic information needed for translation is extracted automatically from aligned corpora (Brown et al. 1990)." ></td>
	<td class="line x" title="14:404	These are just a few of the many recent applications of corpus-based techniques in natural language processing." ></td>
	<td class="line x" title="15:404	 Department of Computer Science, Baltimore, MD 21218-2694." ></td>
	<td class="line x" title="16:404	E-mail: brill@cs.jhu.edu." ></td>
	<td class="line x" title="17:404	 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 4 Along with great research advances, the infrastructure is in place for this line of research to grow even stronger, with on-line corpora, the grist of the corpus-based natural language processing grindstone, getting bigger and better and becoming more readily available." ></td>
	<td class="line x" title="18:404	There are a number of efforts worldwide to manually annotate large corpora with linguistic information, including parts of speech, phrase structure and predicate-argument structure (e.g. , the Penn Treebank and the British National Corpus (Marcus, Santorini, and Marcinkiewicz 1993; Leech, Garside, and Bryant 1994))." ></td>
	<td class="line x" title="19:404	A vast amount of on-line text is now available, and much more will become available in the future." ></td>
	<td class="line x" title="20:404	Useful tools, such as large aligned corpora (e.g. , the aligned Hansards (Gale and Church 1991)) and semantic word hierarchies (e.g. , Wordnet (Miller 1990)), have also recently become available." ></td>
	<td class="line x" title="21:404	Corpus-based methods are often able to succeed while ignoring the true complexities of language, banking on the fact that complex linguistic phenomena can often be indirectly observed through simple epiphenomena." ></td>
	<td class="line x" title="22:404	For example, one could accurately assign a part-of-speech tag to the word race in (1-3) without any reference to phrase structure or constituent movement: One would only have to realize that, usually, a word one or two words to the right of a modal is a verb and not a noun." ></td>
	<td class="line x" title="23:404	An exception to this generalization arises when the word is also one word to the right of a determiner." ></td>
	<td class="line x" title="24:404	(1) (2) (3) He will race/VERB the car." ></td>
	<td class="line x" title="25:404	He will not race/VERB the car." ></td>
	<td class="line x" title="26:404	When will the race/NOUN end?" ></td>
	<td class="line x" title="27:404	It is an exciting discovery that simple stochastic n-gram taggers can obtain very high rates of tagging accuracy simply by observing fixed-length word sequences, without recourse to the underlying linguistic structure." ></td>
	<td class="line x" title="28:404	However, in order to make progress in corpus-based natural language processing, we must become better aware of just what cues to linguistic structure are being captured and where these approximations to the true underlying phenomena fail." ></td>
	<td class="line x" title="29:404	With many of the current corpus-based approaches to natural language processing, this is a nearly impossible task." ></td>
	<td class="line x" title="30:404	Consider the part-of-speech tagging example above." ></td>
	<td class="line x" title="31:404	In a stochastic n-gram tagger, the information about words that follow modals would be hidden deeply in the thousands or tens of thousands of contextual probabilities (P(Tagi I Zagi-lZagi-2) ) and the result of multiplying different combinations of these probabilities together." ></td>
	<td class="line x" title="32:404	Below, we describe a new approach to corpus-based natural language processing, called transformation-based error-driven learning." ></td>
	<td class="line x" title="33:404	This algorithm has been applied to a number of natural language problems, including part-of-speech tagging, prepositional phrase attachment disambiguation, and syntactic parsing (Brill 1992; Brill 1993a; Brill 1993b; Brill and Resnik 1994; Brill 1994)." ></td>
	<td class="line x" title="34:404	We have also recently begun exploring the use of this technique for letter-to-sound generation and for building pronunciation networks for speech recognition." ></td>
	<td class="line x" title="35:404	In this approach, the learned linguistic information is represented in a concise and easily understood form." ></td>
	<td class="line x" title="36:404	This property should make transformation-based learning a useful tool for further exploring linguistic modeling and attempting to discover ways of more tightly coupling the underlying linguistic systems and our approximating models." ></td>
	<td class="line x" title="37:404	544 Brill Transformation-Based Error-Driven Learning UNANNOTATED TEXT STATE /~kI~INOTETAx.T ~D TRUTH Figure 1 Transformation-Based Error-Driven Learning." ></td>
	<td class="line x" title="38:404	RULES 2." ></td>
	<td class="line x" title="39:404	Transformation-Based Error-Driven Leaming Figure I illustrates how transformation-based error-driven learning works." ></td>
	<td class="line x" title="40:404	First, unannotated text is passed through an initial-state annotator." ></td>
	<td class="line x" title="41:404	The initial-state annotator can range in complexity from assigning random structure to assigning the output of a sophisticated manually created annotator." ></td>
	<td class="line x" title="42:404	In part-of-speech tagging, various initialstate annotators have been used, including: the output of a stochastic n-gram tagger; labelling all words with their most likely tag as indicated in the training corpus; and naively labelling all words as nouns." ></td>
	<td class="line x" title="43:404	For syntactic parsing, we have explored initialstate annotations ranging from the output of a sophisticated parser to random tree structure with random nonterminal labels." ></td>
	<td class="line x" title="44:404	Once text has been passed through the initial-state annotator, it is then compared to the truth." ></td>
	<td class="line x" title="45:404	A manually annotated corpus is used as our reference for truth." ></td>
	<td class="line x" title="46:404	An ordered list of transformations is learned that can be applied to the output of the initial-state annotator to make it better resemble the truth." ></td>
	<td class="line x" title="47:404	There are two components to a transformation: a rewrite rule and a triggering environment." ></td>
	<td class="line x" title="48:404	An example of a rewrite rule for part-of-speech tagging is: Change the tag from modal to noun." ></td>
	<td class="line x" title="49:404	and an example of a triggering environment is: The preceding word is a determiner." ></td>
	<td class="line x" title="50:404	Taken together, the transformation with this rewrite rule and triggering environment when applied to the word can would correctly change the mistagged: The~determiner can~modal rusted~verb./." ></td>
	<td class="line x" title="51:404	545 Computational Linguistics Volume 21, Number 4 to: The~determiner can~noun rusted~verb ./." ></td>
	<td class="line x" title="52:404	An example of a bracketing rewrite rule is: change the bracketing of a subtree from: A B C to: C A B where A, B and C can be either terminals or nonterminals." ></td>
	<td class="line x" title="53:404	One possible set of triggering environments is any combination of words, part-of-speech tags, and nonterminal labels within and adjacent to the subtree." ></td>
	<td class="line x" title="54:404	Using this rewrite rule and the triggering environment A = the, the bracketing: ( the ( boy ate ) ) would become: ( ( the boy ) ate ) In all of the applications we have examined to date, the following greedy search is applied for deriving a list of transformations: at each iteration of learning, the transformation is found whose application results in the best score according to the objective function being used; that transformation is then added to the ordered transformation list and the training corpus is updated by applying the learned transformation." ></td>
	<td class="line x" title="55:404	Learning continues until no transformation can be found whose application results in an improvement to the annotated corpus." ></td>
	<td class="line x" title="56:404	Other more sophisticated search techniques could be used, such as simulated annealing or learning with a look-ahead window, but we have not yet explored these alternatives." ></td>
	<td class="line x" title="57:404	Figure 2 shows an example of learning transformations." ></td>
	<td class="line x" title="58:404	In this example, we assume there are only four possible transformations, T1 through T4, and that the objective function is the total number of errors." ></td>
	<td class="line x" title="59:404	The unannotated training corpus is processed by the initial-state annotator, and this results in an annotated corpus with 5,100 errors, determined by comparing the output of the initial-state annotator with the manually derived annotations for this corpus." ></td>
	<td class="line x" title="60:404	Next, we apply each of the possible transformations in turn and score the resulting annotated corpus." ></td>
	<td class="line x" title="61:404	1 In this example, 1 In the real implementation, the search is data driven, and therefore not all transformations need to be examined." ></td>
	<td class="line x" title="62:404	546 Brill Transformation-Based Error-Driven Learning Unannotated Corpus I Initial State Annotator Annotated Corpus Errors = 5,100 TI I Annotated Corpus Errors = 5,100 Annotated Corpus Errors = 3,145 Annotated Corpus Errors = 3,910 T1 Annotated Corpus Annotated T2 Corpus Errors = 2,110 Annotated Corpus Errors = 1,231 W4 Ano ted 4 IAnnotate' Corpus Corpus / Errors = 6,300 Errors = 4,25~ Figure 2 An Example of Transformation-Based Error-Driven Learning." ></td>
	<td class="line x" title="63:404	T1 1'2 T3 T4 Annotated Corpus Errors = 1,410 Annotated Corpus Errors = 1,251 Annotated Corpus Errors = 1,231 Annotated Corpus Errors = 1,231 applying transformation T2 results in the largest reduction of errors, so T2 is learned as the first transformation." ></td>
	<td class="line x" title="64:404	T2 is then applied to the entire corpus, and learning continues." ></td>
	<td class="line x" title="65:404	At this stage of learning, transformation T3 results in the largest reduction of error, so it is learned as the second transformation." ></td>
	<td class="line x" title="66:404	After applying the initial-state annotator, followed by T2 and then T3, no further reduction in errors can be obtained from applying any of the transformations, so learning stops." ></td>
	<td class="line x" title="67:404	To annotate fresh text, this text is first annotated by the initial-state annotator, followed by the application of transformation T2 and then by the application of T3." ></td>
	<td class="line x" title="68:404	To define a specific application of transformation-based learning, one must specify the following: . 2." ></td>
	<td class="line x" title="69:404	. The initial state-annotator." ></td>
	<td class="line x" title="70:404	The space of allowable transformations (rewrite rules and triggering environments)." ></td>
	<td class="line x" title="71:404	The objective function for comparing the corpus to the truth and choosing a transformation." ></td>
	<td class="line x" title="72:404	In cases where the application of a particular transformation in one environment could affect its application in another environment, two additional parameters must be specified: the order in which transformations are applied to a corpus, and whether a transformation is applied immediately or only after the entire corpus has been examined for triggering environments." ></td>
	<td class="line x" title="73:404	For example, take the sequence: AAAAAA and the transformation: 547 Computational Linguistics Volume 21, Number 4 Change the label from A to B if the preceding label is A. If the effect of the application of a transformation is not written out until the entire file has been processed for that one transformation, then regardless of the order of processing the output will be: ABBBBB, since the triggering environment of a transformation is always checked before that transformation is applied to any surrounding objects in the corpus." ></td>
	<td class="line x" title="74:404	If the effect of a transformation is recorded immediately, then processing the string left to right would result in: ABABAB, whereas processing right to left would result in: ABBBBB." ></td>
	<td class="line x" title="75:404	3." ></td>
	<td class="line x" title="76:404	A Comparison With Decision Trees The technique employed by the learner is somewhat similar to that used in decision trees (Breiman et al. 1984; Quinlan 1986; Quinlan and Rivest 1989)." ></td>
	<td class="line x" title="77:404	A decision tree is trained on a set of preclassified entities and outputs a set of questions that can be asked about an entity to determine its proper classification." ></td>
	<td class="line x" title="78:404	Decision trees are built by finding the question whose resulting partition is the purest, 2 splitting the training data according to that question, and then recursively reapplying this procedure on each resulting subset." ></td>
	<td class="line x" title="79:404	We first show that the set of classifications that can be provided via decision trees is a proper subset of those that can be provided via transformation lists (an ordered list of transformation-based rules), given the same set of primitive questions." ></td>
	<td class="line x" title="80:404	We then give some practical differences between the two learning methods." ></td>
	<td class="line x" title="81:404	3.1 Decision Trees c_ Transformation Lists We prove here that for a fixed set of primitive queries, any binary decision tree can be converted into a transformation list." ></td>
	<td class="line x" title="82:404	Extending the proof beyond binary trees is straightforward." ></td>
	<td class="line x" title="83:404	Proof (by induction) Base Case: Given the following primitive decision tree, where the classification is A if the answer to the query X?" ></td>
	<td class="line x" title="84:404	is yes, and the classification is B if the answer is no: X?" ></td>
	<td class="line x" title="85:404	B A 2 One possible measure for purity is entropy reduction." ></td>
	<td class="line x" title="86:404	548 this tree can be converted into the following transformation list: . 2." ></td>
	<td class="line x" title="87:404	3. X? Label with S/* Start State Annotation */ If X, then S --* A S --* B/* Empty Tagging Environment--Always Applies To Entities Currently Labeled With S */ Induction: Assume that two decision trees T1 and T2 have corresponding transformation lists L1 and L2." ></td>
	<td class="line x" title="88:404	Assume that the arbitrary label names chosen in constructing L1 are not used in L2, and that those in L2 are not used in L1." ></td>
	<td class="line x" title="89:404	Given a new decision tree T3 constructed from T1 and T2 as follows: Brill Transformation-Based Error-Driven Learning we construct a new transformation list L3." ></td>
	<td class="line x" title="90:404	Assume the first transformation in L1 is: Label with S' and the first transformation in L2 is: Label with S' The first three transformations in L3 will then be: 1." ></td>
	<td class="line x" title="91:404	Label with S 2." ></td>
	<td class="line x" title="92:404	If X then S --* S' 3." ></td>
	<td class="line x" title="93:404	S --+ S' followed by all of the rules in L1 other than the first rule, followed by all of the rules in L2 other than the first rule." ></td>
	<td class="line x" title="94:404	The resulting transformation list will first label an item as S' if X is true, or as S' if X is false." ></td>
	<td class="line x" title="95:404	Next, the tranformations from L1 will be applied if X is true, since S' is the initial-state label for L1." ></td>
	<td class="line x" title="96:404	If X is false, the transformations from L2 will be applied, because S' is the initial-state label for L2." ></td>
	<td class="line x" title="97:404	\[\] 3.2 Decision Trees # Transformation Lists We show here that there exist transformation lists for which no equivalent decision trees exist, for a fixed set of primitive queries." ></td>
	<td class="line x" title="98:404	The following classification problem is one example." ></td>
	<td class="line x" title="99:404	Given a sequence of characters, classify a character based on whether the position index of a character is divisible by 4, querying only using a context of two characters to the left of the character being classified." ></td>
	<td class="line x" title="100:404	549 Computational Linguistics Volume 21, Number 4 Assuming transformations are applied left to right on the sequence, the above classification problem can be solved for sequences of arbitrary length if the effect of a transformation is written out immediately, or for sequences up to any prespecified length if a transformation is carried out only after all triggering environments in the corpus are checked." ></td>
	<td class="line x" title="101:404	We present the proof for the former case." ></td>
	<td class="line x" title="102:404	Given the input sequence: A A A A A A A A A A 0 1 2 3 4 5 6 7 8 9 the underlined characters should be classified as true because their indices are 0, 4, and 8." ></td>
	<td class="line x" title="103:404	To see why a decision tree could not perform this classification, regardless of order of classification, note that, for the two characters before both A3 and A4, both the characters and their classifications are the same, although these two characters should be classified differently." ></td>
	<td class="line x" title="104:404	Below is a transformation list for performing this classification." ></td>
	<td class="line x" title="105:404	Once again, we assume transformations are applied left to right and that the result of a transformation is written out immediately, so that the result of applying transformation x to character ai will always be known when applying transformation x to ai+l. 1." ></td>
	<td class="line x" title="106:404	Label with S RESULT: A/S A/S A/S A/S 2." ></td>
	<td class="line x" title="107:404	If there is no previous character, RESULT: A/F A/S A/S A/S 3." ></td>
	<td class="line x" title="108:404	If the character two to the left is RESULT: A/F A/S A/F A/S 4." ></td>
	<td class="line x" title="109:404	If the character two to the left is RESULT: A/F A/S A/S A/S 5." ></td>
	<td class="line x" title="110:404	F --+ yes 6." ></td>
	<td class="line x" title="111:404	S--* no A/S A/S A/S A/S A/S A/S A/S then S ~ F A/S A/S A/S A/S A/S A/S A/S labelled with F, then S --* F A/F A/S A/F A/S A/F A/S A/F labelled with F, then F ~ S A/F A/S A/S A/S A/F A/S A/S RESULT: A/yes A/no A/no A/no A/yes A/no A/no A/no A/yes A/no A/no The extra power of transformation lists comes from the fact that intermediate results from the classification of one object are reflected in the current label of that object, thereby making this intermediate information available for use in classifying other objects." ></td>
	<td class="line x" title="112:404	This is not the case for decision trees, where the outcome of questions asked is saved implicitly by the current location within the tree." ></td>
	<td class="line x" title="113:404	3.3 Some Practical Differences Between Decision Trees and Transformation Lists There are a number of practical differences between transformation-based error-driven learning and learning decision trees." ></td>
	<td class="line x" title="114:404	One difference is that when training a decision tree, each time the depth of the tree is increased, the average amount of training material available per node at that new depth is halved (for a binary tree)." ></td>
	<td class="line x" title="115:404	In transformationbased learning, the entire training corpus is used for finding all transformations." ></td>
	<td class="line x" title="116:404	Therefore, this method is not subject to the sparse data problems that arise as the depth of the decision tree being learned increases." ></td>
	<td class="line x" title="117:404	Transformations are ordered, with later transformations being dependent upon the outcome of applying earlier transformations." ></td>
	<td class="line x" title="118:404	This allows intermediate results in 550 Brill Transformation-Based Error-Driven Learning classifying one object to be available in classifying other objects." ></td>
	<td class="line x" title="119:404	For instance, whether the previous word is tagged as to-infinitival or to-preposition may be a good cue for determining the part of speech of a word." ></td>
	<td class="line x" title="120:404	3 If, initially, the word to is not reliably tagged everywhere in the corpus with its proper tag (or not tagged at all), then this cue will be unreliable." ></td>
	<td class="line x" title="121:404	The transformation-based learner will delay positing a transformation triggered by the tag of the word to until other transformations have resulted in a more reliable tagging of this word in the corpus." ></td>
	<td class="line x" title="122:404	For a decision tree to take advantage of this information, any word whose outcome is dependent upon the tagging of to would need the entire decision tree structure for the proper classification of each occurrence of to built into its decision tree path." ></td>
	<td class="line x" title="123:404	If the classification of to were dependent upon the classification of yet another word, this would have to be built into the decision tree as well." ></td>
	<td class="line x" title="124:404	Unlike decision trees, in transformation-based learning, intermediate classification results are available and can be used as classification progresses." ></td>
	<td class="line x" title="125:404	Even if decision trees are applied to a corpus in a left-to-right fashion, they are allowed only one pass in which to properly classify." ></td>
	<td class="line x" title="126:404	Since a transformation list is a processor and not a classifier, it can readily be used as a postprocessor to any annotation system." ></td>
	<td class="line x" title="127:404	In addition to annotating from scratch, rules can be learned to improve the performance of a mature annotation system by using the mature system as the initial-state annotator." ></td>
	<td class="line x" title="128:404	This can have the added advantage that the list of transformations learned using a mature annotation system as the initial-state annotator provides a readable description or classification of the errors the mature system makes, thereby aiding in the refinement of that system." ></td>
	<td class="line x" title="129:404	The fact that it is a processor gives a transformation-based learner greater than the classifier-based decision tree." ></td>
	<td class="line x" title="130:404	For example, in applying transformation-based learning to parsing, a rule can apply any structural change to a tree." ></td>
	<td class="line x" title="131:404	In tagging, a rule such as: Change the tag of the current word to X, and of the previous word to Y, if Z holds can easily be handled in the processor-based system, whereas it would be difficult to handle in a classification system." ></td>
	<td class="line x" title="132:404	In transformation-based learning, the objective function used in training is the same as that used for evaluation, whenever this is feasible." ></td>
	<td class="line x" title="133:404	In a decision tree, using system accuracy as an objective function for training typically results in poor performance 4 and some measure of node purity, such as entropy reduction, is used instead." ></td>
	<td class="line x" title="134:404	The direct correlation between rules and performance improvement in transformation-based learning can make the learned rules more readily interpretable than decision tree rules for increasing population purity, s 4." ></td>
	<td class="line x" title="135:404	Part of Speech Tagging: A Case Study in Transformation-Based Error-Driven Learning In this section we describe the practical application of transformation-based learning to part-of-speech tagging." ></td>
	<td class="line x" title="136:404	6 Part-of-speech tagging is a good application to test the 3 The original tagged Brown Corpus (Francis and Kucera, 1982) makes this distinction; the Penn Treebank (Marcus, Santorini, and Marcinkiewicz, 1993) does not." ></td>
	<td class="line x" title="137:404	4 For a discussion of why this is the case, see Breiman et al.(1984, 94-98)." ></td>
	<td class="line x" title="139:404	5 For a discussion of other issues regarding these two learning algorithms, see Ramshaw and Marcus (1994)." ></td>
	<td class="line x" title="140:404	6 All of the programs described herein are freely available with no restrictions on use or redistribution." ></td>
	<td class="line x" title="141:404	For information on obtaining the tagger, contact the author." ></td>
	<td class="line x" title="142:404	551 Computational Linguistics Volume 21, Number 4 learner, for several reasons." ></td>
	<td class="line x" title="143:404	There are a number of large tagged corpora available, allowing for a variety of experiments to be run." ></td>
	<td class="line oc" title="144:404	Part-of-speech tagging is an active area of research; a great deal of work has been done in this area over the past few years (e.g. , Jelinek 1985; Church 1988; Derose 1988; Hindle 1989; DeMarcken 1990; Merialdo 1994; Brill 1992; Black et al. 1992; Cutting et al. 1992; Kupiec 1992; Charniak et al. 1993; Weischedel et al. 1993; Schutze and Singer 1994)." ></td>
	<td class="line x" title="145:404	Part-of-speech tagging is also a very practical application, with uses in many areas, including speech recognition and generation, machine translation, parsing, information retrieval and lexicography." ></td>
	<td class="line x" title="146:404	Insofar as tagging can be seen as a prototypical problem in lexical ambiguity, advances in part-of-speech tagging could readily translate to progress in other areas of lexical, and perhaps structural, ambiguity, such as wordsense disambiguation and prepositional phrase attachment disambiguation." ></td>
	<td class="line x" title="147:404	7 Also, it is possible to cast a number of other useful problems as part-of-speech tagging problems, such as letter-to-sound translation (Huang, Son-Bell, and Baggett 1994) and building pronunciation networks for speech recognition." ></td>
	<td class="line x" title="148:404	Recently, a method has been proposed for using part-of-speech tagging techniques as a method for parsing with lexicalized grammars (Joshi and Srinivas 1994)." ></td>
	<td class="line x" title="149:404	When automated part-of-speech tagging was initially explored (Klein and Simmons 1963; Harris 1962), people manually engineered rules for tagging, sometimes with the aid of a corpus." ></td>
	<td class="line x" title="150:404	As large corpora became available, it became clear that simple Markov-model based stochastic taggers that were automatically trained could achieve high rates of tagging accuracy (Jelinek 1985)." ></td>
	<td class="line x" title="151:404	Markov-model based taggers assign to a sentence the tag sequence that maximizes Prob(word I tag),Prob(tag I previous n tags)." ></td>
	<td class="line x" title="152:404	These probabilities can be estimated directly from a manually tagged corpus, s These stochastic taggers have a number of advantages over the manually built taggers, including obviating the need for laborious manual rule construction, and possibly capturing useful information that may not have been noticed by the human engineer." ></td>
	<td class="line x" title="153:404	However, stochastic taggers have the disadvantage that linguistic information is captured only indirectly, in large tables of statistics." ></td>
	<td class="line oc" title="154:404	Almost all recent work in developing automatically trained part-of-speech taggers has been on further exploring Markovmodel based tagging (Jelinek 1985; Church 1988; Derose 1988; DeMarcken 1990; Merialdo 1994; Cutting et al. 1992; Kupiec 1992; Charniak et al. 1993; Weischedel et al. 1993; Schutze and Singer 1994)." ></td>
	<td class="line x" title="155:404	4.1 Transformation-based Error-driven Part-of-Speech Tagging Transformation-based part of speech tagging works as follows." ></td>
	<td class="line x" title="156:404	9 The initial-state annotator assigns each word its most likely tag as indicated in the training corpus." ></td>
	<td class="line x" title="157:404	The method used for initially tagging unknown words will be described in a later section." ></td>
	<td class="line x" title="158:404	An ordered list of transformations is then learned, to improve tagging accuracy based on contextual cues." ></td>
	<td class="line x" title="159:404	These transformations alter the tagging of a word from X to Y iff 7 In Brill and Resnik (1994), we describe an approach to prepositional phrase attachment disambiguation that obtains highly competitive performance compared to other corpus-based solutions to this problem." ></td>
	<td class="line x" title="160:404	This system was derived in under two hours from the transformation-based part of speech tagger described in this paper." ></td>
	<td class="line x" title="161:404	8 One can also estimate these probabilities without a manually tagged corpus, using a hidden Markov model." ></td>
	<td class="line x" title="162:404	However, it appears to be the case that directly estimating probabilities from even a very small manually tagged corpus gives better results than training a hidden Markov model on a large untagged corpus (see Merialdo (1994))." ></td>
	<td class="line x" title="163:404	9 Earlier versions of this work were reported in Brill (1992, 1994)." ></td>
	<td class="line x" title="164:404	552 Brill Transformation-Based Error-Driven Learning either: 1." ></td>
	<td class="line x" title="165:404	The word was not seen in the training corpus OR 2." ></td>
	<td class="line x" title="166:404	The word was seen tagged with  at least once in the training corpus." ></td>
	<td class="line x" title="167:404	In taggers based on Markov models, the lexicon consists of probabilities of the somewhat counterintuitive but proper form P(WORD I TAG)." ></td>
	<td class="line x" title="168:404	In the transformationbased tagger, the lexicon is simply a list of all tags seen for a word in the training corpus, with one tag labeled as the most likely." ></td>
	<td class="line x" title="169:404	Below we show a lexical entry for the word half in the transformation-based tagger." ></td>
	<td class="line x" title="170:404	1 half: CD DT JJ NN PDT RB VB This entry lists the seven tags seen for half in the training corpus, with NN marked as the most likely." ></td>
	<td class="line x" title="171:404	Below are the lexical entries for half in a Markov model tagger, extracted from the same corpus: P(half l CD ) = 0.000066 P(half l DT ) = 0.000757 P(half I J J) = 0.000092 P(half INN) = 0.000702 P(half l PDT ) = 0.039945 P(half l RB ) = 0.000443 P(half I VB ) = 0.000027 It is difficult to make much sense of these entries in isolation; they have to be viewed in the context of the many contextual probabilities." ></td>
	<td class="line x" title="172:404	First, we will describe a nonlexicalized version of the tagger, where transformation templates do not make reference to specific words." ></td>
	<td class="line x" title="173:404	In the nonlexicalized tagger, the transformation templates we use are: Change tag a to tag b when: 1." ></td>
	<td class="line x" title="174:404	The preceding (following) word is tagged z. 2." ></td>
	<td class="line x" title="175:404	The word two before (after) is tagged z. 3." ></td>
	<td class="line x" title="176:404	One of the two preceding (following) words is tagged z. 4." ></td>
	<td class="line x" title="177:404	One of the three preceding (following) words is tagged z. 5." ></td>
	<td class="line x" title="178:404	The preceding word is tagged z and the following word is tagged w. 6." ></td>
	<td class="line x" title="179:404	The preceding (following) word is tagged z and the word two before (after) is tagged w. where a, b, z and w are variables over the set of parts of speech." ></td>
	<td class="line x" title="180:404	To learn a transformation, the learner, in essence, tries out every possible transformation, 1I and counts the number of tagging errors after each one is applied." ></td>
	<td class="line x" title="181:404	After 10 A description of the partoof-speech tags is provided in Appendix A. 11 All possible instantiations of transformation templates." ></td>
	<td class="line x" title="182:404	553 Computational Linguistics Volume 21, Number 4 1." ></td>
	<td class="line x" title="183:404	apply initial-state annotator to corpus 2." ></td>
	<td class="line x" title="184:404	while transformations can still be found do 3." ></td>
	<td class="line x" title="185:404	for from_tag = tag1 to tagn 4." ></td>
	<td class="line x" title="186:404	for to_tag = tag1 to tagn 5." ></td>
	<td class="line x" title="187:404	for corpus_position = 1 to corpus_size 6." ></td>
	<td class="line x" title="188:404	if (correct_tag(corpus_position) --= to_tag && current_tag(corpus_position) == from_tag) 7." ></td>
	<td class="line x" title="189:404	num_good_transformations(tag(corpus_position -1))++ 8." ></td>
	<td class="line x" title="190:404	else if (correct_tag(corpus_position) == from_tag && current_tag(corpus_position) == from_tag) 9." ></td>
	<td class="line x" title="191:404	num_bad_transformations(tag(corpus_position-1 ))++ 10." ></td>
	<td class="line x" title="192:404	find maxT (num_good_transformations(T) num_bad_transformations(T)) 11." ></td>
	<td class="line x" title="193:404	if this is the best-scoring rule found yet then store as best rule: Change tag from from_tag to to_tag if previous tag is T 12." ></td>
	<td class="line x" title="194:404	apply best rule to training corpus 13." ></td>
	<td class="line x" title="195:404	append best rule to ordered list of transformations Figure 3 Pseudocode for learning transformations." ></td>
	<td class="line x" title="196:404	all possible transformations have been tried, the transformation that resulted in the greatest error reduction is chosen." ></td>
	<td class="line x" title="197:404	Learning stops when no transformations can be found whose application reduces errors beyond some prespecified threshold." ></td>
	<td class="line x" title="198:404	In the experiments described below, processing was done left to right." ></td>
	<td class="line x" title="199:404	For each transformation application, all triggering environments are first found in the corpus, and then the transformation triggered by each triggering environment is carried out." ></td>
	<td class="line x" title="200:404	The search is data-driven, so only a very small percentage of possible transformations really need be examined." ></td>
	<td class="line x" title="201:404	In figure 3, we give pseudocode for the learning algorithm in the case where there is only one transformation template: Change the tag from X to Y if the previous tag is Z. In each learning iteration, the entire training corpus is examined once for every pair of tags X and Y, finding the best transformation whose rewrite changes tag X to tag Y. For every word in the corpus whose environment matches the triggering environment, if the word has tag X and X is the correct tag, then making this transformation will result in an additional tagging error, so we increment the number of errors caused when making the transformation given the part-of-speech tag of the previous word (lines 8 and 9)." ></td>
	<td class="line x" title="202:404	If X is the current tag and Y is the correct tag, then the transformation will result in one less error, so we increment the number of improvements caused when making the transformation given the part-of-speech tag of the previous word (lines 6 and 7)." ></td>
	<td class="line x" title="203:404	In certain cases, a significant increase in speed for training the transformationbased tagger can be obtained by indexing in the corpus where different transformations can and do apply." ></td>
	<td class="line x" title="204:404	For a description of a fast index-based training algorithm, see Ramshaw and Marcus (1994)." ></td>
	<td class="line x" title="205:404	In figure 4, we list the first twenty transformations learned from training on the Penn Treebank Wall Street Journal Corpus (Marcus, Santorini, and Marcinkiewicz 1993)." ></td>
	<td class="line x" title="206:404	12 The first transformation states that a noun should be changed to a verb if 12 Version 0.5 of the Penn Treebank was used in all experiments reported in this paper." ></td>
	<td class="line x" title="207:404	554 Brill Transformation-Based Error-Driven Learning Change Tag # From To 1 NN VB 2 VBP VB 3 NN VB 4 VB NN 5 VBD VBN 6 VBN VBD 7 VBN VBD 8 VBD VBN 9 VBP VB 10 POS VBZ 11 VB VBP 12 VBD VBN 13 IN WDT 14 VBD VBN 15 VB VBP 16 IN WDT 17 IN DT 18 JJ NNP 19 IN WDT 20 JJR RBR Figure 4 Condition Previous tag is TO One of the previous three tags is MD One of the previous two tags is MD One of the previous two tags is DT One of the previous three tags is VBZ Previous tag is PRP Previous tag is NNP Previous tag is VBD Previous tag is TO Previous tag is PRP Previous tag is NNS One of previous three tags is VBP One of next two tags is VB One of previous two tags is VB Previous tag is PRP Next tag is VBZ Next tag is NN Next tag is NNP Next tag is VBD Next tag is JJ The first 20 nonlexicalized transformations." ></td>
	<td class="line x" title="208:404	the previous tag is TO, as in: to~TO conflict/NN--.VB with." ></td>
	<td class="line x" title="209:404	The second transformation fixes a tagging such as: might/MD vanish/VBP--.VB." ></td>
	<td class="line x" title="210:404	The third fixes might/MD not reply/NN--.VB." ></td>
	<td class="line x" title="211:404	The tenth transformation is for the token's, which is a separate token in the Penn Treebank." ></td>
	<td class="line x" title="212:404	's is most frequently used as a possessive ending, but after a personal pronoun, it is a verb (John's, compared to he 's)." ></td>
	<td class="line x" title="213:404	The transformations changing IN to WDT are for tagging the word that, to determine in which environments that is being used as a synonym of which." ></td>
	<td class="line x" title="214:404	4.2 Lexicalizing the Tagger In general, no relationships between words have been directly encoded in stochastic n-gram taggers." ></td>
	<td class="line x" title="215:404	13 In the Markov model typically used for stochastic tagging, state transition probabilities (P(Tagi I Tagi_l Tagi-n)) express the likelihood of a tag immediately following n other tags, and emit probabilities (P(Wordj I Tagi)) express the likelihood of a word, given a tag." ></td>
	<td class="line x" title="216:404	Many useful relationships, such as that between a word and the previous word, or between a tag and the following word, are not directly captured by Markov-model based taggers." ></td>
	<td class="line x" title="217:404	The same is true of the nonlexicalized transformation-based tagger, where transformation templates do not make reference to words." ></td>
	<td class="line x" title="218:404	To remedy this problem, we extend the transformation-based tagger by adding 13 In Kupiec (1992), a limited amount of lexicalization is introduced by having a stochastic tagger with word states for the 100 most frequent words in the corpus." ></td>
	<td class="line x" title="219:404	555 Computational Linguistics Volume 21, Number 4 contextual transformations that can make reference to words as well as part-of-speech tags." ></td>
	<td class="line x" title="220:404	The transformation templates we add are: Change tag a to tag b when: . 2." ></td>
	<td class="line x" title="221:404	3. 4." ></td>
	<td class="line x" title="222:404	5. 6." ></td>
	<td class="line x" title="223:404	7. . The The The The t. The preceding (following) word is w. The word two before (after) is w. One of the two preceding (following) words is w. current word is w and the preceding (following) word is x. current word is w and the preceding (following) word is tagged z. current word is w. preceding (following) word is w and the preceding (following) tag is The current word is w, the preceding (following) word is w2 and the preceding (following) tag is t. where w and x are variables over all words in the training corpus, and z and t are variables over all parts of speech." ></td>
	<td class="line x" title="224:404	BelOw we list two lexicalized transformations that were learned, training once again on the Wall Street Journal." ></td>
	<td class="line x" title="225:404	Change the tag: (12) From IN to RB if the word two positions to the right is as." ></td>
	<td class="line x" title="226:404	(16) From VBP to VB if one of the previous two words is n't." ></td>
	<td class="line x" title="227:404	TM The Penn Treebank tagging style manual specifies that in the collocation as  as, the first as is tagged as an adverb and the second is tagged as a preposition." ></td>
	<td class="line x" title="228:404	Since as is most frequently tagged as a preposition in the training corpus, the initial-state tagger will mistag the phrase as tall as as: as/IN tall/JJ as/IN The first lexicalized transformation corrects this mistagging." ></td>
	<td class="line x" title="229:404	Note that a bigram tagger trained on our training set would not correctly tag the first occurrence of as." ></td>
	<td class="line x" title="230:404	Although adverbs are more likely than prepositions to follow some verb form tags, the fact that P(as \] IN) is much greater than P(as \] RB), and P(JJ \] IN) is much greater than P(JJ \] RB) lead to as being incorrectly tagged as a preposition by a stochastic tagger." ></td>
	<td class="line x" title="231:404	A trigram tagger will correctly tag this collocation in some instances, due to the fact that P(IN \] RB JJ) is greater than P(IN \] IN JJ), but the outcome will be highly dependent upon the context in which this collocation appears." ></td>
	<td class="line x" title="232:404	The second transformation arises from the fact that when a verb appears in a context such as We do n't eat or We did n't usually drink, the verb is in base form." ></td>
	<td class="line x" title="233:404	A stochastic trigram tagger would have to capture this linguistic information indirectly from frequency counts of all trigrams of the form shown in figure 5 (where a star can match any part-of-speech tag) and from the fact that P(n't \] RB) is fairly high." ></td>
	<td class="line x" title="234:404	14 In the Penn Treebank, n't is treated as a separate token, so don't becomes do/VBP n't/RB." ></td>
	<td class="line x" title="235:404	556 Brill Transformation-Based Error-Driven Learning * RB VBP * RB VB RB * VBP RB * VB Figure 5 Trigram Tagger Probability Tables." ></td>
	<td class="line x" title="236:404	In Weischedel et al.(1993), results are given when training and testing a Markovmodel based tagger on the Penn Treebank Tagged Wall Street Journal Corpus." ></td>
	<td class="line x" title="238:404	They cite results making the closed vocabulary assumption that all possible tags for all words in the test set are known." ></td>
	<td class="line x" title="239:404	When training contextual probabilities on one million words, an accuracy of 96.7% was achieved." ></td>
	<td class="line x" title="240:404	Accuracy dropped to 96.3% when contextual probabilities were trained on 64,000 words." ></td>
	<td class="line x" title="241:404	We trained the transformation-based tagger on the same corpus, making the same closed-vocabulary assumption." ></td>
	<td class="line x" title="242:404	15 When training contextual rules on 600,000 words, an accuracy of 97.2% was achieved on a separate 150,000 word test set." ></td>
	<td class="line x" title="243:404	When the training set was reduced to 64,000 words, accuracy dropped to 96.7%." ></td>
	<td class="line x" title="244:404	The transformation-based learner achieved better performance, despite the fact that contextual information was captured in a small number of simple nonstochastic rules, as opposed to 10,000 contextual probabilities that were learned by the stochastic tagger." ></td>
	<td class="line x" title="245:404	These results are summarized in table 1." ></td>
	<td class="line x" title="246:404	When training on 600,000 words, a total of 447 transformations were learned." ></td>
	<td class="line x" title="247:404	However, transformations toward the end of the list contribute very little to accuracy: applying only the first 200 learned transformations to the test set achieves an accuracy of 97.0%; applying the first 100 gives an accuracy of 96.8%." ></td>
	<td class="line x" title="248:404	To match the 96.7% accuracy achieved by the stochastic tagger when it was trained on one million words, only the first 82 transformations are needed." ></td>
	<td class="line x" title="249:404	To see whether lexicalized transformations were contributing to the transformationbased tagger accuracy rate, we first trained the tagger using the nonlexical transformation template subset, then ran exactly the same test." ></td>
	<td class="line x" title="250:404	Accuracy of that tagger was 97.0%." ></td>
	<td class="line x" title="251:404	Adding lexicalized transformations resulted in a 6.7% decrease in the error rate (see table 1)." ></td>
	<td class="line x" title="252:404	16 We found it a bit surprising that the addition of lexicalized transformations did not result in a much greater improvement in performance." ></td>
	<td class="line x" title="253:404	When transformations are allowed to make reference to words and word pairs, some relevant information is probably missed due to sparse data." ></td>
	<td class="line x" title="254:404	We are currently exploring the possibility of incorporating word classes into the rule-based learner, in hopes of overcoming this problem." ></td>
	<td class="line x" title="255:404	The idea is quite simple." ></td>
	<td class="line x" title="256:404	Given any source of word class information, such 15 In both Weischedel et al.(1993) and here, the test set was incorporated into the lexicon, but was not used in learning contextual information." ></td>
	<td class="line x" title="258:404	Testing with no unknown words might seem like an unrealistic test." ></td>
	<td class="line x" title="259:404	We have done so for three reasons: (1) to allow for a comparison with previously quoted results, (2) to isolate known word accuracy from unknown word accuracy, and (3) in some systems, such as a closed vocabulary speech recognition system, the assumption that all words are known is valid." ></td>
	<td class="line x" title="260:404	(We show results when unknown words are included later in the paper)." ></td>
	<td class="line x" title="261:404	16 The training we did here was slightly suboptimal, in that we used the contextual rules learned with unknown words (described in the next section), and filled in the dictionary, rather than training on a corpus without unknown words." ></td>
	<td class="line x" title="262:404	557 Computational Linguistics Volume 21, Number 4 Table 1 Comparison of Tagging Accuracy With No Unknown Words Training # of Rules Corpus or Context." ></td>
	<td class="line x" title="263:404	Acc." ></td>
	<td class="line x" title="264:404	Method Size (Words) Probs." ></td>
	<td class="line x" title="265:404	(%) Stochastic 64 K 6,170 96.3 Stochastic 1 Million 10,000 96.7 Rule-Based With Lex." ></td>
	<td class="line x" title="266:404	Rules 64 K 215 96.7 Rule-Based With Lex." ></td>
	<td class="line x" title="267:404	Rules 600 K 447 97.2 Rule-Based w/o Lex." ></td>
	<td class="line x" title="268:404	Rules 600 K 378 97.0 as WordNet (Miller 1990), the learner is extended such that a rule is allowed to make reference to parts of speech, words, and word classes, allowing for rules such as Change the tag from X to Y if the following word belongs to word class Z. This approach has already been successfully applied to a system for prepositional phrase attachment disambiguation (Brill and Resnik 1994)." ></td>
	<td class="line x" title="269:404	4.3 Tagging Unknown Words So far, we have not addressed the problem of unknown words." ></td>
	<td class="line x" title="270:404	As stated above, the initial-state annotator for tagging assigns all words their most likely tag, as indicated in a training corpus." ></td>
	<td class="line x" title="271:404	Below we show how a transformation-based approach can be taken for tagging unknown words, by automatically learning cues to predict the most likely tag for words not seen in the training corpus." ></td>
	<td class="line x" title="272:404	If the most likely tag for unknown words can be assigned with high accuracy, then the contextual rules can be used to improve accuracy, as described above." ></td>
	<td class="line x" title="273:404	In the transformation-based unknown-word tagger, the initial-state annotator naively assumes the most likely tag for an unknown word is 'proper noun' if the word is capitalized and 'common noun' otherwise." ></td>
	<td class="line x" title="274:404	17 Below, we list the set of allowable transformations." ></td>
	<td class="line x" title="275:404	Change the tag of an unknown word (from X) to Y if: 1." ></td>
	<td class="line x" title="276:404	. 3." ></td>
	<td class="line x" title="277:404	. 5." ></td>
	<td class="line x" title="278:404	Deleting the prefix (suffix) x, Ixl < 4, results in a word (x is any string of length 1 to 4)." ></td>
	<td class="line x" title="279:404	The first (last) (1,2,3,4) characters of the word are x. Adding the character string x as a prefix (suffix) results in a word (Ixl ~ 4)." ></td>
	<td class="line x" title="280:404	Word w ever appears immediately to the left (right) of the word." ></td>
	<td class="line x" title="281:404	Character z appears in the word." ></td>
	<td class="line x" title="282:404	17 If we change the tagger to tag all unknown words as common nouns, then a number of rules are learned of the form: change tag to proper noun if the prefix is 'E', 'A', 'B', etc. , since the learner is not provided with the concept of upper case in its set of transformation templates." ></td>
	<td class="line x" title="283:404	558 Brill Transformation-Based Error-Driven Learning Change Tag # From To Condition 1 NN NNS Has suffix -s 2 NN CD Has character." ></td>
	<td class="line x" title="284:404	3 NN JJ Has character 4 NN VBN Has suffix -ed 5 NN VBG Has suffix -ing 6 ??" ></td>
	<td class="line x" title="285:404	RB Has suffix -ly 7 ??" ></td>
	<td class="line x" title="286:404	JJ Adding suffix -ly results in a word." ></td>
	<td class="line x" title="287:404	8 NN CD The word $ can appear to the left." ></td>
	<td class="line x" title="288:404	9 NN JJ Has suffix -al 10 NN VB The word would can appear to the left." ></td>
	<td class="line x" title="289:404	11 NN CD Has character 0 12 NN JJ The word be can appear to the left." ></td>
	<td class="line x" title="290:404	13 NNS JJ Has suffix -us 14 NNS VBZ The word it can appear to the left." ></td>
	<td class="line x" title="291:404	15 NN JJ Has suffix -ble 16 NN JJ Has suffix -ic 17 NN CD Has character 1 18 NNS NN Has suffix -ss 19 ??" ></td>
	<td class="line x" title="292:404	JJ Deleting the prefix unresults in a word 20 NN JJ Has suffix -ire Figure 6 The first 20 transformations for unknown words." ></td>
	<td class="line x" title="293:404	An unannotated text can be used to check the conditions in all of the above transformation templates." ></td>
	<td class="line x" title="294:404	Annotated text is necessary in training to measure the effect of transformations on tagging accuracy." ></td>
	<td class="line x" title="295:404	Since the goal is to label each lexical entry for new words as accurately as possible, accuracy is measured on a per type and not a per token basis." ></td>
	<td class="line x" title="296:404	Figure 6 shows the first 20 transformations learned for tagging unknown words in the Wall Street Journal corpus." ></td>
	<td class="line x" title="297:404	As an example of how rules can correct errors generated by prior rules, note that applying the first transformation will result in the mistagging of the word actress." ></td>
	<td class="line x" title="298:404	The 18th learned rule fixes this problem." ></td>
	<td class="line x" title="299:404	This rule states: Change a tag from plural common noun to singular common noun if the word has SUffiX ss." ></td>
	<td class="line x" title="300:404	Keep in mind that no specific affixes are prespecified." ></td>
	<td class="line x" title="301:404	A transformation can make reference to any string of characters up to a bounded length." ></td>
	<td class="line x" title="302:404	So while the first rule specifies the English suffix 's', the rule learner was not constrained from considering such nonsensical rules as: Change a tag to adjective if the word has suffix 'xhqr'." ></td>
	<td class="line x" title="303:404	Also, absolutely no English-specific information (such as an affix list) need be prespecified in the learner." ></td>
	<td class="line x" title="304:404	TM 18 This learner has also been applied to tagging Old English." ></td>
	<td class="line x" title="305:404	See Brill (1993b)." ></td>
	<td class="line x" title="306:404	Although the 559 Computational Linguistics Volume 21, Number 4 J == i i E i i 0 100 200 300 400 Transformation Number Figure 7 Accuracy vs. Transformation Number We then ran the following experiment using 1.1 million words of the Penn Treebank Tagged Wall Street Journal Corpus." ></td>
	<td class="line x" title="307:404	Of these, 950,000 words were used for training and 150,000 words were used for testing." ></td>
	<td class="line x" title="308:404	Annotations of the test corpus were not used in any way to train the system." ></td>
	<td class="line x" title="309:404	From the 950,000 word training corpus, 350,000 words were used to learn rules for tagging unknown words, and 600,000 words were used to learn contextual rules; 243 rules were learned for tagging unknown words, and 447 contextual tagging rules were learned." ></td>
	<td class="line x" title="310:404	Unknown word accuracy on the test corpus was 82.2%, and overall tagging accuracy on the test corpus was 96.6%." ></td>
	<td class="line x" title="311:404	To our knowledge, this is the highest overall tagging accuracy ever quoted on the Penn Treebank Corpus when making the open vocabulary assumption." ></td>
	<td class="line x" title="312:404	Using the tagger without lexicalized rules, an overall accuracy of 96.3% and an unknown word accuracy of 82.0% is obtained." ></td>
	<td class="line x" title="313:404	A graph of accuracy as a function of transformation number on the test set for lexicalized rules is shown in figure 7." ></td>
	<td class="line x" title="314:404	Before applying any transformations, test set accuracy is 92.4%, so the transformations reduce the error rate by 50% over the baseline." ></td>
	<td class="line x" title="315:404	The high baseline accuracy is somewhat misleading, as this includes the tagging of unambiguous words." ></td>
	<td class="line x" title="316:404	Baseline accuracy when the words that are unambiguous in our lexicon are not considered is 86.4%." ></td>
	<td class="line x" title="317:404	However, it is difficult to compare taggers using this figure, as the accuracy of the system depends on the particular lexicon used." ></td>
	<td class="line x" title="318:404	For instance, in our training set the word the was tagged with a number of different tags, and so according to our lexicon the is ambiguous." ></td>
	<td class="line x" title="319:404	If we instead used a lexicon where the is listed unambiguously as a determiner, the baseline accuracy would be 84.6%." ></td>
	<td class="line x" title="320:404	For tagging unknown words, each word is initially assigned a part-of-speech tag based on word and word-distribution features." ></td>
	<td class="line x" title="321:404	Then, the tag may be changed based on contextual cues, via contextual transformations that are applied to the entire corpus, both known and unknown-words." ></td>
	<td class="line x" title="322:404	When the contextual rule learner learns transformations, it does so in an attempt to maximize overall tagging accuracy, and not unknown-word tagging accuracy." ></td>
	<td class="line x" title="323:404	Unknown words account for only a small percentage of the corpus in our experiments, typically two to three percent." ></td>
	<td class="line x" title="324:404	Since the distributional behavior of unknown words is quite different from that of known words, and transformations are not English-specific, the set of transformation templates would have to be extended to process languages with dramatically different morphology, 560 Brill Transformation-Based Error-Driven Learning Table 2 Tagging Accuracy on Different Corpora Corpus Accuracy Penn WSJ 96.6% Penn Brown 96.3% Orig Brown 96.5% since a transformation that does not increase unknown-word tagging accuracy can still be beneficial to overall tagging accuracy, the contextual transformations learned are not optimal in the sense of leading to the highest tagging accuracy on unknown words." ></td>
	<td class="line x" title="325:404	Better unknown-word accuracy may be possible by training and using two sets of contextual rules, one maximizing known-word accuracy and the other maximizing unknown-word accuracy, and then applying the appropriate transformations to a word when tagging, depending upon whether the word appears in the lexicon." ></td>
	<td class="line x" title="326:404	We are currently experimenting with this idea." ></td>
	<td class="line x" title="327:404	In Weischedel et al.(1993), a statistical approach to tagging unknown words is shown." ></td>
	<td class="line x" title="329:404	In this approach, a number of suffixes and important features are prespecified." ></td>
	<td class="line x" title="330:404	Then, for unknown words: p(W I T) -= p(unknown word I T)  p(Capitalize-feature I T) * p(suffixes, hyphenation I T) Using this equation for unknown word emit probabilities within the stochastic tagger, an accuracy of 85% was obtained on the Wall Street Journal corpus." ></td>
	<td class="line x" title="331:404	This portion of the stochastic model has over 1,000 parameters, with 108 possible unique emit probabilities, as opposed to a small number of simple rules that are learned and used in the rule-based approach." ></td>
	<td class="line x" title="332:404	In addition, the transformation-based method learns specific cues instead of requiring them to be prespecified, allowing for the possibility of uncovering cues not apparent to the human language engineer." ></td>
	<td class="line x" title="333:404	We have obtained comparable performance on unknown words, while capturing the information in a much more concise and perspicuous manner, and without prespecifying any information specific to English or to a specific corpus." ></td>
	<td class="line x" title="334:404	In table 2, we show tagging results obtained on a number of different corpora, in each case training on roughly 9.5 x 10 s words total and testing on a separate test set of 1.5-2 x 10 s words." ></td>
	<td class="line x" title="335:404	Accuracy is consistent across these corpora and tag sets." ></td>
	<td class="line x" title="336:404	In addition to obtaining high rates of accuracy and representing relevant linguistic information in a small set of rules, the part-of-speech tagger can also be made to run extremely fast." ></td>
	<td class="line x" title="337:404	Roche and Schabes (1995) show a method for converting a list of tagging transformations into a deterministic finite state transducer with one state transition taken per word of input; the result is a transformation-based tagger whose tagging speed is about ten times that of the fastest Markov-model tagger." ></td>
	<td class="line x" title="338:404	4.4 K-Best Tags There are certain circumstances where one is willing to relax the one-tag-per-word requirement in order to increase the probability that the correct tag will be assigned to each word." ></td>
	<td class="line x" title="339:404	In DeMarcken (1990) and Weischedel et al.(1993), k-best tags are assigned within a stochastic tagger by returning all tags within some threshold of probability of being correct for a particular word." ></td>
	<td class="line x" title="341:404	561 Computational Linguistics Volume 21, Number 4 Table 3 Results from k-best tagging." ></td>
	<td class="line x" title="342:404	# of Rules Accuracy Avg." ></td>
	<td class="line x" title="343:404	# of tags per word 0 96.5 1.00 50 96.9 1.02 100 97.4 1.04 150 97.9 1.10 200 98.4 1.19 250 99.1 1.50 We can modify the transformation-based tagger to return multiple tags for a word by making a simple modification to the contextual transformations described above." ></td>
	<td class="line x" title="344:404	The initial-state annotator is the tagging output of the previously described one-best transformation-based tagger." ></td>
	<td class="line x" title="345:404	The allowable transformation templates are the same as the contextual transformation templates listed above, but with the rewrite rule: change tag X to tag Y modified to add tag X to tag Y or add tag X to word W. Instead of changing the tagging of a word, transformations now add alternative taggings to a word." ></td>
	<td class="line x" title="346:404	When allowing more than one tag per word, there is a trade-off between accuracy and the average number of tags for each word." ></td>
	<td class="line x" title="347:404	Ideally, we would like to achieve as large an increase in accuracy with as few extra tags as possible." ></td>
	<td class="line x" title="348:404	Therefore, in training we find transformations that maximize the function: Number of corrected errors Number of additional tags In table 3, we present results from first using the one-tag-per-word transformation-based tagger described in the previous section and then applying the k-best tag transformations." ></td>
	<td class="line x" title="349:404	These transformations were learned from a separate 240,000 word corpus." ></td>
	<td class="line x" title="350:404	As a baseline, we did k-best tagging of a test corpus." ></td>
	<td class="line x" title="351:404	Each known word in the test corpus was tagged with all tags seen with that word in the training corpus and the five most likely unknown-word tags were assigned to all words not seen in the training corpus." ></td>
	<td class="line x" title="352:404	19 This resulted in an accuracy of 99.0%, with an average of 2.28 tags per word." ></td>
	<td class="line x" title="353:404	The transformation-based tagger obtained the same accuracy with 1.43 tags per word, one third the number of additional tags as the baseline tagger." ></td>
	<td class="line x" title="354:404	2 5." ></td>
	<td class="line x" title="355:404	Conclusions In this paper, we have described a new transformation-based approach to corpus-based learning." ></td>
	<td class="line x" title="356:404	We have given details of how this approach has been applied to part-ofspeech tagging and have demonstrated that the transformation-based approach obtains 19 Thanks to Fred Jelinek and Fernando Pereira for suggesting this baseline experiment." ></td>
	<td class="line x" title="357:404	20 Unfortunately, it is difficult to find results to compare these k-best tag results to." ></td>
	<td class="line x" title="358:404	In DeMarcken (1990), the test set is included in the training set, and so it is difficult to know how this system would do on fresh text." ></td>
	<td class="line x" title="359:404	In Weischedel et al.(1993), a k-best tag experiment was run on the Wall Street Journal corpus." ></td>
	<td class="line x" title="361:404	They quote the average number of tags per word for various threshold settings, but do not provide accuracy results." ></td>
	<td class="line x" title="362:404	562 Brill Transformation-Based Error-Driven Learning competitive performance with stochastic taggers on tagging both unknown and known words." ></td>
	<td class="line x" title="363:404	The transformation-based tagger captures linguistic information in a small number of simple nonstochastic rules, as opposed to large numbers of lexical and contextual probabilities." ></td>
	<td class="line x" title="364:404	This learning approach has also been applied to a number of other tasks, including prepositional phrase attachment disambiguation (Brill and Resnik 1994), bracketing text (Brill 1993a) and labeling nonterminal nodes (Brill 1993c)." ></td>
	<td class="line x" title="365:404	Recently, we have begun to explore the possibility of extending these techniques to other problems, including learning pronunciation networks for speech recognition and learning mappings between syntactic and semantic representations." ></td>
	<td class="line x" title="366:404	Appendix A: Penn Treebank Part-of-Speech Tags (Excluding Punctuation) 1." ></td>
	<td class="line x" title="367:404	CC Coordinating conjunction 2." ></td>
	<td class="line x" title="368:404	CD Cardinal number 3." ></td>
	<td class="line x" title="369:404	DT Determiner 4." ></td>
	<td class="line x" title="370:404	EX Existential 'there' 5." ></td>
	<td class="line x" title="371:404	FW Foreign word 6." ></td>
	<td class="line x" title="372:404	IN Preposition or subordinating conjunction 7." ></td>
	<td class="line x" title="373:404	JJ Adjective 8." ></td>
	<td class="line x" title="374:404	JJR Adjective, comparative 9." ></td>
	<td class="line x" title="375:404	JJS Adjective, superlative 10." ></td>
	<td class="line x" title="376:404	LS List item marker 11." ></td>
	<td class="line x" title="377:404	MD Modal 12." ></td>
	<td class="line x" title="378:404	NN Noun, singular or mass 13." ></td>
	<td class="line x" title="379:404	NNS Noun, plural 14." ></td>
	<td class="line x" title="380:404	NNP Proper noun, singular 15." ></td>
	<td class="line x" title="381:404	NNPS Proper noun, plural 16." ></td>
	<td class="line x" title="382:404	PDT Predeterminer 17." ></td>
	<td class="line x" title="383:404	POS Possessive ending 18." ></td>
	<td class="line x" title="384:404	PP Personal pronoun 19." ></td>
	<td class="line x" title="385:404	PP$ Possessive pronoun 20." ></td>
	<td class="line x" title="386:404	RB Adverb 21." ></td>
	<td class="line x" title="387:404	RBR Adverb, comparative 22." ></td>
	<td class="line x" title="388:404	RBS Adverb, superlative 23." ></td>
	<td class="line x" title="389:404	RP Particle 24." ></td>
	<td class="line x" title="390:404	SYM Symbol 25." ></td>
	<td class="line x" title="391:404	TO 'to' 26." ></td>
	<td class="line x" title="392:404	UH Interjection 27." ></td>
	<td class="line x" title="393:404	VB Verb, base form 28." ></td>
	<td class="line x" title="394:404	VBD Verb, past tense 29." ></td>
	<td class="line x" title="395:404	VBG Verb, gerund or present participle 30." ></td>
	<td class="line x" title="396:404	VBN Verb, past participle 31." ></td>
	<td class="line x" title="397:404	VBP Verb, non-3rd person singular present 32." ></td>
	<td class="line x" title="398:404	VBZ Verb, 3rd person singular present 33." ></td>
	<td class="line x" title="399:404	WDT Wh-determiner 34." ></td>
	<td class="line x" title="400:404	WP Wh-pronoun 35." ></td>
	<td class="line x" title="401:404	WP$ Possessive wh-pronoun 36." ></td>
	<td class="line x" title="402:404	WRB Wh-adverb 563 Computational Linguistics Volume 21, Number 4 Acknowledgments This work was funded in part by NSF grant IRI-9502312." ></td>
	<td class="line x" title="403:404	In addition, this work was done in part while the author was in the Spoken Language Systems Group at Massachusetts Institute of Technology under ARPA grant N00014-89-J-1332, and by DARPA/AFOSR grant AFOSR-90-0066 at the University of Pennsylvania." ></td>
	<td class="line x" title="404:404	Thanks to Mitch Marcus, Mark Villain, and the anonymous reviewers for many useful comments on earlier drafts of this paper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P95-1039
Tagset Reduction Without Information Loss
Brants, Thorsten;"></td>
	<td class="line x" title="1:109	Tagset P.eduction Without Information Loss Thorsten Brants Universit~t des Saarlandes Computerlinguistik D-66041 Saarbrficken, Germany thorst en~coli, unisb." ></td>
	<td class="line x" title="2:109	de Abstract A technique for reducing a tagset used for n-gram part-of-speech disambiguation is introduced and evaluated in an experiment." ></td>
	<td class="line x" title="3:109	The technique ensures that all information that is provided by the original tagset can be restored from the reduced one." ></td>
	<td class="line x" title="4:109	This is crucial, since we are interested in the linguistically motivated tags for part-of-speech disambiguation." ></td>
	<td class="line x" title="5:109	The reduced tagset needs fewer parameters for its statistical model and allows more accurate parameter estimation." ></td>
	<td class="line x" title="6:109	Additionally, there is a slight but not significant improvement of tagging accuracy." ></td>
	<td class="line oc" title="7:109	1 Motivation Statistical part-of-speech disambiguation can be efficiently done with n-gram models (Church, 1988; Cutting et al. , 1992)." ></td>
	<td class="line o" title="8:109	These models are equivalent to Hidden Markov Models (HMMs) (Rabiner, 1989) of order n 1." ></td>
	<td class="line x" title="9:109	The states represent parts of speech (categories, tags), there is exactly one state for each category, and each state outputs words of a particular category." ></td>
	<td class="line x" title="10:109	The transition and output probabilities of the HMM are derived from smoothed frequency counts in a text corpus." ></td>
	<td class="line x" title="11:109	Generally, the categories for part-of-speech tagging are linguistically motivated and do not reflect the probability distributions or co-occurrence probabilities of words belonging to that category." ></td>
	<td class="line x" title="12:109	It is an implicit assumption for statistical part-of-speech tagging that words belonging to the same category have similar probability distributions." ></td>
	<td class="line x" title="13:109	But this assumption does not hold in many of the cases." ></td>
	<td class="line x" title="14:109	Take for example the word cliff which could be a proper (NP) 1 or a common noun (NN) (ignoring capitalization of proper nouns for the moment)." ></td>
	<td class="line x" title="15:109	The two previous words are a determiner (AT) and an 1All tag names used in this paper are inspired by those used for the LOB Corpus (Garside et al. , 1987)." ></td>
	<td class="line x" title="16:109	adjective (J J)." ></td>
	<td class="line x" title="17:109	The probability of cliff being a common noun is the product of the respective contextual and lexical probabilities p(N N \]AT, JJ)  p(c//fflN N), regardless of other information provided by the actual words (a sheer cliff vs. the wise Cliff)." ></td>
	<td class="line x" title="18:109	Obviously, information useful for probability estimation is not encoded in the tagset." ></td>
	<td class="line x" title="19:109	On the other hand, in some cases information not needed for probability estimation is encoded in the tagset." ></td>
	<td class="line x" title="20:109	The distributions for comparative and superlative forms of adjectives in the Susanne Corpus (Sampson, 1995) are very similar." ></td>
	<td class="line x" title="21:109	The number of correct tag assignments is not affected when we combine the two categories." ></td>
	<td class="line x" title="22:109	However, it does not suffice to assign the combined tag, if we are interested in the distinction between comparative and superlative form for further processing." ></td>
	<td class="line x" title="23:109	We have to ensure that the original (interesting) tag can be restored." ></td>
	<td class="line x" title="24:109	There are two contradicting requirements." ></td>
	<td class="line x" title="25:109	On the one hand, more tags mean that there is more information about a word at hand, on the other hand, the more tags, the severer the sparse-data problem is and the larger the corpora that are needed for training." ></td>
	<td class="line x" title="26:109	This paper presents a way to modify a given tagset, such that categories with similar distributions in a corpus are combined without losing information provided by the original tagset and without losing accuracy." ></td>
	<td class="line x" title="27:109	2 Clustering of Tags The aim of the presented method is to reduce a tagset as much as possible by combining (clustering) two or more tags without losing information and without losing accuracy." ></td>
	<td class="line x" title="28:109	The fewer tags we have, the less parameters have to be estimated and stored, and the less severe is the sparse data problem." ></td>
	<td class="line x" title="29:109	Incoming text will be disambiguated with the new reduced tagset, but we ensure that the original tag is still uniquely ide:.ltified by the new tag." ></td>
	<td class="line x" title="30:109	The basic idea is to exploit the fact that some of the categories have a very similar frequency distribution in a corpus." ></td>
	<td class="line x" title="31:109	If we combine categories with 287 similar distribution characteristics, there should be only a small change in the tagging result." ></td>
	<td class="line x" title="32:109	The main change is that single tags are replaced by a cluster of tags, from which the original has to be identified." ></td>
	<td class="line x" title="33:109	First experiments with tag clustering showed that, even for fully automatic identification of the original tag, tagging accuracy slightly increased when the reduced tagset was used." ></td>
	<td class="line x" title="34:109	This might be a result of having more occurrences per tag for a smaller tagset, and probability estimates are preciser." ></td>
	<td class="line x" title="35:109	2.1 Unique Identification of Original Tags A crucial property of the reduced tagset is that the original tag information can be restored from the new tag, since this is the information we are interested in." ></td>
	<td class="line x" title="36:109	The property can be ensured if we place a constraint on the clustering of tags." ></td>
	<td class="line x" title="37:109	Let )'V be the set of words, C the set of clusters (i.e. the reduced tagset), and 7' the original tagset." ></td>
	<td class="line x" title="38:109	To restore the original tag from a combined tag (cluster), we need a unique function foria : W x C ~ 7-, (1) To ensure that there is such a unique function, we prohibit some of the possible combinations." ></td>
	<td class="line x" title="39:109	A cluster is allowed if and only if there is no word in the lexicon which can have two or more of the original tags combined in one cluster." ></td>
	<td class="line x" title="40:109	Formally, seeing tags as sets of words and clusters as sets of tags: VcEC, tl,t2Ec, tl~t2,wE}/Y: wEtl::~w~t2 (2) If this condition holds, then for all words w tagged with a cluster e, exactly one tag two fulfills w E twe A t~.e E c, yielding fo.,(w, c) = t o. So, the original tag can be restored any time and no information from the original tagset is lost." ></td>
	<td class="line x" title="42:109	Example: Assume that no word in the lexicon can be both comparative (JJ R) and superlative adjective (JJT)." ></td>
	<td class="line x" title="43:109	The categories are combined to {JJR,JJT}." ></td>
	<td class="line x" title="44:109	When processing a text, the word easier is tagged as {JJR,JJT}." ></td>
	<td class="line x" title="45:109	Since the lexicon states that easier can be of category J JR but not of category JJT, the original tag must be J JR. 2.2 Criteria For Combining Tags The are several criteria that can determine the quality of a particular clustering." ></td>
	<td class="line x" title="46:109	1." ></td>
	<td class="line x" title="47:109	Compare the trigram probabilities p(BIXi, A), P(BIA, Xi), and p(XilA, B), i = 1, 2." ></td>
	<td class="line x" title="48:109	Combine two tags X1 and X2, if these probabilities coincide to a certain extent." ></td>
	<td class="line x" title="49:109	2." ></td>
	<td class="line x" title="50:109	Maximize the probability that the training corpus is generated by the HMM which is described by the trigram probabilities." ></td>
	<td class="line x" title="51:109	3." ></td>
	<td class="line x" title="52:109	Maximize the tagging accuracy for a training corpus." ></td>
	<td class="line x" title="53:109	Criterion (1) establishes the theoretical basis, while criteria (2) and (3) immediately show the benefit of a particular combination." ></td>
	<td class="line x" title="54:109	A measure of similarity for (1) is currently under investigation." ></td>
	<td class="line x" title="55:109	We chose (3) for our first experiments, since it was the easiest one to implement." ></td>
	<td class="line x" title="56:109	The only additional effort is a separate, previously unused part of the training corpus for this purpose, the clustering part." ></td>
	<td class="line x" title="57:109	We combine those tags into clusters which give the best results for tagging of the clustering part." ></td>
	<td class="line x" title="58:109	2.3 The Algorithm The total number of potential clusterings grows exponential with the size of the tagset." ></td>
	<td class="line x" title="59:109	Since we are interested in the reduction of large tagsets, a full search regarding all potential clusterings is not feasible." ></td>
	<td class="line x" title="60:109	We compute the local maximum which can be found in polynomial time with a best-first search." ></td>
	<td class="line x" title="61:109	We use a slight modification of the algorithm used by (Stolcke and Omohundro, 1994) for merging HMMs." ></td>
	<td class="line x" title="62:109	Our task is very similar to theirs." ></td>
	<td class="line x" title="63:109	Stolcke and Omohundro start with a first order tIMM where every state represents a single occurrence of a word in a corpus, and the goal is to maximize the a posteriori probability of the model." ></td>
	<td class="line x" title="64:109	We start with a second order HMM (since we use trigrams) where each state represents a part of speech, and our goal is to maximize the tagging accuracy for a corpus." ></td>
	<td class="line x" title="65:109	The clustering algorithm works as follows: 1." ></td>
	<td class="line x" title="66:109	Compute tagging accuracy for the clustering part with the original tagset." ></td>
	<td class="line x" title="67:109	2." ></td>
	<td class="line x" title="68:109	Loop: (a) Compute a set of candidate clusters (obeying constraint (2) mentioned in section 2.1), each consisting of two tags from the previous step." ></td>
	<td class="line x" title="69:109	(b) For each candidate cluster build the resulting tagset and compute tagging accuracy for that tagset." ></td>
	<td class="line x" title="70:109	(c) If tagging accuracy decreases for all combinations of tags, break from the loop." ></td>
	<td class="line x" title="71:109	(d) Add the cluster which maximized the tagging accuracy to the tagset and remove the two tags previously used." ></td>
	<td class="line x" title="72:109	3." ></td>
	<td class="line x" title="73:109	Output the resulting tagset." ></td>
	<td class="line x" title="74:109	2.4 Application of Tag Clustering Two standard trigram tagging procedures were performed as the baseline." ></td>
	<td class="line x" title="75:109	Then clustering was performed on the same data and tagging was done with the reduced tagset." ></td>
	<td class="line x" title="76:109	The reduced tagset was only internally used, the output of the tagger consisted of the original tagset for all experiments." ></td>
	<td class="line x" title="77:109	The Susanne Corpus has about 157,000 words and uses 424 tags (counting tags with indices denoting 288 Table 1: Tagging results for the test parts in the clustering experiments." ></td>
	<td class="line x" title="78:109	Exp. 1 and 2 are used as the baseline." ></td>
	<td class="line x" title="79:109	Training Clustering Testing Result (known words) 1." ></td>
	<td class="line x" title="80:109	parts A and B part C 93.7% correct 2." ></td>
	<td class="line x" title="81:109	parts A and C part B 94.6% correct 3." ></td>
	<td class="line x" title="82:109	part A part B part C 93.9% correct 4." ></td>
	<td class="line x" title="83:109	part A part C part B 94.7% correct multi-word lexemes as separate tags)." ></td>
	<td class="line x" title="84:109	The tags are based on the LOB tagset (Garside et al. , 1987)." ></td>
	<td class="line x" title="85:109	Three parts are taken from the corpus." ></td>
	<td class="line x" title="86:109	Part A consists of about 127,000 words, part B of about 10,000 words, and part C of about 10,000 words." ></td>
	<td class="line x" title="87:109	The rest of the corpus, about 10,000 words, is not used for this experiment." ></td>
	<td class="line x" title="88:109	All parts are mutually disjunct." ></td>
	<td class="line x" title="89:109	First, part A and B were used for training, and part C for testing." ></td>
	<td class="line x" title="90:109	Then, part A and C were used for training, and part B for testing." ></td>
	<td class="line x" title="91:109	About 6% of the words in the test parts did not occur in the training parts, i.e. they are unknown." ></td>
	<td class="line x" title="92:109	For the moment we only care about the known words and not about the unknown words (this is treated as a separate problem)." ></td>
	<td class="line x" title="93:109	Table 1 shows the tagging results for known words." ></td>
	<td class="line x" title="94:109	Clustering was applied in the next steps." ></td>
	<td class="line x" title="95:109	In the third experiment, part A was used for trigram training, part B for clustering and part C for testing." ></td>
	<td class="line x" title="96:109	In the fourth experiment, part A was used for trigram training, part C for clustering and part B for testing." ></td>
	<td class="line x" title="97:109	The baseline experiments used the clustering part for the normal training procedure to ensure that better performance in the clustering experiments is not due to information provided by the additional part." ></td>
	<td class="line x" title="98:109	Clustering reduced the tagset by 33 (third exp.), and 31 (fourth exp)." ></td>
	<td class="line x" title="99:109	tags." ></td>
	<td class="line x" title="100:109	The tagging results for the known words are shown in table 1." ></td>
	<td class="line x" title="101:109	The improvement in the tagging result is too small to be significant." ></td>
	<td class="line x" title="102:109	However, the tagset is reduced, thus also reducing the number of parameters without losing accuracy." ></td>
	<td class="line x" title="103:109	Experiments with larger texts and more permutations will be performed to get precise results for the improvement." ></td>
	<td class="line x" title="104:109	3 Conclusions We have shown a method for reducing a tagset used for part-of-speech tagging without losing information given by the original tagset." ></td>
	<td class="line x" title="105:109	In a first experiment, we were able to reduce a large tagset and needed fewer parameters for the n-gram model." ></td>
	<td class="line x" title="106:109	Additionally, tagging accuracy slightly increased, but the improvement was not significant." ></td>
	<td class="line x" title="107:109	Further investigation will focus on criteria for cluster selection." ></td>
	<td class="line x" title="108:109	Can we use a similarity measure of probability distributions to identify optimal clusters?" ></td>
	<td class="line x" title="109:109	How far can we reduce the tagset without losing accuracy?" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W95-0101
Unsupervised Learning Of Disambiguation Rules For Part Of Speech Tagging
Brill, Eric;"></td>
	<td class="line x" title="1:188	Unsupervised Learning of Disambiguation Rules for Part of Speech Tagging Eric BrilP Department of Computer Science Johns Hopkins University brill@cs, j hu." ></td>
	<td class="line x" title="2:188	edu Abstract In this paper we describe an unsupervised learning algorithm for automatically training a rule-based part of speech tagger without using a manually tagged corpus." ></td>
	<td class="line x" title="3:188	We compare this algorithm to the Baum-Welch algorithm, used for unsupervised training of stochastic taggers." ></td>
	<td class="line x" title="4:188	Next, we show a method for combining unsupervised and supervised rule-based training algorithms to create a highly accurate tagger using only a small amount of manually tagged text." ></td>
	<td class="line x" title="5:188	Introduction There has recently been a great deal of work exploring methods for automatically training part of speech taggers, as an alternative to laboriously hand-crafting rules for tagging, as was done in the past \[Klein and Simmons, 1963; Harris, 1962\]." ></td>
	<td class="line oc" title="6:188	Almost all of the work in the area of automatically trained taggers has explored Markov-model based part of speech tagging \[Jelinek, 1985; Church, 1988; Derose, 1988; DeMarcken, 1990; Cutting et al. , 1992; Kupiec, 1992; Charniak et al. , 1993; Weischedel et al. , 1993; Schutze and Singer, 1994; Lin et al. , 1994; Elworthy, 1994; Merialdo, 1995\]." ></td>
	<td class="line x" title="7:188	2 For a Markov-model based tagger, training consists of learning both lexical probabilities (P(wordltag)) and contextual probabilities (P(tagiltagi_l tagi-n))." ></td>
	<td class="line x" title="8:188	Once trained, a sentence can be tagged by searching for the tag sequence that maximizes the product of lexical and contextual probabilities." ></td>
	<td class="line x" title="9:188	The most accurate stochastic taggers use estimates of lexical and contextual probabilities extracted from large manually annotated corpora (eg." ></td>
	<td class="line x" title="10:188	\[Weischedel et al. , 1993; Charniak et al. , 1993\])." ></td>
	<td class="line oc" title="11:188	It is possible to use unsupervised learning to train stochastic taggers without the need for a manually annotated corpus by using the Baum-Welch algorithm \[Baum, 1972; Jelinek, 1985; Cutting et al. , 1992; Kupiec, 1992; Elworthy, 1994; Merialdo, 1995\]." ></td>
	<td class="line x" title="12:188	This algorithm works by iteratively adjusting the lexical and contextual probabilities to increase the overall probability of the training corpus." ></td>
	<td class="line x" title="13:188	If no prior knowledge is available, probabilities are initially either assigned randomly or evenly distributed." ></td>
	<td class="line x" title="14:188	Although less accurate than the taggers built using manually annotated corpora, the fact that they can be trained using only a dictionary listing the allowable parts of speech for each word and not needing a manually tagged corpus is a huge advantage in many situations." ></td>
	<td class="line x" title="15:188	Although a number of manually tagged corpora are available (eg." ></td>
	<td class="line x" title="16:188	\[Francis and Kucera, 1982; Marcus et al. , 1993\]), training on a corpus of one type and then applying the tagger to a corpus of a different type usually results in a tagger with low accuracy \[Weischedel et al. , 1993\]." ></td>
	<td class="line x" title="17:188	Therefore, if tagged text is needed in training, this would require manually tagging 1This work was funded in part by NSF grant IRI-9502312." ></td>
	<td class="line x" title="18:188	2Some other approaches to tagging are described in \[Hindle, 1989; Black et al. , 1992\]." ></td>
	<td class="line x" title="19:188	text each time the tagger is to be apphed to a new language, and even when being applied to a new type of text." ></td>
	<td class="line x" title="20:188	In \[Brill, 1992; Brill, 1994\], a rule-based part of speech tagger is described which achieves highly competitive performance compared to stochastic taggers, and captures the learned knowledge in a set of simple deterministic rules instead of a large table of statistics." ></td>
	<td class="line x" title="21:188	In addition, the learned rules can be converted into a deterministic finite state transducer." ></td>
	<td class="line x" title="22:188	Tagging with this finite state transducer requires n steps to tag a sequence of length n, independent of the number of rules, and results in a part of speech tagger ten times faster than the fastest stochastic tagger \[Roche and Schabes, 1995\]." ></td>
	<td class="line x" title="23:188	One weakness of this rulebased tagger is that no unsupervised training algorithm has been presented for learning rules automatically without a manually annotated corpus." ></td>
	<td class="line x" title="24:188	In this paper we present such an algorithm." ></td>
	<td class="line x" title="25:188	We describe an algorithm for both unsupervised and weakly supervised training of a rule-based part of speech tagger, and compare the performance of this algorithm to that of the Baum-Welch algorithm." ></td>
	<td class="line x" title="26:188	Transformation-Based Error-Driven Learning The rule-based tagger is based on a learning algorithm called transformation-based errordriven learning." ></td>
	<td class="line x" title="27:188	Transformation-based error-driven learning has been applied to a number of natural language problems, including part of speech tagging, prepositional phrase attachment disambiguation, speech generation and syntactic parsing \[Brill, 1992; Brill, 1994; Ramshaw and Marcus, 1994; Roche and Schabes, 1995; Brill and Resnik, 1994; Huang et al. , 1994; Brill, 1993a; Brill, 1993b\]." ></td>
	<td class="line x" title="28:188	Figure 1 illustrates the learning process." ></td>
	<td class="line x" title="29:188	First, unannotated text is passed through an initial-state annotator." ></td>
	<td class="line x" title="30:188	The initial-state annotator can range in complexity from assigning random structure to assigning the output of a sophisticated manually created annotator." ></td>
	<td class="line x" title="31:188	Once text has been passed through the initial-state annotator, it is then compared to the truth as specified in a manually annotated corpus, and transformations are learned that can be applied to the output of the initial state annotator to make it better resemble the truth." ></td>
	<td class="line x" title="32:188	In all of the applications explored to date, the following greedy search is applied: at each iteration of learning, the transformation is found whose application results in the highest score; that transformation is then added to the ordered transformation list and the training corpus is updated by applying the learned transformation." ></td>
	<td class="line x" title="33:188	To define a specific application of transformation-based learning, one must specify the following: 1." ></td>
	<td class="line x" title="34:188	The initial state annotator." ></td>
	<td class="line x" title="35:188	2." ></td>
	<td class="line x" title="36:188	The space of transformations the learner is allowed to examine." ></td>
	<td class="line x" title="37:188	3." ></td>
	<td class="line x" title="38:188	The scoring function for comparing the corpus to the truth and choosing a transformation." ></td>
	<td class="line x" title="39:188	Once an ordered list of transformations is learned, new text can be annotated by first applying the initial state annotator to it and then applying each of the learned transformations, in order." ></td>
	<td class="line x" title="40:188	2 UNANNOTATED TEXT STATE NER RULES Figure 1: Transformation-Based Error-Driven Learning." ></td>
	<td class="line x" title="41:188	Transformation-Based Part of Speech Tagging In transformation-based part of speech tagging, 3 all words are initially tagged with their most likely tag, as indicated in the training corpus." ></td>
	<td class="line x" title="42:188	Below are some of the transformation templates used by the learner." ></td>
	<td class="line x" title="43:188	4 Change tag a to tag b when: 1." ></td>
	<td class="line x" title="44:188	The preceding (following) word is tagged z. 2." ></td>
	<td class="line x" title="45:188	The preceding (following) word is w. 3." ></td>
	<td class="line x" title="46:188	The word two before (after) is w. 4." ></td>
	<td class="line x" title="47:188	One of the two preceding (following) words is tagged z. 5." ></td>
	<td class="line x" title="48:188	The current word is w and the preceding (following) word is x. 6." ></td>
	<td class="line x" title="49:188	The current word is w and the preceding (following) word is tagged z. The evaluation measure is simply tagging accuracy." ></td>
	<td class="line x" title="50:188	In each learning iteration, the system learns that transformation whose application results in the greatest reduction of error." ></td>
	<td class="line x" title="51:188	5 Because the learning algorithm is data-driven, it only needs to consider a small 3For a more detailed description of supervised transformation-based part of speech tagging, see \[Brill, 1994\]." ></td>
	<td class="line x" title="52:188	4In \[Brill, 1994\], a total of 21 templates are used." ></td>
	<td class="line x" title="53:188	5Note an important difference between Markov-mode\] based taggers and the transformation-based tagger: the former attempts to maximize the probability of a string, whereas the latter attempts to minimize the number of errors." ></td>
	<td class="line x" title="54:188	3 percentage of all possible transformations when searching for the best one." ></td>
	<td class="line x" title="55:188	An example of a learned transformation is: Change the tag of a word from VERB to NOUN if the previous word is a DETERMINER." ></td>
	<td class="line x" title="56:188	If the word race occurs more frequently as a verb than as a noun in the training corpus, the initial state annotator will mistag this word as a verb in the sentence: The race was very exciting." ></td>
	<td class="line x" title="57:188	The above transformation will correct this tagging error." ></td>
	<td class="line x" title="58:188	It was shown in \[Brill, 1994\] that the transformation-based tagger achieves a high rate of tagging accuracy." ></td>
	<td class="line x" title="59:188	The transformation-based tagger captures its learned information in a set of simple rules, compared to the many thousands of opaque probabilities learned by Markov-model based taggers." ></td>
	<td class="line x" title="60:188	6 Supervised training is feasible when one has access to a large manually tagged training corpus from the same domain as that to which the trained tagger will be applied." ></td>
	<td class="line x" title="61:188	We next explore unsupervised and weakly supervised training as a practical alternative when the necessary resources are not available for supervised training." ></td>
	<td class="line x" title="62:188	Unsupervised Learning of Transformations In supervised training, the corpus is used for scoring the outcome of applying transformations, in order to find the best transformation in each iteration of learning." ></td>
	<td class="line x" title="63:188	In order to derive an unsupervised version of the learner, an objective function must be found for training that does not need a manually tagged corpus." ></td>
	<td class="line x" title="64:188	We begin our exploration providing the training algorithm with a minimal amount of initial knowledge, namely knowing the allowable tags for each word, and nothing else." ></td>
	<td class="line x" title="65:188	7 The relative likelihoods of tags for words is not known, nor is any information about which tags are likely to appear in which contexts." ></td>
	<td class="line x" title="66:188	This would correspond to the knowledge that could be extracted from an on-line dictionary or through morphological and distributional analysis." ></td>
	<td class="line x" title="67:188	The unsupervised rule learning algorithm is based on the following simple idea." ></td>
	<td class="line x" title="68:188	Given the sentence: The can will be crushed." ></td>
	<td class="line x" title="69:188	with no information beyond the dictionary entry for the word can, the best we can do is randomly guess between the possible tags for can in this context." ></td>
	<td class="line x" title="70:188	However, using an unannotated corpus and a dictionary, it could be discovered that of the words that appear after The in the corpus that have only one possible tag listed in the dictionary, nouns are much more common than verbs or modals." ></td>
	<td class="line x" title="71:188	From this the following rule could be learned: Change the tag of a word from (modal OR noun OR verb) to noun if the previous word is The." ></td>
	<td class="line x" title="72:188	SThe transformation-based tagger is available through anonymous ftp to ftp.cs.jhu.edu in /pub/brill/Programs." ></td>
	<td class="line x" title="73:188	Tin this paper we ignore the problem of unknown words: words appearing in the test set which did not appear in the training set." ></td>
	<td class="line x" title="74:188	We plan to explore ways of processing unknown words in future work, either by initially assigning them all open-class tags, or devising an unsupervised version of the rule-based unknown." ></td>
	<td class="line x" title="75:188	word tagger described in \[Brill, 1994\]." ></td>
	<td class="line x" title="76:188	4 To fully define the learner, we must specify the three components of the learner: the initial state annotator, the set of transformation templates, and the scoring criterion." ></td>
	<td class="line x" title="77:188	Initial State Annotator The unsupervised learner begins with an unannotated text corpus, and a dictionary listing words and the allowable part of speech tags for each word." ></td>
	<td class="line x" title="78:188	The tags are not listed in any particular order." ></td>
	<td class="line x" title="79:188	The initial state annotator tags each word in the corpus with a list of all allowable tags." ></td>
	<td class="line x" title="80:188	Below is an example of the initial-state tagging of a sentence from the Penn Treebank \[Marcus et al. , 1993\], where an underscore is to be read as or." ></td>
	<td class="line x" title="81:188	8 Rival/JJ_NNP gangs/NNS have/VB_VBP turned/VBD_VBN cities/NNS into/IN combat/NN_VB zones/NNS ./." ></td>
	<td class="line x" title="82:188	Transformation Templates The learner currently has four transformation templates." ></td>
	<td class="line x" title="83:188	They are: Change the tag of a word from X to Y if: 1." ></td>
	<td class="line x" title="84:188	The previous tag is T. 2." ></td>
	<td class="line x" title="85:188	The previous word is W. 3." ></td>
	<td class="line x" title="86:188	The next tag is T. 4." ></td>
	<td class="line x" title="87:188	The next word is W. Transformations are used differently in the unsupervised learner than in the supervised learner." ></td>
	<td class="line x" title="88:188	Here, a transformation will reduce the uncertainty as to the correct tag of a word in a particular context, instead of changing one tag to another." ></td>
	<td class="line x" title="89:188	So all learned transformations will have the form: Change the tag of a word from X to Y in context C where X is a set of two or more part of speech tags, and Y is a single part of speech tag, such that Y E X. Below we list some transformations that were actually learned by the system." ></td>
	<td class="line x" title="90:188	Change the tag: From NN_VB_VBP to VBP if the previous tag is NNS From NN_VB to VB if the previous tag is MD From JJ_NNP to JJ if the following tag is NNS 8JJ= Adjective, MD = Modal, NNP = Singular Proper Noun, NN = Singular or Mass Noun, POS = Possessive, VB = Verb, Base Form, VBD = Verb, Past Tense, VBN = Verb, Past Part., VBP = Verb, Non-3rd Person Sing." ></td>
	<td class="line x" title="92:188	Present." ></td>
	<td class="line x" title="93:188	5 Scoring Criterion When using supervised transformation-based learning to train a part of speech tagger, the scoring function is just the tagging accuracy that results from applying a transformation." ></td>
	<td class="line x" title="94:188	With unsupervised learning, the learner does not have a gold standard training corpus with which accuracy can be measured." ></td>
	<td class="line x" title="95:188	Instead, we can try to use information from the distribution of unambiguous words to find reliable disambiguating contexts." ></td>
	<td class="line x" title="96:188	In each learning iteration, the score of a transformation is computed based on the current tagging of the training set." ></td>
	<td class="line x" title="97:188	Recall that this is completely unsupervised." ></td>
	<td class="line x" title="98:188	Initially, each word in the training set is tagged with all tags allowed for that word, as indicated in the dictionary." ></td>
	<td class="line x" title="99:188	In later learning iterations, the training set is transformed as a result of applying previously learned transformations." ></td>
	<td class="line x" title="100:188	To score the transformation: Change the tag of a word from X to Y in context C, where Y E X, we do the following." ></td>
	<td class="line x" title="101:188	For each tag Z E X, Z ~ Y, compute freq(Y)/ freq( Z)  incontext( Z, C) where freq(Y) is the number of occurrences of words unambiguously tagged with tag Y in the corpus, freq(Z) is the number of occurrences of words unambiguously tagged with tag Z in the corpus, and incontext(Z,C) is the number of times a word unambiguously tagged with tag Z occurs in context C in the training corpus." ></td>
	<td class="line x" title="102:188	9 Let n = argmaxz freq(Y)/freq(Z) * incontext(Z, C) Then the score for the transformation Change the tag of a word from X to Y in context Cis: incontext(Y, C) freq(Y)/ freq( R)  incontext( R, C) A good transformation for removing the part of speech ambiguity of a word is one for which one of the possible tags appears much more frequently as measured by unambiguously tagged words than all others in the context, after adjusting for the differences in relative frequency between the different tags." ></td>
	<td class="line x" title="103:188	The objective function for this transformation measures this by computing the difference between the number of unambiguous instances of tag Y in context C and the number of unambiguous instances of the most likely tag R in context C, where R E X, R ~ Y, adjusting for relative frequency." ></td>
	<td class="line x" title="104:188	In each learning iteration, the learner searches for the transformation which maximizes this function." ></td>
	<td class="line x" title="105:188	Learning stops when no positive scoring transformations can be found." ></td>
	<td class="line x" title="106:188	Unsupervised Learning: Results To test the effectiveness of the above unsupervised learning algorithm, we ran a number of experiments using two different corpora and part of speech tag sets: the Penn Treebank Wall Street Journal Corpus \[Marcus et al. , 1993\] and the original Brown Corpus \[Francis and Kucera, 1982\]." ></td>
	<td class="line x" title="107:188	First, a dictionary was created listing all possible tags for each word in the corpus." ></td>
	<td class="line x" title="108:188	This means that the test set contains no unknown words." ></td>
	<td class="line x" title="109:188	We have set up the experiments in this way to facilitate comparisons with results given in other papers, where the same was done." ></td>
	<td class="line x" title="110:188	9An example of a context is: the previous tag is a determiner." ></td>
	<td class="line x" title="111:188	8 B 8 i i,, 4oo 6oo 80o looo Transformagon Number Figure 2: Test Set Accuracy vs Transformation Number for the Penn Treebank Wall Street Journal Corpus Penn Treebank Results In this experiment, a training set of 120,000 words and a separate test set of 200,000 words were used." ></td>
	<td class="line x" title="112:188	We measure the accuracy of the tagger by comparing text tagged by the trained tagger to the gold standard manually annotated corpus." ></td>
	<td class="line x" title="113:188	In the case where the tag of a word is not fully disambiguated by the tagger, a single tag is randomly chosen from the possible tags, and this tag is then compared to the gold standard." ></td>
	<td class="line x" title="114:188	Initial state tagging accuracy on the training set is 90.7%." ></td>
	<td class="line x" title="115:188	After learning 1,151 transformations, training set accuracy increases to 95.0%." ></td>
	<td class="line x" title="116:188	Initial state tagging accuracy on the test set is also 90.7%." ></td>
	<td class="line x" title="117:188	Accuracy increases to 95.1% after applying the learned transformations." ></td>
	<td class="line x" title="118:188	Figure 2 shows test set tagging accuracy as a function of transformation number." ></td>
	<td class="line x" title="119:188	In figure 3, we plot the difference between training and test set accuracies after the apphcation of each transformation, including a smoothed curve." ></td>
	<td class="line x" title="120:188	1 Notice that there is no overtraining: the difference in accuracies on training and test set remain within a very narrow range throughout, with test set accuracy exceeding training set accuracy by a small margin." ></td>
	<td class="line x" title="121:188	Overtraining did not occur when using the original Brown Corpus either." ></td>
	<td class="line x" title="122:188	When training a stochastic tagger using the Baum-Welch algorithm, overtraining often does occur \[Meriaido, 1995; Elworthy, 1994\], requiring an additional held-out training corpus for determining an appropriate number of training iterations." ></td>
	<td class="line x" title="123:188	1The graphs are choppy because after each transformation is applied, correctness for words not yet fully disambiguated is judged after randomly selecting from the possible tags for that word." ></td>
	<td class="line x" title="124:188	8 o ~9 o .* % . ., . * ;  ',.2k    .   . %   g    *  ~ '." ></td>
	<td class="line x" title="126:188	   ~o ~  oO   ~     .oO $~  *~ ~,.~ ~,llk." ></td>
	<td class="line x" title="127:188	O . . .2   ~ *        % % e,~% .~     . ~' ~: .-o~,o '~ ~ -o.-%o.~." ></td>
	<td class="line x" title="128:188	- ~oo~~Ooo % P   , _ ' .--." ></td>
	<td class="line x" title="129:188	 0'." ></td>
	<td class="line x" title="130:188	: . . .:, ~'   o ~ i i i i 0 6o0 800 1000 Translorma~lon Number Figure 3: Difference Between Training and Test Set Accuracies." ></td>
	<td class="line x" title="131:188	Corpus Training Corpus Size (Words) Penn Treebank 120K Brown Corpus 120K Brown Corpus 350K % Correct 95.1 95.6 96.0 Table 1: Unsupervised Training: Test Set Accuracy Brown Corpus Results In this experiment, we also used a training set of 120,000 words and a separate test set of 200,000 words." ></td>
	<td class="line x" title="132:188	Initial state tagging accuracy on the training set is 89.8%." ></td>
	<td class="line x" title="133:188	After learning 1,729 transformations and applying them to the training set, accuracy increases to 95.6%." ></td>
	<td class="line x" title="134:188	Initial state tagging accuracy on the test set is 89.9%, with accuracy increasing to 95.6% after applying the learned transformations." ></td>
	<td class="line x" title="135:188	Expanding the training set to 350,000 words and testing on the same test set, accuracy increases to 96.0%." ></td>
	<td class="line x" title="136:188	All unsupervised learning results are summarized in table 1." ></td>
	<td class="line x" title="137:188	Comparison With Other Results In \[Merialdo, 1995\], tagging experiments are described training a tagger using the BaumWelch algorithm with a dictionary constructed as described above and an untagged corpus." ></td>
	<td class="line x" title="138:188	Experiments were run on Associated Press articles which were manually tagged at the University of Lancaster." ></td>
	<td class="line x" title="139:188	When training on one million words of text, test set accuracy 8 peaks at 86.6%." ></td>
	<td class="line x" title="140:188	In \[Elworthy, 1994\], similar experiments were run." ></td>
	<td class="line x" title="141:188	There, a peak accuracy of 92.0% was attained using the LOB corpus, n Using the Penn Treebank corpus, a peak accuracy of 83.6% resulted." ></td>
	<td class="line x" title="142:188	These results are significantly lower than the results achieved using unsupervised transformation-based learning." ></td>
	<td class="line x" title="143:188	In \[Kupiec, 1992\] a novel twist to the Baum-Welch algorithm is presented, where instead of having contextual probabilities for a tag following one or more previous tags, words are pooled into equivalence classes, where all words in an equivalence class have the same set of allowable part of speech assignments." ></td>
	<td class="line x" title="144:188	Using these equivalence classes greatly reduces the number of parameters that need to be estimated." ></td>
	<td class="line x" title="145:188	Kupiec ran experiments using the original Brown Corpus." ></td>
	<td class="line x" title="146:188	When training on 440,000 words, test set accuracy was 95.7%, excluding punctuation." ></td>
	<td class="line x" title="147:188	As shown above, test set accuracy using the transformation-based algorithm described in this paper gives an accuracy of 96.0% when trained on 350,000 words." ></td>
	<td class="line x" title="148:188	Excluding punctuation, this accuracy is 95.6%." ></td>
	<td class="line x" title="149:188	Note that since the Baum-Welch algorithm frequently overtrains, a tagged text would be necessary to figure out what training iteration gives peak performance." ></td>
	<td class="line x" title="150:188	Weakly Supervised Rule Learning We have explored a method of training a transformation-based tagger when no information is known other than a list of possible tags for each word." ></td>
	<td class="line x" title="151:188	Next we explore weakly supervised learning, where a small amount of human intervention is permitted." ></td>
	<td class="line x" title="152:188	With Markov-model based taggers, there have been two different methods proposed for adding knowledge to a tagger trained using the Baum-Welch algorithm." ></td>
	<td class="line x" title="153:188	One method is to manually alter the tagging model, based on human error analysis." ></td>
	<td class="line oc" title="154:188	This method is employed in \[Kupiec, 1992; Cutting et al. , 1992\]." ></td>
	<td class="line x" title="155:188	Another approach is to obtain the initial probabilities for the model directly from a manually tagged corpus instead of using random or evenly distributed initial probabilities, and then adjust these probabilities using the Baum-Welch algorithm and an untagged corpus." ></td>
	<td class="line x" title="156:188	This approach is described in \[Merialdo, 1995; Elworthy, 1994\]." ></td>
	<td class="line x" title="157:188	A tagged corpus can also be used to improve the accuracy of unsupervised transformationbased learning." ></td>
	<td class="line x" title="158:188	A transformation-based system is a processor and not a classifier." ></td>
	<td class="line x" title="159:188	Being a processor, it can be applied to the output of any initial state annotator." ></td>
	<td class="line x" title="160:188	As mentioned above, in the supervised transformation-based tagger described in \[Brill, 1994\], each word is initially tagged with its most likely tag." ></td>
	<td class="line x" title="161:188	Here, we use the trained unsupervised part of speech tagger as the initial state annotator for a supervised learner." ></td>
	<td class="line x" title="162:188	Transformations will then be learned to fix errors made by the unsupervised learner." ></td>
	<td class="line x" title="163:188	As shown in figure 4, unannotated text is first passed through the unsupervised initial-state annotator, where each word is assigned a list of all allowable tags." ></td>
	<td class="line x" title="164:188	The output of this tagger is then passed to the unsupervised learner, which learns an ordered list of transformations." ></td>
	<td class="line x" title="165:188	The initialstate annotator and learned unsupervised transformations are then applied to unannotated text, which is then input to the supervised learner, along with the corresponding manually tagged corpus." ></td>
	<td class="line x" title="166:188	The supervised learner learns a second ordered list of transformations." ></td>
	<td class="line x" title="167:188	Once the system is trained, fresh text is tagged by first passing it through the unsupervised initial state annotator, then applying each of the unsupervised transformations, in order, and then applying each of the supervised transformations, in order." ></td>
	<td class="line x" title="168:188	The advantage of combining unsupervised and supervised learning over using supervised n\[Elworthy, 1994\] quotes accuracy on ambiguous words, which we have converted to overall accuracy." ></td>
	<td class="line x" title="169:188	I UNTAGGED \] TEXT INITL~L-STATE \[UNSUPERVISED ANNOTATOR: I / / TRANSFORMATIONS LEARNER % \[ SUPERVISED TEXT Figure 4: Combining Unsupervised and Supervised Learning learning alone is that the combined approach allows us to utifize both tagged and untagged text in training." ></td>
	<td class="line x" title="170:188	Since manually tagged text is costly and time-consuming to generate, it is often the case that when there is a corpus of manually tagged text available there will also be a much larger amount of untagged text available, a resource not utilized by purely supervised training algorithms." ></td>
	<td class="line x" title="171:188	One significant difference between this approach and that taken in using the BaumWelch algorithm is that here the supervision influences the learner after unsupervised training, whereas when using tagged text to bias the initial probabilities for Baum-Welch training, supervision influences the learner prior to unsupervised training." ></td>
	<td class="line x" title="172:188	The latter approach has the potential weakness of unsupervised training erasing what was learned from the manually annotated corpus." ></td>
	<td class="line x" title="173:188	For example, in \[Merialdo, 1995\], extracting probability estimates from a 50,000 word manually tagged corpus gave a test set accuracy of 95.4%." ></td>
	<td class="line x" title="174:188	After applying ten iterations of the Baum-Welch algorithm, accuracy dropped to 94.4%." ></td>
	<td class="line x" title="175:188	Using the transformations learned in the above unsupervised training experiment run on the Penn Treebank, we apply these transformations to a separate training corpus." ></td>
	<td class="line x" title="176:188	New supervised transformations are then learned by comparing the tagged corpus that results from applying these transformations with the correct tagging, as indicated in the manually annotated training corpus." ></td>
	<td class="line x" title="177:188	In table 2, we show tagging accuracy on a separate test set using different sizes of manually annotated corpora." ></td>
	<td class="line x" title="178:188	In each case, a 120,000 word untagged corpus was used for initial unsupervised training." ></td>
	<td class="line x" title="179:188	This table also gives results from supervised training using the annotated corpus, without any prior unsupervised training." ></td>
	<td class="line x" title="180:188	12 In all cases, the combined training outperformed the purely supervised training at no added cost in terms of annotated 12The purely supervised learning algorithm is the same as that described in \[Brill, 1994\], except there the most likely tag for every word in the dictionary is provided to the learner." ></td>
	<td class="line x" title="181:188	10 % Correct % Correct Supervised Training Using Unsupervised Not Using Unsup." ></td>
	<td class="line x" title="182:188	Corpus Size (Words) Transformations Transformations 0 95.1 90.8 400 95.4 91.8 1200 95.5 92.9 4000 95.7 93.9 7600 95.8 94.6 10300 96.0 95.1 22300 96.3 95.5 44400 96.6 96.1 61400 96.7 96.3 88200 96.8 96.5 Table 2: Unsupervised + Supervised vs. Purely Supervised Training." ></td>
	<td class="line x" title="183:188	training text." ></td>
	<td class="line x" title="184:188	Conclusions In this paper, we have presented a new algorithm for unsupervised training of a rule-based part of speech tagger." ></td>
	<td class="line x" title="185:188	The rule-based tagger trained using this algorithm significantly outperforms the traditional method of applying the Baum-Welch algorithm for unsupervised training of a stochastic tagger, and achieves comparable performance to a class-based BaumWelch training algorithm." ></td>
	<td class="line x" title="186:188	In addition, we have shown that by combining unsupervised and supervised learning, we can obtain a tagger that significantly outperforms a tagger trained using purely supervised learning." ></td>
	<td class="line x" title="187:188	We are encouraged by these results, and expect an improvement in performance when the number of transformation templates provided to the unsupervised learner increases beyond the four currently used." ></td>
	<td class="line x" title="188:188	We have also demonstrated that overtraining, a problem in Baum-Welch training, is not a problem in transformationbased learning." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C96-1036
N-Th Order Ergodic Multigram HMM For Modeling Of Languages Without Marked Word Boundaries
Law, Hubert Hin-Cheung;Chan, Chorkin;"></td>
	<td class="line x" title="1:121	N-th Order Ergodie Multigram HMM for Modeling of Languages without Marked Word Boundaries Hubert Hin-Cheung LAW Dept. of Computer Science The University of IIong Kong hhcJ_awc s. hku." ></td>
	<td class="line x" title="2:121	hk Chorkin CHAN Dept. of Computer Science The University of Itong Kong cchanOcs, hku." ></td>
	<td class="line x" title="3:121	hk Abstract I,;rgodie IIMMs have been successfully used for modeling sentence production." ></td>
	<td class="line x" title="4:121	llowever for some oriental languages such as Chinese, a word can consist of multiple characters without word boundary markers between adjacent words in a sentence." ></td>
	<td class="line x" title="5:121	This makes wordsegmentation on the training and testing data necessary before ergodic ItMM can be applied as the langnage model." ></td>
	<td class="line x" title="6:121	This paper introduces the N-th order Ergodic Mnltigram HMM for language modeling of such languages." ></td>
	<td class="line x" title="7:121	Each state of the IIMM can generate a variable number of characters corresponding to one word." ></td>
	<td class="line x" title="8:121	The model can be trained without wordsegmented and tagged corpus, and both segmentation and tagging are trained in one single model." ></td>
	<td class="line x" title="9:121	Results on its applicw Lion on a Chinese corpus are reported." ></td>
	<td class="line x" title="10:121	1 Motivation Statistical language modeling offers advantages including minimal domain specific knowledge and hand-written rules, trainability and scalability given a language corpus." ></td>
	<td class="line oc" title="11:121	Language models, such as N-gram class models (Brown et al. , 1992) and Ergodic Hidden Markov Models (Kuhn el, al. , 1994) were proposed and used in applications such as syntactic class (POS) tagging for English (Cutting et al. , 1992), clustering and scoring of recognizer sentence hypotheses." ></td>
	<td class="line x" title="12:121	IIowever, in Chinese and many other oriental languages, there are no boundary markers, such as space, between words." ></td>
	<td class="line x" title="13:121	Therefore preprocessors have to be used to perform word segmentation in order to identify individual words before applying these word-based language models." ></td>
	<td class="line x" title="14:121	As a result current approaches to modeling these languages are separated into two seperated processes." ></td>
	<td class="line x" title="15:121	Word segmentation is by no means a trivial process, since ambiguity often exists." ></td>
	<td class="line x" title="16:121	Pot proper segmentation of a sentence, some linguistic information of the sentence should be used." ></td>
	<td class="line x" title="17:121	iIowever, commonly used heuristics or statistical based approaches, such as maximal matching, fl'equency counts or mutual information statistics, have to perform the segmentation without knowledge such as the resulting word categories." ></td>
	<td class="line x" title="18:121	To reduce the impact of erroneous segmentation on the subsequent language model, (Chang and Chan, 1993) used an N-best segmentation interface between them." ></td>
	<td class="line x" title="19:121	llowever, since this is still a two stage model, the parameters of the whole model cannot be optimized together, and an Nbest interface is inadequate for processing outputs from recognizers which can be highly ambiguous." ></td>
	<td class="line x" title="20:121	A better approach :is to keep all possible segmentations in a lattice form, score the lattice with a language model, and finally retrieve the best candidate by dynamic programming or some searching algorithms." ></td>
	<td class="line x" title="21:121	N-gram models arc usually used for scoring (Gu et al. , 1991) (Nagata, 1994), but their training requires the sentences of the corpus to be manuMly segmented, and even class-tagged if class-based N-gram is used, as in (Nagata, 1994)." ></td>
	<td class="line x" title="22:121	A language model which considers segmentation ambiguities and integrates this with a Ngram model, and able to be trained and tested on a raw, unsegmented and untagged corpus, is highly desirable for processing languages without marked word boundaries." ></td>
	<td class="line x" title="23:121	2 The Ergodie Multigram HMM Model 2.1 Overview Based on the Hidden Markov Model, the Ergodic Multigram llidden Markov Model (l,aw and Chan, 1996), when applied as a language model, can process directly on unsegmented input corpus 204 as it allows a variable mmfl)er of characters in each word class." ></td>
	<td class="line x" title="24:121	Other than that its prol)erties are sin> liar to l';rgodic tlidden Markov Models (Kuhn ct al. , 1994), that both training and scoring can be done directly on a raw, unCagged corpus, given a lexicon with word classes." ></td>
	<td class="line x" title="25:121	Specifically, the N-Oh order F, rgodic Multigram It M M, as in conventional class-based (N+I)-gram model, assumes a (loubly stochastic process in sentence production." ></td>
	<td class="line x" title="26:121	The word-class sequence in a scalene(: follows Che N-Oh order Markov assulnl> tion, i.e. tile identity of a (:lass in the s('.lite\[Ic(~ delmn(Is only on tim previous N classes, and the word observed depelads only on the class it l)elongs to." ></td>
	<td class="line x" title="27:121	The difference is thai, this is a multigram model (Doligne and Bimbot, 1995) in the sense Chat each state (i.e. node in the IIMM) (:a,t genera.re a wu-iable number of ot)served character sequences." ></td>
	<td class="line x" title="28:121	Sentence boundaries are inodelcd as a sl)ecial class." ></td>
	<td class="line x" title="29:121	This model can be apl/lied to a.ll input sent(race or a characCer lattice as a language model." ></td>
	<td class="line x" title="30:121	'Fhe maxinnun likelihood scat(: sequence through l,he model, obtaine(t using the ViCerl)i or Stack I)(> coding AlgoriChln, ret)resenCs the 1)est particular segmentation and class-tagging for the input sentence or lattice, since transition of states denotes a wor(t boundary and state identity denotes tile ClU'rent word class." ></td>
	<td class="line x" title="31:121	2.2 Le.xi('on A lexicon (CK\] P, 1993) of 78,322 words, each con~ tainiug up to 10 characters, is awdlabh~ for use ill this work." ></td>
	<td class="line x" title="32:121	l'ractically all characters have an cnCl:y ill the lexicon, so Chat out-of-vocalmlary words are modeled as indivi(hlal eharacters." ></td>
	<td class="line x" title="33:121	There is a total of 192 syntactic classes, arranged in a hierarchical way." ></td>
	<td class="line x" title="34:121	For example, the month names arc deuoted by the class Ndabc, where lg denotes Nouu, Nd denotes 'lbmporal Nouns, Igda \['or 'l'im(~ lmmes and Ndab for reusabh' tilne names." ></td>
	<td class="line x" title="35:121	'\['here~ is a total of 8 major categories." ></td>
	<td class="line x" title="36:121	Each word ill the dictionary is aullol,al.cd with one or nlore syntactic tags, tel)resenting dilferent syntactic classes Che word cnn possibly belong to." ></td>
	<td class="line x" title="37:121	Also, a frequ(mcy count tbr each word, base(l on a certain corpus, is given, bill without inforniation on its distribution over different syntactic classes." ></td>
	<td class="line x" title="38:121	2.3 T(:rminoh)gy I,el, )42 be the set of all Chinese words in l, hc lexicon." ></td>
	<td class="line x" title="39:121	A word 'wk C W is made up of one or more characters, l,et,s~ r = (.~;I,.';'21';T) denote, a sentence as a T-character sequence." ></td>
	<td class="line x" title="40:121	A funcCion (5~,, is defined such Chat (Sw (~Vk, sit +rI ) is \] if w,." ></td>
	<td class="line x" title="41:121	is a r-character word st  st+,,-1, and 0 otherwise." ></td>
	<td class="line x" title="42:121	1 1,et /2 be the Ul)per bound of r, i.e. t,11o maxinntm uumber of characters ill a word (10 ill this paper)." ></td>
	<td class="line x" title="43:121	I,et (2/ = {clcL} be the set, of syntactic classes, where L is the nmnber of syntactic (:lasses in the lexicon (192 in our case)." ></td>
	<td class="line x" title="44:121	Lot t?" ></td>
	<td class="line x" title="45:121	C W  (/ denote Che relaCion for all syntactic classitications of the." ></td>
	<td class="line x" title="46:121	lexicon, such ChaC ('tot:, el) @ C ill' cl is one of the syntactic classes tbr 'wk." ></td>
	<td class="line x" title="47:121	Each word wk llltlSt belong to one or more of the classes." ></td>
	<td class="line x" title="48:121	A path Chrough the model represents a particular segnmnCation and (:lass Lagging for the Sell-I,(~IIC(' I,et 7 = ('wt, (:It ;  . . 'Wig, Cl K ) t)e a particular segmentation and (;lass tagging for the sentence s~', where Wk is the kth word and elk dCllOtCS tllc (;lass assigned to w,:, as illustrated below." ></td>
	<td class="line x" title="49:121	11) I ~cl 1 ltJk,elk ~I)K ~Cll,( (.Sl   .Stl-I .  .,Stk_ 1 . . .Stk-1   .8IK_ l  . .S'I') l'(,r C Co be proper, I1' 2, .,~_, ) 1 aml (wk,cl~) C l' must be saCistied, where t0 = 1, tic = 7'+ 1 and tk-j < l,, for 1 < k < K. 2.4 ItMM S|:a|;es for l;.he N-th order model In Che tirst order IIMM (class 1)it(am) lnodel, each I1MM state corresl)onds directly to the word-class of a word." ></td>
	<td class="line x" title="50:121	lhlt in general, for an N-Oh order IIMM model, siuce each class depends on N previous classes, each state has to rel)lJesellt C\]I(t COlil\])illat, ion of the classes of the most recelfl; N words, iuctlading the current, word." ></td>
	<td class="line x" title="51:121	I,et Qi represent a stal,(~ of the N-th order Ergo(lit Multigraul IIMM." ></td>
	<td class="line x" title="52:121	Thus Qi = ((%ci~_,) where tie iS the current word (:lass, ci, is the previ()us word class, etc. '\['here is a CeCal of L N states, which may nleall too many l)aranl('ters (l/v+l possible state transitions, each state can transit to L other states) for the model if N is anything greater th an ont. '1'o solve this l)rol)lem, a reasonal)le aSSllllllilion can })c luade that the d('taih'xl (;lass idea titles of a mor(~ (listanl, word have, in general, less influence than the closer ones Co the current word class." ></td>
	<td class="line x" title="53:121	Thus instead of using C as tim classitication relation for all l)revious words, a set of I~I'he ;algorithm to bc described ;tSSUlnCs tlt~Lt, th(,." ></td>
	<td class="line x" title="54:121	(:ha.r;tctcr identities arc known for the S(!lltCttC(~ 8; ?, })It(, it can *also be al)plicd when ca.ch charttctcr position sL becomes a. set of possible (:h~u'a(:ter (:~Lndida.t, es by simply letting &,,(wk,sl +''-I) -i for all words wk which can be constructed from the c\]mr~t(:ter positions stst+, 1 of the input c\]mractcr lattice." ></td>
	<td class="line x" title="55:121	This enal)les the mo(M to 1)e used as the languzLgc model component for r(!(:ognizcrs and for decoding phoncti(: input." ></td>
	<td class="line x" title="56:121	205 classification relations {C(), C(1),C (N-l) } can be used, where C() = C represents the original, most detailed classification relation for the current word, and C (n) is the less detailed classification scheme for the nth previous word at each state." ></td>
	<td class="line x" title="57:121	Thus the number of states reduces to LQ ---L()L (1) L (N-l) in which L('0 _ < L. Each state is represented as Qi = (c~o)elN-_~ O) where C (n) = {cln)}, 1 < I < L (n) is the class tag set for the nth previous word." ></td>
	<td class="line x" title="58:121	However, if no constraints are imposed on the series of classification relations C Oo, the number of possible transitions may increase despite a decrease in the number of states, since state transitions may become possible between every two state, resulting in a total of L()2L (02  L (N1)2 possible transitions." ></td>
	<td class="line x" title="59:121	A constraint is imposed that, given that a word belongs to the class cl n) in the classification C (n), we can determine the corresponding word class c}, ~+0 the given word will belong to in C(~+1), and for every word there is no extra classifications in C (n+l) not corresponding to one in C (n)." ></td>
	<td class="line x" title="60:121	Formally, there exist mapping functions 5 c('0 : COO ~ C('+0, 0 _< n _< N-2, such that if C(n) ~(n+l)\] ~ .~'(n) then ((wk, cl n)) 6 C (n)) => I ' '~1 ~ ), (n+l), C(n+l)) (wk,c v ) 6 for all wk 6 W, and that y(n) is surjective." ></td>
	<td class="line x" title="61:121	In particular, to model sentence boundaries, we allow $ to be a valid class tag for all C(n), and define 5e('~)($) = 2." ></td>
	<td class="line x" title="62:121	The above constraint ensures that given a state Q, :,(c!),o . cl :, 1)) it can only transit to Qi = (c5~),br()(c~))''' J-(N-2)(c~N--~u))) where c~ ) is any state in C ()." ></td>
	<td class="line x" title="63:121	Thus reducing to the maximum number of possible transitions to L()2L0)  L(N1)." ></td>
	<td class="line x" title="64:121	This constraint is easily satisfied by using a hierarchical word-class scheme, such as the one in the CKIP lexicon or one generated by hierarchical word-clustering, so that the classification for more distant words (higher n in C (n)) uses a higher level, less detail tag set in the scheme." ></td>
	<td class="line x" title="65:121	2.5 Sentence Likelihood Formulation Let {} be the set of all possible segmentations and class taggings of a sentence." ></td>
	<td class="line x" title="66:121	Under the Nth order model (.)N, the likelihood of each valid segmentation and tagging 12 of the sentence s T, /~(8T, ~\[oN), can be derived as follows." ></td>
	<td class="line x" title="67:121	P(w,, c** ; w=, c~= ; Wg, e~,,." ></td>
	<td class="line x" title="68:121	IO N) = P(W 1 \]Cll )P(cl 1 I$N)P($MK el.~_,,,+, )  K (\[Ik:= P(W~\]Clk )P(clk IC~*-1 ' ' ' elk_N)) = P(w~lc,,)P(O,,lSN)p($lO,K)  K (\[Ik=u P(w~lclk)P(Ql~ IQ,k-~)) using Nth order Markov assumption and representing the class history as HMM states." ></td>
	<td class="line x" title="69:121	$ denotes the sentence boundary, elk is $ for k _< 0, and Q~k re() c!" ></td>
	<td class="line x" title="70:121	N-l) \] Note that Qlk can be deI lk * ' ' ~k--N+l '' termined from clk and Qlk-~ due to the constraint on the classification, and thus P(Qzk\]Qlk_~) = P(ct~ IQl~-~)." ></td>
	<td class="line x" title="71:121	The likelihood of the sentence s T under the model is given by the sum of the likelihoods of its possible segmentations." ></td>
	<td class="line x" title="72:121	v(s lo ) = v(sLno 3 The Algorithms 3.1 The Parameters As in conventional HMM, the Ergodic Multigram HMM consists of parameters E) N ~-{A, B}, in which A = {aij\], 0 < i,j <_ LQ (Total number of states), denotes the set of state transition probabilities from Qi to Qi, i.e. P(Q31Qi)." ></td>
	<td class="line x" title="73:121	In particular, a0i = P(Qi\[$ N) and ai0 = P($\]Qi) denote the probabilities that the state Qi is the initial and final state in traversing the HMM, respectively, a00 is left undefined." ></td>
	<td class="line x" title="74:121	H = {bj(w~)\], where 1 < j < L Q, denotes the set of word observation probabilities of wk at the state Qj, i.e. P(wk\]Qj)." ></td>
	<td class="line x" title="75:121	The B matrix, as shown above, models the probabilities that wk is observed given N most recent classes, and contains LQ\[W\] parameters (recall that LQ = L()L(1) L(N-1))." ></td>
	<td class="line x" title="76:121	Our ~assumption that wk only depends on the current class reduces the number of parameters to L()\]W\[ for the /3 matrix." ></td>
	<td class="line x" title="77:121	Thus in the model, bj(wk) representing P(Wk\[Qj) are tied together for all states Qj with the same current word-class, i.e. P(wklOj) = P(welc,) if 03 = (c,)." ></td>
	<td class="line x" title="78:121	Also, aij is 0 if Qi cannot transit to Qj." ></td>
	<td class="line x" title="79:121	As a resul~ the number of parameters in the A matrix is only L()LQ." ></td>
	<td class="line x" title="80:121	Given the segmentation and class sequence  of a sentence, the state sequence (Qz~  QI~) can be derived from the class sequence (ehci~.)." ></td>
	<td class="line x" title="81:121	Thus the observation probability of the sentence  P~d' /ON), can s~ ~ given  and the model O N, ~ 1, be reformulated as b ll (wl)ao l I( 206 Given this tbrmulation the training procedure is mostly similar to that of the first order Ergodic Mnltigram HMM." ></td>
	<td class="line x" title="82:121	3.2 Forward and Backward Procedure The forward variable is defined as O't(i) = P(S1.-." ></td>
	<td class="line x" title="83:121	St, QI(t)' Qi\[ ~)N) where Q~(t) is the state of the \[IMM when the word containing the character st as the last character is produced." ></td>
	<td class="line x" title="84:121	The recursive equations for c~t(i) are ~t(j) = ~t(j) = 0brt< 1 w~ .1~ LQ ~ \[~c~t-,'(i)aljbj(w~)l ~w (Wk, stt-r+l ) \['or l <t <7' Similarly, the backward variable is defin('d as lit(i) 7\['(St-b1st Iq+(,) = Qi, O N) 'l'he recursive equations for fit(i) are fit(i) -9 (i) = fit(i) 0 for t > T aio It LQ r=l wkEla2 j==l ~~o (wk, t+,.~ St+l) for I <t <T--1 As A, H arrays and the 5~, fimction are mostly 0s, considerable simplification can be done in irnph'.mentation." ></td>
	<td class="line x" title="85:121	The likelihood of the sentence given the model can be evaluated as LQ P(s'('lO N) = ~f_~.,r(i)aio i=1 The Viterbi algo,'ithm \[br this model can be ob tained by replacing the summations of the forward algorithm with maximizations." ></td>
	<td class="line x" title="87:121	3.3 Re-estimation Algorithm &(i, j) is detined as the probability that given a sentence .s~' and the model (_)N, a word ends at the character st in the state Qi an(l tile next word starts at the character st+l in the state Qj." ></td>
	<td class="line x" title="88:121	Thus ~t(i, j) can be expressed as R s,+, (j) r=l wkCW P(sY'leN) \['or l < t < fl'-I 1 < i,j < LQ." ></td>
	<td class="line x" title="89:121	turthermore dellne %(/) to be the probahility that, given Sl r and O N, a word ends at the character st in the state Qi." ></td>
	<td class="line x" title="90:121	Thus ctt(i)/3,(i) for 1 <t <7',1 <i< LQ." ></td>
	<td class="line x" title="91:121	7,(i)p(sy.lN) Sulnlnation of (t (i, j) ()vet' t gives tile expected number of times that state Qi transits to slate Qj in the sentence, aml stunmation of 7t(i) over t gives the expected number of state Qi occurring in it." ></td>
	<td class="line x" title="92:121	Thtts the quotient of their summation over t gives aij, the new estimation for aij." ></td>
	<td class="line x" title="93:121	'1'1 (l' aij -~_\[, ~'t(,,Y)/~_~ 7,(i) for 1 _< i,j .::( LQ t=l tin1 The initial and fi,,a\[ class probability estimates, a0j and ai0 can be re-estimated as follows." ></td>
	<td class="line x" title="94:121	It r=l wkE'VV = t (si'leN) P aio -c~.r(i)aio /~'Tt(i) To derive bj (w~:), first define ctt ~ (i) as the probability of the sentence prefix (sl   . st) with 'wa, in state Qi as the last coml)lete word." ></td>
	<td class="line x" title="95:121	Thus It 1,~ r=l i=l ( (): t-; ( i)aij bj ( w k )~w ('u)k, S tt--r + l )) This represents the contribution of wk, occurring as the last word in sl, to,~,(j)." ></td>
	<td class="line x" title="96:121	Also define 7't ~ (j) to be the I)robability that, given the sente.nce,s'~' and the model, we is observed to end at character st in the state Qj." ></td>
	<td class="line x" title="97:121	(,~\[~(j)fJt(j) 7~'~(J) p(8~'lO N) Let Qj o Qj, denot(;s the relation that both Qj and Qj, represent the s~me current word class." ></td>
	<td class="line x" title="98:121	Thus summation of 71~k(j) ow:r t gives the e.xpetted munber of times that wk is observed in 207 state Qj, and summation of 7t(J) over t gives the total expected number of occurrence of state Qj." ></td>
	<td class="line x" title="99:121	Since states with the same current word class are tied together by our assumption, the required value of bj(wk) is given by E J' E~I,./~ok (j,) -Dj (Wk ) = Q.ioQj, E ; ET1 7t(J') QjoQj, 4 Experimental Results 4.1 Setup A corpus of daily newspaper articles is divided into training and testing sets for the experiments, which is 21M and 4M in size respectively." ></td>
	<td class="line x" title="100:121	Th(' first order (N=I) algorithms are applied to the training sets, and parameters obtained after different iterations are used for testing." ></td>
	<td class="line x" title="101:121	The initial parameters of the HMM are set based on the frequency counts from the lexicon." ></td>
	<td class="line x" title="102:121	The class-transition probability aij is initialized as the a priori probability of the state P(Qj), estimated fl'om the relative frequency counts of the lexicon, bj(wk) is initialized as the relative count of the word wk within the class corresponding to the current word class in Qj." ></td>
	<td class="line x" title="103:121	Words belonging to multiple classes have their counts distributed equally among them." ></td>
	<td class="line x" title="104:121	Smoothing is then applied by adding each word count by 0.5 and normalizing." ></td>
	<td class="line x" title="105:121	After training, the Viterbi algorithm is used to retrieve the best segmentation and tagging * of each sentence of the test corpus, by tracing the best state sequence traversed." ></td>
	<td class="line x" title="106:121	4.2 Perplexity The test-set perplexity, calculated as m'= exp(M \]-log(J'(Z', i where the summation is taken over all sentences s~ '~ in the testing corpus, and M represents the number of characters in it, is used to measure the performance of the model." ></td>
	<td class="line x" title="107:121	The results for models trained on training corpus subsets of various sizes, and after various iterations are shown (Table 1)." ></td>
	<td class="line x" title="108:121	It is obvious that with small training corpus, over-training occurs with more iterations." ></td>
	<td class="line x" title="109:121	With more training data, the performance improves and over-training is not evident." ></td>
	<td class="line x" title="110:121	4.3 Phonetic Input Decoding A further experiment is performed to use the models to decode phonetic inputs (Gu et el., 1991)." ></td>
	<td class="line x" title="112:121	'Daining Size 2 d 6 8 98K 194.009 214.096 246.613 286.721 1.3M 126.084 122.304 121.606 121.776 6.3M 118.531 113.600 111.745 110.783 21M 116.376 11.1.275 109.282 108.1/12 Table 1: Test Set Perplexities of testing set after different iterations on subsets of training set This is not trivial since each Chinese syllable can correspond to up to 80 different characters." ></td>
	<td class="line x" title="113:121	Sentences from the testing corpus are first expanded into a lattice, formed by generating all the common homophones of each Chinese character." ></td>
	<td class="line x" title="114:121	Tested on 360K characters, a character recognition rate of 91.24:% is obtained for the model trained after 8 iterations with 21M of training text." ></td>
	<td class="line x" title="115:121	The results are satisfactory given that the test corpus contains many personal names and ()tit of vocabulary words, and the highly ambiguous nature of (;he problem." ></td>
	<td class="line x" title="116:121	5 Discussion and Conclusion In this paper the N-th order Ergodic Multigram IIMM is introduced, whose application enables integrated, iterative language model training on nntagged and unsegmented corpus in languages such as Chinese." ></td>
	<td class="line x" title="117:121	The pertbrmanee on higher order models are expected to be better as the size of training corpus is relatively large." ></td>
	<td class="line x" title="118:121	Itowever some form of smoothing may have to be applied when the training corpus size is small." ></td>
	<td class="line x" title="119:121	With some moditication this algorithm would work on phoneme candidate input instead of character candidate input." ></td>
	<td class="line x" title="120:121	This is useful in decoding phonetic strings without character boundaries, such as in continuous Chinese~Japanese~Korean phonetic inpnt, or speech recognizers which output phonemes." ></td>
	<td class="line x" title="121:121	This model also makes a wealth of techniqnes developed for HMM in the speech recognition field available for language modeling in these languages." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C96-2114
Linguistic Indeterminacy As A Source Of Errors In Tagging
Kallgren, Gunnel;"></td>
	<td class="line x" title="1:184	Linguistic Indeterminacy as a Source of Errors in Tagging Gunnel Kiiilgren Department of Linguistics Stockholm University S106 91 Stockholm Sweden gunnel@ling.su.se Abstract Most evaluations of part-of-speech tagging compare the utput of an automatic tagger to some established standard, define the differences as tagging errors and try to remedy them by, e.g., more training of the tagger." ></td>
	<td class="line x" title="2:184	The present article is based on a manual analysis of a large number of tagging errors." ></td>
	<td class="line x" title="3:184	Some clear patterns among the errors can be discerned, and the sources of the errors as well as possible alternative methods of remedy are presented and discussed." ></td>
	<td class="line x" title="4:184	In particular are the problems with undecidable cases treated." ></td>
	<td class="line x" title="5:184	1 Background When the performance of automatic part-of-speech taggers is discussed, it is normally measured relative to some standard material, such as the Brown Corpus, or to a manual tagging or a manual proof-reading of (some smaller part of) the tagged material." ></td>
	<td class="line x" title="6:184	The performance of the automatic tagger is calculated as the difference between the standard material and the output of the tagger to be evaluated, with all differences regarded as errors by the tagger." ></td>
	<td class="line x" title="7:184	In a study carried out on material from a large Swedish corpus, K~illgren (1996) made a careful inspection of all instances where a manual and an automatic tagging differed in a material of 50,000 words of balanced text." ></td>
	<td class="line x" title="8:184	The differences were classified as 'man errors', 'machine errors' or errors common to both." ></td>
	<td class="line x" title="9:184	The errors were furthermore classified according to type, and some clear patterns could be seen." ></td>
	<td class="line x" title="10:184	The present article picks up some of the findings and looks closer at a kind of error which K~ltgren calls 'mirror image errors', where two readings of a word are constantly mixed up with each other in both directions." ></td>
	<td class="line x" title="11:184	Errors of this kind have been noted by others as well, and solutions to the problems they cause have been suggested." ></td>
	<td class="line x" title="12:184	Some such suggestions and the possible outcome of their application to the Swedish material will be discussed in the following." ></td>
	<td class="line x" title="13:184	2 The Linguistic Material Used in the Study The error analysis on which this study is based was carried out on material from the Stockholm-Ume~ Corpus of modem written Swedish." ></td>
	<td class="line x" title="14:184	(See KNlgren 1990)." ></td>
	<td class="line x" title="15:184	It is a carefully composed, balanced corpus." ></td>
	<td class="line x" title="16:184	Its composition follows the principles established by the Brown and LOB corpora, with adjustments for the fact that it should cover the most common genres of the Swedish of the 1990's." ></td>
	<td class="line x" title="17:184	It contains newspaper texts, fact, and fiction on several stylistic levels." ></td>
	<td class="line x" title="18:184	The texts all consist of written prose published sometime between 1990 and 1994." ></td>
	<td class="line x" title="19:184	No spoken language material is included in the corpus." ></td>
	<td class="line x" title="20:184	All words in the SUC are tagged for part-of-speech and for inflectional features." ></td>
	<td class="line x" title="21:184	For a description of the SUC annotation system, see Ejerhed et al.(1992)." ></td>
	<td class="line x" title="23:184	The tagged texts of the SUC are converted into SGML format and additional tags are added in accordance with the TEI Guidelines (Sperberg-McQueen and Bumard 1993, K~illgren 1995) to give the format in which the corpus will finally be distributed." ></td>
	<td class="line x" title="24:184	There are legal permissions allowing the corpus to be used and distributed for non-commercial research purposes." ></td>
	<td class="line x" title="25:184	3 Manual and Automatic Markup The SUC has been annotated by a process that combines automatic and manual steps." ></td>
	<td class="line x" title="26:184	The raw texts get their first analysis from the SWETWOL computerized dictionary (Karlsson 1992) and then pass a step of postprocessing to reach the analysis described in the SUC tagging manual (Ejerhed et al. 1992)." ></td>
	<td class="line x" title="27:184	The coverage of the dictionary is high, but the degree of ambiguity in Swedish is also high, actually higher than in English, so the texts return from dictionary lookup with 51% of the word tokens carrying more than one analysis." ></td>
	<td class="line x" title="28:184	In the next step, a human annotator is to mark for each ambiguous word which of the suggested readings is the correct one and for each unambiguous word whether the suggested reading is correct." ></td>
	<td class="line x" title="29:184	The output of this step is used as the 'man version' in the 676 man-machine comparison (or rather the 'woman version' as the majority of the annotators were female students)." ></td>
	<td class="line x" title="30:184	The entire corpus of 1 million words has passed through this stage of manual disambiguation and annotation, which makes it an important standard that can be used as a tool, e.g., when training probabilistic taggers." ></td>
	<td class="line x" title="31:184	The goal of the experiment reported in Kallgren (1996) was, however, to compare 'sheer' machine tagging to the performance of human annotators." ></td>
	<td class="line oc" title="32:184	The tagger used is thus one that does not need tagged and disambiguated material to be trained on, namely the XPOST originally constructed at Xerox Parc (Cutting et al. 1992, Cutting and Pedersen 1993)." ></td>
	<td class="line x" title="33:184	The XPOST algorithm has been transferred to other languages than English." ></td>
	<td class="line x" title="34:184	Douglass Cutting himself made the first Swedish version of it (Cutting 1993) and a later version has been implemented by Gunnar Eriksson (Eriksson 1995) and refined by Tomas Svensson (Svensson 1996)." ></td>
	<td class="line x" title="35:184	It is this latter version that has been used in the experiment." ></td>
	<td class="line x" title="36:184	Starting from a set of texts and a lexicon, the XPOST looks up all words in the texts and assigns to them a set of one or more readings." ></td>
	<td class="line x" title="37:184	The words are then classified into so-called ambiguity classes according to which set of readings they have been assigned." ></td>
	<td class="line x" title="38:184	The training is performed on ambiguity classes and not on individual word tokens." ></td>
	<td class="line x" title="39:184	Kallgren (1996) gives a more covering description of how XPOST is used on the Swedish material and also sketches the major differences between this algorithm and some others used for tagging, such as PARTS (Church 1988) and VOLSUNGA (DeRose 1988)." ></td>
	<td class="line x" title="40:184	A characteristic tbature of the SUC is its high number of different tags." ></td>
	<td class="line x" title="41:184	The number of part-ofspeech tags used in the SUC is 21." ></td>
	<td class="line x" title="42:184	With the addition of a category for foreign words the number of major categories used is 22 (plus three tags for punctuation), which is in no way a remarkable amount, but the SUC tags are composite." ></td>
	<td class="line x" title="43:184	This means that all words have one tag for part-of-speech, but for many parts-of speech this tag is followed by other tags for various morphological features, Where, e.g., English nouns have a variation between two possible values, singular and plural, the Swedish pattern allows for 1 x 2 x 2 x 2 x 3 = 24 different tags, specifying not only part-ofspeech but also gender, number, definiteness, and case." ></td>
	<td class="line x" title="44:184	The number of different tags actually occurring in texts is mostly around 180." ></td>
	<td class="line x" title="45:184	A remarkable fact is that the high number of different tags does not seem to influence the training and performance of probabilistic taggers negatively in the way that might have been expected." ></td>
	<td class="line x" title="46:184	The morphological errors in the material are not disturbingly many, considering the fact that all Swedish content words have such features." ></td>
	<td class="line x" title="47:184	Morphological agreement provides enough information to make it possible fbr an atttomatic tagger to pick the right form in most cases." ></td>
	<td class="line x" title="48:184	This sensitivity to close context probably explains why the high number of tags does not influence performance when it comes to picking an alternative, but it does not explain why training is so little affected by the high number of different observed situations." ></td>
	<td class="line x" title="49:184	Results from a Comparison between 'Man' and 'Machine' The automatic tagger was run on 50,000 words of text not used in the training of the tagger." ></td>
	<td class="line x" title="50:184	The output was compared to the same texts with manual disambiguation." ></td>
	<td class="line x" title="51:184	All instances where the two differ have been manually inspected." ></td>
	<td class="line x" title="52:184	The evaluation of the results is far from trivial." ></td>
	<td class="line x" title="53:184	The 'correctness' of the tagging must be judged relative to some norm." ></td>
	<td class="line x" title="54:184	One such norm is the SUC tagging manual (Ejerhed et al. 1992)." ></td>
	<td class="line x" title="55:184	Although it is very comprehensive and explicit, no manual can ever foresee and cover all the tricky instances that will occur in unrestricted language." ></td>
	<td class="line x" title="56:184	Another norm is the intuition of the working linguist, with the possibility of consulting other people to get their intuitions." ></td>
	<td class="line x" title="57:184	This also has clear drawbacks." ></td>
	<td class="line x" title="58:184	There will always remain a set of doubtful cases which do not necessarily depend on deficits in the linguistic description." ></td>
	<td class="line x" title="59:184	Be it here sufficient to say that in general \[ prefer the term 'consistent (with a certain norm)' instead of the term 'correct'; nevertheless, in the following discussion I will call the deviances from the applied noun 'errors'." ></td>
	<td class="line x" title="60:184	Table I gives the errors found in a material of 50,498 words sorted according to whether they occurred in automatically or manually tagged text or both." ></td>
	<td class="line x" title="61:184	Where both have an error, the errors can sometimes be of the same type, sometimes of different types." ></td>
	<td class="line x" title="62:184	Table 1." ></td>
	<td class="line x" title="63:184	Tagging Errors According to Source N % Errors only in automatic tagging 359l 7.1 Errors only in manual tagging 503 1.0 Errors in both 110 0.2 Total 4204 8.3 The automatic tagger is truly automatic in that it has not at all been adjusted to the specific task at hand." ></td>
	<td class="line x" title="64:184	With fairly little trimming it could well reach a level of at least 95-96% consistence with the human annotator but now the basic idea was to test it 'raw'." ></td>
	<td class="line x" title="65:184	Humans are not infallible, if anyone thought so, 1.2% of the errors are man-made." ></td>
	<td class="line x" title="66:184	It is still a consolation to see that human annotators are seven times as good as computers when it comes to disambiguation." ></td>
	<td class="line x" title="67:184	5 Types of Errors 677 The errors occurring in the material can be classified according to type." ></td>
	<td class="line x" title="68:184	By 'error type' is here meant a classification of tag pairs with an erroneous tag followed by the correct tag, e.g., an error can be of the type 'preposition suggested where it should have been an adverb'." ></td>
	<td class="line x" title="69:184	This classification shows both which parts-of-speech are most often involved in errors and which readings of a particular word are most often mixed up with each other, and in which direction the errors mostly go." ></td>
	<td class="line x" title="70:184	The classification can also give hints about what could possibly be done about the errors." ></td>
	<td class="line x" title="71:184	5.1 Errors among Content Words It is clear that content words (here: nouns, verbs, adjectives, participles, proper nouns) are seldom involved in errors." ></td>
	<td class="line x" title="72:184	Considering the large proportion of the number of running words that these major categories cover, this is even more remarkable." ></td>
	<td class="line x" title="73:184	If words from these categories are ever mixed up, they are mixed up in very specific patterns, namely with themselves (as when different inflected forms of the same stem coincide) or they are mixed up with words they are related to (e.g. , by derivation)." ></td>
	<td class="line x" title="74:184	Among the ten most common error types for either automatic or manual disambiguation, there are actually only two that involve content words." ></td>
	<td class="line x" title="75:184	One of these error types is almost exclusively in the realm of automatic disambiguation." ></td>
	<td class="line x" title="76:184	Swedish nouns are inflected according to five different declensions, one of which has zero plural." ></td>
	<td class="line x" title="77:184	The automatic tagger sometimes mistakes singular nouns of that declension without modifiers for plurals, but never the other way round." ></td>
	<td class="line x" title="78:184	This is just as could be expected; 'naked' plurals are far more common than 'naked' singulars in all declinations and will thus be favoured by the statistics." ></td>
	<td class="line x" title="79:184	To remedy this situation, it would probably be necessary to have a phrasal lexicon, as most instances of naked singular nouns appear in lexicalized phrases." ></td>
	<td class="line x" title="80:184	As has been pointed out for English material (cf.below) different inflections of the same verb can get mixed up." ></td>
	<td class="line x" title="82:184	This phenomenon can be found in Swedish too, but not very frequently." ></td>
	<td class="line x" title="83:184	The other common error type involving content words concerns adverbs derived from adjectives." ></td>
	<td class="line x" title="84:184	The most frequent derivational pattern for Swedish adverbs makes them identical to neutral singular indefinite adjectives." ></td>
	<td class="line x" title="85:184	Here both manual and automatic disambiguation leads to errors but in different directions." ></td>
	<td class="line x" title="86:184	The automatic tagger suggests adverb where there should have been an adjective, while human annotators sometimes call an adverb an adjective." ></td>
	<td class="line x" title="87:184	Both types mainly occur post-verbally and often at the very end of a graphic sentence, where it may be difficult to decide whether the concerned word is a predicative adjective or an adverb." ></td>
	<td class="line x" title="88:184	It may well be that a subcategorization of verbs might eliminate the problem, but this is a large task to implement both in the lexicon and in the tagger." ></td>
	<td class="line x" title="89:184	However, these errors are neither the most frequent nor the most disturbing ones." ></td>
	<td class="line x" title="90:184	Instead, it is the function words that get mixed up in all their different uses." ></td>
	<td class="line x" title="91:184	Actually, almost all errors concern function words and a scrutiny of them makes it clear how doubtful the whole concept of correctness is in this connection." ></td>
	<td class="line x" title="92:184	5.2 Errors among Function Words The degree of homography or is it polysemy?" ></td>
	<td class="line x" title="93:184	is generally higher among function words than among content words which, of course, leads to more situations where errors can occur." ></td>
	<td class="line x" title="94:184	Furthermore, the number of readings connected with each word token is highly dependent on the linguistic description used as a basis for the tagging system, its theoretical assumptions and the granularity of the system, among other things." ></td>
	<td class="line x" title="95:184	The ten words most frequently involved in errors in the studied material are (with approximate translations and number of errors in parenthesis) the following: 'det' (it~the in neuter gender, 330 errors), 'ett' (a/one in neuter, 254), 'sore' (rel.pron and adv., 180), 'den' (it~the in common gender, 153), 'om' (if, about, 122), 'en' (a/one in common, 109), 'att' (that, inf.marker, 83), 'sS'." ></td>
	<td class="line x" title="97:184	(so, 79), 'ut' (out, 73), 'fOr' (for, 70)." ></td>
	<td class="line x" title="98:184	They are all high frequency function words that play many different syntactic roles depending on their context." ></td>
	<td class="line x" title="99:184	One interesting fact that the classification into error types makes clear is that all the different readings of these words do not get mixed tip at random but in rather strong, often mirror-like patterns." ></td>
	<td class="line x" title="100:184	Let us take the word 'om' as an example." ></td>
	<td class="line x" title="101:184	It can be used as adverb, preposition, or subordinating conjunction and all the six possible mistagged combinations do occur, but with quite varying frequency." ></td>
	<td class="line x" title="102:184	Three of them are almost neglectable and one has a strong unidirectional pattern where the reading as an adverb (more precisely a verbal particle) is often taken for a preposition." ></td>
	<td class="line x" title="103:184	This is an instance of the by far most common error type in the entire material, and is of course directly dependent on the way verbal particles are treated in the underlying linguistic description." ></td>
	<td class="line x" title="104:184	The remaining two error types are the most interesting ones." ></td>
	<td class="line x" title="105:184	They form a bidirectional pattern where the reading as a preposition is confused with the reading as a subordinating conjunction." ></td>
	<td class="line x" title="106:184	Preposition instead of subjunction appears 40 times, subjunction instead of preposition 33 times, altogether 77 of the 122 errors connected with the word 'om'." ></td>
	<td class="line x" title="107:184	All errors on this word were machine-induced, except 8 cases where human annotators took a subjunction to 678 be a preposition." ></td>
	<td class="line x" title="108:184	Some of the error situations may be regarded as truly undecidable." ></td>
	<td class="line x" title="109:184	6 Tagging Undecidable Situations How are bidirectional error patterns like the one above to be treated?" ></td>
	<td class="line x" title="110:184	Looking at their close context, it is often impossible to handle the situation with some smart tagging restriction or other device." ></td>
	<td class="line x" title="111:184	They are also so equal in number and so frequent that one cannot simply decide to let one reading overrule the other and live with the errors that such a happy-golucky solution would give rise to." ></td>
	<td class="line x" title="112:184	(As a practicing corpus tagger, 1 know that this unorthodox method can sometimes be the best way out of problematic situations)." ></td>
	<td class="line x" title="113:184	Another possibility would be to amalgamate tile two readings into one, bivalued or underspecified, depending on how one chooses to see it." ></td>
	<td class="line x" title="114:184	As ah'eady mentioned, these more or iess undecidable bidirectional patterns have been observed and discussed by others working with tile tagging of large corpora, and they have, seemingly independently of each other, come up with similar suggestions." ></td>
	<td class="line x" title="115:184	Below are three quotations dealing with this matter." ></td>
	<td class="line x" title="116:184	The Penn Treebank: 'ltowever, even given explicit criteria for assigning POS tags to potentially ambiguous words, it is not always possible to assign a unique tag to a word with confidence." ></td>
	<td class="line x" title="117:184	Since a major concern of the Treebank is to avoid requiring annotators to make arbitrary decisions, we allow words to be associated with more than one POS tag." ></td>
	<td class="line x" title="118:184	Such multiple tagging indicates either that the word's part of speech simply cannot be decided or that the annotator is unsure which of the alternative tags is the correct one'." ></td>
	<td class="line x" title="119:184	(Marcus et al. 1993, 316)." ></td>
	<td class="line x" title="120:184	The British National Corpus: 'In order to provide more useful results in a substantial proportion of the residual words which cannot be successfully tagged, we have introduced portmanteau tags." ></td>
	<td class="line x" title="121:184	A portmanteau tag is used ill a situation where there is insufficient evidence for Claws to make a clear distinction between two tags." ></td>
	<td class="line x" title="122:184	Thus, in the notoriously difficult choice between a past participle and the past tense of a verb, if there is insufficient probabilistic evidence to choose between the two Claws marks the word as VVN-VVD." ></td>
	<td class="line x" title="123:184	A set of fifteen such portmanteau tags have been declared, covering the major pairs of confusable tags'." ></td>
	<td class="line x" title="124:184	(Garside 1995)." ></td>
	<td class="line x" title="125:184	Constraint Grammar: 'In the rare cases where two analyses were regarded as equally legitimate, both could be marked'." ></td>
	<td class="line x" title="126:184	(Voutilainen and Jfirvinen 1995, 212)." ></td>
	<td class="line x" title="127:184	It is, however, important that the s/tuations where underspecified tags can be used are restricted to welldefined cases and that the reasons for using them are quite clear." ></td>
	<td class="line x" title="128:184	They should have what I call a 'mirror' character, in that the interchange goes in both directions, and they should concern clearly distinct pairs of tags even when a word has several other tags as well." ></td>
	<td class="line x" title="129:184	Such situations are more common in automatic tagging but they occur in manual tagging as well." ></td>
	<td class="line x" title="130:184	The reasons for a situation being undecidable can, however, vary." ></td>
	<td class="line x" title="131:184	Voutilainen and J~irvinen, in their study of inter-annotator agreement, mention three situations where an nnderdetermined analysis was accepted: 'When the judges disagree about the correct analysis even after negotiations." ></td>
	<td class="line x" title="132:184	In this case, comments were added to distinguish it from the other two types." ></td>
	<td class="line x" title="133:184	Neutralisation: both analyses were regarded as equivalent." ></td>
	<td class="line x" title="134:184	(This often indicates a redundancy in the lexicon)." ></td>
	<td class="line x" title="135:184	Global ambiguity: the sentence was agreed to be globally ambiguous'." ></td>
	<td class="line x" title="136:184	(Voutilainen and J~trvinen 1995, 212)." ></td>
	<td class="line x" title="137:184	Marcus et at." ></td>
	<td class="line x" title="138:184	(1993) allow underspecified tagging both for annotators' uncertainty or disagreement and for cases that correspond to Voutilainen and J~irvinen's neutralisation and global ambiguity." ></td>
	<td class="line x" title="139:184	This may be infelicitous." ></td>
	<td class="line x" title="140:184	It is important to keep a clear borderline between situations that could be solved in principle and such that are truly undecidable." ></td>
	<td class="line x" title="141:184	The latter ones may lead us to questions about the nature of language and to what extent natural language really is exact and welldefined." ></td>
	<td class="line x" title="142:184	Introducing underspecified tags would influence the training and performance of a probabilistic tagger in at least the tbllowing ways: a) The concerned words would mostly get more alternative tags, one for each of the unambigous readings plus one for the underspecified one." ></td>
	<td class="line x" title="143:184	According to common tagging principles, this would be a disadvantage, b) There would be fewer obserw, tions of each of the alternative tags, as the competing unambiguous tags both would lose some of their instances to their common underspecified alternative." ></td>
	<td class="line x" title="144:184	This would also be a disadvantage, c) The observations of each tag would hopefully be more correct, as the instances 'lost' to the underspecified tag would be the tricky and atypical cases that otherwise might obscure the contextual patterns of the unambiguous tags." ></td>
	<td class="line x" title="145:184	d) The underspecified instances can later be automatically retrieved for either manual inspection or some more elaborate disambiguation device." ></td>
	<td class="line x" title="146:184	It is still an open question whether the more clearcut distinctions introduced by the underspecified tags compensate 1or the accompanying disadvantages, but at least they have the intellectually pleasing property of showing where there are truly ambiguous situations in language." ></td>
	<td class="line x" title="147:184	By systematic modifications of the tagset along these lines it is possible to decide to what extent the introduction of underspecified tags will improve tile overall performance of a tagger and/or facilitate the task of human annotators." ></td>
	<td class="line x" title="148:184	679 References Church, K. W." ></td>
	<td class="line x" title="149:184	(1988), 'A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text', in Proceedings of the Second Conference on Applied Natural Language Processing (ACL), 136-43 (Austin)." ></td>
	<td class="line x" title="150:184	Cutting, D." ></td>
	<td class="line x" title="151:184	(1993), 'Porting a Stochastic Part-ofSpeech Tagger to Swedish', in Eklund, R." ></td>
	<td class="line x" title="152:184	(ed)." ></td>
	<td class="line x" title="153:184	Nordiska Datalingvistikdagarna, 65-70 (Stockholm)." ></td>
	<td class="line x" title="154:184	Cutting, D. , Kupiec, J. , Pedersen, J. , and Sibun, P." ></td>
	<td class="line x" title="155:184	(1992), 'A Practical Part-of-Speech Tagger', in Proceedings of the Third Conference on Applied Natural Language Processing (ACL) (Trento)." ></td>
	<td class="line x" title="156:184	Cutting, D. and Pedersen, J." ></td>
	<td class="line x" title="157:184	(1993), The Xerox Part-of-Speech Tagger, Xerox PARC technical report." ></td>
	<td class="line x" title="158:184	DeRose, S. J." ></td>
	<td class="line x" title="159:184	(1988), 'Grammatical Category Disambiguation by Statistical Optimization', Computational Linguistics, Volume 14:1, 31-9." ></td>
	<td class="line x" title="160:184	Ejerhed, E. , Kfillgren, G. , Wennstedt, O. and A.str6m, M." ></td>
	<td class="line x" title="161:184	(1992), The Linguistic Annotation System of the Stockholm-Umeh Corpus Project, version 4.31." ></td>
	<td class="line x" title="162:184	Publications from the Department of General Linguistics, University of Ume~, no. 33." ></td>
	<td class="line x" title="163:184	Eriksson, G." ></td>
	<td class="line x" title="164:184	(1995), 'Beskrivning av arbetet reed att utveckla en XPOSTtagger for svenska', Technical report, Telia Research Infovox (Stockholm)." ></td>
	<td class="line x" title="165:184	Garside, R." ></td>
	<td class="line x" title="166:184	(1995), 'Using CLAWS to Annotate the British National Corpus', URL: http ://info.ox.ac.uk/bnc/garside_allc.html." ></td>
	<td class="line x" title="167:184	K~llgren, G." ></td>
	<td class="line x" title="168:184	(1990), ''The First Million is Hardest to Get': Building a Large Tagged Corpus as Automatically as Possible', in Proceedings from Coling '90 (Helsinki)." ></td>
	<td class="line x" title="169:184	K~llgren, G." ></td>
	<td class="line x" title="170:184	(1995), 'Manual for TEI conformant mark-up of the SUC', draft version, Department of Linguistics, Stockholm University." ></td>
	<td class="line x" title="171:184	K~llgren, G." ></td>
	<td class="line x" title="172:184	(1996), 'Man vs. Machine Which is the Most Reliable Annotator?', to appear in Perissinotto, Giorgio (ed.), Research in Humanities Computing 6, Oxford University Press." ></td>
	<td class="line x" title="173:184	Karlsson, F." ></td>
	<td class="line x" title="174:184	(1992), 'SWETWOL: A Comprehensive Morphological Analyzer for Swedish', in Nordic Journal of Linguistics Vol." ></td>
	<td class="line x" title="175:184	15:145." ></td>
	<td class="line x" title="176:184	Marcus, M. P., Marcinkiewicz, M. and Santorini, B." ></td>
	<td class="line x" title="177:184	(1993), 'Building a Large Annotated Corpus of English: The Penn Treebank', in Computational Linguistics Volume 19:2, 313-30." ></td>
	<td class="line x" title="178:184	Sperberg-McQueen, C. M. and Burnard, L." ></td>
	<td class="line x" title="179:184	(1993) (eds.), Guidelines for Electronic Encoding and Interchange (Chicago, Oxford)." ></td>
	<td class="line x" title="180:184	Svensson, T." ></td>
	<td class="line x" title="181:184	(1995), 'Ore tagguppsfittningar i en f6rsta ordningens g6md Markovmodell', Technical report, Yelia Research Infovox (Stockholm)." ></td>
	<td class="line x" title="182:184	Voutilainen, A. and J~irvinen, T." ></td>
	<td class="line x" title="183:184	(1995), 'Specifying a shallow grammatical representation for parsing purposes', in Proceedings of the Seventh Conference of the European Chapter of the Association for Computational Linguistics, 2 l 0-14." ></td>
	<td class="line x" title="184:184	680" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C96-2136
Context-Based Spelling Correction For Japanese OCR
Nagata, Masaaki;"></td>
	<td class="line x" title="1:224	Context-Based Spelling Correction for Japanese OCR Masaaki NAGATA NTT Information and Communication Systems I,aboratorics 1-2356 Take, Yokosuka-Shi, Kanag~wa, 238-03 Japan naga%aenttnly, isl." ></td>
	<td class="line x" title="2:224	ntt." ></td>
	<td class="line x" title="3:224	j p Abstract We present a novel spelling correction method \['or those languages that have no delimiter between words, such ~rs,lap;mese, (.',hinese,,~nd ThM." ></td>
	<td class="line x" title="4:224	It consists of an al)proximate word matching method and an N-best word seg mental|on Mgorithm using a statistical la.nguage model." ></td>
	<td class="line x" title="5:224	For OCR errors, the proposed word-based correction method outperf.ornrs the conventional charactm'b`ased correction method." ></td>
	<td class="line x" title="6:224	When the bmselme character recognition accuracy is 90%, it achieves 96.0% character recognition accuracy and 96.3% word segmentation accuracy, while the cilaracter recognition accuracy of cilaracterb,ased correction is 1 Introduction Automatic spelling correction research dates t)ack in the 1960s." ></td>
	<td class="line x" title="7:224	~lbday, there are some excellent academic ~nd commercial spell checkers available \['or English (Kukich, 1992)." ></td>
	<td class="line x" title="8:224	However, for those languages that have a different morphology and writing system from English, spelling correction remMns one of the signillcant unsolved researcil problems in computational linguistics." ></td>
	<td class="line x" title="9:224	'\['he b,asic strategy for English spelling correction is sitnple: Word boundaries are defined by white space characters." ></td>
	<td class="line x" title="10:224	If the tokenized string is not found in the dictionary, it, is either a nonword or an unknown word." ></td>
	<td class="line x" title="11:224	For a. non word, correction candidates axe generated t)y approxinm.tely matching the string with the dictionary, using context independent word dis|mice measures such,as edit distance (Wagner and l,'ischer, 1974; Kernighan et M. , 19q0)." ></td>
	<td class="line x" title="12:224	It is impossible to apply these 'isolated word error correction' techniques to Japanese in two re`asons: First, in noisy texts, word tokenization is difficult because there are no delimiters between words." ></td>
	<td class="line x" title="13:224	Second, context-independent word distance measures ~re useless because the average word length is very short (< 2), and the chnra.cter set is huge (> 3000)." ></td>
	<td class="line x" title="14:224	There are a large number of one edit distaalce height)ors for a,lapanese word." ></td>
	<td class="line x" title="15:224	In English spelling correction, 'word bound a.ry problem', such as splits (forgot -~.lot gol) a.nd run-ons (in form --+ in.lbrm), mad 'short word problem'(ot -~ on, or, of, at, it, to, etc)." ></td>
	<td class="line x" title="16:224	are also known to I)e very dilIicult." ></td>
	<td class="line x" title="17:224	Context infof mat|on, such as word N-gram, is used to supplement the underlying context-independent co> reel|on tbr these problematic examples (GMe and (~hurch, 1990; Mays et aJ., 1991)." ></td>
	<td class="line x" title="19:224	To the contra.ry, Japanese spelling correction must be essentially context-dependent, because Japanese sentence is, as it were, a. run on sequence of short words, possibly including some typos, something like (lfor.qololinfo'mnyou --~ I forgot to inibrtn you)." ></td>
	<td class="line x" title="20:224	In this pa.per, we present a novel ~t)proach for spelling correction, which is suite.hie for those l~nguages that have no delimiter between words, such f~s aN)anese." ></td>
	<td class="line x" title="21:224	It consists of two stages: First, MI substrings in the input sentence are hypothesized ms words, and those words that approximately matched with the substrings axe retrieved from the dictionary ms well,as those that exactly matched, l,ased on the statisticM language model, the Nd)est word sequences are then selected as correction ca,ndidates from all combinations of exactly and approximately matched words." ></td>
	<td class="line x" title="22:224	Fig ure 1 illustrates this ~pproach." ></td>
	<td class="line x" title="23:224	Out of the list of character recognition candidates for the input sentence '~ b~R~7-~,~Y2~g)k~ 79o ' which means 'to hill out the necessary items in the application form.', the system searches the eombin,~tion of exactly matched words (solid boxes) and apl)roximately matched words (dashed boxes) 1 The major contribution of this paper is its solutions of the word boundary problem and short word problem in Japanese spelling coffee." ></td>
	<td class="line x" title="24:224	tion." ></td>
	<td class="line x" title="25:224	lly introducing a statistical model of word tOCR output tm,ds to be very noisy, est)e(:ially for hand writing." ></td>
	<td class="line x" title="26:224	To (:omt)ensate for this bd,avior, OC'Rs usuMly output at, ordered list of tit(!" ></td>
	<td class="line x" title="27:224	best N elutra(:ters." ></td>
	<td class="line x" title="28:224	The list of the (:~uMid~tes for an int)ut string is called etl~*ra(:ter m~mix." ></td>
	<td class="line x" title="29:224	806 input sentence character matrix 171 i~ d H }\[:~ K,i~, -~ dt fJl itt4 1 D p\] X. -~t7 ~t7j~ h .~ ( 7 b,,7 forward search \[\] T_~K-} \[\] A 5 ~J 3 exactly matched word i  I approximately matched word   I'igure 1: l'ossihle ( 1oinl)hia,l,ioliS or l:,XalCl, ly a, nd Atfl)roxhua,l,ely ~l\] n,l,ched Words Imigl;h a,iid sl/ellhig, l,lie proposed sysLei\[l a,Ccti ra, t, ely phi,ces word bounda, rics in noisy LexLs 1,1u/,L iliclude liOll-words n,nd tlllkl/OWll words." ></td>
	<td class="line x" title="30:224	Ily using t, he c|la,ra,cl,or I)ased CC~l\]l;0xl; lilode\], il; a,c(:ura,1;ely selecl, s (:orr01,iOll (umdid~l.es \['or shorL words \['i'Ol\[i Lhe h~rge n tlllll)cr o\[' .~pproxini;~l;ely ill~l,L(tiletI worcts wiLh Lhe slmlc edit, disl;n,nco." ></td>
	<td class="line x" title="31:224	The gold o\[ our project, is l,o iniplenienl, a, iI h'li,~r ~ci.ive word correcl,or for ~ lia,ndwriLi, m~ FAX OCI, sysl, eili." ></td>
	<td class="line x" title="32:224	ldT~ a,re especia, lly inl;eresl;ed in 1;exl, s t,lia,l, include a,ddresses, IHI,IIICS, ~l,lld \[iiessa,~es, such as order fOrlllS, quesLionnn,ires, a,nd t, ch~gi'ig)h. 2 Noisy Channel Model for CharacLer Recognition FirsL, we (\]lrniula,l,e i;hc, spellinT; c(~rrcct;ion o\[' O(2 I~ <~i'rors in I,he noisy cha, nncl pa, r;~div;ln." ></td>
	<td class="line x" title="33:224	I,el, (< rop resenL 1,tie inpuL sLrhlg a,ncl,\ rct>resenl, l;tie ()(Jlt, oul;Imt, st, rhig." ></td>
	<td class="line x" title="34:224	l'hidhig Llie fllosL tm~lmble sf, i'hig C < given iJle O( ill, oul,ptll, <\' a,nioulil,s I;o ma, xiniizili~; u,e l;,,,,~,io,, r(x IC)/-'((;), : =,H~,,,,~ n(c'lX ) =,~,.~,IID, X \])(X\[~.))\])({." ></td>
	<td class="line x" title="35:224	) (i) (3 {' lieca, use lia,yes' rule sl;;d,es l;ha, L, #'(~'l.V) = s'(xl(:.')#'(~:')." ></td>
	<td class="line x" title="36:224	(.~) /'( X ) /'(C) is ca, tied the hmgu;Lge model." ></td>
	<td class="line x" title="37:224	II, is c.ni puled \[l'Oilt l,lw l,ra,iilhig CoI'\])CiS." ></td>
	<td class="line x" title="38:224	I,et, us cMI P(XI(." ></td>
	<td class="line x" title="39:224	<) l,lte O(',ll, niodel." ></td>
	<td class="line x" title="40:224	IL ca, it |)o conttmLed l'roni t;iic a, priori likelihood o,gLilnn,Les for individ ua, I cila, ra,cl;ers, is p(x IC) = l-i/'{':' I<, ) (:~) where 'n is l, he sl,rhlv; leu~>l,h, t'(:l:ilcT) is c;i,IN'd L|ic COlifUsion lun,l, rix o\[ ch;i,ra,(tl,or<~." ></td>
	<td class="line x" title="41:224	IL i,g t,i';i,hicd tishlg l,lie hipuL a, nd oul,1)ul, sl;rhigs o\[' Lhe ()(ill The coii\['tiSiOll \[lialrix is hig|lly depentionL eli l:iie chn,r;l,ct,er r0co~niLion a,l~orit, iun a,nd Lhe qua, I iLy of the hlput, docunionl,." ></td>
	<td class="line x" title="42:224	IL is a, lld)or inl;ensive 1;aM~ Lo prepa, re a, con\[tiSiOl/ niaJ;rix lbr ca,oh cha,r a,cl,er recognilJon sys|,ellt, sillce .I a,p<~liose ha.s lilore 1;hail 3,0()() c|lai','l, cl;ers." ></td>
	<td class="line x" title="43:224	'l'here\[7)re, we used a, shn pD ()(\]11, model wliere l, lie confusion ni~t, rix is a>p proximai, ed by t, tie correcl; cha,r;~cl,er dist, ribul;ion ovm' t, he r&nk of Ltie ca,ndicl&Lcs." ></td>
	<td class="line x" title="44:224	We asstttlle t;ll~|; l, hera, nk erder disl, i;ibul, ion of l,|io correcL clia,ra,c Lore is a, geonl0Lric disf, ribuLion whoso pa,ra,niet,er is l;he a:CCtll';%cy OI' Lhe firM; ca,ndida, Le." ></td>
	<td class="line x" title="45:224	l,ei, c~ be tha 7-i,h c|la, ra,cl,er in Lhe inpuL sl, rin~4, :l: G \])e l,|ie j t,h ca,ndida,t,e \[or (:~, ;uid p 1)o Life prol> idfilii, y l, ha,i, l, he lh'sl; ca,ndida,Le is correct,." ></td>
	<td class="line x" title="46:224	'l'lie COlt f'tiSiOll pt'olml>ilil~y \['(;v U \[r:i) is a,pproinla,t, od as, r(~:,~l<,,) ~ P(:,:,~ i,~ <:<,,.,.<:<:c) .~ ~,(l p)~ ' (,1) I'klua,l:ion (d) a, hus t,o a,Pl~roxhna,i:e l,he a,ccura,cy of t,hc firsL ca,ndida.im, a,nd tJle l,endency tJmt, tj.relia.bilit;y of Lhe ca.ndhla.l;e ch'cramses al,'uptly as its ranlc incr(m.~es." ></td>
	<td class="line x" title="48:224	For exa.mple, ill the recognition ~ccura.cy of t, he lirsi; candida.t.e p is 0.75, we will as sign i,he prob;dfilii~y of Lllo Iirst,, secmid~ ;rod i, hird cn, ndida.i,cs l,o 0.75, 0.\]9, a,i/d (I.05, respect.ively, regli, i'dloss ~lI' Lhe hipuL a,iid Ollt;pul, cha,r~cl;ers." ></td>
	<td class="line x" title="49:224	(-)11o ~.)J' Lhe I:lelietil,s o\[ usin<g a, siliiplc ()(It nlodel is Lha,L Lhe spelling correct, ion s.yM,eni be coiiles hi~4hly imh3)endenl, of l,lie underlying; ()( i1 cha,raxq, crisl,ics." ></td>
	<td class="line x" title="50:224	Obviously, a, more sophislficaLed O(\]11, niodol would iniprove OI'FOF Col'rect;ioli /)el' retina, liCe, hut, eVeli l,his shnlHe O(II/ilit>d<q works fa,h'iy wdl in our eXllerinient,s 7 2()11(' (if (,lit!" ></td>
	<td class="line x" title="51:224	I)i'ax:l,iciil r(',a,Stlllb ~'(Jr ilSill,~ Lhc ~tXJlllCILrh: di~lrilml, hJli i~ ilia, i, we ll~cd l,hc cuid'H,HO, nlal, ri for ilnl)h!iiicnl, in ~ the O(;R silnlila, l,Ol-." ></td>
	<td class="line x" title="52:224	\'\'c fccl h, i,~ ilnfllir I,t) ii~t! I, hc sliillc con\[llblon lilaA, rix btJLh ftJl' t!lr()l l'~Clicr&l, ioli illld error corrc, d, lon." ></td>
	<td class="line x" title="53:224	{7 0 7 3 Word Segmentation Algorithm 3.1 Statistical Language Model For the language model in Equation (1), we used the part of speech trigram nlodel (POS trigranl or 2nd-order HMM)." ></td>
	<td class="line oc" title="54:224	It is used,as tagging mode\[ in English (Church, 1988; Cutting et al. , 1992) and morphological analysis nlodel (word segmentation and tagging) in Japanese (Nagata, 1994)." ></td>
	<td class="line x" title="55:224	Let the input character sequence be (/ = c\]c.ec  We approxinlate P(C)by P(W, 7'), the joint prol>ability of' word sequence W = wlw2'u),~ and part of speech sequence '\[' = tlt.e, t,,." ></td>
	<td class="line x" title="56:224	P(W,T) is then approximated t>y the product of parts of speech trigram probabilities P(ti\]ti-'2, |i-l) and word output probabilities for given part of speech P(wiltl), 71 p(c) _p(w,~') --= IX p(t, lt,-=,t,-,)p(~,lt,) (5) i=1 P(tilti-,e,ti-~) and /-'(w~lti ) are estimated \[>y computing the relative frequencies of the corresponding events in training corpus a 3.2 Forward-DP Backward-A* Algorithm \[/sing the language model (5), .Japanese morp\[lological analysis can be detined,as finding tile set of word segmentation and parts of speech (1~/, 7'') that maximizes the joint probability of word sequence and tag sequence P(W, 7')." ></td>
	<td class="line x" title="57:224	(V, ~') =,~,-g,,,~ P(w,'J') (~) W~ T This maxinfization search can be efficiently implemented t>y using the forward-DP backward-A* algorithm (Nagata, 1994)." ></td>
	<td class="line oc" title="58:224	It is a natural extension of the Viteri>i algorithm (Church, 1<,)88; Cutting et al. , 1992) for those languages that do not have delimiters between words, and it can generate N-best morphological analysis hypotheses, like tree trellis search (Soong and l\[uang, 1991)." ></td>
	<td class="line x" title="59:224	The algorithm consists of a forward dynamic programming search and a backward A* search." ></td>
	<td class="line x" title="60:224	The fbrward search starts from tile beginning of the input sentence, and proceeds character by character." ></td>
	<td class="line x" title="61:224	At each point in tile sentence, it looks up the combination of the best partial parses ending at the point and word hypotheses starting at the point." ></td>
	<td class="line x" title="62:224	If the connection between a partial parse and a word hypothesis is allowed by the language model, that is, the corresponding part of speech trigram probability is positive, a new continuation parse is made and registered in the best partial path table." ></td>
	<td class="line x" title="63:224	\[,'or example, at point 4 in Figure 1, tile final word of the partial parses ending at 4 are ga b~ ('application'), .~ ('prospect'), SAs a word segmeotal, ion nmdel, the advantage of the POS trigram model is that it can be trained using a smaller <:orpus, than the word bigram mode.1." ></td>
	<td class="line x" title="64:224	and ~ ('inclusive'), while tile word hypotheses starting at 4 are m~ ('form'), ~ ('s~ne'), Y\] ('moon'), and Fq ('circle')." ></td>
	<td class="line x" title="65:224	In tile backward A* search, we consider a partial parse recorded in the best partial path tat>lc `as a state in A* seareiL 'File backward search starts at tile end of the input sentence, and backtracks to tile beginning of the sentence." ></td>
	<td class="line x" title="66:224	Since the prob abilities of the best possible remaining paths are exactly known by the forward search, the backward search is admissible." ></td>
	<td class="line x" title="67:224	We made two extensions to tile original fbrwardDP backward-A* algorithm to handle OCR outputs." ></td>
	<td class="line x" title="68:224	First, it retrieves all words in tile dictionary that match the strings which consist of a combination of the characters in the matrix." ></td>
	<td class="line x" title="69:224	Second, the path probability is changed to the product of the language model probability and the OCR model probability, so as to get the most likely character sequence, according to Equation (1)." ></td>
	<td class="line x" title="70:224	4 Word Model for Non-Words and Unknown Words The identification of non:words and unknown words is a key to implement Japanese spelling cotrector, because word identilication error severely atDets the segmentation of neighboring words." ></td>
	<td class="line x" title="71:224	We take tile following approach for this word boundary problem." ></td>
	<td class="line x" title="72:224	We first tlypothesize all sub: strings in the input sentence as words, and assign a reasonable non-zero probal>ility." ></td>
	<td class="line x" title="73:224	\[,'or example, at point 7 in Figure 1, other than the exactly and approximately matched words starting at 7 such,as,.g,~ ('necessary'), ~,'~ ('necessarily'), and alZ ('pond'), we tlypothesize the sut>strings ~,, ~,~, ~,@~,,.g,@~,  as words." ></td>
	<td class="line x" title="74:224	We then locate the most likely word boundaries using the forwardI)P backward-A* algorithm, taking into account the entire sentence." ></td>
	<td class="line x" title="75:224	We use a statistical word model to assign a probat>ility to each subs|ring (Nagata, 1996)." ></td>
	<td class="line x" title="76:224	It is defined as tile joint probability of tile character sequence if it is an unknown word." ></td>
	<td class="line x" title="77:224	Without loss of generality, we can write, P(~ I<~z>) = pp(c~,:~ I<~z>) = r(k)P(,:,,;~lk) (7) where <'.1    <'+ is the character sequence of length k that constitutes word wi." ></td>
	<td class="line x" title="78:224	We call P(k) the word length model, and P(cl   ck \]k) the spelling nmdel." ></td>
	<td class="line x" title="79:224	We assume that word length probability P(k) obeys a Poisson distribution whose parameter is the average word length A, (.~ __ \] )k This means that we think word length is the in terval between hidden word boundary markers, which are randomly placed where tile average interval equals tile average word length." ></td>
	<td class="line x" title="80:224	Although 808 this word length model is very simple, it plays a key role in making tile word segmentation algo rithm rot>ust." ></td>
	<td class="line x" title="81:224	We al)proximate the spelling probability given word length P(el  ck \]k) |>y tile word-t)a~ed character trigram model, regardless of word length." ></td>
	<td class="line x" title="82:224	Since there are more than 3,000 characters in Japanese, tile amount of training data would be too small if we divided them by word length." ></td>
	<td class="line x" title="83:224	@:~'~) -P(c~ I#, #)P(c= I#, q ) k z=3 where '#' indicates the word t>oundary marker." ></td>
	<td class="line x" title="84:224	Note that tile word-I>,%sed character trigram model is different from tile sentence-b~Lsed character trigram model." ></td>
	<td class="line x" title="85:224	'l'he tbrmer is estimated from tile corpus which is segmented into words." ></td>
	<td class="line x" title="86:224	It a,ssigns large probabilities to character sequences that appear within a word, and small probat>ilities to those that appear across word boundaries." ></td>
	<td class="line x" title="87:224	5 Approximate Match for Correction Candidates As described t>elBre, we hypothesize all sul>strings in the input sentence,as words, and retrieve ap: proximately matched words from the dictionary as correction candidates." ></td>
	<td class="line x" title="88:224	For a word hypoth-." ></td>
	<td class="line x" title="89:224	esis, correction candidates are generated based on tile minimmn edit distance technique (Wagnet anti l!'ischer, 1974)." ></td>
	<td class="line x" title="90:224	Edit distance is defined as the ntiniulum number of editing operations (in sertions, deletions, and substitutions) required to transform one string into another." ></td>
	<td class="line x" title="91:224	If tile target is OCIL output, we can restrict tile type of errors to substitutions only." ></td>
	<td class="line x" title="92:224	Thus, the similarity of two words can be computed as c/n, where c is tile nund)er of matched characters and n is tile length of the misspelled (and dictionary) word." ></td>
	<td class="line x" title="93:224	For longer words (._> 3 characters), it is rea: sonable to generate correction candidates t>y retrieving all words in the dictionary with similarity above a certain threshold (eta >_ 0.5)." ></td>
	<td class="line x" title="94:224	For exampie, at point 0 in Figure 1, g+ b~ ('application') is retrieved by approximately ntatching the string Itt L~;9with the dictionary (c/n = 3/4 = 0.75)." ></td>
	<td class="line x" title="95:224	Ilowever, tbr short words (1 or 2 character word), this strategy is unrealistic because there are a large numt>cr of words with one edit dislance." ></td>
	<td class="line x" title="96:224	Since the total nund)er of one character words and two <:haracter words anlounts to luore than 80% of the total word tokens in Japanese, we cannot neglect these short words." ></td>
	<td class="line x" title="97:224	It is natural to resort to context-dependent word correction methods to overcome tile short word prol>lem." ></td>
	<td class="line x" title="98:224	In English, ((-;ale and (\]hurch, 199(t) achieved good spelling check performance using word bigranLs, llowever, in,lapanese, we cannot use word bigram to rank correction candidates, because we have to rank them betbre we pertbrm word segnmntation." ></td>
	<td class="line x" title="99:224	Therefbre, we used character context instead of word context." ></td>
	<td class="line x" title="100:224	For a short word, correction candidates with the same edit distance are ranked by tile joint probability of tile previous and tile following two characters in the context." ></td>
	<td class="line x" title="101:224	This probw bility is computed using the sentence-based character trigram model." ></td>
	<td class="line x" title="102:224	For 2 character words, for example, we first retrieve a set of words in the dictionary that match exactly one character with the one in the input string." ></td>
	<td class="line x" title="103:224	We then compute the 6 grant probability Ibr all candidate words .siSi+l, and rank them according to the prot>ability." ></td>
	<td class="line x" title="104:224	P(c,_2, ci-l, .sl, si+.t, ci+:~, ci+a ) : P(.s'ilci-~, cl-t ) P(si+l \]ci 4, .'~i) P(ci+=lsl,.si+l)P(ci+al.si+t,ci+.2) (10) For example, at point 12 in Figure 1, there are many two character words whose first character is ~g, such ~s -gEil~ ('mention'), ~E~4$ ('article'), ~0." ></td>
	<td class="line x" title="105:224	.~ ('journalist'), gg.zX." ></td>
	<td class="line x" title="106:224	('entry'), g0,,&~, ('commen> oration'), etc. By using character contexts, tile system selects gg)k. anti ~t\]fti;~ as approximately matched word hypotheses." ></td>
	<td class="line x" title="107:224	6 Experiments 6.1 Language Data and OCR Simulator We used tile NI'R Dialogue Database (Ehara et el., 1990) to train and test tile spelling correction method." ></td>
	<td class="line x" title="109:224	It is a corpus of approximately 800,000 words whose word segmentation anti part ok' speech tagging were laboriously performed by hmu\[." ></td>
	<td class="line x" title="110:224	In this experiment, we used one lburth of tile ATR, Corpus, a portion of tile keyboard dialogues in the conference registration domain." ></td>
	<td class="line x" title="111:224	'l'able 1 shows the nmnber of sentences, words, and characters for training anti test data." ></td>
	<td class="line x" title="112:224	The test data is not included in the training data." ></td>
	<td class="line x" title="113:224	That is, open data were tested in the experiment." ></td>
	<td class="line x" title="114:224	Tat>le it: The Amount of 'laining and '\[>st Data Training set Test set Senten<:es 10945 l O0 Words 150039 1134 C, haracters 268830 2097 For the spelling correction experiment, we used an OC, R simulator because it is very difficult to obtain a large amount of test data with arbitrary recognition accuracies." ></td>
	<td class="line x" title="115:224	The OCR, simulator takes an input string anti generates a character matrix using a conflmion matrix for Japanese handwriting OCI,, developed in our laboratory." ></td>
	<td class="line x" title="116:224	The parameters of the OCR sinmlator are tile recognition accuracy of the lirst candidate (lirst candklate correct rate), anti tile percentage of tile correct the.r809 acters included in tile character matrix (correct candidate included rate)." ></td>
	<td class="line x" title="117:224	In general, the accuracy of current Japanese handwriting OCR is around 90%." ></td>
	<td class="line x" title="118:224	It is lower than that of printed characters (around 98%) due to the wide variability in handwriting." ></td>
	<td class="line x" title="119:224	When the input comes from FAX, it degrades another 10% to 15%, because tile resolution of most FAX machines is 200dpi, while that of scanners is 400dpi." ></td>
	<td class="line x" title="120:224	There\['ore, we made \[bur test sets of' character matrices whose first candidate correct rates and correct candidate included rates were (70%, 90%), (80%, 95%), (90%, 98%), and (95%, 98%), respectively." ></td>
	<td class="line x" title="121:224	The average numt>er of candidates ibr a character w~s 8.9 in these character matrices 4 6.2 Character Recognition Accuracy First, we compared the proposed word-based spelling corrector using the POS trigram model (POSe) with tile conventional character I)msed spelling eorreetor using tile character trigram model (Char3)." ></td>
	<td class="line x" title="122:224	Table 2 shows tile character recognition accuracies after error correction \['or various b~seline OCR accuracies." ></td>
	<td class="line x" title="123:224	We also changed the condition of the approximate word match." ></td>
	<td class="line x" title="124:224	In Tat)le 2, Matehl, Match2, and Match3 represent that tilt approximate mM;ch fbr substrings whose lengths were more than or equal to one, two, and three characters, respectively." ></td>
	<td class="line x" title="125:224	In generM, tile approximate match for short words improves character recognition accuracy by about one percent." ></td>
	<td class="line x" title="126:224	When the lirst candidate correct rate is low (70% and 80%), tile word based corrector significantly outperIbrnL~ tile characterbased corrector." ></td>
	<td class="line x" title="127:224	This is because, by approximate word matching, tile word-based corrector can correct words even if the correct, characters are not present in the matrix." ></td>
	<td class="line x" title="128:224	When the first candidate correct rate is high (90% and 95%), the wordI>~sed corrector still outperl`orms tile character based eorrector, although the ditDrenee is small." ></td>
	<td class="line x" title="129:224	This is because most correct characters are al ready included in the ma.trix." ></td>
	<td class="line x" title="130:224	Table 2: Comparison of Character Recognition Accuracy (Character Trigram vs. POS trigra.m) OCR (thou'3 70% (90%) 74.4% 80% (9a%) 8~.0% ~),~% (98%) !)5.o% M~m:h l 84.6% ~)25% 96.0%,~)6.~% POSe Match2 Mateh3 8a.9% 83.1% 92.0% 90.6% 95.9% 95.6% 96.7% 95.9% ~The par~m/eters ~rre sc|ected considering the filet that the corre.ct candidate included r~ttc increases a.s the tirst candi(hm~ correct rate incrc~Lscs, a.nd that NOllle correct characters ~re l|ev(:r \[)resellt ill tile Illg-trix ewm if the first candidate correct,:~Lt(~ is high." ></td>
	<td class="line x" title="131:224	6.3 Word Segmentation and Word Correction Accuracy First, we deline the performance mea,sures of J apanese word segmentation and word correction." ></td>
	<td class="line x" title="132:224	We will think of' tile output of tile spelling eorrector ~ a set of 2-tuples, word segmentation and orthography." ></td>
	<td class="line x" title="133:224	We then compare tile tuples con tained in the system's output to tile tuptes contained in the standard analysis." ></td>
	<td class="line x" title="134:224	For tile N-best candidate, we will make the union of tile tuples contained in each candidate, in other words, we will make a word lattice from N-best candidates, and compare them to tile tuples in the standard." ></td>
	<td class="line x" title="135:224	For comparison, we count tile number of tuples in tile standard (Std), the number of tuples in the system output (Sys), and tile number of matching tuples (M)." ></td>
	<td class="line x" title="136:224	We' then calculate recall (M/Std) and precision (M/Sys) as accuracy measures." ></td>
	<td class="line x" title="137:224	We define two degrees of equality among tuples for counting the number of matching tuples." ></td>
	<td class="line x" title="138:224	For word segmentation accuracy, two tuples are equal if they have tile same word segmentation regard less of orthography." ></td>
	<td class="line x" title="139:224	For word correction accuracy, two tuples are equal if they have the same word segmentation and orthography." ></td>
	<td class="line x" title="140:224	Table 5 shows the words segmentation accuracy and word correction accuracy." ></td>
	<td class="line x" title="141:224	The word segmen ration accuracy of tile spelling eorrector is signitieantly high, even if the input is very noisy." ></td>
	<td class="line x" title="142:224	For example, when the accuracy of the baseline OCI." ></td>
	<td class="line x" title="143:224	is 80%, since tile a.verage numlmr of char acters and words in the test sentences are 20.1 and 11.3, there are 4.0 (=20.1'(1-0.80)) chm'actee errors in the sentence, in average." ></td>
	<td class="line x" title="144:224	Ilowever, 94.5% word segmentation recall means that there are only 0.62 (=11.3'(1-0.945)) word segmenta tions that are not found in the first candidate." ></td>
	<td class="line x" title="145:224	Moreover, we t>el the word correction accuracy in Table 3 is satisfactory \['or an interactive spelling corrector." ></td>
	<td class="line x" title="146:224	For example, when the accuracy of the b~seline OCI is 90%, there are 2.0 (=20.1'(1 0.90)) cha.racter errors in the test sentence, llow ever, 92.8% reca.ll for the first candidate and 95.6% recall for tile top 5 candidates means that there are only 0.81 (11.3'0-0.928)) words that are not found in the lirst candidate, and if you exa.mine the top 5 candidates, this wdue is reduced to 0.50 (~1.3'(1-0.9S@)." ></td>
	<td class="line x" title="147:224	That is, about half of the er rors in the lirst candidate are corrected by simply selecting tile alternatives in the word lattice." ></td>
	<td class="line x" title="148:224	7 Discussion Previous works oil Japanese OCR error correction are l)ased on either the character trigram model or tile part of speech t)igram model." ></td>
	<td class="line x" title="149:224	Their targets are printed characters, not handwritten characters." ></td>
	<td class="line x" title="150:224	That is, they assutne the underlying OCI.'s ac curacy is over 90%." ></td>
	<td class="line x" title="151:224	Moreover, their treatment of unknown words and short words is rather ad hoe." ></td>
	<td class="line x" title="152:224	810 'l'a,ble 3: Word Segmenta.tion Accura,cy a, nd Word (Jorrection Accuracy for Noisy Texts O(:11 7o% (9o%) 8o% (9~%) 9o% (:)8%) 95% (98%) Wor(l Segtnent;~tion R(x:M1 (llest-5) l)re<:ision (l\]est-5) 89.o% (9e.1%) ~.a% (752%) 94.5% (97.4%) 90.5% (81.7%) 96.a% (97.9%) 9a.(~% (85.s%) 97.3% (98.6%) !\]4.8% (86.8%) Wet(| (-',or re(:t h) n 1 c(:all (l\]est-5) P rc.(:ision (l}t:st-5 77.1% (82.4%) 71.a% (58.2% 87.9% (92.6%) 84.2% (67.2% !\]2.8% (95.6%) 90.1% (72.1% 94.a% (!\]7.0%) !}1.8% (74.0% ('l'Mmo and Nishino, 1989) used 1)~u't of speech bi gra, m a,nd best \[irsl, sea+rob for ()C,I, correction." ></td>
	<td class="line x" title="153:224	They used heuristic templal;es \[Lr ttnkllown words." ></td>
	<td class="line x" title="154:224	( 11;o a,nd M a,rtty,'tma,, 1.()92) used pa,rt of speech I)i graan a, nd \]lea,In search ill order to get, niultiple c,'mdidaJ, es in their int;eracl;ive 0(-:11, correcter r, The proposed Ja,paa\]ese spelling correction meLh.od uses pa,rt of speech trigra,m ;rod N best sea,reh, This (:oml>ina,l,ion is l, heoretica, lly a, nd pra,ctica,lly iilore ;l,CCtlr;l, Le (;\[liLII previous reel, hods." ></td>
	<td class="line x" title="155:224	In addition, t>y using sl,a,t;istiea,I word ntodel, a,nd cc)llteXt; I>a,sed n,l)lm)xin\]a,l,c word \[na, l, ch, il, t)e comes robust enottgh |;o }tm~dle very noisy texts, such a,s the ottl,put o\[' FAX O(111, systetns." ></td>
	<td class="line x" title="156:224	To improve the word correction a,ccuraey, more powerful hmgua,ge models, stteh as word bigram, are required." ></td>
	<td class="line x" title="157:224	(Jelinek, 1.(.)85) pointed out that 'I)()S (pa,rt of speech) elassiliea,tion is too crude a,nd not necessa,rily suited 1,o la+ngtutge modeling'." ></td>
	<td class="line x" title="158:224	Ilowever, il; is 1;c)o expensive to prepa, re a, la,rge m,~nua, lly segmented (:ort~,tts ()f e;tch l,a, rget do Ilia,ill L()(:O\[llpute the word 1)igra,m. 'l'her<q'ore, we a,re thinking o\[' ran,king a, set\[' orga,tfized word seg meni;aJ, ion method I)y generMizing the l'orwm'd Ibtekwa,rd a, lgoritlml \['or those hmgua,ges tha, t ha,ve no delimiter between words (Na,gaJ, a,, 199(i), 8 Conclusion We h;tve present;ed ~ spelling eorrecl, ion met,hod tbr noisy,la,pa,nese texts." ></td>
	<td class="line x" title="159:224	We a,re currently I>uilding a,n intera,ctive Ja,pa,nese spelling correc tor jspcll, where words are the I)msic object: ma." ></td>
	<td class="line x" title="160:224	nipuhtt, ed 1)y t, he user in ope\]'~l;ions such as re pla.ee, a,ceept, and edit." ></td>
	<td class="line x" title="161:224	It is something like the J a,pa,nese countert)a,rt of I Jnix's spelling correcter ispell, with a, user interf~tce similar to kan(t-loka'njZ converter, a, popu\[a,r J a,pa, nese inpul, method ~A(:<:ording to Fig." ></td>
	<td class="line x" title="162:224	6 ill (~\]'a,k;to and NMtim), 1989), they achieved iti)Olll, 95(~1 (:ha, ra<:tcr I'C~:Og, tlil, ioII &(:CIIr;t(:y when |,Ira I)ms('.llnr." ></td>
	<td class="line x" title="163:224	~L(:cllr,~l(:y iS 9\[% for ill;tga.7,ines ~tnd int, ro(\]ll(:lA)ry t(!xl,1)ooks of scien(:(!" ></td>
	<td class="line x" title="164:224	and t,e(:llnology dmu;tiu." ></td>
	<td class="line x" title="165:224	According to TM)Ic." ></td>
	<td class="line x" title="166:224	I in (11,o ~tnd Ma, ruya, tn~t, 1992), they itchicvcd 94.61%,<ha, la,<.:tcr I't?(:O,~,ll\[LiOll a+c(:tu'+t<:y when |,}it |)a, selinc ~tc(:ur+~<:y is 87.46% \[m' pal, elLS h, uhx:tri<: c.gilme.rlng, dora;tin." ></td>
	<td class="line x" title="167:224	We ~t(:hit:vcd 9fi.0% c\]l.+trltci, cr J'e(:og.il, ion +~(:c.ra(:y, when the I)+~ell.c a+(:c.r+~cy i~ 90% in thu cunf('.rcn<:c roy< istr~tLion doma.i It is very (liflic,lt to c,.)nlt)a+v.!" ></td>
	<td class="line x" title="168:224	our rusu\]ts with thu previous rcsUlll\[,~ I>(:ca, ust!" ></td>
	<td class="line x" title="169:224	t,\]l(', expurimerit <:onditio.s a, rc (:Oml)h:Lt:ly dill'rxenL." ></td>
	<td class="line x" title="170:224	for the AS(3l keyt>oaa'd." ></td>
	<td class="line x" title="171:224	|{e\['erel\] ces Kt!rltl(!th W." ></td>
	<td class="line x" title="172:224	(.~,l|llr(:h. 1988." ></td>
	<td class="line x" title="173:224	A Sto<:ha.sti(: P+u't~ Prog,,'am +u.l No.n Phra.se P~H's,.:I' for IJlll'(!s|,rlcIA'd q'ext,, \]n lJrocc(:dng.~ (ff A NI, tLSb;, t)~tges 136143." ></td>
	<td class="line x" title="174:224	I)oug (',utting, Julhul Kut)ie(:, J~ul Pudersen~, +tn(l IJcllelot)c Sibun." ></td>
	<td class="line x" title="175:224	1992." ></td>
	<td class="line x" title="176:224	A t)llajcth:+tl \[)a, nlt-o\[-Sl)eu(:h ~l'a+gger, In t)roc('+:ding.~ (ff A N1,1)-92, I)a+gcs 13;1140." ></td>
	<td class="line x" title="177:224	q'erumasa, lC}l;u% Kunl, aJu Ogura, Tsuyoshl Morimol,u. 19!}0." ></td>
	<td class="line x" title="178:224	ATI I)ia,\]oguc \[)atal>asu." ></td>
	<td class="line x" title="179:224	1, lq'o('ccdi. ,g.~ of ICSI, P, \[m;q, es 1(193-109(J. Wi\]li:-lln A." ></td>
	<td class="line x" title="180:224	(\]a\]l.~ a+nd I<r.n.etll W." ></td>
	<td class="line x" title="181:224	(~lillr(:h. 199(\]." ></td>
	<td class="line x" title="182:224	Pool l'~sthna, t, cs of C, ontcxt are Worse." ></td>
	<td class="line x" title="183:224	th;Ln Nora:." ></td>
	<td class="line x" title="184:224	lit Proccc(liT*ys of I)AHPA A'atur, I Lan(j'u,gc m*d St)etch Workshop, 1)+tgcs 28a-287." ></td>
	<td class="line x" title="185:224	M~trk D. I<crnigha, n, Kenneth W. Chur(:h, and Willi~un A." ></td>
	<td class="line x" title="186:224	(~th!." ></td>
	<td class="line x" title="187:224	1990." ></td>
	<td class="line x" title="188:224	A Spelling Correction Progr~un Based o. a, Noi.sy (-lh.utnel Model." ></td>
	<td class="line x" title="189:224	In l'roccc(lin(js of (701,1N(;-90, l)~tgus 2(-15-210." ></td>
	<td class="line x" title="190:224	K~u'r.n Kuki<:h. 111!12." ></td>
	<td class="line x" title="191:224	'l'ccllniqurs For Aut,omnth:a.lly Col'reeLing Words in Text." ></td>
	<td class="line x" title="192:224	A(:M (/omlmlin 9,%+rO(:g.'~ VoI.L)4, No.4, I)~t~,(:s ;\]77-4119." ></td>
	<td class="line x" title="193:224	Nol)uyms." ></td>
	<td class="line x" title="194:224	\[to ;lld l\[iroshi M :rilya, lllat." ></td>
	<td class="line x" title="195:224	19!12." ></td>
	<td class="line x" title="196:224	A, Method of \])u.te(:ting aim (hJrrecLing lt'rors in the tesults of Jitl)a+ne:.+e OCR." ></td>
	<td class="line x" title="197:224	In Tra.nsaclion o\] In\]otto+ilion Prof'?S,'fi~L~(J ~'OC+dll I of,/f+pfllZ, Vc)l.\],~, No.5, J)a.gC?S {JJ4670 (in J+q)+tnese)." ></td>
	<td class="line x" title="198:224	I're(h!rh:k Je.linek+ 1985." ></td>
	<td class="line x" title="199:224	Self-org+uli/,(!d l,a,.:e,u+~gc Mo(h!ling, \[o17 St)et!(:h Rtx:ognition." ></td>
	<td class="line x" title="200:224	IBM t,.:l)Od,." ></td>
	<td class="line x" title="201:224	I',rh: M+tys, I'rcd J. J)+uncratlt, aJM l/obt:rt I,." ></td>
	<td class="line x" title="202:224	Mcr(:er." ></td>
	<td class="line x" title="203:224	1991." ></td>
	<td class="line x" title="204:224	C, tmtcxt l}+Ls(!(l Spelling (',orn't!ctio /nJormalion l)ro(:cssing ~',; M,nagcmcnl, V.I. 27, Nt).5, I)~g(!s 517-522." ></td>
	<td class="line x" title="205:224	Masa~tki N +tgati,~t. 1!}!t4." ></td>
	<td class="line x" title="206:224	A S tO(:}U a,M,i(:,\];~\[)}ttlcSC Mort)hoh)glcal Amdyzer Using it l'orwa, r(l-l)l ~ l}ackw~r(1-/t* N-Best Se+tr(:h Algorithm." ></td>
	<td class="line x" title="207:224	In tocc(:dinys of (JOI,1N(7-9~, l)a#.e,s 201-207." ></td>
	<td class="line x" title="208:224	Ma,,,+t+tki N itgiLL+L 1996." ></td>
	<td class="line x" title="209:224	alui, onl~tti<: l~Xl, l';-:t(:lion of New Words from J~tl)+tnese q'exl, s u,,,ing Gc,uraJizcd I'orwa.r(1-1btckw~u'd ~qe~tr<:h. q'o att)l)(,.itr ill l)r.'wccd my.~ of EMNI,I'." ></td>
	<td class="line x" title="210:224	I'r~nk KK.,qt)onZ ~tnd l'\]n,e--l'oulg llu~tng." ></td>
	<td class="line x" title="212:224	1991." ></td>
	<td class="line x" title="213:224	A Ti'ccTrellis lbk,~ud l'+tst Suar<:h for I'indi,g the N l}usl Senten(:e. llyl)otheses h!" ></td>
	<td class="line x" title="214:224	(hmtinuous Speech Ie(:ognil, ion." ></td>
	<td class="line x" title="215:224	l. /)roccc<ling* of 1(7A SSILOI, I)a,g:sT05:7(IS. 'l'el,suya,su Ta, k~m +~l~(l FunlihiLo NiMfino." ></td>
	<td class="line x" title="216:224	1989." ></td>
	<td class="line x" title="217:224	lml)h> m(,.nt;tl, ion atit(I Ewdui~tlon of Post-processiug for Ja.l)aJl(!St! I)O(:lltncnL \]{.(!a,(l,.:u's." ></td>
	<td class="line x" title="218:224	lit ~}'nns.(:li,n* o/ hffo,'m++liou t'l'occ.'t,i?+fj,qo(:icly of Japeln, Vol.30, No.l I, l)a, ges 13!)4 1401 ((1.,\];Ltla.llt:Se )." ></td>
	<td class="line x" title="220:224	iol)erl, A. W;~z,ur +ul(I Miclut(!l .I. Fis(:her." ></td>
	<td class="line x" title="221:224	1974." ></td>
	<td class="line x" title="222:224	q'he ~l, rlng-l,tr.,qtring C, orrr.cl,ion Probh!m. In,h)mrud of thc,4UM, Vol.21, No.I, t)a+Zes 168-173." ></td>
	<td class="line x" title="224:224	811" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C96-2192
Tagging Spoken Language Using Written Language Statistics
Nivre, Joakim;Gronqvist, Leif;Gustafsson, Malin;Lager, Torbjorn;Sofkova Hashemi, Sylvana;"></td>
	<td class="line x" title="1:98	Tagging Spoken Language Using Written Language Statistics Joakim Nivre Leif Gr6nqvist Malin Gustafsson TorbjSrn Lager Sylvana Sofkova Dept. of Linguistics G5teborg University S-41298 GSteborg Sweden {j oakim, leifg, malin, lager, sylvana}@ling, gu." ></td>
	<td class="line x" title="2:98	se Abstract This paper reports on two experiments with a probabilistic part-of-speech tagger, trained on a tagged corpus of written Swedish, being used to tag a corpus of (transcribed) spoken Swedish." ></td>
	<td class="line x" title="3:98	The results indicate that with very little adaptations an accuracy rate of 85% can be achieved, with an accuracy rate for known words of 90%." ></td>
	<td class="line x" title="4:98	In addition, two different treatments of pauses were explored but with no significant gain in accuracy under either condition." ></td>
	<td class="line x" title="5:98	1 Introduction What happens when we take a probabilistic partof-speech tagger trained on written language and try to use it on spoken language transcriptions?" ></td>
	<td class="line x" title="6:98	The answer to this question is interesting from several points of view, some more practical and some more theoretically oriented." ></td>
	<td class="line x" title="7:98	From a practical point of view, it, is interesting to know how well a written language tagger can perform on spoken language, because it may save us a lot of work if we can reuse existing taggers instead of developing new ones for spoken language." ></td>
	<td class="line x" title="8:98	Front a more theoretical point of view, the results of such an experiment may tell us something about the ways in which the strncture of spoken language is different (or not so different;) from that of written language." ></td>
	<td class="line x" title="9:98	In this paper, we report on experimental work dealing with the part-of-speech tagging of a corpus of (transcribed) spoken Swedish." ></td>
	<td class="line x" title="10:98	The tagger used implements a standard probabilistic biclass model (see, e. g., (DeRose 1988)) trained on a tagged subset of the Stockhohn-Ume Corpus of written Swedish (Ejerhed et al 1992)." ></td>
	<td class="line x" title="11:98	Given that the transcriptions contain many modifications of standard orthography (in order to capture spoken language variants, reductions, etc)." ></td>
	<td class="line x" title="12:98	a special lexicon had to be developed to map spoken langnage variants onto their canonical written language forms." ></td>
	<td class="line x" title="13:98	In addition, a special tokenizer had to be developed to handle 'recta-symbols' in the transcriptions, such as markers for pauses, overlapping speech, inaudible speech, etc. One of the interesting issues in this context is what use (if any) should be made of information about panses, interruptions, etc. In the experiment reported here, we compare two different treatments of pauses and evaluate the performance of the tagger under these two different conditions." ></td>
	<td class="line x" title="14:98	2 Background 2.1 Probabilistie Part-of-speech Tagging The problem of (automatically) assigning parts of speech to words in context has received a lot of attention within computational corpus linguistics." ></td>
	<td class="line o" title="15:98	A variety of diffexent methods have been investigated, most of which fall into two broad classes:  Probabilistic methods, e. g." ></td>
	<td class="line oc" title="16:98	(DeRose 1988; Cutting et al 1992; Merialdo 1994)." ></td>
	<td class="line x" title="17:98	 Rule-based methods, e. g." ></td>
	<td class="line x" title="18:98	(Brodda 1982; Karlsson 1990; Koskennienfi 1990; Brill 1992)." ></td>
	<td class="line o" title="19:98	Probabilistic taggers have typically been implemented as hidden Markov models, using prohabilistic models with two kinds of' basic probabilities:  The lexical probability of seeing the word w given the part-of-speech t: P(w I t)." ></td>
	<td class="line x" title="20:98	 The contextual pwbability of seeing the part-of-speech ti given the context of n 1 parts-of-speech: P(ti I ti-(,~-,),,ti 1)." ></td>
	<td class="line o" title="21:98	Models of this kind are usually referred to as nclass models, the most common instances of which are the biclass (n = 2) and triclass (n = 3) models." ></td>
	<td class="line x" title="22:98	The lexical and contextual probabilities of an nclass tagger are usually estimated using one of two methods: ~ 1The terms 'RF training' and 'ML training' are taken from Merialdo 1994." ></td>
	<td class="line x" title="23:98	It should be pointed out, though, that the use of relative frequencies to estimate occurrence probabilities is also a case of maximmn likelihood estimation (MLE)." ></td>
	<td class="line x" title="24:98	1078  Relative l,Yequency (RF) training: Given a tagged training corpus, the i)rohabilities (:an be estimated with relative frequencies." ></td>
	<td class="line x" title="25:98	 Maxinnun Likelihood (ML) training: Given an untagged training corpus, the probabilities can be estimated using the Bauin-Welch algorithm (also known as tile Forward-Backward algorithin) (Baron 1972)." ></td>
	<td class="line x" title="26:98	Of these two methods, R.F training seelns to give better estilnations while t)eing more labor intensive (Merialdo 1994)." ></td>
	<td class="line x" title="27:98	With proper training, r> class taggers typically readt all accuracy rate of about 95% \['or English texts (Charniak 1993), and similar results have been reported for other languages such as lh'ench and Swedish (Chanod & Tapanainen 1995; Brants & Samuelsson 1995)." ></td>
	<td class="line x" title="28:98	2.2 Tagging Spoken Language Spoken language transcrit)tions are essentially a Mud of text, and can therefore be tagged with the methods used for otller kinds of text,." ></td>
	<td class="line x" title="29:98	IIowever, sin(:(; t, he transcription of spoken language is a fairly labor-intensive tasks, the availability of suitable training corpora is much more limitexl than for ordinary written texts." ></td>
	<td class="line x" title="30:98	One way to circuinvent this problem is to use taggers trained on written texts to tag spoken language also." ></td>
	<td class="line x" title="31:98	This has apparently been done successflllly for the spoken language part of the British National Corpus, using the CLAWS tagger (Garsi(te)." ></td>
	<td class="line x" title="32:98	However, the application of writte, n language taggers to spol(en language is not entirely unproblematic." ></td>
	<td class="line x" title="33:98	First of all, spoken language transcriptions are typically produced ill a different format and with different conventions than ordinary written texts." ></td>
	<td class="line x" title="34:98	For example, a transcription is likely to contain markers tbr pauses, (aspects of) t)rosody, overlapping speech, etc. Moreover, they do not usually contain the pun(:tuation marks found in ordinary texts." ></td>
	<td class="line x" title="35:98	This means that the application of a written language tagger to spoken language minimally requires a special tokenizer, i. e., a preprocessor segmenting the text into appropriate coding units (words)." ></td>
	<td class="line x" title="36:98	A second type of ditficulty arises from tile fact that spoken language is otten transcribed using non-standard orthograI)hy." ></td>
	<td class="line x" title="37:98	Even if no phonetic t;ranscrit)tion is used, most transcription eonvenlions support the use of modified orthography to capture typical features of st)oken language (such as gem instead of going, kinda instead of kind of, etc.)." ></td>
	<td class="line x" title="38:98	Thus, the application of a written language tagger to spoken language typically requires a special lexicon, mapt)ing spoken language variants onto their canonical written language forms, in addition to a special tokenizer." ></td>
	<td class="line x" title="39:98	The problems considered so far may be seen as problems of a practical nature, but there is also a more filndmnentat problem with tile use of written language statistics to analyze spoken language, namely that the probability estimates derived from written language may not be rcpresentative for spoken language." ></td>
	<td class="line x" title="40:98	In the extreme case, some st)oken language phenomena (such as hesitation markers) Inay l)e (nearly) non-existent; in written language." ></td>
	<td class="line x" title="41:98	But even for words and collocations that occur both ill written and ill spoken language, t;he occurrence probabilities may vary greatly between tile two media." ></td>
	<td class="line x" title="42:98	How riffs affects the performance of taggers and what methods can be use(l to over(;olne or circunlvent tile I)rol)lems m-e issues that, surprisingly, do not seem to have t)een discussed in the literature at all." ></td>
	<td class="line x" title="43:98	The I)resent paper can be seen as a first attempt to ext)lore this area." ></td>
	<td class="line x" title="44:98	2.3 Tagging Swedish As far as we know, the methods for mltomatic part-of-speech tagging have not before been applied (;o (transcribed) spoken Swedish." ></td>
	<td class="line x" title="45:98	For written Swedish, there are a few tagge, d corpora availat)le, such as the Teleman tort)us (see, e. g., (Brants &, Samuelsson 1995)) and the StockholnlUrneh Corpus (Ejerhed et al 1992)." ></td>
	<td class="line x" title="46:98	A subpart of tim latter has been used as training dal;a in the experiments reported t)elow." ></td>
	<td class="line x" title="47:98	3 Method 3.1 The Tagger The tagger used fl)r tile experiments is a standard ItMM tagger using tile Viterbi algorithm to calculate the most probable sequence of parts-ofspee(:h for each string of words actor(ling to the following prol)al)ilistic t)iclass modeh (1) l'(,,,1,,,,,,~,t,,,~,,,) = P(t,)P(w, Itt)II'~ 2 P(til t/--1)I)(WJ I*,~) The tagger is coupled with a tokenizer that segments a transcription into utterances (strings of words), that are fed to the tagger one by one." ></td>
	<td class="line x" title="48:98	Besides ordinary words, the utterances may also contain markers for pauses and inaudibh: stretches of speech." ></td>
	<td class="line x" title="49:98	~ 3.2 Training the Tagger Tile lexical and contextual probabilities were estimated with relative frequencies ill a tagged corpus of written Swedish, a subpart of the StockholmUme' Cortms (SUC) containing 122,377 word tokens (1.8,343 word types)." ></td>
	<td class="line x" title="50:98	Tile tagset included 27 parts-of-speech." ></td>
	<td class="line x" title="51:98	3 2Tile original transcriptions also contain inibrmation about overlapping speech, marking of certain aspects of prosody, and various colninmlts." ></td>
	<td class="line x" title="52:98	This information is currently disregarded by the tokenizer." ></td>
	<td class="line x" title="53:98	3For a lnore detailed description of the linguistic annotation system of the Stockhohn-Ume Cort)us, see (Ejerhed et al 1992)." ></td>
	<td class="line x" title="54:98	1079 3.3 The Spoken Language Lexicon As noted earlier, the spoken language transcriptions contain many deviations fl'om standard orthography." ></td>
	<td class="line x" title="55:98	Therefore, in order to inake optimal use of tile written language statistics, a special lexicon is required to map spoken language variants onto their canonical written forms." ></td>
	<td class="line x" title="56:98	For the present experiments we have developed a lexicon covering 2113 spoken language variants (which are mapped onto 1764 written language forms)." ></td>
	<td class="line x" title="57:98	We know, however, that this lexicon has less than total coverage and that many regular spoken language reductions are not currently covered." ></td>
	<td class="line x" title="58:98	4 3.4 Unknown Words and Collocations The occurrence of 'unknown words', i. e., words not occurring in the training corpus, is a notorious problem in (probabilistic) part-of-speech tagging." ></td>
	<td class="line x" title="59:98	In our case, this problem is even more serious, since we know beforehand that some words will be treated as unknown although they do in fact occur in the training corpus (because of deviations Dom standard orthography)." ></td>
	<td class="line x" title="60:98	In the experiments reported below, we have allowed unknown words to belong to any part-of-speech (which is possible in the given context), but with different weightings for different parts-of-speech." ></td>
	<td class="line x" title="61:98	More precisely, when a word cannot be found in the lexicon, we replace the product in (2) (cf.equation 1 above) with the product in (3), where TTR(ti) is the typetoken ratio of ti (in the training corpus)." ></td>
	<td class="line x" title="63:98	(2) p(t I I td (3) P(t I t_l)P(ti)TTR(t) In this way, we favor parts-of-speech with high probability and high type-token ratio." ></td>
	<td class="line x" title="64:98	In practice, this favors open classes (such as nouns, verbs, adjectives) over closed classes (determiners, conjunctions, etc.), and more frequent ones (e. g., nouns) over less frequent ones (e. g., adjectives)." ></td>
	<td class="line x" title="65:98	In addition to 'unknown words', we have to deal with 'unknown collocations', i. e., biclasses that do not occur in the training data." ></td>
	<td class="line x" title="66:98	If these biclasses are simply assigned zero probability, then in tile extreme case a word which is in the lexicon may fail to get a tag because the contextual probabilities of all its known parts-of-speech are zero in the given context." ></td>
	<td class="line x" title="67:98	In order to prevent this, we use the following formula to assign contextual probabilities to unknown collocations: (4) P(ti l t_l) = P(ti)K The constant K is chosen in such a way that tile contextual probabilities defined by equation (4) are significantly lower than the 'real' contextual probabilities derived from the training corpus, so 4A common example is the ending -igt, which appears in many adjectives (neuter singular) and adverbs and which is usually reduced to -it in ordinary speech." ></td>
	<td class="line x" title="68:98	that they only come into play when no known collocation is possible." ></td>
	<td class="line x" title="69:98	3.5 Pauses and Inaudible Speech As indicated earlier, the utterances to be tagged included markers for pauses and inaudible speech, since these were thought to contain information relevant for tile tagging process." ></td>
	<td class="line x" title="70:98	The symbol for inaudible (and therethre untranscribed) speech () -was simply added to the lexicon and assigned the 't)art-of-speech' major delimiter (mad), which is the category assigned to full stops, etc. in written texts." ></td>
	<td class="line x" title="71:98	The result is that the tagger will not treat the last, word before tile untranscribed passage as immediate context for tile first word after tile passage." ></td>
	<td class="line x" title="72:98	For pintoes we have experimented with two different treatments, which are compared below." ></td>
	<td class="line x" title="73:98	We refer to these different treatments as tagging condition 1 and 2, respectively:  Condition 1: Pauses are simply ignored in tile tagging process, which means that the last word before a pause is treated as immediate context for the first word after the pause." ></td>
	<td class="line x" title="74:98	 Condition 2: Pause symbols are added to the lexicon, where short pauses are categorized as minor delimiters (mid) (commas, etc.), while long pauses are categorized as mad (fllll stops, etc.), which means that the contextual probabilities of words occurring before and after pauses in spoken language will be modelled on the probabilities of words occurring before and after certain punctuation marks in written language." ></td>
	<td class="line x" title="75:98	It was hypothesized that, in certain cases, the tagger might perform better under condition 2, since pauses in spoken language often though by no means always indicate major phrase boundaries or even breaks in the grammatical structure." ></td>
	<td class="line x" title="76:98	3.6 Test Corpus The test corpus was composed of a set of 47 utterances, chosen randomly from a corpus of transcribed spoken Swedish containing 267,206 words." ></td>
	<td class="line x" title="77:98	The utterance length varied from 1 word to 688 words (not counting pauses as words), with a mean length of 29 words." ></td>
	<td class="line x" title="78:98	The test corpus contained 1360 word tokens and 498 word types." ></td>
	<td class="line x" title="79:98	4 Results The number of correctly tagged word tokens under condition 1 was 1153 out of a total of 1360, i. e., 84.8./o. The results for condition 2 were slightly better: 1248/1457 = 85.7%." ></td>
	<td class="line x" title="80:98	However, the latter figures also include the tagged imuses, for which only one category was possible." ></td>
	<td class="line x" title="81:98	If these tokens are subtracted, the results for condition 2 are: 1151/1360 = 84.6%." ></td>
	<td class="line x" title="82:98	1080 5 Discussion The overall ac(:uracy rate for the I,agger is al'Omld 85%, which is not too imi)ressive wh(m (:oInl)m'e(| to the results reporte, d for writt;cn laitguage." ></td>
	<td class="line x" title="83:98	However, if we take a closer look at the results, it; seems that an imt)ortant source of error is the lack of coverage of the, lexicon m,t the training corpus." ></td>
	<td class="line x" title="84:98	Of the |;we lmndred or so errors made 1)y the tagger, more than eighty con(:ern tokens that could not be matched with any word form occurring in the training corpus." ></td>
	<td class="line x" title="85:98	The most; common tyt)e of error in this class is that a word is (~rroneously tagge, d as a noun." ></td>
	<td class="line x" title="86:98	\[t is likely that this is an artifact of the way we assign lexical prol)abilities to unknown words and that a more Sol)histi(:ated method may lint)rove the results for this class of words." ></td>
	<td class="line x" title="87:98	More importantly, though, if we only (:oilsi(ler the resuits for words that were known to the tagger, the accuracy rate goes up to about 90%, mid most of the errors relnailfii~g concern classes that are notoriously difficult even un(ter norlnal cir(:umstmLces, such as adverbs vs verb particles and prepositions vs sut)ordinating conjunctions." ></td>
	<td class="line x" title="88:98	Taken togedmr, these results seen~ to indicate that with a more e.xtensive lexicon, a larger training corpus of written language, and l)erhat)s a more sot)histi(:ated treatment of mtknown words, it should |)e possible to el)Cain results al)proa<',hing those, ()I>taine<l for written language." ></td>
	<td class="line x" title="89:98	As regards the two treatments ()\[' \[)allses, the results are virtually identi(:al in terms of overall accuracy rate." ></td>
	<td class="line x" title="90:98	If we look at individual words, however, we find that the part-of-st)eech assignillellt differs in 25 cases, hi 10 of these (:ases, the corrc(:t part-of-st)eech is assigned under condition 1; in 9 cases, the corre, ct ttLg is tbund under (:ondition 2; ittl(t in 6 cases, l)oth conditions yield an incorrect assignlnent." ></td>
	<td class="line x" title="91:98	The conclusion to draw from the.se results is i)robably that the." ></td>
	<td class="line x" title="92:98	tre&tmcnt of pauses as delimiters yields it t)etter analysis in cases where the pause, marks an interruption or major phrase t)omldary, while it is better t() ignore pauses when they do iloi-, mark any break in grmnlnatical structure." ></td>
	<td class="line x" title="93:98	Unfortunately, these two tyl)eS of t)auses seem to 1)e equally (:ommon, whi(:h means that neither treatment results in any gain in overall accuracy." ></td>
	<td class="line x" title="94:98	However, preliminary observations seem to in(ticate thai, it may be possible to get better results if a more line-grained analysis o\[' t)ause length is taken into account." ></td>
	<td class="line x" title="95:98	This pre-supposes, of course, that lifts kind of informal;ion is available in the transcriptions." ></td>
	<td class="line x" title="96:98	6 Conclusion in this I)aper we, have ret)orted on an experiinent using a probabilistic part-of-speech tagger trained on written language to analyze (transcril)ed) spoken language." ></td>
	<td class="line x" title="97:98	The results indicate that, with little or no adaptations, an overall accuracy rate of 85o/o c:ml 1)e a,chio, vcd, with ~1,i1 itC(;llFO, cy r;~te of 90% \['()r known words." ></td>
	<td class="line x" title="98:98	()n the negative side, we, found that the treatment of pauses as delimiters (a,s ot)t)osed to siml)ly ignoring them) did not result in a 1)ctlx!r performance, of the tagger." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P96-1006
Integrating Multiple Knowledge Sources To Disambiguate Word Sense: An Exemplar-Based Approach
Ng, Hwee Tou;Lee, Hian Beng;"></td>
	<td class="line x" title="1:198	Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-Based Approach Hwee Tou Ng Defence Science Organisation 20 Science Park Drive Singapore 118230 nhweet outrantor, dso." ></td>
	<td class="line x" title="2:198	gov. sg Hian Beng Lee Defence Science Organisation 20 Science Park Drive Singapore 118230 lhianben@trant or." ></td>
	<td class="line x" title="3:198	dso." ></td>
	<td class="line x" title="4:198	gov. sg Abstract In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm." ></td>
	<td class="line x" title="5:198	This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation." ></td>
	<td class="line x" title="6:198	We tested our WSD program, named LEXAS, on both a common data set used in previous work, as well as on a large sense-tagged corpus that we separately constructed." ></td>
	<td class="line x" title="7:198	LEXAS achieves a higher accuracy on the common data set, and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus tagged with the refined senses of WoRDNET." ></td>
	<td class="line x" title="8:198	1 Introduction One important problem of Natural Language Processing (NLP) is figuring out what a word means when it is used in a particular context." ></td>
	<td class="line x" title="9:198	The different meanings of a word are listed as its various senses in a dictionary." ></td>
	<td class="line x" title="10:198	The task of Word Sense Disambiguation (WSD) is to identify the correct sense of a word in context." ></td>
	<td class="line x" title="11:198	Improvement in the accuracy of identifying the correct word sense will result in better machine translation systems, information retrieval systems, etc. For example, in machine translation, knowing the correct word sense helps to select the appropriate target words to use in order to translate into a target language." ></td>
	<td class="line x" title="12:198	In this paper, we present a new approach for WSD using an exemplar-based learning algorithm." ></td>
	<td class="line x" title="13:198	This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech (POS) of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation." ></td>
	<td class="line x" title="14:198	To evaluate our WSD program, named LEXAS (LEXical Ambiguity-resolving _System), we tested it on a common data set involving the noun 'interest' used by Bruce and Wiebe (Bruce and Wiebe, 1994)." ></td>
	<td class="line x" title="15:198	LEXAS achieves a mean accuracy of 87.4% on this data set, which is higher than the accuracy of 78% reported in (Bruce and Wiebe, 1994)." ></td>
	<td class="line x" title="16:198	Moreover, to test the scalability of LEXAS, we have acquired a corpus in which 192,800 word occurrences have been manually tagged with senses from WORDNET, which is a public domain lexical database containing about 95,000 word forms and 70,000 lexical concepts (Miller, 1990)." ></td>
	<td class="line x" title="17:198	These sense tagged word occurrences consist of 191 most frequently occurring and most ambiguous nouns and verbs." ></td>
	<td class="line x" title="18:198	When tested on this large data set, LEXAS performs better than the default strategy of picking the most frequent sense." ></td>
	<td class="line x" title="19:198	To our knowledge, this is the first time that a WSD program has been tested on such a large scale, and yielding results better than the most frequent heuristic on highly ambiguous words with the refined sense distinctions of WOttDNET." ></td>
	<td class="line x" title="20:198	2 Task Description The input to a WSD program consists of unrestricted, real-world English sentences." ></td>
	<td class="line x" title="21:198	In the output, each word occurrence w is tagged with its correct sense (according to the context) in the form of a sense number i, where i corresponds to the i-th sense definition of w as given in some dictionary." ></td>
	<td class="line x" title="22:198	The choice of which sense definitions to use (and according to which dictionary) is agreed upon in advance." ></td>
	<td class="line x" title="23:198	For our work, we use the sense definitions as given in WORDNET, which is comparable to a good desktop printed dictionary in its coverage and sense distinction." ></td>
	<td class="line x" title="24:198	Since WO DNET only provides sense definitions for content words, (i.e. , words in the parts of speech (POS) noun, verb, adjective, and adverb), LEXAS is only concerned with disambiguating the sense of content words." ></td>
	<td class="line x" title="25:198	However, almost all existing work in WSD deals only with disambiguating content words too." ></td>
	<td class="line x" title="26:198	LEXAS assumes that each word in an input sen40 tence has been pre-tagged with its correct POS, so that the possible senses to consider for a content word w are only those associated with the particular POS of w in the sentence." ></td>
	<td class="line x" title="27:198	For instance, given the sentence 'A reduction of principal and interest is one way the problem may be solved.', since the word 'interest' appears as a noun in this sentence, LEXAS will only consider the noun senses of 'interest' but not its verb senses." ></td>
	<td class="line x" title="28:198	That is, LEXAS is only concerned with disambiguating senses of a word in a given POS." ></td>
	<td class="line pc" title="29:198	Making such an assumption is reasonable since POS taggers that can achieve accuracy of 96% are readily available to assign POS to unrestricted English sentences (Brill, 1992; Cutting et al. , 1992)." ></td>
	<td class="line x" title="30:198	In addition, sense definitions are only available for root words in a dictionary." ></td>
	<td class="line x" title="31:198	These are words that are not morphologically inflected, such as 'interest' (as opposed to the plural form 'interests'), 'fall' (as opposed to the other inflected forms like 'fell', 'fallen', 'falling', 'falls'), etc. The sense of a morphologically inflected content word is the sense of its uninflected form." ></td>
	<td class="line x" title="32:198	LEXAS follows this convention by first converting each word in an input sentence into its morphological root using the morphological analyzer of WORD NET, before assigning the appropriate word sense to the root form." ></td>
	<td class="line x" title="33:198	3 Algorithm LEXAS performs WSD by first learning from a training corpus of sentences in which words have been pre-tagged with their correct senses." ></td>
	<td class="line x" title="34:198	That is, it uses supervised learning, in particular exemplar-based learning, to achieve WSD." ></td>
	<td class="line x" title="35:198	Our approach has been fully implemented in the program LExAs." ></td>
	<td class="line x" title="36:198	Part of the implementation uses PEBLS (Cost and Salzberg, 1993; Rachlin and Salzberg, 1993), a public domain exemplar-based learning system." ></td>
	<td class="line x" title="37:198	LEXAS builds one exemplar-based classifier for each content word w. It operates in two phases: training phase and test phase." ></td>
	<td class="line x" title="38:198	In the training phase, LEXAS is given a set S of sentences in the training corpus in which sense-tagged occurrences of w appear." ></td>
	<td class="line x" title="39:198	For each training sentence with an occurrence of w, LEXAS extracts the parts of speech (POS) of words surrounding w, the morphological form of w, the words that frequently co-occur with w in the same sentence, and the local collocations containing w. For disambiguating a noun w, the verb which takes the current noun w as the object is also identified." ></td>
	<td class="line x" title="40:198	This set of values form the features of an example, with one training sentence contributing one training example." ></td>
	<td class="line x" title="41:198	Subsequently, in the test phase, LEXAS is given new, previously unseen sentences." ></td>
	<td class="line x" title="42:198	For a new sentence containing the word w, LI~XAS extracts from the new sentence the values for the same set of features, including parts of speech of words surround41 ing w, the morphological form of w, the frequently co-occurring words surrounding w, the local collocations containing w, and the verb that takes w as an object (for the case when w is a noun)." ></td>
	<td class="line x" title="43:198	These values form the features of a test example." ></td>
	<td class="line x" title="44:198	This test example is then compared to every training example." ></td>
	<td class="line x" title="45:198	The sense of word w in the test example is the sense of w in the closest matching training example, where there is a precise, computational definition of 'closest match' as explained later." ></td>
	<td class="line x" title="46:198	3.1 Feature Extraction The first step of the algorithm is to extract a set F of features such that each sentence containing an occurrence of w will form a training example supplying the necessary values for the set F of features." ></td>
	<td class="line x" title="47:198	Specifically, LEXAS uses the following set of features to form a training example: L3, L2, LI, 1~i, R2, R3, M, KI,." ></td>
	<td class="line x" title="48:198	., Kin, el,, 69, V 3.1.1 Part of Speech and Morphological Form The value of feature Li is the part of speech (POS) of the word i-th position to the left of w. The value of Ri is the POS of the word i-th position to the right of w. Feature M denotes the morphological form of w in the sentence s. For a noun, the value for this feature is either singular or plural; for a verb, the value is one of infinitive (as in the uninflected form of a verb like 'fall'), present-third-person-singular (as in 'falls'), past (as in 'fell'), present-participle (as in 'falling') or past-participle (as in 'fallen')." ></td>
	<td class="line x" title="49:198	3.1.2 Unordered Set of Surrounding Words Kt,  ., Km are features corresponding to a set of keywords that frequently co-occur with word w in the same sentence." ></td>
	<td class="line x" title="51:198	For a sentence s, the value of feature Ki is one if the keyword It'~ appears somewhere in sentence s, else the value of Ki is zero." ></td>
	<td class="line x" title="52:198	The set of keywords K1,, Km are determined based on conditional probability." ></td>
	<td class="line x" title="53:198	All the word tokens other than the word occurrence w in a sentence s are candidates for consideration as keywords." ></td>
	<td class="line x" title="54:198	These tokens are converted to lower case form before being considered as candidates for keywords." ></td>
	<td class="line x" title="55:198	Let cp(ilk ) denotes the conditional probability of sense i of w given keyword k, where Ni,k cp(ilk) = N~ Nk is the number of sentences in which keyword k cooccurs with w, and Ni,k is the number of sentences in which keyword k co-occurs with w where w has sense i. For a keyword k to be selected as a feature, it must satisfy the following criteria: 1." ></td>
	<td class="line x" title="56:198	cp(ilk ) >_ Mi for some sense i, where M1 is some predefined minimum probability." ></td>
	<td class="line x" title="57:198	2." ></td>
	<td class="line x" title="58:198	The keyword k must occur at least M2 times in some sense i, where /1//2 is some predefined minimum value." ></td>
	<td class="line x" title="59:198	3." ></td>
	<td class="line x" title="60:198	Select at most M3 number of keywords for a given sense i if the number of keywords satisfying the first two criteria for a given sense i exceeds M3." ></td>
	<td class="line x" title="61:198	In this case, keywords that co-occur more frequently (in terms of absolute frequency) with sense i of word w are selected over those co-occurring less frequently." ></td>
	<td class="line x" title="62:198	Condition 1 ensures that a selected keyword is indicative of some sense i of w since cp(ilk) is at least some minimum probability M1." ></td>
	<td class="line x" title="63:198	Condition 2 reduces the possibility of selecting a keyword based on spurious occurrence." ></td>
	<td class="line x" title="64:198	Condition 3 prefers keywords that co-occur more frequently if there is a large number of eligible keywords." ></td>
	<td class="line x" title="65:198	For example, M1 = 0.8, Ms = 5, M3 = 5 when LEXAS was tested on the common data set reported in Section 4.1." ></td>
	<td class="line x" title="66:198	To illustrate, when disambiguating the noun 'interest', some of the selected keywords are: expressed, acquiring, great, attracted, expressions, pursue, best, conflict, served, short, minority, rates, rate, bonds, lower, payments." ></td>
	<td class="line x" title="67:198	3.1.3 Local Collocations Local collocations are common expressions containing the word to be disambiguated." ></td>
	<td class="line x" title="68:198	For our purpose, the term collocation does not imply idiomatic usage, just words that are frequently adjacent to the word to be disambiguated." ></td>
	<td class="line x" title="69:198	Examples of local collocations of the noun 'interest' include 'in the interest of', 'principal and interest', etc. When a word to be disambiguated occurs as part of a collocation, its sense can be frequently determined very reliably." ></td>
	<td class="line x" title="70:198	For example, the collocation 'in the interest of' always implies the 'advantage, advancement, favor' sense of the noun 'interest'." ></td>
	<td class="line x" title="71:198	Note that the method for extraction of keywords that we described earlier will fail to find the words 'in', 'the', 'of' as keywords, since these words will appear in many different positions in a sentence for many senses of the noun 'interest'." ></td>
	<td class="line x" title="72:198	It is only when these words appear in the exact order 'in the interest of' around the noun 'interest' that strongly implies the 'advantage, advancement, favor' sense." ></td>
	<td class="line x" title="73:198	There are nine features related to collocations in an example." ></td>
	<td class="line x" title="74:198	Table 1 lists the nine features and some collocation examples for the noun 'interest'." ></td>
	<td class="line x" title="75:198	For example, the feature with left offset = -2 and right offset = 1 refers to the possible collocations beginning at the word two positions to the left of 'interest' and ending at the word one position to the right of 'interest'." ></td>
	<td class="line x" title="76:198	An example of such a collocation is 'in the interest of'." ></td>
	<td class="line x" title="77:198	The method for extraction of local collocations is similar to that for extraction of keywords." ></td>
	<td class="line x" title="78:198	For each 42 Left Offset Right Offset Collocation Example -1 -1 accrued interest 1 1 interest rate -2 -1 principal and interest -1 1 national interest in 1 2 interest and dividends -3 -1 sale of an interest -2 in the interest of -1 2 an interest in a 1 3 interest on the bonds Table 1: Features for Collocations of the nine collocation features, LEXAS concatenates the words between the left and right offset positions." ></td>
	<td class="line x" title="79:198	Using similar conditional probability criteria for the selection of keywords, collocations that are predictive of a certain sense are selected to form the possible values for a collocation feature." ></td>
	<td class="line x" title="80:198	3.1.4 Verb-Object Syntactic Relation LEXAS also makes use of the verb-object syntactic relation as one feature V for the disambiguation of nouns." ></td>
	<td class="line x" title="81:198	If a noun to be disambiguated is the head of a noun group, as indicated by its last position in a noun group bracketing, and if the word immediately preceding the opening noun group bracketing is a verb, LEXAS takes such a verb-noun pair to be in a verb-object syntactic relation." ></td>
	<td class="line x" title="82:198	Again, using similar conditional probability criteria for the selection of keywords, verbs that are predictive of a certain sense of the noun to be disambiguated are selected to form the possible values for this verb-object feature V. Since our training and test sentences come with noun group bracketing, determining verb-object relation using the above heuristic can be readily done." ></td>
	<td class="line x" title="83:198	In future work, we plan to incorporate more syntactic relations including subject-verb, and adjectiveheadnoun relations." ></td>
	<td class="line x" title="84:198	We also plan to use verbobject and subject-verb relations to disambiguate verb senses." ></td>
	<td class="line x" title="85:198	3.2 Training and Testing The heart of exemplar-based learning is a measure of the similarity, or distance, between two examples." ></td>
	<td class="line x" title="86:198	If the distance between two examples is small, then the two examples are similar." ></td>
	<td class="line x" title="87:198	We use the following definition of distance between two symbolic values vl and v2 of a feature f: e(vl, v2) = I c1' cl c2, c. I i=1 Cl,i is the number of training examples with value vl for feature f that is classified as sense i in the training corpus, and C1 is the number of training examples with value vl for feature f in any sense." ></td>
	<td class="line x" title="88:198	C2,i and C2 denote similar quantities for value v2 of feature f. n is the total number of senses for a word W. This metric for measuring distance is adopted from (Cost and Salzberg, 1993), which in turn is adapted from the value difference metric of the earlier work of (Stanfill and Waltz, 1986)." ></td>
	<td class="line x" title="89:198	The distance between two examples is the sum of the distances between the values of all the features of the two examples." ></td>
	<td class="line x" title="90:198	During the training phase, the appropriate set of features is extracted based on the method described in Section 3.1." ></td>
	<td class="line x" title="91:198	From the training examples formed, the distance between any two values for a feature f is computed based on the above formula." ></td>
	<td class="line x" title="92:198	During the test phase, a test example is compared against allthe training examples." ></td>
	<td class="line x" title="93:198	LEXAS then determines the closest matching training example as the one with the minimum distance to the test example." ></td>
	<td class="line x" title="94:198	The sense of w in the test example is the sense of w in this closest matching training example." ></td>
	<td class="line x" title="95:198	If there is a tie among several training examples with the same minimum distance to the test example, LEXAS randomly selects one of these training examples as the closet matching training example in order to break the tie." ></td>
	<td class="line x" title="96:198	4 Evaluation To evaluate the performance of LEXAS, we conducted two tests, one on a common data set used in (Bruce and Wiebe, 1994), and another on a larger data set that we separately collected." ></td>
	<td class="line x" title="97:198	4.1 Evaluation on a Common Data Set To our knowledge, very few of the existing work on WSD has been tested and compared on a common data set." ></td>
	<td class="line x" title="98:198	This is in contrast to established practice in the machine learning community." ></td>
	<td class="line x" title="99:198	This is partly because there are not many common data sets publicly available for testing WSD programs." ></td>
	<td class="line x" title="100:198	One exception is the sense-tagged data set used in (Bruce and Wiebe, 1994), which has been made available in the public domain by Bruce and Wiebe." ></td>
	<td class="line x" title="101:198	This data set consists of 2369 sentences each containing an occurrence of the noun 'interest' (or its plural form 'interests') with its correct sense manually tagged." ></td>
	<td class="line x" title="102:198	The noun 'interest' occurs in six different senses in this data set." ></td>
	<td class="line x" title="103:198	Table 2 shows the distribution of sense tags from the data set that we obtained." ></td>
	<td class="line x" title="104:198	Note that the sense definitions used in this data set are those from Longman Dictionary of Contemporary English (LDOCE) (Procter, 1978)." ></td>
	<td class="line x" title="105:198	This does not pose any problem for LEXAS, since LEXAS only requires that there be a division of senses into different classes, regardless of how the sense classes are defined or numbered." ></td>
	<td class="line x" title="106:198	POS of words are given in the data set, as well as the bracketings of noun groups." ></td>
	<td class="line x" title="107:198	These are used to determine the POS of neighboring words and the LDOCE sense Frequency Percent 1: readiness to give 361 15% attention 2: quality of causing 11 <1% attention to be given 3: activity, subject, etc. 67 3% which one gives time and attention to 178 4: advantage, advancement, or favor 5: a share (in a company, business, etc)." ></td>
	<td class="line x" title="108:198	499 6: money paid for the use 1253 of money 8% 21% 53% Table 2: Distribution of Sense Tags verb-object syntactic relation to form the features of examples." ></td>
	<td class="line x" title="109:198	In the results reported in (Bruce and Wiebe, 1994), they used a test set of 600 randomly selected sentences from the 2369 sentences." ></td>
	<td class="line x" title="110:198	Unfortunately, in the data set made available in the public domain, there is no indication of which sentences are used as test sentences." ></td>
	<td class="line x" title="111:198	As such, we conducted 100 random trials, and in each trial, 600 sentences were randomly selected to form the test set." ></td>
	<td class="line x" title="112:198	LEXAS is trained on the remaining 1769 sentences, and then tested on a separate test set of sentences in each trial." ></td>
	<td class="line x" title="113:198	Note that in Bruce and Wiebe's test run, the proportion of sentences in each sense in the test set is approximately equal to their proportion in the whole data set." ></td>
	<td class="line x" title="114:198	Since we use random selection of test sentences, the proportion of each sense in our test set is also approximately equal to their proportion in the whole data set in our random trials." ></td>
	<td class="line x" title="115:198	The average accuracy of LEXAS over 100 random trials is 87.4%, and the standard deviation is 1.37%." ></td>
	<td class="line x" title="116:198	In each of our 100 random trials, the accuracy of LEXAS is always higher than the accuracy of 78% reported in (Bruce and Wiebe, 1994)." ></td>
	<td class="line x" title="117:198	Bruce and Wiebe also performed a separate test by using a subset of the 'interest' data set with only 4 senses (sense 1, 4, 5, and 6), so as to compare their results with previous work on WSD (Black, 1988; Zernik, 1990; Yarowsky, 1992), which were tested on 4 senses of the noun 'interest'." ></td>
	<td class="line x" title="118:198	However, the work of (Black, 1988; Zernik, 1990; Yarowsky, 1992) were not based on the present set of sentences, so the comparison is only suggestive." ></td>
	<td class="line x" title="119:198	We reproduced in Table 3 the results of past work as well as the classification accuracy of LEXAS, which is 89.9% with a standard deviation of 1.09% over 100 random trials." ></td>
	<td class="line x" title="120:198	In summary, when tested on the noun 'interest', LEXAS gives higher classification accuracy than previous work on WSD." ></td>
	<td class="line x" title="121:198	In order to evaluate the relative contribution of the knowledge sources, including (1) POS and mor43 WSD research Accuracy Black (1988) 72% Zernik (1990) 70% Yarowsky (1992) 72% Bruce & Wiebe (1994) 79% LEXhS (1996) 89% Table 3: Comparison with previous results Knowledge Source POS & morpho surrounding words collocations verb-object Mean Accuracy 77.2% 62.0% 80.2% 43.5% Std Dev 1.44% 1.82% 1.55% 1.79% Table 4: Relative Contribution of Knowledge Sources phological form; (2) unordered set of surrounding words; (3) local collocations; and (4) verb to the left (verb-object syntactic relation), we conducted 4 separate runs of 100 random trials each." ></td>
	<td class="line x" title="122:198	In each run, we utilized only one knowledge source and compute the average classification accuracy and the standard deviation." ></td>
	<td class="line x" title="123:198	The results are given in Table 4." ></td>
	<td class="line x" title="124:198	Local collocation knowledge yields the highest accuracy, followed by POS and morphological form." ></td>
	<td class="line x" title="125:198	Surrounding words give lower accuracy, perhaps because in our work, only the current sentence forms the surrounding context, which averages about 20 words." ></td>
	<td class="line x" title="126:198	Previous work on using the unordered set of surrounding words have used a much larger window, such as the 100-word window of (Yarowsky, 1992), and the 2-sentence context of (Leacock et al. , 1993)." ></td>
	<td class="line x" title="127:198	Verb-object syntactic relation is the weakest knowledge source." ></td>
	<td class="line x" title="128:198	Our experimental finding, that local collocations are the most predictive, agrees with past observation that humans need a narrow window of only a few words to perform WSD (Choueka and Lusignan, 1985)." ></td>
	<td class="line x" title="129:198	The processing speed of LEXAS is satisfactory." ></td>
	<td class="line x" title="130:198	Running on an SGI Unix workstation, LEXAS can process about 15 examples per second when tested on the 'interest' data set." ></td>
	<td class="line x" title="131:198	4.2 Evaluation on a Large Data Set Previous research on WSD tend to be tested only on a dozen number of words, where each word frequently has either two or a few senses." ></td>
	<td class="line x" title="132:198	To test the scalability of LEXAS, we have gathered a corpus in which 192,800 word occurrences have been manually tagged with senses from WoRDNET 1.5." ></td>
	<td class="line x" title="133:198	This data set is almost two orders of magnitude larger in size than the above 'interest' data set." ></td>
	<td class="line x" title="134:198	Manual tagging was done by university undergraduates majoring in Linguistics, and approximately one man-year of efforts were expended in tagging our data set." ></td>
	<td class="line x" title="135:198	These 192,800 word occurrences consist of 121 nouns and 70 verbs which are the most frequently occurring and most ambiguous words of English." ></td>
	<td class="line x" title="136:198	The 121 nouns are: action activity age air area art board body book business car case center century change child church city class college community company condition cost country course day death development difference door effect effort end example experience face fact family field figure foot force form girl government ground head history home hour house information interest job land law level life light line man material matter member mind moment money month name nation need number order part party picture place plan point policy position power pressure problem process program public purpose question reason result right room school section sense service side society stage state step student study surface system table term thing time town type use value voice water way word work world The 70 verbs are: add appear ask become believe bring build call carry change come consider continue determine develop draw expect fall give go grow happen help hold indicate involve keep know lead leave lie like live look lose mean meet move need open pay raise read receive remember require return rise run see seem send set show sit speak stand start stop strike take talk tell think turn wait walk want work write For this set of nouns and verbs, the average number of senses per noun is 7.8, while the average number of senses per verb is 12.0." ></td>
	<td class="line x" title="137:198	We draw our sentences containing the occurrences of the 191 words listed above from the combined corpus of the 1 million word Brown corpus and the 2.5 million word Wall Street Journal (WSJ) corpus." ></td>
	<td class="line x" title="138:198	For every word in the two lists, up to 1,500 sentences each containing an occurrence of the word are extracted from the combined corpus." ></td>
	<td class="line x" title="139:198	In all, there are about 113,000 noun occurrences and about 79,800 verb occurrences." ></td>
	<td class="line x" title="140:198	This set of 121 nouns accounts for about 20% of all occurrences of nouns that one expects to encounter in any unrestricted English text." ></td>
	<td class="line x" title="141:198	Similarly, about 20% of all verb occurrences in any unrestricted text come from the set of 70 verbs chosen." ></td>
	<td class="line x" title="142:198	We estimate that there are 10-20% errors in our sense-tagged data set." ></td>
	<td class="line x" title="143:198	To get an idea of how the sense assignments of our data set compare with those provided by WoRDNET linguists in SEMCOR, the sense-tagged subset of Brown corpus prepared by Miller et al.(Miller et al. , 1994), we compare 44 Test set BC50 WSJ6 Sense 1 40.5% 44.8% Most Frequent LEXAS 47.1% 54.0% 63.7% 68.6% Table 5: Evaluation on a Large Data Set a subset of the occurrences that overlap." ></td>
	<td class="line x" title="145:198	Out of 5,317 occurrences that overlap, about 57% of the sense assignments in our data set agree with those in SEMCOR." ></td>
	<td class="line x" title="146:198	This should not be too surprising, as it is widely believed that sense tagging using the full set of refined senses found in a large dictionary like WORDNET involve making subtle human judgments (Wilks et al. , 1990; Bruce and Wiebe, 1994), such that there are many genuine cases where two humans will not agree fully on the best sense assignments." ></td>
	<td class="line x" title="147:198	We evaluated LEXAS on this larger set of noisy, sense-tagged data." ></td>
	<td class="line x" title="148:198	We first set aside two subsets for testing." ></td>
	<td class="line x" title="149:198	The first test set, named BC50, consists of 7,119 occurrences of the 191 content words that occur in 50 text files of the Brown corpus." ></td>
	<td class="line x" title="150:198	The second test set, named WSJ6, consists of 14,139 occurrences of the 191 content words that occur in 6 text files of the WSJ corpus." ></td>
	<td class="line x" title="151:198	We compared the classification accuracy of LEXAS against the default strategy of picking the most frequent sense." ></td>
	<td class="line x" title="152:198	This default strategy has been advocated as the baseline performance level for comparison with WSD programs (Gale et al. , 1992)." ></td>
	<td class="line x" title="153:198	There are two instantiations of this strategy in our current evaluation." ></td>
	<td class="line x" title="154:198	Since WORDNET orders its senses such that sense 1 is the most frequent sense, one possibility is to always pick sense 1 as the best sense assignment." ></td>
	<td class="line x" title="155:198	This assignment method does not even need to look at the training sentences." ></td>
	<td class="line x" title="156:198	We call this method 'Sense 1' in Table 5." ></td>
	<td class="line x" title="157:198	Another assignment method is to determine the most frequently occurring sense in the training sentences, and to assign this sense to all test sentences." ></td>
	<td class="line x" title="158:198	We call this method 'Most Frequent' in Table 5." ></td>
	<td class="line x" title="159:198	The accuracy of LEXAS on these two test sets is given in Table 5." ></td>
	<td class="line x" title="160:198	Our results indicate that exemplar-based classification of word senses scales up quite well when tested on a large set of words." ></td>
	<td class="line x" title="161:198	The classification accuracy of LEXAS is always better than the default strategy of picking the most frequent sense." ></td>
	<td class="line x" title="162:198	We believe that our result is significant, especially when the training data is noisy, and the words are highly ambiguous with a large number of refined sense distinctions per word." ></td>
	<td class="line x" title="163:198	The accuracy on Brown corpus test files is lower than that achieved on the Wall Street Journal test files, primarily because the Brown corpus consists of texts from a wide variety of genres, including newspaper reports, newspaper editorial, biblical passages, science and mathematics articles, general fiction, romance story, humor, etc. It is harder to dis45 ambiguate words coming from such a wide variety of texts." ></td>
	<td class="line x" title="164:198	5 Related Work There is now a large body of past work on WSD." ></td>
	<td class="line x" title="165:198	Early work on WSD, such as (Kelly and Stone, 1975; Hirst, 1987) used hand-coding of knowledge to perform WSD." ></td>
	<td class="line x" title="166:198	The knowledge acquisition process is laborious." ></td>
	<td class="line x" title="167:198	In contrast, LEXAS learns from tagged sentences, without human engineering of complex rules." ></td>
	<td class="line x" title="168:198	The recent emphasis on corpus based NLP has resulted in much work on WSD of unconstrained realworld texts." ></td>
	<td class="line x" title="169:198	One line of research focuses on the use of the knowledge contained in a machine-readable dictionary to perform WSD, such as (Wilks et al. , 1990; Luk, 1995)." ></td>
	<td class="line x" title="170:198	In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, including (Bruce and Wiebe, 1994; Miller et al. , 1994; Leacock et al. , 1993; Yarowsky, 1994; Yarowsky, 1993; Yarowsky, 1992)." ></td>
	<td class="line x" title="171:198	The work of (Miller et al. , 1994; Leacock et al. , 1993; Yarowsky, 1992) used only the unordered set of surrounding words to perform WSD, and they used statistical classifiers, neural networks, or IR-based techniques." ></td>
	<td class="line x" title="172:198	The work of (Bruce and Wiebe, 1994) used parts of speech (POS) and morphological form, in addition to surrounding words." ></td>
	<td class="line x" title="173:198	However, the POS used are abbreviated POS, and only in a window of -b2 words." ></td>
	<td class="line x" title="174:198	No local collocation knowledge is used." ></td>
	<td class="line x" title="175:198	A probabilistic classifier is used in (Bruce and Wiebe, 1994)." ></td>
	<td class="line x" title="176:198	That local collocation knowledge provides important clues to WSD is pointed out in (Yarowsky, 1993), although it was demonstrated only on performing binary (or very coarse) sense disambiguation." ></td>
	<td class="line x" title="177:198	The work of (Yarowsky, 1994) is perhaps the most similar to our present work." ></td>
	<td class="line x" title="178:198	However, his work used decision list to perform classification, in which only the single best disambiguating evidence that matched a target context is used." ></td>
	<td class="line x" title="179:198	In contrast, we used exemplar-based learning, where the contributions of all features are summed up and taken into account in coming up with a classification." ></td>
	<td class="line x" title="180:198	We also include verb-object syntactic relation as a feature, which is not used in (Yarowsky, 1994)." ></td>
	<td class="line x" title="181:198	Although the work of (Yarowsky, i994) can be applied to WSD, the results reported in (Yarowsky, 1994) only dealt with accent restoration, which is a much simpler problem." ></td>
	<td class="line x" title="182:198	It is unclear how Yarowsky's method will fare on WSD of a common test data set like the one we used, nor has his method been tested on a large data set with highly ambiguous words tagged with the refined senses of WORDNET." ></td>
	<td class="line x" title="183:198	The work of (Miller et al. , 1994) is the only prior work we know of which attempted to evaluate WSD on a large data set and using the refined sense distinction of WORDNET." ></td>
	<td class="line x" title="184:198	However, their results show no improvement (in fact a slight degradation in performance) when using surrounding words to perform WSD as compared to the most frequent heuristic." ></td>
	<td class="line x" title="185:198	They attributed this to insufficient training data in SEMCOm In contrast, we adopt a different strategy of collecting the training data set." ></td>
	<td class="line x" title="186:198	Instead of tagging every word in a running text, as is done in SEMCOR, we only concentrate on the set of 191 most frequently occurring and most ambiguous words, and collected large enough training data for these words only." ></td>
	<td class="line x" title="187:198	This strategy yields better results, as indicated by a better performance of LEXAS compared with the most frequent heuristic on this set of words." ></td>
	<td class="line x" title="188:198	Most recently, Yarowsky used an unsupervised learning procedure to perform WSD (Yarowsky, 1995), although this is only tested on disambiguating words into binary, coarse sense distinction." ></td>
	<td class="line x" title="189:198	The effectiveness of unsupervised learning on disambiguating words into the refined sense distinction of WoRBNET needs to be further investigated." ></td>
	<td class="line x" title="190:198	The work of (McRoy, 1992) pointed out that a diverse set of knowledge sources are important to achieve WSD, but no quantitative evaluation was given on the relative importance of each knowledge source." ></td>
	<td class="line x" title="191:198	No previous work has reported any such evaluation either." ></td>
	<td class="line x" title="192:198	The work of (Cardie, 1993) used a case-based approach that simultaneously learns part of speech, word sense, and concept activation knowledge, although the method is only tested on domain-specific texts with domain-specific word senses." ></td>
	<td class="line x" title="193:198	6 Conclusion In this paper, we have presented a new approach for WSD using an exemplar based learning algorithm." ></td>
	<td class="line x" title="194:198	This approach integrates a diverse set of knowledge sources to disambiguate word sense." ></td>
	<td class="line x" title="195:198	When tested on a common data set, our WSD program gives higher classification accuracy than previous work on WSD." ></td>
	<td class="line x" title="196:198	When tested on a large, separately collected data set, our program performs better than the default strategy of picking the most frequent sense." ></td>
	<td class="line x" title="197:198	To our knowledge, this is the first time that a WSD program has been tested on such a large scale, and yielding results better than the most frequent heuristic on highly ambiguous words with the refined senses of WoRDNET." ></td>
	<td class="line x" title="198:198	7 Acknowledgements We would like to thank: Dr Paul Wu for sharing the Brown Corpus and Wall Street Journal Corpus; Dr Christopher Ting for downloading and installing WoRDNET and SEMCOR, and for reformatting the corpora; the 12 undergraduates from the Linguistics Program of the National University of Singapore for preparing the sense-tagged corpus; and Prof K. P. Mohanan for his support of the sense-tagging project." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P96-1030
Fast Parsing Using Pruning And Grammar Specialization
Rayner, Manny;Carter, David M.;"></td>
	<td class="line x" title="1:171	FAST PARSING USING PRUNING AND GRAMMAR SPECIALIZATION Manny Rayner and David Carter SRI International Suite 23, Millers Yard Cambridge CB2 1RQ United Kingdom manny@cam, sri." ></td>
	<td class="line x" title="2:171	com, dmc~cam, sri." ></td>
	<td class="line x" title="3:171	com Abstract We show how a general grammar may be automatically adapted for fast parsing of utterances from a specific domain by means of constituent pruning and grammar specialization based on explanation-based learning." ></td>
	<td class="line x" title="4:171	These methods together give an order of magnitude increase in speed, and the coverage loss entailed by grammar specialization is reduced to approximately half that reported in previous work." ></td>
	<td class="line x" title="5:171	Experiments described here suggest that the loss of coverage has been reduced to the point where it no longer causes significant performance degradation in the context of a real application." ></td>
	<td class="line x" title="6:171	1 Introduction Suppose that we have a general grammar for English, or some other natural language; by this, we mean a grammar which encodes most of the important constructions in the language, and which is intended to be applicable to a large range of different domains and applications." ></td>
	<td class="line x" title="7:171	The basic question attacked in this paper is the following one: can such a grammar be concretely useful if we want to process input from a specific domain?" ></td>
	<td class="line x" title="8:171	In particular, how can a parser that uses a general grammar achieve a level of efficiency that is practically acceptable?" ></td>
	<td class="line x" title="9:171	The central problem is simple to state." ></td>
	<td class="line x" title="10:171	By the very nature of its construction, a general grammar allows a great many theoretically valid analyses of almost any non-trivial sentence." ></td>
	<td class="line x" title="11:171	However, in the context of a specific domain, most of these will be extremely implausible, and can in practice be ignored." ></td>
	<td class="line x" title="12:171	If we want efficient parsing, we want to be able to focus our search on only a small portion of the space of theoretically valid grammatical analyses." ></td>
	<td class="line x" title="13:171	One possible solution is of course to dispense with the idea of using a general grammar, and simply code a new grammar for each domain." ></td>
	<td class="line x" title="14:171	Many people do this, but one cannot help feeling that something is being missed; intuitively, there are many domain-independent grammatical constraints, which one would prefer only to need to code once." ></td>
	<td class="line x" title="15:171	In the last ten years, there have been a number of attempts to find ways to automatically adapt a general grammar and/or parser to the sub-language defined by a suitable training corpus." ></td>
	<td class="line x" title="16:171	For example, (Briscoe and Carroll, 1993) train an LR parser based on a general grammar to be able to distinguish between likely and unlikely sequences of parsing actions; (Andry et al. , 1994) automatically infer sortal constraints, that can be used to rule out otherwise grammatical constituents; and (Grishman et al. , 1984) describes methods that reduce the size of a general grammar to include only rules actually useful for parsing the training corpus." ></td>
	<td class="line x" title="17:171	The work reported here is a logical continuation of two specific strands of research aimed in this general direction." ></td>
	<td class="line oc" title="18:171	The first is the popular idea of statistical tagging e.g.(DeRose, 1988; Cutting et al. , 1992; Church, 1988)." ></td>
	<td class="line x" title="20:171	Here, the basic idea is that a given small segment S of the input string may have several possible analyses; in particular, if S is a single word, it may potentially be any one of several parts of speech." ></td>
	<td class="line x" title="21:171	However, if a substantial training corpus is available to provide reasonable estimates of the relevant parameters, the immediate context surrounding S will usually make most of the locally possible analyses of S extremely implausible." ></td>
	<td class="line x" title="22:171	In the specific case of part-of-speech tagging, it is well-known (DeMarcken, 1990) that a large proportion of the incorrect tags can be eliminated 'safely'~ i.e. with very low risk of eliminating correct tags." ></td>
	<td class="line x" title="23:171	In the present paper, the statistical tagging idea is generalized to a method called 'constituent pruning'; this acts on local analyses of phrases normally 223 larger than single-word units." ></td>
	<td class="line x" title="24:171	Constituent pruning is a bottom-up approach, and is complemented by a second, top-down, method based on Explanation-Based Learning (EBL; (Mitchell et al. , 1986; van Harmelen and Bundy, 1988))." ></td>
	<td class="line x" title="25:171	This part of the paper is essentially an extension and generalization of the line of work described in (Rayner, 1988; Rayner and Samuelsson, 1990; Samuelsson and Rayner, 1991; Rayner and Samuelsson, 1994; Samuelsson, 1994b)." ></td>
	<td class="line x" title="26:171	Here, the basic idea is that grammar rules tend in any specific domain to combine much more frequently in some ways than in others." ></td>
	<td class="line x" title="27:171	Given a sufficiently large corpus parsed by the original, general, grammar, it is possible to identify the common combinations of grammar rules and 'chunk' them into 'macro-rules'." ></td>
	<td class="line x" title="28:171	The result is a 'specialized' grammar; this has a larger number of rules, but a simpler structure, allowing it in practice to be parsed very much more quickly using an LRbased method (Samuelsson, 1994a)." ></td>
	<td class="line x" title="29:171	The coverage of the specialized grammar is a strict subset of that of the original grammar; thus any analysis produced by the specialized grammar is guaranteed to be valid in the original one as well." ></td>
	<td class="line x" title="30:171	The practical utility of the specialized grammar is largely determined by the loss of coverage incurred by the specialization process." ></td>
	<td class="line x" title="31:171	The two methods, constituent pruning and grammar specialization, are combined as follows." ></td>
	<td class="line x" title="32:171	The rules in the original, general, grammar are divided into two sets, called phrasal and non-phrasal respectively." ></td>
	<td class="line x" title="33:171	Phrasal rules, the majority of which define non-recursive noun phrase constructions, are used as they are; non-phrasal rules are combined using EBL into chunks, forming a specialized grammar which is then compiled further into a set of LRtables." ></td>
	<td class="line x" title="34:171	Parsing proceeds by interleaving constituent creation and deletion." ></td>
	<td class="line x" title="35:171	First, the lexicon and morphology rules are used to hypothesize word analyses." ></td>
	<td class="line x" title="36:171	Constituent pruning then removes all sufficiently unlikely edges." ></td>
	<td class="line x" title="37:171	Next, the phrasal rules are applied bottom-up, to find all possible phrasal edges, after which unlikely edges are again pruned." ></td>
	<td class="line x" title="38:171	Finally, the specialized grammar is used to search for full parses." ></td>
	<td class="line x" title="39:171	The scheme is fully implemented within a version of the Spoken Language Translator system (Rayner et al. , 1993; Agniis et al. , 1994), and is normally applied to input in the form of small lattices of hypotheses produced by a speech recognizer." ></td>
	<td class="line x" title="40:171	The rest of the paper is structured as follows." ></td>
	<td class="line x" title="41:171	Section 2 describes the constituent pruning method." ></td>
	<td class="line x" title="42:171	Section 3 describes the grammar specialization method, focusing on how the current work extends and improves on previous results." ></td>
	<td class="line x" title="43:171	Section 4 describes experiments where the constituent pruning/grammar specialization method was used on sets of previously unseen speech data." ></td>
	<td class="line x" title="44:171	Section 5 concludes and sketches further directions for research, which we are presently in the process of investigating." ></td>
	<td class="line x" title="45:171	2 Constituent Pruning Before both the phrasal and full parsing stages, the constituent table (henceforth, the chart) is pruned to remove edges that are relatively unlikely to contribute to correct analyses." ></td>
	<td class="line x" title="46:171	For example, after the string 'Show flight D L three one two' is lexically analysed, edges for 'D' and 'L' as individual characters are pruned because another edge, derived from a lexical entry for 'D L' as an airline code, is deemed far more plausible." ></td>
	<td class="line x" title="47:171	Similarly, edges for 'one' as a determiner and as a noun are pruned because, when flanked by two other numbers, 'one' is far more likely to function as a number." ></td>
	<td class="line x" title="48:171	Phrasal parsing then creates a number of new edges, including one for 'flight D L three one two' as a noun phrase." ></td>
	<td class="line x" title="49:171	This edge is deemed far more likely to serve as the basis for a correct full parse than any of the edges spanning substrings of this phrase; those edges, too, are therefore pruned." ></td>
	<td class="line x" title="50:171	As a result, full parsing is very quick, and only one analysis (the correct one) is produced for the sentence." ></td>
	<td class="line x" title="51:171	In the absence of pruning, processing takes over eight times as long and produces 37 analyses in total." ></td>
	<td class="line x" title="52:171	2.1 The pruning algorithm Our algorithm estimates the probability of correctness of each edge: that is, the probability that the edge will contribute to the correct full analysis of the sentence (assuming there is one), given certain lexical and/or syntactic information about it." ></td>
	<td class="line x" title="53:171	Values on each criterion (selection of pieces of information) are derived from training corpora by maximum likelihood estimation followed by smoothing." ></td>
	<td class="line x" title="54:171	That is, our estimate for the probability that an edge with property P is correct is (modulo smoothing) simply the number of times edges with property P occur in correct analyses in training divided by the number of times such edges are created during the analysis process in training." ></td>
	<td class="line x" title="55:171	The current criteria are:  The left bigram score: the probability of correctness of an edge considering only the following data about it: its tag (corresponding to its major category symbol plus, for a few categories, some ad224 ditional distinctions derived from feature values); for a lexical edge, its word or semantic word class (words with similar distributions, such as city names, are grouped into classes to overcome data sparseness); or for a phrasal edge, the name of the final (topmost) grammar rule that was used to create it; the tag of a neighbouring edge immediately to its left." ></td>
	<td class="line x" title="56:171	If there are several left neighbours, the one giving the highest probability is used." ></td>
	<td class="line x" title="57:171	 The right bigram score: as above, but considering right neighbours." ></td>
	<td class="line x" title="58:171	 The unigram score: the probability of correctness of an edge considering only the tree of grammar rules, with words or word classes at the leaves, that gave rise to it." ></td>
	<td class="line x" title="59:171	For a lexical edge, this reduces to its word or word class, and its tag." ></td>
	<td class="line x" title="60:171	Other criteria, such as trigrams and finer-grained tags, are obviously worth investigating, and could be applied straightforwardly within the framework described here." ></td>
	<td class="line x" title="61:171	The minimum score derived from any of the criteria applied is deemed initially to be the score of the constituent." ></td>
	<td class="line x" title="62:171	That is, an assumption of full statistical dependence (Yarowsky, 1994), rather than the more common full independence, is made3 When llf events El, E2,, E,~ are fully independent, then the joint probability P(E1 A A En) is the product of P(EI)P(En), but if they are maximally dependent, it is the minimum of these values." ></td>
	<td class="line x" title="63:171	Of course, neither assumption is any more than an approximation to the truth; but assuming dependence has the advantage that the estimate of the joint probability depends much less strongly on n, and so estimates for alternative joint events can be directly compared, without any possibly tricky normalization, even if they are composed of different numbers of atomic events." ></td>
	<td class="line x" title="64:171	This property is desirable: different (sub-)paths through a chart may span different numbers of edges, and one can imagine evaluation criteria which are only defined for some kinds of edge, or which often duplicate information supplied by other criteria." ></td>
	<td class="line x" title="65:171	Taking minima means that the pruning of an edge results from it scoring poorly on one criterion, regardless of other, possibly good scores assigned to it by other criteria." ></td>
	<td class="line x" title="66:171	This fits in with the fact that on the basis of local information alone it is not usually possibly to predict with confidence that a particular edge is highly likely to contribute to the correct analysis (since global factors will also be important) but it often is possible to spot highly unlikely edges." ></td>
	<td class="line x" title="67:171	In other words, our training procedure yields far more probability estimates close to zero than close to one." ></td>
	<td class="line x" title="68:171	recognizer output is being processed, however, the estimate from each criterion is in fact multiplied by a further estimate derived from the acoustic score of the edge: that is, the score assigned by the speech recognizer to the best-scoring sentence hypothesis containing the word or word string for the edge in question." ></td>
	<td class="line x" title="69:171	Multiplication is used here because acoustic and lexicosyntactic likelihoods for a word or constituent would appear to be more nearly fully independent than fully dependent, being based on very different kinds of information." ></td>
	<td class="line x" title="70:171	Next, account is taken of the connectivity of the chart." ></td>
	<td class="line x" title="71:171	Each vertex of the chart is labelled with the score of the best path through the chart that visits that vertex." ></td>
	<td class="line x" title="72:171	In accordance with the dependence assumption, the score of a path is defined as the minimum of the scores of its component edges." ></td>
	<td class="line x" title="73:171	Then the score of each edge is recalculated to be the minimum of its existing score and the scores of its start and end vertices, on the grounds that a constituent, however intrinsically plausible, is not worth preserving if it does not occur on any plausible paths." ></td>
	<td class="line x" title="74:171	Finally, a pruning threshold is calculated as the score of the best path through the chart multiplied by a certain fraction." ></td>
	<td class="line x" title="75:171	For the first pruning phase we use 1/20, and for the second, 1/150, although performance is not very sensitive to this." ></td>
	<td class="line x" title="76:171	Any constituents scoring less than the threshold are pruned out." ></td>
	<td class="line x" title="77:171	2.2 Relation to other pruning methods As the example above suggests, judicious pruning of the chart at appropriate points can greatly restrict the search space and speed up processing." ></td>
	<td class="line x" title="78:171	Our method has points of similarity with some very recent work in Constraint Grammar 2 and is an alternative to several other, related schemes." ></td>
	<td class="line x" title="79:171	Firstly, a remarked earlier, it generalizes tagging: it not only adjudicates between possible labels for the same word, but can also use the existence of a constituent over one span of the chart as justification for pruning another constituent over another span, normally a subsumed one, as in the 'D L' example." ></td>
	<td class="line x" title="80:171	This is especially true in the second stage of pruning, when many constituents of different lengths have been created." ></td>
	<td class="line x" title="81:171	Furthermore, it applies equally well to lattices, rather than strings, of words, and can take account of acoustic plausibility as well as syntactic considerations." ></td>
	<td class="line x" title="82:171	Secondly, our method is related to beam search (Woods, 1985)." ></td>
	<td class="line x" title="83:171	In beam search, incomplete parses of an utterance are pruned or discarded when, on 2Ghrister Samuelsson, personal communication, 8th April 1996; see (Karlsson et al. , 1995) for background." ></td>
	<td class="line x" title="84:171	225 some criterion, they are significantly less plausible than other, competing parses." ></td>
	<td class="line x" title="85:171	This pruning is fully interleaved with the parsing process." ></td>
	<td class="line x" title="86:171	In contrast, our pruning takes place only at certain points: currently before parsing begins, and between the phrasM and full parsing stages." ></td>
	<td class="line x" title="87:171	Potentially, as with any generate-and-test algorithm, this can mean efficiency is reduced: some paths will be explored that could in principle be pruned earlier." ></td>
	<td class="line x" title="88:171	However, as the results in section 4 below will show, this is not in practice a serious problem, because the second pruning phase greatly reduces the search space in preparation for the potentially inefficient full parsing phase." ></td>
	<td class="line x" title="89:171	Our method has the advantage, compared to beam search, that there is no need for any particular search order to be followed; when pruning takes place, all constituents that could have been found at the stage in question are guaranteed already to exist." ></td>
	<td class="line x" title="90:171	Thirdly, our method is a generalization of the strategy employed by (McCord, 1993)." ></td>
	<td class="line x" title="91:171	McCord interleaved parsing with pruning in the same way as us, but only compared constituents over the same span and with the same major category." ></td>
	<td class="line x" title="92:171	Our comparisons are more global and therefore can result in more effective pruning." ></td>
	<td class="line x" title="93:171	3 Grammar specialization As described in Section 1 above, the non-phrasal grammar rules are subjected to two phases of processing." ></td>
	<td class="line x" title="94:171	In the first, 'EBL learning' phase, a parsed training corpus is used to identify 'chunks' of rules, which are combined by the EBL algorithm into single macro-rules." ></td>
	<td class="line x" title="95:171	In the second phase, the resulting set of 'chunked' rules is converted into LR table form, using the method of (Samuelsson, 1994a)." ></td>
	<td class="line x" title="96:171	There are two main parameters that can be adjusted in the EBL learning phase." ></td>
	<td class="line x" title="97:171	Most simply, there is the size of the training corpus; a larger training corpus means a smaller loss of coverage due to grammar specialization." ></td>
	<td class="line x" title="98:171	(Recall that grammar specialization in general trades coverage for speed)." ></td>
	<td class="line x" title="99:171	Secondly, there is the question of how to select the rulechunks that will be turned into macro-rules." ></td>
	<td class="line x" title="100:171	At one limit, the whole parse-tree for each training example is turned into a single rule, resulting in a specialized grammar all of whose derivations are completely 'flat'." ></td>
	<td class="line x" title="101:171	These grammars can be parsed extremely quickly, but the coverage loss is in practice unacceptably high, even with very large training corpora." ></td>
	<td class="line x" title="102:171	At the opposite extreme, each rule-chunk consists of a single rule-application; this yields a specialized grammar identical to the original one." ></td>
	<td class="line x" title="103:171	The challenge is to find an intermediate solution, which specializes the grammar non-triviMly without losing too much coverage." ></td>
	<td class="line x" title="104:171	Several attempts to find good 'chunking criteria' are described in the papers by Rayner and Samuelsson quoted above." ></td>
	<td class="line x" title="105:171	In (Rayner and Samuelsson, 1994), a simple scheme is given, which creates rules corresponding to four possible units: full utterances, recursive NPs, PPs, and non-recursive NPs." ></td>
	<td class="line x" title="106:171	A more elaborate scheme is given in (Samuelsson, 1994b), where the 'chunking criteria' are learned automatically by an entropy-minimization method; the results, however, do not appear to improve on the earlier ones." ></td>
	<td class="line x" title="107:171	In both cases, the coverage loss due to grammar specialization was about 10 to 12% using training corpora with about 5,000 examples." ></td>
	<td class="line x" title="108:171	In practice, this is still unacceptably high for most applications." ></td>
	<td class="line x" title="109:171	Our current scheme is an extension of the one from (Rayner and Samuelsson, 1994), where the rulechunks are trees of non-phrasal rules whose roots and leaves are categories of the following possible types: full utterances, utterance units, imperative VPs, NPs, relative clauses, VP modifiers and PPs." ></td>
	<td class="line x" title="110:171	The resulting specialized grammars are forced to be non-recursive, with derivations being a maximum of six levels deep." ></td>
	<td class="line x" title="111:171	This is enforced by imposing the following dominance hierarchy between the possible categories: utterance > utterance_unit > imperative_VP > NP > {tel, VP_modifier} > PP The precise definition of the rule-chunking criteria is quite simple, and is reproduced in the appendix." ></td>
	<td class="line x" title="112:171	Note that only the non-phrasal rules are used as input to the chunks from which the specialized grammar rules are constructed." ></td>
	<td class="line x" title="113:171	This has two important advantages." ></td>
	<td class="line x" title="114:171	Firstly, since all the phrasal rules are excluded from the speciMization process, the coverage loss associated with missing combinations of phrasal rules is eliminated." ></td>
	<td class="line x" title="115:171	As the experiments in the next section show, the resulting improvement is quite substantial." ></td>
	<td class="line x" title="116:171	Secondly, and possibly even more importantly, the number of specialized rules produced by a given training corpus is approximately halved." ></td>
	<td class="line x" title="117:171	The most immediate consequence is that much larger training corpora can be used before the specialized grammars produced become too large to be handled by the LR table compiler." ></td>
	<td class="line x" title="118:171	If both phrasal and non-phrasal rules are used, we have been unable to compile tables for rules derived from training sets of over 6,000 examples (the process was killed after running for about six hours on a Sun Sparc 20/HS21, SpecINT92=131.2)." ></td>
	<td class="line x" title="119:171	Using only non-phrasal rules, compilation of the tables for a 15,000 example train226 ing set required less than two CPU-hours on the same machine." ></td>
	<td class="line x" title="120:171	4 Experiments This section describes a number of experiments carried out to test the utility of the theoretical ideas presented above." ></td>
	<td class="line x" title="121:171	The basic corpus used was a set of 16,000 utterances from the Air Travel Planning (ATIS; (Hemphill et al. , 1990)) domain." ></td>
	<td class="line x" title="122:171	All of these utterances were available in text form; 15,000 of them were used for training, with 1,000 held out for test purposes." ></td>
	<td class="line x" title="123:171	Care was taken to ensure not just that the utterances themselves, but also the speakers of the utterances were disjoint between test and training data; as pointed out in (Rayner et al. , 1994a), failure to observe these precautions can result in substantial spurious improvements in test data results." ></td>
	<td class="line x" title="124:171	The 16,000 sentence corpus was analysed by the SRI Core Language Engine (Alshawi (ed), 1992), using a lexicon extended to cover the ATIS domain (Rayner, 1994)." ></td>
	<td class="line x" title="125:171	All possible grammatical analyses of each utterance were recorded, and an interactive tool was used to allow a human judge to identify the correct and incorrect readings of each utterance." ></td>
	<td class="line x" title="126:171	The judge was a first-year undergraduate student with a good knowledge of linguistics but no prior experience with the system; the process of judging the corpus took about two and a half person-months." ></td>
	<td class="line x" title="127:171	The input to the EBL-based grammar-specialization process was limited to readings of corpus utterances that had been judged correct." ></td>
	<td class="line x" title="128:171	When utterances had more than one correct reading, a preference heuristic was used to select the most plausible one." ></td>
	<td class="line x" title="129:171	Two sets of experiments were performed." ></td>
	<td class="line x" title="130:171	In the first, increasingly large portions of the training set were used to train specialized grammars." ></td>
	<td class="line x" title="131:171	The coverage loss due to grammar specialization was then measured on the 1,000 utterance test set." ></td>
	<td class="line x" title="132:171	The experiment was carried out using both the chunking criteria from (Rayner and Samuelsson, 1994) (the 'Old' scheme), and the chunking criteria described in Section 3 above (the 'New' scheme)." ></td>
	<td class="line x" title="133:171	The results are presented in Table 1." ></td>
	<td class="line x" title="134:171	The second set of experiments tested more directly the effect of constituent pruning and grammar specialization on the Spoken Language Translator's speed and coverage; in particular, coverage was measured on the real task of translating English into Swedish, rather than the artificial one of producing a correct QLF analysis." ></td>
	<td class="line x" title="135:171	To this end, the first 500 testset utterances were presented in the form of speech hypothesis lattices derived by aligning and conflating the top five sentence strings produced by a version of the DECIPHER (TM) recognizer (Murveit Examples 100 250 500 1000 3000 5000 7000 11000 15000 Old scheme Rules Loss 100 47.8% 181 37.6% 281 27.6% 432 22.7% 839 14.9% 1101 11.2% 1292 10.4% 1550 9.8% 1819 8.7% New scheme Rules Loss 69 35.5% 126 21.8% 180 14.7% 249 10.8% 455 7.8% 585 6.6% 668 62% 808 5.8% 937 5.0% Table 1: EBL rules and EBL coverage number of training examples loss against et al. , 1993)." ></td>
	<td class="line x" title="136:171	The lattices were analysed by four different versions of the parser, exploring the different combinations of turning constituent pruning on or off, and specialized versus unspecialized grammars." ></td>
	<td class="line x" title="137:171	The specialized grammar used the 'New' scheme, and had been trained on the full training set." ></td>
	<td class="line x" title="138:171	Utterances which took more than 90 CPU seconds to process were timed out and counted as failures." ></td>
	<td class="line x" title="139:171	The four sets of outputs from the parser were then translated into Swedish by the SLT transfer and generation mechanism (Agn~ et al. , 1994)." ></td>
	<td class="line x" title="140:171	Finally, the four sets of candidate translations were pairwise compared in the cases where differing translations had been produced." ></td>
	<td class="line x" title="141:171	We have found this to be an effective way of evaluating system performance." ></td>
	<td class="line x" title="142:171	Although people differ widely in their judgements of whether a given translation can be regarded as 'acceptable', it is in most cases surprisingly easy to say which of two possible translations is preferable." ></td>
	<td class="line x" title="143:171	The last two tables summarize the results." ></td>
	<td class="line x" title="144:171	Table 2 gives the average processing times per input lattice for each type of processing (times measured running SICStus Prolog 3#3 on a SUN Sparc 20/HS21), showing how the time is divided between the various processing phases." ></td>
	<td class="line x" title="145:171	Table 3 shows the relative scores of the four parsing variants, measured according to the 'preferable translation' criterion." ></td>
	<td class="line x" title="146:171	5 Conclusions and further directions Table 2 indicates that EBL and pruning each make processing about three times faster; the combination of both gives a factor of about nine." ></td>
	<td class="line x" title="147:171	In fact, as the detailed breakdown shows, even this underestimates the effect on the main parsing phase: when both pruning and EBL are operating, processing times for other components (morphology, pruning and preferences) become the dominant ones." ></td>
	<td class="line x" title="148:171	As we have so 227 EE-tEE+ ppP P+ Morph/lex lookup 0.53 0.54 0.54 0.49 Phrasal parsing 0.27 0.28 0.14 0.14 Pruning 0.57 0.56 Full parsing 12.42 2.61 3.04 0.26 Preferences 3.63 1.57 1.27 0.41 TOTAL Table 2: Breakdown of average time spent on each processing phase for each type of processing (seconds per utterance) EE+ EE-tPPP-tP+ E-/P12-24 25-63 24-65 E+/P24-12 31-50 26-47 E-/P+ 63-25 50-31 5-8 E+/P+ 65-24 47-26 8-5 Table 3: Comparison between translation results on the four different analysis alternatives, measured on the 500-utterance test set." ></td>
	<td class="line x" title="149:171	The entry for a given row and column holds two figures, showing respectively the number of examples where the 'row' variant produced a better translation than the 'column' variant and the number where it produced a worse one." ></td>
	<td class="line x" title="150:171	Thus for example 'EBL+/pruning+' was better than 'EBL-/pruning-' on 65 examples, and worse on 24." ></td>
	<td class="line x" title="151:171	far expended little effort on optimizing these phases of processing, it is reasonable to expect substantial further gains to be possible." ></td>
	<td class="line x" title="152:171	Even more interestingly, Table 3 shows that real system performance, in terms of producing a good translation, is significantly improved by pruning, and is not degraded by grammar specialization." ></td>
	<td class="line x" title="153:171	(The slight improvement in coverage with EBL on is not statistically significant)." ></td>
	<td class="line x" title="154:171	Our interpretation of these results is that the technical loss of grammar coverage due to the specialization and pruning processes is more than counterbalanced by two positive effects." ></td>
	<td class="line x" title="155:171	Firstly, fewer utterances time out due to slow processing; secondly, the reduced space of possible analyses means that the problem of selecting between different possible analyses of a given utterance becomes easier." ></td>
	<td class="line x" title="156:171	To sum up, the methods presented here demonstrate that it is possible to use the combined pruning and grammar specialization method to speed up the whole analysis phase by nearly an order of magnitude, without incurring any real penalty in the form of reduced coverage." ></td>
	<td class="line x" title="157:171	We find this an exciting and significant result, and are further continuing our research in this area during the coming year." ></td>
	<td class="line x" title="158:171	In the last two paragraphs we sketch some ongoing work." ></td>
	<td class="line x" title="159:171	All the results presented above pertain to English only." ></td>
	<td class="line x" title="160:171	The first topic we have been investigating is the application of the methods described here to processing of other languages." ></td>
	<td class="line x" title="161:171	Preliminary experiments we have carried out on the Swedish version of the CLE (Gamb~ick and Rayner 1992) have been encouraging; using exactly the same pruning methods and EBL chunking criteria as for English, we obtain comparable speed-ups." ></td>
	<td class="line x" title="162:171	The loss of coverage due to grammar specialization also appears comparable, though we have not yet had time to do the work needed to verify this properly." ></td>
	<td class="line x" title="163:171	We intend to do so soon, and also to repeat the experiments on the French version of the CLE (Rayner, Carter and Bouillon, 1996)." ></td>
	<td class="line x" title="164:171	The second topic is a more radical departure, and can be viewed as an attempt to make interleaving of parsing and pruning the basic principle underlying the CLE's linguistic analysis process." ></td>
	<td class="line x" title="165:171	Exploiting the 'stratified' nature of the EBL-specialized grammar, we group the chunked rules by level, and apply them one level at a time, starting at the bottom." ></td>
	<td class="line x" title="166:171	After each level, constituent pruning is used to eliminate unlikely constituents." ></td>
	<td class="line x" title="167:171	The intent is to achieve a trainable robust parsing model, which can return a useful partial analysis when no single global analysis is found." ></td>
	<td class="line x" title="168:171	An initial implementation exists, and is currently being tested; preliminary results here are also very positive." ></td>
	<td class="line x" title="169:171	We expect to be able to report on this work more fully in the near future." ></td>
	<td class="line x" title="170:171	Acknowledgements The work reported in this paper was funded by Telia Research AB." ></td>
	<td class="line x" title="171:171	We would like to thank Christer Samuelsson for making the LR compiler available to us, Martin Keegan for patiently judging the results of processing 16,000 ATIS utterances, and Steve Pulman and Christer Samuelsson for helpful comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W96-0101
Using Word Class For Part-Of-Speech Disambiguation
Tzoukermann, Evelyne;Radev, Dragomir R.;"></td>
	<td class="line x" title="1:213	Using word class for Part-of-speech disambiguation Evelyne Tzoukermann and Dragomir R. Radev* AT&T Bell Laboratories 600 Mountain Avenue Murray Hill, NJ 07974-0636 evelyne, s_radevresearch, att." ></td>
	<td class="line x" title="2:213	com *Department of Computer Science Columbia University New York, NY 10027 radevcs, columbia, edu Abstract This paper presents a methodology for improving part-of-speech disambiguation using word classes." ></td>
	<td class="line x" title="3:213	We build on earlier work for tagging French where we showed that statistical estimates can be computed without lexical probabilities." ></td>
	<td class="line x" title="4:213	We investigate new directions for coming up with different kinds of probabilities based on paradigms of tags for given words." ></td>
	<td class="line x" title="5:213	We base estimates not on the words, but on the set of tags associated with a word." ></td>
	<td class="line x" title="6:213	We compute frequencies of unigrams, bigrams, and trigrams of word classes in order to further refine the disambiguation." ></td>
	<td class="line x" title="7:213	This new approach gives a more efficient representation of the data in order to disambiguate word part-of-speech." ></td>
	<td class="line x" title="8:213	We show empirical results to support our claim." ></td>
	<td class="line x" title="9:213	We demonstrate that, besides providing good estimates for disambiguation, word classes solve some of the problems caused by sparse training data." ></td>
	<td class="line x" title="10:213	We describe a part-of-speech tagger built on these principles and we suggest a methodology for developing an adequate training corpus." ></td>
	<td class="line oc" title="11:213	1 Introduction In the part-of-speech hterature, whether taggers are based on a rule-based approach (Klein and Simmons, 1963), (Brill, 1992), (Voutilainen, 1993), or on a statistical one (Bahl and Mercer, 1976), (Leech et al. , 1983), (Merialdo, 1994), (DeRose, 1988), (Church, 1989), (Cutting et al. , 1992), there is a debate as to whether more attention should be paid to lexical probabilities rather than contextual ones." ></td>
	<td class="line x" title="12:213	(Church, 1992) claims that part-of-speech taggers depend almost exclusively on lexical probabilities, whereas other researchers, such as Voutilainen (Karlsson et al. , 1995) argue that word ambiguities vary widely in function of the specific text and genre." ></td>
	<td class="line x" title="13:213	Indeed, part of Church's argument is relevant if a system is based on a large corpus such as the Brown corpus (Francis and Ku~era, 1982) which represents one million surface forms of morpho-syntacticaJly disambiguated words from a range of balanced texts." ></td>
	<td class="line x" title="14:213	Consider, for example, a word like 'cover' as discussed by Voutilainen (Karlsson et al. , 1995): in the Brown and the LOB Corpus (Johansson, 1980), the word 'cover' is a noun 40% of the occurrences and a verb 60% of the other, but in the context of a car maintenance manual, it is a noun 100~0 of the time." ></td>
	<td class="line x" title="15:213	Since, for statistical taggers, 90% of texts can be disambiguated solely applying lexical probabilities, it is, in fact, tempting to think that with more data and more accurate lexical estimates, more text could be better disambiguated." ></td>
	<td class="line x" title="16:213	If this hypothesis is true for English, we show that it does not hold for languages for which publicly available tagged corpora do not exist." ></td>
	<td class="line x" title="17:213	We also argue against Church's position, supporting the claim that more attention needs to be paid to contextual information for part-of-speech disambiguation (Tzoukermann et ai., 1995)." ></td>
	<td class="line x" title="19:213	The problem tackled here is to develop an 'efficient' training corpus." ></td>
	<td class="line x" title="20:213	Unless large effort, money, and time are devoted to this project, only small corpora can be disambiguated manually." ></td>
	<td class="line x" title="21:213	Consequently, the problem of extracting lexical probabilities from a small training corpus is twofold: first, the statistical model may not necessarily represent the use of a particular word in a particular context." ></td>
	<td class="line x" title="22:213	In a morphologically inflected language, this argument is particularly serious since a word can be tagged with a large number of parts of speech, i.e. the ambiguity potential is high." ></td>
	<td class="line x" title="23:213	Second, word ambiguity may vary widely depending on the particular genre of the text, and this could differ from the training corpus." ></td>
	<td class="line x" title="24:213	When there is no equivalent for the Brown corpus in French, how should one build an adequate training corpus which reflects properly lexical probabilities?" ></td>
	<td class="line x" title="25:213	How can the numerous morphological variants that render this task even harder be handled?" ></td>
	<td class="line x" title="26:213	The next section gives examples from French and describes how morphology affects part-ofspeech disambiguation and what types of ambiguities are found in the language." ></td>
	<td class="line x" title="27:213	Section 3 examines different techniques used to obtain lexical probabilities." ></td>
	<td class="line x" title="28:213	Given the problems created by estimating probabilities on a corpus of restricted size, we present in Section 4 a solution for coping with these difficulties." ></td>
	<td class="line x" title="29:213	We suggest a new paradigm called genotype, derived from the concept of ambiguity class (Kupiec, 1992), which gives a more efficient representation of the data in order to achieve more accuracy in part-of-speech disambiguation." ></td>
	<td class="line o" title="30:213	Section 5 shows how our approach differs from the approach taken by Cutting and Kupiec." ></td>
	<td class="line x" title="31:213	The frequencies of unigram, bigram, and trigram genotypes are computed in order to further refine the disambiguation and results are provided to support our claims." ></td>
	<td class="line x" title="32:213	The final section offers a methodology for developing an adequate training corpus." ></td>
	<td class="line x" title="33:213	2 French words and morphological variants To illustrate our position, we consider the case of French, a typical Romance language." ></td>
	<td class="line x" title="34:213	French has a rich morphological system for verbs which can have as many as 48 inflected forms and a less rich inflectional system for nouns and adjectives, the latter varying in gender and number having up to four different forms." ></td>
	<td class="line x" title="35:213	For example, the word 'marine' shown in Table 1, can have as many as eight morphological analyses." ></td>
	<td class="line x" title="36:213	word base form morphological analysis 'marine' 'marine' 'marine' 'marine' 'marine' 'marine' 'marine' 'marine' <marln> <marine> <marine> <mariner> <mariner> <mariner> <mariner> <mariner> tags adjective, feminine singular jfs noun, feminine singular nfs noun, masculine singular nms verb, 1st person, singular, present, indicative vlspi verb, 1st person, singular, present, subjunctive vlsps verb, 2nd person, singular, present, imperative v2spm verb, 3rd person, singular, present, indicative v3spi verb, 3rd person, singular, present, subjunctive v3sps Table 1: Morphological analyses of the word 'marine'." ></td>
	<td class="line x" title="37:213	The same word 'marine', inflected in all forms of the three syntactic categories (adjective, noun, and verb) would have 56 morphologically distinct forms, i.e. 4 for the adjective, 2 for 2 each of the nouns, and 48 for the verb." ></td>
	<td class="line x" title="38:213	At the same time, if we collapse the homographs, these 56 morphologically distinct forms get reduced to 37 homographically distinct forms and the ambiguity lies in the 19 forms which overlap across internal verb categories, but also across nouns and adjectives." ></td>
	<td class="line x" title="39:213	Table 1 shows 5 verb ambiguities, 2 noun ambiguities, a total of 8 homographs including the adjective form." ></td>
	<td class="line x" title="40:213	Part-of-speech Ambiguity of French words." ></td>
	<td class="line x" title="41:213	Once morphological analysis is completed, ambiguity of words is computed in order to locate the difficulties." ></td>
	<td class="line x" title="42:213	Figure 1 shows two corpora of different sizes and the number of words each tag contains." ></td>
	<td class="line x" title="43:213	The figure clearly exhibits that even though Corpus 2 is twice as large as Corpus 1, the distribution of words per tags is very similar, i.e. more than 50% of the words have only one tag and are thus unambiguous, 25% of the words have two tags, 11% of the words have three tags, and about 5% of the words have from four to eight tags." ></td>
	<td class="line x" title="44:213	i o Z corpus 2 (200,182 words) \ \ \\ corpus I (94,882 wo ) ~\ I I I I I I I 1 2 3 4 S 6 7 Number of Tags Figure 1: Number of words per ambiguity level in two different corpora 3 Problems with lexical probabilities There are several ways lexical probabilities could be estimated for a given language, each of them presenting problems: 1." ></td>
	<td class="line x" title="45:213	From raw text: a human tagger could manually disambiguate texts." ></td>
	<td class="line x" title="46:213	There are some problems though due to the fact that there are always words that are overseen (therefore improperly tagged) or there is disagreement between humans (on at least 5% of the words), 3 and cross-checking by another human is required." ></td>
	<td class="line x" title="47:213	In our system, we manually tagged about 76,000 words 1 in this way." ></td>
	<td class="line x" title="48:213	Bootstrapping from already tagged text: this technique generally consists of using a small tagged corpus to train a system and having the system tag another subset of the corpus that gets disambiguated later." ></td>
	<td class="line x" title="49:213	(Derouault and Merialdo, 1986) have used these techniques but the necessary human effort is still considerable." ></td>
	<td class="line x" title="50:213	3." ></td>
	<td class="line x" title="51:213	From the baseform of the word: one could estimate the frequency of the analyzed stem in the process of morphological analysis." ></td>
	<td class="line x" title="52:213	From the inflectional morpheme: similarly, one could estimate the probabifity of the inflectional morpheme given its stem." ></td>
	<td class="line x" title="53:213	This approach is often used for smoothing probabilities, but, considering the high ambiguity of some French suffixes, such as 'e', 'es', etc, it is doubtful that basing the estimates on the suffixes alone would give good results." ></td>
	<td class="line x" title="54:213	From unseen pairs of \[words,tags\]: for a given word, such as 'marine' that can have 8 possible tags, if only the instances \[marine, adj-fem-sing\], \[marine, noun-fem-sing\] are found in the training corpus, one could assume that the remaining unseen instances have a much lower probabifity." ></td>
	<td class="line x" title="55:213	This could create problems in making incorrect assumptions on words." ></td>
	<td class="line x" title="56:213	Out of all the possibifities outfined above, none seems feasible and robust enough." ></td>
	<td class="line x" title="57:213	Therefore, we decided to pay more attention to a different paradigm which captures more information about the word at a morphological and syntactic level." ></td>
	<td class="line x" title="58:213	4 The genotype solution In an attempt to capture the multiple word ambiguities on the one hand and the recurrence of these observations on the other, we came up with a new concept, called genotype." ></td>
	<td class="line x" title="59:213	In biology, the genotype refers to the content of genes or the pattern of genes in the cell." ></td>
	<td class="line x" title="60:213	As used in our context, the genotype is the set of part of speech tags associated with a word." ></td>
	<td class="line x" title="61:213	Each word has a genotype (or series of tags based on morphological features) assigned during morphological analysis, and words, according to their patterns, share the same genotype." ></td>
	<td class="line x" title="62:213	The genotype depends on the tagset, but not on any particular tagging method." ></td>
	<td class="line x" title="63:213	For example, the word 'marine' with the eight morphological analyses fisted in Table 1, has the genotype \[JFS NFS NMS vlsPI V1SPS V2SPM V3SPI V3SPS\] 2, each tag corresponding to an analysis, i.e. the list of potential tags for 'marine' as shown in Table 1." ></td>
	<td class="line x" title="64:213	For each genotype, we compute the frequency with which each of the tags occurs and we select this decision." ></td>
	<td class="line x" title="65:213	This paradigm has the advantage of capturing the morphological variation of words combined with the frequency with which they occur." ></td>
	<td class="line x" title="66:213	A genotype decision is the most frequent tag associated with a genotype in the training corpus." ></td>
	<td class="line x" title="67:213	As explained in Section 4.2, out of a trMning corpus of 76,000 tokens, we extracted a total of 429 unigram genotypes, 6650 bigram genotypes, and 23,802 trigram genotypes with their respective decisions." ></td>
	<td class="line x" title="68:213	1We wish to thank Anne Abeil\]~ and Thierry Poibeau for helping the manual tagging." ></td>
	<td class="line x" title="69:213	2JFS = adjective, feminine, singular; NFS = noun, feminine, singular; NMS = noun, masculine, singular; VISPI = verb, 1st person, singular, present, indicative; vlsPS = verb, 1st person, singular, present, subjunctive; V2SPM = verb, 2nd person, singular, present, imperative; v3sPI = verb, 3rd person, singular, present, indicative; v3sPs = verb, 3rd person, singular, present, subjunctive." ></td>
	<td class="line x" title="70:213	4 4.1 Power of genotypes The genotype concept allows generalizations to be made across words according to tag patterns, thereby gathering estimates not on words but on tag occurrences." ></td>
	<td class="line x" title="71:213	We discovered that in a training corpus of 76,000 tokens, lexical frequencies are not as reliable as genotype frequencies." ></td>
	<td class="line x" title="72:213	In order to illustrate this, Table 2 and Table 3 show convincing results using this approach." ></td>
	<td class="line x" title="73:213	Table 2 presents the set of words corresponding to the genotype \[NFP V2S\], and their resolution with respect to lexicM frequencies and genotype frequencies." ></td>
	<td class="line x" title="74:213	The table shows 12 words from the test corpus which, from a morphological point of view, can be either verb-2nd-person-singular (v2s) or noun-feminineplural (NFP); the first column contains always the same tag NFP, because of the genotype decision; we learned from the training corpus that at each time a word could be tagged NFP or V2S, it is 100% of the times NFP, 0% V2S, therefore the noun form is always picked over the verb form." ></td>
	<td class="line x" title="75:213	Out of the 12 words listed in the Table 2, 4 words (marked unseen in the table) could not be estimated using lexical frequencies alone since they do not appear in the training corpus." ></td>
	<td class="line x" title="76:213	However, since all of them belong to the same genotype, the 4 unseen occurrences are properly tagged." ></td>
	<td class="line x" title="77:213	oeuvres d~penses d@enses toiles affaires avances finances feuilles forces oeuvres t~.ches rdformes genotype lexical correct decision decision decision nfp nfp nfp nfp nfp nfp nfp nfp nfp nfp nfp nfp unseen nfp nfp unseen nfp unseen nfp nfp nfp unseeTt nfp nfp Table 2: \[NFP V2S\] genotype frequencies vs nfp nfp nfp nfp nfp nfp nfp nfp nfp nfp nfp nfp lexical frequencies In Table 3, we demonstrate that the genotype decision for the \[NMS vls v2s v3s\] genotype always favors the noun-masculine-singular form (NMS) over the verb forms (vls for verb-lst-personsingular, v2s for verb-2nd-person-singular, v3s for verb-3rd-person-singular)." ></td>
	<td class="line x" title="78:213	Out of the 12 words listed in Table 3, 5 do not occur in the training corpus and 4 of them can be properly tagged using the genotype estimates." ></td>
	<td class="line x" title="79:213	The word 'suicide', however, which should be tagged as a verb, was improperly tagged as a noun." ></td>
	<td class="line x" title="80:213	Note that we are only considering unigrams of genotypes, which tend to overgeneralize." ></td>
	<td class="line x" title="81:213	However, as shown in Section 4.3, the additional estimates of bigrams and trigrams will use the context to select a more appropriate tag." ></td>
	<td class="line x" title="82:213	4.2 Distribution of genotypes Among all parts of speech, there is a clear division between closed-class parts of speech, which include prepositions and conjunctions, and open-class ones, which includes verbs, nouns, and adjectives." ></td>
	<td class="line x" title="83:213	Similarly, we suggest that genotypes be classified in categories:  Closed-class genotypes contain at least one closed-class part-of-speech, e.g., 'des', which belongs to the \[P R\] (preposition, article) genotype." ></td>
	<td class="line x" title="84:213	5 Table 3: \[NMS suicide chiffre escompte escompte cercle doute nombre avantage pilote peigne doute groupe genotype lexical correct decision decision decision rims rims nms nms nms rims rims rims nms rims nms rims unseen rims unseen unseen v3s rims nms nms unseen nms nms nms nms nms nms nms nms nms unseen nms nms i nms nms nms vls v2s v3s\] genotype frequencies vs lexical frequencies  Semi closed-class genotypes contain only open-class parts-of-speech, but behave very similarly to the closed-class genotype, with respect to the small number of words often homograph in that genotype." ></td>
	<td class="line x" title="85:213	For instance, the word 'ills' (son \[singular and plural\], threads) with the low frequent genotype \[NM NMP\] or the word 'avions' (planes, (we) had) which belong to the genotype \[NFP V1P\]." ></td>
	<td class="line x" title="86:213	 Open-class genotypes contain all other genotypes, such as \[NFS vls v2s v3s\]." ></td>
	<td class="line x" title="87:213	This class, unlike the other two, is productive." ></td>
	<td class="line x" title="88:213	There are several facts which demonstrate the power of genotypes for disambiguation." ></td>
	<td class="line x" title="89:213	First, the number of genotypes on which the estimates are made is much smaller than the number of words on which to compute estimates." ></td>
	<td class="line x" title="90:213	Our results show that in the training corpus of 76,000 tokens, there are 10,696 words, and 429 genotypes." ></td>
	<td class="line x" title="91:213	Estimating probabilities on 429 genotypes rather than 10,696 words is an enormous gain." ></td>
	<td class="line x" title="92:213	Since the distributions in both cases have a very long tail, there are many more words than genotypes for which we cannot obtain reliable statistics." ></td>
	<td class="line x" title="93:213	As an example, we extracted the most frequent open-class genotypes from the training corpus (each of them occurring more than 100 times) shown in Table 4." ></td>
	<td class="line x" title="94:213	It is striking to notice that these 22 genotypes represent almost 10~ of the corpus." ></td>
	<td class="line x" title="95:213	The table shows the genotype in the first column, the number of occurrences in the second one, the part-of-speech distribution in the third one, the best genotype decision and the percent of this selection in the last column." ></td>
	<td class="line x" title="96:213	We can see that words belonging to the same genotype are likely to be tagged with the same tag; for example, the genotype \[NFS Vis V2S V3S\] is tagged as NFS." ></td>
	<td class="line x" title="97:213	That allows us to make predictions for words missing from the training corpus." ></td>
	<td class="line x" title="98:213	4.3 Contextual probabilities via bigram and trigram genotypes Using genotypes at the unigram level tends to result in overgeneralization, due to the fact that the genotype sets are too coarse." ></td>
	<td class="line x" title="99:213	In order to increase the accuracy of part-of-speech disambiguation, we need to give priority to trigrams over bigrams, and to bigrams over unigrams." ></td>
	<td class="line x" title="100:213	In a way similar to decision trees, Table 5 shows how the use of context allows for better disambiguation of genotype." ></td>
	<td class="line x" title="101:213	We have considered a typical ambiguous genotype \[.IMP NMP\] which occurs 607 times in the training corpus, almost evenly distributed between the two alternative 6 genotype nfs vls v2s v3s jms nms jmp nmp rims v3s nfp v2s jfs nfs nms vls v2s v3s nms qsms jfp nfp vls v2s v3s nmp v2s DinS v jms qsms jms nms qsms jfs nfs qsfs nfs nms nfs nms vls v2s v3s jfp nfp qsfp jms nms qsms v3s jfs nfs vls v2s v3s jmp qsmp jmp nmp qsmp # of occ." ></td>
	<td class="line x" title="102:213	899 734 607 612 441 401 405 distribution nfs(797) vls(0) v2s(0) v3s(100) jms(498) nms(230) nmp(291) jmp(316) nms(28) v3s(584) nfp(437) v2s(1) jfs(333) nfs(67) nms(351) vls(0) v2s(0) v3s(51) decision.fs(SS.7%) jms(67.8%) jmp(52.6%) v3s(95.4%) nfp(99.1%) jfs(83.0%) rims(86.7%) 325 292 263 259 249 222 213 169 131 nms(52) qsms(271) jfp(192) nfp(99) vls(3) v2s(0) v3s(259) nmp(254) v2s(1) DinS(21) v(22s) jms(24) qsms(197) jms(19) nms(33) qsms(161) jfs(8) nfs(llO) qsfs(51) nfs(67) nms(64) qsms(83.4%) jfp(65.8%) v3s(98.5%) nmp(98.1%) v(91.6%) qsms(88.7%) qsms(75.6%) nfs(65.1%) nfs(51.1%) 115 126 114 110 112 100 nfs(39) nms(49) vls(O v2s(0) v3s(27) jfp(1)2 nfp(55) qsfp(58) jms(2) nms(18) qsms(52) v3s(42) jfs(39) nfs(27) vls(1) v2s(0) v3s(42) jmp(S) qsmp(103) jmp(8) nmp(47) qsmp(45) nms(42.6%) qsfp(46.0%) qsms(45.6%) jfs(38.2%) qsmp(91.2%) nmp(47.0%) Table 4: The most frequent open-class genotypes tags, JMP and NMP." ></td>
	<td class="line x" title="103:213	As a result, if only unigram training data is used, the best candidate for that genotype would be JMP, occurring 316 out of 607 times." ></td>
	<td class="line x" title="104:213	However, choosing JMP only gives us 52.06% accuracy." ></td>
	<td class="line x" title="105:213	Table 5 clearly demonstrates that the contextual information around the genotype will bring this percentage up significantly." ></td>
	<td class="line x" title="106:213	As an example, let us consider the 5th fine of Table 5, where the number 17 is marked with a square." ></td>
	<td class="line x" title="107:213	In this case, we know that the \[JMP NMP\] genotype has a right context consisting of the genotype \[p r\] (4th column, 5th fine)." ></td>
	<td class="line x" title="108:213	In this case, it is no longer true that JMP is the best candidate." ></td>
	<td class="line x" title="109:213	Instead, NMP Occurs 71 out of 91 times and becomes the best candidate." ></td>
	<td class="line x" title="110:213	Overall, for all possible left and right contexts of \[JMP NMP\], the guess based on both the genotype and the single left or right contexts will be correct 433 times out of 536 (or 80.78%)." ></td>
	<td class="line x" title="111:213	In a similar fashion, the three possible trigram layouts (Left, Middle, and Right) are shown in fines 18-27." ></td>
	<td class="line x" title="112:213	They show that the performance based on trigrams is 95.90%." ></td>
	<td class="line x" title="113:213	This particular example provides strong evidence of the usefulness of contextual disambiguation with genotypes." ></td>
	<td class="line x" title="114:213	The fact that this genotype, very ambiguous as a unigram (52.06%), can be disambiguated as a noun or adjective according to context at the trigram stage with 95.90% accuracy demonstrates the strength of our approach." ></td>
	<td class="line x" title="115:213	4.4 Smoothing probabilities with genotypes In the context of a small training corpus, the problem of sparse data is more serious than with a larger tagged corpus." ></td>
	<td class="line x" title="116:213	Genotypes play an important role for smoothing probabilities." ></td>
	<td class="line x" title="117:213	By paying attention to tags only and thus ignoring the words themselves, this approach handles new words that have not been seen in the training corpus." ></td>
	<td class="line x" title="118:213	Table 6 shows how the training corpus provides coverage for n-gram genotypes that appear in the test corpus." ></td>
	<td class="line x" title="119:213	It is interesting to notice that only 7 n-gram Unigram pos." ></td>
	<td class="line x" title="120:213	total genotype 607 \[jmp nmp\] Bigram Left 230 Right 306 Trigram Left Middle decision distr, cor." ></td>
	<td class="line x" title="121:213	total cor." ></td>
	<td class="line x" title="122:213	total accuracy II Right \[jmp nmPl\[x \] \[jmp nmpl\[p r\] Limp nmpl\[nmp \] \[jmp nmp\]\[a\] \[p r\]\[jmp nmp\] \[b r\]\[jmp nmp\] \[nmp\]\[jmp nmp\] 32 \[jmp nmp\]\[p r\]\[nms\] \[jmp nmp\]\[jmp nmp\]\[x\] 44 \[p r\]\[jmp nmp\]\[p r\] \[b r\]\[jmp nmp\]\[p r\] 46 \[p rl\[nmp\]\[jmp nmp\] \[n z\]\[p rl\[jmp nmp\] jmp 316 316 607 316 nmp 291 jmp, x 71 771 102 433 nmp, x 31 jmp, p t 17 I 71 91 jmp, r 3 nmp, p 71 jmp, nmp 23 23 24 nmp, nmp 1 jmp, a 13 13 13 p, jmp 27 112 141 p, nmp 104 r, jmp 2 r, nmp 8 r, imp 22 72 94 r, nmp 72 nmp, imp 71 71 71 nmp, p, rims 21 21 21 117 imp, jmp, x 3 8 11 nmp, jmp, x 8 p, nmp, p 23 23 23 r, nmp, p 19 19 21 r, jmp, p 2 p, nmp, jmp 27 29 29 r, nmp, jmp 2 z, p, nmp 16 17 17 z, r, nmp 1 607 52.06% 536 80.78% 122 95.90% Table 5: Influence of context for n-gram genotype disambiguation." ></td>
	<td class="line x" title="123:213	12 out of 1564 unigram genotypes (0.8%) are not covered." ></td>
	<td class="line x" title="124:213	The training corpus covers 71.4% of the bigram genotypes that appear in the test corpus and 22.2% of the trigrams." ></td>
	<td class="line x" title="125:213	Coverage of Genotypes test corpus training corpus of genotypes ~ of genotypes % 1-grams 1564 1552 (99.2 %) 2-grams 1563 1116 \] (71.4 %) 3-grams 1562 346 \[ (22.2 %) Table 6: Coverage in the training corpus of n-gram genotypes that appear in the test corpus." ></td>
	<td class="line oc" title="126:213	5 Comparison with other approaches In some sense, this approach is similar to the notion of 'ambiguity classes' explained in (Kupiec, 1992) and (Cutting et al. , 1992) where words that belong to the same part-of-speech figure together." ></td>
	<td class="line o" title="127:213	In this approach, they use the notion of word equivalence or ambiguity classes to describe words belonging to the same part-of-speech categories." ></td>
	<td class="line x" title="128:213	In our work, the entire algorithm bases estimations on genotype only, filtering down the ambiguities and resolving them with statistics." ></td>
	<td class="line x" title="129:213	Moreover, the estimation is achieved on a sequence of n-gram genotypes." ></td>
	<td class="line x" title="130:213	Also, the refinement that is contMned in our system reflects the real morphological ambiguities, due to the rich nature of the morphological output and the choice of tags." ></td>
	<td class="line x" title="131:213	There are three main differences between their work and ours." ></td>
	<td class="line o" title="132:213	First, in their work, the most common words are estimated individually and the less common ones are 8 put together in their respective ambiguity classes; in our work, every word is equally treated by its respective genotype." ></td>
	<td class="line o" title="133:213	Second, in their work, ambiguity classes can be marked with a preferred tag in order to help disambiguation whereas in our work, there is no special annotation since words get disambiguated through the sequential application of the modules." ></td>
	<td class="line o" title="134:213	Third, and perhaps the most important, in our system, the linguistic and statistical estimations are entirely done on the genotypes only, regardless of the words." ></td>
	<td class="line x" title="135:213	Words are not estimated individually given their class categories; rather, genotypes are estimated separately from the words or in the context of other genotypes (biand tri-gram probabilities)." ></td>
	<td class="line x" title="136:213	(Brill, 1995) presents a rule-based part-of-speech tagger for unsupervised training corpus." ></td>
	<td class="line x" title="137:213	Some of the rules of his system and the fact that he uses a minimal training corpus suggests some similarities with our system, but the main aim of the work is to investigate methods to combine supervised and unsupervised training in order to come up with a highly performing tagger." ></td>
	<td class="line oc" title="138:213	(Chanod and Tapanainen, 1995) compare two tagging frameworks for tagging French, one that is statistical, built upon the Xerox tagger (Cutting et al. , 1992), and another based on linguistic constraints only." ></td>
	<td class="line x" title="139:213	The contraints can be 100% accurate or describe the tendency of a particular tagging choice." ></td>
	<td class="line x" title="140:213	The contraint-based tagger is proven to have better performance than the statistical one, since rule writing is more handlable or more controllable than adjusting the parameters of the statistical tagger." ></td>
	<td class="line x" title="141:213	It is difficult to compare any kind of performance since their tagset is very small, i.e. 37 tags, including a number of wordspecific tags (which reduces further the number of 'real' tags), and does not account for several morphological features, such as gender, number for pronouns, etc. Moreover, categories that can be very ambiguous, such as coordinating conjunctions, subordinating conjunctions, relative and interrogative pronouns which tend to be collapsed; consequently, the disambiguation is simplified and results cannot be compared." ></td>
	<td class="line x" title="142:213	6 Implementation and performance of the part-of-speech tagger We have developed a part-of-speech tagger using only a finite-state machine framework." ></td>
	<td class="line x" title="143:213	The input string is represented as a finite-state generator, and the tagging is obtained through composition with a pipeline of finite-state transducers (FST's)." ></td>
	<td class="line x" title="144:213	Besides the modules for pre-processing and tokenization, the tagger includes a morphological FST and a statistical FST, which incorporates linguistic and statistical knowledge." ></td>
	<td class="line x" title="145:213	We have used a toolkit developed at AT&T Bell Laboratories (Pereira et al. , 1994) which manipulates weighted and unweighted finite-state machines (acceptors or transducers)." ></td>
	<td class="line x" title="146:213	Using these tools, we have created a set of programs which generate finitestate transducers from descriptions of linguistic rules (in the form of negative constraints) and for encoding distribution information obtained through statistical learning." ></td>
	<td class="line x" title="147:213	Statistical decisions on genotypes are represented by weights the lower cost, the higher the chance of a particular tag to be picked." ></td>
	<td class="line x" title="148:213	With this representation, we are able to prefer one n-gram decision over another based on the cost." ></td>
	<td class="line x" title="149:213	The morphological FST is generated automatically from a large dictionary of French of about 90,000 entries and on-line corpora, such as Le Monde Newspapers (ECI, 1989 and 1990)." ></td>
	<td class="line x" title="150:213	It takes the text as input and produces an FST that encodes each possible tagging of the input text as one distinct path from the start state to the final state." ></td>
	<td class="line x" title="151:213	The statistical FST is created from 1-gram, 2-gram, and 3-gram genotype data obtained empirically from the training corpus." ></td>
	<td class="line x" title="152:213	It encodes all 1, 2, 3-grams of genotypes extracted from the training corpus with a cost determined as a function of the frequency of the genotype decision in the training corpus." ></td>
	<td class="line x" title="153:213	Table 7 shows how costs are computed for a specific bigram and how these costs are used to make a tagging decision." ></td>
	<td class="line x" title="154:213	The 9 bigram in the example, \[p r\] \[jmp nmp\], occurs 306 times in the training corpus." ></td>
	<td class="line x" title="155:213	All possible taggings, i.e. \[p\] limp\], \[p\] \[nmp\], \[r\] \[jmp\], and \[r\] \[nmp\] appear in the training corpus." ></td>
	<td class="line x" title="156:213	The sub-FST that corresponds to this bigram of genotypes will have \[p r\] \[jmp nmp\] on its input and all 4 possible taggings on its output." ></td>
	<td class="line x" title="157:213	Each tagging sequence has a different costs." ></td>
	<td class="line x" title="158:213	Let f be the total count of the genotype bigram." ></td>
	<td class="line x" title="159:213	Let ft be the number of cases that the tagging is t, for all possible taggings t (in this example there are 4 possible taggings)." ></td>
	<td class="line x" title="160:213	The cost of the transition for tagging t is the negative logarithm of ft divided by f: -log(ft/f)." ></td>
	<td class="line x" title="161:213	The selected transition is the one with the lowest cost; the example in Table 7 illustrates the computation of costs with \[p\] \[nmp\], the selected tagging in bold." ></td>
	<td class="line x" title="162:213	genotype bigram tagging frequency \[p r\] \[imp nmp\] p, jmp p, nmp r, jmp r, nmp cost 27/306 2.43 104/306 1.08 2/306 5.03 8/306 3.64 Table 7: An example of cost computation for the bigram FST \[p r\] \[jmp nmp\]." ></td>
	<td class="line x" title="163:213	In a similar way, the statistical FST contains paths for unigrams and trigrams." ></td>
	<td class="line x" title="164:213	In order to prefer trigrams over bigrams, and bigrams over unigrams, we have added a biased cost to some transitions." ></td>
	<td class="line x" title="165:213	The empirically determined values of the biased cost are as follows: trigram biased cost < bigram biased cost < unigram biased cost." ></td>
	<td class="line x" title="166:213	If a certain bigram or trigram does not appear in the training corpus, the FST will still have a corresponding path, but at a higher cost." ></td>
	<td class="line x" title="167:213	Since negative constraints (such as 'article' followed by 'verb') reflect n-grams that are impossible linguistically and therefore have an expected frequency of appearance equal to 0, we assign them a very high cost (note that in order to keep the graph connected, we cannot assign a cost of ~x~)." ></td>
	<td class="line x" title="168:213	To make the use of biased cost clear, Table 8 shows the unigrams \[p r\] and \[jmp nmp\] that compose the bigram described in Table 7 and the corresponding transition costs." ></td>
	<td class="line x" title="169:213	genotype unigram tagging frequency \[p r\] p 6645/6883 r 238/6883 \[jmp nmp\] jmp 316/607 nmp 291/607 cost biased cost 0.04 1.04 3.36 4.36 0.65 1.65 0.73 1.73 Table 8: An example of biased cost for the unigram sub-FST's \[p r\] and \[jmp nmp\]." ></td>
	<td class="line x" title="170:213	Figure 2 presents the FST that corresponds to Table 7 and Table 8." ></td>
	<td class="line x" title="171:213	The top part shows how the genotype bigram \[p r\] \[jmp nmp\] can be tagged as a sequence of two unigrams; the bottom part uses one bigram to tag it." ></td>
	<td class="line x" title="172:213	The notation on all arcs in the FST is the following: input string : output string / cost e.g., \[p hi'." ></td>
	<td class="line x" title="173:213	p / 1.04 The input is a genotype n-gram, the output represents a possible tag n-gram with the corresponding cost." ></td>
	<td class="line x" title="174:213	The FST shown in Figure 2 is part of a much larger FST containing 2.8 million ares." ></td>
	<td class="line x" title="175:213	The cheapest path for tagging the sequence of two genotypes \[p r\] \[jmp nmp\] can go either 10 Figure 2: Example of an FST that tags the genotype bigram \[p r\] \[jmp nmp\] through one bigram transition shown in bold face in Table 7, or through two adjacent unigram transitions shown in bold face in Table 8." ></td>
	<td class="line x" title="176:213	The corresponding paths through the FST are shown in Figure 2." ></td>
	<td class="line x" title="177:213	In the first case (bigrams), the tagging of \[p\], \[nmp\] is at a cost of 1.08, whereas in the other case (unigrams), the cheapest path or the lowest cost includes the two transitions \[p\] and limp\] for a total cost of 1.04 + 1.65 = 2.69." ></td>
	<td class="line x" title="178:213	In this case, not only do bigrams have precedence over unigrams, but the choice of the tagging sequence \[p\], \[nmp\] is also better than the sequence \[p\] \[jmp\], as it takes into account the context information." ></td>
	<td class="line x" title="179:213	Similarly, if a trigram contained a bigram as a sub-FST, typically the cost of going through the trigram would be smaller than the cost of going through a bigram and a unigram." ></td>
	<td class="line x" title="180:213	In the case where two consecutive genotype unigrams do not compose a bigram seen in the training corpus, there is no context information that can be applied and only the information of the tagging of the individual unigrams is used." ></td>
	<td class="line x" title="181:213	The tagger is based on a tagset of 72 parts of speech." ></td>
	<td class="line x" title="182:213	As said earlier, the training corpus was manually tagged and contained 76,000 words." ></td>
	<td class="line x" title="183:213	The test corpus, also manually tagged, contained 1,500 words." ></td>
	<td class="line x" title="184:213	Taking into account the large number of parts of speech, the tagger disambiguates correctly about 95% of unrestricted text." ></td>
	<td class="line x" title="185:213	We are in the process of improving the tagger performance in refining rules and biased costs." ></td>
	<td class="line x" title="186:213	'7 Steps for building an optimal training corpus This section explains the motivations of our claims for developing taggers for a language." ></td>
	<td class="line x" title="187:213	The following steps are based on our experience and, we believe, will extend to a wide range of language types." ></td>
	<td class="line x" title="188:213	1." ></td>
	<td class="line x" title="189:213	Study morpho-syntactic ambiguity and word frequencies: Part-of-speech ambiguities must be observed as a function of the word frequencies as shown in Section 2." ></td>
	<td class="line x" title="190:213	2. Analyze morphology and morphological features in order to evaluate the ambiguity of the language." ></td>
	<td class="line x" title="191:213	As shown in Section 2, some suffixes may disambiguate a certain number of words, whereas others may be truly ambiguous and overlap over several categories of words." ></td>
	<td class="line x" title="192:213	3." ></td>
	<td class="line x" title="193:213	Determine concise tagset based on trade-off between tagset size and computational complexity." ></td>
	<td class="line x" title="194:213	This requires system tuning and is often dependent on the application." ></td>
	<td class="line x" title="195:213	The more tags, the harder the estimation of probabilities, and the sparser the data." ></td>
	<td class="line x" title="196:213	Having a concise set of tags is therefore a priority." ></td>
	<td class="line x" title="197:213	11 4." ></td>
	<td class="line x" title="198:213	Obtain maximum genotype coverage: genotypes must first be separated into closed, semi-closed, and open class." ></td>
	<td class="line x" title="199:213	Then, the first two classes must be exhaustively covered since their number is relatively small." ></td>
	<td class="line x" title="200:213	Last, open-class genotypes should be examined by order of frequency; since their number is finite, they can also be exhaustively covered." ></td>
	<td class="line x" title="201:213	5." ></td>
	<td class="line x" title="202:213	Capture contextual probabilities: genotypes must be considered in context." ></td>
	<td class="line x" title="203:213	As described in Section 4.3, bigram and trigram genotypes give accurate estimates of the morphosyntactic variations of the language." ></td>
	<td class="line x" title="204:213	We believe that concentrating efforts on these issues will allow part-of-speech tagger developers to optimize time and effort in order to develop adequate basic training material." ></td>
	<td class="line x" title="205:213	8 Conclusion We explored the morpho-syntactic ambiguities of a language, basing our experiments on French." ></td>
	<td class="line x" title="206:213	Several ways to estimate lexical probabilities were discussed and a new paradigm, the genotype, was presented." ></td>
	<td class="line x" title="207:213	This paradigm has the advantage to capture the morphological variation of words along with the frequency at which they occur." ></td>
	<td class="line x" title="208:213	A methodology is presented in order to optimize the construction of a restricted training corpus for developing taggers." ></td>
	<td class="line x" title="209:213	In order to disambiguate word part-of-speech with a small training corpus, genotypes turn out to be much easier to model than the words themselves." ></td>
	<td class="line x" title="210:213	They offer a successful solution to the small training corpus problem as well as to the problem of data sparsness." ></td>
	<td class="line x" title="211:213	Compared to lexical probabilities, they give much more reliable accounts, since only 429 genotypes need to be estimated instead of 10,696 words for lexical probabilities." ></td>
	<td class="line x" title="212:213	Results are even more convincing when genotypes are used in context and bigrams and trigrams are applied to disambiguate." ></td>
	<td class="line x" title="213:213	Additionally, they are used for smoothing which is a particularly important issue in the context of small training corpus." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W96-0102
MBT: A Memory-Based Part Of Speech Tagger-Generator
Daelemans, Walter;Zavrel, Jakub;Berck, Peter;Gillis, Steven;"></td>
	<td class="line x" title="1:379	MBT: A Memory-Based Part of Speech Tagger-Generator Walter Daelemans, Jakub Zavrel Computational Linguistics and AI Tilburg University P.O. Box 90153, NL-5000 LE Tilburg {walter." ></td>
	<td class="line x" title="2:379	daelemans, zavrel}~kub, nl Peter Berck, Steven Gillis Center for Dutch Language and Speech University of Antwerp Universiteitsplein 1, B-2610 Wilrijk {peter." ></td>
	<td class="line x" title="3:379	berck, steven, gillis}@uia, ua." ></td>
	<td class="line x" title="4:379	ac." ></td>
	<td class="line x" title="5:379	be Abstract We introduce a memory-based approach to part of speech tagging." ></td>
	<td class="line x" title="6:379	Memory-based learning is a form of supervised learning based on similarity-based reasoning." ></td>
	<td class="line x" title="7:379	The part of speech tag of a word in a particular context is extrapolated from the most similar cases held in memory." ></td>
	<td class="line x" title="8:379	Supervised learning approaches are useful when a tagged corpus is available as an example of the desired output of the tagger." ></td>
	<td class="line x" title="9:379	Based on such a corpus, the tagger-generator automatically builds a tagger which is able to tag new text the same way, diminishing development time for the construction of a tagger considerably." ></td>
	<td class="line x" title="10:379	Memory-based tagging shares this advantage with other statistical or machine learning approaches." ></td>
	<td class="line x" title="11:379	Additional advantages specific to a memory-based approach include (i) the relatively small tagged corpus size sufficient for training, (ii) incremental learning, (iii) explanation capabilities, (iv) flexible integration of information in case representations, (v) its non-parametric nature, (vi) reasonably good results on unknown words without morphological analysis, and (vii) fast learning and tagging." ></td>
	<td class="line x" title="12:379	In this paper we show that a large-scale application of the memory-based approach is feasible: we obtain a tagging accuracy that is on a par with that of known statistical approaches, and with attractive space and time complexity properties when using IGTree, a tree-based formalism for indexing and searching huge case bases." ></td>
	<td class="line x" title="13:379	The use of IGTree has as additional advantage that optimal context size for disambiguation is dynamically computed." ></td>
	<td class="line x" title="14:379	1 Introduction Part of Speech (POS) tagging is a process in which syntactic categories are assigned to words." ></td>
	<td class="line x" title="15:379	It can be seen as a mapping from sentences to strings of tags." ></td>
	<td class="line x" title="16:379	Automatic tagging is useful for a number of applications: as a preprocessing stage to parsing, in information retrieval, in text to speech systems, in corpus linguistics, etc. The two factors determining the syntactic category of a word are its lexical probability (e.g. without context, man is more probably a noun than a verb), and its contextual probability (e.g. after a pronoun, man is more probably a verb than a noun, as in they man the boats)." ></td>
	<td class="line x" title="17:379	Several approaches have been proposed to construct automatic taggers." ></td>
	<td class="line oc" title="18:379	Most work on statistical methods has used n-gram models or Hidden Markov Model-based taggers (e.g. Church, 1988; DeRose, 1988; Cutting et al. 1992; Merialdo, 1994, etc.)." ></td>
	<td class="line o" title="19:379	In 14 these approaches, a tag sequence is chosen for a sentence that maximizes the product of lexical and contextual probabilities as estimated from a tagged corpus." ></td>
	<td class="line x" title="20:379	In rule-based approaches, words are assigned a tag based on a set of rules and a lexicon." ></td>
	<td class="line x" title="21:379	These rules can either be hand-crafted (Garside et al. , 1987; Klein & Simmons, 1963; Green & Rubin, 1971), or learned, as in Hindle (1989) or the transformation-based error-driven approach of Brill (1992)." ></td>
	<td class="line x" title="22:379	In a memory-based approach, a set of cases is kept in memory." ></td>
	<td class="line x" title="23:379	Each case consists of a word (or a lexical representation for the word) with preceding and following context, and the corresponding category for that word in that context." ></td>
	<td class="line x" title="24:379	A new sentence is tagged by selecting for each word in the sentence and its context the most similar case(s) in memory, and extrapolating the category of the word from these 'nearest neighbors'." ></td>
	<td class="line x" title="25:379	A memorybased approach has features of both learning rule-based taggers (each case can be regarded as a very specific rule, the similarity based reasoning as a form of conflict resolution and rule selection mechanism) and of stochastic taggers: it is fundamentally a form of k-nearest neighbors (k-nn) modeling, a well-known non-parametric statistical pattern recognition technique." ></td>
	<td class="line x" title="26:379	The approach in its basic form is computationally expensive, however; each new word in context that has to be tagged, has to be compared to each pattern kept in memory." ></td>
	<td class="line x" title="27:379	In this paper we show that a heuristic case base compression formalism (Daelemans et al. , 1996), makes the memory-based approach computationally attractive." ></td>
	<td class="line x" title="28:379	2 Memory-Based Learning Memory-based Learning is a form of supervised, inductive learning from examples." ></td>
	<td class="line x" title="29:379	Examples are represented as a vector of feature values with an associated category label." ></td>
	<td class="line x" title="30:379	During training, a set of examples (the training set) is presented in an incremental fashion to the classifier, and added to memory." ></td>
	<td class="line x" title="31:379	During testing, a set of previously unseen feature-value patterns (the test set) is presented to the system." ></td>
	<td class="line x" title="32:379	For each test pattern, its distance to all examples in memory is computed, and the category of the least distant instance(s) is used as the predicted category for the test pattern." ></td>
	<td class="line x" title="33:379	The approach is based on the assumption that reasoning is based on direct reuse of stored experiences rather than on the application of knowledge (such as rules or decision trees) abstracted from experience." ></td>
	<td class="line x" title="34:379	In AI, the concept has appeared in several disciplines (from computer vision to robotics), using terminology such as similarity-based, example-based, memory-based, exemplarbased, case-based, analogical, lazy, nearest-neighbour, and instance-based (Stanfill and Waltz, 1986; Kolodner, 1993; Aha et al. 1991; Salzberg, 1990)." ></td>
	<td class="line x" title="35:379	Ideas about this type of analogical reasoning can be found also in non-mainstream linguistics and pyscholinguistics (Skousen, 1989; Derwing ~ Skousen, 1989; Chandler, 1992; Scha, 1992)." ></td>
	<td class="line x" title="36:379	In computational linguistics (apart from incidental computational work of the linguists referred to earlier), the general approach has only recently gained some popularity: e.g., Cardie (1994, syntactic and semantic disambiguation); Daelemans (1995, an overview of work in the early nineties on memory-based computational phonology and morphology); Jones (1996, an overview of example-based machine translation research); Federici and Pirrelli (1996)." ></td>
	<td class="line x" title="37:379	2.1 Similarity Metric Performance of a memory-based system (accuracy on the test set) crucially depends on the distance metric (or similarity metric) used." ></td>
	<td class="line x" title="38:379	The most straightforward distance metric would be the one in equation (1), where X and Y are the patterns to be compared, and ~i(x~, y/) is the distance between the values of the i-th feature in a pattern with n features." ></td>
	<td class="line x" title="39:379	15 A(X, Y) = ~ ~(xi, yi) (1) i=1 Distance between two values is measured using equation (2), an overlap metric, for symbolic features (we will have no numeric features in the tagging application)." ></td>
	<td class="line x" title="40:379	5(xi, Yi) = 0 if xi = Yi, else 1 (2) We will refer to this approach as IB1 (Aha et al. , 1991)." ></td>
	<td class="line x" title="41:379	We extended the algorithm described there in the following way: in case a pattern is associated with more than one category in the training set (i.e. the pattern is ambiguous), the distribution of patterns over the different categories is kept, and the most frequently occurring category is selected when the ambiguous pattern is used to extrapolate from." ></td>
	<td class="line x" title="42:379	In this distance metric, all features describing an example are interpreted as being equally important in solving the classification problem, but this is not necessarily the case." ></td>
	<td class="line x" title="43:379	In tagging, the focus word to be assigned a category is obviously more relevant than any of the words in its context." ></td>
	<td class="line x" title="44:379	We therefore weigh each feature with its information gain; a number expressing the average amount of reduction of training set information entropy when knowing the value of the feature (Daelemans & van de Bosch, 1992, Quinlan, 1993; Hunt et al. 1966) (Equation 3)." ></td>
	<td class="line x" title="45:379	We will call this algorithm IB-IG." ></td>
	<td class="line x" title="46:379	Y) = (3) i:1 3 IGTrees Memory-based learning is an expensive algorithm: of each test item, all feature values must be compared to the corresponding feature values of all training items." ></td>
	<td class="line x" title="47:379	Without optimisation, it has an asymptotic retrieval complexity of O(NF) (where N is the number of items in memory, and F the number of features)." ></td>
	<td class="line x" title="48:379	The same asymptotic complexity is of course found for memory storage in this approach." ></td>
	<td class="line x" title="49:379	We use IGTrees (Daelemans et al. 1996) to compress the memory." ></td>
	<td class="line x" title="50:379	IGTree is a heuristic approximation of the IB-IG algorithm." ></td>
	<td class="line x" title="51:379	3.1 The IGTree Algorithms IGTree combines two algorithms: one for compressing a case base into a trees, and one for retrieving classification information from these trees." ></td>
	<td class="line x" title="52:379	During the construction of IGTree decision trees, cases are stored as paths of connected nodes." ></td>
	<td class="line x" title="53:379	All nodes contain a test (based on one of the features) and a class label (representing the default class at that node)." ></td>
	<td class="line x" title="54:379	Nodes are connected via arcs denoting the outcomes for the test (feature values)." ></td>
	<td class="line x" title="55:379	A feature relevance ordering technique (in this case information gain, see Section 2.1) is used to determine the order in which features are used as tests in the tree." ></td>
	<td class="line x" title="56:379	This order is fixed in advance, so the maximal depth of the tree is always equal to the number of features, and at the same level of the tree, all nodes have the same test (they are an instance of oblivious decision trees; cf.Langley ~ Sage, 1994)." ></td>
	<td class="line x" title="58:379	The reasoning behind this reorganisation (which is in fact a compression) is that when the computation of feature relevance points to one feature clearly being the most important in classification, search can be restricted to matching a test case to those stored cases that have the same feature value at that feature." ></td>
	<td class="line x" title="59:379	Besides restricting search to those memory cases that match only on this feature, the case memory can be optimised by further restricting search to the 16 |1 I I Procedure BUILD-IG-TI:tEE: Input:  A training set T of cases with their classes (start value: a full case base),  an information-gain-ordered list of features (tests) FiFn (start value: F1Fn)." ></td>
	<td class="line x" title="60:379	Output: A (sub)tree." ></td>
	<td class="line x" title="61:379	1." ></td>
	<td class="line x" title="62:379	If T is unambiguous (all cases in T have the same class c), create a leaf node with class label c. 2." ></td>
	<td class="line x" title="63:379	Else if i = (n + 1), create a leaf node with as label the class occurring most frequently in T. 3." ></td>
	<td class="line x" title="64:379	Otherwise, until i = n (the number of features)  Select the first feature (test) Fi in FiFn, and construct a new node N for feature Fi, and as default class c (the class occurring most frequently in T)." ></td>
	<td class="line x" title="65:379	 Partition T into subsets TITm according to the values vlvm which occur for Fi in T (cases with the same value for this feature in the same subset)." ></td>
	<td class="line x" title="66:379	 For each je1, , m}: BUILD-IG-TREE (Tj, Fi+iF,~), connect the root of this subtree to N and label the arc with vj." ></td>
	<td class="line x" title="67:379	Figure 1: Algorithm for building IGTrees ('BUILD-IG-TREE')." ></td>
	<td class="line x" title="68:379	second most important feature, followed by the third most important feature, etc. A considerable compression is obtained as similar cases share partial paths." ></td>
	<td class="line x" title="69:379	Instead of converting the case base to a tree in which all cases are fully represented as paths, storing all feature values, we compress the tree even more by restricting the paths to those input feature values that disambiguate the classification from all other cases in the training material." ></td>
	<td class="line x" title="70:379	The idea is that it is not necessary to fully store a case as a path when only a few feature values of the case make its classification unique." ></td>
	<td class="line x" title="71:379	This implies that feature values that do not contribute to the disambiguation of the case classification (i.e. , the values of the features with lower feature relevance values than the the lowest value of the disambiguating features) are not stored in the tree." ></td>
	<td class="line x" title="72:379	In our tagging application, this means that only context feature values that actually contribute to disambiguation are used in the construction of the tree." ></td>
	<td class="line x" title="73:379	Leaf nodes contain the unique class label corresponding to a path in the tree." ></td>
	<td class="line x" title="74:379	Nonterminal nodes contain information about the most probable or default classification given the path thus far, according to the bookkeeping information on class occurrences maintained by the tree construction algorithm." ></td>
	<td class="line x" title="75:379	This extra information is essential when using the tree for classification." ></td>
	<td class="line x" title="76:379	Finding the classification of a new case involves traversing the tree (i.e. , matching all feature values of the test case with arcs in the order of the overall feature information gain), and either retrieving a classification when a leaf is reached, or using the default classification on the last matching non-terminal node if a feature-value match fails." ></td>
	<td class="line x" title="77:379	A final compression is obtained by pruning the derived tree." ></td>
	<td class="line x" title="78:379	All leaf-node daughters of a mother node that have the same class as that node are removed from the tree, as their class information does not contradict the default class information already present at the mother node." ></td>
	<td class="line x" title="79:379	Again, this compression does not affect IGTree's generalisation performance." ></td>
	<td class="line x" title="80:379	The recursive algorithms for tree construction (except the final pruning) and retrieval are given in Figures 1 and 2." ></td>
	<td class="line x" title="81:379	For a detailed discussion, see Daelemans et al.(1996)." ></td>
	<td class="line x" title="83:379	17 Procedure SEARCH-IG-TI:tEE: Input:  The root node N of a subtree (start value: top node of a complete IGTree),  an unlabeled case I with information-gain-ordered feature values fifn (start value: flf,~)." ></td>
	<td class="line x" title="84:379	Output: A class label." ></td>
	<td class="line x" title="85:379	1." ></td>
	<td class="line x" title="86:379	If N is a leaf node, output default class c associated with this node." ></td>
	<td class="line x" title="87:379	2." ></td>
	<td class="line x" title="88:379	Otherwise, if test Fi of the current node does not originate an arc labeled with ffi, output default class c associated with N. 3." ></td>
	<td class="line x" title="89:379	Otherwise,  new node M is the end node of the arc originating from N with as label fi." ></td>
	<td class="line x" title="90:379	 SEARCH-IG-TREE (M, f~+lf,~) Figure 2: Algorithm for searching IGTrees ('SEARCH-IG-TREE')." ></td>
	<td class="line x" title="91:379	3.2 IGTree Complexity The asymptotic complexity of IGTree (i.e, in the worst case) is extremely favorable." ></td>
	<td class="line x" title="92:379	Complexity of searching a query pattern in the tree is proportional to F * log(V), where F is the number of features (equal to the maximal depth of the tree), and V is the average number of values per feature (i.e. , the average branching factor in the tree)." ></td>
	<td class="line x" title="93:379	In IB1, search complexity is O(N * F) (with N the number of stored cases)." ></td>
	<td class="line x" title="94:379	Retrieval by search in the tree is independent from the number of training cases, and therefore especially useful for large case bases." ></td>
	<td class="line x" title="95:379	Storage requirements are proportional to N (compare O(N * F) for IB1)." ></td>
	<td class="line x" title="96:379	Finally, the cost of building the tree on the basis of a set of cases is proportional to N * log(V)  F in the worst case (compare O(N) for training in IB1)." ></td>
	<td class="line x" title="97:379	In practice, for our part-of-speech tagging experiments, IGTree retrieval is 100 to 200 times faster than normal memory-based retrieval, and uses over 95% less memory." ></td>
	<td class="line x" title="98:379	4 Architecture of the Tagger The architecture takes the form of a tagger generator, given a corpus tagged with the desired tag set, a POS tagger is generated which maps the words of new text to tags in this tag set according to the same systematicity." ></td>
	<td class="line x" title="99:379	The construction of a POS tagger for a specific corpus is achieved in the following way." ></td>
	<td class="line x" title="100:379	Given an annotated corpus, three datastructures are automatically extracted: a lexicon, a case base for known words (words occurring in the lexicon), and a case base for unknown words." ></td>
	<td class="line x" title="101:379	Case Bases are indexed using IGTree." ></td>
	<td class="line x" title="102:379	During tagging, each word in the text to be tagged is looked up in the lexicon." ></td>
	<td class="line x" title="103:379	If it is found, its lexical representation is retrieved and its context is determined, and the resulting pattern is looked up in the known words case base." ></td>
	<td class="line x" title="104:379	When a word is not found in the lexicon, its lexical representation is computed on the basis of its form, its context is determined, and the resulting pattern is looked up in the unknown words case base." ></td>
	<td class="line x" title="105:379	In each case, output is a best guess of the category for the word in its current context." ></td>
	<td class="line x" title="106:379	In the remainder of this section, we will describe each step in more detail." ></td>
	<td class="line x" title="107:379	We start from a training set of tagged sentences T. 18 4.1 Lexicon Construction A lexicon is extracted from T by computing for each word in T the number of times it occurs with each category." ></td>
	<td class="line x" title="108:379	E.g. when using the first 2 million words of the Wall Street Journal corpus 1 as T, the word once would get the lexical definition RB: 330; IN: 77, i.e. once was tagged 330 times as an adverb, and 77 times as a preposition/subordinating conjunction." ></td>
	<td class="line x" title="109:379	~ Using these lexical definitions, a new, possibly ambiguous, tag is produced for each word type." ></td>
	<td class="line x" title="110:379	E.g. once would get a new tag, representing the category of words which can be both adverbs and prepositions/conjunctions (RB-IN)." ></td>
	<td class="line x" title="111:379	Frequency order is taken into account in this process: if there would be words which, like once, can be RB or IN, but more frequently IN than RB (e.g. the word below), then a different tag (IN-RB) is assigned to these words." ></td>
	<td class="line x" title="112:379	The original tag set, consisting of 44 morphosyntactic tags, was expanded this way to 419 (possibly ambiguous) tags." ></td>
	<td class="line x" title="113:379	In the WSJ example, the resulting lexicon contains 57962 word types, 7464 (13%) of which are ambiguous." ></td>
	<td class="line x" title="114:379	On the same training set, 76% of word tokens are ambiguous." ></td>
	<td class="line x" title="115:379	When tagging a new sentence, words are looked up in the lexicon." ></td>
	<td class="line x" title="116:379	Depending on whether or not they can be found there, a case representation is constructed for them, and they are retrieved from either the known words case base or the unknown words case base." ></td>
	<td class="line x" title="117:379	4.2 Known Words A windowing approach (Sejnowski & Rosenberg, 1987) was used to represent the tagging task as a classification problem." ></td>
	<td class="line x" title="118:379	A case consists of information about a focus word to be tagged, its left and right context, and an associated category (tag) valid for the focus word in that context." ></td>
	<td class="line x" title="119:379	There are several types of information which can be stored in the case base for each word, ranging from the words themselves to intricate lexical representations." ></td>
	<td class="line x" title="120:379	In the preliminary experiments described in this paper, we limited this information to the possibly ambiguous tags of words (retrieved from the lexicon) for the focus word and its context to the right, and the disambiguated tags of words for the left context (as the result of earlier tagging decisions)." ></td>
	<td class="line x" title="121:379	Table 1 is a sample of the case base for the first sentence of the corpus (Pierre Vinken, 61 years old, will join the board as a nonexecutive director nov. 29) when using this case representation." ></td>
	<td class="line x" title="122:379	The final column shows the target category; the disambiguated tag for the focus word." ></td>
	<td class="line x" title="123:379	We will refer to this case representation as ddfat (d for disambiguated, f for focus, a for ambiguous, and t for target)." ></td>
	<td class="line x" title="124:379	The information gain values are given as well." ></td>
	<td class="line x" title="125:379	A search among a selection of different context sizes suggested ddfat as a suitable case representation for tagging known words." ></td>
	<td class="line x" title="126:379	An interesting property of memory-based learning is that case representations can be easily extended with different sources of information if available (e.g. feedback from a parser in which the tagger operates, semantic types, the words themselves, lexical representations of words obtained from a different source than the corpus, etc.)." ></td>
	<td class="line x" title="127:379	The information gain feature relevance ordering technique achieves a delicate relevance weighting of different information sources when they are fused in a single case representation." ></td>
	<td class="line x" title="128:379	The window size used by the algorithm will also dynamically change depending on the information present in the context for the disambiguation of a particular focus symbol (see Schfitze et al. , 1994, and Pereira et al. , 1995 1ACL Data Collection Initiative CD-ROM 1, September 1991." ></td>
	<td class="line x" title="129:379	2We disregarded a category associated with a word when less than 10% of the word tokens were tagged with that category." ></td>
	<td class="line x" title="130:379	This way, noise in the training material is filtered out." ></td>
	<td class="line x" title="131:379	The value for this parameter will have to be adapted for other training sets, and was chosen here to maximise generalization accuracy (accuracy on tagging unseen text)." ></td>
	<td class="line x" title="132:379	19 Table 1: Case representation and information gain pattern for known words." ></td>
	<td class="line x" title="133:379	word case representation d d f a IG .06 .22 .82 .23 Pierre Vinken ;1 years old = ---np np = np np, cd np np, cd nns np, cd nns jj-np cd nns jj-np, np np cd nns JJ for similar approaches)." ></td>
	<td class="line x" title="134:379	4.3 Unknown Words If a word is not present in the lexicon, its ambiguous category cannot be retrieved." ></td>
	<td class="line x" title="135:379	In that case, a category can be guessed only on the basis of the form or the context of the word." ></td>
	<td class="line x" title="136:379	Again, we take advantage of the data fusion capabilities of a memory-based approach by combining these two sources of information in the case representation, and having the information gain feature relevance weighting technique figure out their relative relevance (see Schmid, 1994; Samuelsson, 1994 for similar solutions)." ></td>
	<td class="line x" title="137:379	In most taggers, some form of morphological analysis is performed on unknown words, in an attempt to relate the unknown word to a known combination of known morphemes, thereby allowing its association with one or more possible categories." ></td>
	<td class="line x" title="138:379	After determining this ambiguous category, the word is disambiguated using context knowledge, the same way as known words." ></td>
	<td class="line x" title="139:379	Morphological analysis presupposes the availability of highly language-specific resources such as a morpheme lexicon, spelling rules, morphological rules, and heuristics to prioritise possible analyses of a word according to their plausibility." ></td>
	<td class="line x" title="140:379	This is a serious knowledge engineering bottleneck when the goal is to develop a language and annotation-independent tagger generator." ></td>
	<td class="line x" title="141:379	In our memory-based approach, we provide morphological information (especially about suffixes) indirectly to the tagger by encoding the three last letters of the word as separate features in the case representation." ></td>
	<td class="line x" title="142:379	The first letter is encoded as well because it contains information about prefix and capitalization of the word." ></td>
	<td class="line x" title="143:379	Context information is added to the case representation in a similar way as with known words." ></td>
	<td class="line x" title="144:379	It turned out that in combination with the 'morphological' features, a context of one disambiguated tag of the word to the left of the unknown word and one ambiguous category of the word to the right, gives good results." ></td>
	<td class="line x" title="145:379	We will call this case representation pdassst: 3 three suffix letters (s), one prefix letter (p), one left disambiguated context words (d), and one ambiguous right context word (a)." ></td>
	<td class="line x" title="146:379	As the chance of an unknown word being a function word is small, and cases representing function words may interfere with correct classification of open-class words, only open-class words are used during construction of the unknown words case base." ></td>
	<td class="line x" title="147:379	Table 2 shows part of the case base for unknown words." ></td>
	<td class="line x" title="148:379	3These parameters (optimal context size and number of suffix features) were again optimised for generalization accuracy." ></td>
	<td class="line x" title="149:379	20 Table 2: Case representation and information gain pattern for unknown words." ></td>
	<td class="line x" title="150:379	word case representation p d a s s s t IG .21 .21 .14 .15 .20 .32 Pierre Vinken 61 years old P = np r r e np V np, k e n np 6 nns = 6 1 cd y cd jj-np a r s nns O nns, o 1 d jj 4.4 Control Figure 3 shows the architecture of the tagger-generator: a tagger is produced by extracting a lexicon and two case-bases from the tagged example corpus." ></td>
	<td class="line x" title="151:379	During tagging, the control is the following: words are looked up in the lexicon and separated into known and unknown words." ></td>
	<td class="line x" title="152:379	They are retrieved from the known words case base and the unknown words case base, respectively." ></td>
	<td class="line x" title="153:379	In both cases, context is used, in the case of unknown words, the first and three last letters of the word are used instead of the ambiguous tag for the focus word." ></td>
	<td class="line x" title="154:379	As far as disambiguated tags for left context words are used, these are of course not obtained by retrieval from the lexicon (which provides ambiguous categories), but by using the previous decisions of the tagger." ></td>
	<td class="line x" title="155:379	TAGGER GENERATION TAGGING Tagged Corpus LEXICON word -> a KNOWN WORDS CASE BASE ddfa > t UNKNOWN WORDS ' CASE BASE pdasss -> t TAGGER New Text b Tagged Text Figure 3: Architecture of the tagger-generator: flow of control." ></td>
	<td class="line x" title="156:379	4.5 IGTrees for Tagging As explained earlier, both case bases are implemented as IGTrees." ></td>
	<td class="line x" title="157:379	For the known words case base, paths in the tree represent variable size context widths." ></td>
	<td class="line x" title="158:379	The first feature (the expansion of the root node of the tree) is the focus word, then context features are added as further expansions of the tree until the context disambiguates the focus word completely." ></td>
	<td class="line x" title="159:379	Further expansion is halted at that point." ></td>
	<td class="line x" title="160:379	In some cases, short context sizes (corresponding to bigrams, e.g)." ></td>
	<td class="line x" title="161:379	are sufficient to disambiguate a focus word, in other cases, more context is needed." ></td>
	<td class="line x" title="162:379	IGTrees provide an elegant way of automatic determination of 21 optimal context size." ></td>
	<td class="line x" title="163:379	In the unknown words case base, the trie representation provides an automatic integration of information about the form and the context of a focus word not encountered before." ></td>
	<td class="line x" title="164:379	In general, the top levels of the tree represent the morphological information (the three suffix letter features and the prefix letter), while the deeper levels contribute contextual disambiguation." ></td>
	<td class="line x" title="165:379	5 Experiments In this section, we report first results on our memory-based tagging approach." ></td>
	<td class="line x" title="166:379	In a first set of experiments, we compared our IGTree implementation of memory-based learning to more traditional implementations of the approach." ></td>
	<td class="line x" title="167:379	In further experiments we studied the performance of our system on predicting the category of both known and unknown words." ></td>
	<td class="line x" title="168:379	Experimental Set-up The experimental methodology was taken from Machine Learning practice (e.g. Weiss & Kulikowski, 1991): independent training and test sets were selected from the original corpus, the system was trained on the training set, and the generalization accuracy (percentage of correct category assignments) was computed on the independent test set." ></td>
	<td class="line x" title="169:379	Storage and time requirements were computed as well." ></td>
	<td class="line x" title="170:379	Where possible, we used a 10-fold cross-validation approach." ></td>
	<td class="line x" title="171:379	In this experimental method, a data set is partitioned ten times into 90% training material, and 10% testing material." ></td>
	<td class="line x" title="172:379	Average accuracy provides a reliable estimate of the generalization accuracy." ></td>
	<td class="line x" title="173:379	5.1 Experiment 1: Comparison of Algorithms Our goal is to adhere to the concept of memory-based learning with full memory while at the same time keeping memory and processing speed within attractive bounds." ></td>
	<td class="line x" title="174:379	To this end, we applied the IGTree formalism to the task." ></td>
	<td class="line x" title="175:379	In order to prove that IGTree is a suitable candidate for practical memory-based tagging, we compared three memory-based learning algorithms: (i) IB1, a slight extension (to cope with symbolic values and ambiguous training items) of the well-known k-nn algorithm in statistical pattern recognition (see Aha et al. , 1991), (ii) IBI-IG, an extension of IB1 which uses feature relevance weighting (described in Section 2), and (iii) IGTree, a memoryand processing time saving heuristic implementation of IBi-IG (see Section 3)." ></td>
	<td class="line x" title="176:379	Table 3 lists the results in generalization accuracy, storage requirements and speed for the three algorithms using a ddfat pattern, a 100,000 word training set, and a 10,000 word test set." ></td>
	<td class="line x" title="177:379	In this experiment, accuracy was tested on known words only." ></td>
	<td class="line x" title="178:379	Table 3: Comparison of three memory-based learning techniques." ></td>
	<td class="line x" title="179:379	Algorithm Accuracy Time Memory (Kb) IB1 92.5 0:43:34 977 IBI-IG 96.0 0:49:45 977 i IGTree 96.0 0:00:29 35 The IGTree version turns out to be better or equally good in terms of generalization accuracy, but also is more than 100 times faster for tagging of new words 4, and compresses 4In training, i.e. building the case base, IB1 and IBi-IG (4 seconds) are faster than IGTree (26 seconds) because the latter has to build a tree instead of just storing the patterns." ></td>
	<td class="line x" title="180:379	22 the original case base to 4% of the size of the original case base." ></td>
	<td class="line x" title="181:379	This experiment shows that for this problem, we can use IGTree as a time and memory saving approximation of memory-based learning (IB-IG version), without loss in generalization accuracy." ></td>
	<td class="line x" title="182:379	The time and speed advantage of IGTree grows with larger training sets." ></td>
	<td class="line x" title="183:379	5.2 Experiment 2: Learning Curve A ten-fold cross-validation experiment on the first two million words of the WSJ corpus shows an average generalization performance of IGTree (on known words only) of 96.3%." ></td>
	<td class="line x" title="184:379	We did 10-fold cross-validation experiments for several sizes of datasets (in steps of 100,000 memory items), revealing the learning curve in Figure 4." ></td>
	<td class="line x" title="185:379	Training set size is on' the X-axis, generalization performance as measured in a 10-fold cross-validation experiment is on the Y-axis." ></td>
	<td class="line x" title="186:379	the 'error' range indicate averages plus and minus one standard deviation on each 10-fold cross-validation experiment." ></td>
	<td class="line x" title="187:379	5 0 o) t~ k~ 96.4 96.2 96 95.8 95.6 95.4 95.2 95 94.8 94.6 Part of Speech Tagging Learning Curve I I I I I I I I 500 1000 1500 2000 Training size (xlO00) Figure 4: Learning curve for tagging." ></td>
	<td class="line x" title="188:379	Already at small data set sizes, performance is relatively high." ></td>
	<td class="line x" title="189:379	With increasingly larger data sets, the performance becomes more stable (witness the error ranges)." ></td>
	<td class="line x" title="190:379	It should be noted that in this experiment, we assumed correctly disambiguated tags in the left context." ></td>
	<td class="line x" title="191:379	In practice, when using our tagger, this is of course not the case because the disambiguated tags in the left context of the current word to be tagged are the result of a previous decision of the tagger, which may be a mistake." ></td>
	<td class="line x" title="192:379	To test the influence of this effect we performed a third experiment." ></td>
	<td class="line x" title="193:379	5.3 Experiment 3: Overall Accuracy We performed the complete tagger generation process on a 2 million words training set (lexicon construction and known and unknown words case-base construction), and tested on 200,000 test words." ></td>
	<td class="line x" title="194:379	Performance on known words, unknown words, and total are given in Table 4." ></td>
	<td class="line x" title="195:379	In this experiment, numbers were not stored in the known words case base; they are looked up in the unknown words case base." ></td>
	<td class="line x" title="196:379	~We are not convinced that variation in the results of the experiments in a 10-fold-cv set-up is statistically meaningful (the 10 experiments are not independent), but follow common practice here." ></td>
	<td class="line x" title="197:379	23 Table 4: Accuracy of IGTree tagging on known and unknown words Accuracy Percentage Known 96.7 94.5 Unknown 90.6 5.5 Total 96.4 100.0 6 Related Research A case-based approach, similar to our memory-based approach, was also proposed by Cardie (1993a, 1994) for sentence analysis in limited domains (not only POS tagging but also semantic tagging and structural disambiguation)." ></td>
	<td class="line x" title="198:379	We will discuss only the reported POS tagging results here." ></td>
	<td class="line x" title="199:379	Using a fairly complex case representation based on output from the CIRCUS conceptual sentence analyzer (22 local context features describing syntactic and semantic information about a five-word window centered on the word to be tagged, including the words themselves, and 11 global context features providing information about the major constituents parsed already), and with a tag set of 18 tags (7 open-class, 11 closed class), she reports a 95% tagging accuracy." ></td>
	<td class="line x" title="200:379	A decision-tree learning approach to feature selection is used in this experiment (Cardie, 1993b, 1994) to discard irrelevant features." ></td>
	<td class="line x" title="201:379	Results are based on experiments with 120 randomly chosen sentences from the TIPSTER JV corpus (representing 2056 cases)." ></td>
	<td class="line x" title="202:379	Cardie (p.c)." ></td>
	<td class="line x" title="203:379	reports 89.1% correct tagging for unknown words." ></td>
	<td class="line x" title="204:379	Percentage unknown words was 20.6% of the test words, and overall tagging accuracy (known and unknown) 95%." ></td>
	<td class="line x" title="205:379	Notice that her algorithm gives no initial preference to training cases that match the test word during its initial case retrieval." ></td>
	<td class="line x" title="206:379	On the other hand, after retrieving the top k cases, the algorithm does prefer those cases that match the test word when making its final predictions." ></td>
	<td class="line x" title="207:379	So, it's understandable that the algorithm is doing better on words that it's seen during training as opposed to unknown words." ></td>
	<td class="line x" title="208:379	In our memory-based approach, feature weighting (rather than feature selection) for determining the relevance of features is integrated more smoothly with the similarity metric, and our results are based on experiments with a larger corpus (3 million cases)." ></td>
	<td class="line x" title="209:379	Our case representation is (at this point) simpler: only the (ambiguous) tags, not the words themselves or any other information are used." ></td>
	<td class="line x" title="210:379	The most important improvement is the use of IGTree to index and search the case base, solving the computational complexity problems a case-based approach would run into when using large case bases." ></td>
	<td class="line x" title="211:379	An approach based on k-nn methods (such as memory-based and case-based methods) is a statistical approach, but it uses a different kind of statistics than Markov model-based approaches." ></td>
	<td class="line x" title="212:379	K-nn is a non-parametric technique; it assumes no fixed type of distribution of the data." ></td>
	<td class="line x" title="213:379	The most important advantages compared to current stochastic approaches are that (i) few training items (a small tagged corpus) are needed for relatively good performance, (ii) the approach is incremental: adding new cases does not require any recomputation of probabilities, and (iii) it provides explanation capabilities, and (iv) it requires no additional smoothing techniques to avoid zero-probabilities; the IGTree takes care of that." ></td>
	<td class="line x" title="214:379	Compared to hand-crafted rule-based approaches, our approach provides a solution to the knowledge-acquisition and reusability bottlenecks, and to robustness and coverage problems (similar advantages motivated Markov model-based statistical approaches)." ></td>
	<td class="line x" title="215:379	Compared to learning rule-based approaches such as the one by Brill (1992), a k-nn approach provides a uniform approach for all disambiguation tasks, more flexibility in the engineering of case representations, and a more elegant approach to handling of unknown words (see e.g. Cardie 1994)." ></td>
	<td class="line x" title="216:379	24 7 Conclusion We have shown that a memory-based approach to large-scale tagging is feasible both in terms of accuracy (comparable to other statistical approaches), and also in terms of computational efficiency (time and space requirements) when using IGTree to compress and index the case base." ></td>
	<td class="line x" title="217:379	The approach combines some of the best features of learned rule-based and statistical systems (small training corpora needed, incremental learning, understandable and explainable behavior of the system)." ></td>
	<td class="line x" title="218:379	More specifically, memory-based tagging with IGTrees has the following advantages." ></td>
	<td class="line x" title="219:379	 Accurate generalization from small tagged corpora." ></td>
	<td class="line x" title="220:379	Already at small corpus size (300-400 K tagged words), performance is good." ></td>
	<td class="line x" title="221:379	These corpus sizes can be easily handled by our system." ></td>
	<td class="line x" title="222:379	 Incremental learning." ></td>
	<td class="line x" title="223:379	New 'cases' (e.g. interactively corrected output of the tagger) can be incrementally added to the case bases, continually improving the performance of the overall system." ></td>
	<td class="line x" title="224:379	 Explanation capabilities." ></td>
	<td class="line x" title="225:379	To explain the classification behavior of the system, a path in the IGTree (with associated defaults) can be provided as an explanation, as well as nearest neighbors from which the decision was extrapolated." ></td>
	<td class="line x" title="226:379	 Flexible integration of information sources." ></td>
	<td class="line x" title="227:379	The feature weighting method takes care of the optimal fusing of different sources of information (e.g. word form and context), automatically." ></td>
	<td class="line x" title="228:379	 Automatic selection of optimal context." ></td>
	<td class="line x" title="229:379	The IGTree mechanism (when applied to the known words case base) automatically decides on the optimal context size for disambiguation of focus words." ></td>
	<td class="line x" title="230:379	 Non-parametric estimation." ></td>
	<td class="line x" title="231:379	The IGTree formalism provides automatic, nonparametric estimation of classifications for low-frequency contexts (it is similar in this respect to backed-off training), but avoids non-optimal estimation due to false intuitions or non-convergence of the gradient-descent procedure used in some versions of backedoff training." ></td>
	<td class="line x" title="232:379	 Reasonably good results on unknown words without morphological analysis." ></td>
	<td class="line x" title="233:379	On the WSJ corpus, unknown words can be predicted (using context and word form information) for more than 90%." ></td>
	<td class="line x" title="234:379	 Fast learning and tagging." ></td>
	<td class="line x" title="235:379	Due to the favorable complexity properties of IGTrees (lookup time in IGTrees is independent on number of cases), both tagger generation and tagging are extremely fast." ></td>
	<td class="line x" title="236:379	Tagging speed in our current implementation is about 1000 words per second." ></td>
	<td class="line x" title="237:379	We have barely begun to optimise the approach: a more intelligent similarity metric would also take into account the differences in similarity between different values of the same feature." ></td>
	<td class="line x" title="238:379	E.g. the similarity between the tags rb-in-nn and rb-in should be bigger than the similarity between rb-in and vb-nn." ></td>
	<td class="line x" title="239:379	Apart from linguistic engineering refinements of the similarity metric, we are currently experimenting with statistical measures to compute such more fine-grained similarities (e.g. Stanfill & Waltz, 1986, Cost & Salzberg, 1994)." ></td>
	<td class="line x" title="240:379	Acknowledgements Research of the first author was done while he was a visiting scholar at NIAS (Netherlands Institute for Advanced Studies) in Wassenaar." ></td>
	<td class="line x" title="241:379	Thanks to Antal van den Bosch, Ton Weijters, and Gert Durieux for discussions about tagging, IGTree, and machine learning of natural language." ></td>
	<td class="line x" title="242:379	25 References Aha, D. W., Kibler, D. , & Albert, M." ></td>
	<td class="line x" title="243:379	(1991)." ></td>
	<td class="line x" title="244:379	'Instance-based learning algorithms'." ></td>
	<td class="line x" title="245:379	Machine Learning, 7, 37-66." ></td>
	<td class="line x" title="246:379	Brill, E." ></td>
	<td class="line x" title="247:379	(1992) 'A simple rule-based part-of-speech tagger'." ></td>
	<td class="line x" title="248:379	Proceedings Third ACL Applied, Trento, Italy, 152-155." ></td>
	<td class="line x" title="249:379	Cardie, C." ></td>
	<td class="line x" title="250:379	(1993a)." ></td>
	<td class="line x" title="251:379	'A case-based approach to knowledge acquisition for domain-specific sentence analysis'." ></td>
	<td class="line x" title="252:379	In AAAL93, 798-803." ></td>
	<td class="line x" title="253:379	Cardie, C." ></td>
	<td class="line x" title="254:379	(1993b)." ></td>
	<td class="line x" title="255:379	'Using Decision Trees to Improve Case-Based Learning'." ></td>
	<td class="line x" title="256:379	In Proceedings of the Tenth International Conference on Machine Learning, 25-32." ></td>
	<td class="line x" title="257:379	Cardie, C." ></td>
	<td class="line x" title="258:379	(1994)." ></td>
	<td class="line x" title="259:379	'Domain-Specific Knowledge Acquisition for Conceptual Sentence Analysis'." ></td>
	<td class="line x" title="260:379	Ph.D. Thesis, University of Massachusetts, Amherst, MA." ></td>
	<td class="line x" title="261:379	Chandler, S." ></td>
	<td class="line x" title="262:379	(1992)." ></td>
	<td class="line x" title="263:379	'Are rules and modules really necessary for explaining language'?" ></td>
	<td class="line x" title="264:379	Journal of Psycholinguistic research, 22(6): 593-606." ></td>
	<td class="line x" title="265:379	Church, K." ></td>
	<td class="line x" title="266:379	(1988)." ></td>
	<td class="line x" title="267:379	'A stochastic parts program and noun phrase parser for unrestricted text'." ></td>
	<td class="line x" title="268:379	Proceedings Second A CL Applied NLP, Austin, Texas, 136-143." ></td>
	<td class="line x" title="269:379	Cost, S. and Salzberg, S." ></td>
	<td class="line x" title="270:379	(1993)." ></td>
	<td class="line x" title="271:379	'A weighted nearest neighbour algorithm for learning with symbolic features'." ></td>
	<td class="line x" title="272:379	Machine Learning, 10, 57-78." ></td>
	<td class="line x" title="273:379	Cutting, D. , Kupiec, J. , Pederson, J. , Sibun, P." ></td>
	<td class="line x" title="274:379	(1992)." ></td>
	<td class="line x" title="275:379	A practical part of speech tagger." ></td>
	<td class="line x" title="276:379	Proceedings Third A CL Applied NLP, Trento, Italy, 133-140." ></td>
	<td class="line x" title="277:379	Daelemans, W." ></td>
	<td class="line x" title="278:379	(1995)." ></td>
	<td class="line x" title="279:379	'Memory-based lexical acquisition and processing'." ></td>
	<td class="line x" title="280:379	In Steffens, P. , editor, Machine Translation and the Lexicon, Lecture Notes in Artificial Intelligence 898." ></td>
	<td class="line x" title="281:379	Berlin: Springer, 85-98." ></td>
	<td class="line x" title="282:379	Daelemans, W. , Van den Bosch, A." ></td>
	<td class="line x" title="283:379	(1992)." ></td>
	<td class="line x" title="284:379	'Generalisation performance of backpropagation learning on a syllabification task'." ></td>
	<td class="line x" title="285:379	In M. Drossaers & A. Nijholt (Eds.), TWLT3: Connectionism and Natural Language Processing." ></td>
	<td class="line x" title="286:379	Enschede: Twente University, 27-38." ></td>
	<td class="line x" title="287:379	Daelemans, W. , Van den Bosch, A. , Weijters, T." ></td>
	<td class="line x" title="288:379	(1996)." ></td>
	<td class="line x" title="289:379	'IGTree: Using Trees for Compression and Classification in Lazy Learning Algorithms'." ></td>
	<td class="line x" title="290:379	In Aha, D." ></td>
	<td class="line x" title="291:379	(ed.)." ></td>
	<td class="line x" title="292:379	AI Review Special Issue on Lazy Learning, forthcoming." ></td>
	<td class="line x" title="293:379	DeRose, S." ></td>
	<td class="line x" title="294:379	(1988)." ></td>
	<td class="line x" title="295:379	'Grammatical category disambiguation by statistical optimization." ></td>
	<td class="line x" title="296:379	' Computational Linguistics 14, 31-39." ></td>
	<td class="line x" title="297:379	Derwing, B. L. and Skousen, R." ></td>
	<td class="line x" title="298:379	(1989)." ></td>
	<td class="line x" title="299:379	'Real Time Morphology: Symbolic Rules or Analogical Networks'." ></td>
	<td class="line x" title="300:379	Berkeley Linguistic Society 15: 48-62." ></td>
	<td class="line x" title="301:379	Federici S. and V. Pirelli." ></td>
	<td class="line x" title="302:379	(1996)." ></td>
	<td class="line x" title="303:379	'Analogy, Computation and Linguistic Theory'." ></td>
	<td class="line x" title="304:379	In Jones, D." ></td>
	<td class="line x" title="305:379	(ed)." ></td>
	<td class="line x" title="306:379	New Methods in Language Processing." ></td>
	<td class="line x" title="307:379	London: UCL Press, forthcoming." ></td>
	<td class="line x" title="308:379	Garside, R. , Leech, G. and Sampson, G." ></td>
	<td class="line x" title="309:379	(1987)." ></td>
	<td class="line x" title="310:379	The computational analysis of English: A corpus-based approach, London: Longman, 1987." ></td>
	<td class="line x" title="311:379	Greene, B.B. and l~ubin, G.M." ></td>
	<td class="line x" title="312:379	(1971)." ></td>
	<td class="line x" title="313:379	Automatic Grammatical Tagging of English." ></td>
	<td class="line x" title="314:379	Providence RI: Department of Linguistics, Brown University." ></td>
	<td class="line x" title="315:379	Hindle, Donald." ></td>
	<td class="line x" title="316:379	(1989)." ></td>
	<td class="line x" title="317:379	'Acquiring disambiguation rules from text'." ></td>
	<td class="line x" title="318:379	In Proceedings, 27th Annual Meeting of the Association for Computational Linguistics, Vancouver, BC." ></td>
	<td class="line x" title="319:379	Hunt, E. , J. Matin, P. Stone." ></td>
	<td class="line x" title="320:379	(1966)." ></td>
	<td class="line x" title="321:379	Experiments in Induction." ></td>
	<td class="line x" title="322:379	New York: Academic Press." ></td>
	<td class="line x" title="323:379	Jones, D. Analogical Natural Language Processing." ></td>
	<td class="line x" title="324:379	London: UCL Press, 1996." ></td>
	<td class="line x" title="325:379	Klein S. and Simmons, R." ></td>
	<td class="line x" title="326:379	(1963)." ></td>
	<td class="line x" title="327:379	'A grammatical approach to grammatical coding of English words'." ></td>
	<td class="line x" title="328:379	JACM 10, 334-347." ></td>
	<td class="line x" title="329:379	Kolodner, J." ></td>
	<td class="line x" title="330:379	(1993)." ></td>
	<td class="line x" title="331:379	Case-Based Reasoning." ></td>
	<td class="line x" title="332:379	San Mateo: Morgan Kaufmann." ></td>
	<td class="line x" title="333:379	Langley, P. and Sage, S." ></td>
	<td class="line x" title="334:379	(1994)." ></td>
	<td class="line x" title="335:379	'Oblivious decision trees and abstract cases'." ></td>
	<td class="line x" title="336:379	In D. W. Aha (Ed.), Case-Based Reasoning: Papers from the 1994 Workshop (Technical Report WS-94-01)." ></td>
	<td class="line x" title="337:379	Menlo Park, CA: AAAI Press." ></td>
	<td class="line x" title="338:379	Merialdo, B." ></td>
	<td class="line x" title="339:379	(1994)." ></td>
	<td class="line x" title="340:379	'Tagging English Text with a Probabilistic Model'." ></td>
	<td class="line x" title="341:379	Computational Linguistics 20 (2), 155-172." ></td>
	<td class="line x" title="342:379	Pereira, F. , Y. Singer, N. Tishby." ></td>
	<td class="line x" title="343:379	(1995)." ></td>
	<td class="line x" title="344:379	'Beyond Word N-grams'." ></td>
	<td class="line x" title="345:379	Proceedings Third Workshop on Very Large Corpora, MIT, Cambridge Mass. , 95-106." ></td>
	<td class="line x" title="346:379	Quinlan, J." ></td>
	<td class="line x" title="347:379	(1993)." ></td>
	<td class="line x" title="348:379	C4.5: Programs for Machine Learning." ></td>
	<td class="line x" title="349:379	San Mateo, CA: Morgan Kaufmann." ></td>
	<td class="line x" title="350:379	Salzberg, S." ></td>
	<td class="line x" title="351:379	(1990) 'A nearest hyperrectangle learning method'." ></td>
	<td class="line x" title="352:379	Machine Learning 6, 251-276." ></td>
	<td class="line x" title="353:379	Samuelsson, C." ></td>
	<td class="line x" title="354:379	(1994) 'Morphological Tagging Based Entirely on Bayesian Inference'." ></td>
	<td class="line x" title="355:379	In Proceedings of the 9th Nordic Conference on Computational Linguistics, Stockholm University, Sweden, 1994." ></td>
	<td class="line x" title="356:379	Scha, R." ></td>
	<td class="line x" title="357:379	(1992) 'Virtuele Grammatica's en Creatieve Algoritmen'." ></td>
	<td class="line x" title="358:379	Gramma/TTT 1 (1), 57-77." ></td>
	<td class="line x" title="359:379	Schmid, H." ></td>
	<td class="line x" title="360:379	(1994) 'Part-of-speech tagging with neural networks'." ></td>
	<td class="line x" title="361:379	In Proceedings of COLING, Kyoto, Japan." ></td>
	<td class="line x" title="362:379	Schfitze, H. , and Y. Singer." ></td>
	<td class="line x" title="363:379	(1994) 'Part-of-speech Tagging Using a Variable Context Markov Model' Proceedings of ACL 1994, Las Cruces, New Mexico." ></td>
	<td class="line x" title="364:379	Skousen, R." ></td>
	<td class="line x" title="365:379	(1989)." ></td>
	<td class="line x" title="366:379	Analogical Modeling of Language." ></td>
	<td class="line x" title="367:379	Dordrecht: Kluwer." ></td>
	<td class="line x" title="368:379	Sejnowski, T. J., Rosenberg, C. S." ></td>
	<td class="line x" title="369:379	(1987)." ></td>
	<td class="line x" title="370:379	Parallel networks that learn to pronounce English text." ></td>
	<td class="line x" title="371:379	Complex Systems, 1, 145-168." ></td>
	<td class="line x" title="372:379	Stanfill, C. and Waltz, D." ></td>
	<td class="line x" title="373:379	(1986)." ></td>
	<td class="line x" title="374:379	'Toward memory-based reasoning'." ></td>
	<td class="line x" title="375:379	Communications of the ACM, 29, 1212-1228." ></td>
	<td class="line x" title="376:379	Weiss, S. and Kulikowski, C." ></td>
	<td class="line x" title="377:379	(1991)." ></td>
	<td class="line x" title="378:379	Computer systems that learn." ></td>
	<td class="line x" title="379:379	San-Mateo: Morgan Kaufmann ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W96-0113
A Re-Estimation Method For Stochastic Language Modeling From Ambiguous Observations
Yamamoto, Mikio;"></td>
	<td class="line x" title="1:265	A Re-estimation Method for Stochastic Language Modeling from Ambiguous Observations Mikio Yamamoto Institute of Information Sciences and Electronics University of Tsukuba 1-1-1 Tennodai, Tsukuba, Ibaraki 305, Japan myama@is, t sukuba, ac." ></td>
	<td class="line x" title="2:265	j p Abstract This paper describes a reestimation method for stochastic language models such as the N-gram model and the Hidden Maxkov Model(HMM) from ambiguous observations." ></td>
	<td class="line x" title="3:265	It is applied to model estimation for a tagger from a~ untagged corpus." ></td>
	<td class="line x" title="4:265	We make extensions to a previous algorithm that reestimates the N-gram model from an untagged segmented language (e.g. , English) text as training data." ></td>
	<td class="line x" title="5:265	The new method can estimate not only the N-gram model, but also the HMM from untagged, unsegmented language (e.g. , Japanese) text." ></td>
	<td class="line x" title="6:265	Credit factors for training data to improve the reliability of the estimated models axe also introduced." ></td>
	<td class="line x" title="7:265	In experiments, the extended algorithm could estimate the HMM as well as the N-gram model from an untagged, unsegmented Japanese corpus and the credit factor was effective in improving model accuracy." ></td>
	<td class="line x" title="8:265	The use of credit factors is a useful approach to estimating a reliable stochastic language model from untagged corpora which axe noisy by nature." ></td>
	<td class="line x" title="9:265	1 Introduction Stochastic language models are useful for many language processing applications such as speech recognition, natural language processing and so on." ></td>
	<td class="line x" title="10:265	However, in order to build an accurate stochastic language model, large amounts of tagged text are needed and a tagged corpus may not always match a target application because of, for example, differences between the tag systems." ></td>
	<td class="line x" title="11:265	If the language model can be estimated from untagged corpora and the dictionary of a target application, then the above two problems would be resolved because large amounts of untagged corpora could be easily used and untagged corpora are neutral toward any applications." ></td>
	<td class="line oc" title="12:265	Kupiec (1992) has proposed an estimation method for the N-gram language model using the Baum-Welch reestimation algorithm (Rabiner et al. , 1994) from an untagged corpus and Cutting et al.(1992) have applied this method to an English tagging system." ></td>
	<td class="line x" title="14:265	Takeuchi and Matsumoto (1995) also have developed an extended method for unsegmented languages (e.g. , Japanese) and applied it to their Japanese tagger." ></td>
	<td class="line x" title="15:265	However, Merialdo (1994) and Elworthy (1994) have criticized methods of estimation from an untagged corpus based on the maximum likelihood principle." ></td>
	<td class="line x" title="16:265	They pointed out limitation of such methods revealed by their experiments and said that the optimization of likelihood didn't necessarily improve tagging accuracy." ></td>
	<td class="line x" title="17:265	In other words, the training data extracted from an untagged corpus using only a dictionary are, by nature, too noisy to build a reliable model." ></td>
	<td class="line x" title="18:265	I would like to know whether or not the noise problem occurs in other language models such as the HMM." ></td>
	<td class="line x" title="19:265	Zhou and Nakagawa (1994) have shown, in the experiments of word prediction 155 from the previous word sequence, that the HMM is more powerful than the bigram model and is nearly equivalent to the trigram model, though the number of parameters of the HMM is less than that in the N-gram model." ></td>
	<td class="line x" title="20:265	In general, models with fewer parameters are more robust." ></td>
	<td class="line x" title="21:265	Here, I investigate a method that can estimate HMM parameters from an untagged corpus and also a general technique for supressing noise in untagged training data." ></td>
	<td class="line x" title="22:265	The goals of this paper are as follows." ></td>
	<td class="line x" title="23:265	 Extension of Baum-Welch algorithm: I formulate an algorithm that can be applied to untagged, unsegmented language corpora and estimate not only the N-gram model, but the HMM." ></td>
	<td class="line x" title="24:265	Also, a scaling procedure is defined in the algorithm." ></td>
	<td class="line x" title="25:265	 Credit factor: In order to overcome the noise of untagged corpora, I introduce credit factors that are assigned to training data." ></td>
	<td class="line x" title="26:265	The estimation algorithm can approximately maximize the modified likelihood that is weighted by the credit factors." ></td>
	<td class="line x" title="27:265	The problem of stochastic tagging is formulated in the next section(2) and the extended reestimation method in section 3." ></td>
	<td class="line x" title="28:265	A way of determining the credit factor based on a rulebased tagger is described in section 4." ></td>
	<td class="line x" title="29:265	Experiments which evaluate the proposed method are reported in section 5." ></td>
	<td class="line x" title="30:265	2 Stochastic Tagging Formulation In general, the stochastic tagging problem can be formulated as a search problem in the stochastic space of sequences of tags and words." ></td>
	<td class="line x" title="31:265	In this formulation, the tagger searches for the best sequence that maximizes the probability (Nagata, 1994): (1~ r, T) = arg maxp(W, TIS ) = arg maxp(W, T) (1) W,T W,T where W is a word sequence (wl,w2, ,Wn), T is a tag sequence (tl,t2,,tn) and S is an input sentence." ></td>
	<td class="line x" title="32:265	Since Japanese sentences have no delimiters (e.g. , spaces) between words, a morphological analyzer (tagger) must decide word segmentation in addition to part-of-speech assignment." ></td>
	<td class="line x" title="33:265	The number of segmentation ambiguities of Japanese sentences is large and these ambiguities complicate the work of a Japanese tagger." ></td>
	<td class="line x" title="34:265	Although all possible p(W, T)s on combinations of W and T cannot be estimated, there are some particularly useful approximations such as the N-gram model and the HMM." ></td>
	<td class="line x" title="35:265	The following formulae are straightforward formulations whose observed variables are pairs of words and tags: n p(W, T) ~ ~I p(wi, tilwi-N+l,  , wi-1, ti-N+l,  , ti--1) (2) i=1 n--1 p(W,T) '~' ~ I~ ax(i),x(i+l)bz(i+l) (wi+l'ti+l) (3) x i=0 Formula 2 is the N-gram model and formula 3 is the HMM." ></td>
	<td class="line x" title="36:265	When N of formula 2 is two, the model is called the bigram, when N is three, it is the trigrarm Symbol x of formula 3 denotes a possible path of states of the HMM and x(i) denotes a state of the HMM that is visited at the i-th transition in the path x. ax(i),x(i+l ) is the transition probability from x(i) to x(i + i)." ></td>
	<td class="line x" title="37:265	In particular, ax(0),x(1 ) represents the initial state probability (Trx(1)) of x(1)." ></td>
	<td class="line x" title="38:265	b~(i)(w, t) is an output probability of a pair of word w and tag t on the state x(i)." ></td>
	<td class="line x" title="39:265	A state of the HMM represents an abstract class of a part of the input symbol sequence." ></td>
	<td class="line x" title="40:265	That is, we can regard the HMM as a mixed model of unigram, bigram, trigram, and so on." ></td>
	<td class="line x" title="41:265	156 We can also decrease the number of model parameters by separating the tag model from formulae 2 and 3." ></td>
	<td class="line x" title="42:265	In the models, the N-gram and the HMM are used to model tag sequence and p(wlt ) is used for another part of the model." ></td>
	<td class="line x" title="43:265	n v(w, T) II v(ti Iti-Nti-1)v(wilt ) (4) i=1 n--1 p(W,T) ~ ~ ~I = ax(i),x(i+l)b~(i+l)(ti+l)p(wi+l\]ti+l ) (5) x i=0 The PAIR-HMM, TAG-bigram model, and TAG-HMM based on formulae 3, 4 (where N = 2) and 5, respectively, will be investigated in section 5." ></td>
	<td class="line x" title="44:265	In the next section, I describe an extension to the forward-backward algorithm for determining HMM parameters fi'om ambiguous observations." ></td>
	<td class="line x" title="45:265	3 Re-estimation Method from Ambiguous Observations 3.1 Ambiguous Observation Structure Here, we define an ambiguous observation as a lattice structure with a credit factor for each branch." ></td>
	<td class="line x" title="46:265	In unsegmented languages that have no delimiter between words, such as Japanese, candidates for alignment of tag and word have different segmentation." ></td>
	<td class="line x" title="47:265	That is, they must be represented by a lattice." ></td>
	<td class="line x" title="48:265	We can create a lattice structure from untagged Japanese sentences and a Japanese dictionary." ></td>
	<td class="line x" title="49:265	The following is the definition of the lattice of candidates representing ambiguous word and tag sequences called the morpheme network." ></td>
	<td class="line x" title="50:265	All morphemes on the morpheme network are numbered." ></td>
	<td class="line x" title="51:265	w, or word(s): The spelling of the s-th morpheme." ></td>
	<td class="line x" title="52:265	ts or tag(s): The tag of the s-th morpheme." ></td>
	<td class="line x" title="53:265	suc(s): The set of morpheme numbers that the s-th morpheme connects to." ></td>
	<td class="line x" title="54:265	pre(s): The set of morpheme numbers that connect to the s-th morpheme." ></td>
	<td class="line x" title="55:265	credit(r, s): The credit factor of the connection between the r-th and the s-th morphemes." ></td>
	<td class="line x" title="56:265	For example, a morpheme network can be derieved from the input sentence '~ L ~3 v~' which means 'not to sail' (Fig." ></td>
	<td class="line x" title="57:265	1)." ></td>
	<td class="line x" title="58:265	The real and dotted lines in Figure 1 represent the correct and incorrect paths of morphemes, respectively." ></td>
	<td class="line x" title="59:265	Of course, any algorithm for estimation from untagged corpora cannot determine whether the connections are correct or not." ></td>
	<td class="line x" title="60:265	The connections of dotted lines constitute noise for the estimation algorithm." ></td>
	<td class="line x" title="61:265	The numbers on the lines show the credit factor of each connection that is assigned by the method described in section 4." ></td>
	<td class="line x" title="62:265	The numbers at the right of colons are morpheme numbers." ></td>
	<td class="line x" title="63:265	In Figure 1, word(3) is ' ~ b ', tag(3) is 'verb', pre(3) is the set {1}, sue(3) is the set {6, 7} and the credit factor credit(l, 3) is 0.8." ></td>
	<td class="line x" title="64:265	3.2 Re-estimation Algorithm Given a morpheme network, we can formulate the reestimation algorithm for the HMM parameters." ></td>
	<td class="line x" title="65:265	The original forward-backward algorithm calculates the probability of the partial observation sequence given the state of the HMM at the time (position of word in the input sentence)." ></td>
	<td class="line x" title="66:265	The original algorithm does this by a time synchronous procedure operating on unambiguous observation sequence." ></td>
	<td class="line x" title="67:265	The extended algorithm calculates the probability of the 157 INPUT SENTENCE: (not to sail) ., ~ ~r~(noun): 1  .0.~8." ></td>
	<td class="line x" title="68:265	* ~ L,(verb):3,:--0-'-2----:'0 P~(adjective):6,, .0.:-I'' (ship) (putout) '0.5,-'*(not) ',0.9 0'' :< '' 0.~ 0.3/  '., ',, u.= ~ ~J~(noun):2 .0.7  \[.,(verb):4 -'''' 0.7 ~ ~'=~(post-fix):7 0.9 ~ '.'(symbol):8 0~90 (saling) '', (do) (not),.,-'' o:'-." ></td>
	<td class="line x" title="71:265	''' L ~'0 ~(noun):5  '' (bamboo sword) Figure 1: An example of the morpheme network." ></td>
	<td class="line x" title="72:265	partial ambiguous sequence given the state of the HMM at the node (morpheme candidate) in the morpheme network by a node synchronous procedure." ></td>
	<td class="line x" title="73:265	The algorithm formulation is as follows: initial: c~u(j) = lrjbj(wu, tu)credit(#, u) where fly(i) = 1 where recursion: N c~(j) = ~\] E a~(i)aljbj(wr,t~)credit(s,r) sEpre(r) i=1 N fls(i) = ~ ~ aijbj(wr, tr)flr(j)credit(s,r) u e on(l) v E on(B) rEsuc(s) j=l where on(l) is the set of numbers of the left most morphemes in the morpheme network and on(B) is the set of numbers of the right most morphemes." ></td>
	<td class="line x" title="74:265	The '#' in credit(#, u) means the beginning-of-text indicator." ></td>
	<td class="line x" title="75:265	The trellis, that is often used to explain the originM forward-backward algorithm, is extended into a network trellis." ></td>
	<td class="line x" title="76:265	Figure 2 is an example of the network trellis that is generated from the morpheme network example given above (Fig." ></td>
	<td class="line x" title="77:265	1)." ></td>
	<td class="line x" title="78:265	In this example, c~7(1) means a forward probability of the 7th morpheme at the 1st state of the HMM." ></td>
	<td class="line x" title="79:265	Using the extended forward-backward probabilities we can formulate the reestimation algorithm from ambiguous observations: K 1 k~__ 1 ~ ~ czrk(i)aijbj(ts)fl~(j)credit(r,s) ---rEpre(s) aiJ ---K k=l K k=l t'i(w, t) = wora(sl=~,t,g(~l=t (7/ k=l (6) 158 noun: 1 verb:3 adjective:6 o1:8 n '<-~, . :\[' (X7(1) noun:5 '~7(3) Figure 2: An example of the network trellis K 1 O~(/)~sk (i) E~ E k=l 8Eon(1) ~ = g N (8) k=l sEon(1)j=l where k represents the k-th input sentence and Pk is sum of the probabilities of possible sequences in the k-th morpheme network weighted by the credit factors." ></td>
	<td class="line x" title="80:265	3.3 Scaling In the calculation of forward-backward probabilities, under-flow sometimes occurs if the dictionaxy for making the morpheme network is large and/or the length of the input sentence is long, because the forward-backward algorithm multiplies many small transition and output probabilities together." ></td>
	<td class="line x" title="81:265	This problem is native to speech modeling, but in general, the modeling of text is free from this problem." ></td>
	<td class="line x" title="82:265	However, since Japanese sentences tend to be relatively long and the recent Japanese dictionary for research is large, under-flow is sometimes a problem." ></td>
	<td class="line x" title="83:265	For example, the EDIt Japanese corpus (EDR, 1994) includes sentences that consist of more than fifty words at a frequency of one percent." ></td>
	<td class="line x" title="84:265	In fact, we experienced the underflow problem in preliminary experiments with the EDR corpus." ></td>
	<td class="line x" title="85:265	Application of the scaling technique of the original backward-forward algorithm (Rabiner et al. , 1994) to our reestimation method would solve the under-flow problem." ></td>
	<td class="line x" title="86:265	The original technique is based on synchronous calculation with positions of words in the input sentence in left-to-right fashion." ></td>
	<td class="line x" title="87:265	However, since word boundaries in the morpheme network may or may not cross on the input character sequence, we cannot directly apply this method to the extended algorithm." ></td>
	<td class="line x" title="88:265	Let us introduce synchronous points on a~ input characters sequence to facilitate synchronization of the calculation of forward-backward probabilities." ></td>
	<td class="line x" title="89:265	All possible paths of a morpheme 159 Syncronous points ! !" ></td>
	<td class="line x" title="90:265	I ! I I o ! o ! l l i i i i ~t~ :2  L:4 ~:7 j~ ! !" ></td>
	<td class="line x" title="91:265	I ! !" ></td>
	<td class="line x" title="92:265	| i 2 34 '.':8 Figure 3: An example of syncronous points network have one morpheme on each synchronous point." ></td>
	<td class="line x" title="93:265	The synchronous points are defined as positions of the head character of all morphemes in a morpheme network and are numbered from left to right." ></td>
	<td class="line x" title="94:265	The synchronous point number of the left most word is defined as 1." ></td>
	<td class="line x" title="95:265	A morpheme is associated with the synchronous points which are located in the flow of characters of the morpheme." ></td>
	<td class="line x" title="96:265	The symbols and on(q) function are defined as follows: B: The maximum number of synchronous points in a morpheme network." ></td>
	<td class="line x" title="97:265	on(q): The set of morpheme numbers that are associated with synchronous point q. L,: The left most synchronous point that is associated with the s-th morpheme." ></td>
	<td class="line x" title="98:265	R,: The right most synchronous point that is associated with the s-th morpheme." ></td>
	<td class="line x" title="99:265	Figure 3 is an example of the syncronous points for the morpheme network example given above (Fig." ></td>
	<td class="line x" title="100:265	1)." ></td>
	<td class="line x" title="101:265	The values of the symbols and function defined above are as follows in this example; B = 5, on(2) = {2, 3}, L5 = 3, R5 = 4 and so on." ></td>
	<td class="line x" title="102:265	The scaled forward probabilities are defined with the above definitions." ></td>
	<td class="line x" title="103:265	The notation ~st(i) is used to denote the unscaled forward probabilities of the s-th morpheme on the syncronous point l, &sl(i) to denote the scaled forward probabilities, and &,l(i) to denote the local version of c~ before scaling, cl is the scaling factor of synchronous point I. initial: &sl(i) = ~sl(i) = ~ribi(ws,t,)credit(~,s) where s E on(l) N ^ C 1 : 1/ E E &sl(/) sEon(1) i=1 ^ &sl(i) = Cl&sl(i) where s E on(l) 160 cost 0 1-10 11-20 21-50 51-100 101-200 201-500 501-1000 precision 0.84 0.16 0.13 0.069 0.074 0.0083 0.0017 0 Table 1: The precision on each cost of Juman recursion: &s,/-l(i) if L, 7~l N &,l_l(i)aijbtj(w,,t )credit(r,s) if L, = l rEpre(s) i=1 N ^ C l -~ 1 E E &sl(i) sEon(l) i=1 &st(i) = Cl&t(i) The scaled forward probabilities can be calculated synchronizing with the synchronous points from left to right." ></td>
	<td class="line x" title="104:265	The scaled backward probabilities are defined in the same way using the scaling factors obtained in the calculation of the forward probabilities." ></td>
	<td class="line x" title="105:265	The scaled forward-backward probabilities have the following property: 1 &s(i)aijbj(wr, tr)flr(j) = --as(i)aijbj(wr, tr)fl~(j) (9) Pk where &8 = &~R~ and fls = fl~ns." ></td>
	<td class="line x" title="106:265	Using this property, the reestimation formulae can be replaced with the scaled versions." ></td>
	<td class="line x" title="107:265	The replaced formulae are free of the under-flow problem and their use also obviates the need to calculate the weighted sum of path probabilities of the k-th ambiguous observation, Pk." ></td>
	<td class="line x" title="108:265	4 Credit Factor In the estimation of a Japanese language model from an untagged corpus, the segmentation ambiguity of Japanese sentences severely degrades the model reliability." ></td>
	<td class="line x" title="109:265	I will show that model estimations excluding the credit factors cannot overcome the noise problem in section 5." ></td>
	<td class="line x" title="110:265	Credit factors play a very important role by supressing noise in the training data." ></td>
	<td class="line x" title="111:265	However, a way of calculating the optimal value of credit is not yet available, so a preliminary method described in this section was used for the experiments." ></td>
	<td class="line x" title="112:265	The 'costs' of candidates outputted by a rule-based tagger were used as the source of information related to the credit." ></td>
	<td class="line x" title="113:265	Juman (Matsumoto et al. , 1994) was used in our experiments to generate the morpheme network." ></td>
	<td class="line x" title="114:265	Juman is a rule-based Japanese tagging system which uses hand-coding cost values that represent the implausibility of morpheme connections, and wordand tag-occurences." ></td>
	<td class="line x" title="115:265	Given a cost-width, Juman outputs the candidates of morpheme sequences pruned by this cost-width." ></td>
	<td class="line x" title="116:265	A larger cost-width would result in a larger number of output candidates." ></td>
	<td class="line x" title="117:265	We evaluated the precision of a set of morpheme candidates that have a certain cost." ></td>
	<td class="line x" title="118:265	The precision value was used as the credit factor of each branch in the morpheme network to be outputted by Juman (Table 1)." ></td>
	<td class="line x" title="119:265	In the experiments described in the next section, we approximated the results from this example (see Table 1) by the formula 1/(a* cost + b), where a was 0.5 and b 1.19." ></td>
	<td class="line x" title="120:265	161 5 Experiments 5.1 Implementation The experimental system for model estimation was implemented using the extended reestimation method." ></td>
	<td class="line x" title="121:265	A morpheme network of each input sentence was generated with Juman (Matsumoto et al. , 1994) and the credit factor was attached to each branch as described above." ></td>
	<td class="line x" title="122:265	The system can estimate three kinds of models; the PAIRoHMM (formula 3) with output symbols as pairs of words and tags, the TAG-bigram model (formula 4, where N = 2) and TAG-HMM (formula 5) with output symbols as tags and p(w\]t)." ></td>
	<td class="line x" title="123:265	The scaling technique was used with all estimations." ></td>
	<td class="line x" title="124:265	The numbers of parameters of the TAG-bigram model, the TAG-HMM and the PAIR-HMM are approximated by the equations NT 2 + ND, NS 2 + NS * NT + ND, and NS 2 + NS * ND, respectively, where NT is the number of tags, NS is the number of states of the HMM, and ND is the number of entries in the dictionary." ></td>
	<td class="line x" title="125:265	In all experiments, NT, NS and ND were fixed at 104, 10, and 130,000, respectively." ></td>
	<td class="line x" title="126:265	The numbers of parameters of the TAG-bigram model, TAG-HMM, PAIR-HMM were 10816 + ND, 1140 + ND, and 100+ IOND, respectively." ></td>
	<td class="line x" title="127:265	Note that the number of parameters of the tag model of the TAG-HMM is one tenth that of the TAG-bigram model." ></td>
	<td class="line x" title="128:265	For the model evaluation, a stochastic tagger was implemented." ></td>
	<td class="line x" title="129:265	Given a morpheme network generated by Juman with a cost-width, the implemented tagger selects the most probable path in the network using each stochastic model." ></td>
	<td class="line x" title="130:265	The best path was calculated by the Viterbialgorithm on the paths of the morpheme network." ></td>
	<td class="line x" title="131:265	5.2 Data and Evaluation I used 26108 Japanese untagged sentences as training data and 100 hand-tagged sentences as test data, both from the Nikkei newspaper 1994 corpus (Nihon Keizai Shimbun, Inc. , 1995)." ></td>
	<td class="line x" title="132:265	The test sentences include about 2500 Japanese morphemes." ></td>
	<td class="line x" title="133:265	The tags were defined as the combination of part-of-speech, conjugation, and class of conjugation." ></td>
	<td class="line x" title="134:265	The number of kinds of tags was 104." ></td>
	<td class="line x" title="135:265	In the precision evaluation, the correct morpheme was defined as that matching the segmentation, tag, and spelling of the base form of the hand-tagged morpheme." ></td>
	<td class="line x" title="136:265	The precision was defined as the proportion of correct morphemes relative to the total number of morphemes in the sequence which the tagger outputted as the best alignment of tags and words." ></td>
	<td class="line x" title="137:265	5.3 Results Three kinds of models were estimated using the untagged training data with the initial parameters set to the equivalent probabilities." ></td>
	<td class="line x" title="138:265	Each model was estimated both with and without use of the credit factor." ></td>
	<td class="line x" title="139:265	The reestimation algorithm was iterated for five to twenty times." ></td>
	<td class="line x" title="140:265	The precision of the most plausible segmentation and tag assignment was outputted by the tagger based on each stochastic model estimated either without (Figs." ></td>
	<td class="line x" title="141:265	4 and 5) or with (Fig." ></td>
	<td class="line x" title="142:265	6) the credit factor assignment function described in the previous section." ></td>
	<td class="line x" title="143:265	Two versions of the morpheme network for the estimations were used; one limited by a cost-width of 500 (Fig." ></td>
	<td class="line x" title="144:265	4) and the other by a cost-width of 70 (Figs." ></td>
	<td class="line x" title="145:265	5 and 6)." ></td>
	<td class="line x" title="146:265	The cost-width of 500 required almost all of the morphemes to be used for the estimation." ></td>
	<td class="line x" title="147:265	In other words, a morpheme network of cost-width 500 was equivalent to that extracted from the input sentence with a dictionary only." ></td>
	<td class="line x" title="148:265	Although one experiment (Fig." ></td>
	<td class="line x" title="149:265	5) didn't use the credit factor assignment function, it is regarded as using a special function of the credit factor that returns 0 or 1, that 162 cost-width 0 10 20 50 100 200 500 1000 precision recall 84.4 79.8 79.3 71.0 66.3 43.6 36.5 36.3 94.7 96.0 96.1 97.2 98.0 98.7 98.7 98.7 Table 2: The precision and recall of Juman on each cost-width." ></td>
	<td class="line x" title="150:265	is a step function, with a cost threshold of 70." ></td>
	<td class="line x" title="151:265	However, this function doesn't differentiate among morphemes whose costs are 0 and 70." ></td>
	<td class="line x" title="152:265	The cost-widths (see horizontal axes in Figs." ></td>
	<td class="line x" title="153:265	4, 5 and 6) were provided to Juman to generate the morpheme network used in the stochastic tagger for model evaluation." ></td>
	<td class="line x" title="154:265	The tagger chose the best morpheme sequence from the network by each stochastic model." ></td>
	<td class="line x" title="155:265	A larger cost-width would result in a larger network, lower precision, and higher recall (Table 2)." ></td>
	<td class="line x" title="156:265	Note that the precision of any model will never exceed the recall of Juman (see Table 2)." ></td>
	<td class="line x" title="157:265	If a model is correctly estimated, then a larger cost-width will improve precision." ></td>
	<td class="line x" title="158:265	Therefore, we can estimate model accuracy from the precision at cost-width 500 or 1000." ></td>
	<td class="line x" title="159:265	When estimated without the credit factor (Fig." ></td>
	<td class="line x" title="160:265	4), neither the HMM nor the TAG-bigram model was robust against noisy training data." ></td>
	<td class="line x" title="161:265	It was also observed in the experiments that the accuracy of tagging was degraded by excessive iterations of reestimation." ></td>
	<td class="line x" title="162:265	I conclude that it is hard to estimate the Japanese model from only an untagged corpus and a dictionary." ></td>
	<td class="line x" title="163:265	Precision was improved by the step credit factor function whose threshold is 70 (Fig." ></td>
	<td class="line x" title="164:265	5)." ></td>
	<td class="line x" title="165:265	The precision of the HMMs are better than the precision of the TAG-bigram model, despite the number of parameters of the TAG-HMM being smaller than that for the TAG-bigram model." ></td>
	<td class="line x" title="166:265	The HMM is very capable of modeling language, if the training data is reliable." ></td>
	<td class="line x" title="167:265	Including the variable credit factor in these models is an effective way to improve precision (Fig." ></td>
	<td class="line x" title="168:265	6)." ></td>
	<td class="line x" title="169:265	In particular, the results of the TAG-bigram model were dramatically improved by using the variable credit factor." ></td>
	<td class="line x" title="170:265	Although incorporating the credit factor into the HMM improved the results, they remained at a level similar to that of the TAG-bigram model with the credit factor." ></td>
	<td class="line x" title="171:265	Although it is not clear exactly why the HMM did not improved more, there are at least three possible explanations: (1) theoretical limitation of estimation using a~ untagged corpus, (2) using an untagged corpus, estimation of the HMM is harder than estimation of the bigram model, therefore more corpora are needed to train the HMM or (3) the credit factor in this experiment matched to the bigram model but not to the HMM." ></td>
	<td class="line x" title="172:265	Investigation of these possibilities in the future is needed." ></td>
	<td class="line x" title="173:265	6 Discussion and Future Work Merialdo (1994) and Elworthy (1994) have insisted, based on their experimental results, that the maximum likelihood training using an untagged corpus does not necessarily improve tagging accuracy." ></td>
	<td class="line x" title="174:265	However, their likelihood was the probability with all paths weighted equivalently." ></td>
	<td class="line x" title="175:265	Since more than half of the symbols in the observations may be noise, models estimated in this way are not reliable." ></td>
	<td class="line x" title="176:265	The credit factor was introduced to redefine the likelihood of training data." ></td>
	<td class="line x" title="177:265	The new likelihood was based on the probability with each possible path weighted by the credit factor." ></td>
	<td class="line x" title="178:265	The extended reestimation algorithm can approximately maximize the modified likelihood and improve the model accuracy." ></td>
	<td class="line x" title="179:265	The Baum-Welch reestimation algorithm was also extended in two ways." ></td>
	<td class="line x" title="180:265	The algorithm can be applied to an unsegmented language (e.g. , Japanese), because of the extension for coping with lattice-based observations as training data." ></td>
	<td class="line x" title="181:265	The other extension is that the algorithm can 163 precision 94.00 93.00 92.00 91.00 90.00 89.00 88.00 87.00 86.00 85.00 84.00 83.00 82.00 81.00 80.00  ~,  4  2'-.,.4  \-~ PAIR qrAG-bigram -HM.M Ii ofi.~na_l Juman  --II ', . TAG-bigram % % I % ', TAG-HMN % b  (( 0 )) 10 30 100 300 cost-width 1000 Figure 4: The precision of the models estimated without the credit factor 164 precision 94.00 93.50 93.00 ih 92.50 92.00 91.50  91.00 90.50 -'  PAIR-I-\] MM TAG-HMM,XfE:i-ffq_Cf '''--e  -t I TAG-HMM  O  original Junq TAG-bigrar~ 0 10 30 100 300 1000 cost-width an Figure 5: The precision of the models estimated with the step credit factor." ></td>
	<td class="line x" title="183:265	precision 94.00 93.50 93.00 92.50, 92.00 ---91.50 91.00 90.50 I 0 'POS-bigram rda G  J'- 4 ''''-B TAG-I-IMM op i_n J.u_ an 10 30 100 300 1000 cost-width Figure 6: The precision of the models estimated with the variable credit factor." ></td>
	<td class="line x" title="184:265	165 train the HMM in addition to the N-gram model." ></td>
	<td class="line x" title="185:265	Takeuchi and Matsumoto (1995) proposed the bigram estimation method from an untagged Japanese corpus." ></td>
	<td class="line x" title="186:265	Their algorithm divides a morpheme network into possible sequences that are then used for the normal Baum-Welch algorithm." ></td>
	<td class="line x" title="187:265	This algorithm cannot take advantage of the scaling procedure, because it requires the synchronous calculation of all possible sequences in the morpheme network." ></td>
	<td class="line x" title="188:265	Nagata (1996) recently proposed a generalized forward-backward algorithm that is a character synchronous method for unsegmented languages." ></td>
	<td class="line x" title="189:265	He applied this algorithm to bigram model training from untagged Japanese text for new word extraction." ></td>
	<td class="line x" title="190:265	However, he did not apply this algorithm to the estimation of HMM parameters." ></td>
	<td class="line x" title="191:265	Two additional experiments have been planned." ></td>
	<td class="line x" title="192:265	One is related to the limitations of estimation using untagged corpora." ></td>
	<td class="line x" title="193:265	The other is related to assignment of the credit factor without a rule-based tagger." ></td>
	<td class="line x" title="194:265	The credit factor improved the upper bound of the estimation accuracy from an untagged corpus." ></td>
	<td class="line x" title="195:265	However, at higher levels of tagging accuracy, the reestimation method based on the Baum-Welch algorithm is limited by the noise of untagged corpora." ></td>
	<td class="line x" title="196:265	On this point, I agree with Merialdo (1994) and Elworthy (1994)." ></td>
	<td class="line x" title="197:265	One promising direction for future work would be an integration of models estimated from tagged and untagged corpora." ></td>
	<td class="line x" title="198:265	Although the total model estimated from an untagged corpus is worse than that from a model using a tagged corpus, a part of the model using the untagged corpus may be better, because estimations from untagged corpora can use very extensive training material." ></td>
	<td class="line x" title="199:265	In the bigram model, we can weight each probability of a pair of tags in both models estimated from tagged or untagged corpora." ></td>
	<td class="line x" title="200:265	A smoothing method, such as deleted interpolation (Jelinek, 1985), can be used for weighting." ></td>
	<td class="line x" title="201:265	Another promising avenue for research is the development of improved methods to assign the credit factor without using rule-based taggers." ></td>
	<td class="line x" title="202:265	Any chosen rule-based tagger will impart its own characteristic errors to credit factors it has been used to assign." ></td>
	<td class="line x" title="203:265	Such errors can be misleading in the modeling of language." ></td>
	<td class="line x" title="204:265	In order to assign more neutral values to the credit factor, we can use the estimated model itself." ></td>
	<td class="line x" title="205:265	In the initial estimation of a model, an equivalent credit factor is used for estimation." ></td>
	<td class="line x" title="206:265	After several iterations of reestimation, development data tagged by hand is used to evaluate the estimated model." ></td>
	<td class="line x" title="207:265	The credit factors can be assigned from this evaluation process and be used in the second phase of estimation." ></td>
	<td class="line x" title="208:265	Following the second phase of estimation, new credit factors would be decided by evaluation of the new model." ></td>
	<td class="line x" title="209:265	Such a global iteration is a special version of error correcting learning." ></td>
	<td class="line x" title="210:265	7 Conclusion We have proposed an estimation method from ambiguous observations and a credit factor." ></td>
	<td class="line x" title="211:265	This estimation method can use untagged, unsegmented language corpora as training data and build not only the N-gram model, but also the HMM." ></td>
	<td class="line x" title="212:265	A credit factor can improve the reliability of the model estimated from an untagged corpus." ></td>
	<td class="line x" title="213:265	This method can be further improved and integrated with other language models." ></td>
	<td class="line x" title="214:265	In particular, it is important to formulate a dynamic method to assign the credit factor based on small sets of tagged data for development." ></td>
	<td class="line x" title="215:265	Acknowledgement Thanks are due to the members of both the Itahashi laboratory at the University of Tsukuba and the Nakagawa laboratory at the Toyohashi University of Technology for their help and criticism at various stages of this research." ></td>
	<td class="line x" title="216:265	166 References Cutting, D. , J. Kupiec, J. Pedersen and P. Sibun." ></td>
	<td class="line x" title="217:265	1992." ></td>
	<td class="line x" title="218:265	A practical part-of-speech tagger." ></td>
	<td class="line x" title="219:265	In Proceedings off the Second Conference on Applied Natural Language Processing, pages 133-140." ></td>
	<td class="line x" title="220:265	Association for Computational Linguistics, Morristown, New Jersey." ></td>
	<td class="line x" title="221:265	Elworthy, David." ></td>
	<td class="line x" title="222:265	1994." ></td>
	<td class="line x" title="223:265	Does Baum-Welch re-estimation help taggers?" ></td>
	<td class="line x" title="224:265	In Proceedings of the 4th Conference on Applied Natural Language Processing, pages 53-58." ></td>
	<td class="line x" title="225:265	Association for Computational Linguistics, Morristown, New Jersey." ></td>
	<td class="line x" title="226:265	Japan Electronic Dictionary Research Institute." ></td>
	<td class="line x" title="227:265	1995." ></td>
	<td class="line x" title="228:265	EDR Electronic Dictionary Version 2 Technical Guide." ></td>
	<td class="line x" title="229:265	http://www.iijnet.or.jp/edr." ></td>
	<td class="line x" title="230:265	Jelinek, Frederick." ></td>
	<td class="line x" title="231:265	1985." ></td>
	<td class="line x" title="232:265	Self-organized language modeling for speech recognition." ></td>
	<td class="line x" title="233:265	IBM Report." ></td>
	<td class="line x" title="234:265	(Reprinted in Readings in Speech Recognition, pages 450-506, Morgan Kaufmann)." ></td>
	<td class="line x" title="235:265	Kupiec, Julian." ></td>
	<td class="line x" title="236:265	1992." ></td>
	<td class="line x" title="237:265	Robust part-of-speech tagging using a hidden Markov model." ></td>
	<td class="line x" title="238:265	Computer Speech and Language, 6, pages 225-242." ></td>
	<td class="line x" title="239:265	Matsumoto, Y. , S. Kurohashi, T. Utsuro, Y. Nyoki and M. Nagao 1994." ></td>
	<td class="line x" title="240:265	Japanese morphological analysis system JUMAN manual (in Japanese)." ></td>
	<td class="line x" title="241:265	Merialdo, Bernard." ></td>
	<td class="line x" title="242:265	1994." ></td>
	<td class="line x" title="243:265	Tagging English text with a probabilistic model." ></td>
	<td class="line x" title="244:265	Computational Linguistics, 20(2), pages 155-171." ></td>
	<td class="line x" title="245:265	Nagata, Masaaki." ></td>
	<td class="line x" title="246:265	1994." ></td>
	<td class="line x" title="247:265	A stochastic Japanese morphological analyzer using a forward-DP backward-A* N-best search Mgorithm." ></td>
	<td class="line x" title="248:265	In Proceedings of COLING-94, pages 201-207." ></td>
	<td class="line x" title="249:265	Nagata, Masaaki." ></td>
	<td class="line x" title="250:265	1996." ></td>
	<td class="line x" title="251:265	Automatic extraction of new words from Japanese texts using generalized forward-backward search." ></td>
	<td class="line x" title="252:265	In Proceedings of Empirical Methods in Natural Language Processing, pages 48-59." ></td>
	<td class="line x" title="253:265	Nihon Keizai Shimbun, Inc. 1995." ></td>
	<td class="line x" title="254:265	Nikkei newspaper database 1994, CD-ROM version." ></td>
	<td class="line x" title="255:265	Rabiner, Lawrence and Biing-Hwang Juang." ></td>
	<td class="line x" title="256:265	1994." ></td>
	<td class="line x" title="257:265	Fundamentals of Speech Recognition." ></td>
	<td class="line x" title="258:265	PTR Prentice-Hall, Inc. Takeuchi, Kouichi and Yuji Matsumoto." ></td>
	<td class="line x" title="259:265	1995." ></td>
	<td class="line x" title="260:265	Learning parameters of Japanese morphological analyzer based-on hidden Markov model." ></td>
	<td class="line x" title="261:265	IPSJ Technical Report SIG-NL, 108-3, pages 1319 (in Japanese)." ></td>
	<td class="line x" title="262:265	Zhou, Min and Seiichi Nakagawa." ></td>
	<td class="line x" title="263:265	1994." ></td>
	<td class="line x" title="264:265	A study of stochastic language models for Japanese and English." ></td>
	<td class="line x" title="265:265	In Proceedings of Symposium on Learning in Natural Language Processing, pages 57-64 (in Japanese) ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W96-0205
Automatic Extraction Of New Words From Japanese Texts Using Generalized Forward-Backward Search
Nagata, Masaaki;"></td>
	<td class="line x" title="1:300	Automatic Extraction of New Words from Japanese Texts using Generalized Forward-Backward Search Masaaki NAGATA NTT Information and Communication Systems Laboratories 1-2356 Take, Yokosuka-Shi, Kanagawa, 238-03 Japan nagat anttnly." ></td>
	<td class="line x" title="2:300	 sl." ></td>
	<td class="line x" title="3:300	ntt." ></td>
	<td class="line x" title="4:300	j p Abstract We present a novel new word extraction method from Japanese texts based on expected word frequencies." ></td>
	<td class="line x" title="5:300	First, we compute expected word frequencies from Japanese texts using a robust stochastic N-best word segmenter." ></td>
	<td class="line x" title="6:300	We then extract new words by filtering out erroneous word hypotheses whose expected word frequencies are lower than the predefined threshold." ></td>
	<td class="line x" title="7:300	The method is derived from an approximation of the generalized version of the Forward-Backward algorithm." ></td>
	<td class="line x" title="8:300	When the Japanese word segmenter is trained on a 4.7 million word segmented corpus and tested on 1000 sentences whose out-of-vocabulary rate is 2.1%, the accuracy of the new word extraction method is 43.7% recall and 52.3% precision." ></td>
	<td class="line x" title="9:300	Introduction Segmentation of sentences into words is trivial in English because words are delimited by spaces." ></td>
	<td class="line x" title="10:300	It is a simple task to count word frequencies in a given text." ></td>
	<td class="line x" title="11:300	It is also a simple task to list all new words (unknown words), namely, the words in a given text that are not found in the system dictionary." ></td>
	<td class="line x" title="12:300	However, several languages such as Japanese, Chinese and Thai do not put spaces between words and so in these languages word segmentation, word frequency counting, and new word extraction remain unsolved problems in computational linguistics." ></td>
	<td class="line x" title="13:300	Most Japanese NLP applications require word segmentation as a first stage because there are phonological units and semantic units whose pronunciation and/or meaning is not trivially derivable from the pronunciation and/or meaning of the individual characters." ></td>
	<td class="line x" title="14:300	It is well known that the accuracy of word segmentation greatly depends on the coverage of the dictionary, in other words, the Out-Of-Vocabulary (00V) rate of the target texts." ></td>
	<td class="line x" title="15:300	Our goal is to provide a method to automatically extract new words from Japanese texts." ></td>
	<td class="line x" title="16:300	This nmthod should adapt the dictionary of the word segmenter to new domains and applications." ></td>
	<td class="line x" title="17:300	It should also maintain the dictionary by collecting new words in the target domain." ></td>
	<td class="line x" title="18:300	The application of the word segmenter is described elsewhere (Nagata, 1996)." ></td>
	<td class="line x" title="19:300	The approach we take is as follows: First, we design a statistical language model that can assign a reasonable word probability to an arbitrary substring in the input sentence, whether or not it is truly a word." ></td>
	<td class="line x" title="20:300	Second, we devised a method to obtain the expected word N-gram count in the target texts, using an N-best word segmentation algorithm (Nagata, 1994)." ></td>
	<td class="line x" title="21:300	Finally, we extract new words by filtering out spurious word hypotheses whose expected word frequencies are lower than the threshold." ></td>
	<td class="line x" title="22:300	Japanese Morphological Analysis Before we start, we briefly explain the difficulties of Japanese morphological analysis, especially when the input sentence includes unknown words." ></td>
	<td class="line x" title="23:300	Suppose the input sentence is '-~4)~p/~7 ~}~ ENIAC 69 50 ~ 3o ', which means 'University of Pennsylvania celebrates the 50th anniversary of ENIAC', where the words ~Y5~ JP/~7 (transliteration of 'Pennsylvania') and ENIAC (the name of the world's first computer) are not registered in the system dictionary." ></td>
	<td class="line x" title="24:300	Figure 1 shows three possible analyses of the input sentence, where each box represents a word hypothesis whose meaning and part of speech are shown above and under the box." ></td>
	<td class="line x" title="25:300	The tag <UNK> represents an unknown word." ></td>
	<td class="line x" title="26:300	One of the hardest problems in handling unrestricted Japanese text is the identification of unknown words." ></td>
	<td class="line x" title="27:300	In Figure 1, the string ENIAC is successfully tokenized as an unknown word." ></td>
	<td class="line x" title="28:300	However, there is ambiguity in the segmentation of the string ~ 5/zL~J<~-7~." ></td>
	<td class="line x" title="29:300	In the first analysis, the system considers -~'-.~ 5//1~/~_~7 ('Pennsylvania') as an unknown word, 48 Logprob ''4 ~ ~ \]1t \]< ~ 7' 5k: ~ ~ E N l A C cO (rel prob) ~_~y ~j ENIAC of Pennsylvania '\[ ENIAC -108.95 \[ -'~5.'J1.-'-'<-~7' \] \] \[\] (0.790) <UNK> noun part." ></td>
	<td class="line x" title="30:300	<UNK> part." ></td>
	<td class="line x" title="31:300	pcnqil Vania university ~j. ENIAC of -no.49 i~,v I 1'<=7~'~ I I ENIA I \[\] (0.169) noun <UNK> part." ></td>
	<td class="line x" title="32:300	<UNK> part." ></td>
	<td class="line x" title="33:300	-ll 1.90 oencil Vania ~i~y ~j. ENIAC ~\] (0.041) I -'<~5'7~ I \[ )<=7 \] I ENIAC \] noun <UNK> noun part." ></td>
	<td class="line x" title="34:300	<UNK> part." ></td>
	<td class="line x" title="35:300	numeral suffix part." ></td>
	<td class="line x" title="36:300	verb intl." ></td>
	<td class="line x" title="37:300	sym." ></td>
	<td class="line x" title="38:300	numeral suffix part." ></td>
	<td class="line x" title="39:300	verb intl." ></td>
	<td class="line x" title="40:300	sym." ></td>
	<td class="line x" title="41:300	numeral suffix part." ></td>
	<td class="line x" title="42:300	verb infl.sym." ></td>
	<td class="line x" title="43:300	Figure 1: Japanese Morphological Analysis Example because ~: ('university') is registered in the dictionary." ></td>
	<td class="line x" title="44:300	This is correct." ></td>
	<td class="line x" title="45:300	In the second analysis, the system guesses.'<~-7'~: ('Vania university') as an unknown word, because -'<~/5.'A~ (transliteration of 'pencil') is registered in the dictionary and some university names are registered in the dictionary, such as Y,~ ~/7~---b'~ ('Stanford University') and ~r~'~ ~3~-~ (.'Cambridge University')." ></td>
	<td class="line x" title="46:300	In the third analysis, the system considers,'<-~7' ('Vania') as an unknown word, because both ~:/5,',,1~ and ~ are registered in the dictionary." ></td>
	<td class="line x" title="47:300	It is often the case that we have overlapping word hypotheses if the input sentence contains unknown words, such as -'<~'~\]P.'<~7, ~:7~, and,,<:'T in Figure 1." ></td>
	<td class="line x" title="48:300	We need a criteria to select the most likely word hypothesis from among the overlapping candidates." ></td>
	<td class="line x" title="49:300	In fact, it is fairly difficult to get plausible analyses like the ones shown in Figure 1, because failure to identify an unknown word affects the segmentation of the neighboring words." ></td>
	<td class="line x" title="50:300	Obviously, a robust word segmenter is the essential first step." ></td>
	<td class="line x" title="51:300	In the following sections, we first describe a statistical language model to cope with unknown words." ></td>
	<td class="line x" title="52:300	We then describe the word segmentation algorithm and the new word extraction method, with their derivation as an approximation of a generalization of the Forward-Backward algorithm (Baum, 1972)." ></td>
	<td class="line x" title="53:300	Finally, we show experiment results and prove its effectiveness." ></td>
	<td class="line x" title="54:300	Statistical Language Model Segmentation Model (Tagging Model) Let the input Japanese character sequence be C = ca c2 cm, and segment it into word sequence W = wl w2  wn whose part of speech sequence is 7' = tlt2tn." ></td>
	<td class="line x" title="55:300	The word segmentation task can be defined as finding the set of word segmentation and parts of speech assignment (~V, T) that maximize the joint probability of word sequence and tag sequence given character sequence P(W, TIC )." ></td>
	<td class="line x" title="56:300	Since the maximization is carried out with fixed character sequence C, the word segmenter only has to maximize the joint probability of word sequence and tag sequence P(W, T)." ></td>
	<td class="line x" title="57:300	(w, '/1) = arg,nax P(W, TIC) W,T = arg alas P(W, 7') (i) We call P(W,T) the segmentation model, although it is usually called tagging model in English tagger research." ></td>
	<td class="line x" title="58:300	In this paper, we compare three segmentation models: part of speech trigram, word unigram, and word bigram." ></td>
	<td class="line x" title="59:300	In the part-of-speech trigram model (POS trigram model), the joint probability P(W, T) is approximated by the product of parts of speech trigram probabilities P(tilti_2,ti_l) and word output probabilities for given part of speech P(wiItl) n P(W,T) = ~I P(tilt,_2,q_OP(wilt,) (2) i=1 In the word unigram and word bigram models, the joint probability P(W,T) is approximated by the product of word unigram probabilities P(wi,ti) and word bigram probabilities P( wl, ti lwia, tia), respectively." ></td>
	<td class="line x" title="60:300	P(W,T) = H p(w,,t,) (3) i=a P(W,T) = l'I P(wi,tilwi_a,ti_a) (4) i=a Basically, parameters of these segmentation models are estimated by computing the relative frequencies of the corresponding events in the segmented training corpus." ></td>
	<td class="line x" title="61:300	However, in order to ham dle unknown words, we have introduced a slight modification in computing the relative frequencies, as is described in the next section." ></td>
	<td class="line x" title="62:300	49 Word Model We think of an unknown word as a word having a special part of speech <U~IK>." ></td>
	<td class="line x" title="63:300	We define a statistical word model to assign a word probability to each word hypothesis." ></td>
	<td class="line x" title="64:300	It is formally defined as the joint probability of the character sequence cl  ck if wi is the unknown word." ></td>
	<td class="line x" title="65:300	We decompose it into the product of word length probability and word spelling probability, P(wi\[<U~K>) = P(e, ck I<UNK>) = P(k)P(Cl ck IZ~) (5) where k is the length of the character sequence." ></td>
	<td class="line x" title="66:300	We call P(k) the word length model, and P(cl  ck I k) the word spelling model." ></td>
	<td class="line x" title="67:300	We assume that word length probability P(k) obeys a Poisson distribution whose paraineter is the average word length,~ in the training corpus, (~ 1)~-~ P(k)'~':~)T. e-()~-l) (6) This means that we regard word length as the interval between hidden word boundary markers, which are randomly placed with an average interval equal to the average word length." ></td>
	<td class="line x" title="68:300	Although this word length model is very simple, it plays a key role in making the word segmentation algorithm robust." ></td>
	<td class="line x" title="69:300	We approximate the spelling probability given word length P(O  ck \[k) by the word-based character bigram model, regardless of word length." ></td>
	<td class="line x" title="70:300	Since there are more than 3,000 characters in Japanese, the amount of training data would be too small if we divided them by word length." ></td>
	<td class="line x" title="71:300	k P(cl ca) = P(c, I#) 1Y\[ P(c, I~,-,)P(#1~)(7) Here, special symbol '#' indicates the word boundary marker." ></td>
	<td class="line x" title="72:300	Note that the word-based character bigram model is different from the sentence-based character bigram model." ></td>
	<td class="line x" title="73:300	The former is estimated from the corpus segmented into words." ></td>
	<td class="line x" title="74:300	It assigns a large probability to a character sequence that appears in the beginning (prefixes), the middle, and the end (suffixes) of a word." ></td>
	<td class="line x" title="75:300	It also assigns a small probability to a character sequence that appears across a word boundary." ></td>
	<td class="line x" title="76:300	By using the word model, we can create modified segmentation models that take unknown words into consideration." ></td>
	<td class="line x" title="77:300	The parameters of the modified POS trigram, word unigram, and word bigram are estimated by Equations (8), (9), (10), and (11), in Figure 2." ></td>
	<td class="line x" title="78:300	hi Figure 2, C()." ></td>
	<td class="line x" title="79:300	denotes the count of the specified event in the training corpus." ></td>
	<td class="line x" title="80:300	In the part of speech trigram model, P(wi\[ti) for an unknown word wi is obtained, by definition, from the word model P(wi\]<UNK>)." ></td>
	<td class="line x" title="81:300	In the word unigram model, the unigram count C(wi) for unknown word wi is given as the product of the total unigram count of unknown words C(<UNK>) and the word model probability P(wil<UNK>)." ></td>
	<td class="line x" title="82:300	The higher order Ngram counts involving unknown words are also obtained in the same manner." ></td>
	<td class="line x" title="83:300	In order to compute the parameters in Figure 2, we need the counts involving unknown words, such as C(ti-2, ti-1, <UNK>), C(<UNK>), and C((wi-~,tl-a),<UNK>)." ></td>
	<td class="line x" title="84:300	These counts are important because they represent the contexts in which unknown words likely to appear." ></td>
	<td class="line x" title="85:300	To estimate these counts, we replace all words appearing only once in the training corpus with unknown word tags <UNK>, before computing relative frequencies." ></td>
	<td class="line x" title="86:300	The underlying idea of the replacement is the same as Turing's estimates in back-off smoothing (Katz, 1987)." ></td>
	<td class="line x" title="87:300	We redistribute the probability mass of low count sequences to 'unseen' sequences." ></td>
	<td class="line oc" title="88:300	Generalized Forward Backward Reestimation Generalization of the Forward and Viterbi Algorithm In English part of speech taggers, the maximization of Equation (1) to get the most likely tag sequence, is accomplished by the Viterbi algorithm (Church, 1988), and the maximum likelihood estimates of the parameters of Equation (2) are obtained from untagged corpus by the ForwardBackward algorithm (Cutting et al. , 1992)." ></td>
	<td class="line n" title="89:300	However, it is impossible to apply the Viterbi algorithm and the Forward-Backward algorithm for word segmentation of those languages that have no delimiter between words, such as Japanese and Chinese, because word segmentation hypotheses overlap one another." ></td>
	<td class="line x" title="90:300	Figure 3 shows an example of overlapping word hypotheses and possible word segmentations for the string ~N~t~ig-f~ ('all prefectures in the nation')." ></td>
	<td class="line x" title="91:300	We assume ~\[\] ('all nation'), ~ ('all'), \[~l ('national capital'), ~ii~;g~t,~ ('prefectures'), ~i.~ ('metropolitan road'), ~li ('metropolis'), ~Kff t.~ ('prefectures'), ~ ('road'), ~ ('prefectures'), ~.f ('prefecture'), and ~ ('prefecture') are registered in the dictionary." ></td>
	<td class="line x" title="92:300	There are 15 possible word segmentations in this example." ></td>
	<td class="line x" title="93:300	In Japanese, a lot of words consist of one character." ></td>
	<td class="line x" title="94:300	Moreover, sequence of characters may constitute a different word." ></td>
	<td class="line x" title="95:300	50 C(t,_2,t,_ x,<UNK>) P(tilti-2,ti-~) = c(t,_2,t,_t) ifti ---<lINK> c(~,_~,t,_~) otherwise t:'(wiI<UNK>) if tl = <UNK> P(wi Its) = _~ otherwise c(<U~K>) P(wi,ti) ~_ .c(,o,.t,)  P(w~I<U\]K>) if/~ = <lINK> : ~(w,,t,) otherwise ~, c(w,*,) c((w,_~,~,_O,<U~iK>) P(wl,tilwi_~,ti-~) : c(,o,_~,t,_~) x P(wiI<UNK>) ifti = <UNK> c((w,_~,t,_x),(w,,,,)) otherwise c(w,_x,t,-x) Figure 2: Modified Segmentation Models with Consideration to Unknown Words." ></td>
	<td class="line x" title="96:300	(8) (9) (10) (11) I l Figure 3: Overlapping Word Hypotheses and Possible Word Segmentations For Japanese word segmentation, we define a generalized Forward algorithm and a generalized Viterbi algorithm as follows." ></td>
	<td class="line x" title="97:300	Let the input Japanese character sequence of length n be C = cl c2 . . ." ></td>
	<td class="line x" title="98:300	c,, and cg denote the substring cp+ l  %." ></td>
	<td class="line x" title="99:300	We define a flmction D that maps a character sequence c_q to a list of word hypotheses {wi}." ></td>
	<td class="line x" title="100:300	Function/~ is the generalization of the dictionary." ></td>
	<td class="line x" title="101:300	Here, wi denotes a combination of orthography (formally denoted by wi) and part of speech ti, for simplicity." ></td>
	<td class="line x" title="102:300	We use word bigram as the segmentation model in the following example." ></td>
	<td class="line x" title="103:300	Other segmentation models, such as part of speech trigram and word unigrarn, can be used in the same manner." ></td>
	<td class="line x" title="104:300	In the generalized forward algorithm, the forward probability o~(wi) is the joint probability of the character sequence c~ and the event that the final word in the segmentation of cq0 is wi that spans the substring d. Forward probabilities can be recurslvely computed as follows." ></td>
	<td class="line x" title="105:300	O<p<q wiED(c~) e o < q <., q < <." ></td>
	<td class="line x" title="107:300	02) The generalized forward algorithm starts from the beginning of the input sentence, and proceeds character by character." ></td>
	<td class="line x" title="108:300	At each point q in the sentence, it sums over the product of the forward probability of the word segmentation hypotheses ending at the point ~pq(wl) and the transition probability to the word hypotheses starting at that point P(wi+l \[wi)." ></td>
	<td class="line x" title="109:300	o~ i ~ 2~ 3~ 4~ s~ 6, Figure 4: One Step in the Generalized Forward Algorithrn." ></td>
	<td class="line x" title="110:300	Figure 4 shows a snapshot of the generalized forward algorithm." ></td>
	<td class="line x" title="111:300	Tile input is ~\[\]~i~, and the current point q is 2." ></td>
	<td class="line x" title="112:300	The word hypotheses ending at point 2 (wi 6 n(c~)) are ~I~ (Co 2) and \[\] (c~)." ></td>
	<td class="line x" title="113:300	Those starting at point 2 (wi+x 6 D(c~)) are ~J.~ (c~), ~_ (c~), and ~li (c~)." ></td>
	<td class="line x" title="114:300	The string ~$~ (c25) is not registered in the dictionary." ></td>
	<td class="line x" title="115:300	All combinations of these words are examined." ></td>
	<td class="line x" title="116:300	The generalized Viterbi algorithm can be ob51 tained by replacing summation with maximization in Equation (12)." ></td>
	<td class="line x" title="117:300	Here, Cpq(wi) is the probability of the most likely word segmentation sequence for the character sequence cq0 whose final word wi spans the substring c~." ></td>
	<td class="line x" title="118:300	6;(wi+l) = max max q~(w,)P(w,+~lw,) o_<p<q ~,ev(~) w,+l e D(c;),O _< q < u,q < r < n (13) Note that tile original Forward algorithm and tile Viterbi algorithin is the special case in Equation (12) and (13) where p and q are fixed as p=q-1 andr=q+i. In order to handle unknown words, the dictionary function D returns a word hypothesis tagged as unknown word if the substring cpq is not registered in the dictionary, such as ~i.~gf (%5) in Figure 4." ></td>
	<td class="line x" title="119:300	The word model assigns a reasonable probability to the unknown word." ></td>
	<td class="line x" title="120:300	Therefore, in the generalized forward algorithm and the generalized Viterbi algorithm, we hypothesize all substrings in the input sentence as words, and examine all possible combinations of these word hypotheses." ></td>
	<td class="line x" title="121:300	Since we can define the generalized Backward algorithm in the same manner, we can define the generalized Forward-Backward algorithm to estimate the word N-gram counts in Japanese texts, and to reestimate the word N-gram probabilities in the segmentation model." ></td>
	<td class="line x" title="122:300	However, we give a more intuitive account of the method to introduce an approximation of the generalized Forward-Backward algorithm." ></td>
	<td class="line x" title="123:300	Expected Word N-gram Count By using the above mentioned word segmentation algorithm, we can get all word segmentation hypotheses of the input sentence." ></td>
	<td class="line x" title="124:300	Once we get them, we can estimate word N-gram count in an unsegmented Japanese corpus." ></td>
	<td class="line x" title="125:300	Let Oj be the jth word segmentation hypothesis for the ith sentence in the corpus." ></td>
	<td class="line x" title="126:300	P(O~) can  d be cornputed by using the segmentahon model The Bayes a posleriori estimate of the word unigram count Ci(wi) and the word bigram count Ci(wi_l, wi) ill the ith sentence can be computed as, C'(wo) = ~'~' P(Oj) x n~(w~)) (14) z ,t P(oD 3 i r-,, P(O}) xn~(w~, c (wo,w ) = P(O;) -3 Here,." ></td>
	<td class="line x" title="127:300	n}(w~) and." ></td>
	<td class="line x" title="128:300	ni'(w~'w3 Z) denote the number of tunes the umgram w~ and the bigram w~, w~ appeared in tile jth candidate of tile ith sentence 1 The estimate of the total unigram count C(w~) and the total bigram count C(w~, wE) can be obtained by summing the counts over all sentences in the corpus." ></td>
	<td class="line x" title="129:300	c(,,o) = (16) i c(wo, = (17) i The estimate of the unigram probability and the bigram probability can be obtained as the relative frequency of the associated events." ></td>
	<td class="line x" title="130:300	c(wo) (is) f(w~) -'w C(wo, (19) f(wfllwc')-C(w~) If necessary, we can reestimate the word N-gram probabilities by replacing P(w~) and P(w~lw,~ ) with f(w~) and f(wolw~)." ></td>
	<td class="line x" title="131:300	Extraction of New Words in Texts Expected word unigram counts (expected word frequencies) in the corpus (Equation (16)) can be used as a measure of likelihood that a particular substring in the input texts is actually a word." ></td>
	<td class="line x" title="132:300	Let 0 denote the minimum expected word frequency that we use to classify a given word hypothesis w~ as a word." ></td>
	<td class="line x" title="133:300	C(w)." ></td>
	<td class="line x" title="134:300	> o (20) Those words that are not found in the dictionary and whose expected frequencies in the corpus are larger than the threshold O are extracted as the new words in the input texts." ></td>
	<td class="line x" title="135:300	In theory, expected word N-gram counts can be obtained by the generalized Forward-Backward algorithm." ></td>
	<td class="line x" title="136:300	In order to save computation time, however, we approximated the weighted sum of the word N-gram counts over all the word segmentation hypotheses in a sentence (Equation (14)), by that of the N-best word segmentation hypotheses 2." ></td>
	<td class="line x" title="137:300	1Note that the (Generalized) Forward-Backward algorithm is devised to compute these expected word N-gram count without listing all word segmentation hypotheses." ></td>
	<td class="line x" title="138:300	2If we only use the best word segmentation, it is called the Vitcrbi reestimation." ></td>
	<td class="line x" title="139:300	Our method might be called N-best reestimation." ></td>
	<td class="line x" title="140:300	It is designed to be more accurate than the Viterbi rcestimation and more efficient than the generafized Forward-Backward algorithm." ></td>
	<td class="line x" title="141:300	52 0.2 \[~i -~ 0.1 ~ ~ Figure 5: An example of computing the expected word frequencies N-best word segmentation hypotheses can be obtained by using the Forward-DP Backward-A* algorithm (Nagata, 1994)." ></td>
	<td class="line x" title="142:300	It consists of a forward dynamic programming search to record tlle probabilities of all partial word segmentation hypotheses, and a backward A* algorithm to extract the N-best hypotheses." ></td>
	<td class="line x" title="143:300	It is a generalization of the tree-trellis search (Soong and Huang, 1991), in the sense that its forward Viterbi search is replaced with the generalized Viterbi search described in this paper." ></td>
	<td class="line x" title="144:300	In reestimating the word N-gram probabilities, we introduce two modifications to the normal reestimation procedure." ></td>
	<td class="line x" title="145:300	The first modification is that, instead of using the relative frequency in an unsegmented corpus (Equation (18) and (19)), we combine the N-gram count in the segmented corpus with the estimated N-gram count in the unsegmented corpus to increase estimate reliability." ></td>
	<td class="line x" title="146:300	This is because a fairly large amount of segmented Japanese corpus were available in our experiments." ></td>
	<td class="line x" title="147:300	c,~(w~) + c ,o~(w~) (2~) f(w,) = ~~ Cseo(wc~ ) + ~, C  o(wc,) f(~,lw~) = c~~(w~,w,) + c  A w. ,w,)c~~(w~) + ~2-) (22) where C,~a(." ></td>
	<td class="line x" title="148:300	) denotes the count in the segmented corpus, and Cuns,a(') denotes the estimated count in tile unsegmented corpus." ></td>
	<td class="line x" title="149:300	The second modification is that we prune the expected N-gram counts in the unsegmented corpus if they are lower than a predefined threshold, before computing Equation (21) and (22)." ></td>
	<td class="line x" title="150:300	This is because Cunse#(') is unreliable, especially when C%,,,a(." ></td>
	<td class="line x" title="151:300	) is low." ></td>
	<td class="line x" title="152:300	Examples of Estimating Expected Word Frequencies Finally, we show a simple example of estimating the word N-gram counts in an unsegmented sentence." ></td>
	<td class="line x" title="153:300	Assume that the ith input sentence is the character sequence ~-~-~-)kPq, which means 'introduction to linguistics', and its best three word segmentation hypotheses are as shown in Figure 5." ></td>
	<td class="line x" title="154:300	The leftmost nmnbers in Figure 5 are the relative probabilities of the word segmentation P(O)) hypotheses, corresponding to ~ p(oD ill Equation (14)." ></td>
	<td class="line x" title="155:300	The expected word unigram count of each word hypothesis in the sentence is, C~(.z.Pq) = 0.7 + 0.2 + 0.1 = 1.0 o,n-~-) ---0.7 c'(~-~) = c~(~:) = 0.2 c~(~ -) = c~(~) = 0.1 The expected total number of tile words in tile sentence ~ Ci(w~) is 2.3." ></td>
	<td class="line x" title="156:300	If all word hypotheses are not registered in tile dictionary and the threshold 0 is 0.15, we regard )kPq ('introduction'),,~-~liq: ('linguistics'), ~ ('language'), and q: ('study') as tile new words." ></td>
	<td class="line x" title="157:300	~' ('say') and ~/iq: ('study of languages') are discarded." ></td>
	<td class="line x" title="158:300	Let us give another example that shows the effect of summing tlle expected word unigram counts over all the sentences in the corpus." ></td>
	<td class="line x" title="159:300	Suppose tile sentence '-'-~ 5/J~,~7~q:~: ENIAC  50 J~l~ 5o ', which means 'University of Pennsylvania celebrates the 50th anniversary of ENIAC.', is in the corpus, and the first three word segmentation hypotheses are as shown in Figure 1." ></td>
	<td class="line x" title="160:300	The expected word unigram counts for ~/'~A-,~= 7' ('Pennsylvania'),,<2 7~ ('Vania University'), and \]<~7' ('Vania') are 0.790, 0.169, and 0.041, respectively." ></td>
	<td class="line x" title="161:300	Suppose also the sentence 'zh~4' b\]~gc~2:~'.-~/5/A~'<=7~ 9 ~5~ ~b 7~o ', which means 'White House lies at Pennsylvania Avenue.', is in the corpus, and the expected word unigram counts for -~-:/~/z~,<: 7' ('Pennsylvania'), .'<-:7'~ V ('Vania Avenue'), and J<~7 ('Vania') are 0.825, 0.127, and 0.048, respectively." ></td>
	<td class="line x" title="162:300	The expected word unigram counts in the corpus are, C(-'-~/~/~,<~7) = 0.790 + 0.825 = 1.615 C(,<=7~) = 0.169 C(,<~-7~9) = 0.127 C(,<~7) = 0.041+0.048 = 0.089 Therefore,-'<>'5/z11~,<=7 is definitely more likely to be a new word." ></td>
	<td class="line x" title="163:300	Tile more often the unknown word appears in the corpus, the more it is likely to be extracted, even if there is word segmentation ambiguity in each sentence." ></td>
	<td class="line x" title="164:300	Experiments Language Data We used the EDR .Japanese Corpus Version 1.0 (EDR, 1995) to train and test the word segmen53 tation program." ></td>
	<td class="line x" title="165:300	It is a corpus of approximately 5 million words (200,000 sentences)." ></td>
	<td class="line x" title="166:300	It was collected to build a Japanese Electronic Dictionary, and contains a variety of Japanese sentences taken from newspapers, magazines, dictionaries, encyclopedias, textbooks, etc. It has a variety of annotations on morphology, syntax, and semantics." ></td>
	<td class="line x" title="167:300	We used word segmentation, pronunciation, and part of speech in the morphology information field of the annotation." ></td>
	<td class="line x" title="168:300	In this experiment, we randomly selected 90% of the sentences in the EDR Corpus for training the word segmentation program." ></td>
	<td class="line x" title="169:300	We made two test sets from the rest of the corpus, one for a small size experiment (100 sentences) and the other for a medium size experiment (1000 sentences)." ></td>
	<td class="line x" title="170:300	Table 1 shows the number of sentences, words, and characters for training and test sets." ></td>
	<td class="line x" title="171:300	Note that the test sets were not part of the training set." ></td>
	<td class="line x" title="172:300	That is, open data were tested in the experiment." ></td>
	<td class="line x" title="173:300	Table 1: The amount of training and test data training test-1 test-2 Sentences 192802 100 1000 Words 4746461 2463 25177 Characters 7521293 3912 39875 The training texts contained 133281 word types." ></td>
	<td class="line x" title="174:300	We discarded word types that appeared only once in the training texts." ></td>
	<td class="line x" title="175:300	This resulted in 65152 word types being registered in the dictionary of the word segmenter." ></td>
	<td class="line x" title="176:300	We trained three segmentation models, namely, part of speech trigram, word unigram, and word trigram, after we replaced those words appeared only once in the training texts with the unknown word tag <UNK>, as described in the section of word model." ></td>
	<td class="line x" title="177:300	After this replacement, there were 758172 distinct word bigrams." ></td>
	<td class="line x" title="178:300	Again, we discarded word bigrams that appeared only once in the training texts for saving main memory, and used the remaining 294668 word bigrams." ></td>
	<td class="line x" title="179:300	The word bigram probabilities were smoothed using deleted interpolation (Jelinek, 1985)." ></td>
	<td class="line x" title="180:300	The training texts contained 3534 character types." ></td>
	<td class="line x" title="181:300	We discarded characters that appeared only once in the training texts; 3167 character types remained." ></td>
	<td class="line x" title="182:300	We then replaced the discarded characters with the unknown character tag to train the word spelling model." ></td>
	<td class="line x" title="183:300	There were 91198 distinct character bigrams in the words in the training texts 3 aThere are more than 3000 (some say nlore than 10000) charters in Japanese, and their frequency distribution is skewed." ></td>
	<td class="line x" title="184:300	In order to save memory, we used a type of character bigram model that considers unWe made two spelling models." ></td>
	<td class="line x" title="185:300	The first was trained using all words in the training texts, while the second was trained using those words whose frequency is less than or equal to 2." ></td>
	<td class="line x" title="186:300	In principle, the spelling model of unknown words must be trained using the low frequency words." ></td>
	<td class="line x" title="187:300	However, it nlight suffer from the sparse data problem because the total number of word tokens for training is decreased from 4746461 to 103919." ></td>
	<td class="line x" title="188:300	We also made two length models." ></td>
	<td class="line x" title="189:300	The average word lengths of all words and that of low frequency words were 1.58 and 4.49, respectively." ></td>
	<td class="line x" title="190:300	Note that the average word length is the only parameter of the word length model." ></td>
	<td class="line x" title="191:300	Evaluation Measures Word Segmentation accuracy is expressed in ternrs of recall and precision." ></td>
	<td class="line x" title="192:300	First, we count the number of words in corpus segmentation (Std), the number of words in system segmentation (Sys), and tile number of matching word segmentations (M)." ></td>
	<td class="line x" title="193:300	Recailis defined as M/Std, and precision is defined as M/Sys." ></td>
	<td class="line x" title="194:300	Figure 6 shows an example of computing precision and recall for the sentence 'ta ~ ~ 7 ~ 2-~c~J~-~fi~'~'~'% ', which means 'Rockefeller Laboratory is an academic laboratory founded by an American millionaire, Rockefeller'." ></td>
	<td class="line x" title="195:300	Because of the difference in the segmentation of ~ ~ ~ 7 z: ~--iT~p~, the number of words in corpus segmentation (Std=15) differs from that of system segmentation (Sys=14)." ></td>
	<td class="line x" title="196:300	Note that the system correctly tokenized -~fbJ~E~, although it is not registered in the dictionary." ></td>
	<td class="line x" title="197:300	New word extraction accuracy is described in terms of recall, precision, and F-measure." ></td>
	<td class="line x" title="198:300	First, we count the number of unknown words in the corpus segmentation (Std), the number of unknown words in the system segmentation (Sys), and the number of matching words (M)." ></td>
	<td class="line x" title="199:300	Here, unknown words are those that are not registered in the system dictionary." ></td>
	<td class="line x" title="200:300	Recall is defined as M/Std, and precision is defined as M/Sys." ></td>
	<td class="line x" title="201:300	Since recall and precision greatly depend on the frequency threshold, we used the F-measure to indicate the overall performance." ></td>
	<td class="line x" title="202:300	F-measure is used in Information Retrieval, and is calculated by F= (/32+l.O) xPxR /32 x P+R (23) where P is precision, R is recall, and/3 is the relative importance given to recall over precision." ></td>
	<td class="line x" title="203:300	known characters, like the word bigram model used in the segmentation model." ></td>
	<td class="line x" title="204:300	54 I JC00092627 corpus segmentation ~,~ I/x i ~j~ ~ ~ ~ ~ / ~.~ ~J 2 / ~ ~ @/ / / ~j~i~l '~'~/7~',2 / ~,~i~\] 7Y: I ~' I ~j~J b / ~/~)$ vg-I .~.x I IJJ~J~l system segmentation I ~7~9--/~7~ > ~/~=~-~ /-~ 11:: //~ / ~l )/) / ~Jj~a\] ~/7~'~ / ~ p~1-~'Y~J'~Ii~,~ L. I +' I ~f)=$ re_ I J' I ~JJ1D~.~3 I --~--t$f@~/~IIL/<UNK> o /o /~ Rockefeller laboratory particle (topic) America of big rich man Rockefeller particle (subject) found inflectional suffix auxiliary verb (past) academic laboratory be (period) sys=lS, std=14, matched=13 precision=87.7 (13/18), recall=92.9 (13/14) Figure 6: Comparison between the corpus segmentation (left) and the system segmentation (right)." ></td>
	<td class="line x" title="205:300	words are listed in UNIX sdiff style." ></td>
	<td class="line x" title="206:300	All Word Segmentation Accuracy In order to decide the best configuration of the underlying Japanese word segmenter, we compared three segmentatio n models: part of speech trigram, word unigram, and word bigram." ></td>
	<td class="line x" title="207:300	We also compared three word models: all words, low frequency words, and the combination of the two." ></td>
	<td class="line x" title="208:300	The third word model consisted of the spelling model trained using all words and the length model trained using low frequency words." ></td>
	<td class="line x" title="209:300	Table 2 shows, for the small test set (100 sentences), the segmentation accuracy of the various combinations of the segmentation models and the word models." ></td>
	<td class="line x" title="210:300	It is obvious that word bigram outperformed the part of speech trigram as well as word unigram." ></td>
	<td class="line x" title="211:300	As for the word model, it seems the combination of the spelling model for all words and the length model for low frequency words is the best, but the difference is small." ></td>
	<td class="line x" title="212:300	In the following experiment, we decided to use word bigram as the segmentation model, and the combination of the spelling model of all words and the length model of low frequency words as the word model." ></td>
	<td class="line x" title="213:300	New Word Extraction Accuracy We tested the new word extraction method using the medium size test set (1000 sentences)." ></td>
	<td class="line x" title="214:300	It contains 538 unknown word types." ></td>
	<td class="line x" title="215:300	8 word types appeared twice in the test set." ></td>
	<td class="line x" title="216:300	The other 530 word types appeared only once." ></td>
	<td class="line x" title="217:300	The out-of-vocabulary rate of the test set is 2.2%." ></td>
	<td class="line x" title="218:300	To count the expected word frequencies, we used the top-10 word segmentation hypotheses." ></td>
	<td class="line x" title="219:300	We limited tile maximum character length of the a unknown word to 8 in order to save computation time." ></td>
	<td class="line x" title="220:300	We tested three variations of the new word extraction method." ></td>
	<td class="line x" title="221:300	The first one was 'No Reestimarion'; it uses the word segmenter's outputs as they are when extracting new words." ></td>
	<td class="line x" title="222:300	The second and the third ones carry out reestimation before extraction, where the pruning thresholds of the expected N-gram counts in the reestimation are 0.95 and 0.50, respectively." ></td>
	<td class="line x" title="223:300	Reestimations were carried out three times." ></td>
	<td class="line x" title="224:300	Table 3 shows the new word extraction accuracies for a variety of expected word frequency thresholds 0, with and without reestimation." ></td>
	<td class="line x" title="225:300	In Table 3, we set fl = 1.0 to compute F-measure." ></td>
	<td class="line x" title="226:300	As Table 3 shows, the higher the threshold is, the higher the precision and the lower the recall become." ></td>
	<td class="line x" title="227:300	When we put equal importance on recall and precision, the best value for the expected word frequency threshold is around 0.10 where the recall is 43.7% and the precision is 52.3%." ></td>
	<td class="line x" title="228:300	Figure 7 shows excerpts of correctly extracted new words (matched), incorrectly extracted word hypotheses (sys-matched), and new words that were not extracted (std-matched), when the frequency threshold was 0.5 and reestimation was not carried out." ></td>
	<td class="line x" title="229:300	We find that the overall quality of the extracted word hypotheses is satisfactory, al55 Table 2: Language Models and Segmentation Accuracies (100 test sentences) POS trigram word unigram word bigram word model recall prec." ></td>
	<td class="line x" title="230:300	recall prec." ></td>
	<td class="line x" title="231:300	recall prec." ></td>
	<td class="line x" title="232:300	all words 91.6 88.8 88.7 87.3 94.6 : 89.4 low frequency words 91.5 89.5 88.4 88.0 94.3 90.1 all words + l.f.w, length 91.5 89.3 88.8 87.6 94.7 89.9 Table 3: New Word Extraction Accuracy (1000 test sentences) freq." ></td>
	<td class="line x" title="233:300	>0.00 >0.10 >0.50 >0.90 >0.95 >0.99 No Reestimation freq>0.95, 3 iter." ></td>
	<td class="line x" title="234:300	freq>0.50, 3 iter." ></td>
	<td class="line x" title="235:300	recall prec." ></td>
	<td class="line x" title="236:300	F recall prec." ></td>
	<td class="line x" title="237:300	F recall prec." ></td>
	<td class="line x" title="238:300	F 56.1 34.2 42.5 50.6 37.9 43.4 39.6 56.7 46.6 43.7 52.3 47.6 43.1 52.1 47.2 37.9 63.6 47.5 36.4 65.6 46.8 36.1 65.8 46.6 36.6 65.2 46.9 25.3 76.8 35.8 25.3 77.3 38.1 36.6 65.2 46.9 23.2 78.1 35.8 23.4 78.3 36.1 36.6 65.2 46.9 17.3 81.6 28.5 23.4 78.3 36.1 36.6 65.2 46.9 though the values of recall and precision are not so high." ></td>
	<td class="line x" title="239:300	We discuss the reason for this in the next section." ></td>
	<td class="line x" title="240:300	Discussion The problem of Japanese word segmentation is that people often can not agree on a single word segmentation." ></td>
	<td class="line x" title="241:300	Therefore, the reported performance could be greatly underestimated." ></td>
	<td class="line x" title="242:300	Most of the new words extracted by the system are acceptable as a word (at least for us), and nmy not necessarily be a wrong word entry." ></td>
	<td class="line x" title="243:300	On the other hand, most of the new words not extracted by the system can be divided into shorter words that are registered in the dictionary." ></td>
	<td class="line x" title="244:300	For example, in the first sentence of Figure 8, W'~/~'  ~ :2 ~-~--5/~ .-/('data coinmunication') is regarded as one word in corpus segmentation and counted as an unknown word in the test sentence." ></td>
	<td class="line x" title="245:300	However, the system segmented it into -U--~ ('data') and = :2:~0--5/~ Z/('communication'), both of which are found in the dictionary." ></td>
	<td class="line x" title="246:300	In the second sentence of Figure 8, the system extracted .',3-~'7/c~ ('Duke of Hanover') as a new word, while this word is divided into ~',/--~'~ ('Hanover') and ~ ('Duke') in corpus segmentation." ></td>
	<td class="line x" title="247:300	Most of extraction errors are of this category." ></td>
	<td class="line x" title="248:300	There are three types of obvious extraction errors." ></td>
	<td class="line x" title="249:300	The first type is the truncation of long words." ></td>
	<td class="line x" title="250:300	Some transliterated Western-origin words exceed the predefined maximum length for unknown word." ></td>
	<td class="line x" title="251:300	The third sentence of Figure 8 is an example of this type." ></td>
	<td class="line x" title="252:300	In Japanese, 'illustration' is transliterated into 9 characters ~ ~ 7, b 1/--5/ :/, which exceeds tile maximum unknown word length of 8 characters in our system." ></td>
	<td class="line x" title="253:300	Since 4' ~ 1(the transliteration of 'illust', which also means illustration in Japanese) is registered in the dictionary, t/--5/~ ./(the transliteration of 'ration') is incorrectly extracted as a new word." ></td>
	<td class="line x" title="254:300	The second type is the fragmentation of numerals." ></td>
	<td class="line x" title="255:300	Since we did not use any tokenizers, numerals tend to be divided arbitrarily." ></td>
	<td class="line x" title="256:300	In the second sentence in Figure 8, the system divided '1676' into '16' and '76'." ></td>
	<td class="line x" title="257:300	In fact, it may output '1' and '676', '16  7' and '6', or whatever." ></td>
	<td class="line x" title="258:300	The third type is the concatenation of noun(s) and particle." ></td>
	<td class="line x" title="259:300	In other words, the system sometimes erroneously recognizes a noun phrase as a word." ></td>
	<td class="line x" title="260:300	For example, the Japanese counterparts of 'A of B', 'A and B', and 'A, B' are recognized as a word." ></td>
	<td class="line x" title="261:300	This may be because the probability of one long unknown word can be higher than the product of the probabilities of two short unknown (or infrequent) words and one known word." ></td>
	<td class="line x" title="262:300	The fourth sentence of Figure 8 is an example of this type of error." ></td>
	<td class="line x" title="263:300	The system considered ~li~l\]li~lh~-'v ~l~tlJ ('controllable and observable') as a word, while it is divided into ~-I ('able'), fi~Jt~ ('control'), ~-o ('and'), ~f ('able'), and ~tJ ('observe') in the corpus." ></td>
	<td class="line x" title="264:300	As for reestimation, Table 3 shows no significant improvements in the new word extraction accuracy." ></td>
	<td class="line x" title="265:300	The only effect of reestimation, in our experiment, is to increase the expected word frequencies of the unknown word hypotheses whose expected word frequencies are greater than the pruning threshold of reestimation." ></td>
	<td class="line x" title="266:300	This result does not necessarily mean that reestimation is useless." ></td>
	<td class="line x" title="267:300	This is because most tin56 I mat ched=196 3~1487 b~t,~2000 ~-P~ ~A'~,~ F,A.~--~ b~l,y~ 7~)--b'~p,~ I/b~J~,~ ays-ma~ched=103 90Zf7 000R STK~ m~Y~$J= mix b~lyS; -~--\]1.,~ 77'F-T~7'{~ 7t~--~4~/' ~:~ std-mat ched=342 404 BBNTF.,<>'~ b' =:/~=~--~--~f X~2~ ~bo~SP~ ~-~,9y7~ t~-P~--~-~threshold=0.5 std=538, sys=299, matched=196 recall=36.4 (196/538), precision=65.6 (196/299) Figure 7: Excerpts of correctly extracted new words (matched), incorrectly extracted word hypotheses (sys-raatched), and not extracted new words (std-matched)." ></td>
	<td class="line x" title="270:300	known words appeared only once in the test sentences." ></td>
	<td class="line x" title="271:300	An ideal example to confirm that reestimarion works well would have an unknown word appearing more than twice in the test sentences, and it is trivial to extract the word in one appearance, while it is difficult in the others, because of, for example, successive unknown words." ></td>
	<td class="line x" title="272:300	If the test set were larger, or the out-of-vocabulary rate were higher, we believe that the effectiveness of reestimation would be more clearly shown." ></td>
	<td class="line x" title="273:300	Related Work Recent years have seen several works on corpusbased word segmentation and dictionary construction for both Japanese and Chinese." ></td>
	<td class="line x" title="274:300	For Chinese, (Sproat et al. , 1994) used the word unigram model in their word segmenter based on weighted finite-state transducer." ></td>
	<td class="line x" title="275:300	Word frequencies were estimated by the Viterbi reestimation (a reesthnation procedure using the best analysis) from an unsegmented corpus of 20 million words." ></td>
	<td class="line x" title="276:300	Initial estimates of the word frequencies were derived from the frequencies in the corpus of the strings of hanzi making up each word in the lexicon whether or not each string is actually an instance of the word in question." ></td>
	<td class="line x" title="277:300	(Chang et al. , 1995) proposed an automatic dictionary construction method for Chinese from a large unsegmented corpus (311591 sentences) with the help of a small segmented seed corpus (1000 sentences)." ></td>
	<td class="line x" title="278:300	They combined Viterbi reestimation using the word unigram model with a post filter called the 'Two-Class Classifier', which is a linear discrimination function to decide whether the string is actually a word or not based on features derived from the character N-gram in a large unsegmented corpus." ></td>
	<td class="line x" title="279:300	The system's performance is compared with a word list derived from two online Chinese dictionaries (21141 words)." ></td>
	<td class="line x" title="280:300	Tile reported recall and precision values were 56.88% and 77.37% for two character words, and 6.12% and 85.97% for three character words, respectively." ></td>
	<td class="line x" title="281:300	For Japanese, (Nagao and Mori, 1994) proposed a method of computing an arbitrary length character N-gram, and showed that the character N-gram statistics obtained from a large corpus includes information useful for word extraction." ></td>
	<td class="line x" title="282:300	However, they did not report any evaluation of their word extraction method." ></td>
	<td class="line x" title="283:300	(Teller and Batchelder, 1994) proposed a very naive probabilistic word segmentation method for Japanese, based on character type information and hiragana bigram frequencies." ></td>
	<td class="line x" title="284:300	They claimed 98% word segmentation accuracy, while we clMrn 94.7%." ></td>
	<td class="line x" title="285:300	However, their evaluation method is very optimistic, and completely different from ours." ></td>
	<td class="line x" title="286:300	They count an error only when the system segmentation violates morpheme boundaries." ></td>
	<td class="line x" title="287:300	In other words, they count an error only when the system segmentation is not acceptable to human judgemen% while we count an error whenever tim system segmentation does not exactly match the corpus segmentation, even if it is inconsistent." ></td>
	<td class="line x" title="288:300	We used the word bigram model for word segmentation, and expected word frequency for unknown word extraction." ></td>
	<td class="line x" title="289:300	We compared the results with a segmented Japanese corpus, and reported 43.7% recall and 52.3% precision for 1000 sentences whose out-of-vocabulary rate is 2.1%." ></td>
	<td class="line x" title="290:300	It is impossible to compare our results with (Chang et al. , 1995), because the experiment conditions are completely different in terms of language (Chinese vs. Japanese), the size of seed segmented corpus, the size of target unsegmented corpus and its out-of-vocabulary rate, the size of initial word list, and the type of reference data 57 (on-line dictionary vs. segmented corpus)." ></td>
	<td class="line x" title="291:300	Our idea of filtering erroneous word hypothesis by expected word frequency is simple and straightforward." ></td>
	<td class="line x" title="292:300	The major contribution of this paper is that we present a more accurate method for estimating word frequencies in an unsegmented corpus, even if it includes unknown words." ></td>
	<td class="line x" title="293:300	This is achieved by introducing an explicit statistical model of unknown words, and by using an Nbest word segmentation algorithm (Nagata, 1994) as an approximation of the generalized ForwardBackward algorithm." ></td>
	<td class="line x" title="294:300	In English taggers, (Weischedel et al. , 1993) proposed a statistical model to estimate word output probability p(wi\]tl) for an unknown word from spelling information such as inflectional endings, derivational endings, hyphenation, and capitalization." ></td>
	<td class="line x" title="295:300	Our word model can be thought of a generalization of their statistical model." ></td>
	<td class="line x" title="296:300	One potential benefit of our statistical model and segmentation algorithm is that they are completely independent of the target language and its writing system." ></td>
	<td class="line x" title="297:300	We intend to test our word segmentation method on other languages, such as Chinese and Thai." ></td>
	<td class="line x" title="298:300	Conclusion We present a new word extraction method for Japanese based on expected word frequency, which is computed by using a statistical language model and an N-best word segmentation algorithm." ></td>
	<td class="line x" title="299:300	Although we have encouraging initial results, there are a number of questions to be answered, for example, the minimmn seed segmented corpus size required, the minimum initial word list required, the effect of reestimation for a large unsegmented corpus with various out-of-vocabulary rates." ></td>
	<td class="line x" title="300:300	Besides these questions, we are also thinking of assigning the part of speech to the extracted new words in order to construct a Japanese dictionary automatically." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W96-0206
Better Language Models With Model Merging
Brants, Thorsten;"></td>
	<td class="line x" title="1:234	Better Language Models with Model Merging Thorsten Brants Universitgt des Saarlandes, Computational Linguistics P.O.Box 151150, D-66041 Saarbriicken, Germany thorst en@coli, unisb." ></td>
	<td class="line x" title="2:234	de Abstract This paper investigates model merging, a technique for deriving Markov models from text or speech corpora." ></td>
	<td class="line x" title="3:234	Models are derived by starting with a large and specific model and by successively combining states to build smaller and more general models." ></td>
	<td class="line x" title="4:234	We present methods to reduce the time complexity of the algorithm and report on experiments on deriving language models for a speech recognition task." ></td>
	<td class="line x" title="5:234	The experiments show the advantage of model merging over the standard bigram approach." ></td>
	<td class="line x" title="6:234	The merged model assigns a lower perplexity to the test set and uses considerably fewer states." ></td>
	<td class="line x" title="7:234	Introduction Hidden Markov Models are commonly used for statistical language models, e.g. in part-of-speech tagging and speech recognition (Rabiner, 1989)." ></td>
	<td class="line x" title="8:234	The models need a large set of parameters which are induced from a (text-) corpus." ></td>
	<td class="line x" title="9:234	The parameters should be optimal in the sense that the resulting models assign high probabilities to seen training data as well as new data that arises in an application." ></td>
	<td class="line x" title="10:234	There are several methods to estimate model parameters." ></td>
	<td class="line x" title="11:234	The first one is to use each word (type) as a state and estimate the transition probabilities between two or three words by using the relative frequencies of a corpus." ></td>
	<td class="line x" title="12:234	This method is commonly used in speech recognition and known as word-bigram or word-trigram model." ></td>
	<td class="line x" title="13:234	The relative frequencies have to be smoothed to handle the sparse data problem and to avoid zero probabilities." ></td>
	<td class="line x" title="14:234	The second method is a variation of the first method." ></td>
	<td class="line x" title="15:234	Words are automatically grouped, e.g. by similarity of distribution in the corpus (Pereira et al. , 1993)." ></td>
	<td class="line x" title="16:234	The relative frequencies of pairs or triples of groups (categories, clusters) are used as model parameters, each group is represented by a state in the model." ></td>
	<td class="line x" title="17:234	The second method has the advantage of drastically reducing the number of model parameters and thereby reducing the sparse data problem; there is more data per group than per word, thus estimates are more precise." ></td>
	<td class="line x" title="18:234	The third method uses manually defined categories." ></td>
	<td class="line x" title="19:234	They are linguistically motivated and usually called parts-of-speech." ></td>
	<td class="line x" title="20:234	An important difference to the second method with automatically derived categories is that with the manual definition a word can belong to more than one category." ></td>
	<td class="line x" title="21:234	A corpus is (manually) tagged with the categories and transition probabilities between two or three categories are estimated from their relative frequencies." ></td>
	<td class="line x" title="22:234	This method is commonly used for part-of-speech tagging (Church, 1988)." ></td>
	<td class="line x" title="23:234	The fourth method is a variation of the third method and is also used for part-of-speech tagging." ></td>
	<td class="line x" title="24:234	This method does not need a pre-annotated corpus for parameter estimation." ></td>
	<td class="line x" title="25:234	Instead it uses a lexicon stating the possible parts-of-speech for each word, a raw text corpus, and an initial bias for the transition and output probabilities." ></td>
	<td class="line x" title="26:234	The parameters are estimated by using the Baum-Welch algorithm (Baum et al. , 1970)." ></td>
	<td class="line oc" title="27:234	The accuracy of the derived model depends heavily on the initial bias, but with a good choice results are comparable to those of method three (Cutting et al. , 1992)." ></td>
	<td class="line x" title="28:234	This paper investigates a fifth method for estimating natural language models, combining the advantages of the methods mentioned above." ></td>
	<td class="line x" title="29:234	It is suitable for both speech recognition and partof-speech tagging, has the advantage of automatically deriving word categories from a corpus and is capable of recognizing the fact that a word belongs to more than one category." ></td>
	<td class="line x" title="30:234	Unlike other techniques it not only induces transition and output probabilities, but also the model topology, i.e., the number of states, and for each state the outputs that have a non-zero probability." ></td>
	<td class="line x" title="31:234	The method is called model merging and was introduced by (Omohundro, 1992)." ></td>
	<td class="line x" title="32:234	The rest of the paper is structured as follows." ></td>
	<td class="line x" title="33:234	We first give a short introduction to Markov mo60 dels and present the model merging technique." ></td>
	<td class="line x" title="34:234	Then, techniques for reducing the time complexity are presented and we report two experiments using these techniques." ></td>
	<td class="line x" title="35:234	Markov Models A discrete output, first order Markov Model consists of  a finite set of states QUqs, qe}, q~, qe ~ Q, with q~ the start state, and q~ the end state;  a finite output alphabet ~;  a (IQ\] + 1)  (IQ\] + 1) matrix, specifying the probabilities of state transitions p(q'iq) between states q and q~ (there are no transitions into q~, and no transitions originating in qe); for each state q E Q U {qs}, the sum of the outgoing transition probabilities is 1, ~ p(q'\]q) = qlEQUqe} 1;  a \]Q\]  \[~l matrix, specifying the output probabilities p(a\]q) of state q emitting output o'; for each state q E Q, the sum of the output probabilities is 1, ~ p(cr\]q) = 1." ></td>
	<td class="line x" title="36:234	aE~ A Markov model starts running in the start state q~, makes a transition at each time step, and stops when reaching the end state qe." ></td>
	<td class="line x" title="37:234	The transition from one state to another is done according to the probabilities specified with the transitions." ></td>
	<td class="line x" title="38:234	Each time a state is entered (except the start and end state) one of the outputs is chosen (again according to their probabilities) and emitted." ></td>
	<td class="line x" title="39:234	Assigning Probabilities to Data For the rest of the paper, we are interested in the probabilities which are assigned to sequences of outputs by the Markov models." ></td>
	<td class="line x" title="40:234	These can be calculated in the following way." ></td>
	<td class="line x" title="41:234	Given a model M, a sequence of outputs o = o1 o'k and a sequence of states Q = ql." ></td>
	<td class="line x" title="42:234	  qk (of same length), the probability that the model running through the sequence of states and emitting the given outputs is (/I PM(Q, o') = PM(qilqi-1)PM(o'ilqi PM(qelqi) \i=1 (with q0 = qs)." ></td>
	<td class="line x" title="43:234	A sequence of outputs can be emitted by more than one sequence of states, thus we have to sum over all sequences of states with the given length to get the probability that a model emits a given sequence of outputs: PM(O') = ~ PM(Q, o')." ></td>
	<td class="line x" title="44:234	Q The probabilities are calculated very efficiently with the Viterbi algorithm (Viterbi, 1967)." ></td>
	<td class="line x" title="45:234	Its time complexity is linear to the sequence length despite the exponential growth of the search space." ></td>
	<td class="line x" title="46:234	Perplexity Markov models assign rapidly decreasing probabilities to output sequences of increasing length." ></td>
	<td class="line x" title="47:234	To compensate for different lengths and to make their probabilities comparable, one uses the perplexity PP of an output sequence instead of its probability." ></td>
	<td class="line x" title="48:234	The perplexity is defined as 1 PPM(O')~v/fi ~ The probability is normalized by taking the k th root (k is the length of the sequence)." ></td>
	<td class="line x" title="49:234	Similarly, the log perplexity LP is defined: log PM (o') LPM((r) = log PPM(a) -k Here, the log probability is normalized by dividing by the length of the sequence." ></td>
	<td class="line x" title="50:234	PP and LP are defined such that higher perplexities (log perplexities, resp)." ></td>
	<td class="line x" title="51:234	correspond to lower probabilities, and vice versa." ></td>
	<td class="line x" title="52:234	These measures are used to determine the quality of Markov models." ></td>
	<td class="line x" title="53:234	The lower the perplexity (and log perplexity) of a test sequence, the higher its probability, and thus the better it is predicted by the model." ></td>
	<td class="line x" title="54:234	Model Merging Model merging is a technique for inducing model parameters for Markov models from a text corpus." ></td>
	<td class="line x" title="55:234	It was introduced in (Omohundro, 1992) and (Stolcke and Omohundro, 1994) to induce models for regular languages from a few samples, and adapted to natural language models in (Brants, 1995)." ></td>
	<td class="line x" title="56:234	Unlike other techniques it not only induces transition and output probabilities from the corpus, but also the model topology, i.e., the number of states and for each state the outputs that have non-zero probability." ></td>
	<td class="line x" title="57:234	In n-gram approaches the topology is fixed." ></td>
	<td class="line x" title="58:234	E.g., in a pos-n-gram model, the states are mostly syntactically motivated, each state represents a syntactic category and only words belonging to the same category have a non-zero output probability in a particular state." ></td>
	<td class="line x" title="59:234	However the n-gram-models make the implicit assumption that all words belonging to the same category have a similar distribution in a corpus." ></td>
	<td class="line x" title="60:234	This is not true in most of the cases." ></td>
	<td class="line x" title="61:234	By estimating the topology, model merging groups words into categories, since all words that can be emitted by the same state form a category." ></td>
	<td class="line x" title="62:234	The advantage of model merging in this respect 61 a) a b o~ @ '@ c @ .@ .@ p(SlM~) = ~ ~ 3.7.10 -2 D .@ b) b 5 .@ p(SIMb) = ~ --~ 3.7.10 -2 ) @ p(SIM~) = ~ -~ 3.7.10 -2 C, ~ 0.67, ;@ 0.5 d) @ @ 05 .@~ D 0.5 p(SIMd ) = ~ ~_ 1.6-10 -2 ''--@ . e) p(SiM~ ) = 2~ ~ 6.6.10 -3 4096 -b C o ~,0y Figure 1: Model merging for a corpus S = {ab, ac, abac}, starting with the trivial model in a) and ending with the generalization (a(blc)) + in e)." ></td>
	<td class="line x" title="63:234	Several steps of merging between model b) and c) are not shown." ></td>
	<td class="line x" title="64:234	Unmarked transitions and outputs have probability 1." ></td>
	<td class="line x" title="65:234	62 is that it can recognize that a word (the type) belongs to more than one category, while each occurrence (the token) is assigned a unique category." ></td>
	<td class="line x" title="66:234	This naturally reflects manual syntactic categorizations, where a word can belong to several syntactic classes but each occurrence of a word is unambiguous." ></td>
	<td class="line x" title="67:234	The Algorithm Model merging induces Markov models in the following way." ></td>
	<td class="line x" title="68:234	Merging starts with an initial, very general model." ></td>
	<td class="line x" title="69:234	For this purpose, the maximum likelihood Markov model is chosen, i.e., a model that exactly matches the corpus." ></td>
	<td class="line x" title="70:234	There is one path for each utterance in the corpus and each path is used by one utterance only." ></td>
	<td class="line x" title="71:234	Each path gets the same probability l/u, with u the number of utterances in the corpus." ></td>
	<td class="line x" title="72:234	This model is also referred to as the trivial model." ></td>
	<td class="line x" title="73:234	Figure 1.a shows the trivial model for a corpus with words a, b, c and utterances ab, ac, abac." ></td>
	<td class="line x" title="74:234	It has one path for each of the three utterances ab, ac, and abac, and each path gets the same probability 1/3." ></td>
	<td class="line x" title="75:234	The trivial model assigns a probability of p(SIM~ ) = 1/27 to the corpus." ></td>
	<td class="line x" title="76:234	Since the model makes an implicit independence assumption between the utterances, the corpus probability is calculated by multiplying the utterance's probabilities, yielding 1/3.1/3.1/3 = 1/27." ></td>
	<td class="line x" title="77:234	Now states are merged successively, except for the start and end state." ></td>
	<td class="line x" title="78:234	Two states are selected and removed and a new merged state is added." ></td>
	<td class="line x" title="79:234	The transitions from and to the old states are redirected to the new state, the transition probabilities are adjusted to maximize the likelihood of the corpus; the outputs are joined and their probabilities are also adjusted to maximize the likelihood." ></td>
	<td class="line x" title="80:234	One step of merging can be seen in figure 1.b. States 1 and 3 are removed, a combined state 1,3 is added, and the probabilities are adjusted." ></td>
	<td class="line x" title="81:234	The criterion for selecting states to merge is the probability of the Markov model generating the corpus." ></td>
	<td class="line x" title="82:234	We want this probability to stay as high as possible." ></td>
	<td class="line x" title="83:234	Of all possible merges (generally, there are k(k 1)/2 possible merges, with k the number of states exclusive start and end state which are not allowed to merge) we take the merge that results in the minimal change of the probability." ></td>
	<td class="line x" title="84:234	For the trivial model and u pairwise different utterances the probability is p(SIMtri~) = 1/u ~." ></td>
	<td class="line x" title="85:234	The probability either stays constant, as in Figure 1.b and c, or decreases, as in 1.d and e. The probability never increases because the trivial model is the maximum likelihood model, i.e., it maximizes the probability of the corpus given the model." ></td>
	<td class="line x" title="86:234	Model merging stops when a predefined threshold for the corpus probability is reached." ></td>
	<td class="line x" title="87:234	Some statistically motivated criteria for termination using model priors are discussed in (Stotcke and Omohundro, 1994)." ></td>
	<td class="line x" title="88:234	Using Model Merging The model merging algorithm needs several optimizations to be applicable to large natural language corpora, otherwise the amount of time needed for deriving the models is too large." ></td>
	<td class="line x" title="89:234	Generally, there are O(l 2) hypothetical merges to be tested for each merging step (l is the length of the training corpus)." ></td>
	<td class="line x" title="90:234	The probability of the training corpus has to be calculated for each hypothetical merge, which is O(l) with dynamic programming." ></td>
	<td class="line x" title="91:234	Thus, each step of merging is O(13)." ></td>
	<td class="line x" title="92:234	If we want to reduce the model from size l 42 (the trivial modeli which consists of one state for each token plus initial and final states) to some fixed size, we need O(l) steps of merging." ></td>
	<td class="line x" title="93:234	Therefore, deriving a Markov model by model merging is O(l 4) in time." ></td>
	<td class="line x" title="94:234	(Stolcke and Omohundro, 1994) discuss several computational shortcuts and approximations: 1." ></td>
	<td class="line x" title="95:234	Immediate merging of identical initial and final states of different utterances." ></td>
	<td class="line x" title="96:234	These merges do not change the corpus probability and thus are the first merges anyway." ></td>
	<td class="line x" title="97:234	2." ></td>
	<td class="line x" title="98:234	Usage of the Viterbi path (best path) only instead of summing up all paths to determine the corpus probability." ></td>
	<td class="line x" title="99:234	3." ></td>
	<td class="line x" title="100:234	The assumption that all input samples retain their Viterbi path after merging." ></td>
	<td class="line x" title="101:234	Making this approximation, it is no longer necessary to reparse the whole corpus for each hypothetical merge." ></td>
	<td class="line x" title="102:234	We use two additional strategies to reduce the time complexity of the algorithm: a series of cascaded constraints on the merges and the variation of the starting point." ></td>
	<td class="line x" title="103:234	Constraints When applying model merging one can observe that first mainly states with the same output are merged." ></td>
	<td class="line x" title="104:234	After several steps of merging, it is no longer the same output but still mainly states that output words of the same syntactic category are merged." ></td>
	<td class="line x" title="105:234	This behavior can be exploited by introducing constraints on the merging process." ></td>
	<td class="line x" title="106:234	The constraints allow only some of the otherwise possible merges." ></td>
	<td class="line x" title="107:234	Only the allowed merges are tested for each step of merging." ></td>
	<td class="line x" title="108:234	We consider constraints that divide the states of the current model into equivalence classes." ></td>
	<td class="line x" title="109:234	Only states belonging to the same class are allowed to merge." ></td>
	<td class="line x" title="110:234	E.g., we can divide the states into classes 63 generating the same outputs." ></td>
	<td class="line x" title="111:234	If the current model has N states and we divide them into k > 1 nonempty equivalence classes C1  C~, then, instead of N(N 1)/2, we have to test k .\[C'l(ICl\]) < N(N 1) 2 2 i=1 merges only." ></td>
	<td class="line x" title="112:234	The best case for a model of size N is the division into N/2 classes of size 2." ></td>
	<td class="line x" title="113:234	Then, only N/2 merges must be tested to find the best merge." ></td>
	<td class="line x" title="114:234	The best division into k > 1 classes for some model of size N is the creation of classes that all have the same size N/k (or an approximation if N/k ~ IN)." ></td>
	<td class="line x" title="115:234	Then, N N N(~1) v(v-1) .k2 2 must be tested for each step of merging." ></td>
	<td class="line x" title="116:234	Thus, the introduction of these constraints does not reduce the order of the time complexity, but it can reduce the constant factor significantly (see section about experiments)." ></td>
	<td class="line x" title="117:234	The following equivalence classes can be used for constraints when using untagged corpora: 1." ></td>
	<td class="line x" title="118:234	States that generate the same outputs (unigram constraint) 2." ></td>
	<td class="line x" title="119:234	unigram constraint, and additionally all predecessor states must generate the same outputs (bigram constraint) 3." ></td>
	<td class="line x" title="120:234	trigrams or higher, if the corpora are large enough 4." ></td>
	<td class="line x" title="121:234	a variation of one: states that output words belonging to one ambiguity class, i.e. can be of a certain number of syntactic classes." ></td>
	<td class="line x" title="122:234	Merging starts with one of the constraints." ></td>
	<td class="line x" title="123:234	After a number of merges have been performed, the constraint is discarded and a weaker one is used instead." ></td>
	<td class="line x" title="124:234	The standard n-gram approaches are special cases of using model merging and constraints." ></td>
	<td class="line x" title="125:234	E.g., if we use the unigram constraint, and merge states until no further merge is possible under this constraint, the resulting model is a standard bigram model, regardless of the order in which the merges were performed." ></td>
	<td class="line x" title="126:234	In practice, a constraint will be discarded before no further merge is possible (otherwise the model could have been derived directly, e.g., by the standard n-gram technique)." ></td>
	<td class="line x" title="127:234	Yet, the question when to discard a constraint to achieve best results is unsolved." ></td>
	<td class="line x" title="128:234	The Starting Point The initial model of the original model merging procedure is the maximum likelihood or trivial model." ></td>
	<td class="line x" title="129:234	This model has the advantage of directly representing the corpus." ></td>
	<td class="line x" title="130:234	But its disadvantage is its huge number of states." ></td>
	<td class="line x" title="131:234	A lot of computation time can be saved by choosing an initial model with fewer states." ></td>
	<td class="line x" title="132:234	The initial model must have two properties: 1." ></td>
	<td class="line x" title="133:234	it must be larger than the intended model, and 2." ></td>
	<td class="line x" title="134:234	it must be easy to construct." ></td>
	<td class="line x" title="135:234	The trivial model has both properties." ></td>
	<td class="line x" title="136:234	A class of models that can serve as the initial model as well are n-gram models." ></td>
	<td class="line x" title="137:234	These models are smaller by one or more orders of magnitude than the trivial model and therefore could speed up the derivation of a model significantly." ></td>
	<td class="line x" title="138:234	This choice of a starting point excludes a lot of solutions which are allowed when starting with the maximum likelihood model." ></td>
	<td class="line x" title="139:234	Therefore, starting with an n-gram model yields a model that is at most equivalent to one that is generated when starting with the trivial model, and that can be much worse." ></td>
	<td class="line x" title="140:234	But it should be still better than any n-gram model that is of lower of equal order than the initial model." ></td>
	<td class="line x" title="141:234	Experiments Model Merging vs. Bigrams The first experiment compares model merging with a standard bigram model." ></td>
	<td class="line x" title="142:234	Both are trained on the same data." ></td>
	<td class="line x" title="143:234	We use Ntra~n -14,421 words of the Verbmobil corpus." ></td>
	<td class="line x" title="144:234	The corpus consists of transliterated dialogues on business appointments 1." ></td>
	<td class="line x" title="145:234	The models are tested on Ntest = 2,436 words of the same corpus." ></td>
	<td class="line x" title="146:234	Training and test parts are disjunct." ></td>
	<td class="line x" title="147:234	The bigram model yields a Markov model wit h 1,440 states." ></td>
	<td class="line x" title="148:234	It assigns a log perplexity of 1.20 to the training part and 2.40 to the test part." ></td>
	<td class="line x" title="149:234	Model merging starts with the maximum likelihood model for the training part." ></td>
	<td class="line x" title="150:234	It has 14,423 states, which correspond to the 14,421 words (plus an initial and a final state)." ></td>
	<td class="line x" title="151:234	The initial log perplexity of the training part is 0.12." ></td>
	<td class="line x" title="152:234	This low value shows that the initial model is very specialized in the training part." ></td>
	<td class="line x" title="153:234	1Many thanks to the Verbmobil project for providing these data." ></td>
	<td class="line x" title="154:234	We use dialogues that were recorded in 1993 and 94, and which are now available from the Bavarian Archive for Speech Signals BAS (http://www'phnetik'uni-muenchen'de/ Bas/BasHomeeng.html)." ></td>
	<td class="line x" title="155:234	64 log\]o P/Ntrain 2.52.0 1.5 1.0 0.5 0 1 I 14 lp dlp constraint, t chang~ I ' I ~ I ' i ' I ; I ' I ' I ' I ' I ' I ' I i I 2 3 4 5 6 7 8 9 10 11 12 13 14 10 3 merges I I I I I I i I I I I I I I 13 12 11 10 9 8 7 6 5 4 3 2 1 0 10 3 states Figure 2: Log Perplexity of the training part during merging." ></td>
	<td class="line x" title="156:234	Constraints: same output until 12,500 / none after 12,500." ></td>
	<td class="line x" title="157:234	The thin lines show the further development if we retain the the same-output constraint until no further merge is possible." ></td>
	<td class="line x" title="158:234	The length of the training part is gtrain ---14,421." ></td>
	<td class="line x" title="159:234	log10 p/Ntest 2.82.772.62.52.4 2.32.20 i 14 I ' I I ' I ' I ' I ' \[ ' I ' 1 2 3 4 5 6 7 8 I I I I I I I constraint change ~\%~L~-~lP /Pbigrara (1440 states) /Pmin (113 states) l ' I ' I ' I ' I ' I 9 10 11 12 13 14 xl03 merges I I i ~ I I 4 3 2 1 0  10 3 states 13 12 11 10 9 8 7 6 5 Figure 3: Log Perplexity of Test Part During Merging." ></td>
	<td class="line x" title="160:234	Constraints: Same Output until 12,500 / none after 12,500." ></td>
	<td class="line x" title="161:234	The thin line shows the further development if we retain the same-output constraint, finally yielding a bigram model." ></td>
	<td class="line x" title="162:234	The length of the test part is Ntest = 2,436." ></td>
	<td class="line x" title="163:234	65 We start merging with the same-output (unigram) constraint to reduce computation time." ></td>
	<td class="line x" title="164:234	After 12,500 merges the constraint is discarded and from then on all remaining states are allowed to merge." ></td>
	<td class="line x" title="165:234	The constraints and the point of changing the constraint are chosen for pragmatic reasons." ></td>
	<td class="line x" title="166:234	We want the constraints to be as week as possible to allow the maximal number of solutions but at the same time the number of merges must be manageable by the system used for computation (a SparcServerl000 with 250MB main memory)." ></td>
	<td class="line x" title="167:234	As the following experiment will show, the exact points of introducing/discarding constraints is not important for the resulting model." ></td>
	<td class="line x" title="168:234	There are Ntrain (Nt,ai,~1)/2 ~ 10 s hypothetical first merges in the unconstraint case." ></td>
	<td class="line x" title="169:234	This number is reduced to --~ 7." ></td>
	<td class="line x" title="170:234	105 when using the unigram constraint, thus by a factor of .v 150." ></td>
	<td class="line x" title="171:234	By using the constraint we need about a week of computation time on a SparcServer 1000 for the whole merging process." ></td>
	<td class="line x" title="172:234	Computation would not have been feasible without this reduction." ></td>
	<td class="line x" title="173:234	Figure 2 shows the increase in perplexity during merging." ></td>
	<td class="line x" title="174:234	There is no change during the first 1,454 merges." ></td>
	<td class="line x" title="175:234	Here, only identical sequences of initial and final states are merged (compare figure 1.a to c)." ></td>
	<td class="line x" title="176:234	These merges do not influence the probability assigned to the training part and thus do not change the perplexity." ></td>
	<td class="line x" title="177:234	Then, perplexity slowly increases." ></td>
	<td class="line x" title="178:234	It can never decrease: the maximum likelihood model assigns the highest probability to the training part and thus the lowest perplexity." ></td>
	<td class="line x" title="179:234	Figure 2 also shows the perplexity's slope." ></td>
	<td class="line x" title="180:234	It is low until about 12,000 merges, then drastically increases." ></td>
	<td class="line x" title="181:234	At about this point, after 12,500 merges, we discard the constraint." ></td>
	<td class="line x" title="182:234	For this reason, the curve is discontinuous at 12,500 merges." ></td>
	<td class="line x" title="183:234	The effect of further retaining the constraint is shown by the thin lines." ></td>
	<td class="line x" title="184:234	These stop after t2,983 merges, when all states with the same outputs are merged (i.e. , when a bigram model is reached)." ></td>
	<td class="line x" title="185:234	Merging without a constraint continues until only three states remain: the initial and the final state plus one proper state." ></td>
	<td class="line x" title="186:234	Note that the perplexity changes very slowly for the largest part, and then changes drastically during the last merges." ></td>
	<td class="line x" title="187:234	There is a constant phase between 0 and 1,454 merges." ></td>
	<td class="line x" title="188:234	Between 1,454 and ~11,000 merges the log perplexity roughly linearly increases with the number of merges, and it explodes afterwards." ></td>
	<td class="line x" title="189:234	What happens to the test part?" ></td>
	<td class="line x" title="190:234	Model merging starts with a very special model which then is generalized." ></td>
	<td class="line x" title="191:234	Therefore, the perplexity of some random sample of dialogue data (what the test part is supposed to be) should decrease during merging." ></td>
	<td class="line x" title="192:234	Table 1: Number of states and Log Perplexity for the derived models and an additional, previously test part, consisting of 9,784 words." ></td>
	<td class="line x" title="193:234	(a) standard bigram model, (b) constrained model merging (first experiment), (c) model merging starting with a bigram model(second experiment) (a) (b) (c) model MM start type bigrams merging with bigrams states 1,440 113 113 Log PP 2.78 2.41 2.39 This is exactly what we find in the experiment." ></td>
	<td class="line x" title="194:234	Figure 3 shows the log perplexity of the test part during merging." ></td>
	<td class="line x" title="195:234	Again, we find the discontinuity at the point where the constraint is changed." ></td>
	<td class="line x" title="196:234	And again, we find very little change in perplexity during about 12,000 initial merges, and large changes during the last merges." ></td>
	<td class="line x" title="197:234	Model merging finds a model with 113 states, which assigns a log perplexity of 2.26 to the test part." ></td>
	<td class="line x" title="198:234	Thus, in addition to finding a model with lower log perplexity than the bigram model (2.26 vs. 2.40), we find a model that at the same time has less than 1/10 of the states (113 vs. 1,440)." ></td>
	<td class="line x" title="199:234	To test if we found a model that predicts new data better than the bigram model and to be sure that we did not find a model that is simply very specialized to the test part, we use a new, previously unseen part of the Verbmobil corpus." ></td>
	<td class="line x" title="200:234	This part consists of 9,784 words." ></td>
	<td class="line x" title="201:234	The bigram model assigns a log perplexity of 2.78, the merged model with 113 states assigns a log perplexity of 2.41 (see table 1)." ></td>
	<td class="line x" title="202:234	Thus, the model found by model merging can be regarded generally better than the bigram model." ></td>
	<td class="line x" title="203:234	Im provements The derivation of the optimal model took about a week although the size of the training part was relatively small." ></td>
	<td class="line x" title="204:234	Standard speech applications do not use 14,000 words for training as we do in this experiment, but 100,000, 200,000 or more." ></td>
	<td class="line x" title="205:234	It is not possible to start with a model of 100,000 states and to successively merge them, at least it is not possible on today's machines." ></td>
	<td class="line x" title="206:234	Each step would require the test of,~ 10 9 merges." ></td>
	<td class="line x" title="207:234	In the previous experiment, we abandoned the same-output constraint after 12,500 merges to keep the influence on the final result as small as possible." ></td>
	<td class="line x" title="208:234	It can not be skipped from the beginning because somehow the time complexity has to be reduced." ></td>
	<td class="line x" title="209:234	But it can be further retained, until no further merge under this constraint is possible." ></td>
	<td class="line x" title="210:234	66 log10 P/Ntrain 2.52.0 1.5 1.0 0.5 s/Jr 10 11 12 I I I 4 3 2 Ip / log10 p/Ntest 2.8 2.7 --, 2.6~ ,~ 2.5'' \ 2.4 z-~ 2.32.2' I ' I 10 11 12 I J I 13 14 x 103 merges i i i i 4 3 2 1 0  10 3 states lpbigram lpmin 13 14 103 merges I I 1 0  103 states Figure 4: Log Perplexity of training and test parts when starting with a bigram model." ></td>
	<td class="line x" title="211:234	The starting point is indicated with o, the curves of the previous experiment are shown in thin lines." ></td>
	<td class="line x" title="212:234	This yields a bigram model." ></td>
	<td class="line x" title="213:234	The second experiment uses the bigram model with 1,440 states as its starting point and imposes no constraints on the merges." ></td>
	<td class="line x" title="214:234	The results are shown in figure 4." ></td>
	<td class="line x" title="215:234	We see that the perplexity curves approach very fast their counterparts from the previous experiment." ></td>
	<td class="line x" title="216:234	The states differ from those of the previously found model, but there is no difference in the number of states and corpus perplexity in the optimal point." ></td>
	<td class="line x" title="217:234	So, one could in fact, at least in the shown case, start with the bigram model without loosing anything." ></td>
	<td class="line x" title="218:234	Finally, we calculate the perplexity for the additional test part." ></td>
	<td class="line x" title="219:234	It is 2.39, thus again lower than the perplexity of the bigram model (see table 1)." ></td>
	<td class="line x" title="220:234	It is even slightly lower than in the previous experiment, but most probably due to random variation." ></td>
	<td class="line x" title="221:234	The derived models are not in any case equivalent (with respect to perplexity), regardless whether we start with the trivial model or the bigram model." ></td>
	<td class="line x" title="222:234	We ascribe the equivalence in the experiment to the particular size of the training corpus." ></td>
	<td class="line x" title="223:234	For a larger training corpus, the optimal model should be closer in size to the bigram model, or even larger than a bigram model." ></td>
	<td class="line x" title="224:234	In such a case starting with bigrams does not lead to an optimal model, and a trigram model must be used." ></td>
	<td class="line x" title="225:234	Conclusion We investigated model merging, a technique to induce Markov models from corpora The original procedure is improved by introducing constraints and a different initial model." ></td>
	<td class="line x" title="226:234	The procedures are shown to be applicable to a transliterated speech corpus." ></td>
	<td class="line x" title="227:234	The derived models assign lower perplexities to test data than the standard bigram model derived from the same training corpus." ></td>
	<td class="line x" title="228:234	Additionally, the merged model was much smaller than the bigram model." ></td>
	<td class="line x" title="229:234	The experiments revealed a feature of model merging that allows for improvement of the method's time complexity." ></td>
	<td class="line x" title="230:234	There is a large initial part of merges that do not change the model's perplexity w.r.t, the test part, and that do not influence the final optimal model." ></td>
	<td class="line x" title="231:234	The time needed to derive a model is drastically reduced by abbreviating these initial merges." ></td>
	<td class="line x" title="232:234	Instead of starting with the trivial model, one can start with a smaller, easy-to-produce model, but one has to ensure that its size is still larger than the optimal model." ></td>
	<td class="line x" title="233:234	Acknowledgements I would like to thank Christer Samuelsson for very useful comments on this paper." ></td>
	<td class="line x" title="234:234	This work was supported by the Graduiertenkolleg Kognitionswissenschaft, Saarbriicken." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="A97-1004
A Maximum Entropy Approach To Identifying Sentence Boundaries
Reynar, Jeffrey C.;Ratnaparkhi, Adwait;"></td>
	<td class="line x" title="1:100	A Maximum Entropy Approach to Identifying Sentence Boundaries Jeffrey C. Reynar and Adwait Ratnaparkhi* Department of Computer and Information Science University of Pennsylvania Philadelphia, Pennsylvania~ USA {jcreynar, adwait } @unagi.cis.upenn.edu Abstract We present a trainable model for identifying sentence boundaries in raw text." ></td>
	<td class="line x" title="2:100	Given a corpus annotated with sentence boundaries, our model learns to classify each occurrence of., ?, and / as either a valid or invalid sentence boundary." ></td>
	<td class="line x" title="4:100	The training procedure requires no hand-crafted rules, lexica, part-of-speech tags, or domain-specific information." ></td>
	<td class="line x" title="5:100	The model can therefore be trained easily on any genre of English, and should be trainable on any other Romanalphabet language." ></td>
	<td class="line x" title="6:100	Performance is comparable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains." ></td>
	<td class="line x" title="7:100	1 Introduction The task of identifying sentence boundaries in text has not received as much attention as it deserves." ></td>
	<td class="line x" title="8:100	Many freely available natural language processing tools require their input to be divided into sentences, but make no mention of how to accomplish this (e.g.(Brill, 1994; Collins, 1996))." ></td>
	<td class="line oc" title="10:100	Others perform the division implicitly without discussing performance (e.g.(Cutting et al. , 1992))." ></td>
	<td class="line x" title="12:100	On first glance, it may appear that using a short list of sentence-final punctuation marks, such as., ?, and /, is sufficient." ></td>
	<td class="line x" title="14:100	However, these punctuation marks are not used exclusively to mark sentence breaks." ></td>
	<td class="line x" title="15:100	For example, embedded quotations may contain any of the sentence-ending punctuation marks and." ></td>
	<td class="line x" title="16:100	is used as a decimal point, in email addresses, to indicate ellipsis and in abbreviations." ></td>
	<td class="line x" title="17:100	Both / and ? are somewhat less ambiguous * The authors would like to aclmowledge the support of ARPA grant N66001-94-C-6043, ARO grant DAAH0494-G-0426 and NSF grant SBR89-20230." ></td>
	<td class="line x" title="18:100	but appear in proper names and may be used multiple times for emphasis to mark a single sentence boundary." ></td>
	<td class="line x" title="19:100	Lexically-based rules could be written and exception lists used to disambiguate the difficult cases described above." ></td>
	<td class="line x" title="20:100	However, the lists will never be exhaustive, and multiple rules may interact badly since punctuation marks exhibit absorption properties." ></td>
	<td class="line x" title="21:100	Sites which logically should be marked with multiple punctuation marks will often only have one ((Nunberg, 1990) as summarized in (White, 1995))." ></td>
	<td class="line x" title="22:100	For example, a sentence-ending abbreviation will most likely not be followed by an additional period if the abbreviation already contains one (e.g. note that D. C is followed by only a single . in The president lives in Washington, D.C.)." ></td>
	<td class="line x" title="23:100	As a result, we believe that manually writing rules is not a good approach." ></td>
	<td class="line x" title="24:100	Instead, we present a solution based on a maximum entropy model which requires a few hints about what information to use and a corpus annotated with sentence boundaries." ></td>
	<td class="line x" title="25:100	The model trains easily and performs comparably to systems that require vastly more information." ></td>
	<td class="line x" title="26:100	Training on 39441 sentences takes 18 minutes on a Sun Ultra Sparc and disambiguating the boundaries in a single Wall Street Journal article requires only 1.4 seconds." ></td>
	<td class="line x" title="27:100	2 Previous Work To our knowledge, there have been few papers about identifying sentence boundaries." ></td>
	<td class="line x" title="28:100	The most recent work will be described in (Pa.lmer and Hearst, To appear)." ></td>
	<td class="line x" title="29:100	There is also a less detailed description of Pahner and Hearst's system, SATZ, in (Pahuer and Hearst, 1994)." ></td>
	<td class="line x" title="30:100	1 The SATZ architecture uses either a decision tree or a neural network to disambiguate sentence boundaries." ></td>
	<td class="line x" title="31:100	The neural network achieves 98.5% accuracy on a corpus of Wall Str'eet Journal t~Ve recommend these articles for a more comprehensive review of sentence-boundary identification work than we will be able to provide here." ></td>
	<td class="line x" title="32:100	16 articles using a lexicon which includes part-of-speech (POS) tag information." ></td>
	<td class="line x" title="33:100	By increasing the quantity ol' 1.ra.ining data and decreasing the size of their test,,~rlouS." ></td>
	<td class="line x" title="34:100	Palmer and Hearst achieved performance of !)s.9% with the neural network." ></td>
	<td class="line x" title="35:100	They obtained similar results using the decision tree." ></td>
	<td class="line x" title="36:100	All the results we will present for our a.lgorithms are on their initial, larger test." ></td>
	<td class="line x" title="37:100	corpus." ></td>
	<td class="line x" title="38:100	In (Riley, 1989), Riley describes a decision-tree based approach to the problem." ></td>
	<td class="line x" title="39:100	His performance on /he Brown corpus is 99.8%, using a model learned t'rom a corpus of 25 million words." ></td>
	<td class="line x" title="40:100	Liberman and Church suggest in (Liberlnan and Church, 1992) that." ></td>
	<td class="line x" title="41:100	a system could be quickly built to divide newswire text into sentences with a nearly negligible error rate." ></td>
	<td class="line x" title="42:100	but, do not actually build such a system." ></td>
	<td class="line x" title="43:100	3 Our Approach \e present two systems for identifying sentence boundaries." ></td>
	<td class="line x" title="44:100	One is targeted a.t high performance and uses some knowledge about the structure of English financial newspaper text which may not be applical)le t.o text from other genres or in other languages." ></td>
	<td class="line x" title="45:100	The other system uses no domain-specific knowledge and is aimed at being portable across English t, ext genres and Roman alphabet languages." ></td>
	<td class="line x" title="46:100	Pot.ential sentence boundaries are identified by scamfing the text tbr sequences of characters sepaa'ated by whitespace (tokens) containing one of the symbols !, . or ?." ></td>
	<td class="line x" title="47:100	We use information about the tollen containing the potential sentence boundary, as well as contextual information about the tokens immediately to the left and to the right." ></td>
	<td class="line x" title="48:100	We also conducted tests using wider contexts, but performance did not, improve." ></td>
	<td class="line x" title="49:100	We call the token containing the symbol which marks a putative sentence boundary the Candidate." ></td>
	<td class="line x" title="50:100	'Phe portion of the Candidate preceding the potent.ial sentence boundary is called the Prefix and the portion following it is called the Suffix." ></td>
	<td class="line x" title="51:100	The system that focused on maximizing performance used the following hints, or contextual 'templates':  The Prefix  The Suffix  The presence of particular characters in the Prefix or Suffix  Whether the Candidate is an honorific (e.g. A,l,s. , Dr. , Gen)." ></td>
	<td class="line x" title="52:100	 Whether the Candidate is a. corporate designator (e.g. Coriv.,,5'.p.A. , L.L.C)." ></td>
	<td class="line x" title="53:100	 Features of the word left of the Candidate  Features of the word right of the Candidate The templates specify only the form of the information." ></td>
	<td class="line x" title="54:100	The ~J~acl intbrmation used by the maximum entropy model \['or the potential sentence boundary marked by . in Col7~." ></td>
	<td class="line x" title="55:100	in Example 1 would be: PreviousWordIsCapitalized, Prefix=Corp, Suffix=NULL, PrefixFeature=C.orporateDesignator." ></td>
	<td class="line x" title="56:100	(1) ANLP Corp. chairman Dr. Smith resigned." ></td>
	<td class="line x" title="57:100	The highly portable system uses only the identity of the C',andidate and its neighboring words, and a list of abbreviations induced froln the training data." ></td>
	<td class="line x" title="58:100	2 Specifically, the 'templates' used are:  The Prefix  The Suffix  Whether the Prefix or Suffix is on the list of induced abbreviations  The word left, of the Candidate  The word right of the Candidate  Whether the word to the left or right of the Candidate is on the list of induced abbreviations The intbrmation this model would use for Example 1 would be: PreviousWord=ANLP, FollowingWord=chairman, Prefix=Corp, Suffix=NULL, PrefixFeature=InducedAbbreviation." ></td>
	<td class="line x" title="59:100	The abbreviation list is automatically produced from the training data., and the contextual questions are also automat.ically generated by scanning the training data with question templates." ></td>
	<td class="line x" title="61:100	As a. result, no hand-crafted rules or lists are required by the highly portable system and it can be easily retrained for other languages or text genres." ></td>
	<td class="line x" title="62:100	4 Maxilnum Entropy The model used here for sentence-boundary detection is based on the maximum entropy model used for POS tagging in (Ratnaparkhi, 1996)." ></td>
	<td class="line x" title="63:100	For each potential sentence boundary token (., ?, and !), we estimate a joint probability distribution p of the token and it.s surrounding context, both of which are denoted by c, occurring as an actual sentence I)oundary." ></td>
	<td class="line x" title="65:100	The (list, ribul.ioll is given by: I,." ></td>
	<td class="line x" title="66:100	f,(b,c) p(b, ~) = ~ 1-I j=,,~'),,,,h~,-e ~ ~ {no, y~}, where 2A token in the training data is considered an abbreviation if it is preceded and followed by whitespace, and it contains a . that is not a sentence boundary." ></td>
	<td class="line x" title="67:100	17 the ctj's are the unknown parameters of the model, and where each c U corresponds to a fj, or a feature." ></td>
	<td class="line x" title="68:100	Thus the probability of seeing an actual sentence boundary in the context c is given by p(yes, e)." ></td>
	<td class="line x" title="69:100	The contextual information deemed useful for sentence-boundary detection, which we described earlier, must be encoded using features." ></td>
	<td class="line x" title="70:100	For exampie, a useful feature might be: WSJ Brown Sentences 20478 51672 Candidate P. Marks 32173 61282 Accuracy 98.8% 97.9% False Positives 20 \[ 7.50 False Negatives 171 506 Table 1: Our best pertbrmance on two corpora." ></td>
	<td class="line x" title="71:100	1 if Prefix(c) = Mr &; b.= no \])(b,c) = 0 otherwise This feature will allow the model to discover that the period at the end of the word Mr. seldom occurs as a sentence boundary." ></td>
	<td class="line x" title="72:100	Therefore the parameter corresponding to this feature will hopefully boost the probability p(no, c) if the Prefix is Mr. The parameters are chosen to maximize the likelihood of the I.raining data using the Generalized Iterative Scaling (Darroeh and Ratcliff, 1972) algorithm." ></td>
	<td class="line x" title="73:100	The model also can be viewed under the Maximum Entropy framework, in which we choose a dist.ribution p that maximizes the entropy H(p) H(p) = Ep(b, c)logp(b, c) under the following constraints: Ep(b,c)J)(b,c ) = E~(b,c)fj(b,c),l < j < k where iS(b, c) is the observed distribution of sentenceboundaries and contexts in the training data." ></td>
	<td class="line x" title="74:100	As a result, the model in practice tends not to commit towards a particular outcome (yes or no) unless it ha~s seen sufficient evidence for that outcome; it is maximally uncertain beyond meeting the evidence." ></td>
	<td class="line x" title="75:100	All experiments use a simple decision rule to elassi\[y each potential sentence boundary: a potential sentence boundary is an actual sentence boundary if and only if p(yeslc ) > .5, where p(yes, c) p(yeslc ) = p(yes, c) -Ip(no, c) and where c is the context including the potential sentence boundary." ></td>
	<td class="line x" title="76:100	5 System Performance We trained our system on 39441 sentences (898737 words) of Wall Street Journal text from sections 00 through 24 of the second release of the Penn Treebank 3 (Marcus, Santorini, and Marcinkiewicz, :~We did not train on files which overlapped with Pahner and Hearst's test data, namely sections 03, 04, 05 and 06." ></td>
	<td class="line x" title="77:100	1993)." ></td>
	<td class="line x" title="78:100	We corrected punctuation mistakes and erroneous sentence boundaries in the training data." ></td>
	<td class="line x" title="79:100	Performance figures for our best performing system, which used a hand-crafted list of honorifics and corporate designators, are shown in Table 1." ></td>
	<td class="line x" title="80:100	The first test set, WSJ, is Pahner and Hearst's initial test data and the second is the entire Brown corpus." ></td>
	<td class="line x" title="81:100	We present the Brown corpus performance to show the importance of training on the genre of text on which testing will be performed." ></td>
	<td class="line x" title="82:100	Table 1 also shows the number of sentences in each corpus, the lmmber of candidate punctuation marks, the accuracy over potential sentence boundaries, the nmnber of false positives and the number of false negatives." ></td>
	<td class="line x" title="83:100	Performance on the WSJ corpus was, as we expected, higher than perforlnance on the Brown corpus since we trained the model on financial newspaper text." ></td>
	<td class="line x" title="84:100	Possibly more significant than the system's performance is its portability to new domains and languages." ></td>
	<td class="line x" title="85:100	A trimmed down system which used no information except that derived from the training corpus performs nearly as well, and requires no resources other than a training corpus." ></td>
	<td class="line x" title="86:100	Its performance on the same two corpora is shown in Table 2." ></td>
	<td class="line x" title="87:100	Test False False Corpus Accuracy Positives Negatives WSJ 98.0% 396 245 Brown 97.5% 1260 265 Table 2: Performance on the sa.me two corpora, using the highly portable system." ></td>
	<td class="line x" title="88:100	Since 39441 training sentences is considerably more than might exist ill a new dolnail~ or a language other than English, we experimented with the quantity of training data required to maintain perforlnance." ></td>
	<td class="line x" title="89:100	Table 3 shows performance on the WSJ corpus as a flmction oft, raining set size using the best performing system and the more portable system." ></td>
	<td class="line x" title="90:100	As can seen fl'om the table, performance degrades as the quantity of training data decreases, but even 18 Number of sentences in training corpus 500 1000 2000 4000 8000 16000 39441 Best performing 97.6% 98.4% 98.0% 98.4% 98.3% 98.3% 98.8~Z~, Highly portable 96.5% 97.3% 97.3% 97.6% 97.6% 97.8% 98.0% Table 3: Performance on Wall 't~vet Journal test data a.s a. flmction of training set." ></td>
	<td class="line x" title="91:100	size for both systems." ></td>
	<td class="line x" title="92:100	with only 500 exalnple sentences performance is betI(,~' lhan the baselines of 64.0% if a sentence bound~l\; is guessed at every potential site and 78.4(K, if only token-final instances of sentence-ending punctuation are assumed to be boundaries." ></td>
	<td class="line x" title="93:100	6 Conclusions We have described an approach to identifying sentence boundaries which performs comparably to other state-of-the-art systems that require vastly luore resources." ></td>
	<td class="line x" title="94:100	For example, Riley's performance ot~ the Brown corpus is higher than ours, but his sysl era is trained on the Brown corpus and uses thirty i.ilnes as much data as our system." ></td>
	<td class="line x" title="95:100	Also, Pahner & Hearst's system requires POS tag information, which limits its use to those genres or languages for which there are either POS tag lexica or POS tag annotated corpora, that could be used to train automarie taggers." ></td>
	<td class="line x" title="96:100	In comparison, our system does not require POS tags or any supporting resources beyond the sentence-boundary annotated corpus." ></td>
	<td class="line x" title="97:100	It is theretbre easy and inexpensive to retrain this syst.em tbr different genres of text in English and text in ()tiler l:(.oma.n-a.lphabet languages." ></td>
	<td class="line x" title="98:100	Furthermore, we showed that a small training corpus is sufficient for good performance, and we estimate that annotating enough data to achieve good performance would require only several hours of work, in comparison to the many hours required to generate POS tag and lexical probabilities." ></td>
	<td class="line x" title="99:100	7 Acknowledgments We would like to thank David Palmer for giving us I.he test data he and Marti Hearst used for their sentence detection experiments." ></td>
	<td class="line x" title="100:100	We would also like to thank the anonymous reviewers for their helpful insights." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="A97-1014
An Annotation Scheme For Free Word Order Languages
Skut, Wojciech;Krenn, Brigitte;Brants, Thorsten;Uszkoreit, Hans;"></td>
	<td class="line x" title="1:254	An Annotation Scheme for Free Word Order Languages Wojciech Skut, Brigitte Krenn, Thorsten Brants, Hans Uszkoreit Universit/~t des Saarlandes 66041 Saarbrficken, Germa,ny { skut, krenn, brant s, uszkoret }@col ." ></td>
	<td class="line x" title="2:254	unsb." ></td>
	<td class="line x" title="3:254	de Abstract We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages." ></td>
	<td class="line x" title="4:254	Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme." ></td>
	<td class="line x" title="5:254	The resulting scheme reflects a stratificational notion of language, and makes only minimal assumptions about the interrelation of the particuJar representational strata." ></td>
	<td class="line x" title="6:254	1 Introduction The work reported in this paper aims at providing syntactically annotated corpora ('treebanks') for stochastic grammar induction." ></td>
	<td class="line x" title="7:254	In particular, we focus on several methodological issues concerning the annotation of non-configurational languages." ></td>
	<td class="line x" title="8:254	In section 2, we examine the appropriateness of existing annotation schemes." ></td>
	<td class="line x" title="9:254	On the basis of these considerations, we formulate several additional requirements." ></td>
	<td class="line x" title="10:254	A formMism conrplying with these requirements is described in section 3." ></td>
	<td class="line x" title="11:254	Section 4 deals with the treatment of selected phenomena." ></td>
	<td class="line x" title="12:254	For a description of the annotation tool see section 5." ></td>
	<td class="line x" title="13:254	2 Motivation 2.1 Linguistically Interpreted Corpora Combining raw language data with linguistic intormation offers a promising basis for the development of new efficient and robust NLP methods." ></td>
	<td class="line x" title="14:254	Realworld texts annotated with difihrent strata of linguistic information can be used for grarninar induetion." ></td>
	<td class="line x" title="15:254	The data-drivenness of this approach presents a clear advantage over tile traditional, idealised notion of competence grammar." ></td>
	<td class="line x" title="16:254	2.2 Existing Treebank Formats Corpora annotated with syntactic structures are commonly referred to as trt:tbauk.~." ></td>
	<td class="line x" title="17:254	Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: GrammaticM phenomena are to be described rather than explained." ></td>
	<td class="line x" title="18:254	Theory-independence: Annotations should not be influenced by theory-specific considerations." ></td>
	<td class="line x" title="19:254	Nevertheless, different theory-specific representations shMl be recoverable from the annotation, cf.(Marcus et al. , 1994)." ></td>
	<td class="line x" title="21:254	Multi-stratal representation: Clear separation of different description levels is desirable." ></td>
	<td class="line x" title="22:254	Data-drivenness: The scheme must provide representational means for all phenomena occurring in texts." ></td>
	<td class="line x" title="23:254	Disambiguation is based on human processing skills (cf.(Marcus et at., 1994), (Sampson, 1995), (Black et aal., 1996))." ></td>
	<td class="line x" title="27:254	The typical treebank architecture is as follows: Structures: A context-free backboI~e is augmented with trace-filler representations of non-local dependencies." ></td>
	<td class="line x" title="28:254	The underlying argum~.nt structure is not represented directly, but can be recovered from the tree and trace-filler ammtations." ></td>
	<td class="line x" title="29:254	Syntactic category is encoded in node IM:,els." ></td>
	<td class="line x" title="30:254	Gralnmatical flinctioxls constitute a complex label system (cf.(Bies et al. , 1995), (Sampson, 1995))." ></td>
	<td class="line x" title="32:254	Part-of-Speech is annotated at word level." ></td>
	<td class="line x" title="33:254	Thus the context-li'ee constituent backbone plays a pivotal role in the annotation scherne." ></td>
	<td class="line x" title="34:254	Due to the substantial differences between existing models of constituent structure, tile question arises of how the theory indcp~ndcnc~, requirement can be satisfied." ></td>
	<td class="line x" title="35:254	At this point the mlportance of the underlying argument struc~ur: is emphasised (cf.(Lehmaim et al. , 1996), (Marcus et al. , 1994), (Sampson, 1995))." ></td>
	<td class="line x" title="37:254	2.3 Language-Specific Features Treebanks of the tbrmat described ill tile M)ove section have been designed tbr English." ></td>
	<td class="line x" title="38:254	Tllereff)re, the 88 solutions they offer are not always optirnal for other language types." ></td>
	<td class="line x" title="39:254	As for free word order languages, the following features may cause problems:  local a,nd ram-local dependencies tbrm a continuum rather than clear-cut classes of phenomena;  there exists a rich inventory of discontinuous constituency types (topicalisation, scrambling, clause union, pied piping, extraposition, split NPs and PPs);  word order variation is sensitive to many factors, e.g. category, syntactic flmction, focus;  the gramrn~ticMity of different word permutations does not fit the tr~,ditional binary 'rightwrong' pattern; it, rather tbrms a gradual transition between the two poles." ></td>
	<td class="line x" title="40:254	In light of these facts, serious difficulties can be expected arising from the structurM component of the existing formalisms." ></td>
	<td class="line x" title="41:254	Due to the frequency of discontinuous constituents in non-eonfigurational langua.ges, the filler-trace mechanism would be used very often, yielding syntactic trees fairly different from the underlying predicate-argument structures." ></td>
	<td class="line x" title="42:254	Consider the German sentence (1) d;tra.n wird ihn Anna." ></td>
	<td class="line x" title="43:254	erkennen, da.t\] er weint at-it will him Anita." ></td>
	<td class="line x" title="44:254	recognise tha.t he cries 'Anna." ></td>
	<td class="line x" title="45:254	will recognise Iron a.t his cry' A sample constituent structure is given below: S ~S#t Adv~ V NP#2 NP I I V / \ daran e#1 wird ihn Anna e#e e#.~ erkennen, dass erweint The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes." ></td>
	<td class="line x" title="46:254	This hybrid representation makes the structure less transparent, and therefore more difficult to annotate." ></td>
	<td class="line x" title="47:254	Apart from this rather technical problem, two further arguments speak against phrase structure as the structural pivot of the annotation scheme:  Phrase structure models stipulated tbr nonconfigura.tionM languages differ strongly from each other, presenting a challenge to the intended theory-independence of the schelne." ></td>
	<td class="line x" title="48:254	 Constituent structure serves as an exl)la.natory device for word order variation, which is difficult to reconcile with the descriptivity requirement." ></td>
	<td class="line x" title="49:254	Finally, the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies, which ha:s proved tedious." ></td>
	<td class="line x" title="50:254	Since most methods of handling discontinuous constituents make the fornaalism more powerfifl, the efficiency of processing deteriorates, too." ></td>
	<td class="line x" title="51:254	An Mternative solution is to make argurnent structure the main structural component of the formalism." ></td>
	<td class="line x" title="52:254	This assumption underlies a growing number of recent syntactic theories which give up the context-free constituent ba.ckbone, cf.(McCawley, 1987), (Dowty, 1989), (Reape, 1993), (Kathol and Pollard, 1995)." ></td>
	<td class="line x" title="54:254	These approaches provide an adequate explanation for several issues problematic ibr phrase-structure grammars (clause union, extraposition, diverse second-position phenomena)." ></td>
	<td class="line x" title="55:254	2.4 Annotating Argument Structure Argument structure can be represented in terms of unordered trees (with crossing branches)." ></td>
	<td class="line x" title="56:254	In order to reduce their ambiguity potential, rather simple, 'flat' trees should be employed, while more information can be expressed by a rich system of function labels." ></td>
	<td class="line x" title="57:254	Furthermore, the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions, e.g. every syntactic structure has a unique hea.d. Thus, notions such as head should be distinguished at the level of syntactic flmctions rather than structures." ></td>
	<td class="line x" title="58:254	This requirement speaks against the traditional sort of d~:pendency trees, in which heads are represented as non-terminal nodes, cf.(Hudson, 1984)." ></td>
	<td class="line x" title="60:254	A tree meeting these requirements is given below: (,,)--I Adv V NP NP V CPL NP V daran wird ihn Anna erkennen, &tss er weint Such a word order independent representation has the advantage of all structural ini'orrrlation being encoded in a single data structure." ></td>
	<td class="line x" title="61:254	A unifbrm representation of local and non-local dependencies makes the structure more transparent 1." ></td>
	<td class="line x" title="62:254	3 The Annotation Scheme 3.1 Architecture YVe distinguish the tbllowmg levels of representation: 1A context-Kee constituent backboIm ca.it still be recovered fl'mn tile surfa,ce string a.nd a.rgmnent structure by rea, tta,ching 'extra.cted' structures to ;t higher node." ></td>
	<td class="line x" title="63:254	89 Argument structure, represented in terms of unordered trees." ></td>
	<td class="line x" title="64:254	Grammatical functions, encoded in edge labels, e.g. SB (subject), MO (modifier), HD (head)." ></td>
	<td class="line x" title="65:254	Syntactic categories, expresse(l by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terlninals." ></td>
	<td class="line x" title="66:254	3.2 Argulnent Structure A structure for (2) is shown in fig." ></td>
	<td class="line x" title="67:254	2." ></td>
	<td class="line x" title="68:254	(2) schade, dM~ kein Arzt anwesend ist, tier pity that no doctor present is who sich auskennt is competent 'Pity that no competent doctor is here' Note that the root node does not have a head descendant (HD) as the sentence is a predicative construction consisting of a subject (SB) and a predicate (PD) without a copula." ></td>
	<td class="line x" title="69:254	The subject is itself a sentence in which the copula (is 0 does occur and is assigned the tag HD 2." ></td>
	<td class="line x" title="70:254	The tree resembles traditional constituent structures." ></td>
	<td class="line x" title="71:254	The difference is its word order independence: structural units ('phrases') need not be contiguous substrings." ></td>
	<td class="line x" title="72:254	For instance, the extraposed relative clause (RC) is still treated as part of the subject NP." ></td>
	<td class="line x" title="73:254	As the annotation scheme does not distinguish different bar levels or any similar intermediate categories, only a small set of node labels is needed (currently 16 tags, S, NP, AP )." ></td>
	<td class="line x" title="74:254	3.3 Grammatical Functions Due to the rudimentary character of the argument structure representations, a great deal of reformation has to be expressed by gramnlatical functions." ></td>
	<td class="line x" title="75:254	Their further classification must reflect different kinds of linguistic information: morphology (e.g. , case, inflection), category, dependency type (complementation vs. modification), thematic role, etc. 3 However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation." ></td>
	<td class="line x" title="76:254	In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotalion and r'efincment." ></td>
	<td class="line x" title="77:254	While in the first phase each annotator has to annotate structures as well as categories and functions, the refinement can be done separately for each representation level." ></td>
	<td class="line x" title="78:254	During the first, phase, the focus is on almotating correct structures and a coarse-grained classification of grammatical functions, which represent the following areas of information: 2CP stands for conwlementizer, OA for accusative object and RC for relative clause." ></td>
	<td class="line x" title="79:254	NK denotes a 'kernel NP' component (v. section 4.1)." ></td>
	<td class="line x" title="80:254	aFor an extensive use of gr;tnllnaticM functions Cf." ></td>
	<td class="line x" title="81:254	(K~trlsson et al. , 1995), (Voutilainen, 1994)." ></td>
	<td class="line x" title="82:254	Dependency type: complemcnls are fllrther classified according to features su(:h as category and case: clausal complements (OC), accusative objects (OA), datives (DA), etc. Modifiers are assigned the label MO (further classification with respect to thematic roles is planned)." ></td>
	<td class="line x" title="83:254	Separate labels are defined for dependencies that do not fit the complement/modifier dichotomy, e.g., pre(GL) and postnominal genitives (GR)." ></td>
	<td class="line x" title="84:254	Headedness versus non-headedness: Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD." ></td>
	<td class="line x" title="85:254	Morphological information: Another set of labels represents morphological information." ></td>
	<td class="line x" title="86:254	PM stands for moTThological partich, a label tbr German infinitival zu aml superlative am." ></td>
	<td class="line x" title="87:254	Separable verb prefixes are labeled SVP." ></td>
	<td class="line x" title="88:254	During the second annotation stage, the annotation is enriched with information about, thematic roles, quantifier scope and anaphoric ret)rence." ></td>
	<td class="line x" title="89:254	As already mentioned, this is done separately for each of the three information areas." ></td>
	<td class="line x" title="90:254	3.4 Structure Sharing A phrase or a lexical item can perform multiple functions in a sentence." ></td>
	<td class="line x" title="91:254	Consider ~.qui verbs where the subject of the infinitival VP is not realised syntactically, but co-referent with the subject or object of the matrix equi verb: (3) er bat reich ZU kolnlnen he asked me to come (mich is the imderstood subject of komm~.u.)." ></td>
	<td class="line x" title="92:254	In such cases, an additional edge is drawn from tim embed(led VP node to the controller, thus changing the syntactic tree into a graph." ></td>
	<td class="line x" title="93:254	We call such additional edges secondary links and represent them as dotted lines, see fig." ></td>
	<td class="line x" title="94:254	4, showing the structure of (3)." ></td>
	<td class="line x" title="95:254	4 Treatment of Selected Phenomena As theory-independence is one of our objectives, the annotation scheme incorporates a number of widely accepted linguistic analyses, especially ill the area of verbal, adverbial and adjectival syntax." ></td>
	<td class="line x" title="96:254	However, some other s~andard analyse.s turn out to be proMemarie, mainly due to the partial, idealised character of competence grammars, which often margmalise or ignore such important phenolnena as 'deficient' (e.g. headless) constructions, apl)ositions, temporal expressions, etc. In the following paragraphs, we give annotations for a number of such phenomena." ></td>
	<td class="line x" title="97:254	4.1 Noun Phrases Most linguistic theories treat NPs as structures hea(led by a unique lexical item (no,m) However, this 90 idealised model needs severa.l additional assumptions in order to account for such important phenomena as complex norninal NP components (cf.(4)) or nominalised a.djectives (of." ></td>
	<td class="line x" title="99:254	(5))." ></td>
	<td class="line x" title="100:254	(4) my uncle Peter Smith (5) tier sehr (41iickliche the very lta.ppy 'tire very ha.pl)y one' In (4), different theories make different headedness predictions." ></td>
	<td class="line x" title="101:254	In (5), either a lexical nominalisation rule for the adjective Gliicklichc is stipulated, or the existence of an empty nominal head." ></td>
	<td class="line x" title="102:254	Moreover, the so-called DP analysis views the article der as the head of the phrase." ></td>
	<td class="line x" title="103:254	Further differences concern the a.ttachment of the degree modifier,ehr." ></td>
	<td class="line x" title="104:254	Because of the intended theory-independence of the scheme, we annotate only the cornmon rninimum." ></td>
	<td class="line x" title="105:254	We distinguish an NP kernel consisting of determiners, a.djective phrases and nouns." ></td>
	<td class="line x" title="106:254	All components of this kernel are assigned the label NK aml trea.ted as sibling nodes." ></td>
	<td class="line x" title="107:254	The diff>rence between the particular NK's lies in the positional and part-of-speech information, which is also sufficient to recover theory-specific structures frorn our 'underspecified' representations." ></td>
	<td class="line x" title="108:254	For instance, the first determiner among the NK's can be treated as the specifier of the phrase." ></td>
	<td class="line x" title="109:254	The head of the phrase can be determined in a similar way according to theory-specific assumptions." ></td>
	<td class="line x" title="110:254	In addition, a number of clear-cut NP components can be defined outside that juxtapositional kernel: preand postnorninal genitives (GL, GR), relative clauses (RC), clausal and sentential complements (OC)." ></td>
	<td class="line x" title="111:254	They are all treated as siblings of NK's regardless of their position (in situ or extraposed)." ></td>
	<td class="line x" title="112:254	4.2 Attaehlnent Ainbiguities Adjunct attachment often gives rise to structural ambiguities or structural uncertainty." ></td>
	<td class="line x" title="113:254	However, fill or partial disambiguation takes place in context, and the annotators do not consider unrealistic readings." ></td>
	<td class="line x" title="114:254	In addition, we have adopted a simple convention for those cases in which context information is insufficient f~)r total disaml~iguat,ion: the highest possible attachment site is chosen." ></td>
	<td class="line x" title="115:254	A similar convention has been adopted ibr constructions in which scope ambiguities ha.ve syntactic effe, cts but a. one-to-one correspondence between scope a.nd attachment does not seem reasonable, cf.focus particles such a.s only or also." ></td>
	<td class="line x" title="117:254	If the scope of such a word does not directly correspond to a tree node, the word is attached to the lowest node dominating all subconstituents a.pl)earing ill its scope." ></td>
	<td class="line x" title="118:254	4.3 Coordination A problem for the rudimentary a.rgument structure representations is tile use of incomplete structures in natural language, i.e. t)henornena such as coordination and ellipsis." ></td>
	<td class="line x" title="119:254	Since a precise structural description of non-constituent coordination would require a rich inventor.), of incomplete phrase types, we have agreed on a sort of nnderspecified representations: the coordinated units are assigned structures in which missing lexical material is not represented at the level of primary links." ></td>
	<td class="line x" title="120:254	Fig." ></td>
	<td class="line x" title="121:254	3 shows the representation of the sentence: (6) sie wurde van preuliischen Truppen besetzt site was by Prussiaa, troops occupied und 1887 dem preutlischen Staat angegliedert and 1887 to-the Prussia.n state incorporated 'it was occupied by Prussian troops and incorporated into Prussia i,t 1887' The category of the coordination is labeled CVP here, where C stands for coordination, and VP tar the actual category." ></td>
	<td class="line x" title="122:254	This extra, marking makes it easy to distinguish between 'normal' and coordinated categories." ></td>
	<td class="line x" title="123:254	Multiple coordination as well a.s enumerations are annotated in the same way." ></td>
	<td class="line x" title="124:254	An explicit coordinating conjunction need not be present." ></td>
	<td class="line x" title="125:254	Structure-sharing is expressed using secondary links." ></td>
	<td class="line x" title="126:254	5 The Annotation Tool 5.1 Requirenlents The development of linguistically interpreted corpora, presents a laborious and time-consuming task." ></td>
	<td class="line x" title="127:254	In order to make the annotation process more efficient, extra effort has been put into the development of an annotation tool." ></td>
	<td class="line x" title="128:254	The tool supports immediate graphical feedback and automatic error checking." ></td>
	<td class="line x" title="129:254	Since our scheme permits crossing edges, visualisa.tion as bracketing and indentation would be insufficient." ></td>
	<td class="line x" title="130:254	Instead, the con> plete structure should be represented." ></td>
	<td class="line x" title="131:254	The tool should also permit a convenient handling of node and edge hd)els." ></td>
	<td class="line x" title="132:254	In particular, variable tagsets and label collections should be allowed." ></td>
	<td class="line x" title="133:254	5.2 Implementatioll As the need for certain flmctionalities becomes obvious with growing annota.tion experience, we have decided to iml)lement the tool in two stages." ></td>
	<td class="line x" title="134:254	In the first phase, the ma.in flmctionality for buihling and displaying unordered trees is supplied." ></td>
	<td class="line x" title="135:254	In the second phase, secondary links and additional structural flmctions are supported." ></td>
	<td class="line x" title="136:254	The implementation of the first phase as described in the following paragraphs is completed." ></td>
	<td class="line x" title="137:254	As keyboard input is rnore efficient than mouse input (cf.(Lehmalm et al. , 1!)95)) rnost effort has been put in developing an efficient keyboard interlace." ></td>
	<td class="line x" title="139:254	Menus are supported as a. usefld way of getting 91 help on commands and labels." ></td>
	<td class="line x" title="140:254	In addition to pure annotation, we can attach conlments to structures." ></td>
	<td class="line x" title="141:254	Figure 1 shows a screen dump of the tool." ></td>
	<td class="line x" title="142:254	The largest part of the window contains the graphical representation of tim structure being annot, ate(t. The tbllowing commands are available:  group words and/or phrases to a new phrase;  ungroup a phrase;  change the name of a phrase or an edge;  re-attach a node;  generate the postscript output of a sentence." ></td>
	<td class="line x" title="143:254	The three tagsets used by the annotation tool (for words, phrases, and edges) are variable and are stored together with the corpus." ></td>
	<td class="line x" title="144:254	This allows easy modification if needed." ></td>
	<td class="line x" title="145:254	The tool checks the appropriateness of the input." ></td>
	<td class="line x" title="146:254	For the implementation, we used Tcl/Tk Version 4.1." ></td>
	<td class="line x" title="147:254	The corpus is stored in a SQL database." ></td>
	<td class="line x" title="148:254	5.3 Automation The degree of automation increases with the amount of data available." ></td>
	<td class="line x" title="149:254	Sentences annotated in previous steps are used as training material for further processing." ></td>
	<td class="line x" title="150:254	We distinguish five degrees of automation: 0) Completely manual annotation." ></td>
	<td class="line x" title="151:254	1) The user determines phrase boundaries and syntactic categories (S, NP, etc.)." ></td>
	<td class="line x" title="152:254	The program automatically assigns grammatical fimetion labels." ></td>
	<td class="line x" title="153:254	The annotator can alter the assigned tags." ></td>
	<td class="line x" title="154:254	2) The user only determines the conrponents of a new phrase, the program determines its syntactic category and the grammatical functions of its elements." ></td>
	<td class="line x" title="155:254	Again, the annotator has the option of altering the assigned tags." ></td>
	<td class="line x" title="156:254	3) Additionally, the program performs simple bracketing, i.e., finds 'kernel' phrases." ></td>
	<td class="line x" title="157:254	4) Tile tagger suggests partial or cornplete parses." ></td>
	<td class="line x" title="158:254	So far, about 1100 sentences of our corpus have been annotated." ></td>
	<td class="line x" title="159:254	This amount of data suffices as training material to reliably assign the grammatical functions if the user determines the elements of a phrase and its type (step 1 of the list above)." ></td>
	<td class="line oc" title="160:254	5.4 Assigning GramInatical Function Labels Grammatical functions are assigned using standard statistical part-of-speech tagging methods (cf.e.g.(Cutting et al. , 1992) and (Feldweg, 1995))." ></td>
	<td class="line x" title="163:254	For a phrase Q with children of type T  T~: and grammatical fimctions G,,, (7~:, we use the lexical probabilities PO(GiITi) and the contextual (trigram) probabilities PQ(T; \[Ti-,, Ti-~ ) 92 The lexical and contextual probabilities are determined separately for each type of phrase." ></td>
	<td class="line x" title="164:254	During annotation, the highest rated granmlatical fimction labels Gi a.re calculated using the Viterbi algorithnr and a.ssigned to the structure, i.e., we." ></td>
	<td class="line x" title="165:254	<'Mculate k argma.x H PQ(T, IT,-1, ~_~,) . PQ(G, IT,)." ></td>
	<td class="line x" title="166:254	G i=1 To keep the human annotator from missing errors made by the tagger, we additionally calculate the strongest competitor for each label Gi." ></td>
	<td class="line x" title="167:254	If its probability is close to the winner (closeness is defined by a threshold on the quotient), the assignment is regarded as unreliable, and the annotator is asked to confirm the assignment." ></td>
	<td class="line x" title="168:254	For evaluation, the already annota.ted sentences were divided into two disjoint sets, one tbr training (90% of the corpus), the other one tbr testing (10%)." ></td>
	<td class="line x" title="169:254	The procedure was repeated 10 times with different partitionings." ></td>
	<td class="line x" title="170:254	The tagger rates 90% of all assignments as reliable and carries them out fully automatically." ></td>
	<td class="line x" title="171:254	Accuracy for these cases is 97%." ></td>
	<td class="line x" title="172:254	Most errors are due to wrong identification of the subject and different kinds of objects in sentences and VPs." ></td>
	<td class="line x" title="173:254	Accuracy of the unreliable 10% of assignments is 75%, i.e., the annotator has to alter the choice in 1 of 4 cases when asked ibr confirmation." ></td>
	<td class="line x" title="174:254	Overall accuracy of the tagger is 95%." ></td>
	<td class="line x" title="175:254	Owing to the partial automation, the average annotation efficiency improves by 25% (from around 4 minutes to 3 minutes per sentence)." ></td>
	<td class="line x" title="176:254	6 Conclusion As the annotation scheme described ill this paper focusses on annotating argunlent structure rather than constituent trees, it differs from existing treebanks in several aspects." ></td>
	<td class="line x" title="177:254	These differences can be illustrated by a comparison with the Penn Treeba.nk annotation scheme." ></td>
	<td class="line x" title="178:254	The following features of our fornlMisrn a.re then of particular importance: * simpler (i.e. 'fiat') representation structures  complete absence of ernl.)ty categories  no special nlechanisnls tbr handling discontinuous constituency The current tagset conlprises only 16 node labels and 34 function tags, yet a. finely grained cla.ssification will take place in the nea.r future." ></td>
	<td class="line x" title="179:254	We have argued that the selected approach is better suited for producing higl, quality interpreted col pora m languages exhil)iting free constituent order." ></td>
	<td class="line x" title="180:254	In general, the resulting interpreted data also are closer to semantic annotation and more netltra.l with respect to particular synta, ctic theories." ></td>
	<td class="line x" title="181:254	As modern linguistics is a.lso becorning rnore aware of the irnportance of larger sets of m~turally occurGeneral: _Corpus: \[RefCorpus Teslkopie." ></td>
	<td class="line x" title="182:254	IE\] Editor: IThorsten JB, _Parser \[-~1 ~ei0ad Sentence: No.: 4 / 1269 Comment: I Origin: refcorp.tt Last edited: Thorsten, 07/02/97, 17:39:29 l Es o spielt PPER VVFIN 509\[~ S11 eben 2 keine 3 Rolle 's ob die 7 MusR 8 gef'allig 9 ist -,~ nuq2 etwasa ADV PlAT NN $, KOUS ART NN ADJD VAFIN $( ADV PlAT 6 10 5O5 + Neues mu',, 14 15 16 Move: Matches: 0 F_Dependency: / Selection: I ! /~ommand:L~i .__1\[ ~\] I\[ i x.ou,, i -Paren tlabel: Node no.: Parent!abel:IlNext II Prey 1\['~ JB \[ Switching to sentence no. 4 Done." ></td>
	<td class="line x" title="183:254	J Figure 1: Screen dump of the annotation tool ring data, interpreted corpora, are a valuable resource for theoreticzd and descriptive linguistic research." ></td>
	<td class="line x" title="184:254	In a.ddition the a.t~proach provides empirical material lot psycholinguistic investigation, since preferences for the choice of certain syntactic constructions, linea.rizations, and atta.chments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alterna,tives m la.rger amounts of texts." ></td>
	<td class="line x" title="185:254	Syntactically a.nnotated corpora of German haze been missing until now." ></td>
	<td class="line x" title="186:254	In the second phase of the project Verbnmbi\] a. treebank for 30,000 German spoken sentences a.s well a.s for the S~tllle anlounl, of English ~md .\]apanese sentences will be created." ></td>
	<td class="line x" title="187:254	We will closely coordinate the further develolmlent of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation." ></td>
	<td class="line x" title="188:254	Since the combinatorics of syntactic constructions crea.tes a demand tbr very large corpora, efficiency of annotation is an important criterion tbr the success of the developed methodology a.nd tools." ></td>
	<td class="line x" title="189:254	Our annotation tool supplies efficient ma.nipulation and immediate visualization of argument structures." ></td>
	<td class="line x" title="190:254	Partial automation included it, the current version significantly reduces the manual effort." ></td>
	<td class="line x" title="191:254	Its extension is subject to fllrther investigations." ></td>
	<td class="line x" title="192:254	7 Acknowledgements This work is part of the DFG Somlerforschungsbereich 378 Re.~o'urc~-Adaptrm Coguitiv~, Proc~:s.~e~, Project (;3 Conc,:r'r~',.t Gramm.ar Proces.~ug." ></td>
	<td class="line x" title="193:254	We wish to thank Ta,nia, Avgustinova, Berthold Crysmann, La.rs Konieczny, Stephan Oepen, Karel Oliva, Christian Wei6 and two anonymous reviewers {'or their help:\[ul comments on the content of this paper." ></td>
	<td class="line x" title="194:254	We also wish to thank Robert Maclntyre and Ann Taylor for valualde discussions on the Penn Treebank annotation." ></td>
	<td class="line x" title="195:254	Special thanks go to Oliver 93 Plaehn, who implemented the annotation tool, and to our fearless annotators Roland Hendriks, Kerstin K15ckner, Thomas Schulz, and Bernd-Paul Simon." ></td>
	<td class="line x" title="196:254	94 Sie PPER E~ Schade ADJD wurde VAFIN E~  da's $." ></td>
	<td class="line x" title="197:254	KOUS Figure 2: ff E E kein Arzl anwesend ist der sich PlAT NN ADJD VAFIN $, PRELS PRF Headed a,nd non-hea,ded structures, ext, ral.)osition auskennl VVFIN von preu'sischen APPR ADJA  Truppen besetzt und NN VVPP KON 1887 dem preu'sischen CARD ART ADJA + Staatsverband NN t angeglieded WPP Figure." ></td>
	<td class="line x" title="198:254	3: (',oordina,tion EF PPER bat VVFIN I I I I reich zu kommen PPER PTKZU VVINF Figure 4: Equi construction 95 References Ann Bies et al. 1995." ></td>
	<td class="line x" title="199:254	BTuck~t, ing Guidelin~:.~ for Treebank H Slyh' Penn Treebank Project." ></td>
	<td class="line x" title="200:254	Technical report, University of Pennsylvania." ></td>
	<td class="line x" title="201:254	Ezra Black et al. 1996." ></td>
	<td class="line x" title="202:254	Beyond Skeleton Parsing: Producing a Comprehensive Large-Scale General-English Treebank With Full Grammatical Analysis." ></td>
	<td class="line x" title="203:254	In The 16th International Conference on Computational Linguistics, pages 107-113, Copenhagen, Denmark." ></td>
	<td class="line x" title="204:254	Doug Cutting, Julian Kupiec, Jan Pedersen, and Penelope Sibun." ></td>
	<td class="line x" title="205:254	1992." ></td>
	<td class="line x" title="206:254	A practical part-of-speech tagger." ></td>
	<td class="line x" title="207:254	In Proceedings of the 3rd Conference on Applied Natural Language Processing (ACL), pages 133-140." ></td>
	<td class="line x" title="208:254	David Dowty." ></td>
	<td class="line x" title="209:254	1989." ></td>
	<td class="line x" title="210:254	Towards a minimalist theory of syntactic structure." ></td>
	<td class="line x" title="211:254	In Tilburg Conference on Discontinuous Constituency." ></td>
	<td class="line x" title="212:254	Helmut Feldweg." ></td>
	<td class="line x" title="213:254	1995." ></td>
	<td class="line x" title="214:254	Implementation and evaluation of a German HMM for POS disambiguation." ></td>
	<td class="line x" title="215:254	In Proceedings of EACL-SIGDAT-95 Workshop, Dublin, Ireland." ></td>
	<td class="line x" title="216:254	Richard Hudson." ></td>
	<td class="line x" title="217:254	1984." ></td>
	<td class="line x" title="218:254	Word Grammar." ></td>
	<td class="line x" title="219:254	Basil Blackwell Ltd. Fred Karlsson, Atro Voutilainen, J uha Heikkila, and Arto Anttila." ></td>
	<td class="line x" title="220:254	1995." ></td>
	<td class="line x" title="221:254	(,'onstrai,.~ G'rammar." ></td>
	<td class="line x" title="222:254	A Language-Independent System for Parsing Unre,slricted Text." ></td>
	<td class="line x" title="223:254	Mouton de Gruyter, Berlin, New York." ></td>
	<td class="line x" title="224:254	Kathol, Andreas and Carl Pollard." ></td>
	<td class="line x" title="225:254	1995." ></td>
	<td class="line x" title="226:254	Extra position via Complex Domain Formation." ></td>
	<td class="line x" title="227:254	In Proceedings of the 33rd Annual Meeting of the ACL, pages 174-180, Cambridge, MA." ></td>
	<td class="line x" title="228:254	Association for Computational Linguistics." ></td>
	<td class="line x" title="229:254	Sabine Lehmann et al. 1996." ></td>
	<td class="line x" title="230:254	TSNLP Test Suites for Natural Language Processing." ></td>
	<td class="line x" title="231:254	In The 16th lnternational Conference on Computational Linguistics, pages 711-717, Copenhagen, Denmark." ></td>
	<td class="line x" title="232:254	Mitchell Marcus et al. 1994." ></td>
	<td class="line x" title="233:254	The Penn Treebank: Annotating Predicate Argument Structure." ></td>
	<td class="line x" title="234:254	In Proceedings of the Human Language Technology Workshop, San Francisco." ></td>
	<td class="line x" title="235:254	Morgan Kaufmann." ></td>
	<td class="line x" title="236:254	James McCawley." ></td>
	<td class="line x" title="237:254	1987." ></td>
	<td class="line x" title="238:254	Some additional evidence for discontimfity." ></td>
	<td class="line x" title="239:254	In Huck and Ojeda (eds.), Discontinuous Const.iluency: Synl.a.v and Semanf.ies, pp 185-200." ></td>
	<td class="line x" title="240:254	New York, Academic Press." ></td>
	<td class="line x" title="241:254	Mike Reape." ></td>
	<td class="line x" title="242:254	1993." ></td>
	<td class="line x" title="243:254	A Formal Theory o\] Word Ord~:r: A Ca.s~,gtudy iTt W~st. G~.r'm.,nw." ></td>
	<td class="line x" title="244:254	PhD." ></td>
	<td class="line x" title="245:254	thesis, University of Edinburgh." ></td>
	<td class="line x" title="246:254	Geoffrey Sampson." ></td>
	<td class="line x" title="247:254	1995." ></td>
	<td class="line x" title="248:254	E,gli.~h \]'or th~ Computer." ></td>
	<td class="line x" title="249:254	The SUSANNE Corp',~ and Analytic 5'cheme." ></td>
	<td class="line x" title="250:254	Clarendon Press, Oxford." ></td>
	<td class="line x" title="251:254	Atro Voutilainen." ></td>
	<td class="line x" title="252:254	1994." ></td>
	<td class="line x" title="253:254	Designing a Parsing Grammar. University of Helsinki, Dept. of General Linguistics." ></td>
	<td class="line x" title="254:254	Publications No. 22 ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="A97-1017
Probabilistic And Rule-Based Tagger Of An Inflective Language - A Comparison
Haji8d>, Jan;Hladka, Barbora;"></td>
	<td class="line x" title="1:179	Probabilistic and Rule-Based Tagger of an Inflective Languagea Comparison Jan Haji~ and Barbora Hladk~i Institute of Formal and Applied Linguistics Faculty of Mathematics and Physics Malostransk~ nm. 25 CZ-118 00 Prague 1 e-maih (hajic,hladka} ~ufal.mff.cuni.cz Abstract We present results of probabilistic tagging of Czech texts in order to show how these techniques work for one of the highly morphologically ambiguous inflective languages." ></td>
	<td class="line x" title="2:179	After description of the tag system used, we show the results of four experiments using a simple probabilistic model to tag Czech texts (unigram, two bigram experiments, and a trigram one)." ></td>
	<td class="line x" title="3:179	For comparison, we have applied the same code and settings to tag an English text (another four experiments) using the same size of training and test data in the experiments in order to avoid any doubt concerning the validity of the comparison." ></td>
	<td class="line x" title="4:179	The experiments use the source channel model and maximum likelihood training on a Czech handtagged corpus and on tagged Wall Street Journal (WSJ) from the LDC collection." ></td>
	<td class="line x" title="5:179	The experiments show (not surprisingly) that the more training data, the better is the success rate." ></td>
	<td class="line x" title="6:179	The results also indicate that for inflective languages with 1000+ tags we have to develop a more sophisticated approach in order to get closer to an acceptable error rate." ></td>
	<td class="line x" title="7:179	In order to compare two different approaches to text tagging -statistical and rule-based -we modified Eric Brill's rule-based part of speech tagger and carried out two more experiments on the Czech data, obtaining similar results in terms of the error rate." ></td>
	<td class="line x" title="8:179	We have also run three more experiments with greatly reduced tagset to get another comparison based on similar tagset size." ></td>
	<td class="line x" title="9:179	1 INTRODUCTION Languages with rich inflection like Czech pose a special problem for morphological disambiguation (which is usually called tagging1)." ></td>
	<td class="line x" title="10:179	For example, the ending '-u' is not only highly ambiguous, but at the same time it carries complex information: it corresponds to the genitive, the dative and the locative singular for inanimate nouns, or the dative singular for animate nouns, or the accusative singular for feminine nouns, or the first person singular present tense active participle for certain verbs." ></td>
	<td class="line x" title="11:179	There are two different techniques for text tagging: a stochastic technique and a rule-based technique." ></td>
	<td class="line x" title="12:179	Each approach has some advantages -for stochastic techniques there exists a good theoretical framework, probabilities provide a straightforward way how to disambiguate tags for each word and probabilities can be acquired automatically from the data; for rule-based techniques the set of meaningful rules is automatically acquired and there exists an easy way how to find and implement improvements of the tagger." ></td>
	<td class="line x" title="13:179	Small set of rules can be used, in contrast to the large statistical tables." ></td>
	<td class="line x" title="14:179	Given the success of statistical methods in different areas, including text tagging, given the very positive results of English statistical taggers and given the fact that there existed no statistical tagger for any Slavic language we wanted to apply statistical methods even for the Czech language although it exhibits a rich inflection accompanied by a high degree of ambiguity." ></td>
	<td class="line x" title="15:179	Originally, we expected that the result would be plain negative, getting no more than about two thirds of the tags correct." ></td>
	<td class="line x" title="16:179	However, as we show below, we got better results than we had expected." ></td>
	<td class="line x" title="17:179	We used the same statistical approach to tag both the English text and the Czech text." ></td>
	<td class="line x" title="18:179	For English, we obtained results comparable with the results presented in (Merialdo, 1992) as well as in (Church, 1992)." ></td>
	<td class="line x" title="19:179	For Czech, we obtained results which are less satisfactory than those for English." ></td>
	<td class="line x" title="20:179	Given the comparability of the accuracy of the rule-based part-of-speech (POS) tagger (Brill, 1992) with the accuracy of the stochastic tagIThe development of automatic tagging of Czech is/was supported fully or partially by the following grants/projects: Charles University GAUK 39/94, Grant Agency of the Czech Republic GACR 405/96/K214 and Ministry of Education VS96151." ></td>
	<td class="line x" title="21:179	111 ger and given the fact that a rule-based POS tagger has never been used for a Slavic language we have tried to apply rule-based methods even for Czech." ></td>
	<td class="line x" title="22:179	2 STATISTICAL EXPERIMENTS 2.1 CZECH EXPERIMENTS 2.1.1 CZECH TAGSET Czech experiment is based upon ten basic POS classes and the tags describe the possible combinations of morphological categories for each POS class." ></td>
	<td class="line x" title="23:179	In most cases, the first letter of the tag denotes the part-of-speech; the letters and numbers which follow it describe combinations of morphological categories (for a detailed description, see Table 2.1 and Table Cat." ></td>
	<td class="line x" title="24:179	Var." ></td>
	<td class="line x" title="25:179	see Tab." ></td>
	<td class="line x" title="26:179	2.2) g 2.2)." ></td>
	<td class="line x" title="27:179	Morph." ></td>
	<td class="line x" title="28:179	Categ." ></td>
	<td class="line x" title="29:179	Poss." ></td>
	<td class="line x" title="30:179	Description Val." ></td>
	<td class="line x" title="31:179	gender M masc." ></td>
	<td class="line x" title="32:179	anim." ></td>
	<td class="line x" title="33:179	I masc." ></td>
	<td class="line x" title="34:179	inanim." ></td>
	<td class="line x" title="35:179	N neuter F feminine number n S singular P plural tense t M past P present F future mood m O indicative R imperative case c 1 nominative 2 genitive 3 dative 4 accusative 5 vocative 6 locative 7 instrumental voice s A active voice P passive voice polarity a N negative A affirmative deg." ></td>
	<td class="line x" title="36:179	of comp." ></td>
	<td class="line x" title="37:179	d 1 base form 2 comparative 3 superlative person p 1 1st 2 2nd 3 3rd Table 2.1 Note especially, that Czech nouns are divided into four classes according to gender (Sgall, 1967) and into seven classes according to ease." ></td>
	<td class="line x" title="38:179	POS Class nouns noun, abbreviations adjectives Ngnc NZ Agncda verbs, infinitives VTa verbs, transgressives VWntsga verbs, common Vpnstmga pronouns, personal PPpnc pronouns, 3rd person PP3gnc pronouns, possessive PRgncpgn 'svfij' --'his' referring to PSgnc subject reflexive particle 'se' PEc pronouns, demonstrative PDgnca adverbs Od a conjunctions S numerals C gnc prepositions Rpreposition interjections F particles K sentence boundaries T_SB punctuation T_IP unknown tag X Table 2.2 Not all possible combinations of morphological categories are meaningful, however." ></td>
	<td class="line x" title="39:179	In addition to these usual tags we have used special tags for sentence boundaries, punctuation and a so called 'unknown tag'." ></td>
	<td class="line x" title="40:179	In the experiments, we used only those tags which occurred at least once in the training corpus." ></td>
	<td class="line x" title="41:179	To illustrate the form of the tagged text, we present here the following examples from our training data, with comments: word Itag #comments doIRdo #'to' (prepositions have their own individuals tags) oddflulNIS2 #'unit' (noun, masculine inanimate, singular, genitive) kiRk snfdanilNFS3 pou#,ijeIV3SAPOMA prolRpro ns\[PP1P4 ~:' for' (preposition) ~' breakfast' (noun, feminine, singular, dative) ~' uses' (verb, 3rd person, singular, active, present, indicative, masc." ></td>
	<td class="line x" title="42:179	animate, affirmative) #'for' (preposition) ~' US' (pronoun, personal, 1st person, plural, accusative) 112 2.1.2 CZECH TRAINING DATA For training, we used the corpus collected during the 1960's and 1970's in the Institute for Czech Language at the Czechoslovak Academy of Sciences." ></td>
	<td class="line x" title="43:179	The corpus was originally hand-tagged, including the lemmatization and syntactic tags." ></td>
	<td class="line x" title="44:179	We had to do some cleaning, which means that we have disregarded the lemmatization information and the syntactic tag, as we were interested in words and tags only." ></td>
	<td class="line x" title="45:179	Tags used in this corpus were different from our suggested tags: number of morphological categories was higher in the original sample and the notation was also different." ></td>
	<td class="line x" title="46:179	Thus we had to carry out conversions of the original data into the format presented above, which resulted in the so-called Czech 'modified' corpus, with the following features: tokens 621 015 words 72 445 tags 1 171 average number of tags per token 3.65 Table 2.3 V~Te used the complete 'modified' corpus (621015 tokens) in the experiments No. 1, No. 3, No. 4 and a small part of this corpus in the experiment No. 2, as indicated in Table 2.4." ></td>
	<td class="line x" title="47:179	tokens 110 874 words 22 530 tags 882 average number of tags per token 2.36 Table 2.4 2.2 ENGLISH EXPERIMENTS 2.2.1 ENGLISH TAGSET For the tagging of English texts, we used the Penn Treebank tagset which contains 36 POS tags and 12 other tags (for punctuation and the currency symbol)." ></td>
	<td class="line x" title="48:179	A detailed description is available in (Santorini, 1990)." ></td>
	<td class="line x" title="49:179	2.2.2 ENGLISH TRAINING DATA For training in the English experiments, we used WSJ (Marcus et al. , 1993)." ></td>
	<td class="line x" title="50:179	We had to change the format of WSJ to prepare it for our tagging software." ></td>
	<td class="line x" title="51:179	V~e used a small (100k tokens) part of WSJ in the experiment No. 6 and the complete corpus (1M tokens) in the experiments No. 5, No. 7 and No. 8." ></td>
	<td class="line x" title="52:179	Table 2.5 contains the basic characteristics of the training data." ></td>
	<td class="line x" title="53:179	tokens words tags average number of tags per token Experiment Experiments No. 6 No. 5, No. 7, No. 8 110 530 1 287 749 13 582 51 433 45 45 1.72 2.34 Table 2.5 2.3 CZECH VS ENGLISH Differences between Czech as a morphologically ambiguous inflective language and English as language with poor inflection are also reflected in the number of tag bigrams and tag trigrams." ></td>
	<td class="line x" title="54:179	The figures given in Table 2.6 and 2.7 were obtained from the training files." ></td>
	<td class="line x" title="55:179	Czech WSJ corpus x<=4 24 064 x<--10 459 4<x<=16 5 577 10<x<--100 411 16<x<=64 2 706 100<x<=1000 358 x>64 1 581 x>1000 225 bigrams 33 928 bigrams 1 453 Table 2.6 Number of bigrams with frequency x x<----4 4<x<=16 Czech corpus 155 399 16 371 x<=lO 10<x<=100 WSJ 11 810 4 571 16<x<=64 4 380 100<x<=1000 1 645 x>64 933 x> 1000 231 trigrams 177 083 trigrams 18 257 Table 2.7 Number of trigrams with frequency x It is interesting to note the frequencies of the most ambiguous tokens encountered in the whole 'modified' corpus and to compare them with the English data." ></td>
	<td class="line x" title="56:179	Table 2.8 and Table 2.9 contain the first tokens with the highest number of possible tags in the complete Czech 'modified' corpus and in the complete WSJ." ></td>
	<td class="line x" title="57:179	Token Frequency #tags in train, data in train, data jejich 1 087 51 jeho 1 087 46 jeho~ 163 35 jejich~ 150 25 vedoucl 193 22 Table 2.8 In the Czech 'modified' corpus, the token 'vedouc/' appeared 193 times and was tagged by twenty two different tags: 13 tags for adjective and 9 tags 113 for noun." ></td>
	<td class="line x" title="58:179	The token 'vedoucf' means either: 'leading' (adjective) or 'manager' or 'boss' (noun)." ></td>
	<td class="line x" title="59:179	The following columns represent the tags for the token 'vedouc/' and their frequencies in the training data; for example 'vedoucf' was tagged twice as adjective, feminine, plural, nominative, first degree, affirmative." ></td>
	<td class="line x" title="60:179	2 4 6 11 1 4 5 2 11 3 12 2 2 vedouci\[AFPllA vedouci\[AFP41A vedoucl AFSllA vedouci AFS21A vedouci AFS31A vedoue~ AFS41A vedouci AFS71A vedoucl AIPllA vedoucl A M P 11A vedouc AMP41A vedoucl AMSllA vedoucl ANPllA vedoucl ANS41A 10 vedouci 1 vedouci 1 vedouci 1 vedoud 2 vedoucl 34 vedouci 17 vedouci 61 vedouc~ 1 vedouci NFS1 NFS2 NFS3 NFS4 NFS7 NMP1 NMP4 NMS1 NMS5 Token Frequency #tags in train, data in train, data a 25 791 7 down 1 052 7 put 380 6 set 362 6 that 10 902 6 the 56 265 6 Table 2.9 It is clear from these figures that the two languages in question have quite different properties and that nothing can be said without really going through an experiment." ></td>
	<td class="line x" title="61:179	2.4 THE ALGORITHM We have used the basic source channel model (described e.g. in (Merialdo, 1992))." ></td>
	<td class="line x" title="62:179	The tagging procedure  selects a sequence of tags T for the sentence W:  : PV --+ T. In this case the optimal tagging procedure is (W) -argmaxTPr(T\[W) = : argmaxTPr(TlW) * Pr(W) = = argrnaxTPr(W,T) = -argmaxTPr(W\[T) * Pr(T)." ></td>
	<td class="line x" title="63:179	Our implementation is based on generating the (W,T) pairs by means of a probabilistic model using approximations of probability distributions Pr(WIT) and Pr(T)." ></td>
	<td class="line x" title="64:179	The Pr(T) is based on tag bigrams and trigrams, and Pr(WIT ) is approximated as the product of Pr(wi\[tl)." ></td>
	<td class="line x" title="65:179	The parameters have been estimated by the usual maximum likelihood training method, i.e. we approximated them as the relative frequencies found in the training data with smoothing based on estimated unigram probability and uniform distributions." ></td>
	<td class="line x" title="66:179	2.5 THE RESULTS The results of the Czech experiments are displayed in Table 2.10." ></td>
	<td class="line x" title="67:179	No. 1 No. 2 No. 3 No. 4 test data 1 294 1 294 1 294 1 294 (tokens) prob." ></td>
	<td class="line x" title="68:179	unigram bigram bigram trigram model incorrect tags tagging accuracy 444 65.70% 334 74.19% 239 81.53% Table 2.10 244 81.14% These results show, not surprisingly, of course, that the more data, the better (results experiments of No.2 vs. No.3), but in order to get better results for a trigram tag prediction model, we would need far more data." ></td>
	<td class="line x" title="69:179	Clearly, if 88% trigrams occur four times or less, then the statistics is not reliable." ></td>
	<td class="line x" title="70:179	The following tables show a detailed analysis of the errors of the trigram experiment." ></td>
	<td class="line x" title="71:179	\[ \[\[A IC \[F \]K IN lO A 32 0 0 0 6 3 C 0 4 0 0 1 0 F 0 0 0 0 0 0 K 0 0 0 0 0 0 N 4 0 0 0 64 8 O 0 0 0 0 1 0 P 0 0 0 0 0 3 R 0 0 0 0 1 1 S 0 0 0 0 0 0 V 0 0 0 0 3 8 T 0 0 0 0 1 0 X 0 0 0 0 0 0 Table 2.11a I\] P \[ R I s I V I T I X I A 2 2 2 2 1 0 50 C 0 0 0 0 0 0 5 F 0 0 0 0 0 0 0 K 0 0 1 0 0 1 2 N 0 4 2 2 5 4 93 O 0 0 0 1 1 0 3 P 19 0 0 0 1 2 23 R 0 0 0 0 0 2 4 S 0 0 0 0 0 2 2 V 0 3 8 28 1 2 53 T 0 0 0 0 0 0 1 X 5 0 1 2 0 0 8 Table 2.11b The letters in the first column and row denote POS classes, the interpunction (T) and the 'unknown tag' (X)." ></td>
	<td class="line x" title="72:179	The numbers show how many times the tagger assigned an incorrect POS tag to a token in the test file." ></td>
	<td class="line x" title="73:179	The total number of errors was 244." ></td>
	<td class="line x" title="74:179	Altogether, fifty times the adjectives (A) were 114 tagged incorrectly, nouns (N) 93 times, numbers (C) 5 times and etc.(see the last unmarked column in Table 2.11b); to provide a better insight, we should add that in 32 cases, when the adjective was correctly tagged as an adjective, but the mistakes appeared in the assignment of morphological categories (see Table 2.12), 6 times the adjective was tagged as a noun, twice as a pronoun, 3 times as an adverb and so on (see the second row in Table 2.11a)." ></td>
	<td class="line x" title="76:179	A detailed look at Table 2.12 reveals that for 32 correctly marked adjectives the mistakes was 17 times in gender, once in number, three times in gender and case simultaneously and so on." ></td>
	<td class="line x" title="77:179	\[ A\[\[ g \[ n \[ c I g&~ g&:~ c&:~ g&n~zc\[ g~zc&:d\[ 1321117\]1161 3 I 2 I 1 I 1 I 1 I Table 2.12 Similar tables can be provided for nouns (Table 2.13), numerals (Table 2.14), pronouns(Table 2.15) and verbs (Table 2.16a, Table 2.16b)." ></td>
	<td class="line x" title="78:179	N l\[ g In t c I g&c \[ n&c I ->NZ \] 64 \[\[ 11 \[ 5 \[ 41 \[ 2 \[ 4 \[ 1 \] Table 2.13 Cllg c 4 \[\[1 3 Table 2.14 P Ilg c g&clVD->PV 19ll8 7 3 I 1 Table 2.15 V I P t n Is I n&t I p&t t&a I 22\]3 6 5151 1 I 1 1 I Table 2.16a v II gt~a I pan~t I v->VT 6 II 1 I1 \]4 Table 2.16b The results of our experiments with English are displayed in Table 2.17." ></td>
	<td class="line x" title="79:179	test data (tokens) INo5 1 294 No. 6 1 294 INo." ></td>
	<td class="line x" title="80:179	7 1 294 No. 8 1 294 prob." ></td>
	<td class="line x" title="81:179	unigram bigram bigram trigram model 136 89.5% incorrect tags 41 96.83% tagging accuracy 81 93.74% 37 97.14% Table 2.17 To illustrate the results of our tagging experiments, we present here short examples taken from the test data." ></td>
	<td class="line x" title="82:179	Cases of incorrect tag assignment are in boldface." ></td>
	<td class="line x" title="83:179	-Czech word\[hand tag exp. exp. exp. exp. No.4 No.3 No.2 No.12 na\[Rna Rna Rna pfid~\[NFS6 NFS6 NFS6 vlasti\[NFS2 NFS2 NFS2 rady\[NFS2 NFS2 NFS2 ~en\[NFP2 NFP2 NFP2 Gusta\[NFS1 T_SB T_SB Fu~ov\[NFS1 NFS1 NFS1 a\[SS SS SS p~edseda\[NMS1 NMS1 NMS1 dv\[NZ NZ NZ ssm\[NZ NZ NZ Juraj\[NMS1 NMS1 NMS1 Varhol~\[NMS1 NMS1 NMS1 -English word \[ hand tag Rna Rna NFS6 NFS6 NFS2 NFS2 NFS2 NFS2 NFP2 NFP2 AFP21A XX NFP2 NFS1 SS SS NMS1 NMS1 NZ NZ NZ NZ NMS1 XX NMS1 NMS1 exp. exp. exp. exp. No.8 No.7 No.6 No.5 With\[IN IN IN IN IN stock\[NN NN NN NN NN prices\[NNS NNS NNS NNS NNS hovering\[VBG VBG VBG IN VBG near\[IN IN IN JJ IN record\[NN NN NN NN NN levels\[NNS NNS NNS NNS NNS,\[, alDT fiT fiT DT DT number\[NN NN NN NN NN of\[IN IN IN IN IN companieslNNS NNS NNS NNS NNS have\[VBP VBP VBP VBP VBP been\[VBN VBN VBN VBN VBN announcing\[VBG VBG VBG IN VBG stock\[NN NN NN NN NN splits\]NNS NNS VBZ NN VBZ .\[." ></td>
	<td class="line x" title="84:179	2.6 A PROTOTYPE OF RANK XEROX POS TAGGER FOR CZECH (Schiller, 1996) describes the general architecture of the tool for noun phrase mark-up based on finitestate techniques and statistical part-of-speech disambiguation for seven European languages." ></td>
	<td class="line oc" title="85:179	For Czech, we created a prototype of the first step of this process -the part-of-speech (POS) tagger -using Rank Xerox tools (Tapanainen, 1995), (Cutting et al. , 1992)." ></td>
	<td class="line x" title="86:179	2.6.1 POS TAGSET The first step of POS tagging is obviously a definition of the POS tags." ></td>
	<td class="line x" title="87:179	We performed three ex2We used a speciM tag XX for unknown words." ></td>
	<td class="line x" title="88:179	115 periments." ></td>
	<td class="line x" title="89:179	These experiments differ in the POS tagset." ></td>
	<td class="line x" title="90:179	During the first experiment we designed tagset which contains 47 tags." ></td>
	<td class="line x" title="91:179	The POS tagset can be described as follows: Category Symbol Pos." ></td>
	<td class="line x" title="92:179	Value Description case c NOM nominative GEN genitive bAT dative ACC accusative VOC vocative locative kind verb LOC INS INV PAP PRI INF IMP TRA Nm instrumental invariant past paticiple present participle infinitive imperative transgressive 2.6.2 RESULTS Figures representing the results of all experiments are presented in the following table." ></td>
	<td class="line o" title="93:179	We have also included the results of English tagging using the same Xerox tools." ></td>
	<td class="line x" title="94:179	language tags Czech 47 Czech 43 Czech 34 English _\[ 76 ambiguity ~ 39% 36% 14% 36% tagging accuracy 91.7% 93.0% 96.2% 97.8% Table 2.20 The results show that the more radical reduction of Czech tags (from 1171 to 34) the higher accuracy of the results and the more comparable are the Czech and English results." ></td>
	<td class="line x" title="95:179	However, the difference in the error rate is still more than visible -here we can speculate that the reason is that Czech is 'free' word order language, whereas English is not." ></td>
	<td class="line x" title="96:179	Table 2.18 POS tag Description NOUN_c nouns + case ADJ_c adjectives + case PRON_c pronouns + case NUM_c numerals + case VERB_k verbs + kind of verb ADV adverbs PROP PREP proper names prepositions PSE reflexive particles 'se' CLIT clitics CONJ INTJ conjunctions interjections PTCL particles DATE dates CM comma PUNCT interpunction SENT sentence bundaries Table 2.19 The analysis of the results of the first experiment showed very high ambiguity between the nominative and accusative cases of nouns, adjectives, pronouns and numerals." ></td>
	<td class="line x" title="97:179	That is why we replaced the tags for nominative and accusative of nouns, adjectives, pronouns and numerals by new tags NOUNANA, ADJANA, PRONANA and NUMANA (meaning nominative or accusative, undistinguished)." ></td>
	<td class="line x" title="98:179	The rest of the tags stayed unchanged." ></td>
	<td class="line x" title="99:179	This led 43 POS tags." ></td>
	<td class="line x" title="100:179	In the third experiment we deleted the morphological information for nouns and adjectives alltogether." ></td>
	<td class="line x" title="101:179	This process resulted in the final 34 POS tags." ></td>
	<td class="line x" title="102:179	3 A RULE-BASED EXPERIMENT FOR CZECH A simple rule-based part of speech (RBPOS) tagger is introduced in (Brill, 1992)." ></td>
	<td class="line x" title="103:179	The accuracy of this tagger for English is comparable to a stochastic English POS tagger." ></td>
	<td class="line x" title="104:179	From our point of view, it is very interesting to compare the results of Czech stochastic POS (SPOS) tagger and a modified RBPOS tagger for Czech." ></td>
	<td class="line x" title="105:179	3.1 TRAINING DATA We used the same corpus used in the case of the SPOS tagger for Czech." ></td>
	<td class="line x" title="106:179	RBPOS requires different input format; we thus converted the whole corpus into this format, preserving the original contents." ></td>
	<td class="line x" title="107:179	3.2 LEARNING It is an obvious fact that the Czech tagset is totally different from the English tagset." ></td>
	<td class="line x" title="108:179	Therefore, we had to modify the method for the initial guess." ></td>
	<td class="line x" title="109:179	For Czech the algorithm is: 'If the word is W_SB (sentence boundary) assign the tag T_SB, otherwise assign the tag NNSI'." ></td>
	<td class="line x" title="110:179	3.2.1 LEARNING RULES TO PREDICT THE MOST LIKELY TAG FOR UNKNOWN WORDS The first stage of training is learning rules to predict the most likely tag for unknown words." ></td>
	<td class="line x" title="111:179	These rules operate on word types; for example, if 3The percentage of ambiguous word forms in the test file." ></td>
	<td class="line x" title="112:179	116 a word ends by 'd37;, it is probably a masculine adjective." ></td>
	<td class="line x" title="113:179	To compare the influence of the size of the training files on the accuracy of the tagger we performed two subexperiments4: TAGGED-CORPUS (tokens) TAGGED-CORPUS (words) TAGGED-CORPUS (tags) No. 1 No. 2 15 297 5 031 738 495 UNTAGGED-CORPUS 621 015 621 015 (tokens) 72 445 72 445 UNTAGGED-CORPUS (words) 101 LEXRULEOUTFILE (rules) 75 Table 3.1 We present here an example of rules taken from LEXRULEOUTFILE from the exp. No. 1: u hassuf 1 NIS2 # change the tag to NIS2 if the suffix is 'u' y hassuf 1 NFS2 # change the tag to NFS2 if the suffix is 'y' ho hassuf 2 AIS21A # change the tag to AIS21A if the suffix is 'ho' ch hassuf 3 NFP6 # change the tag to NFP6 if the suffix is 'ch' nej addpref 3 O2A # change the tag to O2A if adding the prefix 'nej' results in a word 3.2.2 LEARNING CONTEXTUAL CUES The second stage of training is learning rules to improve tagging accuracy based on contextual cues." ></td>
	<td class="line x" title="114:179	These rules operate on individual word tokens." ></td>
	<td class="line x" title="115:179	4We use the same names of files and variables as Eric Brill in the rule-based POS tagger's documentation." ></td>
	<td class="line x" title="116:179	TAGGED-CORPUS -manually tagged training corpus, UNTAGGED-CORPUS -collection of all untagged texts, LEXRULEOUTFILE -the list of transformations to determine the most likely tag for unknown words, TAGGED-CORPUS-2 -manually tagged training corpus, TAGGED-CORPUS-ENTIRE -Czech 'modified' corpus (the entire manually tagged corpus), CONTEXT-RULEFILE -the list of transformations to improve accuracy based on contextual cues." ></td>
	<td class="line x" title="117:179	No. 1 No. 2 TAGGED-CORPUS-2 37 892 9 989 (tokens) TAGGED-CORPUS-2 12 676 4 635 (words) TAGGED-CORPUS-2 717 479 (tags) TAGGED-ENTIRE-CORPUS 621 015 621 015 (tokens) TAGGED-ENTIRE-CORPUS 72 445 72 445 (words) TAGGED-ENTIRE-CORPUS 1 171 1 171 (tags) CONTEXT-RULEFILE 487 61 (rules) Table 3.2 We present here an example of the rules taken from CONTEXT-RULEFILE from the exp. No. 1: AFP21A AIP21A # change the tag AFP21A to AIP21A NEXT1OR2TAG if the following tag is NIP2 NIP2 NIS2 NIS6 PREV1OR2OR3TAG Rv # change the tag NIS2 to NIS6 if the preceding tag is Rv NIS1 NIS4 # change the tag NIS1 to NIS4 PREVIOR2TAG if the preceding tag is Rna Rna 3.2.3 RESULTS The tagger was tested on the same test file as for the statistical experiments." ></td>
	<td class="line x" title="118:179	We obtained the following results: I TEST-FILE errors tagging accuracy II No. 1 No. 2 1 294 1 294 262 294 79.75% 77.28% Table 3." ></td>
	<td class="line x" title="119:179	3 4 CONCLUSION The results, though they might seem negative compared to English, are still better than our original expectations." ></td>
	<td class="line x" title="120:179	Before trying some completely different approach, we would like to improve the current simple approach by some other simple measures: adding a morphological analyzer (Hajji, 1994) as a frontend to the tagger (serving as a 'supplier' of possible tags, instead of just taking all tags occurring in the training data for a given token), simplifying the tagset, adding more data." ></td>
	<td class="line x" title="121:179	However, the desired positive effect of some of these measures is not guaranteed: for example, the average number of tags per 117 token will increase after a morphological analyzer is added." ></td>
	<td class="line x" title="122:179	Success should be guaranteed, however, by certain tagset reductions, as the original tagset (even after the reductions mentioned above) is still too detailed." ></td>
	<td class="line x" title="123:179	This is especially true when comparing it to English, where some tags represent, in fact, a set of tags to be discriminated later (if ever)." ></td>
	<td class="line x" title="124:179	For example, the tag VB used in the WSJ corpus actually means 'one of the (five different) tags for 1st person sg., 2nd person sg., 1st person pl. , etc.'." ></td>
	<td class="line x" title="127:179	First, we will reduce the tagset to correspond to our morphological analyzer which already uses a reduced one." ></td>
	<td class="line n" title="128:179	Then, the tagset will be reduced even further, but nevertheless, not as much as we did for the Xeroxtools-based experiment, because that tagset is too 'rough' for many applications, even though the results are good." ></td>
	<td class="line x" title="129:179	Regarding tagset reduction, we should note that we haven't performed a 'combined' experiment, i.e. using the full (1100+) tagset for (thus) 'intermediate' tagging, but only the reduced tagset for the final results." ></td>
	<td class="line x" title="130:179	However, it can be quite simply derived from the tables 2.10, 2.11a and 2.11b, that the error rate would not drop much: it will remain high at about 6.5070 (based on the results of experiment No. 4) using the very small tagset of 12 (= number or lines in table 2.11a) tags used for part of speech identification." ></td>
	<td class="line x" title="131:179	This is even much higher than the error rate reported here for the smallest tagset used in the 'pure' experiment (sect." ></td>
	<td class="line x" title="132:179	2.6, table 2.20), which was at 3.8~0." ></td>
	<td class="line x" title="133:179	This suggests that maybe the pure methods (which are obviously also simple to implement) are in general better than the 'combined' methods." ></td>
	<td class="line x" title="134:179	Another possibility of an improvement is to add more data to allow for more reliable trigram estimates." ></td>
	<td class="line x" title="135:179	We will also add contemporary newspaper texts to our training data in order to account for recent language development." ></td>
	<td class="line x" title="136:179	Hedging against failure of all these simple improvements, we are also working on a different model using independent predictions for certain grammatical categories (and the lemma itself), but the final shape of the model has not yet been determined." ></td>
	<td class="line x" title="137:179	This would mean to introduce constraints on possible combinations of morphological categories and take them into account when 'assembling' the final tag." ></td>
	<td class="line x" title="138:179	ACKNOWLEDGMENTS: The authors wish to thank Eva Hajidovd for her comments and suggestions and Eric BriU, Jean-Pierre Chanod and Anne Schiller who made their software tools available." ></td>
	<td class="line x" title="139:179	Eric Brill." ></td>
	<td class="line x" title="140:179	1993." ></td>
	<td class="line x" title="141:179	A Corpus Based Approach To Language Learning." ></td>
	<td class="line x" title="142:179	PhD Dissertation, Department of Computer and Information Science, University of Pennsylvania." ></td>
	<td class="line x" title="143:179	Eric Brill." ></td>
	<td class="line x" title="144:179	1994." ></td>
	<td class="line x" title="145:179	Some Advances in Transformation-Based Part of Speech Tagging." ></td>
	<td class="line x" title="146:179	In: Proceedings of the Twelfth National Conference on Artificial Intelligence." ></td>
	<td class="line x" title="147:179	Jan Hajic." ></td>
	<td class="line x" title="148:179	1994." ></td>
	<td class="line x" title="149:179	Unification Morphology Grammar. PhD Dissertation, Institute of Formal and Applied Linguistics, Charles University, Prague, Czech Republic." ></td>
	<td class="line x" title="150:179	Kenneth W. Church." ></td>
	<td class="line x" title="151:179	1992." ></td>
	<td class="line x" title="152:179	Current Practice In Part Of Speech Tagging And Suggestions For The Future." ></td>
	<td class="line x" title="153:179	For Henry Kucera, Studies in Slavic Philology and Computational Linguistics, Michigan Slavic Publications, Ann Arbor." ></td>
	<td class="line xc" title="154:179	Doug Cutting, Julian Kupiec, Jan Pedersen and Penelope Sibun 1992." ></td>
	<td class="line x" title="155:179	A Practical Part-of-Speech Tagger." ></td>
	<td class="line x" title="156:179	In: Proceedings of the Third Conference on Applied Natural Language Processing, Trento, Italy." ></td>
	<td class="line x" title="157:179	Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz 1993." ></td>
	<td class="line x" title="158:179	Building A Large Annotated Corpus Of English: The Penn Treebank." ></td>
	<td class="line x" title="159:179	Computational Linguistics, 19(2):313-330." ></td>
	<td class="line x" title="160:179	Bernard Merialdo." ></td>
	<td class="line x" title="161:179	1992." ></td>
	<td class="line x" title="162:179	Tagging Text With A Probabilistie Model." ></td>
	<td class="line x" title="163:179	Computational Linguistics, 20(2):155--171 Beatrice Santorini." ></td>
	<td class="line x" title="164:179	1990." ></td>
	<td class="line x" title="165:179	Part Of Speech Tagging Guidelines For The Penn Treebank Project." ></td>
	<td class="line x" title="166:179	Technical report MS-CIS-90-47, Department of Computer and Information Science, University of Pennsylvania." ></td>
	<td class="line x" title="167:179	Anne Schiller." ></td>
	<td class="line x" title="168:179	1996." ></td>
	<td class="line x" title="169:179	Multilingual Finite-State Noun Phrase Extraction." ></td>
	<td class="line x" title="170:179	ECAI'96, Budapest, Hungary." ></td>
	<td class="line x" title="171:179	Petr Sgall." ></td>
	<td class="line x" title="172:179	1967." ></td>
	<td class="line x" title="173:179	The Generative Description of a Language and the Czech Declension (In Czech)." ></td>
	<td class="line x" title="174:179	Studie a prdce lingvistickd, 6." ></td>
	<td class="line x" title="175:179	Prague." ></td>
	<td class="line x" title="176:179	Pasi Tapanalnen." ></td>
	<td class="line x" title="177:179	1995." ></td>
	<td class="line x" title="178:179	RXRC Finite-State Compiler." ></td>
	<td class="line x" title="179:179	Technical Report MLTT-20, Rank Xerox Research Center, Meylen, France." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="J97-3003
Automatic Rule Induction For Unknown-Word Guessing
Mikheev, Andrei;"></td>
	<td class="line x" title="1:319	Automatic Rule Induction for Unknown-Word Guessing Andrei Mikheev' University of Edinburgh Words unknown to the lexicon present a substantial problem to NLP modules that rely on morphosyntactic information, such as part-of-speech taggers or syntactic parsers." ></td>
	<td class="line x" title="2:319	In this paper we present a technique for fully automatic acquisition of rules that guess possible part-of-speech tags for unknown words using their starting and ending segments." ></td>
	<td class="line x" title="3:319	The learning is performed from a general-purpose lexicon and word frequencies collected from a raw corpus." ></td>
	<td class="line x" title="4:319	Three complimentary sets of word-guessing rules are statistically induced: prefix morphological rules, suffix morphological rules and ending-guessing rules." ></td>
	<td class="line x" title="5:319	Using the proposed technique, unknown-word-guessing rule sets were induced and integrated into a stochastic tagger and a rule-based tagger, which were then applied to texts with unknown words." ></td>
	<td class="line x" title="6:319	1." ></td>
	<td class="line x" title="7:319	Introduction Words unknown to the lexicon present a substantial problem to NLP modules (as, for instance, part-of-speech (pos-) taggers) that rely on information about words, such as their part of speech, number, gender, or case." ></td>
	<td class="line x" title="8:319	Taggers assign a single POS-tag to a word-token, provided that it is known what Pos-tags this word can take on in general and the context in which this word was used." ></td>
	<td class="line x" title="9:319	A Pos-tag stands for a unique set of morpho-syntactic features, as exemplified in Table 1, and a word can take several Pos-tags, which constitute an ambiguity class or POS-class for this word." ></td>
	<td class="line x" title="10:319	Words with their POs-classes are usually kept in a lexicon." ></td>
	<td class="line x" title="11:319	For every input word-token, the tagger accesses the lexicon, determines possible POS-tags this word can take on, and then chooses the most appropriate one." ></td>
	<td class="line x" title="12:319	However, some domain-specific words or infrequently used morphological variants of general-purpose words can be missing from the lexicon and thus, their POs-classes should be guessed by the system and only then sent to the disambiguation module." ></td>
	<td class="line x" title="13:319	The simplest approach to POS-class guessing is either to assign all possible tags to an unknown word or to assign the most probable one, which is proper singular noun for capitalized words and common singular noun otherwise." ></td>
	<td class="line x" title="14:319	The appealing feature of these approaches is their extreme simplicity." ></td>
	<td class="line x" title="15:319	Not surprisingly, their performance is quite poor: if a word is assigned all possible tags, the search space for the disambiguation of a single POS-tag increases and makes it fragile; if every unknown word is classified as a noun, there will be no difficulties for disambiguation but accuracy will suffer--such a guess is not reliable enough." ></td>
	<td class="line x" title="16:319	To assign capitalized unknown words the category proper noun seems a good heuristic, but may not always work." ></td>
	<td class="line x" title="17:319	As argued in Church (1988), who proposes a more elaborated heuristic, Dermatas and Kokkinakis (1995) proposed a simple probabilistic approach to unknown-word guessing: HCRC, Language Technology Group, University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9LW, Scotland, UK." ></td>
	<td class="line x" title="18:319	Q 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 3 Table 1 The most frequent open-class tags from the Penn tag set." ></td>
	<td class="line x" title="19:319	Tag Meaning Example Tag Meaning Example NN common noun table NNS noun plural tables NNP proper noun John NNPS plural proper noun Vikings JJ adjective green RB adverb naturally VB verb base form take VBD verb past took VBG gerund taking VBN past participle taken VBZ verb present, 3d person takes VBP verb, present, non-3d take the probability that an unknown word has a particular Pos-tag is estimated from the probability distribution of hapax words (words that occur only once) in the previously seen texts." ></td>
	<td class="line x" title="20:319	1 Whereas such a guesser is more accurate than the naive assignments and easily trainable, the tagging performance on unknown words is reported to be only about 66% correct for English." ></td>
	<td class="line x" title="21:319	2 More advanced word-guessing methods use word features such as leading and trailing word segments to determine possible tags for unknown words." ></td>
	<td class="line x" title="22:319	Such methods can achieve better performance, reaching tagging accuracy of up to 85% on unknown words for English (Brill 1994; Weischedel et al. 1993)." ></td>
	<td class="line oc" title="23:319	The Xerox tagger (Cutting et al. 1992) comes with a set of rules that assign an unknown word a set of possible pos-tags (i.e. , POS-class) on the basis of its ending segment." ></td>
	<td class="line x" title="24:319	We call such rules endingguessing rules because they rely only on ending segments in their predictions." ></td>
	<td class="line x" title="25:319	For example, an ending-guessing rule can predict that a word is a gerund or an adjective if it ends with ing." ></td>
	<td class="line x" title="26:319	The ending-guessing approach was elaborated in Weischedel et al.(1993), where an unknown word was guessed by using the probability for an unknown word to be of a particular Pos-tag, given its capitalization feature and its ending." ></td>
	<td class="line x" title="28:319	Brill (1994, 1995) describes a system of rules that uses both ending-guessing and more morphologically motivated rules." ></td>
	<td class="line x" title="29:319	A morphological rule, unlike an ending-guessing rule, uses information about morphologically related words already known to the lexicon in its prediction." ></td>
	<td class="line x" title="30:319	For instance, a morphologically motivated guessing rule can say that a word is an adjective if adding the suffix ly to it will result in a word." ></td>
	<td class="line x" title="31:319	Clearly, ending-guessing rules have wider coverage than morphologically oriented ones, but their predictions can be less accurate." ></td>
	<td class="line x" title="32:319	The major topic in the development of word-Pos guessers is the strategy used for the acquisition of the guessing rules." ></td>
	<td class="line x" title="33:319	A rule-based tagger described in Voutilainen (1995) was equipped with a set of guessing rules that had been hand-crafted using knowledge of English morphology and intuitions." ></td>
	<td class="line x" title="34:319	A more appealing approach is automatic acquisition of such rules from available lexical resources, since it is usually less labor-intensive and less error-prone." ></td>
	<td class="line x" title="35:319	Zhang and Kim (1990) developed a system for automated learning of morphological word formation rules." ></td>
	<td class="line x" title="36:319	This system divides a string into three regions and infers from training examples their correspondence to underlying morphological features." ></td>
	<td class="line x" title="37:319	Kupiec (1992) describes a guessing component that uses a prespecified list of suffixes (or rather endings) and then statistically learns the 1 A similar idea for estimating lexical prior probabilities for unknown words was suggested in Baayen and Sproat (1995)." ></td>
	<td class="line x" title="38:319	2 The best result was detected for GermanM2% accuracy and the worst result for Italian--50% accuracy." ></td>
	<td class="line x" title="39:319	406 Andrei Mikheev Unknown-Word Guessing predictive properties of those endings from an untagged corpus." ></td>
	<td class="line x" title="40:319	In Brill (1994, 1995) a transformation-based learner that learns guessing rules from a pretagged training corpus is outlined: First the unknown words are labeled as common nouns and a list of generic transformations is defined." ></td>
	<td class="line x" title="41:319	Then the learner tries to instantiate the generic transformations with word features observed in the text." ></td>
	<td class="line x" title="42:319	A statistical-based suffix learner is presented in Schmid (1994)." ></td>
	<td class="line x" title="43:319	From a training corpus, it constructs a suffix tree where every suffix is associated with its information measure to emit a particular pos-tag." ></td>
	<td class="line x" title="44:319	Although the learning process in these systems is fully automated and the accuracy of obtained guessing rules reaches current state-of-the-art levels, for estimation of their parameters they require significant amounts of specially prepared training data--a large training corpus (usually pretagged), training examples, and so on." ></td>
	<td class="line x" title="45:319	In this paper, we describe a novel, fully automatic technique for the induction of Pos-class-guessing rules for unknown words." ></td>
	<td class="line x" title="46:319	This technique has been partially outlined in (Mikheev 1996a, 1996b) and, along with a level of accuracy for the induced rules that is higher than any previously quoted, it has an advantage in terms of quantity and simplicity of annotation of data for training." ></td>
	<td class="line x" title="47:319	Unlike many other approaches, which implicitly or explicitly assume that the surface manifestations of morpho-syntactic features of unknown words are different from those of general language, we argue that within the same language unknown words obey general morphological regularities." ></td>
	<td class="line x" title="48:319	In our approach, we do not require large amounts of annotated text but employ fully automatic statistical learning using a pre-existing general-purpose lexicon mapped to a particular tag set and word-frequency distribution collected from a raw corpus." ></td>
	<td class="line x" title="49:319	The proposed technique is targeted to the acquisition of both morphological and ending-guessing rules, which then can be applied cascadingly using the most accurate guessing rules first." ></td>
	<td class="line x" title="50:319	The rule induction process is guided by a thorough guessing-rule evaluation methodology that employs precision, recall, and coverage as evaluation metrics." ></td>
	<td class="line x" title="51:319	In the rest of the paper we first introduce the kinds of guessing rules to be induced and then present a semi-unsupervised 3 statistical rule induction technique using data derived from the CELEX lexical database (Burnage 1990)." ></td>
	<td class="line x" title="52:319	Finally we evaluate the induced guessing rules by removing all the hapax words from the lexicon and tagging the Brown Corpus (Francis and Kucera 1982) by a stochastic tagger and a rule-based tagger." ></td>
	<td class="line x" title="53:319	2." ></td>
	<td class="line x" title="54:319	Guessing-Rule Schemata There are two kinds of word-guessing rules employed by our cascading guesser: morphological rules and nonmorphological ending-guessing rules." ></td>
	<td class="line x" title="55:319	Morphological wordguessing rules describe how one word can be guessed given that another word is known." ></td>
	<td class="line x" title="56:319	Unlike morphological guessing rules, nonmorphological rules do not require the base form of an unknown word to be listed in the lexicon." ></td>
	<td class="line x" title="57:319	Such rules guess the pos-class for a word on the basis of its ending or leading segments alone." ></td>
	<td class="line x" title="58:319	This is especially important when dealing with uninflected words and domain-specific sublanguages where many highly specialized words can be encountered." ></td>
	<td class="line x" title="59:319	In English, as in many other languages, morphological word formation is realized by affixation: prefixation and suffixation." ></td>
	<td class="line x" title="60:319	Thus, in general, each kind of guessing rule can be further subcategorized depending on whether it is applied to the beginning or tail of an un3 The induction technique can be considered to be semi-unsupervised since it uses the annotation stated in the lexicon." ></td>
	<td class="line x" title="61:319	At the same time it does not require additional annotation since that annotation already exists regardless of the rule induction task." ></td>
	<td class="line x" title="62:319	407 Computational Linguistics Volume 23, Number 3 known word." ></td>
	<td class="line x" title="63:319	To mirror this classification, we will introduce a general schemata for guessing rules and a guessing rule will be seen as a particular instantiation of this schemata." ></td>
	<td class="line x" title="64:319	Definition A guessing-rule schemata is a structure G =x:{b.e} \[-S +M ?/-class --*R-class\] where  x indicates whether the rule is applied to the beginning or end of a word and has two possible values, b-beginning and e-end;  S is the affix to be segmented; it is deleted (-) from the beginning or end of an unknown word according to the value of x;  M is the mutative segment (possibly empty), which should be added (+) to the result string after the segmentation;  /-class is the required Pos-class (set of one or more pos-tags) of the stem; the result string after the -S and +M operations should be checked ()?" ></td>
	<td class="line x" title="65:319	in the lexicon for having this particular Pos-class; if/-class is set to be 'void' no checking is required;  R-class is the POs-class to assign (--,) to the unknown word if all the above operations (-S +M ?I) have been successful." ></td>
	<td class="line x" title="66:319	For example, the rule e\[-ied +y ?(VB VBP) --*(JJ VBD VBN)\] says that if there is an unknown word which ends with ied, we should strip this ending from it and append the string y to the remaining part." ></td>
	<td class="line x" title="67:319	If we then find this word in the lexicon as (VB VBP) (base verb or verb of present tense non-3d form), we conclude that the unknown word is of the category (JJ VBD VBN) (adjective, past verb, or participle)." ></td>
	<td class="line x" title="68:319	Thus, for instance, if the word specified was unknown to the lexicon, this rule first would try to segment the required ending ied (specified ied = specif), then add to the result the mutative segment y (specif + y = specify), and, if the word specify was found in the lexicon as (VB VBP), the unknown word specified would be classified as (JJ VBD VBN)." ></td>
	<td class="line x" title="69:319	Since the mutative segment can be an empty string, regular morphological formations can be captured as well." ></td>
	<td class="line x" title="70:319	For instance, the rule b\[-un +'' ?(VBD VBN) --*(JJ)\] says that if segmenting the prefix un from an unknown word results in a word that is found in the lexicon as a past verb and participle (VBD VBN), we conclude that the unknown word is an adjective 0J)." ></td>
	<td class="line x" title="71:319	This rule will, for instance, correctly classify the word unscrewed if the word screwed is listed in the lexicon as (VBD VBN)." ></td>
	<td class="line x" title="72:319	When setting the S segment to an empty string and the M segment to a non-empty string, the schemata allows for cases when a secondary form is listed in the lexicon and the base form is not." ></td>
	<td class="line x" title="73:319	For instance, the rule e\[-'' +ed ?(VBD VBN) --*(VB VBP)\] says that if adding the segment ed to the end of an unknown word results in a word 408 Andrei Mikheev Unknown-Word Guessing that is found in the lexicon as a past verb and participle (VBD VBN), then the unknown word is a base or non-3d present verb (VB VBP)." ></td>
	<td class="line x" title="74:319	The general schemata can also capture ending-guessing rules if the/-class is set to be 'void'." ></td>
	<td class="line x" title="75:319	This indicates that no stem lookup is required." ></td>
	<td class="line x" title="76:319	Naturally, the mutative segment of such rules is always set to an empty string." ></td>
	<td class="line x" title="77:319	For example, an endingguessing rule e\[-ing +'' ?---*(JJ NN VBG)\] says that if a word ends with ing it can be an adjective, a noun, or a gerund." ></td>
	<td class="line x" title="78:319	Unlike a morphological rule, this rule does not check whether the substring preceding the ing-ending is listed in the lexicon with a particular POs-class." ></td>
	<td class="line x" title="79:319	The proposed guessing-rule schemata is in fact quite similar to the set of generic transformations for unknown-word guessing developed by Brill (1995)." ></td>
	<td class="line x" title="80:319	There are, however, three major differences:  Brill's transformations do not check whether the stem belongs to a particular POS-class while the schemata proposed here does (?/-class) and therefore imposes more rigorous constraints;  Brill's transformations do not account for irregular morphological cases like try-tries whereas our schemata does (+M segment);  Brill's guessing rules produce a single most likely tag for an unknown word, whereas our guesser is intended to imitate the lexicon and produce all possible tags." ></td>
	<td class="line x" title="81:319	Brill's system has two transformations that our schemata do not capture: when a particular character appears in a word and when a word appears in a particular context." ></td>
	<td class="line x" title="82:319	The latter transformation is, in fact, due to the peculiarities of Brill's tagging algorithm and, in other approaches, is captured at the disambiguation phase of the tagger itself." ></td>
	<td class="line x" title="83:319	The former feature is indirectly captured in our approach." ></td>
	<td class="line x" title="84:319	It has been noticed (as in \[Weischedel et al. , 1993\], for example) that capitalized and hyphenated words have a different distribution from other words." ></td>
	<td class="line x" title="85:319	Our morphological rules account for this difference by checking the stem of the word." ></td>
	<td class="line x" title="86:319	The ending-guessing rules, on the other hand, do not use information about stems." ></td>
	<td class="line x" title="87:319	Thus if the ending s predicts that a word can be a plural noun or a 3d form of a verb, the information that this word was capitalized can narrow the considered set of POS-tags to plural proper noun." ></td>
	<td class="line x" title="88:319	We therefore decided to collect ending-guessing rules separately for capitalized words, hyphenated words, and all other words." ></td>
	<td class="line x" title="89:319	In our experiments, we restricted ourselves to the production of six different guessing-rule sets, which seemed most appropriate for English:  Suffix  suffix morphological rules with no mutative endings (0)." ></td>
	<td class="line x" title="90:319	Such rules account for the regular suffixation as, for instance, book + ed = booked;  Suffix I suffix morphological rules with a mutative ending in the last letter." ></td>
	<td class="line x" title="91:319	Such rules account for many cases of the irregular suffixation as, for instance, try y + ied = tried;  Prefix prefix morphological rules with no mutative segments (0)." ></td>
	<td class="line x" title="92:319	Such rules account for the regular prefixation as, for instance, Un qscrew ~ unscrew; 409 Computational Linguistics Volume 23, Number 3  Endingending-guessing rules for hyphenated words;  Ending c ending-guessing rules for capitalized words;  Ending* ending-guessing rules for all other (nonhyphenated and noncapitalized) words." ></td>
	<td class="line x" title="93:319	3." ></td>
	<td class="line x" title="94:319	Guessing-Rule Induction As already mentioned, we see features that our guessing-rule schemata is intended to capture as general language regularities rather than properties of rare or corpusspecific words only." ></td>
	<td class="line x" title="95:319	This significantly simplifies training data requirements: we can induce guessing rules from a general-purpose lexicon." ></td>
	<td class="line x" title="96:319	4 First, we no longer depend on the size or even existence of an annotated training corpus." ></td>
	<td class="line x" title="97:319	Second, we do not require any annotation to be done for the training; instead, we reuse the information stated in the lexicon, which we can automatically map to a particular tag set that a tagger is trained to." ></td>
	<td class="line x" title="98:319	We also use the actual frequencies of word usage, collected from a raw corpus." ></td>
	<td class="line x" title="99:319	This allows for the discrimination between rules that are no longer productive (but have left their imprint on the basic lexicon) and rules that are productive in real-life texts." ></td>
	<td class="line x" title="100:319	For guessing rules to capture general language regularities, the lexicon should be as general as possible (i.e. , should list all possible pos-tags for a word) and large." ></td>
	<td class="line x" title="101:319	The corresponding corpus should also be large enough to obtain reliable estimates of word-frequency distribution for at least 10,000-15,000 words." ></td>
	<td class="line x" title="102:319	Since a word can take on several different POS-tags, in the lexicon it can be represented as a \[string/Pos-class\] record, where the POs-class is a set of one or more POS-tags." ></td>
	<td class="line x" title="103:319	For instance, the entry for the word book, which can be a noun (NN) or a verb (VB) would look like \[book (NN VB)\]." ></td>
	<td class="line x" title="104:319	Thus the nth entry of the lexicon (Wn) can be represented as \[W C\]n where W is the surface lexical form and C is its pos-class." ></td>
	<td class="line x" title="105:319	Different lexicon entries can share the same POs-class but they cannot share the same surface lexical form." ></td>
	<td class="line x" title="106:319	In our experiments, we used a lexicon derived from CRLEX (Burnage 1990), a large multilingual database that includes extensive lexicons of English, Dutch, and German." ></td>
	<td class="line x" title="107:319	We constructed an English lexicon of 72,136 word forms with morphological features, which we then mapped into the Penn Treebank tag set (Marcus, Marcinkiewicz, and Santorini 1993)." ></td>
	<td class="line x" title="108:319	The most frequent open-class tags of this tag set are shown in Table 1." ></td>
	<td class="line x" title="109:319	Word-frequency distribution was estimated from the Brown Corpus, which reflects multidomain language use." ></td>
	<td class="line x" title="110:319	As usual, we separated the test sample from the training sample." ></td>
	<td class="line x" title="111:319	Here we followed the suggestion that the unknown words actually are quite similar to words that occur only once (hapax words) in the corpus (Dermatas and Kokkinakis 1995; Baayen and Sproat 1995)." ></td>
	<td class="line x" title="112:319	We put all the hapax words from the Brown Corpus that were found in the CnLEx-derived lexicon into the test collection (test lexicon) and all other words from the CELEx-derived lexicon into the training lexicon." ></td>
	<td class="line x" title="113:319	In the test lexicon, we also included the hapax words not found in the CELEx-derived lexicon, assigning them the POS-tags they had in the Brown Corpus." ></td>
	<td class="line x" title="114:319	Then we filtered out words shorter than four characters, nonwords such as numbers or alpha-numerals, which usually are handled at the tokenization phase, and all closed-class words, s which we assume will always be present in the lexicon." ></td>
	<td class="line x" title="115:319	Thus after all these transformations we obtained a lexicon of 59,268 entries for training and the test lexicon of 17,868 entries." ></td>
	<td class="line x" title="116:319	4 As opposed to a corpus-specific one." ></td>
	<td class="line x" title="117:319	5 The closed class consists of a finite and well-established list of words such as prepositions, articles, wh-words, etc. 410 Andrei Mikheev Unknown-Word Guessing Our guessing-rule induction technique uses the training and test data prepared as described above and can be seen as a sampling for the best performing rule set from a collection of automatically produced rule sets." ></td>
	<td class="line x" title="118:319	Here is a brief outline of its major phases: Rule Extraction Phase (Section 3.1) sets of word-guessing rules, (e.g. , Prefix, Suffix , Suffix 1, Ending, etc)." ></td>
	<td class="line x" title="119:319	are extracted from the lexicon and cleaned of redundant and infrequently used rules; Rule Scoring Phase (Section 3.2) each rule from the extracted rule sets is ranked according to its accuracy, and rules that scored above a certain threshold are included in the working rule sets; Rule Merging Phase (Section 3.3) rules that have not scored high enough are merged together into more general rules, then rescored, and, depending on their score, added to the working rule sets; Direct Evaluation Phase (Sections 3.4) working rule sets produced with different thresholds are evaluated to obtain the best-performing ones." ></td>
	<td class="line x" title="120:319	3.1 Rule Extraction Phase For the extraction of the initial sets of prefix and suffix morphological guessing rules (Prefix, Suffix , and Suffix1), we define the operator Vn where the index n specifies the length of the mutative ending of the main word." ></td>
	<td class="line x" title="121:319	Thus when the index n is set to 0 the result of the application of the V0 operator will be a morphological rule with no mutative segment." ></td>
	<td class="line x" title="122:319	The V1 operator will extract the rules with the alterations in the last letter of the main word." ></td>
	<td class="line x" title="123:319	When the ~ operator is applied to a pair of entries from the lexicon (\[W C\]i and \[W C\]j), first, it segments the last (or first) n characters of the shorter word (Wj) and stores this in the M element of the rule." ></td>
	<td class="line x" title="124:319	Then it tries to segment an affix by subtracting the shorter word (Wj) without the mutative ending from the longer word (Wi)." ></td>
	<td class="line x" title="125:319	If the subtraction results in an non-empty string and the mutative segment is not duplicated in the affix, the system creates a morphological rule with the POs-class of the shorter word (Cj) as the/-class, the POS-class of the longer word (Ci) as the R-class and the segmented affix itself in the S field." ></td>
	<td class="line x" title="126:319	For example: \[booked (JJ VBD VBN)\] V0 \[book (NN VB)\] --+ e\[-ed +'' ?(NN VB) ---+(JJ VBD VBN)\] \[advisable (JJ)\] V1 \[advise (NN VB)\] ---+ e\[-able +'e' ?(NN VB) ---~(JJ) \] The V operator is applied to all possible pairs of lexical entries sequentially, and, if a rule produced by such an application has already been extracted from another pair, its frequency count (f) is incremented." ></td>
	<td class="line x" title="127:319	Thus, prefix and suffix morphological rules together with their frequencies are produced." ></td>
	<td class="line x" title="128:319	Next, we cut out the most infrequent rules, which might bias further learning." ></td>
	<td class="line x" title="129:319	To do that we eliminate all the rules with frequency f less than a certain threshold 8, which usually is set quite low: 2-4." ></td>
	<td class="line x" title="130:319	Such filtering reduces the rule sets more than tenfold." ></td>
	<td class="line x" title="131:319	To collect the ending-guessing rules, we set the upper limit on the ending length equal to five characters and thus collect from the lexicon all possible word-endings of length 1, 2, 3, 4, and 5, together with the POS-classes of the words in which these endings appeared." ></td>
	<td class="line x" title="132:319	We also set the minimum length of the remaining substring to three characters." ></td>
	<td class="line x" title="133:319	We define the unary operator A, which produces a set of ending-guessing 411 Computational Linguistics Volume 23, Number 3 rules from a word in the lexicon (\[W C\]i)." ></td>
	<td class="line x" title="134:319	For instance, from a lexicon entry Idifferent (JJ)\] the operator A will produce five ending-guessing rules: A \[different 0J)\] = { e\[--t + ?-~ (J J)\] e\[--nt +  ?---+ (JJ)\] e\[-ent +  ?~ (J J)\] e\[-rent +  ?---* (J3)\] e\[-erent +  ?--+ 0J)\] The G operator is applied to each entry in the lexicon, and if a rule it produces has already been extracted from another entry in the lexicon, its frequency count (f) is incremented." ></td>
	<td class="line x" title="135:319	Then the infrequent rules with f < 0 are eliminated from the endingguessing rule set." ></td>
	<td class="line x" title="136:319	After applying the/k and V operations to the training lexicon, we obtained rule collections of 40,000-50,000 entries." ></td>
	<td class="line x" title="137:319	Filtering out the rules with frequency counts of 1 reduced the collections to 5,000-7,000 entries." ></td>
	<td class="line x" title="138:319	3.2 Rule Scoring Phase Of course, not all acquired rules are equally good at predicting word classes: some rules are more accurate in their guesses and some rules are more frequent in their application." ></td>
	<td class="line x" title="139:319	For every rule acquired, we need to estimate whether it is an effective rule worth retaining in the working rule set." ></td>
	<td class="line x" title="140:319	To do so, we perform a statistical experiment as follows: we take each rule from the extracted rule sets, one by one, take each wordtype from the training lexicon and guess its POs-class using the rule, if the rule is applicable to the word." ></td>
	<td class="line x" title="141:319	For example, if a guessing rule strips off a particular suffix and a current word from the lexicon does not have this suffix, we classify that word and the rule as incompatible and the rule as not applicable to that word." ></td>
	<td class="line x" title="142:319	If a rule is applicable to a word, we compare the result of the guess with the information listed in the lexicon." ></td>
	<td class="line x" title="143:319	If the guessed class is the same as the class stated in the lexicon, we count it as a hit or success, otherwise it is a failure." ></td>
	<td class="line x" title="144:319	Then, since we are interested in the application of the rules to word-tokens in the corpus, we multiply the result of the guess by the corpus frequency of the word." ></td>
	<td class="line x" title="145:319	If we keep the sample space for each rule separate from the others, we have a binomial experiment." ></td>
	<td class="line x" title="146:319	The value of a guessing rule closely correlates with its estimated proportion of success (/5), which is the proportion of all positive outcomes (x) of the rule application to the total number of the trials (n), which are, in fact, the number of all the word tokens that are compatible to the rule in the corpus: x: number of successful guesses = n: number of the compatible to the rule word-tokens The 15 estimate is a good indicator of the rule accuracy but it frequently suffers from large estimation error due to insufficient training data." ></td>
	<td class="line x" title="147:319	For example, if a rule was found to apply just once and the total number of observations was also one, its estimate p has the maximal value (1) but clearly this is not a very reliable estimate." ></td>
	<td class="line x" title="148:319	We tackle this problem by calculating the lower confidence limit 71' L for the rule estimate, which can be seen as the minimal expected value of/~ for the rule if we were to draw a large number of samples." ></td>
	<td class="line x" title="149:319	Thus with a certain confidence c~ we can assume that if we used more training data, the rule estimate/~ would be not worse than the 7rL." ></td>
	<td class="line x" title="150:319	The rule estimate then will be taken at its lowest possible value which is the ~L limit itself." ></td>
	<td class="line x" title="151:319	First we adjust the rule estimate so that we have no zeros in positive (/~) or negative (1 \]5) outcome probabilities, by adding some floor values to the numerator and denominator: 412 Andrei Mikheev Unknown-Word Guessing df 1 2 3 4 5  30 40 60 infinity to.a/o5 6.314 2.920 3.353 2.132 2.015  1.697 1.684 1.671 1.645 Figure 1 Values of d/ df based on sample size." ></td>
	<td class="line x" title="152:319	t(1_0.90)/2 ~ to.05 \]5~ = xi+0.5 The lower confidence limit 7r L then is calculated as (Hayslett 1981): ni+l ' 7rL /~* .(n-l) =~._~(n-1) / ff/~*(l~-/~*) = -t(I_cQ/2 * Sp ~(1-c~)/2 * d/ where t(l_c0/2 is a coefficient of the t-distribution." ></td>
	<td class="line x" title="153:319	It has two parameters: c~, the level of confidence and dr, the number of degrees of freedom, which is one less than the sample size (dr n 1)." ></td>
	<td class="line x" title="154:319	e/ = t(l_~)/2 can be looked up in the tables for the t-distribution listed df df in every textbook on statistics." ></td>
	<td class="line x" title="155:319	We adopted 90% confidence for which t(1_o.9o)/2=to.o5 takes values depending on the sample size as in Figure 1." ></td>
	<td class="line x" title="156:319	Using ~-L instead of \]~ for rule scoring favors higher estimates (/3) obtained over larger samples (n)." ></td>
	<td class="line x" title="157:319	Even if one rule has a high estimate value but that estimate was obtained over a small sample, another rule with a lower estimate value but obtained over a large sample might be valued higher by ~rL." ></td>
	<td class="line x" title="158:319	This rule-scoring function resembles the one used by Tzoukermann, Radev, and Gale (1995) for scoring Pos-disambiguation rules for the French tagger." ></td>
	<td class="line x" title="159:319	The main difference between the two functions is that there the t value was implicitly assumed to be 1, which corresponds to a confidence level of 68% on a very large sample." ></td>
	<td class="line x" title="160:319	Another important consideration for rating a word-guessing rule is that the longer the affix or ending (S) of this rule, the more confident we are that it is not a coincidental one, even on small samples." ></td>
	<td class="line x" title="161:319	For example, if the estimate for the word-ending o was obtained over a sample of five words and the estimate for the word-ending fulness was also obtained over a sample of five words, the latter is more representative, even though the sample size is the same." ></td>
	<td class="line x" title="162:319	Thus we need to adjust the estimation error in accordance with the length of the affix or ending." ></td>
	<td class="line x" title="163:319	A good way to do this is to decrease it proportionally to a value that increases along with the increase of the length." ></td>
	<td class="line x" title="164:319	A suitable solution is to use the logarithm of the affix length: ^ .(o,-,I /pt(1 ^* scorei -= Pt to.os * V n. Pi )/(1 + log(ISil)) When the length of S (the affix or ending) is 1, the estimation error is not changed since log(l) is 0." ></td>
	<td class="line x" title="165:319	For the rules with an affix or ending length of 2 the estimation error is reduced by 1 + log(2) = 1.3, for the length 3 this will be 1 + log(3) = 1.48, etc. The longer the length, the smaller the sample that will be considered representative enough for a confident rule estimation." ></td>
	<td class="line x" title="166:319	Setting the threshold (0s) at a certain level we include in the working rule sets only those rules whose scores are higher than the threshold." ></td>
	<td class="line x" title="167:319	The method for finding the optimal threshold is based on empirical evaluations of the rule sets and is described in Section 3.4." ></td>
	<td class="line x" title="168:319	Usually, the threshold is set in the range of 65-80 points and the rule sets are reduced down to a few hundred entries." ></td>
	<td class="line x" title="169:319	For example, when we set 413 Computational Linguistics Volume 23, Number 3 Table 2 Top scored Prefix and Suffix  guessing rules." ></td>
	<td class="line x" title="170:319	Prefix /-class R-class Suffix /-class R-class re JJ NN VBG JJ NN VBG ex NN NN selfNN NN inter JJ JJ non Jl Jl un RB RB dis JJ JJ antiNN JJ de jj VBD VBN JJ VBD VBN in RB RB ment VB VBP NN ing NN VB VBP JJ NN VBG ed NN VB VBP JJ VBD VBN s NN VB VBP NNS VBZ ment NN VB VBP NN ly JJ NN RB ', RB ness JJ NN ship NN NN able NN VB VBP JJ s NN NNS the threshold (0s) to 75 points, the obtained ending-guessing rule collection (Ending*) comprised 1,876 rules, the suffix rule collection without mutation (Suffix ) comprised 591 rules, the suffix rule collection with mutation (Suffix 1) comprised 912 entries and the prefix rule collection (Prefix) comprised 235 rules." ></td>
	<td class="line x" title="171:319	Table 2 shows the highest-rated rules from the induced Prefix and Suffix  rule sets." ></td>
	<td class="line x" title="172:319	In general, it looks as though the induced morphological guessing rules largely consist of the standard rules of English morphology and also include a small proportion of rules that do not belong to the known morphology of English." ></td>
	<td class="line x" title="173:319	For instance, the suffix rule e\[ -et +'' ?(NN) --,(NN)\] does not stand for any well-known morphological rule, but its prediction is as good as those of the standard morphological rules." ></td>
	<td class="line x" title="174:319	The same situation can be seen with the prefix rule b\[ -st +'' ?(NNS) --+(NNS)I, which is quite predictive but at the same time is not a standard English morphological rule." ></td>
	<td class="line x" title="175:319	The ending-guessing rules, naturally, include some proper English suffixes but mostly they are simply highly predictive ending segments of words." ></td>
	<td class="line x" title="176:319	3.3 Rule Merging Phase Rules which have scored lower than the threshold are merged together into more general rules." ></td>
	<td class="line x" title="177:319	These new rules, if they score above the threshold, can also be included in the working rule sets." ></td>
	<td class="line x" title="178:319	We merge together two rules if they scored below the threshold and have the same affix (S), mutative segment (M), and initial class (i).6 We define the rule-merging operator : Ai @ Aj = At: \[Si, Mi, Ii, Ri U Rj\] if Si = Sj & Mi = Mj & Ii = Ij This operator merges two rules with the same affix (S), mutative segment (M) and the initial class (I) into one rule, with the resulting class being the union of the two merged resulting classes." ></td>
	<td class="line x" title="179:319	For example, e\[-s +'' ?(NN VB) --*(NNS)\]  e\[--S +'' ?(NI~ VB) ---~(NNB VBZ)I = e\[-s +'' ?(NN VB) --fiNNS VBZ)\] b\[--un +'' ?(VBD VBN) -*(JJ)\]  b\[--un +'' ?(VBD VBN) --*(VBN)\] = b\[--un +'' ?(VBD VBN) --*(JJ VBN)\] 6 For ending-guessing rules, this is always the case." ></td>
	<td class="line x" title="180:319	414 Andrei Mikheev Unknown-Word Guessing Possible Tags JJ NN NNS RB VB VBD VBG VBN VBZ Lexicon Information V V V Guesser Assigned V V V v V Figure 2 Lexicon entry and guesser's categorization for \[developed (JJ VBD VBN)\]." ></td>
	<td class="line x" title="181:319	The score of the resulting rule will be higher than the scores of the individual rules since the number of positive observations increases and the number of the trials remains the same." ></td>
	<td class="line x" title="182:319	After a successful application of the  operator, the resulting general rule is substituted for the two merged ones." ></td>
	<td class="line x" title="183:319	To perform such rule merging over a rule set the rules that have not been included into the working rule set are first sorted by their score and the rules with the best scores are merged first." ></td>
	<td class="line x" title="184:319	After each successful merging, the resulting rule is rescored." ></td>
	<td class="line x" title="185:319	This is done recursively until the score of the resulting rule does not exceed the threshold, at which point it is added to the working rule sets." ></td>
	<td class="line x" title="186:319	This process is applied until no merges can be done to the rules that scored poorly." ></td>
	<td class="line x" title="187:319	In our experiment we noticed that the merging added 30-40% new rules to the working rule sets, and therefore the final number of rules for the induced sets were: Prefix 348, Suffix  975, Suffix 11,263 and Ending* 2,196." ></td>
	<td class="line x" title="188:319	3.4 Direct Evaluation Phase There are two important questions that arise at the rule acquisition stage: how to choose the scoring threshold Os and what the performance of the rule sets produced with different thresholds is. The task of assigning a set of POS-tags to a word is actually quite similar to the task of document categorization where a document is assigned a set of descriptors that represent its contents." ></td>
	<td class="line x" title="189:319	There are a number of standard parameters (Lewis 1991) used for measuring performance on this kind of task." ></td>
	<td class="line x" title="190:319	For example, suppose that a word can take on one or more POS-tags from the set of open-class POS-tags: qJ NN NNS RB VB VBD VBG VBN VBZ)." ></td>
	<td class="line x" title="191:319	To see how well the guesser performs, we can compare the results of the guessing with the Pos-tags known to be true for the Word (i.e. , listed in the lexicon)." ></td>
	<td class="line x" title="192:319	Let us take, for instance, a lexicon entry \[developed (JJ VBD VBN)\]." ></td>
	<td class="line x" title="193:319	Suppose that the guesser categorized it as \[developed (JJ NN RB VBD VBZ)\]." ></td>
	<td class="line x" title="194:319	We can represent this situation as in Figure 2." ></td>
	<td class="line x" title="195:319	The performance of the guesser can be measured in:  recall the percentage of POS-tags correctly assigned by the guesser, i.e., two (jJ VBD) out of three (JJ VBD VBN) or 66%." ></td>
	<td class="line x" title="196:319	100% recall would mean that the guesser had assigned all the correct pos-tags but not necessarily only the correct ones." ></td>
	<td class="line x" title="197:319	So, for example, if the guesser had assigned all possible POS-tags to the word its recall would have been 100%." ></td>
	<td class="line x" title="198:319	 precision the percentage of POS-tags the guesser assigned correctly (JJ VBD) over the total number of POS-tags it assigned to the word (Jl NN RB VBD VBZ), i.e., 2/5 or 40%." ></td>
	<td class="line x" title="199:319	100% precision would mean that the guesser did not assign incorrect POS-tags, although not necessarily all the correct ones were assigned." ></td>
	<td class="line x" title="200:319	So, if the guesser had assigned only (JJ) its precision would have been 100%." ></td>
	<td class="line x" title="201:319	 coverage the proportion of words guesser was able to classify, but not necessarily correctly." ></td>
	<td class="line x" title="202:319	So, for example, if we had evaluated a guesser with 415 Computational Linguistics Volume 23, Number 3 Table 3 Comparative performance of different guessing rule sets." ></td>
	<td class="line x" title="203:319	Measure Sample Xerox Ending Suffix  Suffix I Prefix Cascade Recall Training 0.958045 0.965378 0.978751 0.966475 0.973135 0.966327 Test 0.956262 0.951916 0.973245 0.956031 0.947015 0.952491 Precision Training 0.648983 0.760492 0.977273 0.969032 0.959782 0.82257 Test 0.719206 0.782712 0.979964 0.96761 0.935075 0.851626 Coverage Training 0.872842 0.946309 0.493283 0.307658 0.048635 0.950581 Test 0.856372 0.918876 0.367574 0.26542 0.0653175 0.926553 100 random words from the lexicon and the guesser had assigned something to 80 of them, its coverage would have been 80%." ></td>
	<td class="line x" title="204:319	The interpretation of these percentages is by no means straightforward, as there is no straightforward way of combining these different measures into a single one." ></td>
	<td class="line x" title="205:319	For example, these measures assume that all combinations of POS-tags will be equally hard to disambiguate for the tagger, which is not necessarily the case." ></td>
	<td class="line x" title="206:319	Obviously, the most important measure is recall since we want all possible categories for a word to be guessed." ></td>
	<td class="line x" title="207:319	Precision seems to be slightly less important since the disambiguator should be able to handle additional noise but obviously not in large amounts." ></td>
	<td class="line x" title="208:319	Coverage is a very important measure for a rule set, since a rule set that can guess very accurately but only for a tiny proportion of words is of questionable value." ></td>
	<td class="line x" title="209:319	Thus, we will try to maximize recall first, then coverage, and, finally, precision." ></td>
	<td class="line x" title="210:319	We will measure the aggregate by averaging over measures per word (micro-average), i.e., for every single word from the test collection the precision and recall of the guesses are calculated, and then we average over these values." ></td>
	<td class="line x" title="211:319	To find the optimal threshold (0s) for the production of a guessing rule set, we generated a number of similar rule sets using different thresholds and evaluated them against the training lexicon and the test lexicon of unseen 17,868 hapax words." ></td>
	<td class="line x" title="212:319	Every word from the two lexicons was guessed by a rule set and the results were compared with the information the word had in the lexicon." ></td>
	<td class="line x" title="213:319	For every application of a rule set to a word, we computed the precision and recall, and then using the total number of guessed words we computed the coverage." ></td>
	<td class="line x" title="214:319	We noticed certain regularities in the behavior of the metrics in response to the change of the threshold: recall improves as the threshold increases while coverage drops proportionally." ></td>
	<td class="line x" title="215:319	This is not surprising: the higher the threshold, the fewer the inaccurate rules included in the rule set, but at the same time the fewer the words that can be handled." ></td>
	<td class="line x" title="216:319	An interesting behavior is shown by precision: first, it grows proportionally along with the increase of the threshold, but then, at high thresholds, it decreases." ></td>
	<td class="line x" title="217:319	This means that among very confident rules with very high scores, there are many quite general ones." ></td>
	<td class="line x" title="218:319	The best thresholds were obtained in the range of 70-80 points." ></td>
	<td class="line x" title="219:319	Table 3 displays the metrics for the best-scored (by aggregate of the three metrics on the training and the test samples) rule sets." ></td>
	<td class="line oc" title="220:319	As the baseline standard, we took the ending-guessing rule set supplied with the Xerox tagger (Cutting et al. 1992)." ></td>
	<td class="line n" title="221:319	When we compared the Xerox ending guesser with the induced ending-guessing rule set (Ending*), we saw that its precision was about 6% poorer and, most importantly, it 416 Andrei Mikheev Unknown-Word Guessing could handle 6% fewer unknown words." ></td>
	<td class="line x" title="222:319	Finally, we measured the performance of the cascading application of the induced rule sets when the morphological guessing rules were applied before the ending-guessing rules (Prefix+Suffix+Suffix 1 +Ending -c*)." ></td>
	<td class="line x" title="223:319	We detected that the cascading application of the morphological rule sets together with the ending-guessing rules increases the overall precision of the guessing by about 8%." ></td>
	<td class="line n" title="224:319	This made the improvement over the baseline Xerox guesser 13% in precision and 7% in coverage on the test sample." ></td>
	<td class="line x" title="225:319	4." ></td>
	<td class="line x" title="226:319	Unknown-Word Tagging The direct evaluation phase gave us a basis for setting the threshold to produce the best-performing rule sets." ></td>
	<td class="line x" title="227:319	The task of unknown-word guessing is, however, a subtask of the overall part-of-speech tagging process." ></td>
	<td class="line x" title="228:319	Our main interest is in how the advantage of one rule set over another will affect the tagging performance." ></td>
	<td class="line x" title="229:319	Therefore, we performed an evaluation of the impact of the word guessers on tagging accuracy." ></td>
	<td class="line x" title="230:319	In this evaluation we used the cascading guesser with two different taggers: a c++ implemented bigram HMM tagger akin to one described in Kupiec (1992) and the rule-based tagger of Brill (1995)." ></td>
	<td class="line o" title="231:319	Because of the similarities in the algorithms with the LISP implemented Xerox tagger, we could directly use the Xerox guessing rule set with the HMM tagger." ></td>
	<td class="line x" title="232:319	Brill's tagger came pretrained on the Brown Corpus and had a corresponding guessing component." ></td>
	<td class="line o" title="233:319	This gave us a search-space of four basic combinations: the HMM tagger equipped with the Xerox guesser, the Brill tagger with its original guesser, the HMM tagger with our cascading (Prefix+Suffix+Suffixl+Ending-C*) guesser and the Brill tagger with the cascading guesser." ></td>
	<td class="line x" title="234:319	We also tried hybrid tagging using the output of the HMM tagger as the input to Brill's final state tagger, but it gave poorer results than either of the taggers and we decided not to consider this tagging option." ></td>
	<td class="line x" title="235:319	4.1 Setting up the Experiment We evaluated the taggers with the guessing components on all fifteen subcorpora of the Brown Corpus, one after another." ></td>
	<td class="line x" title="236:319	The HMM tagger was trained on the Brown Corpus in such a way that the subcorpus used for the evaluation was not seen at the training phase." ></td>
	<td class="line x" title="237:319	All the hapax words and capitalized words with frequency less than 20 were not seen at the training of the cascading guesser." ></td>
	<td class="line x" title="238:319	These words were not used in the training of the tagger either." ></td>
	<td class="line x" title="239:319	This means that neither the HMM tagger nor the cascading guesser had been trained on the texts and words used for evaluation." ></td>
	<td class="line o" title="240:319	We do not know whether the same holds for the Brill tagger and the Brill and Xerox guessers since we took them pretrained." ></td>
	<td class="line x" title="241:319	For words that the guessing components failed to guess, we applied the standard method of classifying them as common nouns (NN) if they were not capitalized inside a sentence and proper nouns (NNP) otherwise." ></td>
	<td class="line x" title="242:319	When we used the cascading guesser with the Brill tagger we interfaced them on the level of the lexicon: we guessed the unknown words before the tagging and added them to the lexicon listing the most likely tags first as required." ></td>
	<td class="line x" title="243:319	7 Here we want to clarify that we evaluated the overall results of the Brill tagger rather than just its unknown-word tagging component." ></td>
	<td class="line x" title="244:319	Another point to mention is that, since we included the guessed words in the lexicon, the Brill tagger could use for the transformations all relevant Postags for unknown words." ></td>
	<td class="line x" title="245:319	This is quite different from the output of the original Brill's guesser, which provides only one Pos-tag for an unknown word." ></td>
	<td class="line x" title="246:319	In our tagging experiments, we measured the error rate of tagging on unknown 7 We estimated the most likely tags from the training data." ></td>
	<td class="line x" title="247:319	417 Computational Linguistics Volume 23, Number 3 words using different guessers." ></td>
	<td class="line x" title="248:319	Since, arguably, the guessing of proper nouns is easier than is the guessing of other categories, we also measured the error rate for the subcategory of capitalized unknown words separately." ></td>
	<td class="line x" title="249:319	The error rate for a category of words was calculated as follows: Error x = Wrongly_Tagged_Words_from_Set_X Total_Words_in_Set_X Thus, for instance, the error rate of tagging the unknown words is the proportion of the mistagged unknown words to all unknown words." ></td>
	<td class="line x" title="250:319	To see the distribution of the workload between different guessing rule sets we also measured the coverage of a guessing rule set: CoverageR = Assigned_Wordsday_Rule_Set_R Total _Unknown _Words We collected the error and coverage measures for each of the fifteen subcorpora 8 of the Brown Corpus separately, and, using the bootstrap replicate technique (Efron and Tibshirani 1993), we calculated the mean and the standard error for each combination of the taggers with the guessing components." ></td>
	<td class="line x" title="251:319	For the fifteen accuracy means {al, d2 , a15} obtained upon tagging the fifteen subcorpora of the Brown Corpus, we generated a large number of bootstrap replicates of the form {bl, b2,, b15} where each mean was randomly chosen with replacements such as, for instance, {bl = a11, b2 = a4, b3 =, b4 = an , b14 = a~9, b15 = a4}." ></td>
	<td class="line x" title="252:319	Using these replicates, we calculated the mean and standard error of the whole bootstrap distribution as follows: deB = \[0*(b) 0*(.)\]2/(B 1) where  B is the number of bootstrap replications;  0* (b) is the mean estimate of the bth bootstrap replication;  0'()." ></td>
	<td class="line x" title="253:319	= Y~-I O*(b)/B is the mean estimate of the whole bootstrap distribution; This way of calculating the estimated standard error for the mean does not assume the normal distribution and hence provides more accurate results." ></td>
	<td class="line x" title="254:319	We noticed a certain inconsistency in the markup of proper nouns (NNP) in the Brown Corpus supplied with the Penn Treebank." ></td>
	<td class="line x" title="255:319	Quite often obvious proper nouns as, for instance, Summerdale, Russia, or Rochester were marked as common nouns (NN) and sometimes lower-cased common nouns such as business or church were marked as proper nouns." ></td>
	<td class="line x" title="256:319	Thus we decided not to count as an error the mismatch of the NN/NNP tags." ></td>
	<td class="line x" title="257:319	Using the HMM tagger with the lexicon containing all the words from 8 Each subcorpus belongs to a different genre ranging from news to fiction." ></td>
	<td class="line x" title="258:319	418 Andrei Mikheev Unknown-Word Guessing Table 4 Results of tagging the unknown words in the Brown Corpus." ></td>
	<td class="line o" title="259:319	Unknown Words Unknown Common Words Unknown Proper Nouns Tagger Guesser Metrics Error Error Coverage Error Coverage HMM Xerox mean 17.851643 30.022169 37.567270 10.785563 63.797113 s-error 0.484710 0.469922 1.687396 0.613745 1.714969 HMM Cascade mean 12.378716 21.266264 36.507909 7.776456 64.795969 s-error 0.917656 0.403957 2.336381 0.853958 2.206457 Brill Brill mean 14.688501 27.411736 38.998687 6.439525 62.160917 s-error 0.908172 0.539634 2.627234 0.501082 4.010992 Brill Cascade mean 11.327863 20.986240 37.933048 5.548990 63.816586 s-error 0.761576 0.480798 2.353510 0.561009 3.775991 the Brown Corpus, we obtained the error rate (mean) 0* (.)=4.003093 with the standard error deB=0.155599." ></td>
	<td class="line x" title="260:319	This agrees with the results on the closed dictionary (i.e. , without unknown words) obtained by other researchers for this class of the model on the same corpus (Kupiec 1992; DeRose 1988)." ></td>
	<td class="line x" title="261:319	The Brill tagger showed some better results: error rate (mean) 0* (.)=3.327366 with the standard error deB=O. 123903." ></td>
	<td class="line x" title="262:319	Although our primary goal was not to compare the taggers themselves but rather their performance with the guessing components, we attribute the difference in their performance to the fact that Brill's tagger uses the information about the most likely tag for a word whereas the HMM tagger did not have this information and instead used the priors for a set of POS-tags (ambiguity class)." ></td>
	<td class="line x" title="263:319	When we removed from the lexicon all the hapax words and, following the recommendation of Church (1988), all the capitalized words with frequency less than 20, we obtained some 51,522 unknown word-tokens (25,359 wordtypes) out of more than a million word-tokens in the Brown Corpus." ></td>
	<td class="line x" title="264:319	We tagged the fifteen subcorpora of the Brown Corpus by the four combinations of the taggers and the guessers using the lexicon of 22,260 word-types." ></td>
	<td class="line x" title="265:319	4.2 Results of the Experiment Table 4 displays the tagging results on the unknown words obtained by the four different combinations of taggers and guessers." ></td>
	<td class="line x" title="266:319	It shows the overall error rate on unknown words and also displays the distribution of the error rate and the coverage between unknown proper nouns and the other unknown words." ></td>
	<td class="line x" title="267:319	Indeed the error rate on the proper nouns was much smaller than on the rest of the unknown words, which means that they are much easier to guess." ></td>
	<td class="line x" title="268:319	We can also see a difference in the distribution (coverage) of the unknown words using different taggers." ></td>
	<td class="line x" title="269:319	This can be accounted for by the fact that the unguessed capitalized words were taken by default to be proper nouns and that the Brill tagger and the HMM tagger had slightly different strategies to apply to the first word of a sentence." ></td>
	<td class="line n" title="270:319	The cascading guesser outperformed the other two guessers in general and most importantly in the non-proper noun category, where it had an advantage of 6.5% over Brill's guesser and about 8.7% over Xerox's guesser." ></td>
	<td class="line x" title="271:319	In our experiments the category of unknown proper nouns had a larger share (6364%) than we expect in real life because all the capitalized words with frequency less than 20 were taken out of the lexicon." ></td>
	<td class="line n" title="272:319	The cascading guesser also helped to improve the accuracy on unknown proper nouns by about 1% in comparison to Brill's guesser and about 3% in comparison to Xerox's guesser." ></td>
	<td class="line x" title="273:319	The cascading guesser outperformed the other two guessers on every subcorpus of the Brown Corpus." ></td>
	<td class="line x" title="274:319	Table 5 shows the distribution of the workload and the tagging accuracy among the different rule sets of the cascading guesser." ></td>
	<td class="line x" title="275:319	The default assignment of the NN tag to unguessed words 419 Computational Linguistics Volume 23, Number 3 Table 5 Distribution of the error rate and coverage in the cascading guesser." ></td>
	<td class="line x" title="276:319	Metrics Prefix Suffix  Suffix 1 Ending -c* Default Error Coverage Error Coverage Error Coverage Error Coverage Error Coverage mean 10.92 5.64 11.95 33.78 17.33 7.00 26.84 46.61 44.00 8.17 s-error 0.95 0.19 0.65 0.84 1.19 0.17 0.91 0.83 3.17 0.25 performed very poorly, having the error rate of 44%." ></td>
	<td class="line n" title="277:319	When we compared this distribution to that of the Xerox guesser we saw that the accuracy of the Xerox guesser itself was only about 6.5% lower than that of the cascading guesser 9 and the fact that it could handle 6% fewer unknown words than the cascading guesser resulted in the increase of incorrect assignments by the default strategy." ></td>
	<td class="line x" title="278:319	There were three types of mistaggings on unknown words detected in our experiments." ></td>
	<td class="line x" title="279:319	Mistagging of the first type occurred when a guesser provided a broader POS-class for an unknown word than a lexicon would, and the tagger had difficulties with its disambiguation." ></td>
	<td class="line x" title="280:319	This was especially the case with the words that were guessed as noun/adjective (NN JJ) but, in fact, act only as one of them (as do, for example, many hyphenated words)." ></td>
	<td class="line x" title="281:319	Another highly ambiguous group is the ing words, which, in general, can act as nouns, adjectives, and gerunds and only direct lexicalization can restrict the search-space, as in the case of the word seeing, which cannot act as an adjective." ></td>
	<td class="line x" title="282:319	The second type of mistagging was caused by incorrect assignments by the guesser." ></td>
	<td class="line x" title="283:319	Usually this was the case with irregular words such as cattle or data, which were wrongly guessed to be singular nouns (NN) but in fact were plural nouns (NN8)." ></td>
	<td class="line x" title="284:319	We also did not include the 'foreign word' category (FW) in the set of tags to guess, but this did not do too much harm because these words were very infrequent in the texts." ></td>
	<td class="line x" title="285:319	And the third type of mistagging occurred when the word-POS guesser assigned the correct Pos-class to a word but the tagger still disambiguated this class incorrectly." ></td>
	<td class="line x" title="286:319	This was the most frequent type of error, which accounted for more than 60% of the mistaggings on unknown words." ></td>
	<td class="line x" title="287:319	5." ></td>
	<td class="line x" title="288:319	Conclusion We have presented a technique for fully automated statistical acquisition of rules that guess possible Pos-tags for words unknown to the lexicon." ></td>
	<td class="line x" title="289:319	This technique does not require specially prepared training data and uses for training a pre-existing generalpurpose lexicon and word frequencies collected from a raw corpus." ></td>
	<td class="line x" title="290:319	Using such training data, three types of guessing rules are induced: prefix morphological rules, suffix morphological rules, and ending-guessing rules." ></td>
	<td class="line x" title="291:319	Evaluation of tagging accuracy on unknown words using texts and words unseen at the training phase showed that tagging with the automatically induced cascading guesser was consistently more accurate than previously quoted results known to the author (85%)." ></td>
	<td class="line x" title="292:319	Tagging accuracy on unknown words using the cascading guesser was 87.7-88.7%." ></td>
	<td class="line n" title="293:319	The cascading guesser outperformed the guesser supplied with the Xerox tagger and the guesser supplied with Brill's tagger both on unknown proper nouns 9 We attribute this to the 13% lower precision of the Xerox guesser." ></td>
	<td class="line x" title="294:319	420 Andrei Mikheev Unknown-Word Guessing (which is a relatively easy-to-guess category of words) and on the rest of the unknown words, where it had an advantage of 6.5-8.5.%." ></td>
	<td class="line x" title="295:319	When the unknown words were made known to the lexicon, the accuracy of tagging was 93.6-94.3% which makes the accuracy drop caused by the cascading guesser to be less than 6% in general." ></td>
	<td class="line x" title="296:319	Another important conclusion from the evaluation experiments is that the morphological guessing rules do improve guessing performance." ></td>
	<td class="line x" title="297:319	Since they are more accurate than ending-guessing rules they were applied first and improved the precision of the guesses by about 8%." ></td>
	<td class="line x" title="298:319	This resulted in about 2% higher accuracy in the tagging of unknown words." ></td>
	<td class="line x" title="299:319	The ending-guessing rules constitute the backbone of the guesser and cope with unknown words without clear morphological structure." ></td>
	<td class="line x" title="300:319	For instance, discussing the problem of unknown words for the robust parsing Bod (1995, 84) writes: 'Notice that richer, morphological annotation would not be of any help here; the words 'return', 'stop' and 'cost' do not have a morphological structure on the basis of which their possible lexical categories can be predicted'." ></td>
	<td class="line x" title="301:319	When we applied the ending-guessing rules to these words, the words return and stop were correctly classified as noun/verbs (NN VB VBP) and only the word cost failed to be guessed by the rules." ></td>
	<td class="line x" title="302:319	The acquired guessing rules employed in our cascading guesser are, in fact, of a standard nature, which, in some form or other, is present in other word-Pos guessers." ></td>
	<td class="line o" title="303:319	For instance, our ending-guessing rules are akin to those of Xerox and the morphological rules resemble some rules of Brill's, but ours use more constraints and provide a set of all possible tags for a word rather than a single best tag." ></td>
	<td class="line x" title="304:319	The two additional types of features used by Brill's guesser are implicitly represented in our approach as well: One of the Brill schemata checks the context of an unknown word." ></td>
	<td class="line x" title="305:319	In our approach we guess the words using their features only and provide several possibilities for a word; then at the disambiguation phase the context is used to choose the right tag." ></td>
	<td class="line x" title="306:319	As for Brill's schemata that checks the presence of a particular character in an unknown word, we capture a similar feature by collecting the endingguessing rules for proper nouns and hyphenated words separately." ></td>
	<td class="line o" title="307:319	We believe that the technique for the induction of the ending-guessing rules is quite similar to that of Xerox 1 or Schmid (1994) but differs in the scoring and pruning methods." ></td>
	<td class="line x" title="308:319	The major advantage of the proposed technique can be seen in the cascading application of the different sets of guessing rules and in far superior training data." ></td>
	<td class="line x" title="309:319	We use for training a pre-existing general-purpose (as opposed to corpus-tuned) lexicon." ></td>
	<td class="line x" title="310:319	This has three advantages:  the size of the training lexicon is large and does not depend on the size or even the existence of the annotated corpus." ></td>
	<td class="line x" title="311:319	This allows for the induction of more rules than from a lexicon derived from an annotated corpus." ></td>
	<td class="line o" title="312:319	For instance, the ending guesser of Xerox includes 536 rules whereas our Ending * guesser includes 2,196 guessing rules;  the information listed in a general-purpose lexicon can be considered to be of better quality than that derived from an annotated corpus, since it lists all possible readings for a word rather than only those that happen to occur in the corpus." ></td>
	<td class="line n" title="313:319	We also believe that general-purpose lexicons contain less erroneous information than those derived from annotated corpora; 10 Xerox's technique is not documented and can be determined only by inspection of the source code." ></td>
	<td class="line x" title="314:319	421 Computational Linguistics Volume 23, Number 3  the amount of work required to prepare the training lexicon is minimal and does not require any additional manual annotation." ></td>
	<td class="line x" title="315:319	Our experiments with the lexicon derived from the CELEX lexical database and word frequencies derived from the Brown Corpus resulted in guessing rule sets that proved to be domainand corpus-independent (but tag-set-dependent), producing similar results on texts of different origins." ></td>
	<td class="line x" title="316:319	An interesting by-product of the proposed rule-induction technique is the automatic discovery of the template morphological rules advocated in Mikheev and Liubushkina (1995)." ></td>
	<td class="line x" title="317:319	The induced morphological guessing rules turned out to consist mostly of the expected prefixes and suffixes of English and closely resemble the rules employed by the ispel| UNIX spell-checker." ></td>
	<td class="line x" title="318:319	The rule acquisition and evaluation methods described here are implemented as a modular set of c++ and AWK tools, and the guesser is easily extendible to sublanguage-specific regularities and retrainable to new tag sets and other languages, provided that these languages have affixational morphology." ></td>
	<td class="line x" title="319:319	Acknowledgments I would like to thank the anonymous referees for helpful comments on an earlier draft of this paper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P97-1029
Morphological Disambiguation By Voting Constraints
Oflazer, Kemal;Tur, Gokhan;"></td>
	<td class="line x" title="1:190	Morphological Disambiguation by Voting Constraints Kemal Oflazer and GSkhan Tfir Department of Computer Engineering and Information Science Bilkent University, Bilkent, TR-06533, Turkey {ko, tur}cs, bilkent, edu." ></td>
	<td class="line x" title="2:190	tr Abstract We present a constraint-based morphological disambiguation system in which individual constraints vote on matching morphological parses, and disambiguation of all the tokens in a sentence is performed at the end by selecting parses that receive the highest votes." ></td>
	<td class="line x" title="3:190	This constraint application paradigm makes the outcome of the disambiguation independent of the rule sequence, and hence relieves the rule developer from worrying about potentially conflicting rule sequencing." ></td>
	<td class="line x" title="4:190	Our results for disambiguating Turkish indicate that using about 500 constraint rules and some additional simple statistics, we can attain a recall of 95-96~ and a precision of 94-95~ with about 1.01 parses per token." ></td>
	<td class="line x" title="5:190	Our system is implemented in Prolog and we are currently investigating an efficient implementation based on finite state transducers." ></td>
	<td class="line x" title="6:190	1 Introduction Automatic morphological disambiguation is an important component in higher level analysis of natural language text corpora." ></td>
	<td class="line oc" title="7:190	There has been a large number of studies in tagging and morphological disambiguation using various techniques such as statistical techniques, e.g., (Church, 1988; Cutting et al. , 1992; DeRose, 1988), constraint-based techniques (Karlsson et al. , 1995; Voutilainen, 1995b; Voutilainen, Heikkil/i, and Anttila, 1992; Voutilainen and Tapanainen, 1993; Oflazer and KuruSz, 1994; Oflazer and Till 1996) and transformation-based techniques (Brilt, 1992; Brill, 1994; Brill, 1995)." ></td>
	<td class="line x" title="8:190	This paper presents a novel approach to constraint based morphological disambiguation which relieves the rule developer from worrying about conflicting rule ordering requirements." ></td>
	<td class="line x" title="9:190	The approach depends on assigning votes to constraints according to their complexity and specificity, and then letting constraints cast votes on matching parses of a given lexical item." ></td>
	<td class="line x" title="10:190	This approach does not reflect the outcome of matching constraints to the set of morphological parses immediately." ></td>
	<td class="line x" title="11:190	Only after all applicable rules are applied to a sentence, all tokens are disambiguated in parallel." ></td>
	<td class="line x" title="12:190	Thus, the outcome of the rule applications is independent of the order of rule applications." ></td>
	<td class="line x" title="13:190	Rule ordering issue has been discussed by Voutilainen(1994), but he has recently indicated 1 that insensitivity to rule ordering is not a property of their system (although Voutilainen(1995a) states that it is a very desirable property) but rather is achieved by extensively testing and tuning the rules." ></td>
	<td class="line x" title="14:190	In the following sections, we present an overview of the morphological disambiguation problem, highlighted with examples from Turkish." ></td>
	<td class="line x" title="15:190	We then present our approach and results." ></td>
	<td class="line x" title="16:190	We finally conclude with a very brief outline of our investigation into efficient implementations of our approach." ></td>
	<td class="line x" title="17:190	2 Morphological Disambiguation In all languages, words are usually ambiguous in their parts-of-speech or other morphological features, and may represent lexical items of different syntactic categories, or morphological structures depending on the syntactic and semantic context." ></td>
	<td class="line x" title="18:190	In languages like English, there are a very small number of possible word forms that can be generated from a given root word, and a small number of part-ofspeech tags associated with a given lexical form." ></td>
	<td class="line x" title="19:190	On the other hand, in languages like Turkish or Finnish with very productive agglutinative morphology, it is possible to produce thousands of forms (or even millions (Hankamer, 1989)) from a given root word and the kinds of ambiguities one observes are quite different than what is observed in languages like English." ></td>
	<td class="line x" title="20:190	In Turkish, there are ambiguities of the sort typically found in languages like English (e.g. , book/noun vs book/verb type)." ></td>
	<td class="line x" title="21:190	However, the agglutinative nature of the language usually helps resolution of such ambiguities due to the restrictions on morphotactics of subsequent morphemes." ></td>
	<td class="line x" title="22:190	On the 1Voutilainen, Private communication." ></td>
	<td class="line x" title="23:190	222 other hand, this very nature introduces another kind of ambiguity, where a lexical form can be morphologically interpreted in many ways not usually predictable in advance." ></td>
	<td class="line x" title="24:190	Furthermore, Turkish allows very productive derivational processes and the information about the derivational structure of a word form is usually crucial for disambiguation (Oflazer and Tiir, 1996)." ></td>
	<td class="line x" title="25:190	Most kinds of morphological ambiguities that we have observed in Turkish typically fall into one the following classes: ~ 1." ></td>
	<td class="line x" title="26:190	the form is uninflected and assumes the default inflectional features, e.g., I. taS (made of stone) \[ \[CAT=ADJ\] \[ROOT=taS\]\] 2." ></td>
	<td class="line x" title="27:190	taS (stone) \[ \[CAT=NOUN\] \[ROOT=taS\] \[AGR=3SG\] \[POSS=NONE\] \[CASE=NOM\] \] 3." ></td>
	<td class="line x" title="28:190	taS (overflow)!" ></td>
	<td class="line x" title="29:190	\[ \[CAT=VERB\] \[ROOT=t aS\] \[SENSE=POS\] \[TAMI=IMP\] \[AGR=2SQ\]\] 2." ></td>
	<td class="line x" title="30:190	Lexically different affixes (conveying different morphological features) surface the same due to the morphographemic context, e.g., 1." ></td>
	<td class="line x" title="31:190	ev+\[n\]in (of the house) \[ \[CAT=NOUN\] \[ROOT=ev\] \[AGR=3SG\] \[PDSS=NONE\] \[CASE=GEN\] \] 2." ></td>
	<td class="line x" title="32:190	ev+in (your house) \[ \[CAT=NOUN\] \[ROOT=ev\] \[AGR=3SG\] \[POSS=2SG\] \[CASE=NOM\] \] 3." ></td>
	<td class="line x" title="33:190	The root of one of the parses is a prefix string of the root, of the other parse, and the parse with the shorter root word has a suffix which surfaces as the rest of the longer root word, e.g., 1." ></td>
	<td class="line x" title="34:190	koyu+\[u\]n (your dark (thing)) \[ \[CAT=ADJ\] \[ROOT=koyu\] \[CONV=NOUN=NONE\] \[AGR=3SG\] \[POSS=2SG\] \[CASE=NOM\]\] 2." ></td>
	<td class="line x" title="35:190	koyun (sheep) \[ \[CAT=NOUN\] \[ROOT=koyun\] \[AGR=3SG\] \[POSS=NONE\] \[CASE=NOM\] \] 3." ></td>
	<td class="line x" title="36:190	koy+\[n\]un (of the bay) \[ \[CAT=NOUN\] \[ROOT=koy\] \[AGR=3SG\] \[POSS=NONE\] \[CASE=GEN\] \] 4." ></td>
	<td class="line x" title="37:190	koy+un (ybur bay) \[ \[CAT=NOUN\] \[R00T=koy\] \[AGR=bSG\] \[POSS=RSG\] \[CASE=NOM\]\] 2Output of the morphological analyzer is edited for clarity, and English glosses have been given." ></td>
	<td class="line x" title="38:190	We have also provided the morpheme structure, where \[\]s, indicate elision." ></td>
	<td class="line x" title="39:190	Glosses are given as linear feature value sequences corresponding to the morphemes (which are not shown)." ></td>
	<td class="line x" title="40:190	The feature names are as follows: CAT-major category, TYPE-minor category, R00T-main root form, AGR -number and person agreement, P0SS possessive agreement, CASE surface case, CONV conversion to the category following with a certain suffix indicated by the argument after that, TAMl-tense, aspect, mood marker 1, SENSE-verbal polarity." ></td>
	<td class="line x" title="41:190	Upper cases in morphological output indicates one of the non-ASCII special Turkish characters: e.g., G denotes ~, U denotes /i, etc. 5." ></td>
	<td class="line x" title="42:190	koy+\[y\]un (put)!" ></td>
	<td class="line x" title="43:190	\[ \[CAT=VERB\] \[ROOT=koy\] \[SENSE=POS\] \[TAMI=IMP\] \[AGR=2PL\] \] 4." ></td>
	<td class="line x" title="44:190	The roots take different numbers of unrelated inflectional and/or derivational suffixes which when concatenated turn out to have the same surface form, e.g., I. yap+madan (without having done (it)) \[ \[CAT=VERB\] \[ROOT=yap\] \[SENSE=POS\] \[CONV=ADVERB=MADAN\] \] 2." ></td>
	<td class="line x" title="45:190	yap+ma+dan (from doing (it)) \[ \[CAT=VERB\] \[ROOT=yap\] \[SENSE=POS\] \[CONV=NOUN=MA\] \[TYPE=INFINITIVE\] \[AGR=3SG\] \[POSS=NONE\] \[CASE=ABL\] \] 5." ></td>
	<td class="line x" title="46:190	One of the ambiguous parses is a lexicalized form while another is form derived by a productive derivation as in 1 and 2 below." ></td>
	<td class="line x" title="47:190	6." ></td>
	<td class="line x" title="48:190	The same suffix appears in different positions in the morphotactic paradigm conveying different information as in 2 and 3 below." ></td>
	<td class="line x" title="49:190	1." ></td>
	<td class="line x" title="50:190	uygulama / (application) \[ \[CAT=NOUN\] \[ROOT=uygulama\] \[AGR=3SG\] \[POSS=NONE\] \[CASE=NDM\] \] 2." ></td>
	<td class="line x" title="51:190	uygula+ma / ((the act of) applying) \[ \[CAT=VERB\] \[ROOT=uygula\] \[SENSE=POS\] \[CONV=NOUN=MA\] \[TYPE=INFINITIVE\] \[AGR=3SG\] \[POSS=NONE\] \[CASE=NOM\] \] 3." ></td>
	<td class="line x" title="52:190	uygula+ma / (do not apply)!" ></td>
	<td class="line x" title="53:190	\[ \[CAT=VERB\] \[ROOT=uygula\] \[SENSE=NEG\] \[TAMI=IMP\] \[AGR=2SG\] \]  The main intent of our system is to achieve morphological disambiguation by choosing for a given ambiguous token, the correct parse in a given context." ></td>
	<td class="line x" title="54:190	It is certainly possible that a given token may have nmltiple correct parses, usually with the same inflectional features, or with inflectional features not ruled out by the syntactic context, but one will be the 'correct' parse usually on semantic grounds." ></td>
	<td class="line x" title="55:190	We consider a token fully disambiguated if it has only one morphological parse remaining after automatic disambiguation." ></td>
	<td class="line x" title="56:190	We Consider a token as correctly disambiguated, if one of the parses remaining for that token is the correct intended parse." ></td>
	<td class="line x" title="57:190	We evaluate the resulting disambiguated text by a number of metrics defined as follows (Voutilainen, 1995a): #Parses Ambiguity#Tokens Recall = #Tokens Correctly Disambiguated #Tokens Precision = #Tokens Correctly Disambiguated #Parses In the ideal case where each token is uniquely and correctly disambiguated with the correct parse, both recall and precision will be 1.0." ></td>
	<td class="line x" title="58:190	On the other hand, a 223 text where each token is annotated with all possible parses, 3 the recall will be 1.0, but the precision will be low." ></td>
	<td class="line x" title="59:190	The goal is to have both recall and precision as high as possible." ></td>
	<td class="line x" title="60:190	3 Constraint-based Morphological Disambiguation This section outlines our approach to constraintbased morphological disambiguation where constraints vote on matching parses of sequential tokens." ></td>
	<td class="line x" title="61:190	3.1 Constraints on morphological parses We describe constraints on the morphological parses of tokens using rules with two components R= (Cl,C~.,,C,~;V) where the Ci are (possibly hierarchical) feature constraints on a sequence of the morphological parses, and V is an integer denoting the vote of the rule." ></td>
	<td class="line x" title="63:190	To illustrate the flavor of our rules we can give the following examples: 1." ></td>
	<td class="line x" title="64:190	The following rule with two constraints matches parses with case feature ablative, preceding a parse matching a postposition subcategorizing for an ablative nominal form." ></td>
	<td class="line x" title="65:190	\[ \[case : abl\], \[cat : postp, subcat : abl\] \] 2." ></td>
	<td class="line x" title="66:190	The rule \[ \[agr : '2SG', case : gen\], \[cat : noun, poss : ' 2SG '\] \] matches a nominal form with a possessive marker 2SG, following a pronoun with 2SG agreement and genitive case, enforcing the simplest form of noun phrase constraints." ></td>
	<td class="line x" title="67:190	3." ></td>
	<td class="line x" title="68:190	In general constraints can make references to tile derivational structure of the lexical form and hence be hierarchicah For instance, the following rule is an example of a rule employing a hierarchical constraint: \[ \[cat : adj, stem : \[taml : narr\] \], \[cat : noun, st em :no\] \] which matches tile derived participle reading of a verb with narrative past tense, if it is followed by an underived noun parse." ></td>
	<td class="line x" title="69:190	3.2 Determining the vote of a rule There are a number of ways votes can be assigned to rules." ></td>
	<td class="line x" title="70:190	For the purposes of this work the vote of a rule is determined by its static properties, but it is certainly conceivable that votes can be assigned or learned by using statistics from disambiguated corpora." ></td>
	<td class="line x" title="71:190	4 For static vote assignment, intuitively, we would like to give high votes to rules that are more specific: i.e., to rules that have aAssuming no unknown words." ></td>
	<td class="line x" title="72:190	4We have left this for future work." ></td>
	<td class="line x" title="73:190	 higher number of constraints,  higher number of features in the constraints,  constraints that make reference to nested stems (from which the current form is derived),  constraints that make reference to very specific features or values." ></td>
	<td class="line x" title="74:190	Let R = (C1,C2,'',C~;V) be a constraint rule." ></td>
	<td class="line x" title="75:190	The vote V is determined as n v = i=l where V(Ci) is the contribution of constraint Ci to the vote of the rule R. A (generic) constraint has the following form: C -\[(fl : vl) (f2 : v2)&5 (fro : vm)\] where fi is the name of a morphological feature, and vi is one of the possible values for that feature." ></td>
	<td class="line x" title="76:190	The contribution of fi : vi in the vote of a constraint depends on a number of factors: 1." ></td>
	<td class="line x" title="77:190	The value vi may be a distinguished value that has a more important function in disambiguation." ></td>
	<td class="line x" title="78:190	5 In this case, the weight of the feature constraint is w(vi)(> 1)." ></td>
	<td class="line x" title="79:190	2." ></td>
	<td class="line x" title="80:190	The feature itself may be a distinguished feature which has more important function in disambiguation." ></td>
	<td class="line x" title="81:190	In this case the weight of the feature is w(fi)(> 1)." ></td>
	<td class="line x" title="82:190	3." ></td>
	<td class="line x" title="83:190	If the feature fi refers to the stem of a derived form and the value part of the feature constraint is a full fledged constraint C' on the stem structure, the weight of the feature constraint is found by recursively computing the vote of C' and scaling the resulting value by a factor (2 in our current system) to improve its specificity." ></td>
	<td class="line x" title="84:190	4." ></td>
	<td class="line x" title="85:190	Otherwise, the weight of the feature constraint is 1." ></td>
	<td class="line x" title="86:190	For example suppose we have the following constraint: \[cat :noun, case : gen, stem:\[cat:adj, stem:\[cat:v\], suffix=mis\]\] Assuming the value gen is a distinguished value with weight 4 (cf., factor 1 above), the vote of this constraint is computed as follows: 1." ></td>
	<td class="line x" title="88:190	cat :noun contributes 1, 2." ></td>
	<td class="line x" title="89:190	case:gen contributes 4, 3." ></td>
	<td class="line x" title="90:190	stem:\[cat:adj, stem: \[cat:v\],suffix=mis\] contributes 8 computed as follows: (a) cat :adj contributes 1, 5For instance, for Turkish we have noted that the genitive case marker is usually very helpful in disambiguation." ></td>
	<td class="line x" title="91:190	224 (b) suffYx=mS.s contributes 1, (c) stem: \[cat:v\] contributes 2 = 2* 1, the 1 being from cat : v, (d) the sum 4 is scaled by 2 to give 8." ></td>
	<td class="line x" title="92:190	4. Votes from steps 1, 2 and 3(d) are added up to give 13 as the constraint vote." ></td>
	<td class="line x" title="93:190	We also employ a set of rules which express preferences among the parses of single lexical form independent of the context in which the form occurs." ></td>
	<td class="line x" title="94:190	The weights for these rules are currently manually determined." ></td>
	<td class="line x" title="95:190	These rules give negative votes to the parses which are not preferred or high votes to certain parses which are always preferred." ></td>
	<td class="line x" title="96:190	Our experience is that such preference rules depend on the kind of the text one is disambiguating." ></td>
	<td class="line x" title="97:190	For instance if one is disambiguating a manual of some sort, imperative readings of verbs are certainly possible, whereas in normal plain text with no discourse, such readings are discouraged." ></td>
	<td class="line x" title="98:190	3.3 Voting and selecting parses A rule R = (C1,62,'', Cn; V) will match a sequence of tokens wi, Wi+l,  ., wi+n-1 within a sentence wl through ws if some morphological parse of every token wj,i < j < i + n 1 is subsumed by the corresponding constraint Cj-i+l. When all constraints match, the votes of all the matching parses are incremented by V. If a given constraint matches more than one parse of a token, then the votes of all such matching parses are incremented." ></td>
	<td class="line x" title="100:190	After all rules have been applied to all token positions in a sentence and votes are tallied, morphological parses are selected in the following manner." ></td>
	<td class="line x" title="101:190	Let vt and Vh be the votes of the lowest and highest scoring parses for a given token." ></td>
	<td class="line x" title="102:190	All parses with votes equal to or higher than vt + m * (Vh -vt) are selected with m (0 _< m _< 1) being a parameter." ></td>
	<td class="line x" title="103:190	m = 1 selects the highest scoring parse(s)." ></td>
	<td class="line x" title="104:190	4 Results from Disambiguating Turkish Text We have applied our approach to disambiguating Turkish text." ></td>
	<td class="line x" title="105:190	Raw text is processed by a preprocessor which segments the text into sentences using various heuristics about punctuation, and then tokenizes and runs it through a wide-coverage highperformance morphological analyzer developed using two-level morphology tools by Xerox (Karttunen, 1993)." ></td>
	<td class="line x" title="106:190	The preprocessor module also performs a number of additional functions such as grouping of lexicalizcd and non-lexicalized collocations, compound verbs, etc. , (Ofiazer and Kurubz, 1994; Oflazer and Tiir, 1996)." ></td>
	<td class="line x" title="107:190	The preprocessor also uses a second morphological processor for dealing with unknown words which recovers any derivational and inflectional information from a word even if the root word is not known." ></td>
	<td class="line x" title="108:190	This unknown word processor has a (nominal) root lexicon which recognizes S +, where S is the Turkish surface alphabet (in the two-level morphology sense), but then tries to interpret an arbitrary postfix string of the unknown word, as a sequence of Turkish suffixes subject to all morphographemic constraints (Oflazer and Tfir, 1996)." ></td>
	<td class="line x" title="109:190	We have applied our approach to four texts labeled ARK, HIST, MAN, EMB, with statistics given in Table 1." ></td>
	<td class="line x" title="110:190	The tokens considered are those that are generated after morphological analysis, unknown word processing and any lexical coalescing is done." ></td>
	<td class="line x" title="111:190	The words that are counted as unknown are those that could not even be processed by the unknown noun processor as they violate Turkish morphographemic constraints." ></td>
	<td class="line x" title="112:190	Whenever an unknown word has more than one parse it is counted under the appropriate group." ></td>
	<td class="line x" title="113:190	6 The fourth and fifth columns in this table give the average parses per token and the initial precision assuming initial recall is 100%." ></td>
	<td class="line x" title="114:190	We have disambiguated these texts using a rule base of about 500 hand-crafted rules." ></td>
	<td class="line x" title="115:190	Most of the rule crafting was done using the general linguistic constraints and constraints that we derived from the first text, ARK. In this sense, this text is our 'training data', while the other three texts were not considered in rule crafting." ></td>
	<td class="line x" title="116:190	Our results are summarized in Table 2." ></td>
	<td class="line x" title="117:190	The last four columns in this table present results for different values for the parameter rn mentioned above, m = 1 denoting the case when only the highest scoring parse(s) is (are) selected." ></td>
	<td class="line x" title="118:190	The columns for m < 1 are presented in order to emphasize that drastic loss of precision for those cases." ></td>
	<td class="line x" title="119:190	Even at m = 0.95 there is considerable loss of precision and going up to m = 1 causes a dramatic increase in precision without a significant loss in recall." ></td>
	<td class="line x" title="120:190	It can be seen that we can attain very good recall and quite acceptable precision with just voting constraint rules." ></td>
	<td class="line x" title="121:190	Our experience is that we can in principle add highly specialized rules by covering a larger text base to improve our recall and precision for the m = 1." ></td>
	<td class="line x" title="122:190	A post-mortem analysis has shown that cases that have been missed are mostly due to morphosyntactic dependencies that span a context much wider that 5 tokens that we currently employ." ></td>
	<td class="line x" title="123:190	4.1 Using root and contextual statistics We have employed two additional sources of information: root word usage statistics, and contextual statistics." ></td>
	<td class="line x" title="124:190	We have statistics compiled from previously disambiguated text, on root frequencies." ></td>
	<td class="line x" title="125:190	After the application of constraints as described above, for 6The reason for the (comparatively) high number of unknown words in MAN, is that tokens found in such texts, like.\[10, denoting a function key in the computer can not be parsed as a Turkish root word!" ></td>
	<td class="line x" title="126:190	225 Text Sent." ></td>
	<td class="line x" title="127:190	ARK 492 HIST 270 MAN 204 EMB 198 Tokens 7928 5212 2756 5177 Parses/ Token 1.823 1.797 1.840 1.914 Init." ></td>
	<td class="line x" title="128:190	Prec." ></td>
	<td class="line x" title="129:190	0 0.55 0.15% 0.56 0.02% 0.54 0.65% 0.52 0.09% Distribution of Morphological Parses 1 2 3 4 >4 49.34% 30.93% 9.19% 8.46% 1.93% 50.63% 30.68% 8.62% 8.36% 1.69% 49.01% 31.70% 6.37% 8.91% 3.36% 43.94% 34.58% 9.60% 9.46% 2.33% Table 1: Statistics on Texts Vote Range Selected(m) TEXT 1.0 0.95 0.8 0.6 ARK Rec." ></td>
	<td class="line x" title="130:190	98.05 98.47 98.69 98.77 Prec." ></td>
	<td class="line x" title="131:190	94.13 87.65 84.41 82.43 Amb." ></td>
	<td class="line x" title="132:190	1.042 1.123 1.169 1.200 HIST Rec." ></td>
	<td class="line x" title="133:190	97.03 97.65 98.81 97.01 Prec." ></td>
	<td class="line x" title="134:190	94.13 87.10 84.41 82.29 Amb." ></td>
	<td class="line x" title="135:190	1.058 1.121 1.169 1.189 'I~IAN Rec." ></td>
	<td class="line x" title="136:190	97.03 97.92 97.81 98.77 Prec." ></td>
	<td class="line x" title="137:190	91.05 83.51 79.85 77.34 Amb." ></td>
	<td class="line x" title="138:190	1.068 1.172 1.237 1.277 EMB Rec." ></td>
	<td class="line x" title="139:190	96.51 97.48 97.76 97.94 Prec." ></td>
	<td class="line x" title="140:190	91.28 84.36 77.87 75.79 Amb." ></td>
	<td class="line x" title="141:190	1.057 1.150 1.255 1.292 Table 2: Results with voting constraints TEXT V V+R V+R+C ARK Rec." ></td>
	<td class="line x" title="142:190	98.05 97.60 96.98 Prec." ></td>
	<td class="line x" title="143:190	94.13 95.28 '96.19 Amb." ></td>
	<td class="line x" title="144:190	1.042 1.024 1.008 HIST Rec." ></td>
	<td class="line x" title="145:190	97:03 96.52 95.62 Prec." ></td>
	<td class="line x" title="146:190	94.13 92.59 94.33 Amb." ></td>
	<td class="line x" title="147:190	1.058 1.042 1.013 MAN Rec." ></td>
	<td class="line x" title="148:190	97.03 96.47 95.84 Prec." ></td>
	<td class="line x" title="149:190	91.05 93.08 94.47 Amb." ></td>
	<td class="line x" title="150:190	1.058 1.042 1.014 EMB Rec." ></td>
	<td class="line x" title="151:190	96.51 96.47 95.37 Prec." ></td>
	<td class="line x" title="152:190	91.28 93.08 94.45 Amb." ></td>
	<td class="line x" title="153:190	1.057 1.036 1.009 Table 3: Results with voting constraints and root statistics, context statistics tokens which are still ambiguous with ambiguity resulting from different root words, we discard parses if the frequencies of the root words for those parses are considerably lower than the frequency of the root of the highest scoring parse." ></td>
	<td class="line x" title="154:190	The results after applying this step on top of voting, with m = 1, are shown in the fourth column of Table 3 (labeled V+R)." ></td>
	<td class="line x" title="155:190	On top of this, we use the following heuristic using context statistics to eliminate any further ambiguities." ></td>
	<td class="line x" title="156:190	For every remaining ambiguous token with unambiguous immediate left and right contexts (i.e. , the tokens in the immediate left and right are unambiguous), we perform the following, by ignoring the root/stem feature of ~he parses: 1." ></td>
	<td class="line x" title="157:190	For every ambiguous parse in such an unambiguous context, we count how many times, this parse occurs unambiguously in exactly the same unambiguous context, in the rest of the text." ></td>
	<td class="line x" title="158:190	2." ></td>
	<td class="line x" title="159:190	We then choose the parse whose count is substantially higher than the others." ></td>
	<td class="line x" title="160:190	The results after applying this step on of the previous two steps are shown in the last column of Table 3 (labeled V+R+C)." ></td>
	<td class="line x" title="161:190	One can see from the last three columns of this table, the impact of each of the steps." ></td>
	<td class="line x" title="162:190	By ignoring root/stem features during this process, we essentially are considering just the top level inflectional information of the parses." ></td>
	<td class="line x" title="163:190	This is very similar to Brill's use of contexts to induce transformation rules for his tagger (Brill, 1992; Brill, 1995), but instead of generating transformation rules from a training text, we gather statistics and apply them to parses in the text being disambiguated." ></td>
	<td class="line x" title="164:190	5 Efficient Implementation Techniques and Extensions The current implementation of the voting approach is meant to be a proof of concept implementation and is rather inefficient." ></td>
	<td class="line x" title="165:190	However, the use of regular relations and finite state transducers (Kaplan and Kay, 1994) provide a very efficient implementation method." ></td>
	<td class="line x" title="166:190	For this, we view the parses of the tokens making up a sentence as making up a acyclic a finite state recognizer with the states marking word boundaries and the ambiguous interpretations of the tokens as the state transitions between states, the rightmost node denoting the final state, as depicted in Figure 1 for a sentence with 5 tokens." ></td>
	<td class="line x" title="167:190	In Figure 1, the transition labels are triples of the sort (wi, pj, O) for the jth parse of token i, with the 0 indicating the initial vote of the parse." ></td>
	<td class="line x" title="168:190	The rules imposing constraints can also be represented as transducers which increment the votes of the matching transi226 (wl,pl,O) (w2,pl,O) (W3,pl,O) (w4,pl,O) (w5,pl,O) (wl,p3,0) (W2,p5,0) (w3,p4,0) (W4,p3,0) (W5,p4,0) Figure 1: Sentence as a finite state recognizer." ></td>
	<td class="line x" title="169:190	tion labels by an appropriate amount." ></td>
	<td class="line x" title="170:190	~ Such transducers ignore and pass through unchanged, parses that they are not sensitive to." ></td>
	<td class="line x" title="171:190	When a finite state recognizer corresponding to the input sentence (which actually may be considered as an identity transducer) is composed with a constraint transducer, one gets a slightly modified version of the sentence transducer with possibly additional transitions and states, where the votes of some of the labels have been appropriately increlnented." ></td>
	<td class="line x" title="172:190	When the sentence transducer is composed with all the constraint transducers in sequence, all possible votes are cast and the final sentence transducer reflects all the votes." ></td>
	<td class="line x" title="173:190	The parse corresponding to each token with the highest vote can then be selected." ></td>
	<td class="line x" title="174:190	The key point here is that due to the nature of the composition operator, the constraint transducers can be composed off-line first, giving a single constraint transducer and then this one is composed with every sentence transducer once (See Figure 2)." ></td>
	<td class="line x" title="175:190	The idea of voting can further be extended to a path voting framework where rules vote on paths containing sequences of matching parses and the path from the start state to the final stated with the highest votes received, is then selected." ></td>
	<td class="line x" title="176:190	This can be implemented again using finite state transducers as described above (except that path vote is apportioned equally to relevant parse votes), but instead of selecting highest scoring parses, one selects the path from the start state to one of the final states where the sum of the parse votes is maximum." ></td>
	<td class="line x" title="177:190	We have recently completed a prototype implementation of this approach (in C) for English (Brown Corpus) and have obtained quite similar results (Tiir, Oflazer, and Oz-kan, 1997)." ></td>
	<td class="line x" title="178:190	6 Conclusions We have presented an approach to constraint-based morphological disambiguation which uses constraint voting as its primary mechanism for parse selection and alleviates the rule developer from worrying about rule ordering issues." ></td>
	<td class="line x" title="179:190	Our approach is quite general and is applicable to any language." ></td>
	<td class="line x" title="180:190	Rules describing language specific linguistic constraints vote on matching parses of tokens, and at the end, parses TSuggested by Lauri Karttunen (private communication)." ></td>
	<td class="line x" title="181:190	for every token receiving the highest tokens are selected." ></td>
	<td class="line x" title="182:190	We have applied this approach to Turkish, a language with complex agglutinative word forms exhibiting morphological ambiguity phenomena not usually found in languages like English and have obtained quite promising results." ></td>
	<td class="line x" title="183:190	The convenience of adding new rules in without worrying about where exactly it goes in terms of rule ordering (something that hampered our progress in our earlier work on disambiguating Turkish morphology (Oflazer and KuruSz, 1994; Oflazer and Tiir, 1996)), has also been a key positive point." ></td>
	<td class="line x" title="184:190	Furthermore, it is also possible to use rules with negative votes to disallow impossible cases." ></td>
	<td class="line x" title="185:190	This has been quite useful for our work on tagging English (Tfir, Oflazer, and 0z-kan, 1997) where such rules with negative weights were used to fine tune the behavior of the tagger in various problematic cases." ></td>
	<td class="line x" title="186:190	The proposed approach is also amenable to an efficient implementation by finite state transducers (Kaplan and Kay, 1994)." ></td>
	<td class="line x" title="187:190	By using finitestate transducers, it is furthermore possible to use a bit more expressive rule formalism including for instance the Kleene * operator so that one can use a much smaller set of rules to cover the same set of local linguistic phenomena." ></td>
	<td class="line x" title="188:190	Our current and future work in this framework involves the learning of constraints and their votes from corpora, and combining learned and handcrafted rules." ></td>
	<td class="line x" title="189:190	7 Acknowledgments This research has been supported in part by a NATO Science for Stability Grant TU-LANGUAGE." ></td>
	<td class="line x" title="190:190	We thank Lauri Karttunen of Rank Xerox Research Centre in Grenoble for providing the Xerox two-level morphology tools on which the Turkish morphological analyzer was built." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P97-1031
A Flexible POS Tagger Using An Automatically Acquired Language Model
Mrquez, Llus;Padro, Lluis;"></td>
	<td class="line x" title="1:213	A Flexible POS Tagger Using an Automatically Acquired Language Model* Llufs Mhrquez LSIUPC c/Jordi Girona 1-3 08034 Barcelona." ></td>
	<td class="line x" title="2:213	Catalonia lluismisi, upc." ></td>
	<td class="line x" title="3:213	es Llu/s Padr6 LSIUPC c/Jordi Girona 1-3 08034 Barcelona." ></td>
	<td class="line x" title="4:213	Catalonia padro@isi, upc." ></td>
	<td class="line x" title="5:213	es Abstract We present an algorithm that automatically learns context constraints using statistical decision trees." ></td>
	<td class="line x" title="6:213	We then use the acquired constraints in a flexible POS tagger." ></td>
	<td class="line x" title="7:213	The tagger is able to use information of any degree: n-grams, automatically learned context constraints, linguistically motivated manually written constraints, etc. The sources and kinds of constraints are unrestricted, and the language model can be easily extended, improving the results." ></td>
	<td class="line x" title="8:213	The tagger has been tested and evaluated on the WSJ corpus." ></td>
	<td class="line x" title="9:213	1 Introduction In NLP, it is necessary to model the language in a representation suitable for the task to be performed." ></td>
	<td class="line x" title="10:213	The language models more commonly used are based on two main approaches: first, the linguistic approach, in which the model is written by a linguist, generally in the form of rules or constraints (Voutilainen and Jgrvinen, 1995)." ></td>
	<td class="line oc" title="11:213	Second, the automatic approach, in which the model is automatically obtained from corpora (either raw or annotated) 1, and consists of n-grams (Garside et al. , 1987; Cutting et ah, 1992), rules (Hindle, 1989) or neural nets (Schmid, 1994)." ></td>
	<td class="line x" title="12:213	In the automatic approach we can distinguish two main trends: The low-level data trend collects statistics from the training corpora in the form of n-grams, probabilities, weights, etc. The high level data trend acquires more sophisticated information, such as context rules, constraints, or decision trees (Daelemans et al. , 1996; M/~rquez and Rodriguez, 1995; Samuelsson et al. , 1996)." ></td>
	<td class="line x" title="13:213	The acquisition methods range from supervised-inductivelearning-from-example algorithms (Quinlan, 1986; *This research has been partially funded by the Spanish Research Department (CICYT) and inscribed as TIC96-1243-C03-02 I When the model is obtained from annotated corpora we talk about supervised learning, when it is obtained from raw corpora training is considered unsupervised." ></td>
	<td class="line x" title="14:213	Aha et al. , 1991) to genetic algorithm strategies (Losee, 1994), through the transformation-based error-driven algorithm used in (Brill, 1995), Still another possibility are the hybrid models, which try to join the advantages of both approaches (Voutilainen and Padr6, 1997)." ></td>
	<td class="line x" title="15:213	We present in this paper a hybrid approach that puts together both trends in automatic approach and the linguistic approach." ></td>
	<td class="line x" title="16:213	We describe a POS tagger based on the work described in (Padr6, 1996), that is able to use bi/trigram information, automatically learned context constraints and linguistically motivated manually written constraints." ></td>
	<td class="line x" title="17:213	The sources and kinds of constraints are unrestricted, and the language model can be easily extended." ></td>
	<td class="line x" title="18:213	The structure of the tagger is presented in figure 1." ></td>
	<td class="line x" title="19:213	Language Model." ></td>
	<td class="line x" title="20:213	I~:.i:;:;~: I / le~ed | t wri.e. |  l i.wco us Figure h Tagger architecture." ></td>
	<td class="line x" title="21:213	Corpus We also present a constraint-acquisition algorithm that uses statistical decision trees to learn context constraints from annotated corpora and we use the acquired constraints to feed the POS tagger." ></td>
	<td class="line x" title="22:213	The paper is organized as follows." ></td>
	<td class="line x" title="23:213	In section 2 we describe our language model, in section 3 we describe the constraint acquisition algorithm, and in section 4 we expose the tagging algorithm." ></td>
	<td class="line x" title="24:213	Descriptions of the corpus used, the experiments performed and the results obtained can be found in sections 5 and 6." ></td>
	<td class="line x" title="25:213	2 Language Model We will use a hybrid language model consisting of an automatically acquired part and a linguist-written part." ></td>
	<td class="line x" title="26:213	238 The automatically acquired part is divided in two kinds of information: on the one hand, we have bigrams and trigrams collected from the annotated training corpus (see section 5 for details)." ></td>
	<td class="line x" title="27:213	On the other hand, we have context constraints learned from the same training corpus using statistical decision trees, as described in section 3." ></td>
	<td class="line x" title="28:213	The linguistic part is very small --since there were no available resources to develop it further-and covers only very few cases, but it is included to illustrate the flexibility of the algorithm." ></td>
	<td class="line x" title="29:213	A sample rule of the linguistic part: i0.0 (XvauxiliarY)." ></td>
	<td class="line x" title="30:213	(-\[VBN IN, : JJ JJS JJR\])+ <VBN> ; This rule states that a tag past participle (VBN) is very compatible (10.0) with a left context consisting of a %vauxiliar% (previously defined macro which includes all forms of 'have' and 'be') provided that all the words in between don't have any of the tags in the set \[VBN IN, : JJ JJS J JR\]." ></td>
	<td class="line x" title="31:213	That is, this rule raises the support for the tag past participle when there is an auxiliary verb to the left but only if there is not another candidate to be a past participle or an adjective inbetween." ></td>
	<td class="line x" title="32:213	The tags \[IN, :\] prevent the rule from being applied when the auxiliary verb and the participle are in two different phrases (a comma, a colon or a preposition are considered to mark the beginning of another phrase)." ></td>
	<td class="line x" title="33:213	The constraint language is able to express the same kind of patterns than the Constraint Grammar formalism (Karlsson et al. , 1995), although in a different formalism." ></td>
	<td class="line x" title="34:213	In addition, each constraint has a compatibility value that indicates its strength." ></td>
	<td class="line x" title="35:213	In the middle run, the system will be adapted to accept CGs." ></td>
	<td class="line x" title="36:213	3 Constraint Acquisition Choosing, from a set of possible tags, the proper syntactic tag for a word in a particular context can be seen as a problem of classification." ></td>
	<td class="line x" title="37:213	Decision trees, recently used in NLP basic tasks such as tagging and parsing (McCarthy and Lehnert, 1995: Daelemans et al. , 1996; Magerman, 1996), are suitable for performing this task." ></td>
	<td class="line x" title="38:213	A decision tree is a n-ary branching tree that represents a classification rule for classifying the objects of a certain domain into a set of mutually exclusive classes." ></td>
	<td class="line x" title="39:213	The domain objects are described as a set of attribute-value pairs, where each attribute measures a relevant feature of an object taking a (ideally small) set of discrete, mutually incompatible values." ></td>
	<td class="line x" title="40:213	Each non-terminal node of a decision tree represents a question on (usually) one attribute." ></td>
	<td class="line x" title="41:213	For each possible value of this attribute there is a branch to follow." ></td>
	<td class="line x" title="42:213	Leaf nodes represent concrete classes." ></td>
	<td class="line x" title="43:213	Classify a new object with a decision tree is simply following the convenient path through the tree until a leaf is reached." ></td>
	<td class="line x" title="44:213	Statistical decision trees only differs from common decision trees in that leaf nodes define a conditional probability distribution on the set of classes." ></td>
	<td class="line x" title="45:213	It is important to note that decision trees can be directly translated to rules considering, for each path from the root to a leaf, the conjunction of all questions involved in this path as a condition and the class assigned to the leaf as the consequence." ></td>
	<td class="line x" title="46:213	Statistical decision trees would generate rules in the same manner but assigning a certain degree of probability to each answer." ></td>
	<td class="line x" title="47:213	So the learning process of contextual constraints is performed by means of learning one statistical decision tree for each class of POS ambiguity -~ and converting them to constraints (rules) expressing compatibility/incompatibility of concrete tags in certain contexts." ></td>
	<td class="line x" title="48:213	Learning Algorithm The algorithm we used for constructing the statistical decision trees is a non-incremental supervised learning-from-examples algorithm of the TDIDT (Top Down Induction of Decision Trees) family." ></td>
	<td class="line x" title="49:213	It constructs the trees in a top-down way, guided by the distributional information of the examples, but not on the examples order (Quinlan, 1986)." ></td>
	<td class="line x" title="50:213	Briefly." ></td>
	<td class="line x" title="51:213	the algorithm works as a recursive process that departs from considering the whole set of examples at the root level and constructs the tree ina top-down way branching at any non-terminal node according to a certain selected attribute." ></td>
	<td class="line x" title="52:213	The different values of this attribute induce a partition of the set of examples in the corresponding subsets, in which the process is applied recursively in order to generate the different subtrees." ></td>
	<td class="line x" title="53:213	The recursion ends, in a certain node, either when all (or almost all) the remaining examples belong to the same class, or when the number of examples is too small." ></td>
	<td class="line x" title="54:213	These nodes are the leafs of the tree and contain the conditional probability distribution, of its associated subset, of examples, on the possible classes." ></td>
	<td class="line x" title="55:213	The heuristic function for selecting the most useful attribute at each step is of a crucial importance in order to obtain simple trees, since no backtracking is performed." ></td>
	<td class="line x" title="56:213	There exist two main families of attribute-selecting functions: information-based (Quinlan, 1986: Ldpez, 1991) and statistically--based (Breiman et al. , 1984; Mingers, 1989)." ></td>
	<td class="line x" title="57:213	Training Set For each class of POS ambiguity the initial example set is built by selecting from the training corpus Classes of ambiguity are determined by the groups of possible tags for the words in the corpus, i.e, nounadjective, noun-adjective-verb, preposition-adverb, etc. 239 all the occurrences of the words belonging to this ambiguity class." ></td>
	<td class="line x" title="58:213	More particularly, the set of attributes that describe each example consists of the part-of-speech tags of the neighbour words, and the information about the word itself (orthography and the proper tag in its context)." ></td>
	<td class="line x" title="59:213	The window considered in the experiments reported in section 6 is 3 words to the left and 2 to the right." ></td>
	<td class="line x" title="60:213	The following are two real examples from the training set for the words that can be preposition and adverb at the same time (IN-RB conflict)." ></td>
	<td class="line x" title="61:213	VB DT NN <'as',IN> DT JJ NN IN NN <'once',RB> VBN TO Approximately 90% of this set of examples is used for the construction of the tree." ></td>
	<td class="line x" title="62:213	The remaining 10% is used as fresh test corpus for the pruning process." ></td>
	<td class="line x" title="63:213	Attribute Selection Function For the experiments reported in section 6 we used a attribute selection function due to L6pez de Mintaras (L6pez." ></td>
	<td class="line x" title="64:213	1991), which belongs to the informationbased family." ></td>
	<td class="line x" title="65:213	Roughly speaking, it defines a distance measure between partitions and selects for branching the attribute that generates the closest partition to the correc* partaion, namely the one that joins together all the examples of the same class." ></td>
	<td class="line x" title="66:213	Let X be aset of examples, C the set of classes and Pc(X) the partition of X according to the values of C. The selected attribute will be the one that generates the closest partition of X to Pc(X)." ></td>
	<td class="line x" title="67:213	For that we need to define a distance measure between partitions." ></td>
	<td class="line x" title="68:213	Let PA(X) be the partition of X induced by the values of attribute A. The average information of such partition is defined as follows: I(PA(X)) = ~, p(X,a) log,.p(X,a), aEPa(X) where p(X. a) is the probability for an element of X belonging to the set a which is the subset of X whose examples have a certain value for the attribute .4, and it is estimated bv the ratio ~ This average  IXl ' information measure reflects the randomness of distribution of the elements of X between the classes of the partition induced by .4 If we consider now the intersection between two different partitions induced by attributes .4 and B we obtain I(PA(X) N PB(X))= E Z p(X. aMb) log,.p(X, aAb)." ></td>
	<td class="line x" title="69:213	aEP.a(A'} bEPB;XI Conditioned information of PB(X) given PA(X) iS I(PB(X)IPA(X)) = I( PA(X) M Ps(X)) I(P~(X)) = Z Z p(X, nb) log, p(X'anb) p(X,a) a~Pa(X ~, bEPBtX ~ It is easy to show that the measure d(Pa(.Y)." ></td>
	<td class="line x" title="70:213	PB(X)) = \[(Ps(X)iPA(X)) + I(PA(X)IPB(X)) is a distance." ></td>
	<td class="line x" title="71:213	Normalizing we obtain d(PA(X).PB(,\')) d. ,v(Pa(X)." ></td>
	<td class="line x" title="72:213	PB(.V)) = I(Pa(X)aPB(X)) ' with values in \[0,1\]." ></td>
	<td class="line x" title="73:213	So the selected attribute will be that one that minimizes the measure: d.v(Pc(X), PA(X))." ></td>
	<td class="line x" title="74:213	Branching Strategy Usual TDIDT algorithms consider a branch for each value of the selected attribute." ></td>
	<td class="line x" title="75:213	This strategy is not feasible when the number of values is big (or even infinite)." ></td>
	<td class="line x" title="76:213	In our case the greatest number of values for an attribute is 45 --the tag set size-which is considerably big (this means that the branching factor could be 45 at every level of the tree 3)." ></td>
	<td class="line x" title="77:213	Some s.vsterns perform a previous recasting of the attributes in order to have only binary-valued attributes and to deal with binary trees (Magerman, 1996)." ></td>
	<td class="line x" title="78:213	This can always be done but the resulting features lose their intuition and direct interpretation, and explode in number." ></td>
	<td class="line x" title="79:213	We have chosen a mixed approach which consist of splitting for all values and afterwards joining the resulting subsets into groups for which we have not enough statistical evidence of being different distributions." ></td>
	<td class="line x" title="80:213	This statistical evidence is tested with a X ~' test at a 5% level of significance." ></td>
	<td class="line x" title="81:213	In order to avoid zero probabilities the following smoothing is performed." ></td>
	<td class="line x" title="82:213	In a certain set of examples, the probability of a tag ti is estimated by I~,l+-~ ri(4) =,+~ where m is the number of possible tags and n the number of examples." ></td>
	<td class="line x" title="83:213	Additionally." ></td>
	<td class="line x" title="84:213	all the subsets that don't imply a reduction in the classification error are joined together in order to have a bigger set of examples to be treated in the following step of the tree construction." ></td>
	<td class="line x" title="85:213	The classification error of a certain node is simply: I maxt<i<m (t)(ti))." ></td>
	<td class="line x" title="86:213	Experiments reported in (.\I&rquez and Rodriguez." ></td>
	<td class="line x" title="87:213	1995) show that in this way more compact and predictive trees are obtained." ></td>
	<td class="line x" title="88:213	Pruning the Tree Decision trees that correctly classify all examples of the training set are not always the most predictive ones." ></td>
	<td class="line x" title="89:213	This is due to the phenomenon known as o,'erfitting." ></td>
	<td class="line x" title="90:213	It occurs when the training set has a certain amount of misclassified examples, which is obviously the case of our training corpus (see section 5)." ></td>
	<td class="line x" title="91:213	If we 3In real cases the branching factor is much lower since not all tags appear always in all positions of the context." ></td>
	<td class="line x" title="92:213	240 force the learning algorithm to completely classify the examples then the resulting trees would fit also the noisy examples." ></td>
	<td class="line x" title="93:213	The usual solutions to this problem are: l) Prune the tree." ></td>
	<td class="line x" title="94:213	either during the construction process (Quinlan." ></td>
	<td class="line x" title="95:213	1993) or afterwards (Mingers, 1989); 2) Smooth the conditional probability distributions using fresh corpus a (Magerman, 1996)." ></td>
	<td class="line x" title="96:213	Since another important, requirement of our problem is to have small trees we have implemented a post-pruning technique." ></td>
	<td class="line x" title="97:213	In a first step the tree is completely expanded and afterwards it is pruned following a minimal cost-complexity criterion (Breiman et al 1984)." ></td>
	<td class="line x" title="98:213	Roughly speaking this is a process that iteratively cut those subtrees producing only marginal benefits in accuracy, obtaining smaller trees at each step." ></td>
	<td class="line x" title="99:213	The trees of this sequence are tested using a, comparatively small, fresh part of the training set in order to decide which is the one with the highest degree of accuracy on new examples." ></td>
	<td class="line x" title="100:213	Experimental tests (M&rquez and Rodriguez, 1995) have shown that the pruning process reduces tree sizes at about 50% and improves their accuracy in a 2-5%." ></td>
	<td class="line x" title="101:213	An Ezample Finally, we present a real example of the simple acquired contextual constraints for the conflict IN-RB (preposition-adverb)." ></td>
	<td class="line x" title="102:213	P(IN)=0.$1 \] Pnorprobability P(RB)=0.19 \[ di~tnbunon T  ~dnghlm~g er s U-'< C,,.dm,,.wl: P(IN)=0.013 ' ' ' probuiJilm di.~tnbut.m P~RB~0.987 Figure 2: Example of a decision tree branch, The tree branch in figure 2 is translated into the following constraints: -5.81 <\['as  As'\],IN> (\[RB'I) (\[IN\]); 2.366 <\['as  As'\],RS> (\[RB\]) (\[IN\]); which express the compatibility (either positive or negative) of the word-tag pair in angle brackets with the given context." ></td>
	<td class="line x" title="103:213	The compatibility value for each constraint is the mutual information between the tag and the context (Cover and Thomas, 1991)." ></td>
	<td class="line x" title="104:213	It is directly' computed from the probabilities in the tree." ></td>
	<td class="line x" title="105:213	~Of course, this can be done only in the case of statistical decision trees." ></td>
	<td class="line x" title="106:213	4 Tagging Algorithm Usual tagging algorithms are either n-gram oriented -such as Viterbi algorithm (Viterbi." ></td>
	<td class="line x" title="107:213	1967)or adhoc for every case when they must deal with more complex information." ></td>
	<td class="line x" title="108:213	We use relaxation labelling as a tagging algorithm." ></td>
	<td class="line x" title="109:213	Relaxation labelling is a generic name for a family of iterative algorithms which perform function optimization, based on local information." ></td>
	<td class="line x" title="110:213	See (Torras." ></td>
	<td class="line x" title="111:213	1989) for a summary." ></td>
	<td class="line x" title="112:213	Its most remarkable feature is that it can deal with any kind of constraints, thus the model can be improved by adding any constraints available and it makes the tagging algorithm independent of the complexity of the model." ></td>
	<td class="line x" title="113:213	The algorithm has been applied to part-of-speech tagging (Padr6, 1996), and to shallow parsing (Voutilainen and Padro." ></td>
	<td class="line x" title="114:213	1997)." ></td>
	<td class="line x" title="115:213	The algorithm is described as follows: Let." ></td>
	<td class="line x" title="116:213	V = {Vl.t'2  v,} be a set of variables (words)." ></td>
	<td class="line x" title="117:213	Let ti = {t\].t~  t~,} be the set of possible labels (POS tags) for variable vi." ></td>
	<td class="line x" title="118:213	Let CS be a set of constraints between the labels of the variables." ></td>
	<td class="line x" title="119:213	Each constraint C E CS states a 'compatibility value' C, for a combination of pairs variable-label." ></td>
	<td class="line x" title="120:213	Any number of variables may be involved in a constraint." ></td>
	<td class="line x" title="121:213	The aim of the algorithm is to find a weighted labelling 5 such that 'global consistency' is maximized." ></td>
	<td class="line x" title="122:213	Maximizing 'global consistency' is defined i is as maximizing for all vi, ~i P} x Sii, where pj the weight for label j in variable vi and Sij the support received by the same combination." ></td>
	<td class="line x" title="123:213	The support for the pair variable-label expresses how compatible that pair is with the labels of neighbouring variables." ></td>
	<td class="line x" title="124:213	according to the constraint set." ></td>
	<td class="line x" title="125:213	It is a vector optimization and doesn't maximize only the sum of the supports of all variables." ></td>
	<td class="line x" title="126:213	It finds a weighted labelling such that any other choice wouldn't increase the support for any variable." ></td>
	<td class="line x" title="127:213	The support is defined as the sum of the influence of every constraint on a label." ></td>
	<td class="line x" title="128:213	c Z Inf(r) r6R,j where: l~ij is the set of constraints on label j for variable i, i.e. the constraints formed by any combination of variable-label pairs that includes the pair (ci." ></td>
	<td class="line x" title="129:213	t i )." ></td>
	<td class="line x" title="130:213	Inf(r) = C, x p~'t,'n) x  x,v~(m) . is the product of the current weights ~ for the labels appearing 5A weighted labelling is a weight assignment for each label of each variable such that the weights for the labels of the same variable add up to one." ></td>
	<td class="line x" title="131:213	Gp~(rn) is the weight assigned to label k for variable r at time m. 241 in the constraint except (vi,t}) (representing how applicable the constraint is in the current context) multiplied by Cr which is the constraint compatibility value (stating how compatible the pair is with the context)." ></td>
	<td class="line x" title="132:213	Briefly, what the algorithm does is: i. Start with a random weight assignment r. 2." ></td>
	<td class="line x" title="133:213	Compute the support value for each label of each variable." ></td>
	<td class="line x" title="134:213	3." ></td>
	<td class="line x" title="135:213	Increase the weights of the labels more compatible with the context (support greater than 0) and decrease those of the less compatible labels (support less than 0) s, using the updating function: i(m + 1) = p~(m)  (1 + s~j) PJ I~, Zp~(m ) x (i + Sit:) k=l where -l<Sij <_+1 4." ></td>
	<td class="line x" title="136:213	If a stopping/convergence criterion 9 is satisfied, stop, otherwise go to step 2." ></td>
	<td class="line x" title="137:213	The cost of the algorithm is proportional to the product of the number of words by the number of constraints." ></td>
	<td class="line x" title="138:213	5 Description of the corpus We used the Wall Street Journal corpus to train and test the system." ></td>
	<td class="line x" title="139:213	We divided it in three parts: 1,100 Kw were used as a training set, 20 Kw as a modeltuning set, and 50 Kw as a test set." ></td>
	<td class="line x" title="140:213	The tag set size is 45 tags." ></td>
	<td class="line x" title="141:213	36.4% of the words in the corpus are ambiguous, and the ambiguity ratio is 2.44 tags/word over the ambiguous words, 1.52 overall." ></td>
	<td class="line x" title="142:213	We used a lexicon derived from training corpora, that contains all possible tags for a word, as well as their lexical probabilities." ></td>
	<td class="line x" title="143:213	For the words in test corpora not appearing in the train set, we stored all possible tags, but no lexical probability (i.e. we assume uniform distribution) l." ></td>
	<td class="line x" title="144:213	The noise in the lexicon was filtered by manually checking the lexicon entries for the most frequent 200 words in the corpus 11 to eliminate the tags due to errors in the training set." ></td>
	<td class="line x" title="145:213	For instance the original ZWe use lexical probabilities as a starting point." ></td>
	<td class="line x" title="146:213	SNegative values for support indicate incompatibility." ></td>
	<td class="line x" title="147:213	9We use the criterion of stopping when there are no more changes, although more sophisticated heuristic procedures are also used to stop relaxation processes (Eklundh and Rosenfeld, 1978; Richards et hhi., 1981)." ></td>
	<td class="line x" title="149:213	1That is, we assumed a morphological analyzer that provides all possible tags for unknown words." ></td>
	<td class="line x" title="150:213	l~The 200 most frequent words in the corpus cover over half of it." ></td>
	<td class="line x" title="151:213	lexicon entry (numbers indicate frequencies in the training corpus) for the very common word the was ~he CD i DT 47715 JJ 7 NN I NNP 6 VBP 1 since it appears in the corpus with the six different tags: CD (cardinal), DT (determiner), JJ (adjective), NN (noun)." ></td>
	<td class="line x" title="152:213	NNP (proper noun) and VBP (verb-personal form)." ></td>
	<td class="line x" title="153:213	It is obvious that the only correct reading for the is determiner." ></td>
	<td class="line x" title="154:213	The training set was used to estimate bi/trigram statistics and to perform the constraint learning." ></td>
	<td class="line x" title="155:213	The model-tuning set was used to tune the algorithm parameterizations, and to write the linguistic part of the model." ></td>
	<td class="line x" title="156:213	The resulting models were tested in the fresh test set." ></td>
	<td class="line x" title="157:213	6 Experiments and results The whole WSJ corpus contains 241 different classes of ambiguity." ></td>
	<td class="line x" title="158:213	The 40 most representative classes t-' were selected for acquiring the corresponding decision trees." ></td>
	<td class="line x" title="159:213	That produced 40 trees totaling up to 2995 leaf nodes, and covering 83.95% of the ambiguous words." ></td>
	<td class="line x" title="160:213	Given that each tree branch produces as many constraints as tags its leaf involves, these trees were translated into 8473 context constraints." ></td>
	<td class="line x" title="161:213	We also extracted the 1404 bigram restrictions and the 17387 trigram restrictions appearing in the training corpus." ></td>
	<td class="line x" title="162:213	Finally, the model-tuning set was tagged using a bigram model." ></td>
	<td class="line x" title="163:213	The most common errors commited by the bigram tagger were selected for manually writing the sample linguistic part of the model, consisting of a set of 20 hand-written constraints." ></td>
	<td class="line x" title="164:213	From now on C will stands for the set of acquired context constraints." ></td>
	<td class="line x" title="165:213	B for the bigram model, T for th.e trigram model, and H for the hand-written constraints." ></td>
	<td class="line x" title="166:213	Any combination of these letters will indicate the joining of the corresponding models (BT, BC, BTC, etc.)." ></td>
	<td class="line x" title="167:213	In addition, ML indicates a baseline model conraining no constraints (this will result in a mostlikely tagger) and HMM stands for a hidden Markov model bigram tagger (Elworthy, 1992)." ></td>
	<td class="line x" title="168:213	We tested the tagger on the 50 Kw test set using all the combinations of the language models." ></td>
	<td class="line x" title="169:213	Results are reported below." ></td>
	<td class="line x" title="170:213	The effect of the acquired rules on the number of errors for some of the most common cases is shown in table 1." ></td>
	<td class="line x" title="171:213	XX/Y'Y stands for an error consisting of a word tagged ~t%_' when it should have been XX." ></td>
	<td class="line x" title="172:213	Table 2 contains the meaning of all the involved tags." ></td>
	<td class="line x" title="173:213	Figures in table 1 show that in all cases the learned constraints led to an improvement." ></td>
	<td class="line x" title="174:213	It is remarkable that when using C alone, the number of errors is lower than with any bigram 12In terms of number of examples." ></td>
	<td class="line x" title="175:213	242 JJ/NN+NN/JJ VBD/VBN+VBN/VBD IN/RB+RB/IN VB/VBP+VBP/VB NN/NNP+NNP/NN NNP/NNPS+NNPS/NNP ''that' 187 Total ML C B 73+137 70+94 73+112 176+190 71+66 88+69 31+132 40+69 66+107 128+147 30+26 49+43 70+11 44+12 72+17 45+14 37+19 45+13 53 66 BC 69+102 63+56 43+17 32+27 45+16 46+15 45 T I TC 57+103 \[ 61+95 56+57 55+57 77+68 47+67 31+32 32+18 69+27 50+18 54+12 51+12 60 I 40 BT\[ BTC 67+101 t 62+93 65+60 59+61 65+98 46-z-83 28+32 ') ' ' '} .8,--3." ></td>
	<td class="line x" title="176:213	71+20 62+t.5 53+14 51+14 57 . 45 1341 it 631 II 821 630 II 7o3!" ></td>
	<td class="line x" title="177:213	603 731 ~s51 i Table 1: Number of some common errors commited by each model NN JJ VBD VBN RB IN VB VBP NNP NNPS Noun \[ I ambiguous Adjective B 91.35% Verb past." ></td>
	<td class="line x" title="178:213	tense T 91.82% 'verb past participle BT 91.92% Adverb Preposition B C 91.96% Verb base form C 92.72% Verb personal form TC 92.82% Proper noun BTC 92.55% Plural proper noun Table 4: Results of our Table 2: Tag meanings of constraint kinds and/or trigram model, that is, the acquired model performs better than the others estimated from the same training corpus." ></td>
	<td class="line x" title="179:213	We also find that the cooperation of a bigram or trigram model with the acquired one, produces even better results." ></td>
	<td class="line x" title="180:213	This is not true in the cooperation of bigrams and trigrams with acquired constraints (BTC), in this case the synergy is not enough to get a better joint result." ></td>
	<td class="line x" title="181:213	This might be due to the fact that the noise in B and T adds up and overwhelms the context constraints." ></td>
	<td class="line x" title="182:213	The results obtained by the baseline taggers can be found in table 3 and the results obtained using all the learned constraints together with the bi/trigram models in table 4." ></td>
	<td class="line x" title="183:213	\] ambiguous I overall ML \[ 85.31%194.66% HMM 91.75% 97.00% Table 3: Results of the baseline taggers On the one hand." ></td>
	<td class="line x" title="184:213	the results in tables 3 and 4 show that our tagger performs slightly worse than a HMM tagger in the same conditions 13, that is, when using only bigram information." ></td>
	<td class="line x" title="185:213	13Hand analysis of the errors commited by the algorithm suggest that the worse results may be due to noise in the training and test corpora, i.e., relaxation algorithm seems to be more noise-sensitive than a Markov model." ></td>
	<td class="line x" title="186:213	Further research is required on this point." ></td>
	<td class="line x" title="187:213	overall 96.86% 97.03% 97.06% 97.08% 97.36% 97.39% 97.29% tagger using every combination On the other hand, those results also show that since our tagger is more flexible than a HMM, it can easily accept more complex information to improve its results up to 97.39% without modifying the algorithm." ></td>
	<td class="line x" title="188:213	I I ambiguus H 86.41% BH 91.88% TH 92.04% BTH 92.32% CH 91.97% BCH 92.76% TCH 92.98% BTCH 92.71% overall 95.06% 97.05% 97.11% 97.21% 97.08% 97.37% 97.45% 97.35% Table .5: Results of our tagger using every combination of constraint kinds and hand written constraints Table 5 shows the results adding the hand written constraints." ></td>
	<td class="line x" title="189:213	The hand written set is very small and only covers a few common error cases." ></td>
	<td class="line x" title="190:213	That produces poor results when using them alone (H)." ></td>
	<td class="line x" title="191:213	but they are good enough to raise the results given by the automatically acquired models up to 97.-15%." ></td>
	<td class="line x" title="192:213	Although the improvement obtained might seem small, it must be taken into .account that we are moving very close to the best achievable result with these techniques." ></td>
	<td class="line x" title="193:213	First, some ambiguities can only be solved with semantic information, such as the Noun-Adjective ambiguity for word principal in the phrase lhe principal office." ></td>
	<td class="line x" title="194:213	It could be an adjective, meaning the 243 main office, or a noun, meaning the school head ofrice, Second, the WSJ corpus contains noise (mistagged words) that affects both the training and the test sets." ></td>
	<td class="line x" title="195:213	The noise in the training set produces noisy -and so less precisemodels." ></td>
	<td class="line x" title="196:213	In the test set, it produces a wrong estimation of accuracy, since correct answers are computed as wrong and vice-versa." ></td>
	<td class="line x" title="197:213	For instance, verb participle forms are sometimes tagged as such (VBIV) and also as adjectives (J J) in other sentences with no structural differences:   failing_VBG ~o_TO voluntarily_KB submit_VB the_DT reques~ed_VBN informa%ion.NN . . ." ></td>
	<td class="line x" title="198:213	  a_DT large_JJ sample_NN of_IN married_JJ women_NNS with_IN at_II~ least_JJS one_CD childgN  Another structure not coherently tagged are noun chains when the nouns are ambiguous and can be  also adjectives:   Mr._NNP Hahn_NNP,_, the_DT 62-year-old_JJ chairman_NN and_CC chief_NN executive_JJ officer_NN of_IN Georgia-Pacific_~NP Corp._NNP    Burger_NgP King_~NP 's_POS chief_JJ ezecutive_NN officer_NN,_, Barry_NNP Gibbons_NNP,_, stars_VBZ inlN ads_NNS saying_VBG    and_CC Barrett_NNP B._NNP Weekes_NNP,_, chairma~t-NN,_, president_NN and_CC chief_JJ ezecutive_JJ officer_NN . _.   the_DT compaay_NN includes_VBZ NeiI_NNP Davenport_NNP,_, 47_CD,_, president_NN and_CC chief_NN ezecu~ive_NN officer_NN ;_: All this makes that the performance cannot reach 100%, and that an accurate analysis of the noise in WS3 corpus should be performed to estimate the actual upper bound that a tagger can achieve on these data." ></td>
	<td class="line x" title="199:213	This issue will be addressed in further work." ></td>
	<td class="line x" title="200:213	7 Conclusions We have presented an automatic constraint learning algorithm based on statistical decision trees." ></td>
	<td class="line x" title="201:213	We have used the acquired constraints in a partof-speech tagger that allows combining any kind of constraints in the language model." ></td>
	<td class="line x" title="202:213	The results obtained show a clear improvement in the performance when the automatically acquired constraints are added to the model." ></td>
	<td class="line x" title="203:213	That indicates that relaxation labelling is a flexible algorithm able to combine properly different information kinds, and that the constraints acquired by the learning algorithm capture relevant context information that was not included in the n-gram models." ></td>
	<td class="line x" title="204:213	It is difficult to compare the results to other works, since the accuracy varies greatly depending on the corpus, the tag set, and the lexicon or morphological analyzer used." ></td>
	<td class="line x" title="205:213	The more similar conditions reported in previous work are those experiments performed on the WSJ corpus: (Brill, 1992) reports 3-4% error rate, and (Daelemans et al. , 1996) report 96.7% accuracy." ></td>
	<td class="line x" title="206:213	We obtained a 97.39% accuracy with trigrams plus automatically acquired constraints, and 97.45% when hand written constraints were added." ></td>
	<td class="line x" title="207:213	8 Further Work Further work is still to be done in the following directions:  Perform a thorough analysis of the noise in the WSJ corpus to determine a realistic upper  bound for the performance that can be expected from a POS tagger." ></td>
	<td class="line x" title="208:213	On the constraint learning algorithm:  Consider more complex context features, such as non-limited distance or barrier rules in the style of (Samuelsson et al. , 1996)." ></td>
	<td class="line x" title="209:213	 Take into account morphological, semantic and other kinds of information." ></td>
	<td class="line x" title="210:213	 Perform a global smoothing to deal with lowfrequency ambiguity classes." ></td>
	<td class="line x" title="211:213	On the tagging algorithms  Study the convergence properties of the algorithm to decide whether the lower results at convergence are produced by the noise in the corpus." ></td>
	<td class="line x" title="212:213	 Use back-off techniques to minimize interferences between statistical and learned constraints." ></td>
	<td class="line x" title="213:213	 Use the algorithm to perform simultaneously POS tagging and word sense disambiguation, to take advantage of cross influences between both kinds of information." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P97-1032
Comparing A Linguistic And A Stochastic Tagger
Samuelsson, Christer;Voutilainen, Atro;"></td>
	<td class="line x" title="1:153	Comparing a Linguistic and a Stochastic Tagger Christer Samuelsson Atro Voutilainen Lucent Technologies Research Unit for Multilingu~l Language Technology Bell Laboratories P.O. Box 4 600 Mountain Ave, Room 2D-339 FIN-00014 University of Helsinki.Murray Hill, NJ 07974, USA Finland christ erresearch, bell-labs, tom Afro." ></td>
	<td class="line x" title="2:153	Vout ilainenHelsinki." ></td>
	<td class="line x" title="3:153	FI Abstract Concerning different approaches to automatic PoS tagging: EngCG-2, a constraintbased morphological tagger, is compared in a double-blind test with a state-of-the-art statistical tagger on a common disambiguation task using a common tag set." ></td>
	<td class="line x" title="4:153	The experiments show that for the same amount of remaining ambiguity, the error rate of the statistical tagger is one order of magnitude greater than that of the rule-based one." ></td>
	<td class="line x" title="5:153	The two related issues of priming effects compromising the results and disagreement between human annotators are also addressed." ></td>
	<td class="line x" title="6:153	1 Introduction There are currently two main methods for automatic part-of-speech tagging." ></td>
	<td class="line x" title="7:153	The prevailing one uses essentially statistical language models automatically derived from usually hand-annotated corpora." ></td>
	<td class="line o" title="8:153	These corpus-based models can be represented e.g. as collocational matrices (Garside et al.(eds)." ></td>
	<td class="line oc" title="10:153	1987: Church 1988), Hidden Markov models (cf.Cutting et al. 1992), local rules (e.g. Hindle 1989) and neural networks (e.g. Schmid 1994)." ></td>
	<td class="line o" title="12:153	Taggers using these statistical language models are generally reported to assign the correct and unique tag to 95-97% of words in running text." ></td>
	<td class="line x" title="13:153	using tag sets ranging from some dozens to about 130 tags." ></td>
	<td class="line x" title="14:153	The less popular approach is based on hand-coded linguistic rules." ></td>
	<td class="line x" title="15:153	Pioneering work was done in the 1960's (e.g. Greene and Rubin 1971)." ></td>
	<td class="line x" title="16:153	Recently, new interest in the linguistic approach has been shown e.g. in the work of (Karlsson 1990: Voutilainen et al. 1992; Oflazer and Kuru6z 1994: Chanod and Tapanainen 1995: Karlsson et al.(eds)." ></td>
	<td class="line x" title="18:153	1995; Voutilainen 1995)." ></td>
	<td class="line x" title="19:153	The first serious linguistic competitor to data-driven statistical taggers is the English Constraint Grammar parser." ></td>
	<td class="line x" title="20:153	EngCG (cf.Voutilainen et al. 1992; Karlsson et al.(eds)." ></td>
	<td class="line x" title="23:153	1995)." ></td>
	<td class="line x" title="24:153	The tagger consists of the following sequentially applied modules: 1." ></td>
	<td class="line x" title="25:153	Tokenisation 2." ></td>
	<td class="line x" title="26:153	Morphological analysis (a) Lexical component (b) Rule-based guesser for unknown words 3." ></td>
	<td class="line x" title="27:153	Resolution of morphological ambiguities The tagger uses a two-level morphological analyser with a large lexicon and a morphological description that introduces about 180 different ambiguity-forming morphological analyses, as a result of which each word gets 1.7-2.2 different analyses on an average." ></td>
	<td class="line x" title="28:153	Morphological analyses are assigned to unknown words with an accurate rulebased 'guesser'." ></td>
	<td class="line x" title="29:153	The morphological disambiguator uses constraint rules that discard illegitimate morphological analyses on the basis of local or global context conditions." ></td>
	<td class="line x" title="30:153	The rules can be grouped as ordered subgrammars: e.g. heuristic subgrammar 2 can be applied for resolving ambiguities left pending by the more 'careful' subgrammar 1." ></td>
	<td class="line x" title="31:153	Older versions of EngCG (using about 1,150 constraints) are reported (~butilainen et al. 1992; Voutilainen and HeikkiUi 1994; Tapanainen and Voutilainen 1994; Voutilainen 1995) to assign a correct analysis to about 99.7% of all words while each word in the output retains 1.04-1.09 alternative analyses on an average, i.e. some of the ambiguities remait~ unresolved." ></td>
	<td class="line x" title="32:153	These results have been seriously questioned." ></td>
	<td class="line x" title="33:153	One doubt concerns the notion 'correct analysis'." ></td>
	<td class="line x" title="34:153	For example Church (1992) argues that linguists who manually perform the tagging task using the doubleblind method disagree about the correct analysis in at least 3% of all words even after they have negotiated about the initial disagreements." ></td>
	<td class="line x" title="35:153	If this were the case, reporting accuracies above this 97% 'upper bound' would make no sense." ></td>
	<td class="line x" title="36:153	However, Voutilainen and J~rvinen (1995) empirically show that an interjudge agreement virtually of 1()0% is possible, at least with the EngCG tag set if not with the original Brown Corpus tag set." ></td>
	<td class="line x" title="37:153	This consistent applicability of the EngCG tag set is explained by characterising it as grammatically rather than semantically motivated." ></td>
	<td class="line x" title="38:153	246 Another main reservation about the EngCG figures is the suspicion that, perhaps partly due to the somewhat underspecific nature of the EngCG tag set, it must be so easy to disambiguate that also a statistical tagger using the EngCG tags would reach at least as good results." ></td>
	<td class="line x" title="39:153	This argument will be examined in this paper." ></td>
	<td class="line x" title="40:153	It will be empirically shown (i) that the EngCG tag set is about as difficult for a probabilistic tagger as more generally used tag sets and (ii) that the EngCG disambiguator has a clearly smaller error rate than the probabilistic tagger when a similar (small) amount of ambiguity is permitted in the output." ></td>
	<td class="line x" title="41:153	A state-of-the-art statistical tagger is trained on a corpus of over 350,000 words hand-annotated with EngCG tags." ></td>
	<td class="line x" title="42:153	then both taggers (a new version known as En~CG-21 with 3,600 constraints as five subgrammars-, and a statistical tagger) are applied to the same held-out benchmark corpus of 55,000 words, and their performances are compared." ></td>
	<td class="line x" title="43:153	The results disconfirm the suspected 'easiness' of the EngCG tag set: the statistical tagger's performance figures are no better than is the case with better known tag sets." ></td>
	<td class="line x" title="44:153	Two caveats are in order." ></td>
	<td class="line x" title="45:153	What we are not addressing in this paper is the work load required for making a rule-based or a data-driven tagger." ></td>
	<td class="line x" title="46:153	The rules in EngCG certainly took a considerable effort to write, and though at the present state of knowledge rules could be written and tested with less effort, it may well be the case that a tagger with an accuracy of 95-97% can be produced with less effort by using data-driven techniques." ></td>
	<td class="line x" title="47:153	3 Another caveat is that EngCG alone does not resolve all ambiguities, so it cannot be compared to a typical statistical tagger if full disambiguation is required." ></td>
	<td class="line x" title="48:153	However, '~butilainen (1995) has shown that EngCG combined with a syntactic parser produces morphologically unambiguous output with an accuracy of 99.3%, a figure clearly better than that of the statistical tagger in the experiments below (however." ></td>
	<td class="line x" title="49:153	the test data was not the same)." ></td>
	<td class="line x" title="50:153	Before examining the statistical tagger, two practical points are addressed: the annotation of tile corpora used." ></td>
	<td class="line x" title="51:153	and the modification of the EngCG tag set for use in a statistical tagger." ></td>
	<td class="line x" title="52:153	1An online version of EngCG-2 can be found at, ht tp://www.ling.helsinki.fi/'avoutila/engcg-2.ht ml." ></td>
	<td class="line x" title="53:153	:The first three subgrammars are generally highly reliable and almost all of the total grammar development time was spent on them: the last two contain rather rough heuristic constraints." ></td>
	<td class="line x" title="54:153	3However, for an interesting experiment suggesting otherwise, see (Chanod and Tapanainen 1995)." ></td>
	<td class="line x" title="55:153	2 Preparation of Corpus Resources 2.1 Annotation of training corpus The stochastic tagger was trained on a sample of 357,000 words from the Brown University Corpus of Present-Day English (Francis and Ku6era 1982) that was annotated using the EngCG tags." ></td>
	<td class="line x" title="56:153	The corpus was first analysed with the EngCG lexical analyser, and then it was fully disambiguated and, when necessary, corrected by a human expert." ></td>
	<td class="line x" title="57:153	This annotation took place a few years ago." ></td>
	<td class="line x" title="58:153	Since then, it has been used in the development of new EngCG constraints (the present version, EngCG-2, contains about 3,600 constraints): new constraints were applied to the training corpus, and whenever a reading marked as correct was discarded, either the analysis in the corpus, or the constraint itself, was corrected." ></td>
	<td class="line x" title="59:153	In this way, the tagging quality of the corpus was continuously improved." ></td>
	<td class="line x" title="60:153	2.2 Annotation of benchmark corpus Our comparisons use a held-out benchmark corpus of about 55,000 words of journalistic, scientific and manual texts, i.e., no,training effects are expected for either system." ></td>
	<td class="line x" title="61:153	The benchmark corpus was annotated by first applying the preprocessor and morphological aaalyser, but not the morphological disambiguator, to the text." ></td>
	<td class="line x" title="62:153	This morphologically ambiguous text was then independently and fully disambiguated by two experts whose task was also to detect any errors potentially produced by the previously applied components." ></td>
	<td class="line x" title="63:153	They worked independently, consulting written documentation of the tag set when necessary." ></td>
	<td class="line x" title="64:153	Then these manually disambiguated versions were automatically compared with each other." ></td>
	<td class="line x" title="65:153	At this stage, about 99.3% of all analyses were identical." ></td>
	<td class="line x" title="66:153	When the differences were collectiyely examined, virtually all were agreed to be due to clerical mistakes." ></td>
	<td class="line x" title="67:153	Only in the analysis of 21 words, different (meaning-level) interpretations persisted, and even here both judges agreed the ambiguity to be genuine." ></td>
	<td class="line x" title="68:153	One of these two corpus versions was modified to represent the consensus, and this 'consensus corpus' was used as a benchmark in the evaluations." ></td>
	<td class="line x" title="69:153	As explained in Voutilainen and J/irvinen (1995)." ></td>
	<td class="line x" title="70:153	this high agreement rate is due to two main factors." ></td>
	<td class="line x" title="71:153	Firstly, distinctions based on some kind of vague semantics are avoided, which is not always case with better known tag sets." ></td>
	<td class="line x" title="72:153	Secondly." ></td>
	<td class="line x" title="73:153	the adopted analysis of most of the constructions where humans tend to be uncertain is documented as a collection of tag application principles in the form of a grammarinn's manual (for further details, cf.Voutilainen and J/irvinen 1995)." ></td>
	<td class="line x" title="75:153	Tile corpus-annotation procedure allows us t.o perform a text-book statistical hypothesis test." ></td>
	<td class="line x" title="76:153	Let tile null hypothesis be that any two human evaluators will necessarily disagree in at least 3% of 247 the cases." ></td>
	<td class="line x" title="77:153	Under this assumption, the probability of an observed disagreement of less than 2.88% is less than 5%." ></td>
	<td class="line x" title="78:153	This can be seen as follows: For the relative frequency of disagreement, fn, we have t-.--that f. is approximately --, N(p, ~/~), where p is the actual disagreement probability and n is the number of trials, i.e., the corpus size." ></td>
	<td class="line x" title="79:153	This means fn-P v/ff that P(( ~ < z) ~ ~(x) where b is the standard normal distribution function." ></td>
	<td class="line x" title="80:153	This in turn means that P ( f, < p + z P~ p-----~) ),~ ~ ( z ) Here n is 55,000 and ~(-1.645) = 0.05." ></td>
	<td class="line x" title="81:153	Under the null hypothesis, p is at least 3% and thus: . /O.O3.0.97 P(f. < o.o31.64%/-g,o-g6 ) = P(A <__ 0.0288) < 0.05 We can thus discard the null hypothesis at significance level 5% if the observed disagreement is less than 2.88%." ></td>
	<td class="line x" title="82:153	It was in fact 0.7% before error cor.21) rection, and virtually zero (~ after negotiation." ></td>
	<td class="line x" title="83:153	This means that we can actually discard the hypotheses that the human evaluators in average disagree in at least 0.8% of the cases before error correction, and in at least 0.1% of the cases after negotiations, at significance level 5%." ></td>
	<td class="line x" title="84:153	2.3 Tag set conversion The EugCG morphological analyser's output formally differs from most tagged corpora; consider the following 5-ways ambiguous analysis of ''walk': walk walk <SV> <SVO> V SUBJUNCTIVE VFIN walk <SV> <SVO> V IMP VFIN walk <SV> <SVG> V INF walk <SV> <SVO> V PRES -SG3 VFIN walk N NOM SG Statistical taggers usually employ single tags to indicate analyses (e.g. ''NN' for ''N NOM SG')." ></td>
	<td class="line x" title="85:153	Therefore a simple conversion program was made for producing the following kind of output, where each reading is represented as a single tag: walk V-SUBJUNCTIVE V-IMP V-INF V-PRES-BASE N-NOM-SG The conversion program reduces the multipart EngCG tags into a set of 80 word tags and 17 punctuation tags (see Appendix) that retain the central linguistic characteristics of the original EngCG tag set." ></td>
	<td class="line x" title="86:153	A reduced version of the benchmark corpus was prepared with this conversion program for the statistical tagger's use." ></td>
	<td class="line x" title="87:153	Also EngCG's output was converted into this format to enable direct comparison with the statistical tagger." ></td>
	<td class="line x" title="88:153	8 The Statistical Tagger The statistical tagger used in the experiments is a classical trigram-based HMM decoder of the kind described in e.g.(Church 1988), (DeRose 1988) and numerous other articles." ></td>
	<td class="line x" title="90:153	Following conventional notation, e.g.(Rabiner 1989, pp." ></td>
	<td class="line x" title="92:153	272-274) and (Krenn and Samuelsson 1996, pp." ></td>
	<td class="line x" title="93:153	42-46), the tagger recursively calculates the ~, 3, 7 and 6 variables for each word string position t = 1  T and each possible state 4 si : i = 1,,n: a,(i) = P(W<,;S, = si) .'3,(i) = P(W>, IS, = s~) 7ti) --&(i) = Here W W5t W>t Sst P(W; & = si) P(&=siIW) = P(W) ~,(i)." ></td>
	<td class="line x" title="94:153	3,(i) r6 y~o~,(i)." ></td>
	<td class="line x" title="95:153	3,(i) i=l max P(S<t-l, S= = si; W<,) S<,_t = l/V1 = wlq,, ~VT = Wkr -~'VI = wk~, . . ., Wt = wk, 'l~Vt+l = wk,+ t,  ., I'VT = Wkr -= S1 = si~  St = si, where St = si is the event of the tth word being emitted from state si and Wt = wk, is the event of the tth word being the particular word w~, that was actually observed in the word string." ></td>
	<td class="line x" title="97:153	Note that for t = 1  T-1 ; i,jl  n at+~(j) 3,(0 = ~ 3,+1(j) 'Pij .aj~,+~ j=l where pij = P(St+I = sj I St = si) are the transition probabilities, encoding the tag N-gram probabilities, and ajk = = P(Wt=wkIS,=sj) = P(Wt=w~l,\'t=zj) 4The N-I th-order HMM corresponding to an N-gram tagger is encoded as a first-order HMM, where each state corresponds to a sequence of,V-I tags, i.e., for a trigram tagger, each state corresponds to a tag pair." ></td>
	<td class="line x" title="98:153	248 are the lexical probabilities." ></td>
	<td class="line x" title="99:153	Here X, is the random variable of assigning a tag to the tth word and xj is the last tag of the tag sequence encoded as state sj." ></td>
	<td class="line x" title="100:153	Note that si # sj need not imply zi # zj." ></td>
	<td class="line x" title="101:153	More precisely, the tagger employs the converse lexical probabilities P(Xt = zj I Wt = w,) ajk a~ k = P(X, = zj) P(W, = wk) This results in slight variants a', fl', 7' and 6' of the original quantities: ~,(i) 6,(i) ' = = I-\[ P(Wu = o4(i ) 6;(i) .=1 ~,(i) r H P(W~ =w~=) /3;(i) u=t+l and thus Vi, t 7~(i) = a;(i) ./3;(i) = ka;(i) ./3;(i1 i=1 ~,(i) .~,(i) and Vt ~e,(i) ./3t(i) i=1 = 7t(0 argmax6;(i) = argmax6t(i) l<i<n l<i<n The rationale behind this is to facilitate estimating the model parameters from sparse data." ></td>
	<td class="line x" title="102:153	In more detail, it is easy to estimate P(tag I word) for a previously unseen word by backing off to statistics derived from words that end with the same sequence of letters (or based on other surface cues), whereas directly estimating P(word I tag) is more difficult." ></td>
	<td class="line x" title="103:153	This is particularly useful for languages with a rich inflectional and derivational morphology, but also for English: for example, the suffix '-tion' is a strong indicator that the word in question is a noun; the suffix '-able' that it is an adjective." ></td>
	<td class="line x" title="104:153	More technically, the lexicon is organised as a reverse-suffix tree, and smoothing the probability estimates is accomplished by blending the distribution at the current node of the tree with that of higherlevel nodes, corresponding to (shorter) suffixes of the current word (suffix)." ></td>
	<td class="line x" title="105:153	The scheme also incorporates probability distributions for the set of capitalized words, the set of all-caps words and the set of infrequent words, all of which are used to improve the estimates for unknown words." ></td>
	<td class="line x" title="106:153	Employing a small amount of back-off smoothing also for the known words is useful to reduce lexical tag omissions." ></td>
	<td class="line x" title="107:153	Empirically, looking two branching points up the tree for known words, and all the way up to the root for unknown words, proved optimal." ></td>
	<td class="line x" title="108:153	The method for blending the distributions applies equally well to smoothing the transition probabilities pij, i.e., the tag N-gram probabilities, and both the scheme and its application to these two tasks are described in detail in (Samuelsson 1996), where it was also shown to compare favourably to (deleted) interpolation, see (Jelinek and Mercer 1980), even when the back-off weights of the latter were optimal." ></td>
	<td class="line x" title="109:153	The 6 variables enable finding the most probable state sequence under the HMM, from which the most likely assignment of tags to words can be directly established." ></td>
	<td class="line x" title="110:153	This is the normal modus operandi of an HMM decoder." ></td>
	<td class="line x" title="111:153	Using the 7 variables, we can calculate the probability of being in state si at string position t, and thus having emitted wk, from this state, conditional on the entire word string." ></td>
	<td class="line x" title="112:153	By summing over all states that would assign the same tag to this word, the individual probability of each tag being assigned to any particular input word, conditional on the entire word string, can be calculated: P(X, = zilW) = = Z P(S,=sj t W) = E 7,(J) 8j:rj=r i $j:rj =~'= This allows retaining multiple tags for each word by simply discarding only low-probability tags; those whose probabilities are below some threshold value." ></td>
	<td class="line x" title="113:153	Of course, the most probable tag is never discarded, even if its probability happens to be less than the threshold value." ></td>
	<td class="line x" title="114:153	By varying the threshold, we can perform a recall-precision, or error-rate-ambiguity, tradeoff." ></td>
	<td class="line x" title="115:153	A similar strategy is adopted in (de Marcken 1990)." ></td>
	<td class="line x" title="116:153	4 Experiments The statistical tagger was trained on 357,000 words from the Brown corpus (Francis and Ku~era 1982), reannotated using the EngCG annotation scheme (see above)." ></td>
	<td class="line x" title="117:153	In a first set of experiments, a 35,000 word subset of this corpus was set aside and used to evaluate the tagger's performance when trained on successively larger portions of the remaining 322,000 words." ></td>
	<td class="line x" title="118:153	The learning curve, showing the error rate alter full disambiguation as a function of the amount of training data used, see Figure 1, has levelled off at 322,000 words, indicating that little is to be gained from further training." ></td>
	<td class="line x" title="119:153	We also note that the absolute value of the error rate is 3.51% -a typical state-of-the-art figure." ></td>
	<td class="line x" title="120:153	Here, previously unseen words contribute 1.08% to the total error rate, while the contribution from lexical tag omissions is 0.08% 95% confidence intervals for the error rates would range from + 0.30% for 30,000 words to + 0.20~c at 322.000 words." ></td>
	<td class="line x" title="121:153	The tagger was then trained on the entire set of 357,000 words and confronted with the separate 55,000-word benchmark corpus, and run both in full 249 8 v 6 .~ 5 ~ 4 ~ 3 o 2 1 0 Learning curve, I I I I I I 0 50 I00 150 200 250 300 Training set (kWords) Figure 1: Learning curve for the statistical tagger on the Brown corpus." ></td>
	<td class="line x" title="122:153	Ambiguity (Tags/word) 1.000 1.012 1.025 1.026 1.035 1.038 1.048 1.051 1.059 1.065 1.070 1.078 1.093 Error rate (%) Statistical Tagger EngCG (~) (7) 4.72 4.68 4.20 3.75 (3.72) (3.48) 3.40 (3.20) 3.14 (2.99) 2.87 (2.80) 2.69 2.55 0.43 0.29 0.15 0.12 0.10 Table h Error-rate-ambiguity tradeoff for both taggets on the benchmark corpus." ></td>
	<td class="line x" title="123:153	Parenthesized numbers are interpolated." ></td>
	<td class="line x" title="124:153	and partial disambiguation mode." ></td>
	<td class="line x" title="125:153	Table 1 shows the error rate as a function of remaining ambiguity (tags/word) both for the statistical tagger, and for the EngCG-2 tagger." ></td>
	<td class="line x" title="126:153	The error rate for full disanabiguation using the 6 variables is 4.72% and using the 7 variables is 4.68%, both -4-0.18% with confidence degree 95%." ></td>
	<td class="line x" title="127:153	Note that the optimal tag sequence obtained using the 7 variables need not equal the optimal tag sequence obtained using the 6 variables." ></td>
	<td class="line x" title="128:153	In fact, the former sequence may be assigned zero probability by the HMM, namely if one of its state transitions has zero probability." ></td>
	<td class="line x" title="129:153	Previously unseen words account for 2.01%, and lexical tag omissions for 0.15% of the total error rate." ></td>
	<td class="line x" title="130:153	These two error sources are together exactly 1.00% higher on the benchmark corpus than on the Brown corpus, and account for almost the entire difference in error rate." ></td>
	<td class="line x" title="131:153	They stem from using less complete lexical information sources, and are most likely the effect of a larger vocabulary overlap between the test and training portions of the Brown corpus than between the Brown and benchmark corpora." ></td>
	<td class="line x" title="132:153	The ratio between the error rates of the two taggets with the same amount of remaining ambiguity ranges from 8.6 at 1.026 tags/word to 28,0 at 1.070 tags/word." ></td>
	<td class="line x" title="133:153	The error rate of the statistical tagger can be further decreased, at the price of increased remaining ambiguity, see Figure 2." ></td>
	<td class="line x" title="134:153	In the limit of retaining all possible tags, the residual error rate is entirely due to lexical tag omissions, i.e., it is 0.15%, with in average 14.24 tags per word." ></td>
	<td class="line x" title="135:153	The reason that this figure is so high is that the unknown words, which comprise 10% of the corpus, are assigned all possible tags as they are backed off all the way to the root of the reverse-suffix tree." ></td>
	<td class="line x" title="136:153	5 v 4 3 2 O 0 Error-rate-ambiguity trade-off i ! i l i l i I I I I i I r2 4 6 8 i0 12 14 Remaining ambiguity (Tags/Word) Figure 2: Error-rate-ambiguity tradeoff for the statistical tagger on the benchmark corpus." ></td>
	<td class="line x" title="137:153	5 Discussion Recently voiced scepticisms concerning the superior EngCG tagging results boil down to the following:  The reported results are due to the simplicity of the tag set employed by the EngCG system." ></td>
	<td class="line x" title="138:153	 The reported results are an effect of trading high ambiguity resolution for lower error rate." ></td>
	<td class="line x" title="139:153	 The results are an effect of so-called priming of the huraan annotators when preparing the test corpora, compromising the integrity of the experimental evaluations." ></td>
	<td class="line x" title="140:153	In the current article, these points of criticism were investigated." ></td>
	<td class="line x" title="141:153	A state-of-the-art statistical tagger, capable of performing error-rate-ambiguity tradeoff, was trained on a 357,000-word portion of the Brown corpus reannotated with the EngCG tag set, and both taggers were evaluated using a separate 55,000-word benchmark corpus new to both 250 systems." ></td>
	<td class="line x" title="142:153	This benchmark corpus was independently disambiguated by two linguists, without access to the results of the automatic taggers." ></td>
	<td class="line x" title="143:153	The initial differences between the linguists' outputs (0.7% of all words) were jointly examined by the linguists; practically all of them turned out to be clerical errors (rather than the product of genuine difference of opinion)." ></td>
	<td class="line x" title="144:153	In the experiments, the performance of the EngCG-2 tagger was radically better than that of the statistical tagger: at ambiguity levels common to both systems, the error rate of the statistical tagger was 8.6 to 28 times higher than that of EngCG2." ></td>
	<td class="line x" title="145:153	We conclude that neither the tag set used by EngCG-2, nor the error-rate-ambiguity tradeoff, nor any priming effects can possibly explain the observed difference in performance." ></td>
	<td class="line x" title="146:153	Instead we must conclude that the lexical and contextual information sources at the disposal of the EngCG system are superior." ></td>
	<td class="line x" title="147:153	Investigating this empirically by granting the statistical tagger access to the same information sources as those available in the Constraint Grammar framework constitutes future work." ></td>
	<td class="line x" title="148:153	Acknowledgements Though Voutilainen is the main author of the EngCG-2 tagger, the development of the system has benefited from several other contributions too." ></td>
	<td class="line x" title="149:153	Fred Karlsson proposed the Constraint Grammar framework in the late 1980s." ></td>
	<td class="line x" title="150:153	Juha Heikkil and Timo J~irvinen contributed with their work on English morphology and lexicon." ></td>
	<td class="line x" title="151:153	Kimmo Koskenniemi wrote the software for morphological analysis." ></td>
	<td class="line x" title="152:153	Pasi Tapanainen has written various implementations of the CG parser, including the recent CG-2 parser (Tapanainen 1996)." ></td>
	<td class="line x" title="153:153	The quality of the investigation and presentation was boosted by a number of suggestions to improvements and (often sceptical) comments from numerous ACL reviewers and UPenn associates, in particular from Mark Liberman." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W97-0307
Tagging Grammatical Functions
Brants, Thorsten;Skut, Wojciech;Krenn, Brigitte;"></td>
	<td class="line x" title="1:273	Tagging Grammatical Functions Thorsten Brants, Wojciech Skut, Brigitte Krenn Universit~t des Saarlandes Computational Linguistics D-66041 Saarbr/icken, Germany {brant s, skut,krenn}@coli.uni-sb, de Abstract This paper addresses issues in automated treebank construction." ></td>
	<td class="line x" title="2:273	We show how standard part-of-speech tagging techniques extend to the more general problem of structural annotation, especially for determining grammatical functions and syntactic categories." ></td>
	<td class="line x" title="3:273	Annotation is viewed as an interactive process where manual and automatic processing alternate." ></td>
	<td class="line x" title="4:273	Efficiency and accuracy results are presented." ></td>
	<td class="line x" title="5:273	We also discuss further automation steps." ></td>
	<td class="line x" title="6:273	1 Introduction The aim of the work reported here is to construct a corpus of German annotated with syntactic structures (treebank)." ></td>
	<td class="line x" title="7:273	The required size of the treebank and granularity of encoded information make it necessary." ></td>
	<td class="line x" title="8:273	to ensure high annotation efficiency and accuracy." ></td>
	<td class="line x" title="9:273	Annotation automation has thus become one of the central issues of the project." ></td>
	<td class="line x" title="10:273	In this section, we discuss the relation between automatic and manual annotation." ></td>
	<td class="line x" title="11:273	Section 2 focuses on the annotation format employed in our treebank." ></td>
	<td class="line x" title="12:273	The annotation software is presented in section 3." ></td>
	<td class="line x" title="13:273	Sections 4 and 5 deal with automatic assignment of grammatical functions and phrasal categories." ></td>
	<td class="line x" title="14:273	Experiments on automating the annotation are presented in section 6." ></td>
	<td class="line x" title="15:273	1.1 Automatic vs. Manual Annotation A problem for corpus annotation is the trade-off between efficiency, accuracy and coverage." ></td>
	<td class="line x" title="16:273	Although accuracy increases significantly as annotators gain expertise, incorrect hand-parses still occur." ></td>
	<td class="line x" title="17:273	Their frequency depends on the granularity of the encoded information." ></td>
	<td class="line x" title="18:273	Due to this residual error rate, automatic annotation of frequently occurring phenomena is likely to yield better results than even well-trained human annotators." ></td>
	<td class="line x" title="19:273	For infrequently occurring constructions, however, manual annotation is more reliable, as is manual annotation of phenomena involving non-syntactic information (e.g. , resolution of attachment ambiguities based on world knowledge)." ></td>
	<td class="line x" title="20:273	As a consequence, efficiency and reliability of annotation can be significantly increased by combining automatic annotation with human processing skills and supervision, especially if this combination is implemented as an interactive process." ></td>
	<td class="line x" title="21:273	2 Annotation Scheme Existing treebanks of English ((Marcus et al. , 1994), (Sampson, 1995), (Black et al. , 1996)) contain conventional phrase-structure trees augmented with annotations for discontinuous constituents." ></td>
	<td class="line x" title="22:273	As this encoding strategy is not well-suited to a free word order language like German, we have focussed on a less surface-oriented level of description, most closely related to the LFG f-structure, and representations used in dependency grammar." ></td>
	<td class="line x" title="23:273	To avoid confusion with theory-specific constructs, we use the generic term argument structure to refer to our annotation format." ></td>
	<td class="line x" title="24:273	The main advantages of the model are: it is relatively theory-independent and closely related to semantics." ></td>
	<td class="line x" title="25:273	For more details on the linguistic specifications of the annotation scheme see (Skut et al. , 1997)." ></td>
	<td class="line x" title="26:273	A similar approach has been also successfully applied in the TSNLP database, cf.(Lehmann et al. , 1996)." ></td>
	<td class="line x" title="28:273	In contrast to conventional phrase-structure grammars, argument structure annotations are not influenced by word order." ></td>
	<td class="line x" title="29:273	Local and non-local dependencies are represented in the same way, the latter indicated by crossing branches in the hierarchical structure, as shown in figure 1 where in the VP the terminals of the direct object OA (den Traum yon der kleinen Gastst~tte) are not adjacent to the head HD aufgegeben 1." ></td>
	<td class="line x" title="30:273	For a related handling 1 See appendix A for a description of tags used throng64 Den ART The Traum NN dream + von APPR of der kleinen Gastst'atte hat er noch ART ADJA NN VAFIN PPER ADV the small inn has he yet 'He has not yet given up the dream of a small inn'." ></td>
	<td class="line x" title="31:273	nicht PTKNEG not aufgegeben VVPP given up Figure 1: Example sentence of non-projective phenomena see (Tapanainen and J/irvinen, 1997)." ></td>
	<td class="line x" title="32:273	Such a representation permits clear separation of word order (in the surface string) and syntactic dependencies (in the hierarchical structure)." ></td>
	<td class="line x" title="33:273	Thus we avoid explicit explanatory statements about the complex interrelation between word order and syntactic structure in free word order languages." ></td>
	<td class="line x" title="34:273	Such statements are generally theory-specific and therefore are not appropriate for a descriptive approach to annotation." ></td>
	<td class="line x" title="35:273	The relation between syntactic dependencies and surface order can nontheless be inferred from the data." ></td>
	<td class="line x" title="36:273	This provides a promising way of handling free word order phenomena." ></td>
	<td class="line x" title="37:273	2." ></td>
	<td class="line x" title="38:273	3 Annotation Tool Since syntactic annotation of corpora is timeconsuming, a partially automated annotation tool has been developed in order to increase efficiency." ></td>
	<td class="line x" title="39:273	3.1 The User Interface For optimal human-machine interaction, the tool supports immediate graphical representation of the structure being annotated." ></td>
	<td class="line x" title="40:273	Since keyboard input is most efficient for assigning categories to words and phrases, cf.(Lehmann et al. , 1996; Marcus et al. , 1994), and structural manipulations are executed most efficiently using the mouse, both an elaborate keyboard and optical interface is provided." ></td>
	<td class="line x" title="42:273	As suggested by Robert MacIntyre 3, it is hout this paper." ></td>
	<td class="line x" title="43:273	2'Free' word order is a function of several interacting parameters such as category, case and topic-focus articulation." ></td>
	<td class="line x" title="44:273	Varying the order of words in a sentence yields a continuum of grammaticality judgments rather than a simple right-wrong distinction." ></td>
	<td class="line x" title="45:273	3personal communication, Oct. 1996 most efficient to use one hand for structural commands with the mouse and the other hand for short keyboard input." ></td>
	<td class="line x" title="46:273	By additionally offering online menus for commands and labels, the tool suits beginners as well as experienced users." ></td>
	<td class="line x" title="47:273	Commands such as 'group words', 'group phrases', 'ungroup', 'change labels', 're-attach nodes', 'generate postscript output', etc. are available." ></td>
	<td class="line x" title="48:273	The three tagsets (word, phrase, and edge labels) used by the annotation tool are variable." ></td>
	<td class="line x" title="49:273	They are stored together with the corpus, which allows easy modification and exchange of tagsets." ></td>
	<td class="line x" title="50:273	In addition, appropriateness checks are performed automatically." ></td>
	<td class="line x" title="51:273	Comments can be added to structures." ></td>
	<td class="line x" title="52:273	Figure 2 shows a screen dump of the graphical interface." ></td>
	<td class="line x" title="53:273	3.2 Automating Annotation Existing treebank annotation tools are characterised by a high degree of automation." ></td>
	<td class="line x" title="54:273	The task of the annotator is to correct the output of a parser, i.e., to eliminate wrong readings, complete partial parses, and adjust partially incorrect ones." ></td>
	<td class="line x" title="55:273	Since broad-coverage parsers for German, especially robust parsers that assign predicate-argument structure and allow crossing branches, are not available, or require an annotated traing corpus (cf.(Collins, 1996), (Eisner, 1996))." ></td>
	<td class="line x" title="57:273	As a consequence, we have adopted a bootstrapping approach, and gradually increased the degree of automation using already annotated sentences as training material for a stochastic processing module." ></td>
	<td class="line x" title="58:273	This aspect of the work has led to a new model of human supervision." ></td>
	<td class="line x" title="59:273	Here automatic annotation and human supervision are combined interactively whereby annotators are asked to confirm the local 65 G_enm'al: Corpus: IRefCorpus Tes~,ople I J~\] Editor: IThorsten J~\] I-~-Ii _,,oa, li E. ,t i i O~,o." ></td>
	<td class="line x" title="60:273	i -Sentence: No.: 4 / 1269 Comment: I Odgln: refcorp.tt Last edited: Thorsten, 28/05/97, 14:08:48 Es o spleit I ebe~ keine 3 Roll% PPER WFIN ADV PlAT NN I<U 511 KOUS ART NN gef'~llg 9 iS~o ADJD VAFIN -Move: I.~r~, II _'' I ~_o'o:', I -,o II +,o I D ~,,,e, I -' II +,oo I Mat~eo:o i r--_Dependeney: / -s~''n: I _Command: I | i ~-'~' I I IB _ our o,o 'u'' ;i' mu, I i T ag: Node no.: I J Zag: I IB I-''' II '-'~ I1-~-I I Switchin~ to sentence no, 4." ></td>
	<td class="line x" title="61:273	Done." ></td>
	<td class="line x" title="62:273	Figure 2: Screen dump of the annotation tool predictions of the parser." ></td>
	<td class="line x" title="63:273	The size of such 'supervision increments' varies from local trees of depth one to larger chunks, depending on the amount of training data available." ></td>
	<td class="line x" title="64:273	We distinguish six degrees of automation: 0) Completely manual annotation." ></td>
	<td class="line x" title="65:273	1) The user determines phrase boundaries and syntactic categories (S, NP, VP, )." ></td>
	<td class="line x" title="66:273	The program automatically assigns grammatical functions." ></td>
	<td class="line x" title="67:273	The annotator can alter the assigned tags (cf.figure 3)." ></td>
	<td class="line x" title="69:273	2) The user only determines the components of a new phrase (local tree of depth 1), while both category and function labels are assigned automatically." ></td>
	<td class="line x" title="70:273	Again, the annotator has the option of altering the assigned tags (cf.figure 4)." ></td>
	<td class="line x" title="72:273	3) The user selects a substring and a category, whereas the entire structure covering the substring is determined automatically (cf.figure 5)." ></td>
	<td class="line x" title="74:273	4) The program performs simple bracketing, i.e., finds 'kernel phrases' without the user having to explicitly mark phrase boundaries." ></td>
	<td class="line x" title="75:273	The task can be performed by a chunk parser that is equipped with an appropriate finite state grammar (Abney, 1996)." ></td>
	<td class="line x" title="76:273	5) The program suggests partiM or complete parses." ></td>
	<td class="line x" title="77:273	A set of 500 manually annotated training sentences (step 0) was sufficient for a statistical tagger to reliably assign grammatical functions, provided the user determines the elements of a phrase and its category (step 1)." ></td>
	<td class="line x" title="78:273	Approximately 700 additional sentences have been annotated this way." ></td>
	<td class="line x" title="79:273	Annotation efficiency increased by 25 %, namely from an average annotation time of 4 minutes to 3 minutes per sentence (300 to 400 words per hour)." ></td>
	<td class="line x" title="80:273	The 1,200 sentences were used to train the tagger for automation step 2." ></td>
	<td class="line x" title="81:273	Together with improvements in the user interface, this increased the efficiency by another 33%, from approximately 3 to 2 minutes (600 words per hour)." ></td>
	<td class="line x" title="82:273	The fastest annotators cover up to 66 das 1993 startende Bonusprogramm for Vielflieger ART CARD ADJA NN APPR NN 'the bonus program for .h'equent fliers starting in 1993' Figure 3: Example for automation level 1: the user has marked das, the AP, Bonusprogramm, and the PP as a constituent of category NP, and the tool's task is to determine the new edge labels (marked with question marks), which are, from left to right, NK, NK, NK, MNR." ></td>
	<td class="line x" title="83:273	das 1993 startende Bonusprogramm ffir Vielflieger ART CARD ADJA NN APPR NN 'the bonus program for frequent fliers starting in 1993' Figure 4: Example for automation level 2: the user has marked das, the AP, Bonusprogramm and the PP as a constituent, and the tool's task is to determine the new node and edge labels (marked with question marks)." ></td>
	<td class="line x" title="84:273	1000 words per hour." ></td>
	<td class="line x" title="85:273	At present, the treebank comprises 3000 sentences, each annotated independently by two annotators." ></td>
	<td class="line x" title="86:273	1,200 of the sentences are compared with the corresponding second annotation and are cleaned, 1,800 are currently cleaned." ></td>
	<td class="line x" title="87:273	In the following sections, the automation steps 1 and 2 are presented in detail." ></td>
	<td class="line oc" title="88:273	4 Tagging Grammatical Functions 4.1 The Tagger In contrast to a standard part-of-speech tagger which estimates lexical and contextual probabilities of tags from sequences of word-tag pairs in a corpus, (e.g.(Cutting et al. , 1992; Feldweg, 1995)), the tagger for grammatical functions works with lexical and contextual probability measures Pq()." ></td>
	<td class="line x" title="90:273	depending on the category of the mother node (Q)." ></td>
	<td class="line x" title="91:273	Each phrasal category (S, VP, NP, PP etc)." ></td>
	<td class="line x" title="92:273	is represented by a different Markov model." ></td>
	<td class="line x" title="93:273	The categories of the dau++++ ++ das 1993 startende Bonusprograrnm for Vielflieger ART CARD ADJA NN APPR NN 'the bonus program for frequent fliers starting in 1993' Figure 5: Example for automation level 3: the user has marked the words as a constituent, and the tool's task is to determine simple sub-phrases (the AP and PP) as well as the new node and edge labels (cf.previous figures ~br the resulting structure)." ></td>
	<td class="line x" title="95:273	Selbst ADV himself l'l besucht hat Peter VVPP VAFIN NE visited has Peter +l Sabine nie NE ADV Sabine never 'Peter never visited Sabine himself' Figure 6: Example sentence ghter nodes correspond to the outputs of the Markov model, while grammatical functions correspond to states." ></td>
	<td class="line x" title="96:273	The structure of a sample sentence is shown in figure 6." ></td>
	<td class="line x" title="97:273	Figure 7 shows those parts of the Markov models for sentences (S) and verb phrases (VP) that represent the correct paths for the example." ></td>
	<td class="line x" title="98:273	4 Given a sequence of word and phrase categories T = T1Tk and a parent category Q, we calculate the sequence of grammatical functions G = G1  Gk that link T and Q as argmaxPQ(GIT ) (1) G Pq(a)." ></td>
	<td class="line x" title="99:273	Pq(TIC) = argmax a PQ(T) = argm xPq(a)." ></td>
	<td class="line x" title="100:273	Pq(TJG) G Assuming the Markov property we have 4cf.appendix A for a description of tags used in the example 67 VP VA FIN NE A D V &--@-------@-- O ~m m ~a o ~ .2." ></td>
	<td class="line x" title="102:273	~ ~ADV VVPP NE Ps(ADVIMO) 1 PVp (VVPP IHD) l Pvp(N~IOA) 1 N ~ o o ~ d d Figure 7: Parts of the Markov models used in Selbst besucht hat Peter Sabine hie (cf.figure 6)." ></td>
	<td class="line x" title="104:273	All unused states, transitions and outputs are omitted." ></td>
	<td class="line x" title="105:273	and k PQ(TIG) = II PQ(~qlG,) (2) i=1 k Pq(a) = II P (a, lC,) (3) i=1 The contexts Ci are modeled by a fixed number of surrounding elements." ></td>
	<td class="line x" title="106:273	Currently, we use two grammatical functions, which results in a trigram model: PO(G) = H Po(GiIGi-2, Gi-1) (4) i=1 The contexts are smoothed by linear interpolation of unigrams, bigrams, and trigrams." ></td>
	<td class="line x" title="107:273	Their weights are calculated by deleted interpolation (Brown et al. , 1992)." ></td>
	<td class="line x" title="108:273	The predictions of the tagger are correct in approx." ></td>
	<td class="line x" title="109:273	94% of Ml cases." ></td>
	<td class="line x" title="110:273	In section 4.3, we demonstrate how to cope with wrong predictions." ></td>
	<td class="line x" title="111:273	4.2 Serial Order As the annotation format permits trees with crossing branches, we need a convention for determining the relative position of overlapping sibling phrases in order to assign them a position in a Markov model." ></td>
	<td class="line x" title="112:273	For instance, in figure 6 the range of the terminal node positions of VP overlaps with those of the subject $B and the finite verb HD." ></td>
	<td class="line x" title="113:273	Thus there is no single a-priori position for the VP node 5." ></td>
	<td class="line x" title="114:273	The position of a phrase depends on the position of its descendants." ></td>
	<td class="line x" title="115:273	We define the relative order of two phrases recursively as the order of their anchors, i.e., some specified daughter nodes." ></td>
	<td class="line x" title="116:273	If the anchors are words, we simply take their linear order." ></td>
	<td class="line x" title="117:273	The exact definition of the anchor is based on linguistic knowledge." ></td>
	<td class="line x" title="118:273	We choose the most intuitive alternative and define the anchor as the head of the phrase (or some equivalent function)." ></td>
	<td class="line x" title="119:273	Noun phrases do not necessarily have a unique head; instead, we use the last element in the noun kernel (elements of the noun kernel are determiners, adjectives, and nouns) to mark the anchor position." ></td>
	<td class="line x" title="120:273	Except for NPs, we employ a default rule that takes the leftmost element as the anchor in case the phrase has no (unique) head." ></td>
	<td class="line x" title="121:273	Thus the position of the VP in figure 6 is defined as equal to the string position of besucht." ></td>
	<td class="line x" title="122:273	The position of the VP node in figure 1 is equal to that of anfgegeben, and the position of the NP in figure 3 is equivalent to that of Bonusprograrara." ></td>
	<td class="line x" title="123:273	4.3 Reliability Experience gained from the development of the Penn Treebank (Marcus et al. , 1994) has shown that auSWithout crossing edges, the serial order of phrases is trivial: phrase Q1 precedes phrase Q2 if and only if all terminal nodes derived from Qa precede those of Q2." ></td>
	<td class="line x" title="124:273	This suffices to uniquely determine the order of sibling nodes." ></td>
	<td class="line x" title="125:273	68 tomatic annotation is useful only if it is absolutely correct, while wrong analyses are often difficult to detect and their correction can be time-consuming." ></td>
	<td class="line x" title="126:273	To prevent the human annotator from missing errors, the tagger for grammatical functions is equipped with a measure for the reliability of its output." ></td>
	<td class="line x" title="127:273	Given a sequence of categories, the tagger calculates the most probable sequence of grammatical functions." ></td>
	<td class="line x" title="128:273	In addition, it computes the probabilities of the second-best functions of each daughter node." ></td>
	<td class="line x" title="129:273	If some of these probabilities are close to that of the best sequence, the alternatives are regarded as equally suited and the most probable one is not taken to be the sole winner, the prediction is marked as unreliable in the output of the tagger." ></td>
	<td class="line x" title="130:273	These unreliable predictions can be further classified in that we distinguish 'unreliable' sequences as opposed to 'almost reliable' ones." ></td>
	<td class="line x" title="131:273	The distance between two probabilities for the best and second-best alternative, Pbest and Pseond, is measured by their quotient." ></td>
	<td class="line x" title="132:273	The classification of reliability is based on thresholds." ></td>
	<td class="line x" title="133:273	In the current implementation we employ three degrees of reliability which are separated by two thresholds 01 and 02." ></td>
	<td class="line x" title="134:273	01 separating unreliable decisions from those considered almost reliable." ></td>
	<td class="line x" title="135:273	02 marks the difference between almost and fully reliable predictions." ></td>
	<td class="line x" title="136:273	Unreliable: Pbes-----k< 01 Pseeond The probabilities of alternative assignments are within some small specified distance." ></td>
	<td class="line x" title="137:273	In this case, it is the annotator who has to specify the grammatical function." ></td>
	<td class="line x" title="138:273	Almost reliable: 01 < Pbes_____t__ < 02 Psecond The probability of an alternative is within some larger distance." ></td>
	<td class="line x" title="139:273	In this case, the most probable function is displayed, but the annotator has to confirm it." ></td>
	<td class="line x" title="140:273	Reliable: Pbes-----L__> 02 Psecond The probabilitiesof all alternatives are much smaller than that of the best assignment, thus the latter is assigned." ></td>
	<td class="line x" title="141:273	For efficiency, an extended Viterbi algorithm is used." ></td>
	<td class="line x" title="142:273	Instead of keeping track of the best path only (of." ></td>
	<td class="line x" title="143:273	(Rabiner, 1989)), we keep track of all paths that fall into the range marked by the probability of the best path and 02, i.e., we keep track of all alternative paths with probability Palt for which Pbest Part _> 02 ' Suitable values for 01 and 02 were determined empirically (cf.section 6)." ></td>
	<td class="line x" title="145:273	5 Tagging Phrase Categories The second level of automation (cf.section 3) automates the recognition of phrasal categories, and so frees the annotator from typing phrase labels." ></td>
	<td class="line x" title="147:273	The task is performed by an extension of the tagger presented in the previous section where different Markov models for each category were introduced." ></td>
	<td class="line x" title="148:273	The annotator determines the category of the current phrase, and the tool runs the appropriate model to determine the edge labels." ></td>
	<td class="line x" title="149:273	To assign the phrase label automatically, we run all models in parallel." ></td>
	<td class="line x" title="150:273	Each model assigns grammatical functions and, more important for this step, a probability to the phrase." ></td>
	<td class="line x" title="151:273	The model assigning the highest probability is assumed to be most adequate, and the corresponding label is assigned to the phrase." ></td>
	<td class="line x" title="152:273	Formally, we calculate the phrase category Q (and at the same time the sequence of grammatical functions G = G1  Gk) on the basis of the sequence of daughters T = T1  Tk with argmax maXPQ(G\]T)." ></td>
	<td class="line x" title="153:273	O G This procedure is equivalent to a different view on the same problem involving one large (combined) Markov model that enables a very efficient calculation of the maximum." ></td>
	<td class="line x" title="154:273	Let ~Q be the set of all grammatical functions that can occur within a phrase of type Q. Assume that these sets are pairwise disjoint." ></td>
	<td class="line x" title="155:273	One can easily achieve this property by indexing all used grammatical functions with their associated phrases and, if necessary, duplicating labels, e.g., instead of using HD, MO,  , use the indexed labels HDs, HDvp, MONp, This property makes it possible to determine a phrase category by inspecting the grammatical functions involved." ></td>
	<td class="line x" title="156:273	When applied, the combined model assigns grammatical functions to the elements of a phrase (not knowing its category in advance)." ></td>
	<td class="line x" title="157:273	If transitions between states representing labels with different indices are forced to zero probability (together with smoothing applied to other transitions), all labels assigned to a phrase get the same index." ></td>
	<td class="line x" title="158:273	This uniquely identifies a phrase category." ></td>
	<td class="line x" title="159:273	The two additional conditions G e GQi :=v G  GQ2 (Qi  Q2) and G1 E CO A G2 ~ GQ :::V P(G2\[G1) = 0 69 are sufficient to calculate argmax P( G\[T) G using the Viterbi algorithm and to identify both the phrase category and the respective grammatical functions." ></td>
	<td class="line x" title="160:273	Again, as described in section 4, we calculate probabilities for alternative candidates in order to get reliability estimates." ></td>
	<td class="line x" title="161:273	The overall accuracy of this approach is approx." ></td>
	<td class="line x" title="162:273	95%, and higher if we only consider the reliable cases." ></td>
	<td class="line x" title="163:273	Details about the accuracy are reported in the next section." ></td>
	<td class="line x" title="164:273	6 Experiments To investigate the possibility of automating annotation, experiments were performed with the cleaned part of the treebank 6 (approx." ></td>
	<td class="line x" title="165:273	1,200 sentences, 24,000 words)." ></td>
	<td class="line x" title="166:273	The first run of experiments was carried out to test tagging of grammatical functions, the second run to test tagging of phrase categories." ></td>
	<td class="line x" title="167:273	6.1 Grammatical Functions This experiment tested the reliability of assigning grammatical functions given the category of the phrase and the daughter nodes (supplied by the annotator)." ></td>
	<td class="line x" title="168:273	Let us consider the sentence in figure 6: two sequences of grammatical functions are to be determined, namely the grammatical functions of the daughter nodes of S and VP." ></td>
	<td class="line x" title="169:273	The information given for selbst besucht Sabine is its category (VP) and the daughter categories: adverb (ADV), past participle (wee), and proper noun (NE)." ></td>
	<td class="line x" title="170:273	The task is to assign the functions modifier (MO) to ADV, head (SO) to wee and direct (accusative) object (OA) to NE." ></td>
	<td class="line x" title="171:273	Similarly, function tags are assigned to the components of the sentence (S)." ></td>
	<td class="line x" title="172:273	The tagger described in section 4 was used." ></td>
	<td class="line x" title="173:273	The corpus was divided into two disjoint parts, one for training (90% of the respective corpus), and one for testing (10%)." ></td>
	<td class="line x" title="174:273	This procedure was repeated 10 times with different partitions." ></td>
	<td class="line x" title="175:273	Then the average accuracy was calculated." ></td>
	<td class="line x" title="176:273	The thresholds for search beams were set to 61 = 5 and 62 = 100, i.e., a decision is classified as reliable if there is no alternative with a probability larger than 1~0 of the best function tag." ></td>
	<td class="line x" title="177:273	The prediction is classified as unreliable if the probability of an alternative is larger than ~ of the most probable tag." ></td>
	<td class="line x" title="178:273	6The corpus is part of the German newspaper text provided on the ECI CD-ROM." ></td>
	<td class="line x" title="179:273	It has been part-ofspeech tagged and manually corrected previously, cf.(Thielen and Schiller, 1995)." ></td>
	<td class="line x" title="181:273	Table 1: Levels of reliability and the percentage cases where the tagger assigned a correct grammatical function (or would have assigned if a decision is forced)." ></td>
	<td class="line x" title="182:273	reliable marked unreliable overall cases correct 89% 96.7% 7% 84.3% 4% 57.3% 100% 94.2% If there is an akernative between these two thresholds, the prediction is classified as almost reliable and marked in the output (cf.section 4.3: marked assignments are to be confirmed by the annotator, unreliable assignments are deleted, annotation is left to the annotator)." ></td>
	<td class="line x" title="184:273	Table 1 shows tagging accuracy depending on the three different levels of reliability." ></td>
	<td class="line x" title="185:273	The results confirm the choice of reliability measures: the lower the reliability, the lower the accuracy." ></td>
	<td class="line x" title="186:273	Table 2 shows tagging accuracy depending on the category of the phrase and the level of reliability." ></td>
	<td class="line x" title="187:273	The table contains the following information: the number of all mother-daughter relations (i.e. , number of words and phrases which are immediately dominated by a mother node of a particular category), the overall accuracy for that phrasal category and the accuraciees for the three reliability intervals." ></td>
	<td class="line x" title="188:273	6.2 Error Analysis for Function Assignment The inspection of tagging errors reveals several sources of wrong assignments." ></td>
	<td class="line x" title="189:273	Table 3 shows the 10 most frequent errors 7 which constitute 25% of all errors (1509 errors occurred during 10 test runs)." ></td>
	<td class="line x" title="190:273	Read the table in the following way: line 2 shows the second-most frequent error." ></td>
	<td class="line x" title="191:273	It concerns NPs occurring in a sentence (S); this combination occurred 1477 times during testing." ></td>
	<td class="line x" title="192:273	In 286 of these occurrences the N P is assigned the grammatical function OA (accusative object) manually, but of these 286 cases the tagger assigned the function SB (subject) 56 times." ></td>
	<td class="line x" title="193:273	The errors fall into the following classes: 1." ></td>
	<td class="line x" title="194:273	There is insufficient information in the node labels to disambiguate the grammatical function." ></td>
	<td class="line x" title="195:273	Line 1 is an example for insufficient information." ></td>
	<td class="line x" title="196:273	The tag NP is uninformative about its case and therefore the tagger has to distinguish SB (subject) and 7See appendix A for a description of tags used in the table." ></td>
	<td class="line x" title="197:273	70 Table 2: Tagging accuracy for assigning grammatical functions depending on the category of the mother node." ></td>
	<td class="line x" title="198:273	For each category, the first row shows the percentage of branches that occur within this category and the overall accuracy, the following rows show the relative percentage and accuracy for different levels of reliability." ></td>
	<td class="line x" title="199:273	cases correct S 26% 89.1% decision 85% 92.7% marked 8% 81.9% no decision 7% 52.9% VP 7% 90.9% decision 97% 92.2% marked 1% 57.7% no decision 2% 52.3% NP 26% 96.4% decision 86% 98.6% marked 10% 86.8% no decision 4% 73.0% PP 24% 97.9% decision 92% 99.2% marked 6% 85.8% no decision 2% 75.5% others 18% 94.7% decision 91% 98.0% marked 6% 82.8% no decision 3% 22.1% Table 3: The 10 most frequent errors in assigning grammatical functions." ></td>
	<td class="line x" title="200:273	The table shows a mother and a daughter node category, the frequency of this particular combination (sum over 10 test runs), the grammatical function assigned manually (and its frequency) and the grammatical function assigned by the tagger (and its frequency)." ></td>
	<td class="line x" title="201:273	phrase elem f original assigned 5." ></td>
	<td class="line x" title="202:273	6. 7." ></td>
	<td class="line x" title="203:273	8. 9." ></td>
	<td class="line x" title="204:273	10." ></td>
	<td class="line x" title="205:273	1." ></td>
	<td class="line x" title="206:273	S 2." ></td>
	<td class="line x" title="207:273	S 3." ></td>
	<td class="line x" title="208:273	NP 4." ></td>
	<td class="line x" title="209:273	S PP VP S S S VP NP 1477 NP 1477 PP 470 VP 613 PP 252 NP 286 NP 1477 NP 1477 S 186 PP 453 SB OA PG PD PG DA PD MO MO SBP 894 OA 286 SB 52 MNR 47 OC 30 MNR 32 OA 72 SB 33 SB 78 PD 21 MO 65 56 50 42 30 26 25 21 21 21 OA (accusative object) on the basis of its position, which is not very reliable in German." ></td>
	<td class="line x" title="210:273	Missing information in the labels is the main source of errors." ></td>
	<td class="line x" title="211:273	Therefore, we currently investigate the benefits of a morphological component and percolation of selected information to parent nodes." ></td>
	<td class="line x" title="212:273	2." ></td>
	<td class="line x" title="213:273	Due to the n-gram approach, the tagger only sees a local window of the sentences." ></td>
	<td class="line x" title="214:273	Some linguistic knowledge is inherently global, e.g., there is at most one subject in a sentence and one head in a VP." ></td>
	<td class="line x" title="215:273	Errors of this type may be reduced by introducing finite state constraints that restrict the possible sequences of functions within each phrase." ></td>
	<td class="line x" title="216:273	3." ></td>
	<td class="line x" title="217:273	The manual annotation is wrong, and a correct tagger prediction is counted as an error." ></td>
	<td class="line x" title="218:273	At earlier stages of annotation, the main source of errors was wrong or missing manual annotation." ></td>
	<td class="line x" title="219:273	In some cases, the tagger was able to abstract from these errors during the training phase and subsequently assigned the correct tag for the test data." ></td>
	<td class="line x" title="220:273	However, when performing a comparison against the corpus, these differences are marked as errors." ></td>
	<td class="line x" title="221:273	Most of these errors were eliminated by comparing two independent annotations and cleaning up the data." ></td>
	<td class="line x" title="222:273	6.3 Phrase Categories In this experiment, the reliability of assigning phrase categories given the categories of the daughter nodes (they are supplied by the annotator) was tested." ></td>
	<td class="line x" title="223:273	Consider the sentence in figure 6: two phrase categories are to be determined (VP and S)." ></td>
	<td class="line x" title="224:273	The information given for selbst besucM Sabine is the sequence of categories: adverb (ADV), past participle 71 Table 4: Levels of reliability and the percentage of cases in which the tagger assigned a correct phrase category (or would have assigned if a decision is forced)." ></td>
	<td class="line x" title="225:273	reliable marked unreliable overall cases correct 79% 98.5% 16% 90.4% 5% 65.9% 100% 95.4% (VVPP), and proper noun (NE)." ></td>
	<td class="line x" title="226:273	The task is to assign category VP." ></td>
	<td class="line x" title="227:273	Subsequently, S is to be assigned based on the categories of the daughters VP, VAFIN, NE, and ADV." ></td>
	<td class="line x" title="228:273	The extended tagger using a combined model as described in section 5 was applied." ></td>
	<td class="line x" title="229:273	Again, the corpus is divided into two disjoint parts, one for training (90% of the corpus), and one for testing (10%)." ></td>
	<td class="line x" title="230:273	The procedure is repeated 10 times with different partitions." ></td>
	<td class="line x" title="231:273	Then the average accuracy was calculated." ></td>
	<td class="line x" title="232:273	The same thresholds for search beams as for the first set of experiments were used." ></td>
	<td class="line x" title="233:273	Table 4 shows tagging accuracy depending on the three different levels of reliability." ></td>
	<td class="line x" title="234:273	Table 5 shows tagging accuracy depending on the category of the phrase and the level of reliability." ></td>
	<td class="line x" title="235:273	The table contains the following information: the percentage of occurrences of the particular phrase, the overall accuracy for that phrasal category and the accuracy for each of the three reliability intervals." ></td>
	<td class="line x" title="236:273	6.4 Error Analysis for Category Assignment When forced to make a decision (even in unreliable cases) 435 errors occured during the 10 test runs (4.5% error rate)." ></td>
	<td class="line x" title="237:273	Table 6 shows the 10 mostfrequent errors which constitute 50% of all errors." ></td>
	<td class="line x" title="238:273	The most frequent error was the confusion of S and VP." ></td>
	<td class="line x" title="239:273	They differ in that sentences S contain finite verbs and verb phrases VP contain non-finite verbs." ></td>
	<td class="line x" title="240:273	But the tagger is trained on data that contain incomplete sentences and therefore sometimes erroneously assumes an incomplete S instead of a VP." ></td>
	<td class="line x" title="241:273	To avoid this type of error, the tagger should be able to take the neighborhood of phrases into account." ></td>
	<td class="line x" title="242:273	Then, it could detect the finite verb that completes the sentence." ></td>
	<td class="line x" title="243:273	Adjective phrases AP and noun phrases NP are confused by the tagger (line 5 in table 6), since almost all AP's can be NP's." ></td>
	<td class="line x" title="244:273	This error could also Table 5: Tagging accuracy for assigning phrase categories, depending on the manually assigned category." ></td>
	<td class="line x" title="245:273	For each category, the first row shows the percentage of phrases belongi:lg to a specific category (according to manual ~,zsignment) and the percentage of correct assignments." ></td>
	<td class="line x" title="246:273	The following rows show the relative percentage and accuracy for different levels of reliability." ></td>
	<td class="line x" title="247:273	cases correct S 20% 97.5% decision 96% 99.7% marked 2% 63.2% no decision 2% 29.0% VP 9% 93.2% decision 71% 96.4% marked 24% 91.3% no decision 5% 60.9% NP 29% 96.1% decision 81% 99.3% marked 13% 91.8% no decision 6% 64.9% PP 24% 98.7% decision 94% 99.6% marked 4% 92.5% no decision 2% 70.8% others 18% 89.0% decision 42% 91.7% marked 45% 90.6% no decision 12% 73.2% 72 Table 6: The 10 most frequent errors in assigning phrase categories (summed over reliability levels)." ></td>
	<td class="line x" title="248:273	The table shows the phrase category assigned manually (and its frequency) and the category erroneously assigned by the tagger (and its frequency)." ></td>
	<td class="line x" title="249:273	I 2." ></td>
	<td class="line x" title="250:273	3. 4." ></td>
	<td class="line x" title="251:273	5. 6." ></td>
	<td class="line x" title="252:273	7. 8." ></td>
	<td class="line x" title="253:273	9. 10." ></td>
	<td class="line x" title="254:273	phrase f assigned VP 828 S NP 2812 NM NP 2812 PP NP 2812 S AP 419 NP DL 20 CS PP 2298 NP S 1910 NP AP 419 PP MPN 293 NP 46 32 31 25 15 15 15 15 11 11 be fixed by inspecting the context and detecting the associated NP." ></td>
	<td class="line x" title="255:273	As for assigning grammatical functions, insufficient information in the labels is a significant source of errors, cf.the second-most frequent error." ></td>
	<td class="line x" title="257:273	A large number of cardinal-noun pairs forms a numerical component (NM), like 7 Millionen, 50 Prozent, etc (7 million, 50 percent)." ></td>
	<td class="line x" title="258:273	But this combination also occurs in NPs like 20 Leule, 3 Monate,  (20 people, 3 months), which are mis-tagged since they are less frequent." ></td>
	<td class="line x" title="259:273	This can be fixed by introducing an extra tag for nouns denoting numericals." ></td>
	<td class="line x" title="260:273	7 Conclusion A German newspaper corpus is currently being annotated with a new annotation scheme especially designed for free word order languages." ></td>
	<td class="line x" title="261:273	Two levels of automatic annotation (level 1: assigning grammatical functions and level 2: assigning phrase categories) have been presented and evaluated in this paper." ></td>
	<td class="line x" title="262:273	The overall accuracy for assigning grammatical functions is 94.2%, ranging from 89% to 98%, depending on the type of phrase." ></td>
	<td class="line x" title="263:273	The least accuracy is achieved for sentences, the best for prepositional phrases." ></td>
	<td class="line x" title="264:273	By suppressing unreliable decisions, precision can be increased to range from 92% to 99%." ></td>
	<td class="line x" title="265:273	The overall accuracy for assigning phrase categories is 95.4%, ranging from 89% to 99%, depending the category." ></td>
	<td class="line x" title="266:273	By suppressing unreliable decisions, precision can also be increased to range from 92% to over 99%." ></td>
	<td class="line x" title="267:273	In the error analysis, the following sources of misinterpretation could be identified: insufficient linguistic information in the nodes (e.g. , missing case information), and insufficient information about the global structure of phrases (e.g. , missing valency information)." ></td>
	<td class="line x" title="268:273	Morphological information in the tagset, for example, helps to identify the objects and the subject of a sentence." ></td>
	<td class="line x" title="269:273	Using a more fine-grained tagset, however, requires methods for adjusting the granularity of the tagset to the size (and coverage) of the corpus, in order to cope with the sparse data problem." ></td>
	<td class="line x" title="270:273	8 Acknowledgements This work is part of the DFG Sonderforschungsbereich 378 Resource-Adaptive Cognitive Processes, Project C3 Concu rent Grammar Processing." ></td>
	<td class="line x" title="271:273	We wish to tl~ank the universities of Stuttgart and Tiibingen for kindly providing us with a handcorrected part-of-speech tagged corpus." ></td>
	<td class="line x" title="272:273	We also wish to thank Jason Eisner, Robert MacIntyre and Ann Taylor for valuable discussions on dependency parsing and the Penn Treebank annotation." ></td>
	<td class="line x" title="273:273	Special thanks go to Oliver Plaehn, who implemented the annotation tool, and to our six fearless annotators." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W97-0811
An Experiment In Semantic Tagging Using Hidden Markov Model Tagging
Segond, Frederique;Schiller, Anne;Grefenstette, Gregory;Chanod, Jean-Pierre;"></td>
	<td class="line x" title="1:94	An Experiment in Semantic Tagging using Hidden Markov Model Tagging Fr~d~rique Segond, Anne Schiller, Gregory Grefenstette, Jean-Pierre Chanod Rank Xerox Research Centre, 6 Chemin de Maupertuis, F-38240 Meylan, France {Segond, Schiller, Grefenstette, Chanod}@grenoble.rxrc.xerox.com Abstract The same word can have many different meanings depending on the context in which it is used." ></td>
	<td class="line x" title="2:94	Discovering the meaning of a word, given the text around it, has been an interesting problem for both the psychology and the artificial intelligence research communities." ></td>
	<td class="line x" title="3:94	In this article, we present a series of experiments, using methods which have proven to be useful for eliminating part-of-speech ambiguity, to see if such simple methods can be used to resolve semantic ambiguities." ></td>
	<td class="line x" title="4:94	Using a publicly available semantic lexicon, we find the Hidden Markov Models work surprising well at choosing the right semantic categories, once the sentence has been stripped of purely functional words." ></td>
	<td class="line x" title="5:94	1 Introduction Any natural language processing system treating anything beyond the most restricted domains is confronted with the problem of distinguishing between uses of polysemous words." ></td>
	<td class="line x" title="6:94	The idea behind semantically tagging words is that sense markings added to words may be used by some automatic process in order to choose the proper senses of words in a given context." ></td>
	<td class="line x" title="7:94	For example, the word bark would receive at least two possible semantic tags and these tags along with the tags of other words in the surrounding context would allow the process to distinguish between the senses the bark of a tree, and the bark of a dog." ></td>
	<td class="line x" title="8:94	(See \[Dagan and Itai, 1994; Gale et al. , 1992a; Gale et al, 1992b; Ng and Lee, 1996; Wilks, 1996; Yarowski, 1992; Yarowski, 1995\] for recent work on word sense disambiguation)." ></td>
	<td class="line x" title="9:94	Semantic tagging is considered to be a much more difficult task than part-of-speech tagging." ></td>
	<td class="line x" title="10:94	Despite this current thinking, we decided to perform an experiment to see how well words can be semantically disambiguated using techniques that have proven to be effective in part-of-speech tagging." ></td>
	<td class="line x" title="11:94	We decided to use the 45 semantic tags available through the WordNet package." ></td>
	<td class="line x" title="12:94	In this typology, the word bark has two a priori semantic tags: bark as a ''covering, natural covering, cover' receives tag 20 (nouns denoting plants); and bark as ''noise, cry' has tag 11 (nouns denoting natural events)." ></td>
	<td class="line x" title="13:94	This semantic tagset has two advantages: it is a reasonable size, so that statistical techniques that we are testing do not need an inordinate amount of training data; and secondly, a semantically tagged corpus is available that we can use for testing." ></td>
	<td class="line x" title="14:94	2 WordNet Semantic tags Part-of-speech tagging is better understood than semantic tagging." ></td>
	<td class="line x" title="15:94	For one thing, no consensus on semantic tags exists, contrary to the general consensus on the higher level part-of-speech tags." ></td>
	<td class="line x" title="16:94	And it seems more likely that syntactic tags be generalizable over wider textual domains than semantic ones." ></td>
	<td class="line x" title="17:94	Despite this, the WordNet team has taken upon themselves to create a general semantic tagging scheme and to apply it on a large scale: every set of synonymous senses, synsets, are tagged with one of 45 tags as WordNet version 1.5 ~." ></td>
	<td class="line x" title="18:94	In their schema, there are3 tags for adjectives (relational adjectives, participial adjectives and all others), 1 tag for all adverbs, 26 tags for nouns (act, animal, man-made artifact, attributes, body parts, substance, and time), and 15 tags for verbs (from grooming and dressing verbs, to verbs of weather)." ></td>
	<td class="line x" title="19:94	These tags are assigned for the most general uses of words." ></td>
	<td class="line x" title="20:94	For example, the noun blood is tagged as 07 (an attribute of people and objects), as 08 (body part) and as 14 (groupings of people and objects)." ></td>
	<td class="line x" title="21:94	Blood is not tagged as 27 (substance) or as 13 (food), though it might well be considered as such in certain contexts." ></td>
	<td class="line x" title="22:94	1 Ftp-able at clarity princeton edu 78 3 HMM Tagging We wanted to see how well these WordNet semantic tags could be disambiguated using the same well-understood techniques employed in statistical part-of-speech disambiguation." ></td>
	<td class="line x" title="23:94	Part-of-speech disambiguation relies on the fact that certain sequences of parts of speech are more probable than others." ></td>
	<td class="line x" title="24:94	Often, this probability is estimated from the frequency of sequences of tags in hand tagged texts." ></td>
	<td class="line oc" title="25:94	In our experiments, we used the Hidden Markov Model (HMM) tagging method described in \[Cutting et aL, 1992\]." ></td>
	<td class="line o" title="26:94	In this method, the probability of seeing a given tag depends on the ambiguity class of the word and on the ambiguity class of the words preceding it." ></td>
	<td class="line x" title="27:94	An ambigmty class of a word is the set of words which each have exactly the same set of ambiguous tags." ></td>
	<td class="line o" title="28:94	This class is used during the Xerox HMM tagging in place of more specific lexical (= word-based) probabilities." ></td>
	<td class="line n" title="29:94	Lexical probabilities would more accurately inform the tagger with the frequency with which a certain word receives a certain tag, but acquiring this frequency requires much greater amounts of tagged text than is necessary with the ambiguity class method." ></td>
	<td class="line x" title="30:94	The HMM training and tagging programs in our experiment \[Wilkens and Kupiec, 1995\] are based on bigrams, i.e. only the immediate context of a word is taken into account." ></td>
	<td class="line x" title="31:94	The use of this statistical disambiguation combines with the advantage of the limited number of WordNet tags so that training can be performed on a relatively small corpus." ></td>
	<td class="line x" title="32:94	4 Data Preparation and Tagger Training In order to make a HMM for semantic tags we performed the following steps: 1." ></td>
	<td class="line x" title="33:94	We derived a lexicon from the WordNet data files which contains all possible semantic tags for each noun, adjective, adverb and verb." ></td>
	<td class="line x" title="34:94	Words having no semantic tags (determiners, prepositions, auxiliary verbs, etc)." ></td>
	<td class="line x" title="35:94	are assigned their part of speech tags." ></td>
	<td class="line x" title="36:94	2." ></td>
	<td class="line x" title="37:94	With version 1.5 of WordNet is delivered about one-fifth of the Brown corpus which has been semantically tagged by the WordNet team." ></td>
	<td class="line x" title="38:94	From these 11,182 sentences, we constructed a traming corpus and a test corpus of equal size, taking all even numbered sentences for the training corpus and all odd-numbered sentences for the test corpus." ></td>
	<td class="line x" title="39:94	From both corpora, in order to use ''semantically relevant' tokens for the HMM bigrams, we retained all nouns, verbs, adverbs, and adjectives and deleted all function words except prepositions, commas, final stops, personal pronouns and interrogative adverbs." ></td>
	<td class="line x" title="40:94	3." ></td>
	<td class="line x" title="41:94	We computed a HMM model based on the training corpus, ran the resulting semantic tagger on an untagged version of test corpus and we compared the tags assigned by the semantic tagger to original tags in the test corpus." ></td>
	<td class="line x" title="42:94	5 Tagging Results 5.1 Test 1 As described above, the semantically tagged text provided by WordNet (CO) was transformed into a training corpus (C 1)." ></td>
	<td class="line x" title="43:94	(co) The/DT Fulton_County_Grand_Jury/03 said/32 Friday/28 an/DT investlgation/09 of/IN Atlanta/15 's/POS recent/00 pmmary_election/04 produced/39 ''/'' no/DT evxdence/09 '/' that/IN any/DT irregularmes/04 took_place/30 ./." ></td>
	<td class="line x" title="44:94	(CI) Fulton_County_Grand Jury/03 sald/32 Friday/28 investigation/09 of/IN Atlanta/15 recent/00 prlmary_election/04 produced/39 evldence/09 that/IN irregularities/04 took place/30 ./." ></td>
	<td class="line x" title="45:94	The lexicon used for this experiment contains 3,282 different ambiguity classes made of 52 semantic tags (45 WordNet tags + 6 pan-of-speech tags + 1 tag for non-lexicalized word-forms)." ></td>
	<td class="line x" title="46:94	The training corpus consists of 75,000 tokens and covers about 72% of all possible ambiguity classes." ></td>
	<td class="line x" title="47:94	The test corpus contains 90,000 tokens." ></td>
	<td class="line x" title="48:94	46% of the words are ambiguous, i.e. the lexicon provides at least two (and at most 15) different semantic tags for these words." ></td>
	<td class="line x" title="49:94	For the test corpus the overall accuracy was of 86% and the accuracy over ambiguous tokens of 71% correctly chosen WordNet semantic tags." ></td>
	<td class="line x" title="50:94	5.2 Test 2 In fact, the first experiment combined syntactic and semantic tagging, as the WordNet tags are classified by part-of-speech categories." ></td>
	<td class="line x" title="51:94	Thus we run a second experiment which applies semantic tagging after part-of-speech tagging." ></td>
	<td class="line x" title="52:94	We simulated the part-of-speech tagging step by adding a syntactic category to the training and test corpus: (C3) FFulton_County_Grand_Jury=NOUN / 03 79 said=VERB/32 Friday=NOUN/28 mvestlgation/09 of/IN Atlanta=NOUN/15 recent=ADJ/00 primary_election=NOUN / 04 produced=VERB/39 evidence=NOUN/09 that/IN irregularities=NOUN / 04 tookplace=VERB/30 ./." ></td>
	<td class="line x" title="53:94	We modified the lexicon accordingly." ></td>
	<td class="line x" title="54:94	For example, a single lexicon entry for bark was divided into two entries for the verb and for the noun reading: (LI) bark {06, 11, 20, 30, 32, 35} (L2} bark=VERB {30, 32, 35} bark=NOUN {06, 11, 20}." ></td>
	<td class="line x" title="55:94	Using part-of-speech pre-tagging, the number of ambiguity classes decreases (1685) and only 27% of the word forms in the test corpus are ambiguous." ></td>
	<td class="line x" title="56:94	With this method, the accuracy over the entire text is of 89%." ></td>
	<td class="line x" title="57:94	This improvement is mainly due to the lower overall ambiguity rate: part-of-speech pre-tagging solved the ''semantic' ambiguity for 40% of the ambiguous words in Test 1." ></td>
	<td class="line x" title="58:94	The error rate for those words which remain ambiguous after part-of-speech disambiguation is almost identical (71% correctly chosen tags) for both test cases." ></td>
	<td class="line x" title="59:94	5.3 Test 3 For the part-of-speech tagging problem, it is known that assigning the most common part of speech for each lexical item gives a baseline of 90% accuracy \[Brill, 1992\]." ></td>
	<td class="line x" title="60:94	In order to see what a similar baseline is for semantic tagging over part-of-speech tagged text, we performed the following experiment." ></td>
	<td class="line x" title="61:94	From the training corpus, we calculated the most frequent semantic tag for each partof-speech tagged lemma 2." ></td>
	<td class="line x" title="62:94	On the test corpus, we assigned the most frequent semantic tag to each known word, and for unknown nouns, verbs, adverbs, and adjectives, we assigned the most common semantic tag per part-of-speech." ></td>
	<td class="line x" title="63:94	Capitalized unknown nouns were assigned the S03 tag." ></td>
	<td class="line x" title="64:94	Non-semantically tagged words were considered correctly tagged." ></td>
	<td class="line x" title="65:94	The result of this tagging resulted in a baseline of 81% of correctly chosen semantic tags over all words, worse than the two preceding tests." ></td>
	<td class="line x" title="66:94	6 Discussion and Conclusion We found it surprising that the same statistical techniques that improve part-of-speech tag disambiguation from a baseline of 90% to 95-96% work almost as well with semantic tags once function words are removed from the text to be tagged." ></td>
	<td class="line x" title="67:94	The HMM technique improved the baseline 81% to 89% correctly chosen se2 Ties were resolved by randomly choosing one of the semantic tags." ></td>
	<td class="line x" title="68:94	mantic tags." ></td>
	<td class="line x" title="69:94	These experiments show renewed promise for a statistical approach to the problem of sense disambiguation, with a relatively small training set." ></td>
	<td class="line x" title="70:94	Future plans include analyzing the kind of errors we get, to classify them." ></td>
	<td class="line x" title="71:94	Starting from this classification we hope to be able to answer the following questions: what type of semantic tags should be used, should a nonbinary HMM be used, and how much ambiguity can be resolved using local clues." ></td>
	<td class="line x" title="72:94	We also plan to consider reasonable applications for semantic tagging." ></td>
	<td class="line x" title="73:94	One possibility would be to use semantic tagging in the framework of an intelligent on line dictionary lookup such as LocoLex \[Bauer et al, 1995\]." ></td>
	<td class="line x" title="74:94	LocoLex is a tool that has been developed at RXRC and which looks up a word in a bilingual dictionary taking the syntactic context into account." ></td>
	<td class="line x" title="75:94	For instance, in a sentence such as They like to swzm the part of speech tagger in LocoLex determines that hke is a verb and not a preposition." ></td>
	<td class="line x" title="76:94	Accordingly, the dictionary lookup component provides the user with the translation for the verb only." ></td>
	<td class="line x" title="77:94	LocoLex also detects multi-word expressions 3." ></td>
	<td class="line x" title="78:94	For instance, when stuck appears in the sentence my own parents stuck together the translation displayed after the user clicks on stuck is the one for the whole phrase sttck together and not only for the word stick." ></td>
	<td class="line x" title="79:94	Currently LocoLex is purely syntactic and cannot distinguish between the different meanings of a noun like bark." ></td>
	<td class="line x" title="80:94	If, in addition to the current syntactic tags, we had access to the semantic tags provided by WordNet for this word (natural event and plants) and we were able to include this label in the online dictionary, this would improve the bilingual dictionary access of Locolex even further." ></td>
	<td class="line x" title="81:94	Current bilingual dictionaries often include some semantic marking." ></td>
	<td class="line x" title="82:94	For instance, in the OUP-Hachette English French dictionary, under bark we find the label Bot(anical) attached to one meaning and the collocator (of dog) associated with the other one." ></td>
	<td class="line x" title="83:94	It is possible that some type of automated matching between these indications and the WordNet semantic tags 4 would allow the integration of a semantic tagger into LocoLex." ></td>
	<td class="line x" title="84:94	Using only existing dictionary labels might still not be completely satisfying for machine translation." ></td>
	<td class="line x" title="85:94	Indeed looking back at the example my own parents stuck together even if we retrieved the multi-word expression meaning it will be difficult to decide which translation to choose with existing dictionary indications s. 3 Multi-words expressions include ldtomattc expression (to sweep something under the rug), phrasal verbs (to spa up), or compounds (warmng hght) 4 Or some other derwed tag set." ></td>
	<td class="line x" title="86:94	5 Especially considering that WordNet provides only two senses of stick together $35 and $41." ></td>
	<td class="line x" title="87:94	80 For instance for stzck together the Oxford-Hachette English French dictionary gives: stick together 1." ></td>
	<td class="line x" title="88:94	(become fixed to each other) (pages) se coller 2." ></td>
	<td class="line x" title="89:94	(CoU) (remain loyal~ se serrer les coudes (Faro) 6tre sohdalre 3." ></td>
	<td class="line x" title="90:94	(Coil) (not separate) rester ensemble It appears clearly that using general dictionary labels would not be enough to choose the third meaning only." ></td>
	<td class="line x" title="91:94	We would need to investigate further how to make better use of dictionary information such as collocators, etc. Another interesting application we would like to examine is how useful semantic tagging could be in determining the genre or topic of a text." ></td>
	<td class="line x" title="92:94	Here, an initial idea would be to just count the number of occurrence of a given semantic tag and from this to determine the topic or the genre of a given text." ></td>
	<td class="line x" title="93:94	This could be useful in machine translation system to help, for instance, in choosing the appropriate lexicon (containing the specific terminology)." ></td>
	<td class="line x" title="94:94	Assuming that such dictionaries are less ambiguous, this could in return, improve the accuracy of the lexical semantic choice in automatic translation." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C98-1060
Combining Stochastic and Rule-Based Methods for Disambiguation in Agglutinative Languages
N., Ezeiza;Alegria, Iaki;J.M., Areola;R., Urizar;I., Aduriz;"></td>
	<td class="line x" title="1:157	Combining Stochastic and Rule-Based Methods for Disambiguation in Agglutinative Languages Ezeiza N., Alegria I., Areola J.M., Urizar R. Informatika Fakultatea 649 P.K Donostia E-20080 jibecran@si.ehu.es http://ixa.si.ehu,es Aduriz I. UZEI Aldapeta, 20." ></td>
	<td class="line x" title="2:157	Donostia E-20009 uzei@sarenet.es Laburpena Artikulu honetan metodo estokastiko cta erregeletan oinarritutako mctodoen arteko konbinaketa euskarari aplikatzearcn emaitzak aurkeztuko dimgu.Desanbiguazioan erabilitako metodoak Murrizpcn Gramatika (CG) eta MULTEXT proiektu~ garatutako HMMn oinarritutako etiketatzailea dira." ></td>
	<td class="line x" title="3:157	Euskara hizkuntza eranskaria izaki, hitz bakoitzari dagozkion irakurketa guztiak esleitzeko analizatzaile morfoiogikoa beharrezkoa da." ></td>
	<td class="line x" title="4:157	Ondoren, CG crregelak informazio morfologiko guztiari aplikatzen zaizkio eta prozesu honek t~stuen anbiguotasuna gutxitzen du." ></td>
	<td class="line x" title="5:157	Azkenik, geratntako etiketen artean bakarra hautatzeko MULTEXT proiekmko tresnak erabiltzen dira." ></td>
	<td class="line x" title="6:157	Metodo cstokastikoa soilik crabil~ean, crrore-tasa %14 ingurukoa da, baina etiketatzailearen doitasuna hitz ezezagunekin lexikoa aberastuz gero %2 hobe daitekeen arren." ></td>
	<td class="line x" title="7:157	Metodo biak konbinatzen direnemL berriz, prozesu osoaren errore-tasa %3.5ekoa da." ></td>
	<td class="line x" title="8:157	Ikasketarako eorpusa nahikoa txikia dcla, HMM credua lchenengo mailakoa eta euskararako Murrizpen Gramatika oraindik ere garapcn prozcsuan dagoela kontuan izanik, gure ustez metodo konbinatu hau erabilita emaitza onak lor daitezkc eta bestc hizkuntza cranskarietarako bereziki egokia izml daiteke." ></td>
	<td class="line x" title="9:157	Resum En aquest article presentem els resultats de la eombinaci6 de mc3todes cstoc~tsties i basats en regles aplicats a la desambiguaci6 morfosin~ctica de l'euskara." ></td>
	<td class="line x" title="10:157	Els m6todes utilitzats per a la desambiguaei6 s6n: les Gramatiques de Restriccions (CG) i l'etiquetador basat en HMM del projecte MULTEXT." ></td>
	<td class="line x" title="11:157	E1 carAeter aglutinant de l'euskara fa necessari la utilitzaci6 d'un analitzador morfol6gic per assignar a cada paraula totes les seves interpretacions." ></td>
	<td class="line x" title="12:157	Los regles de CG s'apliquen utilitzant la informaeio morfol6gica completa i aquest procds reducix parcialment l'ambigtiitat dels textos." ></td>
	<td class="line x" title="13:157	A confinuaci6, s'apliqucn les eines de MULTEXT per escollir una finiea etiqueta." ></td>
	<td class="line x" title="14:157	Utilitzant nom6s el m6tode estoeastic la taxa d'error 6s aproxilnadament del 14%, eneara que la precisi6 de l'etiquetador cs pot inerementar cn un 2% utilitzant los paraules desconegudes per enriquir el 16xic." ></td>
	<td class="line x" title="15:157	En canvi, la eombinaci6 d'ambd6s m6todes permet reduir l'crror fins al 3.5%." ></td>
	<td class="line x" title="16:157	Tenint en compte que el corpus d'aprenentatge 6s bastant petit, que el model HMM ds de primer ordre i que la Gramfitica de Restriccions de reuskara esfft encara cn fase de desenvolupament, creiem que els resultats del m6tode combinat s6n bons i que ta combinaeio de m6todes 6s especialment adequada per a llengfies aglutin,'mts." ></td>
	<td class="line x" title="17:157	Resumen En este artieulo presentamos los resultados de la combinaci6n de m6todos estochsticos y basados en reglas aplicados al euskara." ></td>
	<td class="line x" title="18:157	Los m6todos utilizados para la desambiguaei6n son las Gram~tticas de Restriceiones (CG) y el etiquetador basado en HMM del proyceto MULTEXT." ></td>
	<td class="line x" title="19:157	Siendo el cuskara una lengua aglutinante, ser~ necesario un analizador morfol6gico para asignar a cada palabra todas sus interpretacioncs." ></td>
	<td class="line x" title="20:157	A continuaci6n se aplican las reglas de CG utilizando toda la informaei6n morfol6gica y este proceso disminuye la ambigfledad de los textos." ></td>
	<td class="line x" title="21:157	Pot 61timo, las herramientas de MULTEXT escoger~n una finica etiqueta." ></td>
	<td class="line x" title="22:157	Utilizmado finicamente el m6todo estoc~tstieo la tasa de error es de alrededor del 14%, aunque la precisi6n del etiquetador puede incrementarse en un 2% utilizmldo las palabras desconocidas para enriquecer el 16xico." ></td>
	<td class="line x" title="23:157	En cambio, combinando mnbos m6todos la tasa de error del proceso completo es del 3.5%." ></td>
	<td class="line x" title="24:157	Teniendo en cuenta que el corpus de aprendizajc es bastante pequefio, que el modelo HMM es de primer orden y que la Gramfitica de Restricci6n del euskara est~ afin en fase de desarrollo, creemos el mdtodo combinado obticne buenos resultados y puede ser adecuado para otras lenguas aglutinantes." ></td>
	<td class="line x" title="25:157	379 Combining Stochastic and Rule-Based Methods for Disambiguation in Agglutinative Languages Ezeiza N., Alegria I., Arriola J.M., Urizar R. Informatika Fakultatea 649 P.K Donostia E-20080 jibecran@si.ehu.es http://ixa.si.ehu.es Abstract In this paper we present the results of the combination of stochastic and rule-based disambiguation methods applied to Basque languagel." ></td>
	<td class="line x" title="26:157	The methods we have used in disambiguation are Constraint Grammar formalism and an HMM based tagger developed within the MULTEXT project." ></td>
	<td class="line x" title="27:157	As Basque is an agglutinative language, a morphological analyser is needed to attach all possible readings to each word." ></td>
	<td class="line x" title="28:157	Then, CG rules are applied using all the morphological features and this process decreases morphological ambiguity of texts." ></td>
	<td class="line x" title="29:157	Finally, we use the MULTEXT project tools to select just one from the possible remaining tags." ></td>
	<td class="line x" title="30:157	Using only the stochastic method the error rate is about 14%, but the accuracy may be increased by about 2% enriching the lexicon with the unknown words." ></td>
	<td class="line x" title="31:157	When both methods are combined, the error rate of the whole process is 3.5%." ></td>
	<td class="line x" title="32:157	Considering that the training corpus is quite small, that the HMM model is a first order one and that Constraint Grammar of Basque language is still in progress, we think that this combined method can achieve good results, and it would be appropriate for other agglutinative languages." ></td>
	<td class="line x" title="33:157	Introduction Based on the results of the combination of stochastic and rule-based disambiguation methods applied to Basque language, we will show that the results of the combination are significantly better than the ones obtained applying the methods separately." ></td>
	<td class="line x" title="34:157	As Basque is an agglutinative and highly inThis research has been supported by the Education Department of the Government of the Basque Country and the Interministerial Commision for Science and Technology." ></td>
	<td class="line x" title="35:157	Aduriz I. UZEI Aldapeta, 20." ></td>
	<td class="line x" title="36:157	Donostia E-20009 uzei@sarenet.es flected language, a morphological analyser is needed to attach all possible interpretations to each word." ></td>
	<td class="line x" title="37:157	This process, which may not be necessary in other languages such as English, makes the tagging task more complex." ></td>
	<td class="line x" title="38:157	We use MORFEUS, a robust morphological analyser for Basque developed at the University of the Basque Country (Alegria et al., 1996)." ></td>
	<td class="line x" title="39:157	We present it briefly in section 1, in the overview of the whole system, the lemmatiser/tagger for Basque EUSLEM." ></td>
	<td class="line x" title="40:157	We have added to MORFEUS a lemma disambiguation process, described in section 2, which discards some of the analyses of the word based on statistical measures." ></td>
	<td class="line x" title="41:157	Another important issue concerning a tagger is the tagset itself." ></td>
	<td class="line x" title="42:157	We discuss the design of the tagset in section 3." ></td>
	<td class="line x" title="43:157	In section 4, we present the results of the application of rule-based and stochastic disambiguation methods to Basque." ></td>
	<td class="line x" title="44:157	These results are deeply improved by combining both methods as explained in section 5." ></td>
	<td class="line x" title="45:157	Finally, we discuss some possible improvements of the system and future research." ></td>
	<td class="line x" title="46:157	1 Overview of the system The disambiguation system is integrated in EUSLEM, a lemmatiser/tagger for Basque (Aduriz et al., 1996)." ></td>
	<td class="line x" title="47:157	EUSLEM has three main modules:  MORFEUS, the morphological analyser based on the two-level formalism." ></td>
	<td class="line x" title="48:157	It is a robust and wide coverage analyser for Basque." ></td>
	<td class="line x" title="49:157	 the module that treats multiword lexical units." ></td>
	<td class="line x" title="50:157	It has not been used in the experiments in order to simplify the process." ></td>
	<td class="line x" title="51:157	 the disambiguation module, which will be described in sections 5 and 6." ></td>
	<td class="line x" title="52:157	MORFEUS plays an important role in the lemmatisefftagger, because it assigns every token all the morphological features." ></td>
	<td class="line x" title="53:157	The most important functions are:  incremental analysis, which is divided in 380 three phases, using the two level formalism in all of them: 1) the standard analyser processes words according to the standard lexicon and standard rules of the language; 2) the analyser of linguistic variants analyses dialectal variants and competence errors2; and 3) the analyser of unknown words or guesser processes the remaining words." ></td>
	<td class="line x" title="54:157	lemma disambiguation, presented below." ></td>
	<td class="line x" title="55:157	2 Lemma disambiguation The lemma disambiguation has been added to the previously developed analyser for two main reasons: ., the average number of interpretations in unknown words is significantly higher than in standard words." ></td>
	<td class="line x" title="56:157	* there could be more than one lemma per tag." ></td>
	<td class="line x" title="57:157	Since the disambiguation module won't deal with this kind of ambiguity, it has to be solved to lemmatise the text." ></td>
	<td class="line x" title="58:157	We use different methods for the disambiguation of linguistic variants and unknown words." ></td>
	<td class="line x" title="59:157	In the case of linguistic variants we try to select the lemma that is 'nearest' to the standard one according to the number of non-standard morphemes and roles." ></td>
	<td class="line x" title="60:157	We choose the interpretation that has less non-standard uses." ></td>
	<td class="line x" title="61:157	before after variants  2.58 2.52 unknown 13.1 6.21 Table 1Number of readings." ></td>
	<td class="line x" title="62:157	In the case of unknown words, the procedure uses the following criteria:  for each category and subcategory pair, leave at least one interpretation." ></td>
	<td class="line x" title="63:157	 assign a weight to each lemma according to the final t.figram and the category and subcategory pair." ></td>
	<td class="line x" title="64:157	select the lemma according to its length and weight -best combination of high weight and short lemma." ></td>
	<td class="line x" title="65:157	These procedures have been tested with a small corpus and the produced error-rate is 0.2%." ></td>
	<td class="line x" title="66:157	This is insignificant considering that the average number of interpretations of unknown words decreases by 7, as shown in table 1." ></td>
	<td class="line x" title="67:157	3 Designing the tagset The choice of a tagset is a critical aspect when designing a tagger." ></td>
	<td class="line x" title="68:157	Before defining the tagset z This module is very usefld since Basque is still in normalisation process." ></td>
	<td class="line x" title="69:157	we have had to take some aspects into account: there was not any exhaustive tagset for automatic use, and the output of the morphological analyser is too rich and does not offer a directly applicable tagset." ></td>
	<td class="line x" title="70:157	While designing the general tagset, we tried to meet the following requirements:  it had to take into account all the problems concerning ellipsis, derivation and composition (Aduriz et al., 1995)." ></td>
	<td class="line x" title="71:157	 in addition, it had to be general, far from ad hoc tagsets." ></td>
	<td class="line x" title="72:157	 it had to be coherent with the information provided by the morphological analyser." ></td>
	<td class="line x" title="73:157	Bearing all these considerations in mind, the tagset has been structured in four levels:  in the first level, general categories are included (noun, verb, etc.)." ></td>
	<td class="line x" title="74:157	There are 20 tags." ></td>
	<td class="line x" title="75:157	 in the second level each category tag is further refined by subcategory tags." ></td>
	<td class="line x" title="76:157	There are 48 tags." ></td>
	<td class="line x" title="77:157	 the third level includes other interesting information, as declension case, verb tense, etc. There are 318 tags in the training corpus, but using a larger corpus we found 185 new tags." ></td>
	<td class="line x" title="78:157	 the output of the morphological analysis constitutes the last level of tagging." ></td>
	<td class="line x" title="79:157	There are 2,943 different interpretations in this training corpus, but we have found more than 9,000 in a larger corpus." ></td>
	<td class="line x" title="80:157	ambiguity rate first 35.11% second 40.68% third 62.24% fourth 64.42% tags/token 1.48 1.-57' 2.20 3: 8 Table 2Ambiguity of each level." ></td>
	<td class="line x" title="81:157	The morphological ambiguity will differ depending on the level of tagging used in each case, as shown in table 2." ></td>
	<td class="line nc" title="82:157	4 Morphological Disambiguation There are two kinds of methods for morphological disambiguation: on one hand, statistical methods need little effort and obtain very good results (Church, 1988; Cutting el al., 1992), at least when applied to English, but when we try to apply them to Basque we encounter additional problems; on the other hand, some rule-based systems (Brill, 1992; Voutilainen et al., 1992) are at least as good as statistical systems and are better adapted to free-order languages and agglutinative languages." ></td>
	<td class="line x" title="83:157	So, we 381 have selected one of each group: Constraint Grammar formalism (Karlsson et al., 1995) and the HMM based TATOO tagger (Armstrong et al., 1995), which has been designed to be applied it to the output of a morphological analyser and the tagset can be switched easily without changing the input text." ></td>
	<td class="line x" title="84:157	 second \[\] third 70 ks I M M* M+CG M*+CG Figure 1-Initial ambiguity3." ></td>
	<td class="line x" title="85:157	We have used the second and third levels tagsets for the experiments and a small corpus -28,300 wordsdivided in a training corpus of 27,000 words and a text of 1,300 words for testing." ></td>
	<td class="line x" title="86:157	 second \[\] third M M* M+CG M*+CG Figure 2Number of tags per token." ></td>
	<td class="line x" title="87:157	The initial ambiguity of the training corpus is relatively high, as shown infig." ></td>
	<td class="line x" title="88:157	1, and the average number of tags per token is also higher than in other languages -see fig." ></td>
	<td class="line x" title="89:157	2." ></td>
	<td class="line x" title="90:157	The number of ambiguity classes is also high -290 and 1138 respectivelyand some of the classes in the test corpus aren't in the training corpus, specially in the 3rd level tagset." ></td>
	<td class="line x" title="91:157	This means that the training corpus doesn't cover all the phenomena of the language, so we would need a larger corpus to assure that it is general and representative of the language." ></td>
	<td class="line x" title="92:157	We tried both supervised and unsupervised 4 3 These measures are taken after the process denoted in each column: M--* morphological analysis; M* --, morphological analysis with enriched lexicon; CG --, Contraint Grammar." ></td>
	<td class="line x" title="93:157	4 Even if we used the same corpus for both training 382 training using the 2nd level tagset and only supervised training using the third level tagset." ></td>
	<td class="line x" title="94:157	The results are shown infig." ></td>
	<td class="line x" title="95:157	3(S)." ></td>
	<td class="line x" title="96:157	Accuracy is below 90% and 75% respectively." ></td>
	<td class="line x" title="97:157	Using unknown words to enrich the lexicon, the results are improved -seefig." ></td>
	<td class="line x" title="98:157	3(S*)-, but are still far from the accuracy of other systems." ></td>
	<td class="line x" title="99:157	We have also written some biases -to be exact 11to correct the most evident errors in the 2nd level." ></td>
	<td class="line x" title="100:157	We didn't write more biases for the following reasons:  They can use just the previous tag to change the probabilities, and in some cases we need a wider context to the left and/or to the right." ></td>
	<td class="line x" title="101:157	 They can't use the lemma or the word." ></td>
	<td class="line x" title="102:157	 From the beginning of this research, our intention was to combine this method with Constraint Grammar." ></td>
	<td class="line x" title="103:157	Using these biases, the error rate decreases by 5% in supervised training and by 7% in unsupervised one -fig." ></td>
	<td class="line x" title="104:157	3(S+ B)." ></td>
	<td class="line x" title="105:157	We also used biases 5 with the enriched lexicon and the accuracy increases by less than 2% in both experiments -fig." ></td>
	<td class="line x" title="106:157	3(S+B*)." ></td>
	<td class="line x" title="107:157	This is not a great improvement when trying to decrease an error rate greater than 10%, but the enrichment of the lexicon may be a good way to improve the system." ></td>
	<td class="line x" title="108:157	The logical conclusions of these experiments are:  the statistical approach might not be a good approach for agglutinative and free-order languages -as pointed out by Oflazer and KuruOz (1994)." ></td>
	<td class="line x" title="109:157	 writing good disambiguation rules may really improve the accuracy of the disambiguation task." ></td>
	<td class="line x" title="110:157	As we mentioned above, it is difficult to define accurate rules using stochastic models, so we use the Constraint Grammar for Basque 6 (Aduriz et al., 1997) for this purpose." ></td>
	<td class="line x" title="111:157	The morphological disambiguator uses around 800 constraint rules that discard illegitimate analyses on the basis of local or global context methods to compare the results, the latter performed better using a larger corpus." ></td>
	<td class="line x" title="112:157	These biases were written taking into account the errors made in the first experiment." ></td>
	<td class="line x" title="113:157	The rules were designed having syntactic analysis as the main goal." ></td>
	<td class="line x" title="114:157	conditions." ></td>
	<td class="line x" title="115:157	The application of CG formalism 7 is quite satisfactory, obtaining a recall of 99,8% but there are still 2.16 readings per token. The ambiguity rate after applying CG of Basque drop from 41% to 12% using 2nd level tagset and 64% to 22% using 3rd level tagset --fig." ></td>
	<td class="line x" title="116:157	2-and the error rate in terms of the tagsets is approximately 1%." ></td>
	<td class="line x" title="117:157	\[\] supervised \[\] unsupervised ~I third level 100 T'------ 97.5 __ . 929  .:ii 60  Figure 3Accuracy of the experiments 8." ></td>
	<td class="line x" title="118:157	5 Combining methods There have been some approaches to the combination of statistical and linguistic methods applied to POS disambiguation (Leech et al., 1994; Tapanainen and Voutilainen, 1994; Oflazer and TOt, 1997) to improve the accuracy of the systems." ></td>
	<td class="line x" title="119:157	Oflazer and Tt~r (1997) use simple statistical information and constraint rules." ></td>
	<td class="line x" title="120:157	They include a constraint application paradigm to make the disambiguation independent of the rule sequence." ></td>
	<td class="line x" title="121:157	The approach of Tapanainen and Voutilainen (1994) disambiguates the text using XT and ENGCG independently; then the ambiguities remaining in ENGCG are solved using the results of XT." ></td>
	<td class="line x" title="122:157	We propose a similar combination, applying both disambiguation methods one after the other, but training the stochastic tagger on the output of the CG disambiguator." ></td>
	<td class="line x" title="123:157	Since in the output of CG of Basque the avera7 These results were obtained using the CG-2 parser, which ,allows grouping the rules in different ordered subgrammars depending on their accuracy." ></td>
	<td class="line x" title="124:157	This morphological disam-biguator uses only the first two subgrammars." ></td>
	<td class="line x" title="125:157	8 S '~ stochastic; * -~ with enriched lexicon; B -~ with biases; CG -~ Constraint Granunar." ></td>
	<td class="line x" title="126:157	ge number of possible tags is still high -1.131.14 for 2nd level tagset and 1.29-1.3 for 3rd level tagsetand the stochastic tagger produces relatively high error rate -around 15% in 2nd level and almost 30% in 3rd level-, we first apply constraint rules and then train the stochastic tagger on the output of the rulebased disambiguator." ></td>
	<td class="line x" title="127:157	Fig." ></td>
	<td class="line x" title="128:157	I(CG) shows the ambiguity left by Basque CG in terms of the tagsets." ></td>
	<td class="line x" title="129:157	Although the ambiguity rate is significantly lower than in previous experiments, the remaining ambiguities are hard to solve even using all the linguistic information available." ></td>
	<td class="line x" title="130:157	We have also experimented with the enriched lexicon and the results are very encouraging, as shown infig.3(CG+S*)." ></td>
	<td class="line x" title="131:157	Considering that the number of ambiguity classes is still high -around 240 in the 2nd level and more than 1000 in the 3rd level-, we think that the results are very good." ></td>
	<td class="line x" title="132:157	For the 2nd level tagging, the error rate after combining both methods is less than 3.5%, half of it comes from MORFEUS and Basque CG and the rest is made by the stochastic disambiguation." ></td>
	<td class="line x" title="133:157	This is due to the fact that generally the types of ambiguity remaining after CG is applied are hard to solve." ></td>
	<td class="line x" title="134:157	Examining the errors, we find that half of them are made in unknown words trying to distinguish between proper names of persons and places." ></td>
	<td class="line x" title="135:157	We use two different tags because it is interesting for some applications and the tagset was defined based on morphological features." ></td>
	<td class="line x" title="136:157	This kind of ambiguity is very hard to solve and in some applications this distinction is not important." ></td>
	<td class="line x" title="137:157	So in this case the accuracy of the tagger would be 98%." ></td>
	<td class="line x" title="138:157	The accuracy in the third level tagset is around 91% using the combined method, which is not too bad bearing in mind the number of tags -310-, the precision of the input -1.29 tags/tokenand that the training corpus does not cover all the phenomena of the language 9." ></td>
	<td class="line x" title="139:157	We want to point out that the experiments with the 3rd level tagset show even clearer that the combined method performs much better than the stochastic." ></td>
	<td class="line x" title="140:157	Moreover, we think that CG disambiguation is even convenient at this level because &the initial ambiguity -63%." ></td>
	<td class="line x" title="141:157	9 in a corpus of,around 900,000 words we tbund 185 new tags and more than 1700 new classes." ></td>
	<td class="line x" title="142:157	383 Conclusion We have presented the results of applying different disambiguation methods to an agglutinative and highly inflected language with a relatively free order in sentences." ></td>
	<td class="line x" title="143:157	On one hand, this latter characteristic of Basque makes it difficult to learn appropriate probabilities, particularly first order stochastic models." ></td>
	<td class="line x" title="144:157	We solve this problem in part with CG for Basque, which uses a larger context and can tackle the free word-order problem." ></td>
	<td class="line x" title="145:157	However, it is a very hard work to write a full grammar and disambiguate texts completely using CG formalism, so we have complemented this method with a stochastic disambiguation process and the results are quite encouraging." ></td>
	<td class="line x" title="146:157	Comparing the results of Tapanainen and Voutilainen (1994) with ours, we see that they achieve 98.5% recall combining 1.02-1.04 readings from ENGCG and 96% accuracy in XT, while we begin with 1.13-1.14 readings, the quality of our stochastic tagger is less than 90% and our result is better than 96%." ></td>
	<td class="line x" title="147:157	Unlike Tapanainen and Voutilainen (1994), we think that training on the output of the CG the statistical disambiguation works quite better 1, at least using such a small training corpus." ></td>
	<td class="line x" title="148:157	In the future we will compile a larger corpus and to decrease the number of readings left by CG On the other hand, we think that the information given by the second level tag is not sufficient to decide which of the choices is the correct one, but the training corpus is quite small." ></td>
	<td class="line x" title="149:157	However, translating the results of the 3rd level to the 2rid one we obtain around 97% of accuracy." ></td>
	<td class="line x" title="150:157	So, we think that improving the 3rd level tagging would improve the 2nd level tagging too." ></td>
	<td class="line x" title="151:157	We also want to experiment unsupervised learning in the 3rd level tagging with a large training corpus." ></td>
	<td class="line x" title="152:157	Along with this, the future research will focus on the following processes:  morphosyntactic treatment for the elaboration of morphological information (nominalisation, ellipsis, etc.)." ></td>
	<td class="line x" title="153:157	 treatment of multiword lexical units (MWLU)." ></td>
	<td class="line x" title="154:157	We are planning to integrate this module to process unambiguous MWLU, to decreases the ambiguity rate and to make the input of the disambiguation more precise." ></td>
	<td class="line x" title="155:157	10 With their method accuracy is 2% lower." ></td>
	<td class="line x" title="156:157	Acknowledgement We are in debt with the research-team of the General Linguistics Department of the University of Helsinki for giving us permission to use CG Parser." ></td>
	<td class="line x" title="157:157	We also want to thank Gilbert Robert for tuning TATOO." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W98-1110
Generalized Unknown Morpheme Guessing For Hybrid POS Tagging Of Korean
Cha, Jeongwon;Lee, Gary Geunbae;Lee, Jong-Hyeok;"></td>
	<td class="line x" title="1:167	Generalized unknown morpheme guessing for hybrid POS tagging of Korean* Jeongwon Cha and Geunbae Lee and Jong-Hyeok Lee Department of Computer Science & Engineering Pohang University of Science & Technology Pohang, Korea {himen, gblee, jhlee}@postech.ac.kr Abstract Most of errors in Korean morphological analysis and POS (Part-of-Speech) tagging are caused by unknown morphemes." ></td>
	<td class="line x" title="2:167	This paper presents a generalized unknown morpheme handling method with P OSTAG (POStech TAGger) which is a statistical/rule based hybrid POS tagging system." ></td>
	<td class="line x" title="3:167	The generalized unknown morpheme guessing is based on a combination of a morpheme pattern dictionary which encodes general lexical patterns of Korean morphemes with a posteriori syllable tri-gram estimation." ></td>
	<td class="line x" title="4:167	The syllable tri-grams help to calculate lexical probabilities of the unknown morphemes and are utilized to search the best tagging result." ></td>
	<td class="line x" title="5:167	In our scheme, we can guess the POS's of unknown morphemes regardless of their numbers and positions in an eojeol, which was not possible before in Korean tagging systems." ></td>
	<td class="line x" title="6:167	In a series of experiments using three different domain corpora, we can achieve 97% tagging accuracy regardless of many unknown morphemes in test corpora." ></td>
	<td class="line x" title="7:167	1 Introduction Part-of-speech (POS) tagging has many difficult problems to attack such as insufficient training data, inherent POS ambiguities: and most seriously unknown words." ></td>
	<td class="line x" title="8:167	Unknown words are ubiquitous in any application and cause major tagging failures in many cases." ></td>
	<td class="line x" title="9:167	Since Korean is an agglutinative language, we have unknown morpheme problems instead of unknown words in our POS tagging." ></td>
	<td class="line x" title="10:167	The usual way of unknown-morpheme handling before was to guess possible POS's for an unknown-morpheme by checking connectable ' This project was supported by KOSEF (teukjeongkicho #970-1020-301-3, 1997)." ></td>
	<td class="line x" title="11:167	functional morphemes in the same eojeol l (Kang, 1993)." ></td>
	<td class="line x" title="12:167	In this way, they could guess possible POS's for a single unknown-morpheme only when it is positioned in the begining of an eojeol." ></td>
	<td class="line x" title="13:167	If an eojeol contains more than one unknown-morphemes or if unknown-morphemes appear other than the first position, all the previous methods cannot efficiently estimate them." ></td>
	<td class="line x" title="14:167	sO, we propose a morpheme-pattern dictionary which enables us to treat unknownmorphemes in the same way as registered known morphemes, and thereby to guess them regardless of their numbers and positions in an eojeol." ></td>
	<td class="line x" title="15:167	The unknown-morpheme handling using the morpheme-pattern dictionary is integrated into a hybrid POS disambiguation." ></td>
	<td class="line oc" title="16:167	The POS disambiguation has usually been performed by statistical approaches mainly using hidden markov model (HMM) (Cutting et al. , 1992; Kupiec." ></td>
	<td class="line x" title="17:167	1992; Weischedel et al. , 1993)." ></td>
	<td class="line n" title="18:167	However." ></td>
	<td class="line n" title="19:167	since statistical approaches take into account neighboring tags only within a limited window (usually two or three), sometimes the decision cannot cover all linguistic contexts necessary for POS disambiguation." ></td>
	<td class="line n" title="20:167	Also the approaches are inappropriate for idiomatic expressions for which lexical terms need to be directly referenced." ></td>
	<td class="line n" title="21:167	The statistical approaches are not enough especially for agglutinative languages (such as Korean) which have usually complex morphological structures." ></td>
	<td class="line x" title="22:167	In agglutinative languages, a word (called eojeol in Korean) usually consists of separable single stem-morpheme plus one or more functional morphemes, and the POS tag should be assigned to each morpheme to cope with the complex morphological phenomena." ></td>
	<td class="line x" title="23:167	Recently, rule-based approaches are tAn eojeol is a Korean spacing unit(similar to English word) which usually consists of one or more stem morphemes and functional morphemes." ></td>
	<td class="line n" title="24:167	85 re-studied to overcome the limitations of staffstical approaches by learning symbolic tagging rules automatically from a corpus (Brill, 1992; Bril!." ></td>
	<td class="line x" title="25:167	1994)." ></td>
	<td class="line x" title="26:167	Some systems even perform the POS tagging as part of a syntactic analysis process (Voutilainen, 1995)." ></td>
	<td class="line x" title="27:167	However, rule-based approaches alone, in general, are not very robust, and not portable enough to be adjusted to new tag sets and new languages." ></td>
	<td class="line x" title="28:167	Also the." ></td>
	<td class="line x" title="29:167	performance is usually no better than the statistical counterparts (Brill, 1992)." ></td>
	<td class="line n" title="30:167	To gain the portability and robustness and also to overcome the limited coverage of statistical approaches, we adopt a hybrid method that can combine both statistical and rule-based approaches for POS disambiguation." ></td>
	<td class="line x" title="31:167	2 Linguistic characteristics of Korean Korean is classified as an agglutinative language in which ~.n eojeol consists of several number of morphemes that have clear-cut morpheme boundaries." ></td>
	<td class="line x" title="32:167	For examples." ></td>
	<td class="line x" title="33:167	'Q~ ~'~Tl~l ~t r-l-(I caught a cold)' consists of 3 eojeols and 7 morpheme, such as2 Q(I)/T + ~(au:~iliary particle)/jS, %1-71(cold)/MC + oil(other particle)/jO, ~ e.l(catch)/DR + N(past tense)/eGS + c\]-(final ending)/eGE." ></td>
	<td class="line x" title="34:167	Below are the characteristics of Korean that must be considered for morphological-level natural language processing and POS tagging." ></td>
	<td class="line x" title="35:167	As an agglutinative language, Korean POS tagging is usually performed on a morpheme basis rather than an eojeol basis." ></td>
	<td class="line x" title="36:167	So, morphological analysis is essential to POS tagging because morpheme segmentation is much more important and difficult than POS assignment." ></td>
	<td class="line x" title="37:167	Moreover: morphological analysis should segment out unknown morphemes as well as known morphemes, so unknown morpheme handling should be integrated into the morphological analysis process." ></td>
	<td class="line x" title="38:167	There are three possible analyses fl'om the eojeol 'Q~' : 'Q(I)/T' + ' ~(subject-marker)/jS', ~ ut-(sprout)/DR' + '~(adnominal)/eCNMG', ?~(fly)/DI' + '~(adnominal)/eCNMG', so morpheme '~Here, '+' is a morpheme boundary in an eojeol and '/' is for the POS tag symbols (see Fig." ></td>
	<td class="line x" title="39:167	1)." ></td>
	<td class="line x" title="40:167	segmentation is often ambiguous." ></td>
	<td class="line x" title="41:167	 Korean is a postpositional language with many kind of noun-endings (particles), verb-endings (other endings), and prefinal verb-endings (prefinal endings)." ></td>
	<td class="line x" title="42:167	It is these functional morphemes, rather than eojeol's order, which determine most of the grammatical relations such as noun's syntactic flmctions, verb's tense, aspect, modals, and even modi~ing relations between eojeots." ></td>
	<td class="line x" title="43:167	For example." ></td>
	<td class="line x" title="44:167	~/jS' is an atuxiliary particle, so eojeol ~'G-~-' has a subject role due to the particle :~/jS'." ></td>
	<td class="line x" title="45:167	 Complex spelling changes frequently occur between morphemes when two morphemes combine to form an eojeol." ></td>
	<td class="line x" title="46:167	These spelling changes make it difficult to segment the original morphemes out before assigning the POS tag symbols." ></td>
	<td class="line x" title="47:167	Fig." ></td>
	<td class="line x" title="48:167	1 shows a tag set extracted from 100 full POS tag hierarchies in Korean." ></td>
	<td class="line x" title="49:167	This tag set will be used in our experiments in section 6." ></td>
	<td class="line x" title="50:167	3 Unknown morpheme guessing during morphological analysis Morphological analysis is a basic step to natural language processing which segments input texts into morphotactically connectable morphemes and assigns all possible POS tags to each morpheme by looking up a morpheme dictionary." ></td>
	<td class="line x" title="51:167	Our morphological analysis follows general three steps (Sproat, 1992): morpheme segmentation, original morpheme recovery from spelling changes, and morphotactics modeling." ></td>
	<td class="line x" title="52:167	Input texts are scanned from left to right., character3by character, to be matched to morphemes in a morpheme dictionary." ></td>
	<td class="line x" title="54:167	The morpheme dictionary (Fig." ></td>
	<td class="line x" title="55:167	2) has a separate entry for each variant form (called allomorph) of the original morpheme form so we can easily reconstruct the original inorphemes from spelling changes." ></td>
	<td class="line x" title="56:167	For morphotactics modeling, we used the POS tags and the morphotactic adjacency symbols in the dictionary." ></td>
	<td class="line x" title="57:167	The full hierarchy of POS tags and morphotactic adjacency symbols are encoded in the morpheme dictionary for each mor3The character sequence in '~' is 'u.', ' ~ ', ~-', 86 tag descrip t ion' tag MC MPP T B DI I js eGS eCNMM eCC + SO S. sf common noun place name pronoun adverb irregular verb i-predicative particle auxiliary particle description tag prefinal ending nominal ending conjunctive ending prefix other symbol sentence closer foreign word MPN person name MPO other proper noun G adnoun K interjection HR regular adjective E existential predicate jO other particle eCNDI ~aux conj ending eCNMG adnominal ending y predicative particle snfllx s' left parenthesis s\] sentence connection sh i Chinese character MPC MD S DR. HI jC eGE eCNDC eCNB b Su S ~ S, description country name bound noun numeral regular verb irregular adjective case particle final ending quote conj ending adverbial ending auxiliary verb unit symbol right parenthesis sentence comma Figure 1: A tag set with 41 tags from 100 full hierarchical POS tag symbols P0S-tag<0r~ginal form> (allomorph) \[morphotactic adjacency symbols\] MCC< 7\]-.~> (71---~) \[@>D aI->H e}> DN >\] MCK< ~ -~-> (~ @) \[-8->D 8l->\] DIS ~< ~L-t 71-> (~t--t 7\]-) \[-~->-~ %~>\] DI u < ~oI-.~> ( ~ o\]~-) \[e>\] DI=<~oF~-> (o~ol-R-) \[R->~>\] DI*<~ ~> (~) \[R->':q >\] ~> HI e < 71--~'> (71-~) \[-~ \] HI e < 7l-'~> (7}-~) \[@>q >\] Figure 2: Morpheme dictionary pheme." ></td>
	<td class="line x" title="58:167	To model the morpheme's connectability to one another, besides the morpheme dictionary, the separate morpheme-connectivity table encodes all the connectable pairs of morpheme groups using the morpheme's tag and morphotactic adjacency symbol patterns." ></td>
	<td class="line x" title="59:167	After an input eojeol is segmented by trie indexed dictionary search, the morphological analysis checks if each segmentation is grammatically connectable by looking into the morpheme-connectivity table." ></td>
	<td class="line x" title="60:167	For unknown morpheme guessing, we develop a general unknown morpheme estimation method for number-free and position-free unknown morpheme handling." ></td>
	<td class="line x" title="61:167	Using a morpheme pattern dictionary, we can look up unknown morphemes in the dictionary exactly same way as we do the registered morphemes." ></td>
	<td class="line x" title="62:167	And when morphemes are checked if they are connectable, we can use the iEformation of the adjacent morphemes in the same eojeol." ></td>
	<td class="line x" title="63:167	The basic idea of the morpheme-pattern dictionary is to collect all the possible general lexical patterns of Korean morphemes and encode each lexical syllable pattern with all the candidate POS tags." ></td>
	<td class="line x" title="64:167	So we can assign initial POS tags to each unknown morpheme by only matching the syllable patterns in the pattern dictionary." ></td>
	<td class="line x" title="65:167	In this way, we don't need a special rule-based unknown morpheme handling module in our morphological analyzer, and all the possible POS tags for unknown morphemes can be assigned just like the registered morphemes." ></td>
	<td class="line x" title="66:167	This method can guess the POS of each and every unknown morpheme, if more than one unknown morphemes are in an eojeol, regardless of their positions since the morpheme segmentation is applied to both the unknown morphemes and the registered morphemes dur87 ing the trie indexed dictionary search." ></td>
	<td class="line x" title="67:167	3.1 Morpheme pattern dictionary The morpheme pattern dictionary covers all necessary syllable patterns for unknown morphemes including common nouns, propernouns, adnominals, adverbs, regular and irregular verbs, regular and irregular adjectives, and special symbols for foreign words." ></td>
	<td class="line x" title="68:167	The lexical." ></td>
	<td class="line x" title="69:167	patterns for morphemes are collected from the previous studies (Kang, 1993) where the constraints of Korean syllable patterns as to the morpheme connectabilities are well described." ></td>
	<td class="line x" title="70:167	Fig." ></td>
	<td class="line x" title="71:167	3 shows some example entries of the morpheme pattern dictionary, where ;Z', ;V', ;*' are meta characters which indicate a consonant, a vowel, and any number of Korean characters respectively." ></td>
	<td class="line x" title="72:167	For example, ~L7_-l--g\]' (thanks), which is a morpheme and an eojeol at the same time, is matched '(ZV*N)' (shown in Fig." ></td>
	<td class="line x" title="73:167	3) in the morpheme pattern dictionary, and is recovered into the original morpheme form '2_~'." ></td>
	<td class="line x" title="74:167	4 A hybrid tagging model sentence i ~ .: l  ~: lictloOary ::: morphologiea I !!i  ~' ~mo, p.ho~ = \] i.o~,iCth,~,.'l  .,." ></td>
	<td class="line x" title="75:167	ili )-." ></td>
	<td class="line x" title="76:167	':.i tta~t:::.)t i analyzer i'i ~." ></td>
	<td class="line x" title="77:167	'~::'::':, tram, re .>-:4  t e'~'~': ~J . ~ .:;;:::::Tir~ii~/i. :~~:iil ~'~6ii=/iinn|i~?'J  ~ statistical,~!" ></td>
	<td class="line x" title="78:167	' ~':''=';~::1 L Pstagger i?,~ r : Ir (i! .:=,~','~t1~ ':.::.\]  ~::?i: ~~ :-:.::':J post errorlJ corrector ~!" ></td>
	<td class="line x" title="79:167	Figure 4: Statistical and rule-based hybrid architecture for Korean POS tagging." ></td>
	<td class="line x" title="80:167	Fig." ></td>
	<td class="line x" title="81:167	4 shows a proposed hybrid architecture for Korean POS tagging with generalized unknownmorpheme guessing." ></td>
	<td class="line x" title="82:167	There are three major components: the morphological analyzer with unknown-morpheme handler, the statistical tagger, and the rule-based error corrector." ></td>
	<td class="line x" title="83:167	The morphological analyzer segments the morphemes out of eojeols in a sentence and reconstructs the original morphemes from spelling changes from irregular conjugations." ></td>
	<td class="line x" title="84:167	It also assigns all possible POS tags to each morpheme by consulting a morpheme dictionary." ></td>
	<td class="line x" title="85:167	The unknown-morpheme handler integrated into the morphological analyzer assigns the POS's of the morphemes which are not registered in tim dictionary." ></td>
	<td class="line x" title="86:167	The statistical tagger runs the Viterbi algorithm (Forney, 1973) on the morpheme graph for searching the optimal tag sequence for POS disambiguation." ></td>
	<td class="line x" title="87:167	For remeding the defects of a statistical tagger, we introduce a post errorcorrection mechanism." ></td>
	<td class="line x" title="88:167	The error-corrector is a rule-based transformer (Brill, 1992), and it corrects the mis-tagged morphemes by considering the lexical patterns and the necessary contextual information." ></td>
	<td class="line x" title="89:167	4.1 Statistical POS tagger Statistical tagging model has the morpheme graph as input and selects the best morpheme and POS tag sequence r for sentences represented ill the graph." ></td>
	<td class="line x" title="90:167	The morpheme-graph is a compact way of representing nmltiple morpheme sequences for a sentence." ></td>
	<td class="line x" title="91:167	We put each morpheme with the tag as a node and the morpheme connectivity as a link." ></td>
	<td class="line oc" title="92:167	Our statistical tagging model is adjusted from standard bi-grams using the Viterbi-search (Cutting et al. , 1992) plus on-the-fly extra computing of lexical probabilities for unknown morphemes." ></td>
	<td class="line x" title="93:167	The equation of statistical tagging model used is a modified hi-gram model with left to right search: n )3Pr(tilrni) T  = argmaz'T I~ aPr(tiIti-t Pr(ti) i=1 (1) 4A Korean eojeol can be segmented into many different ways, so selecting the best morpheme segmentation sequence is as important as selecting the best POS sequence in Korean POS tagging." ></td>
	<td class="line x" title="94:167	88 POS-tag<original form> (allomorph) \[morphotactic adjacency symbols\] HI~<ZV*~> (ZV*~) \[~>o1>\] HI e <ZV* ~> (ZV* 71-) \[~>1 HI ~ <ZV*ZV n > (ZV*q-) \[~>\] HI ~ <ZV*ZV ~a > (ZV**.\]) \[@ %'>\] HI <ZV*ZV > (ZV* %,>\] DIm<ZV* > (ZV* t) DI <ZV* > (ZV*N) DI= (ZV*G)." ></td>
	<td class="line x" title="95:167	DI=<ZV*~-> (ZV*~) \[-~>,:q >\] Figure 3: Morpheme where T' is an optimal tag sequence that maximizes the forward Viterbi scores." ></td>
	<td class="line x" title="96:167	Pr(tilti-1) is a bi-gram tag transition probability and Pr( ti lrni ) PT(td is a modified morpheme lexical probability." ></td>
	<td class="line x" title="97:167	This equation is finally selected from the extensive experiments using the following six different equations: /x T' = argmazr ~I Pr(tilti-:)Pr(miltd (2) i=1 T' = argrnaxr rI LPr(tilti-t)~3Pr(rnil ti) (3) i=1 I,l T' = argrnaxr I~ Pr(tilti-1)Pr(tdrni) (4) i=1 n T' = argrnaxr ~I ~Pr(tilti-:)13Pr(tilrni) (5)  i=1 n pr(tilrni) T' = argmaxT rI Pr(tilti-l) Pr(ti) (6) i=l,,.2.,,,Pr(tilmi) T  = argrnaxT li otz-'r(ti ti-t)p ~ (7) i=1 In the experiments, we used 10204 morpheme training corpus from :'Kemong Encyclopedia 5,." ></td>
	<td class="line x" title="99:167	Table 1 shows the tagging performance of each equation." ></td>
	<td class="line x" title="100:167	Training of the statistical tagging model requires parameter estimation process for two parameters, that is, morpheme lexical probabilities and bi-gram tag transition probabilities." ></td>
	<td class="line x" title="101:167	Several studies show that using as much as tagged corpora for training gives much better Sprovided from ETRI pattern dictionary performance than unsupervised training using Baum-Welch algorithm (blerialdo, 1994)." ></td>
	<td class="line x" title="102:167	So we decided to use supervised training using tagged corpora with relative frequency counts." ></td>
	<td class="line x" title="103:167	The three necessary probabilities can be estimated as follows: Pr(tilmi) .~ f(tilmi) = N(mi, ti) (8) N(t ) Pr(ti) ~ f(ti) = 41 (9) Pr(tilti-1) .~ f(tilti-t) = N(ti-:, ti) N(ti-t) (10) where N(mi, ti) indicates the total number of occurrences of morpheme .mi together with specific tag ti, while N(mi) shows the total number .of occurrences of morpheme rni in the tagged training corpus." ></td>
	<td class="line x" title="104:167	The N(ti_l,ti) and N(ti-1) can be interpreted similarly for two consecutive tags ti-1 and ti." ></td>
	<td class="line x" title="105:167	4.2 Lexlcal probability estimation for unknown morpheme guessing The lexical probabilities for unknown morphemes cannot be pre-calculated using the equation (8), so a special method should be applied." ></td>
	<td class="line x" title="106:167	We suggest to use syllable tri-grams since Korean syllables can duly play important roles as restricting units for guessing POS of a Pr(tilmi) morpheme." ></td>
	<td class="line x" title="107:167	So the lexical probability e,-(td for unknown morphemes can be estimated using the frequency of syllable tri-gram products according to the following formula: m = ele2e~ (11) 89 equation 2 equation 3 eojeol 86.80 90.48 morpheme 91.32 94.93 eq:uation 4 89.40 94.40 equation 5 equation 6 89.62 91.73 94.48 95.77 equation 7(equation l) 92.48 96.12 Table 1: Tagging performance of each equation." ></td>
	<td class="line x" title="108:167	The a and,3 are weights, and we set a = 0.4 and = 0.6." ></td>
	<td class="line x" title="109:167	The eojeol shows eojeol-unit tagging correctness while morpheme shows morpheme-unit correctness." ></td>
	<td class="line x" title="110:167	Pr(tlm) Pr(t) ~ Prt(ell#'#)Prt(e21#'el) n \[I Prt(eilei-2, el-t) i=3 Pr(#1e,,-t, en) (12) Prt ( eilei-2, ei-t ) .~ .ft(eilei-2, ei-1) +.h(eilei) + fft(e) (13) where ~m' is a morpheme, 'e' is a syllable, 't' is a POS tag, '#' is a morpheme boundary symbol, and ft(eilei-~., el-l) is a frequency data for tag ~t' with cooccurrence syllables el-2, ei-1, ei." ></td>
	<td class="line x" title="112:167	A tri-gram probabilities are smoothed by." ></td>
	<td class="line x" title="113:167	equation (13) to cope with the sparse-data problem." ></td>
	<td class="line x" title="114:167	For example, '~=1-'." ></td>
	<td class="line x" title="115:167	o,_ is a name of a person, so is an unknown morpheme." ></td>
	<td class="line x" title="116:167	The lexical probability of '~'~'' -, o,~ as tag MPN is estimated using the formula: Pr(AfPN) Pr.wpN(~rl#, #) xPrMpN('~I#, ~) x Pr,~teN(~l~ I', ~) x PrMpN (#1 All tri-grams for Korean syllables were precalculated and stored in the table, and are applied with the candidate tags during the unknown morpheme POS guessing and smoothing." ></td>
	<td class="line x" title="117:167	5 A posteriori error correction rules The statistical morpheme tagging covers only the limited range of contextual information." ></td>
	<td class="line x" title="118:167	Moreover, it cannot refer to tile lexical patterns as a context for POS disambiguation." ></td>
	<td class="line x" title="119:167	As mentioned before, Korean eojeol has very complex morphological structure so it is necessary to look at the functional morphemes selectively to get the grammatical relations between eojeols." ></td>
	<td class="line x" title="120:167	For these reasons, we designed errorcorrecting rules for eojeols to compensate estimation and modeling errors of the statistical morpheme tagging." ></td>
	<td class="line x" title="121:167	However, designing the error-correction rules with knowledge engineering is tedious and error-prone." ></td>
	<td class="line x" title="122:167	Instead, we adopted Brill's approach (Brill, 1992) to auto." ></td>
	<td class="line x" title="123:167	matically learn the error-correcting rules from small amount of tagged corpus." ></td>
	<td class="line x" title="124:167	Fortunately, Brill showed that we don't need a large amount of tagged corpus to extract the symbolic tagging rules compared with the case in tile statistical tagging." ></td>
	<td class="line x" title="125:167	Table 2 shows some rule schemata we used to extract the error-correcting rules: where a rule schema designates the context of rule applications, i.e the morpheme position and the lexical/tag decision in the context eojeol." ></td>
	<td class="line x" title="126:167	The rules which can be automatically learned using table 2's schemata are in the form of table 3, where \[current eojeol or morpheme\] consists of morpheme (with current tag) sequence in the eojeol, and \[corrected eojeol or morpheme\] consists of morpheme (with corrected tag) sequence ill the same eojeol." ></td>
	<td class="line x" title="127:167	For example, the rule \[N (Chinese ink)/MC + ~-/jS\]\[N1FT, MC\] --~ \[N(to eat)/DR + ~-/eCNMG\] says that the current eojeol was statistically tagged as common-noun (MC) plus auxiliary particle (iS), but when the next first eojeol's (N1) first position morpheme tag (FT) is another common-noun (MC), the eojeol should be tagged as regular verb (DR) plus adnominal ending (eCNMG)." ></td>
	<td class="line x" title="128:167	This statistical error is caused from the ambiguity of the morpheme 'N' which has two meanings as 'Chinese ink:' (noun) and 'to eat' (verb)." ></td>
	<td class="line x" title="129:167	Since the morpheme segmentation is very difficult ill Korean, many of the tagging errors also come from the morpheme segmentation errors." ></td>
	<td class="line x" title="130:167	Our errorcorrecting rules call cope with these morpheme 90 rule schema N1FT P1LT N2FT N3FT P1LM P1FM N1FM description next first eojeol (N1) first morpheme's tag (FT) previous first eojeol (P1) last morpheme's tag (LT) next second eojeol (N2) first morpheme's tag (FT) next third eojeol (N3) first morpheme's tag (FT) previous first eojeol (P1) last morpheme's lexical form (LM) previous first eojeol (P1) first morpheme's lexical form (FM) next first eojeol (N1) first morpheme's lexical form (FM) Table 2: Some rule schemata to extract the error-correcting rules automatically from the tagged corpus." ></td>
	<td class="line x" title="131:167	POSTAG has about 24 rule schemata in this form." ></td>
	<td class="line x" title="132:167	\[current eojeol or morpheme\] \[rule schemata, referenced morpheme or tag\] \[ \[corrected eojeol or morpheme\] I Table 3: Error correction rule format segmentation errors by correcting the errors in the whole eojeol together." ></td>
	<td class="line x" title="133:167	For example, the following rule can correct morpheme segmentation errors: \[~/MC 4ol.7_./jO\]\[P1LM, @\] -~ \[~ o\]/DR + _7-~eCCl." ></td>
	<td class="line x" title="134:167	This rule says that the eojeol '@old' is usually segmented as common-noun '~' (meaning string or rope) plus other-particle 'o\]v_.', but when the morpheme '~' appears before the eojeol, it should be segmented as regular-verb,;@o\] ' (meaning shrink) plus conjunctive-ending ':2.'." ></td>
	<td class="line x" title="135:167	This kind of segmentation-error correction can greatly enhance the tagging performance in Korean." ></td>
	<td class="line x" title="136:167	The rules are automatically learned by Comparing the correctly tagged corpus with the outputs of the statistical tagger." ></td>
	<td class="line x" title="137:167	The training is leveraged (Brill, 1992) so the error-correcting rules are gradually learned as the statistical tagged texts are corrected by the rules learned so far." ></td>
	<td class="line x" title="138:167	6 Experiment results For morphological analysis and POS tagging experiments, we used 130000 morpheme-balanced training corpus for statistical parameter estimation and 50000 morpheme corpus for learning the post error-correction rules." ></td>
	<td class="line x" title="139:167	These training corpora were collected from various sources such as internet documents, encyclopedia, newspapers, and school textbooks." ></td>
	<td class="line x" title="140:167	For the test set, we carefully selected three different document sets aiming for a broad coverage." ></td>
	<td class="line x" title="141:167	The document set 1 (25299 morphemes; 1338 sentences) is collected from :'Kemong encyclopedia 6,, hotel reservation dialog corpus 7 and internet document, and contains 10% of unknown morphemes." ></td>
	<td class="line x" title="142:167	The documents set 2 (15250 morphemes; 5774 sentences) is solely collected from various internet documents from assorted domains such as broadcasting scripts and newspapers, and has about 8.5% of unknown morphemes." ></td>
	<td class="line x" title="143:167	The document set 3 (20919 morphemes; 555 sentence) is from Korean standard document collection set called KTSET 2.0 s and contains academic articles and electronic newspapers." ></td>
	<td class="line x" title="144:167	This document set contains about 1470 unknown morphemes (mainly technical jargons)." ></td>
	<td class="line x" title="145:167	Table 4 showsour taggingperformance for these three document sets." ></td>
	<td class="line x" title="146:167	This experiment shows efficiency of our unknown morpheme handling and guessing techniques since we can confirm the sharp performance drops between tagger-a and tagger-b." ></td>
	<td class="line x" title="147:167	The post error correction rules are also proved to be effective by the performance drops between the full tagger and taggera, but the drop rates are mild due to the performance saturation at tagger-a, which means that our statistical tagging alone already achieves state-of-the-art performance for Korean morpheme tagging." ></td>
	<td class="line x" title="148:167	6from ETRI :from Sogang University, Seoul." ></td>
	<td class="line x" title="149:167	Korea Sfrom KT(Korea Telecom) 91 document set full tag~ier tagger-a tagger-b tagger-c set 1 97.2 96.4 89.5 87.1 set 2 96.9 96.0 92.8 89.0 set 3 97.4 96.7 88.7 84.8 total 97.2 96.4 90.3 I 87.0 Table 4: Tagging and unknown morpheme guessing performance (all in %)." ></td>
	<td class="line x" title="150:167	Experiments are performed on three different document sets as e~xplained in the text." ></td>
	<td class="line x" title="151:167	The full tagger designates our POS tagger with all the morphological processing capabilities." ></td>
	<td class="line x" title="152:167	The tagger-a is a version without employing post error-correction rules." ></td>
	<td class="line x" title="153:167	The tagger-b is a more degraded version which does not utilize our unknown morpheme guessing capability but treats all unknown morphemes as nouns." ></td>
	<td class="line x" title="154:167	The tagger-c is an even more deteriorated version which rejects all unknown morphemes as tagging failures." ></td>
	<td class="line x" title="155:167	The performance drops as we degrade the version from the full tagger." ></td>
	<td class="line x" title="156:167	7 Conclusion and future works This paper presents a pattern-dictionary based unknown-morpheme guessing method for a statistical/rule-based hybrid tagging system which itself exhibits many novel ideas of POS tagging such as experiment-based new statistical model for Korean, rule based error correction and hierarchically expandable tag sets." ></td>
	<td class="line x" title="157:167	The system POSTAG was developed to test." ></td>
	<td class="line x" title="158:167	these novel ideas especially for agglutinative languages such as Korean." ></td>
	<td class="line x" title="159:167	Japanese is also similar to Korean in linguistic characteristics and will be a good target of these ideas." ></td>
	<td class="line x" title="160:167	POSTAG integrates morphological analysis with generalized unknown-morpheme handling so that unknowmmorpheme can be processed in the same manner as registered morphemes using morpheme pattern dictionary." ></td>
	<td class="line x" title="161:167	POSTAG adopted a hybrid approach by cascading statistical tagging to rulebased error-correction." ></td>
	<td class="line x" title="162:167	Cascaded training was implemented to selectively learn statistical tagging error-correction rules by Brill style transformation approach." ></td>
	<td class="line x" title="163:167	POSTAG also employs hierarchical tag sets that are flexible enough to expand/shrink according to the given applications." ></td>
	<td class="line x" title="164:167	The hierarchical tag sets can be mapped to any other existing tag set as long as they axe decently classified, and therefore can encoverage a corpus sharing in Korean tagging community." ></td>
	<td class="line x" title="165:167	POSTAG is constantly being improved by expanding the morpheme dictionary, pattern-dictionary, and tagged corpus for statistical training and rule learning." ></td>
	<td class="line x" title="166:167	Since generalized unknown-morpheme handing is integrated into the system, POSTAG is a good tagger for open domain applications such as internet indexing, filtering, and summarization., and we are now developing a web indexer using the POSTAG technology." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W98-1207
Automation Of Treebank Annotation
Brants, Thorsten;Skut, Wojciech;"></td>
	<td class="line x" title="1:234	I I B i / / / / / / B | / / B B B Automation of Treebank Annotation Thorsten Brants and Wojciech Skut Universitt des Saarlandes Computational Linguistics D-66041 Saarbrficken, Germany {brant s, skut }Qcoli." ></td>
	<td class="line x" title="2:234	uni-sb, de  Abstract This paper describes applications of stochastic and symbolic NLP methods to treebank annotation." ></td>
	<td class="line x" title="3:234	In paxticular we focus on (1) the automation of treebank annotation, (2) the comparison of conflicting annotations for the same sentence and (3) the automatic detection of inconsistencies." ></td>
	<td class="line x" title="4:234	These techniques are currently employed for building a German treebank." ></td>
	<td class="line x" title="5:234	1 Introduction The emergence of new statistical NLP methods increases the demand for corpora annotated with syntactic structures." ></td>
	<td class="line x" title="6:234	The construction of such a corpus (a treebank) is a time-consuming task that can hardly be carried out unless some annotation work is automated." ></td>
	<td class="line x" title="7:234	Purely automatic annotation, however, is not reliable enough to be employed without some form of human supervision and hand-correction." ></td>
	<td class="line x" title="8:234	This interactive annotation strategy requires tools for error detection and consistency checking." ></td>
	<td class="line x" title="9:234	The present paper reviews our experience with the development of automatic annotation tools which are currently used for building a corpus of German newspaper text." ></td>
	<td class="line x" title="10:234	The next section gives an overview of the annotation format." ></td>
	<td class="line x" title="11:234	Section 3 describes three applications of statistical NLP methods to treebank annotation." ></td>
	<td class="line x" title="12:234	Finally, section 4 discusses mechanisms for comparing structures assigned by different annotators." ></td>
	<td class="line x" title="13:234	2 Annotating Argument Structure 2.1 Annotation Scheme Unlike most treebanks of English, our corpus is annotated with predicate-argumenl s~ructures and not phrase-structure trees." ></td>
	<td class="line x" title="14:234	The reason is the free word order in German, a feature seriously affecting the transparency of traditional phrase structures." ></td>
	<td class="line x" title="15:234	Thus local and non-local dependencies are represented in Dar'uber PROAV rnu's nachgedacht werden VMFIN VVPP VAINF must thought-over be 'it has to be thought over' about-it $." ></td>
	<td class="line x" title="16:234	Figure h Sample structure from the Treebank the same way, at the cost of allowing crossing tree branches, as shown in figure 11." ></td>
	<td class="line x" title="17:234	Such a direct representation of the predicateargument relation makes annotation easier than it would be if additional trace-filler co-references were used for encoding discontinuous constituents." ></td>
	<td class="line x" title="18:234	Furthermore, our scheme facilitates automatic extraction of valence frames and the construction of semantic representations." ></td>
	<td class="line x" title="19:234	On the other hand, the predicate-argument structures used for annotating our corpus can still be converted automatically into phrase-structure trees if necessary, cf.(Skut et al. , 1997a)." ></td>
	<td class="line x" title="21:234	For more details on the annotation scheme v." ></td>
	<td class="line x" title="22:234	(Skut et al. , 1997b)." ></td>
	<td class="line x" title="23:234	2.2 The Annotation Mode In order to make annotation more reliable, each sentence is annotated independently by two annotators." ></td>
	<td class="line x" title="24:234	Afterwards, the results are compared, and both annotators have to agree on a unique structure." ></td>
	<td class="line x" title="25:234	In 1 The nodes and edges are labeled with category and function symbols, respectively (see appendix A)." ></td>
	<td class="line x" title="26:234	Brants and Skut 49 Automation of Treebank Annotation Thorsten Brants and Wojciech Skut (1998) Automation of Treebank Annotation." ></td>
	<td class="line x" title="27:234	In D.M.W. Powers (ed)." ></td>
	<td class="line x" title="28:234	NeMLaP3/CoNLL98: New Methods in Language Processing and Computational Natural Language Learning, ACL, pp 49-57." ></td>
	<td class="line x" title="29:234	case of persistent disagreement or uncertainty, the grammarian supervising the annotation work is consulted." ></td>
	<td class="line x" title="30:234	It has turned out that comparing annotations involves significantly more effort than annotation proper." ></td>
	<td class="line x" title="31:234	As we do not want to abandon the annotateand-compare strategy, additional effort has been put into the development of tools supporting the comparison of annotated structures (see section 4)." ></td>
	<td class="line x" title="32:234	3 Automation The efficiency of annotation can be significantly increased by using automatic annotation tools." ></td>
	<td class="line x" title="33:234	Nevertheless, some form of human supervision and handcorrection is necessary to ensure sufficient reliability." ></td>
	<td class="line x" title="34:234	As pointed out by (Marcus, Santorini, and Marcinkiewicz, 1994), such a semi-automatic annotation strategy turns out to be superior to purely manual annotation in terms of accuracy and efficiency." ></td>
	<td class="line x" title="35:234	Thus in most treebank projects, the task of the annotators consists in correcting the output of a parser, cf.(Marcus, Santorini, and Marcinkiewicz, 1994), (Black et al. , 1996)." ></td>
	<td class="line x" title="37:234	As for our project, the unavailability of broad-coverage argument-structure and dependency parsers made us adopt a bootstrapping strategy." ></td>
	<td class="line x" title="38:234	Having started with completely manual annotation, we axe gradually increasing the degree of automation." ></td>
	<td class="line x" title="39:234	The corpus annotated so far serves as training material for annotation tools based on statistical NLP methods, and the degree of automation increases with the amount of annotated sentences." ></td>
	<td class="line x" title="40:234	Automatic processing and manual input are combined interactively: the annotator specifies some information, another piece of information is added automatically, the annotator adds new information or corrects parts of the structure, new parts are added automatically, and so on." ></td>
	<td class="line x" title="41:234	The size and type of such annotation increments depends on the size of the training corpus." ></td>
	<td class="line x" title="42:234	Currently, manual annotation consists in specifying the hierarchical structure, whereas category and function labels as well as simple substructures are assigned automatically." ></td>
	<td class="line x" title="43:234	These automation steps are described in the following sections." ></td>
	<td class="line x" title="44:234	3.1 Tagging Grammatical Functions Assigning grammatical functions to a given hierarchical structure is based on a generalization of standard paxt-of-speech tagging techniques." ></td>
	<td class="line oc" title="45:234	In contrast to a standard probabilistic POS tagger (e.g.(Cutting et al. , 1992; Feldweg, 1995)), the tagger for grammatical functions works with lexical (1) Selbst besucht ADV VVPP himself visited hat Peter Sabine VAFIN NE NE has Peter Sabine 'Peter never visited Sabine himself' l hie ADV never Figure 2: Example sentence and contextual probability measures PO.(') depending on the category of a mother node (Q)." ></td>
	<td class="line x" title="47:234	This additional parameter is necessary since the sequence of grammatical functions depends heavily on the type of phrase in which it occurs." ></td>
	<td class="line x" title="48:234	Thus each category (S, VP, NP, PP etc)." ></td>
	<td class="line x" title="49:234	defines a separate Markov model." ></td>
	<td class="line x" title="50:234	Under this perspective, categories of daughter nodes correspond to the outputs of a Markov model (i.e. , like words in POS tagging)." ></td>
	<td class="line x" title="51:234	Grammatical functions can be viewed as states of the model, analogously to tags in a standard part-of-speech tagger." ></td>
	<td class="line x" title="52:234	Given a sequence of word and phrase categories T = T1Tk and a parent category Q, we calculate the sequence of grammatical functions G = G1 Gk that link T and Q as axgmax PQ( a\[T) (1) G PQ(G)." ></td>
	<td class="line x" title="53:234	Po(TIG) = axgmax a PQ(T) = axgmaxPo(a )  Pq(TIG) G Assuming the Markov property we have k PQ(TIG) = I'I PQ(T~IG,) (2) i----1 and (using a trigram model) k po(G) : IX Po(V, IGi-2, G,-1) (a) i=1 The contexts are smoothed by linear interpolation of unigrams, bigrams, and trigrams." ></td>
	<td class="line x" title="54:234	Their weights are calculated by deleted interpolation (Brown et al. , 1992)." ></td>
	<td class="line x" title="55:234	Brants and Skut 50 Automation of Treebank.4nnotation II Ii II II II I!" ></td>
	<td class="line x" title="56:234	II II II II I!" ></td>
	<td class="line x" title="57:234	II II II II II II I!" ></td>
	<td class="line x" title="58:234	II II The structure of a sample sentence is shown jn figure 2." ></td>
	<td class="line x" title="59:234	Here, the probability of the S node having this particular sequence of children is calculated as Ps(G,T) = Ps(OCl$,$)-Ps(VPlOC)  Ps(HDI$, OC)." ></td>
	<td class="line x" title="60:234	Ps(VAFINIHD)  Ps(SBIOC, HD)-Ps(NEISB ) -Ps(NGIHD, SB)Ps(ADVING ) ($ indicates the start of the sequence)." ></td>
	<td class="line x" title="61:234	The predictions of the tagger are correct in approx." ></td>
	<td class="line x" title="62:234	94% of all cases." ></td>
	<td class="line x" title="63:234	During the annotation process this is further increased by exploiting a precision/recall trade-off (cf.section 3.5)." ></td>
	<td class="line x" title="65:234	3.2 Tagging Phrasal Categories The second level of automation is the recognition of phrasal categories, which frees the annotator from typing phrase labels." ></td>
	<td class="line x" title="66:234	The task is performed by an extension of the grammatical function tagger presented in the previous section." ></td>
	<td class="line x" title="67:234	Recall that each phrasal category defines a different Markov model." ></td>
	<td class="line x" title="68:234	Given the categories of the children nodes in a phrase, we can run these models in parallel." ></td>
	<td class="line x" title="69:234	The model that assigns the most probable sequence of grammatical functions determines the category label to be assigned to the parent node." ></td>
	<td class="line x" title="70:234	Formally, we calculate the phrase category Q (and at the same time the sequence of grammatical functions G = G1  Gk) on the basis of the sequence of daughters 7' = T1  Tk with argmax maxPQ(GIT)." ></td>
	<td class="line x" title="71:234	Q 6 This procedure can also be performed using one large {combined) Markov model that enables a very efficient calculation of the maximum." ></td>
	<td class="line x" title="72:234	The overall accuracy of this approach is 95%." ></td>
	<td class="line x" title="73:234	3.3 Tagging Hierarchical Structure The next automation step is the recognition of syntactic structures." ></td>
	<td class="line x" title="74:234	In general, this task is much more difficult than assigning category and function labels, and requires a significantly larger training corpus than the one currently available." ></td>
	<td class="line x" title="75:234	What can be done at the present stage is the recognition of relatively simple structures such as NPs and PPs." ></td>
	<td class="line x" title="76:234	(Church, 1988) used a simple mechanism to mark the boundaries of NPs." ></td>
	<td class="line x" title="77:234	He used part-of-speech tagging and added two flags to the part-of-speech tags to mark the beginning and the end of an NP." ></td>
	<td class="line x" title="78:234	Our goal is more ambitious in that we mark not only the phrase boundaries of NPs but also the complete structure of a wider class of phrases, starting with APs, NPs and PPs." ></td>
	<td class="line x" title="79:234	em Dichter ART NN 1 + a poet E~ in Tel Aviv lebender APPIq NE NE ADJA ----0 4--1." ></td>
	<td class="line x" title="80:234	in Tel Aviv living 'a poet living in Tel Aviv' Figure 3: Structural tags (Ratnaparkhi, 1997) uses an iterative procedure to assign two types of tags (start X and join X, where X denotes the type of the phrase) combined with a process to build trees." ></td>
	<td class="line x" title="81:234	We go one step further and assign simple structures in one pass." ></td>
	<td class="line x" title="82:234	Furthermore, the nodes and branches of these tree chunks have to be assigned category and function labels." ></td>
	<td class="line x" title="83:234	The basic idea is to encode structures of limited depth using a finite number of tags." ></td>
	<td class="line x" title="84:234	Given a sequence of words (w0, wl  wn/, we consider the structural relation ri holding between wi and wi-1 for 1 < i < n. For the recognition of NPs and PPs, it is sufficient to distinguish the following seven values of rl which uniquely identify sub-structures of limited depth." ></td>
	<td class="line x" title="85:234	ri -0 if parent(wi) =parent(wi_l) + if parent(wi) =parent2(wi_l) ++ if parent(wi) = parentZ(wi_a) if parent2(wi) =parent(wi_l) -if parentZ(wi) = parent(wi_l) = if parent2(wi) = parentg-(wi_l) 1 else If more than one of the conditions above are met, the first of the corresponding tags in the list is assigned." ></td>
	<td class="line x" title="86:234	A structure tagged with these symbols is shown in figure 3." ></td>
	<td class="line x" title="87:234	In addition, we encode the POS tag ti assigned to w~." ></td>
	<td class="line x" title="88:234	On the basis of these two pieces of information we define structural tags as pairs Si = (ri, ti)." ></td>
	<td class="line x" title="89:234	Such Brants and Skut 51 Automation of Treebank Annotation tags constitute a finite alphabet of symbols describing the structure and syntactic category of phrases of depth < 3." ></td>
	<td class="line x" title="90:234	The task is to assign the most probable sequence of structural tags ((So, $1,  , Sn)) to a sequence of part-of-speech tags (To, T1,  , Tn)." ></td>
	<td class="line x" title="91:234	Given a sequence of part-of-speech tags T = T1  T~, we calculate the sequence of structural tags S = $1  Sk such that argmax P( S\]T) (4) s P(S) P(TIS) = argmax s P(T) = argmaxP(S)." ></td>
	<td class="line x" title="92:234	P(TIS) S The part-of-speech tags are encoded in the structural tag (t), so S uniquely determines T. Therefore, we have P(T\[S) = 1 ifTi = ii and 0 otherwise, which simplifies calculations: argmax P(S)." ></td>
	<td class="line x" title="93:234	P(T\[S) (5) s = argmax H P(SiISi-2, Si-1)P(TdSO S i=1 As in the previous models, the contexts are smoothed by linear interpolation of unigrams, bigrams, and trigrams." ></td>
	<td class="line x" title="94:234	Their weights are calculated by deleted interpolation." ></td>
	<td class="line x" title="95:234	This chunk tagging technique can be applied to treebank annotation in two ways." ></td>
	<td class="line x" title="96:234	Firstly, we could use it as a preprocessor; the annotator would then complete and correct the output of the chunk tagger." ></td>
	<td class="line x" title="97:234	The second alternative is to combine this chunking with manual input in an interactive way." ></td>
	<td class="line x" title="98:234	Then the annotator has to determine the boundaries of the sub-structure that is to be build by the program." ></td>
	<td class="line x" title="99:234	Obviously, the second solution is favorable since the user supplies information about chunk boundaries, while in the preprocessing mode the tagger has to find both the boundaries and the internal structure of the chunks." ></td>
	<td class="line x" title="100:234	The assignment of structural tags is correct in more than 94% of the cases." ></td>
	<td class="line x" title="101:234	For detailed results see section 3.6.3." ></td>
	<td class="line x" title="102:234	3.4 Interaction and Alternation To illustrate the interaction of manual input and the automatic annotation techniques described above, we show the way in which the structure in figure 3 is constructed." ></td>
	<td class="line x" title="103:234	The current version of the annotation tool supports automatic assignment of category and phrase labels, so the user has to specify the hierarchical structure step by step 2." ></td>
	<td class="line x" title="104:234	The starting point is the plain string of words together with their part-of-speech tags." ></td>
	<td class="line x" title="105:234	The annotator first selects the words Tel Aviv and executes the command 'group' (this is all done with the mouse)." ></td>
	<td class="line x" title="106:234	Then the program inserts the category label MPN (multi-lexeme proper noun) and assigns the grammatical function PNC (proper noun component) to both words (cf.sections 3.2 and 3.1)." ></td>
	<td class="line x" title="108:234	Having completed the first sub-structure, the annotator selects the newly created MPN and the preposition in, and creates a new phrase." ></td>
	<td class="line x" title="109:234	The tool automatically inserts the phrase label PP and the grammatical functions AC (adpositional case marker) and NK (noun kernel component)." ></td>
	<td class="line x" title="110:234	The following two steps are to determine the components of the AP and, finally, those of the NP." ></td>
	<td class="line x" title="111:234	At any time, the annotator has the opportunity to change and correct entries made by the program." ></td>
	<td class="line x" title="112:234	This interactive annotation mode is favorable from the point of view of consistency checking." ></td>
	<td class="line x" title="113:234	The first reason is that the annotation increments are rather small, so the annotator corrects not an entire parse tree, but a fairly simple local structure." ></td>
	<td class="line x" title="114:234	The automatic assignment of phrase and function labels is generally more reliable than manual input because it is free of typically human errors (see the precision results in (Brants, Skut, and Krenn, 1997))." ></td>
	<td class="line x" title="115:234	Thus the annotator can concentrate on the more difficult task, i.e., building complex syntactic structures." ></td>
	<td class="line x" title="116:234	The second reason is that errors corrected at lower levels in the structure facilitate the recognition of structures at higher levels, thus many wrong readings are excluded by confirming or correcting a choice at a lower level." ></td>
	<td class="line x" title="117:234	The partial automation of the annotation process (automatic regocnition of phrase labels and grammatical functions) has reduced the average annotation time from about 10 to 1.5 2 minutes per sentence, i.e. 600 800 tokens per minute, which is comparable to the figures published by the creators of the Penn Treebank in (Marcus, Santorini, and Marcinkiewicz, 1994)." ></td>
	<td class="line x" title="118:234	The test version of the annotation tool using the statistical chunking technique described in section 3.3 permits even larger annotation increments and we expect a further increase in annotation speed." ></td>
	<td class="line x" title="119:234	The user just has to select the words constituting an ~The chunk tagger has not yet been fully integrated into the annotation tool." ></td>
	<td class="line x" title="120:234	Brants and Skut 52 Automation of Treebank Annotation II !i II II II II II II II il II II II II II II II m m m m m m m m m m | m m m m m m m NP or PP." ></td>
	<td class="line x" title="121:234	The program assigns a sequence of structural tags to them; these tags are then converted to a tree structure and all labels are inserted." ></td>
	<td class="line x" title="122:234	3.5 tteliability To make automatic annotation more reliable, the program assigning labels performs an additional reliability check." ></td>
	<td class="line x" title="123:234	We do not only calculate the best assignment, but also the second-best alternative and its probability." ></td>
	<td class="line x" title="124:234	If the probability of the alternative comes very close to that of the best sequence of labels, we regard the choice as unreliable, and the annotator is asked for confirmation." ></td>
	<td class="line x" title="125:234	Currently, we employ three reliability levels, expressed by quotients of probabilities Pbest/PsecondIf this quotient is close to one (i.e. , smaller than some threshold 01), the decision counts as unreliable, and annotation is left to the annotator." ></td>
	<td class="line x" title="126:234	If the quotient is very large (i.e. , greater than some threshold 02 > 91), the decision is regarded as reliable and the respective annotation is made by the program." ></td>
	<td class="line x" title="127:234	If the quotient fails between 91 and 02, the decision is tagged as 'almost reliable'." ></td>
	<td class="line x" title="128:234	The annotation is inserted by the program, but has to be confirmed by the annotator." ></td>
	<td class="line x" title="129:234	This method enables the detection of a number of errors that are likely to be missed if the annotator is not asked for confirmation." ></td>
	<td class="line x" title="130:234	The results of using these reliability levels are reported in the experiments section below." ></td>
	<td class="line x" title="131:234	3.6 Experiments This section reports on the accuracy achieved by the methods described in the previous sections." ></td>
	<td class="line x" title="132:234	At present, our corpus contains approx." ></td>
	<td class="line x" title="133:234	6300 sentences (115,000 tokens) of German newspaper text (Frankfurter Rundschan)." ></td>
	<td class="line x" title="134:234	Results of tagging grammatical functions and phrase categories have improved slightly compared to those reported for a smaller corpus of approx." ></td>
	<td class="line x" title="135:234	1200 sentences (Brants, Skut, and Krenn, 1997)." ></td>
	<td class="line x" title="136:234	Accuracy figures for tagging the hierarchical structure are published for the first time." ></td>
	<td class="line x" title="137:234	For each experiment, the corpus was divided into two disjoint parts: 90% training data and 10% test data." ></td>
	<td class="line x" title="138:234	This procedure was repeated ten times, and the results were averaged." ></td>
	<td class="line x" title="139:234	The thresholds 01 and 02 determining the reliability levels were set to 91 = 5 and 02 = 100." ></td>
	<td class="line x" title="140:234	3.6.1 Grammatical Functions We employ the technique described in section 3.1 to assign grammatical functions to a structure defined by an annotator." ></td>
	<td class="line x" title="141:234	Grammatical functions are Table 1: Levels of reliability and the percentage of cases in which the tagger assigned a correct grammatical function (or would have assigned ifa decision had been forced)." ></td>
	<td class="line x" title="142:234	grammatical function reliable marked unreliable overall cases correct 88% 97.0% 8% 85.0% 4% 59.5% 100% 94.6% Table 2: Levels of reliability and the percentage of cases in which the tagger assigned a correct phrase category (or would have assigned it if a decision had been forced)." ></td>
	<td class="line x" title="143:234	phrase category reliable marked unreliable overall cases correct 76% 99.0% 19% 91.5% 5% 56.7% 100% 95.4% represented by edge labels." ></td>
	<td class="line x" title="144:234	Additionally, we exploit the recall/accuracy tradeoff as described in section 3.5." ></td>
	<td class="line x" title="145:234	The tagset of grammatical functions consists of 45 tags." ></td>
	<td class="line x" title="146:234	Tagging results are shown in table 1." ></td>
	<td class="line x" title="147:234	Overall accuracy is 94.6%." ></td>
	<td class="line x" title="148:234	88% of all predictions are classified as reliable, which is the most important class for the actual annotation task." ></td>
	<td class="line x" title="149:234	Accuracy in this class is 97.0%." ></td>
	<td class="line x" title="150:234	It depends on the category of the phrase, e.g. accuracy for reliable cases reaches 99% for 51Ps and PPs." ></td>
	<td class="line x" title="151:234	3.6.2 Phrasal Categories Now the task is to assign phrasal categories to a structure specified by the annotator, i.e., only the hierarchical structure is given." ></td>
	<td class="line x" title="152:234	We employ the technique of competing Markov models as described in section 3.2 to assign phrase categories to the structure." ></td>
	<td class="line x" title="153:234	Additionally, we compute alternatives to assign one of the three reliability levels to each decision as described in section 3.5." ></td>
	<td class="line x" title="154:234	The tagset for phrasal categories consists of 25 tags." ></td>
	<td class="line x" title="155:234	As can be seen from table 2, the results of assigning phrasal categories are even better than those of assigning grammatical functions." ></td>
	<td class="line x" title="156:234	Overall accuracy is 95.4%." ></td>
	<td class="line x" title="157:234	Tags that are regarded as reliable (76% of all cases) have an accuracy of 99.0%, which results Brants and Skut 53 Automation of Treebank Annotation Table 3: Chunk tagger accuracy with respect to hierarchical structure." ></td>
	<td class="line x" title="158:234	structural tags reliable marked unreliable overall cases correct 86% 95.8% 11% 93.2% 3% 67.0% 100% 94.4% in very reliable annotations." ></td>
	<td class="line x" title="159:234	3.6.3 Chunk Tagger The chunk tagger described in section 3.3 assigns tags encoding structural information to a sequence of words and tags." ></td>
	<td class="line x" title="160:234	The accuracy figures presented here refer to the correct assignments of these tags (see table 3)." ></td>
	<td class="line x" title="161:234	The assignment of structural tags allows us to construct a tree; the labels are afterwards assigned in a bottom-up fashion by the function/category label tagger described in earlier sections." ></td>
	<td class="line x" title="162:234	Overall accuracy is 94.4% and reaches 95.8% in the reliable cases." ></td>
	<td class="line x" title="163:234	A different measure of the chunker's correctness is the percentage of complete phrases recognized correctly." ></td>
	<td class="line x" title="164:234	In order to determine this percentage, we extracted all chunks of the maximal depth recognizable by the chunker." ></td>
	<td class="line x" title="165:234	In a cross evaluation, 87.3% of these chunks were recognized correctly as far as the hierarchical structure is concerned." ></td>
	<td class="line x" title="166:234	4 Comparing Trees Annotations produced by different annotators are compared automatically and differences are marked." ></td>
	<td class="line x" title="167:234	The output of the comparison is given to the annotators." ></td>
	<td class="line x" title="168:234	First, each of the annotators goes through the differences on his own and corrects obvious errors." ></td>
	<td class="line x" title="169:234	Then remaining differences are resolved in a discussion of the annotators." ></td>
	<td class="line x" title="170:234	Additionally, the program calculates the probabilities of the two different annotations." ></td>
	<td class="line x" title="171:234	This is intended to be a first step towards resolving conflicting annotations automatically." ></td>
	<td class="line x" title="172:234	Both parts, tree matching and the calculation of probabilities for complete trees are described in the following sections." ></td>
	<td class="line x" title="173:234	4.1 Tree Matching The problem addressed here is the comparison of two syntactic structures that share identical terminal nodes (the words of the annotated sentence)." ></td>
	<td class="line x" title="174:234	proc compare(A, B) for each non-terminal node X in A: search node Y in B such that yield(X) = yield(Y) if Y exists: emit different labels if any if Y does not exist: emit X and its yield end end Figure 4: Basic asymmetric algorithm to compare annotation A with annotation B of the same sentence (Calder, 1997) presents a method of comparing the structure of context free trees found in different annotations." ></td>
	<td class="line x" title="175:234	This section presents an extension of this algorithm that compares predicate-argument structures possibly containing crossing branches (cf.figure 2)." ></td>
	<td class="line x" title="177:234	Node and edge labels, representing phrasal categories and grammatical functions, are also taken into account." ></td>
	<td class="line x" title="178:234	Phrasal (non-terminal) nodes are compared on the basis of their yields: the yield of a nonterminal node X in an annotation A is the ordered set of terminals that are (directly or indirectly) dominated by X. The yield need not be contiguous since predicate-argument structures allow discontinuous constituents." ></td>
	<td class="line x" title="179:234	If both annotations contain nonterminal nodes that cover the same terminal nodes, the labels of the nonterminal nodes and their edges are compared." ></td>
	<td class="line x" title="180:234	This results in a combined measure of structural and labeling differences, which is very useful in cleaning the corpus and keeping track of the development of the treebank." ></td>
	<td class="line x" title="181:234	We use the basic algorithm shown in figure 4 to determine the differences in two annotations A and B. The basic form is asymmetric." ></td>
	<td class="line x" title="182:234	Therefore, a complete comparison consists of two runs, one for each direction, and the outputs of both runs are combined." ></td>
	<td class="line x" title="183:234	Figures 5 and 6 show examples of the output of the algorithm." ></td>
	<td class="line x" title="184:234	These outputs can be directly used to mark the corresponding nodes and edges." ></td>
	<td class="line x" title="185:234	The yield is sufficient to uniquely determine corresponding nodes since the annotations used here do not contain unary branching nodes." ></td>
	<td class="line x" title="186:234	If unary branching occurs, both the parent and the child have the same terminal yield and further mechanism to determine corresponding nodes are needed." ></td>
	<td class="line x" title="187:234	(Calder, 1997) points out possible solutions to this problem." ></td>
	<td class="line x" title="188:234	Brants and Skut 54 Automation of Treebank Annotation II II II II II II II II II II II II II II II II II II II II I m m m m m m m m m m m m m m m m m m m /2 tt Selbst besucht hat Peter 0 1 2 3 ADV VVPP VAFIN NE himself visited has Peter Sabine 'Peter never visited Sabine himself' ++ Sabine nie 4 5 NE ADV never sentence I errors I (1) structure: 500 VP lOCI 0 1 4 (Selbst besucht Sabine) (2) structure: 500 VP lOCI 0 I 4 S (Selbst besucht Sabine hie) Figure 5: Erroneous annotation (2) of the example sentence in figure 2 (hie should be attached to S instead of VP), together with the output of the tree comparison algorithm All nodes are numbered to enable identification." ></td>
	<td class="line x" title="189:234	Additionally, this output can be used to highlight the corresponding nodes and edges." ></td>
	<td class="line x" title="190:234	(3) Selbst 0 ADV himself besucht hat Peter Sabine 1 2 3 4 VVPP VAFIN NE NE visited has Peter Sabine 'Peter never visited Sabine himself' ? nie 5 ADV never sentence 1 errors 1 (1) edge: 5 (ADV) \[NG\] nie (3) edge: 5 (ADV) \[MO\] hie Figure 6: Erroneous annotation (3) of the example sentence in figure 2 (nie should have grammatical function NG instead of MO), together with the output of the tree comparison algorithm." ></td>
	<td class="line x" title="191:234	4.2 Probabilities The probabilities of each sub-structure of depth one are calculated separately according to the model described in sections 3.1 and 3.2." ></td>
	<td class="line x" title="192:234	Subsequently, the product of these probabilities is used as a scoring function for the complete structure This method is based on the assumption that productions at different levels in a structure are independent, which is inherent to context free rules." ></td>
	<td class="line x" title="193:234	Using the formulas from sections 3.1 and 3.2, the probability P(A) of an annotation A is evaluated as P(A) = HP(Qi) i=l nnt = H PQ,(T,, G,) i--1 rtnt ki = 1\] 1\] Poi(gi,ylg,,~-2, g/,~-l) i=lj=l  PQ(ti,y I gij) A annotation (structure) for a sentence nnt number of nonterminal nodes in A nt number of terminal nodes in A n number of nodes = nnt + nt Qi ith phrase in A T/ sequence of tags in Qi Gi sequence of gramm, func." ></td>
	<td class="line x" title="194:234	in Qi ki number of elements in Qi tij tag of jth child in Qi gl,i grammatical function of jth child in Qi Probabilities computed in this way cannot be used directly to compare two annotations since they favor annotations with fewer nodes." ></td>
	<td class="line x" title="195:234	Each new nonterminal node introduces a new element in the product and makes it smaller." ></td>
	<td class="line x" title="196:234	Therefore, we normalize the probabilities w.r.t. the number of nodes in the annotation, which yields the perplexity PP(A) of an annotation A: PP(A)=~'p~A) (6) 4.3 Application to a Corpus The procedures of tree matching and probability calculation were applied to our corpus, which currently consists of approx." ></td>
	<td class="line x" title="197:234	6300 sentences (115,000 tokens) of German newspaper text, each sentence annotated at least twice." ></td>
	<td class="line x" title="198:234	We measured the agreement of independent annotations after first annotation but before correction Brants and Skut 55 Automation of Treebank Annotation Table 4: Comparison of independent semi-automatic annotations (1) after first, independent annotation and (2) after comparison but before the final discussion (current stage)." ></td>
	<td class="line x" title="199:234	word level: (1) ident, parent node (2) ident, gram." ></td>
	<td class="line x" title="200:234	func." ></td>
	<td class="line x" title="201:234	node level: (3) identical nodes (4) identical nodes/labels (5) ident, node/gram, func." ></td>
	<td class="line x" title="202:234	sentence level: (6) identical structure (7) identical annotation <1> (2> 92.3% 98.7% 93.8% 99.1% 87.6% 98.1% 84.2% 97.4% 76.6% 96.3% 48.6% 90.8% 34.6% 87.9% (1), and after correction but before the final discussion (2), which is the current stage of the corpus." ></td>
	<td class="line x" title="203:234	The results are shown in table 4." ></td>
	<td class="line x" title="204:234	As for measuring differences, we can count them at word, node and sentence level." ></td>
	<td class="line x" title="205:234	At the word level, we are interested in (1) the number of correctly assigned parent categories (does a word belong to a PP, NP, etc.?), and (2) the number of correctly assigned grammatical functions (is a word a head, modifier, subject, etc.?)." ></td>
	<td class="line x" title="206:234	At the node level (non-terminals, phrases) we measure (3) the number of identical nodes, i.e., if there is a node in one annotation, we check whether it corresponds to a node in the other annotation having the same yield." ></td>
	<td class="line x" title="207:234	Additionally, we count (4) the number of identical nodes having the same phrasal category, and (5) the number of identical nodes having the same phrasal category and the same grammatical function within its parent phrase." ></td>
	<td class="line x" title="208:234	At the sentence level, we measure (6) the number of annotated sentences having the same structure, and, which is the strictest measure, (7) the number of sentences having the same structure and the same labels (i.e. , exactly the same annotation)." ></td>
	<td class="line x" title="209:234	At the node level, we find 87.6% agreement in independent annotations." ></td>
	<td class="line x" title="210:234	A large amount of the differences come from misinterpretation of the annotation guidelines by the annotators and are eliminated after comparison, which results in 98.1% agreement." ></td>
	<td class="line x" title="211:234	This kind of comparison is the one most frequently used in the statistical parsing community for comparing parser output." ></td>
	<td class="line x" title="212:234	The sentence level is the strictest measure, and the agreement is low (34.6% identical annotations after first annotation, 87.9% after comparison)." ></td>
	<td class="line x" title="213:234	But at this level, one error (e.g. a wrong label) renders Table 5: Using model perplexities to compare different annotations: Accuracy of using the hypothesis that a correct annotation has a lower perplexity than a wrong annotation." ></td>
	<td class="line x" title="214:234	recall precision 30% 95.3% 45% 92.2% 60% 88.6% 85% 81.4% 100% 65.8% the whole annotation to be wrong and the sentence counts as an error." ></td>
	<td class="line x" title="215:234	If we make the assumption that a correct annotation always has a lower perplexity than a wrong annotation for the same sentence, the system would make a correct decision for 65.8% of the sentences (see table 5, last row)." ></td>
	<td class="line x" title="216:234	For approx." ></td>
	<td class="line x" title="217:234	70% of all sentences, at least one of the initial annotations was completely correct." ></td>
	<td class="line x" title="218:234	This means that the two initial annotations and the automatic comparison yield a corpus with approx." ></td>
	<td class="line x" title="219:234	65.8%  70% = 46% completely correct annotations (complete structure and all tags)." ></td>
	<td class="line x" title="220:234	One can further increase precision at the cost of recall by requiring the difference in perplexity to exceed some minimum distance." ></td>
	<td class="line x" title="221:234	This precision/recall tradeoff is also shown in table 5." ></td>
	<td class="line x" title="222:234	5 Conclusion The techniques and automatic tools described in this paper are designed to support annotation proper, online/offline consistency checking and the comparison of independent annotations of the same sentences." ></td>
	<td class="line x" title="223:234	Most of the techniques employ stochastic processing methods, which guarantee high accuracy and robustness." ></td>
	<td class="line x" title="224:234	The bootstrapping approach adopted in our project makes the degree of automation a function of available training data." ></td>
	<td class="line x" title="225:234	Easier processing tasks are automated first." ></td>
	<td class="line x" title="226:234	Experience gained and data annotated at a lower level allow to increase the level of automation step by step." ></td>
	<td class="line x" title="227:234	The current size of our corpus (approx." ></td>
	<td class="line x" title="228:234	6300 sentences) enables reliable automatic assignment of category and function labels as well as simple structures." ></td>
	<td class="line x" title="229:234	Future work will be concerned with developing automatic annotation methods handling complex structures, which should ultimately lead to the development of a parser for predicate-argument trees containing crossing branches." ></td>
	<td class="line x" title="230:234	Brants and Skut 56 Automation of Treebank Annotation II !1 II II II II II II II II II II II II II II II II | I | | | I II II II II 6 Acknowledgements This work is part of the DFG Sonderforschungsbereich 378 Resource-Adaptive Cognitive Processes, Project C3 Concurrent'Grammar Processing." ></td>
	<td class="line x" title="231:234	We wish to thank the universities of Stuttgart and Tiibingen for kindly providing us with a handcorrected part-of-speech tagged corpus." ></td>
	<td class="line x" title="232:234	We also wish to thank Oliver Plaehn, who did a great job in implementing the annotation tool, and Peter Sch~ifer, who built the tree comparison tool." ></td>
	<td class="line x" title="233:234	Special thanks go to the five annotators continually increasing the size and the quality of our corpus." ></td>
	<td class="line x" title="234:234	And finally, we thank Sabine Kramp for proof-reading this paper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E99-1018
POS Disambiguation And Unknown Word Guessing With Decision Trees
Orphanos, Giorgos S.;Christodoulakis, Dimitris N.;"></td>
	<td class="line x" title="1:133	Proceedings of EACL '99 POS Disambiguation and Unknown Word Guessing with Decision Trees Giorgos S. Orphanos Computer Engineering & Informatics Dept. and Computer Technology Institute University of Patras 26500 Rion, Patras, Greece geoffan@cti.gr Dimitris N. Christodoulalds Computer Engineering & Informatics Dept. and Computer Technology Institute University of Patras 26500 Rion, Patras, Greece dxri@cti.gr Abstract This paper presents a decision-tree approach to the problems of part-ofspeech disambiguation and unknown word guessing as they appear in Modem Greek, a highly inflectional language." ></td>
	<td class="line x" title="2:133	The learning procedure is tag-set independent and reflects the linguistic reasoning on the specific problems." ></td>
	<td class="line x" title="3:133	The decision trees induced are combined with a highcoverage lexicon to form a tagger that achieves 93,5% overall disambiguation accuracy." ></td>
	<td class="line x" title="4:133	1 Introduction Part-of-speech (POS) taggers are software devices that aim to assign unambiguous morphosyntactic tags to words of electronic texts." ></td>
	<td class="line x" title="5:133	Although the hardest part of the tagging process is performed by a computational lexicon, a POS tagger cannot solely consist of a lexicon due to: (i) morphosyntactic ambiguity (e.g. , 'love' as verb or noun) and (ii) the existence of unknown words (e.g. , proper nouns, place names, compounds, etc.)." ></td>
	<td class="line x" title="6:133	When the lexicon can assure high coverage, unknown word guessing can be viewed as a decision taken upon the POS of open-class words (i.e. , Noun, Verb, Adjective, Adverb or Participle)." ></td>
	<td class="line x" title="7:133	Towards the disambiguation of POS tags, two main approaches have been followed." ></td>
	<td class="line x" title="8:133	On one hand, according to the linguistic approach, experts encode handcrafted rules or constraints based on abstractions derived from language paradigms (usually with the aid of corpora) (Green and Rubin, 1971; Voutilainen 1995)." ></td>
	<td class="line oc" title="9:133	On the other hand, according to the data-driven approach, a frequency-based language model is acquired from corpora and has the forms of ngrams (Church, 1988; Cutting et al. , 1992), rules (Hindle, 1989; Brill, 1995), decision trees (Cardie, 1994; Daelemans et al. , 1996) or neural networks (Schmid, 1994)." ></td>
	<td class="line x" title="10:133	In order to increase their robusmess, most POS taggers include a guesser, which tries to extract the POS of words not present in the lexicon." ></td>
	<td class="line oc" title="11:133	As a common strategy, POS guessers examine the endings of unknown words (Cutting et al. 1992) along with their capitalization, or consider the distribution of unknown words over specific parts-of-speech (Weischedel et aL, 1993)." ></td>
	<td class="line x" title="12:133	More sophisticated guessers further examine the prefixes of unknown words (Mikheev, 1996) and the categories of contextual tokens (Brill, 1995; Daelemans et aL, 1996)." ></td>
	<td class="line x" title="13:133	This paper presents a POS tagger for Modem Greek (M. Greek), a highly inflectional language, and focuses on a data-driven approach for the induction of decision trees used as disambiguation/guessing devices." ></td>
	<td class="line x" title="14:133	Based on a high-coverage 1 lexicon, we prepared a tagged corpus capable of showing off the behavior of all POS ambiguity schemes present in M. Greek (e.g. , Pronoun-Clitic-Article, Pronoun-Clitic, Adjective-Adverb, Verb-Noun, etc.), as well as the characteristics of unknown words." ></td>
	<td class="line x" title="15:133	Consequently, we used the corpus for the induction of decision trees, which, along with 1 At present, the lexicon is capable of assigning full morphosyntactic attributes (i.e. , POS, Number, Gender, Case, Person, Tense, Voice, Mood) to -870.000 Greek word-forms." ></td>
	<td class="line x" title="16:133	134 Proceedings of EACL '99 the lexicon, are integrated into a robust POS tagger for M. Greek texts." ></td>
	<td class="line x" title="17:133	The disambiguating methodology followed is highly influenced by the Memory-Based Tagger (MBT) presented in (Daelemans et aL, 1996)." ></td>
	<td class="line x" title="18:133	Our main contribution is the successful application of the decision-tree methodology to M. Greek with three improvements/customizations: (i) injection of linguistic bias to the learning procedure, (ii) formation of tag-set independent training patterns, and (iii) handling of set-valued features." ></td>
	<td class="line x" title="19:133	2 Tagger Architecture Figure 1 illustrates the functional components of the tagger and the order of processing: Raw Text -I I I words with one tag I I I rere un~ownl I ~an wr, 4;; Disambiguator I tags' I &Guesser I I words with one tag Ta ed Text Figure 1." ></td>
	<td class="line x" title="20:133	Tagger Architecture Raw text passes through the Tokenizer, where it is converted to a stream of tokens." ></td>
	<td class="line x" title="21:133	Non-word tokens (e.g. , punctuation marks, numbers, dates, etc)." ></td>
	<td class="line x" title="22:133	are resolved by the Tokenizer and receive a tag corresponding to their category." ></td>
	<td class="line x" title="23:133	Word tokens are looked-up in the Lexicon and those found receive one or more tags." ></td>
	<td class="line x" title="24:133	Words with more than one tags and those not found in the Lexicon pass through the Disambiguator/Guesser, where the contextually appropriate tag is decided/guessed." ></td>
	<td class="line x" title="25:133	The Disambiguator/Guesser is a 'forest' of decision trees, one tree for each ambiguity scheme present in M. Greek and one tree for unknown word guessing." ></td>
	<td class="line x" title="26:133	When a word with two or more tags appears, its ambiguity scheme is identified." ></td>
	<td class="line x" title="27:133	Then, the corresponding decision tree is selected, which is traversed according to the values of morphosyntactic features extracted from contextual tags." ></td>
	<td class="line x" title="28:133	This traversal returns the contextually appropriate POS." ></td>
	<td class="line x" title="29:133	The ambiguity is resolved by eliminating the tag(s) with different POS than the one returned by the decision tree." ></td>
	<td class="line x" title="30:133	The POS of an unknown word is guessed by traversing the decision tree for unknown words, which examines contextual features along with the word ending and capitalization and returns an open-class POS." ></td>
	<td class="line x" title="31:133	3 Training Sets For the study and resolution of lexical ambiguity in M. Greek, we set up a corpus of 137.765 tokens (7.624 sentences), collecting sentences from student writings, literature, newspapers, and technical, financial and sports magazines." ></td>
	<td class="line x" title="32:133	We made sure to adequately cover all POS ambiguity schemes present in M. Greek, without showing preference to any scheme, so as to have an objective view to the problem." ></td>
	<td class="line x" title="33:133	Subsequently, we tokenized the corpus and inserted it into a database and let the lexicon assign a morphosyntactic tag to each word-token." ></td>
	<td class="line x" title="34:133	We did not use any specific tag-set; instead, we let the lexicon assign to each known word all morphosyntactic attributes available." ></td>
	<td class="line x" title="35:133	Table 1 shows a sample sentence after this initial tagging (symbolic names appearing in the tags are explained in Appendix A)." ></td>
	<td class="line x" title="36:133	2638 2638 2638 2638 2638 2638 2638 2638 Table 1." ></td>
	<td class="line x" title="37:133	An example-sentence from the tagged corpus 1 Ot The Art (MscFemSglNom) 2 axuvff\]o~t~ answers vrb(,B_SglActPSsjv + iB~,SglKctFutlnd)+ Nra% ( FemP1 rNomAc cVoc) 3 ~oI) of ' Prn ( C MScNtrsngGen)+~ Clt + Art (MscNtrSngGen) 4 ~." ></td>
	<td class="line x" title="38:133	Mr. Abr 5 n=~0~o~ eap,dopoulos 'ou' Cap N~ + vrb + Adj + Pep +Aav 6 .illaV were Vrb (-c--sg!" ></td>
	<td class="line x" title="39:133	~ir I c~Ind!i .,_i~i/, 7 aa~iq clear Adj (MscFemPlrNomAccVoc) 8 I . ! N1212 Art Nnn 135 Proceedings of EACL '99 Table 2." ></td>
	<td class="line x" title="40:133	A fragment from the training set Verb-Noun .Examplel ~: .~.~::~,~i:.~:::.~::~::~:: ~I ::~ :i:~:::~.i~ ~ ~:-~:< ~./Tiig~,.: ~ :S,.;;.i:~ ~ ~:,;;/'\[Manuil : i~iD~.:i:l:~,i:~i~';~::;::i~i:ii%~!::~:~~J~':.~~::~i~ :~i~:~i~.': !~:.~ i:::~ :~::~i':i~i~." ></td>
	<td class="line x" title="41:133	:~'~il;:.,;~< :!'~ ;: '?~::' '.~::!~ ~ s:~-:ii'.:." ></td>
	<td class="line x" title="42:133	~-.'~'~'.~.~'.:~ ;~:~:!.:',~t~-'::i.'~ ~." ></td>
	<td class="line x" title="43:133	l 1 Adj (FemSglNomAcc) ;Vrb(_B_SglPntActZmv) + ~Prn( C FemSglGen) + Clt + Nnn Nnn (FemSglNomAccVoc) ~rt ( FemSglGen ) Nnn (FemSglNomAccVoc) '' 'i iqzm '+ Vrb + 'Adj +-Pep !Vrb (_B_SglFutPstActIndSjv) + i,, . 'N'~'. + Adv Nnn (FemPlrNomAccVoc) 4 Prn (_A_SglGenAcc) + Vrb (_B_SglFutPstActIndSjv) + Adj (FemSglNomAccVoc) Vrb,  Pps  Nnn ( Nt rSgl P i rNomGenAccVoc ) 5 Art (FemPlrAcc)  ~r b 'i-_B~Sg-i-~ ~P' -s tJ%c-E Z nclS jv ~   ~ p~~-c'fise~Er~i-6%n3 '~' -6iE'i~ ' Nnn(FemPlrNomAccVoc) ',+ Art (MscNtrSglGen)  6 ' Pci  Vrb (B_SglPntFcsFutPstActIndSjv) !Prn (A_SglGenAcc) + Pps Vrb ~+ Nrns (MscSglNom)  7  3/rb (B_SglFutPstActIndSjv) + ~rb (_C_PlrPntFcsActIndSjv)  N~-~ Nnn ( FemPlrNomAccVoc ) '  ' Vrb 8 Pcl ~Vrb (_B_SglFutPstActIndSjv) + i Nnn ( Nt rSgl P1 rNomGenAc cVoc) ! 9 Adj (FemSglNomAcc) Nrb (_C_SglPntFcsActIndSjv) + ~t (MscSglAcc + Nnn .l~nn (FemSglNomAccVoc) ~t rSglNomAcc )  10 Pcl + Adv Mrb( B SglPntFcsFutPstActXndSjv)~  Vrb : i+ Nnn (MscSglNom) '~ To words with POS ambiguity (e.g. , tokens #2 and #3 in Table 1) we manually assigned their contextually appropriate POS." ></td>
	<td class="line x" title="44:133	To unknown words (e.g. , token #5 in Table 1), which by default received a disjunct of open-class POS labels, we manually assigned their real POS and declared explicitly their inflectional ending." ></td>
	<td class="line x" title="45:133	At a next phase, for all words relative to a specific ambiguity scheme or for all unknown words, we collected from the tagged corpus their automatically and manually assigned tags along with the automatically assigned tags of their neighboring tokens." ></td>
	<td class="line x" title="46:133	This way, we created a training set for each ambiguity scheme and a training set for unknown words." ></td>
	<td class="line x" title="47:133	Table 2 shows a 10-example fragment from the training set for the ambiguity scheme Verb-Noun." ></td>
	<td class="line x" title="48:133	For reasons of space, Table 2 shows the tags of only the previous (column Tagi_l) and next (column Tagi+~) tokens in the neighborhood of an ambiguous word, whereas more contextual tags actually comprise a training example." ></td>
	<td class="line x" title="49:133	A training example also includes the manually assigned tag (column Manual Tagi) along with the automatically assigned tag 2 (column Tagi) of the ambiguous word." ></td>
	<td class="line x" title="50:133	One can notice that some contextual tags are missing (e.g. , Tagi_~ of Example 7; the ambiguous word is the first in the sentence), or some contextual tags may exhibit POS ambiguity (e.g. , Tagi+l of Example 1), an incident implying that the learner must learn from incomplete/ambiguous examples, since this is the case in real texts." ></td>
	<td class="line x" title="51:133	If we consider that a tag encodes 1 to 5 morphosyntaetic features, each feature taking one or a disjunction of 2 to 11 values, then the total number of different tags counts up to several hundreds 3." ></td>
	<td class="line x" title="52:133	This fact prohibits the feeding of the training algorithms with patterns that have the form: (Tagi_2, Tagi_b Tagi, Tagi.~, Manual_Tagi), which is the ease for similar systems that learn POS disambiguation (e.g. , Daelemans et al. , 1996)." ></td>
	<td class="line x" title="53:133	On the other hand, it would be inefficient (yielding to information loss) to generate a simplified tag-set in order to reduce its size." ></td>
	<td class="line x" title="54:133	The 'what the training patterns should look like' bottleneck was surpassed by assuming a set of functions that extract from a tag the value(s) of specific features, e.g.: Gender(Art (MscSglAcc + NtrSglNomAcc)) = MSC + Ntr With the help of these functions, the training examples shown in Table 2 are interpreted to patterns that look like: (POS(Tagi_2), POS(Tagi_l), Gender(Tagi), POS(TagH), Gender(Tagi+l), Manual_Tagi), 2 In case the learner needs to use morphosyntactic information of the word being disambiguated." ></td>
	<td class="line x" title="55:133	3 The words of the corpus received from the lexicon 690 different tags having the form shown in Table 2." ></td>
	<td class="line x" title="56:133	136 Proceedings of EACL '99 that is, a sequence of feature-values extracted from the previous/current/next tags along with the manually assigned POS label." ></td>
	<td class="line x" title="57:133	Due to this transformation, two issues automatically arise: (a) A feature-extracting function may return more than one feature value (as in the Gander() example); consequently, the training algorithm should be capable of handling set-valued features." ></td>
	<td class="line x" title="58:133	(b) A featureextracting function may return no value, e.g. Gender(Vrb( C PlrPntkctlndSjv)) = None, thus we added an extra value -the value None-to each feature 4." ></td>
	<td class="line x" title="59:133	To summarize, the training material we prepared consists of: (a) a set of training examples for each ambiguity scheme and a set of training examples for unknown words 5, and (b) a set of features accompanying each example-set, denoting which features (extracted from the tags of training examples) will participate in the training procedure." ></td>
	<td class="line x" title="60:133	This configuration offers the following advantages: 1." ></td>
	<td class="line x" title="61:133	A training set is examined only for the features that are relative to the corresponding ambiguity scheme, thus addressing its idiosyncratic needs." ></td>
	<td class="line x" title="62:133	2." ></td>
	<td class="line x" title="63:133	What features are included to each featureset depends on the linguistic reasoning on the specific ambiguity scheme, introducing this way linguistic bias to the learner." ></td>
	<td class="line x" title="64:133	3." ></td>
	<td class="line x" title="65:133	The learning is tag-set independent, since it is based on specific features and not on the entire tags." ></td>
	<td class="line x" title="66:133	4." ></td>
	<td class="line x" title="67:133	The learning of a particular ambiguity scheme can be fine-tuned by including new features or excluding existing features from its feature-set, without affecting the learning of the other ambiguity schemes." ></td>
	<td class="line x" title="68:133	4 Decision Trees 4.1 Tree Induction In the previous section, we stated the use of linguistic reasoning for the selection of feature4 e.g.: Gender = {Masculine, Feminine, Neuter, None}." ></td>
	<td class="line x" title="69:133	5 The training examples for unknown words, except contextual tags, also include the capitalization feature and the suffixes of unknown words." ></td>
	<td class="line x" title="70:133	sets suitable to the idiosyncratic properties of the corresponding ambiguity schemes." ></td>
	<td class="line x" title="71:133	Formally speaking, let FS be the feature-set attached to a training set TS." ></td>
	<td class="line x" title="72:133	The algorithm used to transform TS into a decision tree belongs to the TDIDT (Top Down Induction of Decision Trees) family (Quinlan, 1986)." ></td>
	<td class="line x" title="73:133	Based on the divide and conquer principle, it selects the best Fbe, t feature from FS, partitions TS according to the values of Fbest and repeats the procedure for each partition excluding Fbest from FS, continuing recursively until all (or the majority of) examples in a partition belong to the same class C or no more features are left in FS." ></td>
	<td class="line x" title="74:133	During each step, in order to find the feature that makes the best prediction of class labels and use it to partition the training set, we select the feature with the highest gain ratio, an information-based quantity introduced by Quinlan (1986)." ></td>
	<td class="line x" title="75:133	The gain ratio metric is computed as follows: Assume a training set TS with patterns belonging to one of the classes C1, C2,  Ck." ></td>
	<td class="line x" title="76:133	The average information needed to identify the class of a pattern in TS is: info(TS)  freq(Cj,TS) = x log 2 (freq(Cj' TS)) j=l ITS I ITS I Now consider that TS is partitioned into TSI, TSz,  TS., according to the values of a feature F from FS." ></td>
	<td class="line x" title="77:133	The average information needed to identify the class of a pattern in the partitioned TS is: info F (TS ) = 1TSl I xinfo(TSi) i=l \[TSI The quantity: gain(F) = info(TS) info F (TS) measures the information relevant to classification that is gained by partitioning TS in accordance with the feature F. Gain ratio is a normalized version of information gain: gain ratio(F) = gain(F) split info(F) Split info is a necessary normalizing factor, since gain favors features with many values, and represents the potential information generated by dividing TS into n subsets: split info(F) = - ITsi I lg2 (IIT:~ I) i=1 ITS\[ \[ 137 Proceedings of EACL '99 Taking into consideration the formula that computes the gain ratio, we notice that the best feature is the one that presents the minimum entropy in predicting the class labels of the training set, provided the information of the feature is not split over its values." ></td>
	<td class="line x" title="78:133	The recursive algorithm for the decision tree induction is shown in Figure 2." ></td>
	<td class="line x" title="79:133	Its parameters are: a node N, a training set TS and a feature set FS." ></td>
	<td class="line x" title="80:133	Each node constructed, in a top-down leftto-right fashion, contains a default class label C (which characterizes the path constructed so far) and if it is a non-terminal node it also contains a feature F from FS according to which further branching takes place." ></td>
	<td class="line x" title="81:133	Every value vi of the feature F tested at a non-terminal node is accompanied by a pattern subset TSj (i.e. , the subset of patterns containing the value vi)." ></td>
	<td class="line x" title="82:133	If two or more values of F are found in a training pattern (set-valued feature), the training pattern is directed to all corresponding branches." ></td>
	<td class="line x" title="83:133	The algorithm is initialized with a root node, the entire training set and the entire feature set." ></td>
	<td class="line x" title="84:133	The root node contains a dummy 6 feature and a blank class label." ></td>
	<td class="line x" title="85:133	InduceTree( Node N, TrainingSet TS, FeatureSet FS ) Begin For each value v= of the feature F tested by node N Do Begin Create the subset TSl and assign it to vi; If TSi is empty Then continue; /* goto For */ If all pattems in TS~ belong to the same class C Then Create under vi a leaf node N' with label C; Else Begin Find the most frequent class C in TS~; If FS is empty Then Create under vj a leaf node N' with label C; Else Begin Find the feature F' ~th the highest gain ratio; Create under vja non-terminal node N' with label C and set N' to test F'; Create the feature subset FS' = FS {F'}; InduceTree( N', TSi, FS' ); End End End End Figure 2." ></td>
	<td class="line x" title="86:133	Tree-Induction Algorithm 6 The dummy feature contains the sole value None." ></td>
	<td class="line x" title="87:133	4.2 Tree Traversal Each tree node, as already mentioned, contains a class label that represents the 'decision' being made by the specific node." ></td>
	<td class="line x" title="88:133	Moreover, when a node is not a leaf, it also contains an ordered list of values corresponding to a particular feature tested by the node." ></td>
	<td class="line x" title="89:133	Each value is the origin of a subtree hanging under the non-terminal node." ></td>
	<td class="line x" title="90:133	The tree is traversed from the root to the leaves." ></td>
	<td class="line x" title="91:133	Each non-terminal node tests one after the other its feature-values over the testing pattern." ></td>
	<td class="line x" title="92:133	When a value is found, the traversal continues through the subtree hanging under that value." ></td>
	<td class="line x" title="93:133	If none of the values is found or the current node is a leaf, the traversal is finished and the node's class label is returned." ></td>
	<td class="line x" title="94:133	For the needs of the POS disambiguation/guessing problem, tree nodes contain POS labels and test morphosyntactic features." ></td>
	<td class="line x" title="95:133	Figure 3 illustrates the tree-traversal algorithm, via which disarnbiguation/guessing is performed." ></td>
	<td class="line x" title="96:133	The lexical and/or contextual features of an ambiguous/unknown word constitute a testing pattern, which, along with the root of the decision tree corresponding to the specific ambiguity scheme, are passed to the tree-traversal algorithm." ></td>
	<td class="line x" title="97:133	ClassLabel TraverseTree( Node N, TestingPattem P ) Begin If N is a non-terminal node Then For each value vl of the feature F tested by N Do If vl is the value of F in P Then Begin N' = the node hanging under vj; Return TraverseTree( N', P ); End Retum the class label of N; End Figure 3." ></td>
	<td class="line x" title="98:133	Tree-Traversal Algorithm 4.3 Subtree Ordering The tree-traversal algorithm of Figure 3 can be directly implemented by representing the decision tree as nested if-statements (see Appendix B), where each block of code following an if-statement corresponds to a subtree." ></td>
	<td class="line x" title="99:133	When an if-statement succeeds, the control is transferred to the inner block and, since there is no backtracking, no other featurevalues of the same level are tested." ></td>
	<td class="line x" title="100:133	To classify a pattern with a set-valued feature, only one value 138 Proceedings of EACL '99 from the set steers the traversal; the value that is tested first." ></td>
	<td class="line x" title="101:133	A fair policy suggests to test first the most important (probable) value, or, equivalently, to test first the value that leads to the subtree that gathered more training patterns than sibling subtrees." ></td>
	<td class="line x" title="102:133	This policy can be incarnated in the tree-traversal algorithm if we previously sort the list of feature-values tested by each non-terminal node, according to the algorithm of Figure 4, which is initialized with the root of the tree." ></td>
	<td class="line x" title="103:133	OrderSubtrees( Node N ) Begin If N is a non-terminal node Then Begin Sort the feature-values and sub-trees of node N according to the number of training pattems each sub-tree obtained; For each child node N' under node N Do OrderSubtrees( N' ); End End Figure 4." ></td>
	<td class="line x" title="104:133	Subtree-Ordering Algorithm This ordering has a nice side-effect: it increases the classification speed, as the most probable paths are ranked first in the decision tree." ></td>
	<td class="line x" title="105:133	4.4 Tree Compaction A tree induced by the algorithm of Figure 2 may contain many redundant paths from root to leaves; paths where, from a node and forward, the same decision is made." ></td>
	<td class="line x" title="106:133	The tree-traversal definitely speeds up by eliminating the tails of the paths that do not alter the decisions taken thus far." ></td>
	<td class="line x" title="107:133	This compaction does not affect the performance of the decision tree." ></td>
	<td class="line x" title="108:133	Figure 5 illustrates the tree-compaction algorithm, which is initialized with the root of the tree." ></td>
	<td class="line x" title="109:133	CompactTree( Node N ) Begin For each child node N' under node N Do Begin If N' is a leaf node Then Begin If N' has the same class label with N Then Delete N'; End Else Begin CompactTree( N' ); If N' is now a leaf node And has the same class label with N Then Delete N'; End End End Figure 5." ></td>
	<td class="line x" title="110:133	Tree-Compaction Algorithm Table 3." ></td>
	<td class="line x" title="111:133	Statistics and Evaluation Measurements POSAmbiguity Schemes Pronoun-Article 7,13 34,19 14,5 1,96 Pronoun-Article-Clitic 4,70 22,54 39,1 4,85 pron0un-Prep0sition 2,14 10,26 12,2 1,35 Adjective-Adverb 1,53 7,33 31,1 13,4 Pronoun-Clitic 1,4i 6,76 38,0 5,78 Preposition-Particle-Conjuncti0n i,~21 ~ 4,89 20,8 8,94 2'49  12,1 6,93 Verb-Noun  0<52  Adje.ctive-Ad~ erb-NOun  0,51  2,44  5!,." ></td>
	<td class="line x" title="112:133	0  30,4 Adjective-~o~ 0~,46  ~,20  38,2  18~2 Par6icie-con ~unctiOn  0,3.9  !,8.7  It3.8 1,38 Adverb-Conjunction ' 0,36  1,72, 22,.8 ' i~8,1 Pronoun-Adverb 0,34 1,63 4,31 4,31 Verb-Adverb 0,0'6  0,28 16,8 1,99 Other 0,29 1,39 30,1 12,3 Total POS Ambiguity 20,85 \[ 24,1 5,48 Unknown Words 2,53 1 38,6 15,8 Totals 23,38 25,6 6,61 139 Proceedings of EACL '99 5 Evaluation To evaluate our approach, we first partitioned the datasets described in Section 3 into training and testing sets according to the 10-fold crossvalidation methodL Then, (a) we found the most frequent POS in each training set and (b) we induced a decision tree from each training set." ></td>
	<td class="line x" title="113:133	Consequently, we resolved the ambiguity of the testing sets with two methods: (a) we assigned the most frequent POS acquired from the corresponding training sets and (b) we used the induced decision trees." ></td>
	<td class="line x" title="114:133	Table 3 concentrates the results of our experiments." ></td>
	<td class="line x" title="115:133	In detail: Column (1) shows in what percentage the ambiguity schemes and the unknown words occur in the corpus." ></td>
	<td class="line x" title="116:133	The total problematic word-tokens in the corpus are 23,38%." ></td>
	<td class="line x" title="117:133	Column (2) shows in what percentage each ambiguity scheme contributes to the total POS ambiguity." ></td>
	<td class="line x" title="118:133	Column (3) shows the error rates of method (a)." ></td>
	<td class="line x" title="119:133	Column (4) shows the error rates of method (b)." ></td>
	<td class="line x" title="120:133	To compute the total POS disambiguation error rates of the two methods (24,1% and 5,48% respectively) we used the contribution percentages shown in column (2)." ></td>
	<td class="line x" title="121:133	6 Discussion and Future Goals We have shown a uniform approach to the dual problem of POS disambiguation and unknown word guessing as it appears in M. Greek, reinforcing the argument that 'machine-learning researchers should become more interested in NLP as an application area' (Daelemans et al. , 1997)." ></td>
	<td class="line x" title="122:133	As a general remark, we argue that the linguistic approach has good performance when the knowledge or the behavior of a language can be defined explicitly (by means of lexicons, syntactic grammars, etc.), whereas empirical (corpus-based statistical) learning should apply when exceptions, complex interaction or ambiguity arise." ></td>
	<td class="line x" title="123:133	In addition, there is always the opportunity to bias empirical learning with linguistically motivated parameters, so as to 7 In this method, a dataset is partitioned 10 times into 90% training material and 10% testing material." ></td>
	<td class="line x" title="124:133	Average accuracy provides a reliable estimate of the generalization accuracy." ></td>
	<td class="line x" title="125:133	meet the needs of the specific language problem." ></td>
	<td class="line x" title="126:133	Based on these statements, we combined a highcoverage lexicon and a set of empirically induced decision trees into a POS tagger achieving ~5,5% error rate for POS disambiguation and ~16% error rate for unknown word guessing." ></td>
	<td class="line x" title="127:133	The decision-tree approach outperforms both the naive approach of assigning the most frequent POS, as well as the ~20% error rate obtained by the n-gram tagger for M. Greek presented in (Dermatas and Kokkinakis, 1995)." ></td>
	<td class="line x" title="128:133	Comparing our tree-induction algorithm and IGTREE, the algorithm used in MBT (Daelemans et al. , 1996), their main difference is that IGTREE produces oblivious decision trees by supplying an a priori ordered list of best features instead of re-computing the best feature during each branching, which is our case." ></td>
	<td class="line x" title="129:133	After applying IGTREE to the datasets described in Section 3, we measured similar performance (-7% error rate for disambiguation and -17% for guessing)." ></td>
	<td class="line x" title="130:133	Intuitively, the global search for best features performed by IGTREE has similar results to the local searches over the fragmented datasets performed by our algorithm." ></td>
	<td class="line x" title="131:133	Our goals hereafter aim to cover the following:  Improve the POS tagging results by: a) finding the optimal feature set for each ambiguity scheme and b) increasing the lexicon coverage." ></td>
	<td class="line x" title="132:133	 Analyze why IGTREE is still so robust when, obviously, it is built on less information." ></td>
	<td class="line x" title="133:133	 Apply the same approach to resolve Gender, Case, Number, etc. ambiguity and to guess such attributes for unknown words." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W99-0608
Improving POS Tagging Using Machine-Learning Techniques
Mrquez, Llus;Rodriguez Hontoria, Horacio;Carmona, Josep;Montolio, Josep;"></td>
	<td class="line x" title="1:185	Improving POS Tagging Using Machine-Learning Techniques Llufs Mhrquez 1, Horacio Rodrfguez 2, Josep Carmona 1 and Josep Montolio 1 1 TALP Research Center." ></td>
	<td class="line x" title="2:185	Dep." ></td>
	<td class="line x" title="3:185	LSI Universitat Polit~cnica de Catalunya c/Jordi Girona 1-3." ></td>
	<td class="line x" title="4:185	08034 Barcelona." ></td>
	<td class="line x" title="5:185	Catalonia lluism@l si.upc." ></td>
	<td class="line x" title="6:185	es 2 Dep." ></td>
	<td class="line x" title="7:185	IMA Universitat de Girona Horacio." ></td>
	<td class="line x" title="8:185	Rodriguez@ima." ></td>
	<td class="line x" title="9:185	udg." ></td>
	<td class="line x" title="10:185	es Abstract In this paper we show how machine learning techniques for constructing and combining several classifiers can be applied to improve the accuracy of an existing English POS tagger (MSxquez and Rodrfguez, 1997)." ></td>
	<td class="line x" title="11:185	Additionally, the problem of data sparseness is also addressed by applying a technique of generating convez pseudo-data (Breiman, 1998)." ></td>
	<td class="line x" title="12:185	Experimental results and a comparison to other state-of-theart tuggers are reported." ></td>
	<td class="line x" title="13:185	Keywords: POS Tagging, Corpus-based modeling, Decision Trees, Ensembles of Classifiers." ></td>
	<td class="line x" title="14:185	1 Introduction The study of general methods to improve the performance in classification tasks, by the combination of different individual classifiers, is a currently very active area of research in supervised learning." ></td>
	<td class="line x" title="15:185	In the machine learning (ML) literature this approach is known as ensemble, stacked, or combined classifiers." ></td>
	<td class="line x" title="16:185	Given a classification problem, the main goal is to construct several independent classifiers, since it has been proven that when the errors committed by individual classifiers are uncorrelated to a sufficient degree, and their error rates are low enough, the resulting combined classifier performs better than all the individual systems (Ali and Pazzani, 1996; Tumer and Ghosh, 1996; Dietterich, 1997)." ></td>
	<td class="line x" title="17:185	Several methods have been proposed in order to construct ensembles of classifiers that make uncorrelated errors." ></td>
	<td class="line x" title="18:185	Some of them are general, and they can be applied to any learning algorithln, while other are specific to particular algorithms." ></td>
	<td class="line x" title="19:185	From a different perspective, there exist methods for constructing homogeneous ensembles, in the sense that a unique learning algorithm has been used to acquire each individual classifier, and heterogeneous ensembles that combine different types of learning paradigms 1." ></td>
	<td class="line x" title="20:185	Impressive results have been obtained by applying these techniques on the so-called unstable learning algorithms (e.g. induction of decision trees, neural networks, rule-induction systems, etc.)." ></td>
	<td class="line x" title="21:185	Several applications to real tasks have been performed, and, regarding NLP, we find ensembles of classifiers in context-sensitive spelling correction (Golding and Roth, 1999), text categorization (Schapire and Singer, 1998; Blum and Mitchell, 1998), and text filtering (Schapire et al. , 1998)." ></td>
	<td class="line x" title="22:185	Combination of classitiers have also been applied to POS tagging." ></td>
	<td class="line x" title="23:185	For instance, van Halteren (1996) combined a number of similar tuggers by way of a straightforward majority vote." ></td>
	<td class="line x" title="24:185	More recently, two parallel works (van Halteren et al. , 1998; Brill and Wu, 1998) combined, with a remarkable success, the output of a set of four tuggers based on different principles and feature modelling." ></td>
	<td class="line x" title="25:185	Finally, in the work by MSxquez et al.(1998) the combination of taggers is used in a bootstrapping algorithm to train a part of speech tagger from a limited amount of training material." ></td>
	<td class="line x" title="27:185	The aim of the present work is to improve an existing POS tagger based on decision trees (Mkrquez and Rodriguez, 1997), by using ensembles of classifiers." ></td>
	<td class="line x" title="28:185	This tagger treats separately the different types (classes) of ambiguity by considering a different decision tree for each class." ></td>
	<td class="line x" title="29:185	This fact allows a selective construction of ensembles of decision trees focusing on the most relevant ambiguity classes, which greatly vary in size and difficulty." ></td>
	<td class="line x" title="30:185	Another goal of the present work is to try to alleviate the problem of data sparseness by applying a method, due 1An excellent survey covering all these topics call be found in (Dietterich, 1997)." ></td>
	<td class="line x" title="31:185	53 to Breiman (1998), for generating new pseudoexamples from existing data." ></td>
	<td class="line x" title="32:185	As we will see in section 4.2 this technique will be combined with the construction of an ensemble of classifiers." ></td>
	<td class="line x" title="33:185	The paper is organized as follows: we start by presenting the two versions of the POS tagger and their evaluation on the reference corpus (sections 2 and 3)." ></td>
	<td class="line x" title="34:185	Sections 4 and 5 are, respectively, devoted to present the machine-learning improvements and to test their implementation." ></td>
	<td class="line x" title="35:185	Finally, section 6 concludes." ></td>
	<td class="line x" title="36:185	2 Tree-based Taggers Decision trees have been successfully applied to a number of different NLP problems and, in particular, in POS tagging they have proven to be an efficient and compact way of capturing the relevant information for disambiguating." ></td>
	<td class="line x" title="37:185	See (MSxquez, 1999) for a broad survey on this issue." ></td>
	<td class="line x" title="38:185	In this approach to tagging, the ambiguous words in the training corpus are divided into classes corresponding to the sets of tags they can take (i.e, 'noun-adjective', 'noun-adjectiveverb', etc.)." ></td>
	<td class="line x" title="39:185	These sets are called ambiguity classes and a decision tree is acquired for each of them." ></td>
	<td class="line x" title="40:185	Afterwards, the tree-base is applied in a particular disambiguation algorithm." ></td>
	<td class="line x" title="41:185	Regarding the learning algorithm, we use a particular implementation of a top-down induction of decision trees (TDIDT) algorithm, belonging to the supervised learning family." ></td>
	<td class="line x" title="42:185	This algorithm is quite similar to the well-known CART (Breiman et al. , 1984), and C4.5 (Quinlan, 1993), but it incorporates some particularities in order to better fit the domain at hand." ></td>
	<td class="line x" title="43:185	Training examples are collected from annotated corpora and they consist of the target word to be disambiguated and some information of its local context in the sentence." ></td>
	<td class="line x" title="44:185	All words not present in the training corpus are considered unknown." ></td>
	<td class="line x" title="45:185	In principle, we have to assume that they can take any tag corresponding to open categories (i.e. , noun, proper noun, verb, adjective, adverb, cardinal, etc.), which sum up to 20 in the Penn Treebank tagset." ></td>
	<td class="line x" title="46:185	In this approach, an additional ambiguity class for unknown words is considered, and so, they are treated exactly in the same way as the other ambiguous words, except by the type of information used for acquiring the trees, which is enriched with a number of morphological features." ></td>
	<td class="line x" title="47:185	Once the tree-model has been acquired, it can be used in many ways to disambiguate a real text." ></td>
	<td class="line x" title="48:185	In the following sections, 2.1 and 2.2, we present two alternatives." ></td>
	<td class="line x" title="49:185	2.1 RTT: A Reductionistic Tree-based Tagger RTT is a reductionistic tagger in the sense of Constraint Grammars (Karlsson et al. , 1995)." ></td>
	<td class="line x" title="50:185	In a first step a word-form frequency dictionary provides each input word with all possible tags with their associated lexical probability." ></td>
	<td class="line x" title="51:185	After that, an iterative process reduces the ambiguity (discarding low probable tags) at each step until a certain stopping criterion is satisfied." ></td>
	<td class="line x" title="52:185	More particularly, at each step and for each ambiguous word (at a sentence level) the work performed in parallel is: 1) The target word is 'passed' through its corresponding decision tree; 2) The resulting probability distribution is used to multiplicatively update the probability distribution of the word; and 3) The tags with very low probabilities are filtered out." ></td>
	<td class="line x" title="53:185	For more details, we refer the reader to (Mgrquez and Rodrfguez, 1997)." ></td>
	<td class="line oc" title="54:185	2.2 STT: A Statistical Tree-based Tagger The aim of statistical or probabilistic tagging (Church, 1988; Cutting et al. , 1992) is to assign the most likely sequence of tags given the observed sequence of words." ></td>
	<td class="line o" title="55:185	For doing so, two kinds of information are used: the lexical probabilities, i.e, the probability of a particular tag conditional on the particular word, and the contextual probabilities, which describe the probability of a particular tag conditional on the surrounding tags." ></td>
	<td class="line x" title="56:185	Contextual (or transition) probabilities are usually reduced to the conditioning of the preceding tag (bigrams), or pair of tags (trigrams), however, the general formulation allows a broader definition of context." ></td>
	<td class="line x" title="57:185	In this way, the set of acquired statistical decision trees can be seen as a compact representation of a rich contextual model, which can be straightforwardly incorporated inside a statistical tagger." ></td>
	<td class="line x" title="58:185	The point here is that the context is not restricted to the n-1 preceding tags as in the n-gram formulation." ></td>
	<td class="line x" title="59:185	Instead, it is extended to all the contexS4 tual information used for learning the decision trees." ></td>
	<td class="line x" title="60:185	The Viterbi algorithm (described for instance in (Deaose, 1988)),." ></td>
	<td class="line x" title="61:185	in which n-gram probabilities are substituted by the application of the corresponding decision trees, allows the calculation of the most-likely sequence of tags with a linear cost on the sequence length." ></td>
	<td class="line x" title="62:185	However, one problem appears when applying conditionings on the right context of the target word, since the disambiguation proceeds from left to right and, so, the right hand side words may be ambiguous." ></td>
	<td class="line x" title="63:185	Although dynamic programming can be used to Calculate the most likely sequence of tags to the right (in a forward-backward approach), we use a simpler approach which consists of calculating the contextual probabilities by a weighted average of all possible tags for the right context." ></td>
	<td class="line x" title="64:185	Additionally, the already presented tagger allows a straightforward incorporation of n-gram probabilities, by linear interpolation, in a backoff approach including, from most general to most specific, unigrams, bigrams, trigrams and decision trees." ></td>
	<td class="line x" title="65:185	From now on, we will refer to STT as STT + when using n-gram information." ></td>
	<td class="line x" title="66:185	Due to the high ambiguity of unknown words, their direct inclusion in the statistical tagger would result in a severe decreasing of performance." ></td>
	<td class="line x" title="67:185	To avoid this situation, we apply the tree for unknown words in a pre-process for filtering low probable tags." ></td>
	<td class="line x" title="68:185	In this way, when entering to the tagger the average number of tags per unknown word is reduced from 20 to 3.1." ></td>
	<td class="line x" title="69:185	3 Evaluation of the Taggers 3.1 Domain of Application We have used a portion of about 1,17 Mw of the Wall Street Journal (WSJ) corpus, tagged according to the Penn Treebank tag set (45 different tags)." ></td>
	<td class="line x" title="70:185	The corpus has been randomly partitioned into two subsets to train (85%) and test (15%) the system." ></td>
	<td class="line x" title="71:185	See table 1 for some details about the used corpus." ></td>
	<td class="line x" title="72:185	The training corpus has been used to create a word form lexicon --of 45,469 entries-with the associated lexical probabilities for each word." ></td>
	<td class="line x" title="73:185	The training corpus contains 239 different ambiguity classes, with a number of examples ranging from few dozens to several thousands (with a maximum of 34,489 examples for the preposition-adverb-particle ambiguity)." ></td>
	<td class="line x" title="74:185	It is noticeable that only the 36 most frequent ambiguity classes concentrate up to 90% of the ambiguous occurrences of the training corpus." ></td>
	<td class="line x" title="75:185	Table 2 contains more information about the number of ambiguity classes necessary to cover a concrete percentage of the training corpus." ></td>
	<td class="line x" title="76:185	Training examples for the unknown-word ambiguity class were collected from the training corpus in the following way: First, the training corpus is randomly divided into twenty parts of equal size." ></td>
	<td class="line x" title="77:185	Then, the first part is used to extract the examples which do not occur in the remaining nineteen parts, that is, taking the 95% of the corpus as known and the remaining 5% to extract the examples." ></td>
	<td class="line x" title="78:185	This procedure is repeated with each of the twenty parts, obtaining approximately 22,500 examples from the whole corpus." ></td>
	<td class="line x" title="79:185	The choice of dividing by twenty is not arbitrary." ></td>
	<td class="line x" title="80:185	95%-5% is the proportion that results in a percentage of unknown words very similar to the test set (i.e. , 2.25%) 2 . Finally, the test set has been used as completely fresh material to test the taggers." ></td>
	<td class="line x" title="81:185	All results on tagging accuracy reported in this paper have been obtained against this test set." ></td>
	<td class="line x" title="82:185	3.2 Results In this experiment we used six basic discretevalued features to disambiguate know n ambiguous words, which are: the part-of-speech tags of the three preceding and two following words, and the orthography of the word to be disambiguated." ></td>
	<td class="line x" title="83:185	For tagging unknown words, we used 20 attributes that can be classified into three groups:  Contextual information: part-of-speech tags of the two preceding and following words." ></td>
	<td class="line x" title="84:185	 Orthographic and Morphological information (about the target word): prefixes (first two symbols) and suffixes (last three symbols); Length; Multi-word?" ></td>
	<td class="line x" title="85:185	Capitalized?" ></td>
	<td class="line x" title="86:185	Other capital letters?" ></td>
	<td class="line x" title="87:185	Numerical characters?" ></td>
	<td class="line x" title="88:185	Contain dots?" ></td>
	<td class="line x" title="89:185	 Dictionary-related information: Does the target word contains any known word as a prefix (or a suffix)?" ></td>
	<td class="line x" title="90:185	Is the target word :See (Mrquez, 1999) for a discussion on the appropriateness of this procedure." ></td>
	<td class="line x" title="91:185	55 Training Test S W W/S AW T/W T/AW T/DW U 40,977 998,354 24.36 339,916(34.05%) 1.48 2.40 --7,167 175,412 24.47 59,440 (33.89%) 1.45 2.40 3.49 3,941 (2.25%) Total 48,144 1,173,766 24.38 399,356 (34.02%) 1.47 2.40 --Table 1: Information about the WSJ training and test corpora." ></td>
	<td class="line x" title="92:185	S: number of sentences; W: number of words; W/S: average number of words per sentence; AW: number and percentage of ambiguous words; T/W: average number of tags per word; T/AW: average number of tags per ambiguous t:nown word; T/DW: average number of tags per ambiguous word (including unknown words); and U: number and percentage of Unknown words Classes I 8 11 14 18 36 57 111 239 J Table 2: Number of ambiguity classes that cover the x% of the ambiguous words of the training corpus the prefix (or the suffix) of any word in the lexicon?" ></td>
	<td class="line x" title="93:185	The last group of features are inspired in those applied by Brill (1995) when addressing unknown words." ></td>
	<td class="line x" title="94:185	The learning algorithm 3 acquired, in about thirty minutes, a base of 191 trees (the other ambiguity classes had not enough examples) which required about 0,68 Mb of storage." ></td>
	<td class="line x" title="95:185	The results of the taggers working with this tree-base is presented in table 3." ></td>
	<td class="line x" title="96:185	MFT stands for a baseline most-frequent-tag tagger." ></td>
	<td class="line x" title="97:185	RTT, STT, and STT + stand for the basic versions of the taggers presented in section 2." ></td>
	<td class="line x" title="98:185	The overall accuracy is reported in the first column." ></td>
	<td class="line x" title="99:185	Columns 2, 3, and 4 contain the tagging accuracy on some specific groups of words: unknown words, ambiguous words (excluding unknown words) and known words which is the complementary of the set of unknown words." ></td>
	<td class="line x" title="100:185	Column 5 shows the speed of each tagger 4 and, finally, the 'Memory' column reflects the size of the used language model (the lexicon is not considered)." ></td>
	<td class="line x" title="101:185	Three main conclusions can be extracted:  RTT and STT approaches obtain almost the same results in accuracy, however RTT is faster." ></td>
	<td class="line x" title="102:185	ZThe programs were implemented using PERL-5.0 and they were run on a SUN UltraSparc2 machine with 194Mb of RAM." ></td>
	<td class="line x" title="103:185	4More than absolute figures what is important here is the performance of each tagger relative to the others." ></td>
	<td class="line x" title="104:185	5TT obtains better results when it incorporates bigrams and trigrams, with a slight time-space penalty." ></td>
	<td class="line x" title="105:185	The accuracy of all taggers is comparable to the best state-of-the art taggers under the open vocabulary assumption (see section 5.2)." ></td>
	<td class="line x" title="106:185	4 Machine-Learning-based Improvements Our purpose is to improve the performance on two types of ambiguity classes, namely: Most frequent ambiguity classes." ></td>
	<td class="line x" title="107:185	We focused on the 26 most representative classes, which concentrate the 86% of the ambiguous occurrences." ></td>
	<td class="line x" title="108:185	From these, eight (24.1%) were already resolved at almost 100% of accuracy, while the remaining eighteen (61.9%) left some room for improvement." ></td>
	<td class="line x" title="109:185	Section 4.1 explain which methods have been applied to construct ensembles for these eighteen classes plus the unknown-word ambiguity class." ></td>
	<td class="line x" title="110:185	Ambiguity classes with few examples." ></td>
	<td class="line x" title="111:185	We considered the set of 82 ambiguity classes with a number of examples between 50 and 3,000 and an accuracy rate lower than 95%." ></td>
	<td class="line x" title="112:185	They agglutinate 48,322 examples (14.24% of the total ambiguous occurrences)." ></td>
	<td class="line x" title="113:185	Section 4.2 explains the applied method to increase the number of examples of these classes." ></td>
	<td class="line x" title="114:185	56 i Tagger MFT RTT STT . STT + Overall Known Ambiguous Unknown Speed Memory 92.75% 94.25% 83.40% 27.43% 2818 w/s 0 Mb 96.61% 97.01% 91.36% 79.22% 426 w/s 0.68 Mb 96.63% 97.02% 91.40% 79.60% ~ 321 w/s 0.68 Mb 96.84% 97.21% 91.95% 80.70% ! 302 w/s 0.90 Mb Table 3: Tagging accuracy, speed, and storage requirement of RTT and STT taggers 4.1 Ensembles of Decision Trees The general methods for constructing ensembles of classifiers are based on four techniques: 1) Resampling the training data, e.g. Boosting (Freund and Schapire, 1995), Bagging (Breiman, 1996), and Cross-validated Committees (Parmanto et al. , 1996); 2) Combining different input features (Cherkauer, 1996; Tumer and Ghosh, 1996); 3) Changing output representation, e.g. ECOC (Dietterich and Bakiri, 1995) and PWC-CC (Moreira and Mayoraz, 1998); and 4) Injecting randomness (Dietterich, 1998)." ></td>
	<td class="line x" title="115:185	We tested several of the preceding methods on our domain." ></td>
	<td class="line x" title="116:185	Below, we briefly describe those that reported major benefits." ></td>
	<td class="line x" title="117:185	4.i.1 Bagging (BAG) From a training set of n examples, severaI samples of the same size are extracted by randomly drawing, with replacement, n times." ></td>
	<td class="line x" title="118:185	Such new training sets are called bootstrap replicates." ></td>
	<td class="line x" title="119:185	In each replicate, some examples appear multiple times, while others do not appear." ></td>
	<td class="line x" title="120:185	A classifier is induced from each bootstrap replicate and then they are combined in a voting approach." ></td>
	<td class="line x" title="121:185	The technique is called bootstrap aggregation, from which the acronym bagging is derived." ></td>
	<td class="line x" title="122:185	In our case, the bagging approach was performed following the description of Breiman (1996), constructing 10 replicates for each data set 5." ></td>
	<td class="line x" title="123:185	4.1.2 Combining Feature Selection Criteria, (FSC) In this case, the idea is to obtain different classifiers by applying several different functions for feature selection inside the tree induction algorithm." ></td>
	<td class="line x" title="124:185	In particular, we have selected a set of seven functions that achieve a similar accuracy, namely: Gini Impurity Index, Information Gain and Gain Ratio, Chi-square statistic (X2), Symmetrical Tau criterion, RLM (a distance-based method), and a version of RELIEF-F which uses the Information Gain function to assign weights to the features." ></td>
	<td class="line x" title="125:185	The first five are described, for instance, in (Sestito and Dillon, 1994), RLM is due to LSpez de Mntaras (1991), and, finally, RELIEF-F is described in (Kononenko et al. , 1995)." ></td>
	<td class="line x" title="126:185	Since the applied feature selection functions are based on different principles, we expect to obtain biased classifiers with complementary information." ></td>
	<td class="line x" title="127:185	4.1.3 Combining Features (FCOMB) We have extended the basic set of six features with lexical information about words appearing in the local context of the target word, and with the ambiguity classes of the same words." ></td>
	<td class="line x" title="128:185	In this way, we consider information about the surrounding words at three different levels of specificity: word form, POS tag, and ambiguity class." ></td>
	<td class="line x" title="129:185	Very similar to Brill's lexical patterns (Brill, 1995), we also have included features to capture collocational information." ></td>
	<td class="line x" title="130:185	Such features are obtained by composition of the already described single attributes and they are sequences of contiguous words and/or POS tags (up to three items)." ></td>
	<td class="line x" title="131:185	The resulting features were grouped according to their specificity to generate ensembles of eight trees 6." ></td>
	<td class="line x" title="132:185	The idea here is that specific information (lexical attributes and collocational patterns) would produce classifiers that cover concrete cases (hopefully, with a high precision), while more general information (POS tags) would produce more general (but probably less precise) trees." ></td>
	<td class="line x" title="133:185	The combination of both type of trees should perform better because of the complementarity of the information." ></td>
	<td class="line x" title="134:185	5Several authors indicate that most of the potential improvement provided by bagging is obtained within the first ten replicates." ></td>
	<td class="line x" title="135:185	6The features for dealing with unknown words were combined in a similar way to create ensembles of 10 trees." ></td>
	<td class="line x" title="136:185	For details, see (Mrquez, 1999)." ></td>
	<td class="line x" title="137:185	57 4.2 Generating Pseudo-Examples (CPD) Breiman (1998), describes a simple and effective method for generating new pseudo-examples fl'om existing data and incorporating them into a tree-based learning algorithm to increase prediction accuracy in domains with few training exalnples." ></td>
	<td class="line x" title="138:185	We call this method CPD (standing for generation of Convex Pseudo-Data)." ></td>
	<td class="line x" title="139:185	The method for obtaining new data from the old is similar to the process of gene combination to create new generations in genetic algorithms." ></td>
	<td class="line x" title="140:185	First, two examples of the same class are selected at random from the training set." ></td>
	<td class="line x" title="141:185	Then, a new example is generated from them by selecting attributes from one or another parent according to a certain probability." ></td>
	<td class="line x" title="142:185	This probability depends on a single generation parameter (a real number between 0 and 1), which regulates the amount of change allowed in the combination step." ></td>
	<td class="line x" title="143:185	In the original paper, Breiman does not propose any optimization of the generation parameter, instead, he performs a limited amount of trials with different values and simply reports the best result." ></td>
	<td class="line x" title="144:185	In our domain, we observed a big variance on the results depending on the concrete values of the generation parameter." ></td>
	<td class="line x" title="145:185	Instead of trying to tune it, we generate several training sets using different values of the generation parameter and we construct an ensemble of decision trees." ></td>
	<td class="line x" title="146:185	In this way, we make the global classifier independent of the particular choice, and we generally obtain a combined result which is more accurate than any of the individuals." ></td>
	<td class="line x" title="147:185	5 Experiments and Results 5.1 Constructing and Evaluating Ensembles First, the three types of ensembles were applied to the 19 selected ambiguity classes in order to decide which is the best in each case." ></td>
	<td class="line x" title="148:185	The evaluation was performed by means of a 10-fold cross-validation,using the training corpus." ></td>
	<td class="line x" title="149:185	The obtained results confirm that all methods contribute to improve accuracy in almost all domains." ></td>
	<td class="line x" title="150:185	The absolute improvement is not very impressive but the variance is generally very low and, so, the gain is statistically significant in the majority of cases." ></td>
	<td class="line x" title="151:185	Summarizing, BAG wins in 8 cases, FCOMB in 9, and FSC in 2 (including the unknown-word class)." ></td>
	<td class="line x" title="152:185	These results are reported in table 4, in which the error rate of a single basic tree is compared to the results of the ensembles for each ambiguity class r. The last column presents the percentage of error reduction for the best method in each row." ></td>
	<td class="line x" title="153:185	Second, CPD was applied to the 82 selected ambiguity classes, with positive results in 59 cases, from which 25 were statistically significant (again in a 10-fold cross-validation experiment)." ></td>
	<td class="line x" title="154:185	These 25 classes agglutinate 20,937 examples and the error rate was diminished, o11 average, from 20.16% to 18.17%." ></td>
	<td class="line x" title="155:185	5.2 Tagging with the Enriched Model Ensembles of classifiers were learned for the ambiguity classes explained in the previous sections using the best technique in each case." ></td>
	<td class="line x" title="156:185	These ensembles were included in the tree-base, used by the basic taggers of section 3, substituting the corresponding individual trees, and both taggers were tested again using the enriched model." ></td>
	<td class="line x" title="157:185	At runtime, the combination of classifiers was done by averaging the results of each individual decision tree." ></td>
	<td class="line x" title="158:185	In order to test the relative improvement of each component, the inclusion of the ensembles is performed in three steps: 'CPD ~ stands for the ensembles for infrequent ambiguity classes, 'ENS' stands for the ensembles for frequent ambiguity classes and unknown words, and 'CPD-~ENS' stands for the inclusion of both." ></td>
	<td class="line x" title="159:185	Results are described in table 5." ></td>
	<td class="line x" title="160:185	Some important conclusions are:  The best result of each tagger is significantly better than each corresponding basic version, and the accuracy consistently grows as more components are added." ></td>
	<td class="line x" title="161:185	 The relative improvement of STT + is lower than those of RTT and STT, suggesting than the better the tree-based model is, the less relevant is the inclusion of n-gram information." ></td>
	<td class="line x" title="162:185	 The special treatment of low frequent ambiguity classes results in a very small contribution, indicating that there is no much 7These figures are calculated by averaging the resu|ts of the ten folds." ></td>
	<td class="line x" title="163:185	58 A-class #exs IN-RB-RP i 2 VBD-VBN 3 NN-VB-VBP 4 VB-VBP 5 JJ-NN 6 NNS-VBZ I 7 NN-NNP 8 JJ-VBD-VBN 9 NN-VBG I0 JJ-NNP ii JJ-RB 12 DT-IN-RB-WDT 13 JJR-RBR 14 NNP-NNPS-NNS 15 JJ-NN-RB 16 JJ-NN-VB 17 JJ-NN-VBG 18 JJ-VBG Total 19 unknown-word 34,489 25,882 24,522 17,788 17,077 15,295 13,824 11,403 9,597 8 724 8 722 8 419 2 868 2 808 2 625 2 145 1.986 1.980 210.154 22.594 %exs 10.16% 7.63% 7.23% 5.24% 5.03% 4.51% 4.07% 3.36% 2.83% 2.57% 2.57% 2.48% o.85% 0.83% 0.77% 0.63% 0.59% 0.58% 61.93% B asic 8.30% 7.44% 4.10% 4.13% 14.71% 5.14% 9.67% 19.18% 14.11% 5.10% 10.45% 7.01% 16.40% 36.50% i5.31% 13.32% 20.30% 21.11% 9.35% 20.87% BAG 7.31% 5.93% 3.7o% 3.62% 13.30% 4.37% 9.10% 17.91% 12.53% 4.5o% 8.86% 6.49% 15.84% 36.50% 13.32% 13.87% 17.98% 18.89% 8.38% 17,47% FSC 7.79% 6.64% 3.84% 3.94% 13.50% 4.59% 8.37% 18.05% 12.93% 4.56% 9.75% 6.84% 15.28% 35.14% 11.83% 12.99% 18.79% 19.39% 8.61% 16.86% FCOMB 7.23% 6.28% 3.58% 3.76% 13.55% 4.34% 6.83% 17.27% 12.99% 4.35% 9.68% 6.53% 14.72% 35.oo% 12.44% 12.75% 18.23% 19.60% 8.25% 17.21% BestER 12.89% 20.30% 12.68% 12.35% 9.59% 15.56% 29.37% 9.96% 11.20% 14.71% 15.22% 7.42% 10.24% 4.11% 22.73% 4.28% 11.43% 10.52% 13.40% 19.26% Table 4: classes Comparative results (error rates) of different ensembles on the most significant ambiguity Tagger RTT RTT(cPD) RTT(ENS) RTT(cPD+ENS) STT STT(cPD) STT(ENs) STT(cPD+ENS) STT + STT+(cPD) STT+(ENS) 5TT+(CPD+ENS) Overall Known Ambig." ></td>
	<td class="line x" title="164:185	Unknown 96.61% 97.00% 91.36% 79.21% 96.66% 97.06% 91.51% 79.25% 96.99% 97.30% 92.23% 83.25% 97.05% 97.37% 92.48% 83.30% 96.63% 97.02% 91.40% 79.60% 96.69% 97.07% 91.56% 79.69% 97.05% 97.36% 92.38% 83.78% 97.10% 97.40% 92.51% 83.68% 96.84% 97.21% 91.95% 80.70% 96.88% 97.25% 92.09% 80.77% 97.19% 97.48% 92.73% 84.47% 97.22% 97.51% 92.81% 84.54% Speed Memory 426 w/s 0.68Mb 366 w/s 0.93Mb 97 w/s 3.53Mb 89 w/s 3.78Mb 321 w/s 0.68Mb 261 w/s 0.93Mb 70 w/s 3.53Mb 64 w/s 3.78Mb 302 w/s 0.90Mb 235 w/s 1.15Mb 65 w/s 3.75Mb 60 w/s 3.97Mb Table 5: Tagging accuracy, speed, and storage requirements of enriched RTT and 5TT taggers to win from these classes, unless we were able to fix their errors in a much greater proportion than we really did." ></td>
	<td class="line x" title="165:185	 The price to pay for the enriched models is a substantial overhead in storage requirement and speed decreasing, which in the worst case is divided by 5." ></td>
	<td class="line x" title="166:185	In order to compare our results to others, we list in table 6 the results reported by several state-of-the-art PO5 taggers, tested on the WSJ corpus with the open vocabulary assumption." ></td>
	<td class="line x" title="167:185	In that table, TBL stands for Brill's transformation-based error-driven tagget (Brill, 1995), ME stands for a tagger based on the maimum entropy modelling (Ratnaparkhi, 1996), SPATTER stands for a statistical parser based on decision trees (Magerman, 1996), IGTREE stands for the memory-based tagger by Daelemans et al.(1996), and, finally, TComb stands for a tagger that works by combination of a statistical trigram-based tagger, 59 Tagger TBL ME SPATTER IGTREE TComb STT+(CPD+ENS) Train Test 950 Kw 150 Kw 963 Kw 193 Kw ~975 Kw 47 Kw 2,000 kw 200 Kw 1,i00 Kw 265 Kw Overall Known Unknown 96.6% -82.2% 96.5% -86.2% 96.5% --96.4% 96.7% 90.6% 97.2% --Ambig 998 Kw 175 Kw 97.2% 97.5% 84.5% 92.8% Table 6: Comparison of different tuggers on the WSJ corpus TBL and ME (Brill and Wu, 1998)." ></td>
	<td class="line x" title="169:185	Comparing to all the individual tuggers we observe that our approach reports the highest accuracy, and that it is comparable to that of YComb obtained by the combination of three tuggers." ></td>
	<td class="line x" title="170:185	This is encouraging, since we have improved an individual POS tagger which could be further introduced as a better component in an ensemble of tuggers." ></td>
	<td class="line x" title="171:185	Unfortunately, the performance on unknown words is difficult to compare, since it strongly depends on the used lexicon." ></td>
	<td class="line x" title="172:185	For instance, IGTREE does not include in the lexicon the numbers appearing in the training set, and, so, any number in the test set is considered unknown (they report an unusually high percentage of Unknown words: 5.5% compared to our 2.25%)." ></td>
	<td class="line x" title="173:185	The fact that numbers are very easy to recognize could explain their outstanding results on tagging unknown words." ></td>
	<td class="line x" title="174:185	ME also reports a higher percentage of unknown words, 3.2%,  while TBL says nothing about this issue." ></td>
	<td class="line x" title="175:185	6 Conclusions and Further Work In this paper, we have applied several ML techniques for constructing ensembles of classifiers to address the most representative and/or difficult cases of ambiguity within a decision-treebased English POS tagger." ></td>
	<td class="line x" title="176:185	As a result, the overall accuracy has been significantly improved." ></td>
	<td class="line x" title="177:185	Comparing to other approaches, we see that our tagger performs better on the WSJ corpus and under the open vocabulary assumption, than a number of state-of-the-art POS tuggers, and similar to another approach based on the combination of several tuggers s. 8However, it has to be said that the pure statistical or machine-learning based approaches to POS tagging still significantly underperform some sophisticated manually constructed systems, such as the English shallow parser based on Constraint Grammars developed at the Helsinki University (Samuelsson and Voutilainen, 1997)." ></td>
	<td class="line x" title="178:185	The cost of this improvement has been quantiffed in terms of storage requirement and speed of the resulting enriched tuggers." ></td>
	<td class="line x" title="179:185	Of course, there exists a clear tradeoff between accuracy and efficiency which should be resolved on the basis of the user needs." ></td>
	<td class="line x" title="180:185	Although all proposed techniques are fully automatic, it has to be said that the construction of appropriate ensembles requires a significant human and computational effort." ></td>
	<td class="line x" title="181:185	There are several features that should be further studied with respect to the used methods for constructing the ensembles of decision trees, the way they are combined and included in the tuggers, etc. However, we are now more interested on experimenting with the inclusion of our tagger as a component in an ensemble of preexisting tuggers, in the style of (Brill and Wu, 1998; van Halteren et al. , 1998)." ></td>
	<td class="line x" title="182:185	More generally, one may think that, after all the involved effort, the achieved improvement seems small." ></td>
	<td class="line x" title="183:185	On this particular, we think that we are moving very close to the best achievable results using fully statistically-based techniques, and that some kind of specific human knowledge should be jointly considered in order to achieve the next qualitative step." ></td>
	<td class="line x" title="184:185	We also think that other issues than simply 'accuracy rates' are becoming more important in order to test and evaluate the real utility of different approaches for tagging." ></td>
	<td class="line x" title="185:185	Such aspects, that should be studied in the near future, refer to the ability of adapting to new domains (tuning), the types of errors committed and their influence on the task at hand, the language independence assumption, etc. Acknowledgments This research has been partially funded by the Spanish Research Department (CICYT's ITEM project TIC96-1243-C03-02), by the EU Corn60 mission (EuroWordNet LE4003) and by the Catalan Research Department (CIRIT's consolidated research group 1997SGR 00051, and CREL project)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="A00-1026
Extracting Molecular Binding Relationships From Biomedical Text
Rindflesch, Thomas C.;Rajan, Jayant V.;Hunter, Lawrence;"></td>
	<td class="line x" title="1:141	Extracting Molecular Binding Relationships from Biomedical Text Thomas C. RINDFLESCH National Library of Medicine 8600 Rockville Pike Bethesda, MD, 20894 tcr@nlm.nih.gov Jayant V. RAJAN University of Rochester Rochester, NY, 14620 Jayant.Rajan@ mc.rochester.edu Lawrence HUNTER National Cancer Institute 7550 Wisconsin Avenue Bethesda, MD, 20894 lhunter@nih.gov Abstract ARBITER is a Prolog program that extracts assertions about macromolecular binding relationships from biomedical text." ></td>
	<td class="line x" title="2:141	We describe the domain knowledge and the underspecified linguistic analyses that support the identification of these predications." ></td>
	<td class="line x" title="3:141	After discussing a formal evaluation of ARBITER, we report on its application to 491,000 MEDLINE ~ abstracts, during which almost 25,000 binding relationships suitable for entry into a database of macromolecular function were extracted." ></td>
	<td class="line x" title="4:141	Introduction Far more scientific information exists in the literature than in any structured database." ></td>
	<td class="line x" title="5:141	Convenient access to this information could significantly benefit research activities in various fields." ></td>
	<td class="line x" title="6:141	The emerging technology of information extraction (Appelt and Israel 1997, Hearst 1999) provides a means of gaining access to this information." ></td>
	<td class="line x" title="7:141	In this paper we report on a project to extract biomolecular data from biomedical text." ></td>
	<td class="line x" title="8:141	We concentrate on molecular binding affinity, which provides a strong indication of macromolecular function and is a core phenomenon in molecular biology." ></td>
	<td class="line x" title="9:141	Our ultimate goal is to automatically construct a database of binding relationships asserted in MEDLINE citations." ></td>
	<td class="line x" title="10:141	The National Library of Medicine's MEDLINE textual database is an online repository of more than 10 million citations from the biomedical literature." ></td>
	<td class="line x" title="11:141	All citations contain the title of the corresponding article along with other bibliographic information." ></td>
	<td class="line x" title="12:141	In addition, a large number of citations contain author-supplied abstracts." ></td>
	<td class="line x" title="13:141	Initial studies indicate that there are approximately 500,000 MEDLINE citations relevant to molecular binding affinity." ></td>
	<td class="line x" title="14:141	Our decision to apply information extraction technology to binding relationships was guided not only by the biological importance of this phenomenon but also by the relatively straightforward syntactic cuing of binding predications in text." ></td>
	<td class="line x" title="15:141	The inflectional forms of a single verb, bind, indicate this relationship in the vast majority of cases, and our initial work is limited to these instances." ></td>
	<td class="line x" title="16:141	For example, our goal in this project is to extract the binding predications in (2) from the text in (1)." ></td>
	<td class="line x" title="17:141	(1) CC chemokine receptor 1 (CCR1) is expressed in neutrophils, monocytes, lymphocytes, and eosinophils, and binds the leukocyte chemoattractant and hematopoiesis regulator macrophage inflammatory protein (MIP)1 alpha, as well as several related CC chemokines." ></td>
	<td class="line x" title="18:141	(2) <CC chemokine receptor 1> BINDS <leukocyte chemoattractant> <CC chemokine receptor 1> BINDS <hematopoiesis regulator macrophage inflammatory protein1 alpha> <CC chemokine receptor 1> BINDS <related CC chemokine> Considerable interest in information extraction has concentrated on identifying named entities in text pertaining to current events (for example, Wacholder et al. 1997, Voorhees and Harman 1998, and MUC-7); however, several recent efforts have been directed at biomolecular data (Blaschke et al. 1999, Craven and Kumlien 1999, and Rindflesch et al. 2000, for example)." ></td>
	<td class="line x" title="19:141	The overall goal is to transform the information 188 encoded in text into a more readily accessible tbrmat, typically a template with slots named for the participants in the scenario of interest." ></td>
	<td class="line x" title="20:141	The template for molecular binding can be thought of as a simple predication with predicate 'bind' and two arguments which participate (symmetrically) in the relationship: BINDS(<X>, <Y>)." ></td>
	<td class="line x" title="21:141	Various strategies, both linguistic and statistical, have been used in information extraction efforts." ></td>
	<td class="line x" title="22:141	We introduce a Prolog program called ARBITER (Assess and Retrieve Binding Terminology) that takes advantage of an existing domain knowledge source and relies on syntactic cues provided by a partial parser in order to identify and extract binding relations from text." ></td>
	<td class="line x" title="23:141	We discuss the syntactic processing used and then report on a formal evaluation of ARBITER against a test collection of 116 MEDLINE citations in which the binding relations were marked by hand." ></td>
	<td class="line x" title="24:141	Finally, we provide a brief overview of the results of applying ARBITER to the 500,000 MEDLINE citations discussing molecular binding affinity." ></td>
	<td class="line x" title="25:141	1 Extracting Binding Relationships from Text Our strategy for extracting binding relationships from text divides the task into two phases: During the first phase we identify all potential binding arguments, and then in the second phase we extract just those binding terms which are asserted in the text as participating in a particular binding predication." ></td>
	<td class="line x" title="26:141	In support of this processing, we rely on the linguistic and domain knowledge contained in the National Library of Medicine's Unified Medical Language System ~ (UMLS ) as well an existing tool, the SPECIALIST minimal commitment parser (Aronson et al. 1994)." ></td>
	<td class="line x" title="27:141	The UMLS (Humphreys et al. 1998) consists of several knowledge sources applicable in the biomedical domain: the Metathesaums, Semantic Network, and SPECIALIST Lexicon (McCray et al. 1994)." ></td>
	<td class="line x" title="28:141	The Metathesaurus was constructed from more than forty controlled vocabularies and contains more than 620,000 biomedical concepts." ></td>
	<td class="line x" title="29:141	The characteristic of the Metathesaurus most relevant for this project is that each concept is associated with a semantic type that categorizes the concept into subareas of biology or medicine." ></td>
	<td class="line x" title="30:141	Examples pertinent to binding terminology include the semantic types 'Amino Acid, Peptide, or Protein' and 'Nucleotide Sequence'." ></td>
	<td class="line x" title="31:141	The SPECIALIST Lexicon (with associated lexical access tools) supplies syntactic information for a large compilation of biomedical and general English terms." ></td>
	<td class="line oc" title="32:141	The SPECIALIST minimal commitment parser relies on the SPECIALIST Lexicon as well as the Xerox stochastic tagger (Cutting et al. 1992)." ></td>
	<td class="line x" title="33:141	The output produced is in the tradition of partial parsing (Hindle 1983, McDonald 1992, Weischedel et al. 1993) and concentrates on the simple noun phrase, what Weischedel et al.(1993) call the 'core noun phrase,' that is a noun phrase with no modification to the right of the head." ></td>
	<td class="line x" title="35:141	Several approaches provide similar output based on statistics (Church 1988, Zhai 1997, for example), a finite-state machine (AitMokhtar and Chanod 1997), or a hybrid approach combining statistics and linguistic rules (Voutilainen and Padro 1997)." ></td>
	<td class="line x" title="36:141	The SPECIALIST parser is based on the notion of barrier words (Tersmette et al. 1988), which indicate boundaries between phrases." ></td>
	<td class="line o" title="37:141	After lexical look-up and resolution of category label ambiguity by the Xerox tagger, complementizers, conjunctions, modals, prepositions, and verbs are marked as boundaries." ></td>
	<td class="line x" title="38:141	Subsequently, boundaries are considered to open a new phrase (and close the preceding phrase)." ></td>
	<td class="line x" title="39:141	Any phrase containing a noun is considered to be a (simple) noun phrase, and in such a phrase, the right-most noun is labeled as the head, and all other items (other than determiners) are labeled as modifiers." ></td>
	<td class="line x" title="40:141	An example of the output from the SPECIALIST parser is given below in (4)." ></td>
	<td class="line x" title="41:141	The partial parse produced serves as the basis for the first phase of extraction of binding relationships, namely the identification of those simple noun phrases acting as potential binding arguments (referred to as 'binding terms')." ></td>
	<td class="line x" title="42:141	1.1 Identifying binding terminology In order to identify binding terminology in text we rely on the approach discussed in (Rindfiesch et al. 1999)." ></td>
	<td class="line o" title="43:141	Text with locally-defined acronyms expanded is submitted to the Xerox tagger and the SPECIALIST parser." ></td>
	<td class="line x" title="44:141	Subsequent processing concentrates on the heads of simple noun 1RQ 189 phrases and proceeds in a series of cascaded steps that depend on existing domain knowledge as well as several small, special-purpose resources in order to determine whether each noun phrase encountered is to be considered a binding term." ></td>
	<td class="line x" title="45:141	As the first step in the process, an existing program, MetaMap, (Aronson et al. 1994) attempts to map each simple noun phrase to a concept in the UMLS Metathesaurus." ></td>
	<td class="line x" title="46:141	The semantic type for concepts corresponding to successfully mapped noun phrases is then checked against a small subset of UMLS semantic types referring to bindable entities, such as 'Amino Acid, Peptide, or Protein', 'Nucleotide Sequence', 'Carbohydrate', 'Cell', and 'Virus'." ></td>
	<td class="line x" title="47:141	For concepts with a semantic type in this set, the corresponding noun phrase is considered to be a binding term." ></td>
	<td class="line x" title="48:141	The heads of noun phrases that do not map to a concept in the Metathesaurus are tested against a small set of general 'binding words,' which often indicate that the noun phrase in which they appear is a binding term." ></td>
	<td class="line x" title="49:141	The set of binding words includes such nouns as cleft, groove, membrane, ligand, motif, receptor, domain, element, and molecule." ></td>
	<td class="line x" title="50:141	The head of a noun phrase that did not submit to the preceding steps is examined to see whether it adheres to the morphologic shape of a normal English word." ></td>
	<td class="line x" title="51:141	In this context such a word is often an acronym not defined locally and indicates the presence of a binding term (Fukuda et al. 1998)." ></td>
	<td class="line x" title="52:141	A normal English word has at least one vowel and no digits, and a text token that contains at least one letter and is not a norreal English word functions as a binding word in this context." ></td>
	<td class="line x" title="53:141	The final step in identifying binding terms is to join contiguous simple noun phrases qualifying as binding terms into a single macro-noun phrase." ></td>
	<td class="line x" title="54:141	Rindflesch et al.(1999) use the term 'macro-noun phrase' to refer to structures that include reduced relative clauses (commonly introduced by prepositions or participles) as well as appositives." ></td>
	<td class="line x" title="56:141	Two binding terms joined by a form of be are also treated as though they formed a macro-noun phrase, as in Jel42 is an IgG which binds The results of identifying binding terms (and thus potential binding arguments) are given in (4) for the sentence in (3)." ></td>
	<td class="line x" title="57:141	In (4) evidence supporting identification as a binding term is given in braces." ></td>
	<td class="line x" title="58:141	Note that in the underspecified syntactic analysis, prepositional phrases are treated as (simple) noun phrases that have a preposition as their first member." ></td>
	<td class="line x" title="59:141	(3) Je142 is an IgG which binds to the small bacterial protein, HPr and the structure of the complex is known at high resolution." ></td>
	<td class="line x" title="60:141	(4) \[binding_term(\[ head(Je142)\], { Morphology Shape Rule } \[aux0s)\], \[det(an), head(IgG)\] { Metathesaurus } ), \[pron(which)\], \[verb(binds)\], binding_term(\[prep(to), det(the), mod(small), mod(bacterial), head(protein), punc(,)\], { Metathesaurus } \[head(HPr)\] { Morphology Shape Rule } ), \[conj(and)\], \[det(the), head(structure)I, binding_term(\[prep(of), det(the), head(complex)\] { General Binding Word } ), \[aux(is)\], \[verb(known)\], \[prep(at), mod(high), head(resolution), punc(.)l\] 1.2 Identifying binding terms as arguments of relationships Before addressing the strategy for determining the arguments of binding predications, we discuss the general treatment of macro-noun phrases during the second part of the processing." ></td>
	<td class="line x" title="61:141	Although ARBITER attempts to recover complete macro-noun phrases during the first phase, only the most specific (and biologically useful) part of a macro-noun phrase is recovered during the extraction of binding predications." ></td>
	<td class="line x" title="62:141	Terms referring to specific molecules are more useful than those referring to general classes of bindable entities, such as receptor, ligand, protein, or molecule." ></td>
	<td class="line x" title="63:141	The syntactic head of a macro-noun phrase (the first simple noun phrase in the list) is not always the most specific or most useful term in the construction." ></td>
	<td class="line x" title="64:141	l_qt~ 190 The Specificity Rule for determining the most specific part of the list of simple binding terms constituting a macro-noun phrase chooses the first simple term in the list which has either of the following two characteristics: a) The head was identified by the Morphology Shape Rule." ></td>
	<td class="line x" title="65:141	b) The noun phrase maps to a UMLS concept having one of the following semantic types: 'Amino Acid, Peptide, or Protein', 'Nucleic Acid, Nucleoside, or Nucleotide', 'Nucleotide Sequence', 'Immunologic Factor', or 'Gene or Genome'." ></td>
	<td class="line x" title="66:141	For example, in (5), the second simple term, TNF-alpha promoter, maps to the Metathesaurus with semantic type 'Nucleotide Sequence' and is thus considered to be the most specific term in this complex-noun phrase." ></td>
	<td class="line x" title="67:141	(5) binding_term( \[transcriptionally active kappaB motifs\], \[in the TNF-alpha promoter\], \[in normal cells\]) In identifying binding terms as arguments of a complete binding predication, as indicated above, we examine only those binding relations cued by some form of the verb bind (bind, binds, bound, and binding)." ></td>
	<td class="line x" title="68:141	The list of minimal syntactic phrases constituting the partial parse of the input sentence is examined from left to right; for each occurrence of a form of binds, the two binding terms serving as arguments are then sought." ></td>
	<td class="line x" title="69:141	(During the tagging process, we force bind, binds, and bound to be labeled as 'verb,' and binding as 'noun')." ></td>
	<td class="line x" title="70:141	A partial analysis of negation and coordination is undertaken by ARBITER, but anaphora resolution and a syntactic treatment of relativization are not attempted." ></td>
	<td class="line x" title="71:141	With the added constraint that a binding argument must have been identified as a binding term based on the domain knowledge resources used, the partial syntactic analysis available to ARBITER supports the accurate identification of a large number of binding predications asserted in the research literature." ></td>
	<td class="line x" title="72:141	1.2.1 Arguments of binding It is convenient to categorize binding predications into two classes depending on which form of bind cues the predication: a) binding and b) bind, binds, and bound." ></td>
	<td class="line x" title="73:141	In our test collection (discussed below), about half of the binding relationships asserted in the text are cued by the gerundive or participial form binding." ></td>
	<td class="line x" title="74:141	In this syntactic predication, the resources available from the underspecified syntactic parse serve quite well as the basis for correctly identifying the arguments of the binding relationship." ></td>
	<td class="line x" title="75:141	The most common argument configuration associated with binding is for both arguments to occur to the right, cued by prepositions, most commonly of and to; however, other frequent patterns are of-by and to-by." ></td>
	<td class="line x" title="76:141	Another method of argument cuing for binding is for the subject of the predication to function syntactically as a modifier of the head binding in the same simple noun phrase." ></td>
	<td class="line x" title="77:141	The object in this instance is then cued by either of or to (to the right)." ></td>
	<td class="line x" title="78:141	A few other patterns are seen and some occurrences of binding do not cue a complete predication; either the subject is missing or neither argument is explicitly mentioned." ></td>
	<td class="line x" title="79:141	However, the examples in (6) fairly represent the interpretation of binding." ></td>
	<td class="line x" title="80:141	(6) These results suggest that 2 amino acids, Thr-340 and Ser-343, play important but distinct roles in promoting the binding of arrestin to rhodopsin." ></td>
	<td class="line x" title="81:141	<arrestin> BINDS <rhodopsin> Surprisingly, arrestin binding to phosphorylated T340E did not increase to the level observed for wild-type rhodopsin." ></td>
	<td class="line x" title="82:141	<arrestirt> BINDS <phosphorylated t340e> 1.2.2 Arguments of bind The arguments of forms of bind other than binding invariably occur on either side of the cuing verb form." ></td>
	<td class="line x" title="83:141	The default strategy for identifying both arguments in these instances is to choose the closest binding term on either side of the verb." ></td>
	<td class="line x" title="84:141	In the cases we have investigated, this strategy works often enough to be useful for the surface object." ></td>
	<td class="line x" title="85:141	However, due to predicate coordination as well as relativization, such a strategy often fails to identify correctly the surface subject of bind (binds or bound) when more than 191 one binding term precedes the verb." ></td>
	<td class="line x" title="86:141	We therefore use the strategy summarized in (7) for recovering the surface subject in such instances." ></td>
	<td class="line x" title="87:141	(7) When more than one binding term precedes a form of bind other than binding, choose the most specific of these binding terms as the surface subject of the predication." ></td>
	<td class="line x" title="88:141	'Most specific' is determined (recursively) for a series of binding terms in the same way that the most specific part of a complex binding term is determined." ></td>
	<td class="line x" title="89:141	The input text (8) provides an example of a binding predication cued by binds in which the arguments appear (immediately) on either side of the cuing verb." ></td>
	<td class="line x" title="90:141	The two macro-noun phrases serving as potential arguments are underlined." ></td>
	<td class="line x" title="91:141	(8) A transcription factor, Auxin Response Factor 1, that binds to tl!e sequence TGTCTC in auxin response elements was cloned from Arabidopsis by using a yeast one-hybrid system." ></td>
	<td class="line x" title="92:141	(9) <auxin response factor 1> BINDS <sequence tgtctc> In the extracted binding relationship in (9), the Specificity Rule chooses Auxin Response Factor 1 from the first macro-noun phrase because it maps to the UMLS Metathesaurus with semantic type 'Amino Acid, Peptide, or Protein'." ></td>
	<td class="line x" title="93:141	In the second argument, the sequence TGTCTC has a head that submits to the Morphology Shape Rule and hence is considered to be more specific than auxin response elements." ></td>
	<td class="line x" title="94:141	In (10), the Specificity Rule applies correctly to select the surface subject of the binding predication when multiple binding terms appear to the left of the verb." ></td>
	<td class="line x" title="95:141	(10) Phosphatidylinositol transfer protein has a single lipid-binding site that can reversibly bind phosphatidylinositol and phosphatidylcholine and transfer these lipids between membrane compartments in vitro." ></td>
	<td class="line x" title="96:141	<phosphatidylinositol transfer protein> BINDS <phosphatidylcholine> <phosphatidylinositol transfer protein> BINDS <phosphatidylinositol> Both Phosphatidylinositol transfer protein and a single lipid-binding site occur to the left of bind and have been identified as binding terms by the first phase of processing." ></td>
	<td class="line x" title="97:141	However, Phosphatidylinositol transfer protein maps to the corresponding Metathesaurus concept with semantic type 'Amino Acid, Peptide, or Protein, thus causing it to be more specific than a single lipidbinding site." ></td>
	<td class="line x" title="98:141	The second predication listed in (10) was correctly extracted due to coordination processing." ></td>
	<td class="line x" title="99:141	ARBITER pursues limited coordination identification in the spirit of Agarwal and Boggess (1992) and Rindflesch (1995)." ></td>
	<td class="line x" title="100:141	Only binding terms are considered as candidates for coordination." ></td>
	<td class="line x" title="101:141	For each conjunction encountered, the phrase immediately to the right is examined; if it is a binding term, all contiguous binding terms occurring immediately to the left of the conjunct are considered to be coordinate with the right conjunct." ></td>
	<td class="line x" title="102:141	Coordination inside the simple noun phrase is not considered, and therefore structures such as The TCR alpha beta or -gamma delta chains are not recognized." ></td>
	<td class="line x" title="103:141	Nonetheless, as indicated in (11), this limited approach to noun phrase coordination is often effective." ></td>
	<td class="line x" title="104:141	(11) Purified recombinant NC 1, like authentic NC 1, also bound specifically to fibronectin, collagen type I, and a laminin 5/6 complex." ></td>
	<td class="line x" title="105:141	<authentic ncl> BINDS <laminin 5 / 6 complex> <authentic ncl > BINDS <collagen type i> <authentic ncl> BINDS <fibronectin> <purified recombinant ncl > BINDS <laminin 5 / 6 complex> <purified recombinant ncl> BINDS <collagen type i> <purified recombinant ncl > BINDS <fibronectin> 192 Although the particular underspecified syntactic analysis used in the identification of binding predications in the biomedical research literature is limited in several important ways, it appears adequate to enable this project with a useful level of effectiveness, and this is supported by evaluation." ></td>
	<td class="line x" title="106:141	2 Evaluation In order to determine ARBITER's effectiveness, the program was formally evaluated against a gold standard of MEDLINE citations in which the binding predications asserted were marked by hand." ></td>
	<td class="line x" title="107:141	A search of MEDLINE limited to one month (June 1997) and based on the text words ((bind, binds, binding, or bound) and (protein or proteins)) retrieved 116 citations with 1,141 sentences; of these, 346 contained some form of the verb bind." ></td>
	<td class="line x" title="108:141	260 binding predications were identified in the binding sentences." ></td>
	<td class="line x" title="109:141	(The binding sentences also contained 2,025 simple noun phrases, 1,179 of which were marked as being binding terms)." ></td>
	<td class="line x" title="110:141	In processing this test collection, ARBITER extracted 181 binding predications, 132 of which were correct." ></td>
	<td class="line x" title="111:141	Since ARBITER missed 128 marked binding predications (false negatives) and incorrectly identified 49 such relationships, recall and precision as measures of effectiveness are 51% and 73%, respectively." ></td>
	<td class="line x" title="112:141	In comparing ARBITER's output against that marked in the gold standard, fairly stringent matching criteria were used." ></td>
	<td class="line x" title="113:141	A binding predication extracted from a particular sentence by ARBITER had to appear in that same sentence in the gold standard (not just the same citation) in order to be counted as correct." ></td>
	<td class="line x" title="114:141	Further, in the gold standard, only the most specific component of a macro-noun phrase was marked as being the correct argument for a particular binding predication." ></td>
	<td class="line x" title="115:141	If ARBITER retrieved any other part of a macro-noun phrase in identifying the arguments of that predication, it was assessed as an error." ></td>
	<td class="line x" title="116:141	A large number of ARBITER errors are due to two phenomena: difficulties in correctly identifying binding terms during the first phase of processing and syntactic complexity confounding argument identification during the second phase." ></td>
	<td class="line x" title="117:141	An example of the first error type is seen in (12), where the failure to identify ran as a binding term caused ARBITER to miss the correct binding predication asserted in this sentence (indicated by '-FN->')." ></td>
	<td class="line x" title="118:141	(12) Requirement of guanosine triphosphatebound ran for signal-mediated nuclear protein export." ></td>
	<td class="line x" title="119:141	-FN-> <guanosine triphosphate> BINDS <Ran> -FP-> < guanosine triphosphate> BINDS <signal mediate nuclear protein export> This error then led to the false positive error ('-FP->') when the program wrongly interpreted the next noun phrase in the sentence (signalmediated nuclear protein export) as the second argument in this predication." ></td>
	<td class="line x" title="120:141	The interaction of coordination and negation in (13) caused ARBITER to partially misinterpret the binding predications in this sentence." ></td>
	<td class="line x" title="121:141	(13) The nonvisual arrestins, beta-arrestin and arrestin3, but not visual arrestin, bind specifically to a glutathione S-transferaseclathrin terminal domain fusion protein." ></td>
	<td class="line x" title="122:141	<arrestin3> BINDS <glutathione s-transferase-clathrin terminal domain fusion protein> <beta arrestin> BINDS <glutathione s-transferase-clathrin terminal domain fusion protein> <nonvisual arrestin> BINDS <glutathione s-transferase-clathrin terminal domain fusion protein> -FN-> <visual arrestin> DOES_NOT_BIND <glutathione s-transferase-clathrin terminal domain fusion protein> Although some of the coordination in (13) was processed properly, resulting in the relationships listed, the negated coordination associated with the noun phrase visual arrestin was not interpreted correctly, and thus ARBITER failed to 1Q'2 193 identify the predication marked as a false negative." ></td>
	<td class="line x" title="123:141	3 Application As an initial application of ARBITER we ran the program on 491,356 MEDLINE citations, which were retrieved using the same search strategy responsible for the gold standard." ></td>
	<td class="line x" title="124:141	During this run, 331,777 sentences in 192,997 citations produced 419,782 total binding assertions." ></td>
	<td class="line x" title="125:141	Extrapolating from the gold standard evaluation, we assume that this is about half of the total binding predications asserted in the citations processed and that somewhat less than three quarters of those extracted are correct." ></td>
	<td class="line x" title="126:141	The initial list of 419,982 binding triples represents what ARBITER determined was asserted in the text being processed." ></td>
	<td class="line x" title="127:141	Many of these assertions, such as those in (14), while correct, are too general to be useful." ></td>
	<td class="line x" title="128:141	(14) <receptors> BINDS <Peptides> <Erythrocytes> BINDS <Antibodies> Further processing on ARBITER raw output extracted specific protein names and genomic structures and reduced the number of such binding predications to 345,706." ></td>
	<td class="line x" title="129:141	From these more specific binding predication, we began the construction of a database containing binding relations asserted in the literature." ></td>
	<td class="line x" title="130:141	More detailed discussion of this database can be found in (Rajan et al. in prep); however, here we give an initial description of its characteristics." ></td>
	<td class="line x" title="131:141	We submitted the 345,706 more specific ARBITER binding predications to a search in GenBank (Benson et al. 1998) and determined that 106,193 referred to a GenBank entry." ></td>
	<td class="line x" title="132:141	The number of Genbank entries with at least one binding assertion is 11,617." ></td>
	<td class="line x" title="133:141	Preliminary results indicate that the database we are constructing will have some of the following characteristics:  10,769 bindings between two distinct Genbank entries (5,569 unique)  875 more binding assertions found between an entry and a specific DNA sequence  27,345 bindings between a Genbank entry and a UMLS Metathesaurus concept  5,569 unique relationships among pairs of entries (involving 11,617 unique entries) Conclusion The cooperation of structured domain knowledge and underspecified syntactic analysis enables the extraction of macromolecular binding relationships from the research literature." ></td>
	<td class="line x" title="134:141	Although our implementation is domain-specific, the underlying principles are amenable to broader applicability." ></td>
	<td class="line x" title="135:141	ARBITER makes a distinction between first labeling binding terms and then identifying certain of these terms as arguments in a binding predication." ></td>
	<td class="line x" title="136:141	The first phase of this processing is dependent on biomedical domain knowledge accessible from the UMLS." ></td>
	<td class="line x" title="137:141	Applying the techniques we propose in other areas would require at least a minimum of semantic classification of the concepts involved." ></td>
	<td class="line x" title="138:141	General, automated techniques that could supply this requirement are becoming increasingly available (Morin and Jacquemin 1999, for example)." ></td>
	<td class="line x" title="139:141	Although we concentrated on the inflectional forms of a single verb, the principles we invoke to support argument identification during the second phase of processing apply generally to English predication encoding strategies (with a minimum of effort necessary to address prepositional cuing of gerundive arguments for specific verbs)." ></td>
	<td class="line x" title="140:141	The approach to noun phrase coordination also applies generally, so long as hypernymic classification is available for the heads of the potential conjuncts." ></td>
	<td class="line x" title="141:141	Acknowledgements We are grateful to John Wilbur for assistance with accessing GenBank, to Alan Aronson for modifications to MetaMap, and to James Mork for providing the distributed system that supported the processing of MEDLINE citations." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="A00-1031
TnT - A Statistical Part-Of-Speech Tagger
Brants, Thorsten;"></td>
	<td class="line x" title="1:207	TnT -A Statistical Part-of-Speech Tagger Thorsten Brants Saarland University Computational Linguistics D-66041 Saarbriicken, Germany thorst en@coli, uni-sb, de Abstract Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger." ></td>
	<td class="line x" title="2:207	Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework." ></td>
	<td class="line x" title="3:207	A recent comparison has even shown that TnT performs significantly better for the tested corpora." ></td>
	<td class="line x" title="4:207	We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words." ></td>
	<td class="line x" title="5:207	Furthermore, we present evaluations on two corpora." ></td>
	<td class="line x" title="6:207	1 Introduction A large number of current language processing systems use a part-of-speech tagger for pre-processing." ></td>
	<td class="line x" title="7:207	The tagger assigns a (unique or ambiguous) part-ofspeech tag to each token in the input and passes its output to the next processing level, usually a parser." ></td>
	<td class="line x" title="8:207	Furthermore, there is a large interest in part-ofspeech tagging for corpus annotation projects, who create valuable linguistic resources by a combination of automatic processing and human correction." ></td>
	<td class="line x" title="9:207	For both applications, a tagger with the highest possible accuracy is required." ></td>
	<td class="line x" title="10:207	The debate about which paradigm solves the part-of-speech tagging problem best is not finished." ></td>
	<td class="line pc" title="11:207	Recent comparisons of approaches that can be trained on corpora (van Halteren et al. , 1998; Volk and Schneider, 1998) have shown that in most cases statistical aproaches (Cutting et al. , 1992; Schmid, 1995; Ratnaparkhi, 1996) yield better results than finite-state, rule-based, or memory-based taggers (Brill, 1993; Daelemans et al. , 1996)." ></td>
	<td class="line n" title="12:207	They are only surpassed by combinations of different systems, forming a 'voting tagger'." ></td>
	<td class="line x" title="13:207	Among the statistical approaches, the Maximum Entropy framework has a very strong position." ></td>
	<td class="line x" title="14:207	Nevertheless, a recent independent comparison of 7 taggets (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words." ></td>
	<td class="line x" title="15:207	This tagger, TnT, not only yielded the highest accuracy, it also was the fastest both in training and tagging." ></td>
	<td class="line x" title="16:207	The tagger comparison was organized as a 'blackbox test': set the same task to every tagger and compare the outcomes." ></td>
	<td class="line x" title="17:207	This paper describes the models and techniques used by TnT together with the implementation." ></td>
	<td class="line x" title="18:207	The reader will be surprised how simple the underlying model is. The result of the tagger comparison seems to support the maxime 'the simplest is the best'." ></td>
	<td class="line x" title="19:207	However, in this paper we clarify a number of details that are omitted in major previous publications concerning tagging with Markov models." ></td>
	<td class="line x" title="20:207	As two examples, (Rabiner, 1989) and (Charniak et al. , 1993) give good overviews of the techniques and equations used for Markov models and part-ofspeech tagging, but they are not very explicit in the details that are needed for their application." ></td>
	<td class="line x" title="21:207	We argue that it is not only the choice of the general model that determines the result of the tagger but also the various 'small' decisions on alternatives." ></td>
	<td class="line x" title="22:207	The aim of this paper is to give a detailed account of the techniques used in TnT." ></td>
	<td class="line x" title="23:207	Additionally, we present results of the tagger on the NEGRA corpus (Brants et al. , 1999) and the Penn Treebank (Marcus et al. , 1993)." ></td>
	<td class="line x" title="24:207	The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in (Ratnaparkhi, 1996)." ></td>
	<td class="line x" title="25:207	For a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999)." ></td>
	<td class="line x" title="26:207	2 Architecture 2.1 The Underlying Model TnT uses second order Markov models for part-ofspeech tagging." ></td>
	<td class="line x" title="27:207	The states of the model represent tags, outputs represent the words." ></td>
	<td class="line x" title="28:207	Transition probabilities depend on the states, thus pairs of tags." ></td>
	<td class="line x" title="29:207	Output probabilities only depend on the most recent category." ></td>
	<td class="line x" title="30:207	To be explicit, we calculate argmax P(tilti-1, ti-2)P(wilti P(tr+l ItT) tiiT (i) for a given sequence of words w I W T of length T. tl tT are elements of the tagset, the additional 224 tags t-l, to, and tT+l are beginning-of-sequence and end-of-sequence markers." ></td>
	<td class="line x" title="31:207	Using these additional tags, even if they stem from rudimentary processing of punctuation marks, slightly improves tagging results." ></td>
	<td class="line x" title="32:207	This is different from formulas presented in other publications, which just stop with a 'loose end' at the last word." ></td>
	<td class="line x" title="33:207	If sentence boundaries are not marked in the input, TnT adds these tags if it encounters one of \[.!?\] as a token." ></td>
	<td class="line x" title="34:207	Transition and output probabilities are estimated from a tagged corpus." ></td>
	<td class="line x" title="35:207	As a first step, we use the maximum likelihood probabilities /5 which are derived from the relative frequencies: Unigrams: /5(t3) = f(t3) (2) N f(t2, t3) (3) Bigrams: P(t31t~)= f(t2) f(ta, t2, t3) Trigrams: /5(t3ltx,t~) f(tl,t2) (4) Lexical: /5(w3 It3) /(w3, t3) (5) f(t3) for all tl, t2, t3 in the tagset and w3 in the lexicon." ></td>
	<td class="line x" title="36:207	N is the total number of tokens in the training corpus." ></td>
	<td class="line x" title="37:207	We define a maximum likelihood probability to be zero if the corresponding nominators and denominators are zero." ></td>
	<td class="line x" title="38:207	As a second step, contextual frequencies are smoothed and lexical frequences are completed by handling words that are not in the lexicon (see below)." ></td>
	<td class="line x" title="39:207	2.2 Smoothing Trigram probabilities generated from a corpus usually cannot directly be used because of the sparsedata problem." ></td>
	<td class="line x" title="40:207	This means that there are not enough instances for each trigram to reliably estimate the probability." ></td>
	<td class="line x" title="41:207	Furthermore, setting a probability to zero because the corresponding trigram never occured in the corpus has an undesired effect." ></td>
	<td class="line x" title="42:207	It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability." ></td>
	<td class="line x" title="43:207	The smoothing paradigm that delivers the best results in TnT is linear interpolation of unigrams, bigrams, and trigrams." ></td>
	<td class="line x" title="44:207	Therefore, we estimate a trigram probability as follows: P(t3ltl, t2) = AlP(t3) + Ag_/5(t31t2) + A3/5(t3\[t1, t2) (6) /5 are maximum likelihood estimates of the probabilities, and A1 + A2 + A3 = 1, so P again represent probability distributions." ></td>
	<td class="line x" title="45:207	We use the context-independent variant of linear interpolation, i.e., the values of the As do not depend on the particular trigram." ></td>
	<td class="line x" title="46:207	Contrary to intuition, this yields better results than the context-dependent variant." ></td>
	<td class="line x" title="47:207	Due to sparse-data problems, one cannot estimate a different set of As for each trigram." ></td>
	<td class="line x" title="48:207	Therefore, it is common practice to group trigrams by frequency and estimate tied sets of As." ></td>
	<td class="line x" title="49:207	However, we are not aware of any publication that has investigated frequency groupings for linear interpolation in part-of-speech tagging." ></td>
	<td class="line x" title="50:207	All groupings that we have tested yielded at most equivalent results to contextindependent linear interpolation." ></td>
	<td class="line x" title="51:207	Some groupings even yielded worse results." ></td>
	<td class="line x" title="52:207	The tested groupings included a) one set of As for each frequency value and b) two classes (low and high frequency) on the two ends of the scale, as well as several groupings in between and several settings for partitioning the classes." ></td>
	<td class="line x" title="53:207	The values of Ax, A2, and A3 are estimated by deleted interpolation." ></td>
	<td class="line x" title="54:207	This technique successively removes each trigram from the training corpus and estimates best values for the As from all other ngrams in the corpus." ></td>
	<td class="line x" title="55:207	Given the frequency counts for uni-, bi-, and trigrams, the weights can be very efficiently determined with a processing time linear in the number of different trigrams." ></td>
	<td class="line x" title="56:207	The algorithm is given in figure 1." ></td>
	<td class="line x" title="57:207	Note that subtracting 1 means taking unseen data into account." ></td>
	<td class="line x" title="58:207	Without this subtraction the model would overfit the training data and would generally yield worse results." ></td>
	<td class="line x" title="59:207	2.3 Handling of Unknown Words Currently, the method of handling unknown words that seems to work best for inflected languages is a suffix analysis as proposed in (Samuelsson, 1993)." ></td>
	<td class="line x" title="60:207	Tag probabilities are set according to the word's ending." ></td>
	<td class="line x" title="61:207	The suffix is a strong predictor for word classes, e.g., words in the Wall Street Journal part of the Penn Treebank ending in able are adjectives (JJ) in 98% of the cases (e.g. fashionable, variable), the rest of 2% are nouns (e.g. cable, variable)." ></td>
	<td class="line x" title="62:207	The probability distribution for a particular suffix is generated from all words in the training set that share the same suffix of some predefined maximum length." ></td>
	<td class="line x" title="63:207	The term suffix as used here means 'final sequence of characters of a word' which is not necessarily a linguistically meaningful suffix." ></td>
	<td class="line x" title="64:207	Probabilities are smoothed by successive abstraction." ></td>
	<td class="line x" title="65:207	This calculates the probability of a tag t given the last m letters li of an n letter word: P(tlln-r,+l,ln)." ></td>
	<td class="line x" title="66:207	The sequence of increasingly more general contexts omits more and more characters of the suffix, such that P(tlln_m+2,,ln), P(t\[ln-m+3,,l~),  , P(t) are used for smoothing." ></td>
	<td class="line x" title="67:207	The recursiou formula is P(tll,_i+l, . . ." ></td>
	<td class="line x" title="68:207	ln) = P(tlln-i+l, In) + OiP(tll~-,, ln) (7) 1 +0~ 225 set )%1 ---)%2 = )%3 = 0 foreach trigram tl,t2,t3 with f(tl,t2,t3) > 0 depending on the maximum of the following three values: f(tl,t2,ts)-1 case f(h,t2)-I ' increment )%3 by f(tl,t2,t3) f(t2,t3)-I case f(t2)-I ' increment )%2 by f(tl,t2,ts) f(t3)--i case N-1 ' increment )%1 by f(tl,t2,t3) end end normalize )%1, )%2, )%3 Figure 1: Algorithm for calculting the weights for context-independent linear interpolation )%1, )%2, )%3 when the n-gram frequencies are known." ></td>
	<td class="line x" title="69:207	N is the size of the corpus If the denominator in one of the expressions is 0, we define the result of that expression to be 0." ></td>
	<td class="line x" title="70:207	for i = m 0, using the maximum likelihood estimates/5 from frequencies in the lexicon, weights Oi and the initialization P(t) =/5(t)." ></td>
	<td class="line x" title="71:207	(8) The maximum likelihood estimate for a suffix of length i is derived from corpus frequencies by P(ti/~-i+l,  l~) = f(t, 1~-~+1, l~)  (9) For the Markov model, we need the inverse conditional probabilities P(/,-i+l, lnlt) which are obtained by Bayesian inversion." ></td>
	<td class="line x" title="72:207	A theoretical motivated argumentation uses the standard deviation of the maximum likelihood probabilities for the weights 0i (Samuelsson, 1993) This leaves room for interpretation." ></td>
	<td class="line x" title="73:207	1) One has to identify a good value for m, the longest suffix used The approach taken for TnT is the following: m depends on the word in question." ></td>
	<td class="line x" title="74:207	We use the longest suffix that we can find in the training set (i.e. , for which the frequency is greater than or equal to 1), but at most 10 characters." ></td>
	<td class="line x" title="75:207	This is an empirically determined choice 2) We use a context-independent approach for 0i, as we did for the contextual weights )%i. It turned out to be a good choice to set all 0i to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set 1 $ Oi = ~--~(/5(tj) 15)2 (10) s 1 j=l for all i = 0 m 1, using a tagset of s tags and the average $ /5 = 1 ~/5(tj) (11) 8 j----I This usually yields values in the range 0.03  0.10." ></td>
	<td class="line x" title="76:207	3) We use different estimates for uppercase and lowercase words, i.e., we maintain two different suffix tries depending on the capitalization of the word." ></td>
	<td class="line x" title="77:207	This information improves the tagging results 4) Another freedom concerns the choice of the words in the lexicon that should be used for suffix handling." ></td>
	<td class="line x" title="78:207	Should we use all words, or are some of them better suited than others?" ></td>
	<td class="line x" title="79:207	Accepting that unknown words are most probably infrequent, one can argue that using suffixes of infrequent words in the lexicon is a better approximation for unknown words than using suffixes of frequent words." ></td>
	<td class="line x" title="80:207	Therefore, we restrict the procedure of suffix handling to words with a frequency smaller than or equal to some threshold value." ></td>
	<td class="line x" title="81:207	Empirically, 10 turned out to be a good choice for this threshold." ></td>
	<td class="line x" title="82:207	2.4 Capitalization Additional information that turned out to be useful for the disambiguation process for several corpora and tagsets is capitalization information." ></td>
	<td class="line x" title="83:207	Tags are usually not informative about capitalization, but probability distributions of tags around capitalized words are different from those not capitalized." ></td>
	<td class="line x" title="84:207	The effect is larger for English, which only capitalizes proper names, and smaller for German, which capitalizes all nouns." ></td>
	<td class="line x" title="85:207	We use flags ci that are true if wi is a capitalized word and false otherwise These flags are added to the contextual probability distributions." ></td>
	<td class="line x" title="86:207	Instead of P(tsItl,t2) (12) we use P(t3, c3\[tl, cl, t2, c2) (13) and equations (3) to (5) are updated accordingly." ></td>
	<td class="line x" title="87:207	This is equivalent to doubling the size of the tagset and using different tags depending on capitalization." ></td>
	<td class="line x" title="88:207	226 2.5 Beam Search The processing time of the Viterbi algorithm (Rabiner, 1989) can be reduced by introducing a beam search." ></td>
	<td class="line x" title="89:207	Each state that receives a 5 value smaller than the largest 5 divided by some threshold value is excluded from further processing." ></td>
	<td class="line x" title="90:207	While the Viterbi algorithm is guaranteed to find the sequence of states with the highest probability, this is no longer true when beam search is added." ></td>
	<td class="line x" title="91:207	Nevertheless, for practical purposes and the right choice of 0, there is virtually no difference between the algorithm with and without a beam." ></td>
	<td class="line x" title="92:207	Empirically, a value of 0 = 1000 turned out to approximately double the speed of the tagger without affecting the accuracy." ></td>
	<td class="line x" title="93:207	The tagger currently tags between 30~000 and 60,000 tokens per second (including file I/O) on a Pentium 500 running Linux." ></td>
	<td class="line x" title="94:207	The speed mainly depends on the percentage of unknown words and on the average ambiguity rate." ></td>
	<td class="line x" title="95:207	3 Evaluation We evaluate the tagger's performance under several aspects." ></td>
	<td class="line x" title="96:207	First of all, we determine the tagging accuracy averaged over ten iterations." ></td>
	<td class="line x" title="97:207	The overall accuracy, as well as separate accuracies for known and unknown words are measured." ></td>
	<td class="line x" title="98:207	Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set)." ></td>
	<td class="line x" title="99:207	An important characteristic of statistical taggers is that they not only assign tags to words but also probabilities in order to rank different assignments." ></td>
	<td class="line x" title="100:207	We distinguish reliable from unreliable assignments by the quotient of the best and second best assignments 1." ></td>
	<td class="line x" title="101:207	All assignments for which this quotient is larger than some threshold are regarded as reliable, the others as unreliable." ></td>
	<td class="line x" title="102:207	As we will see below, accuracies for reliable assignments are much higher." ></td>
	<td class="line x" title="103:207	The tests are performed on partitions of the corpora that use 90% as training set and 10% as test set, so that the test data is guaranteed to be unseen during training." ></td>
	<td class="line x" title="104:207	Each result is obtained by repeating the experiment 10 times with different partitions and averaging the single outcomes." ></td>
	<td class="line x" title="105:207	In all experiments, contiguous test sets are used." ></td>
	<td class="line x" title="106:207	The alternative is a round-robin procedure that puts every 10th sentence into the test set." ></td>
	<td class="line x" title="107:207	We argue that contiguous test sets yield more realistic results because completely unseen articles are tagged." ></td>
	<td class="line x" title="108:207	Using the round-robin procedure, parts of an article are already seen, which significantly reduces the percentage of unknown words." ></td>
	<td class="line x" title="109:207	Therefore, we expect even 1 By definition, this quotient is co if there is only one possible tag for a given word." ></td>
	<td class="line x" title="110:207	higher results when testing on every 10th sentence instead of a contiguous set of 10%." ></td>
	<td class="line x" title="111:207	In the following, accuracy denotes the number of correctly assigned tags divided by the number of tokens in the corpus processed." ></td>
	<td class="line x" title="112:207	The tagger is allowed to assign exactly one tag to each token." ></td>
	<td class="line x" title="113:207	We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens." ></td>
	<td class="line x" title="114:207	The latter are interesting, since usually unknown tokens are much more difficult to process than known tokens, for which a list of valid tags can be found in the lexicon." ></td>
	<td class="line x" title="115:207	3.1 Tagging the NEGRA corpus The German NEGRA corpus consists of 20,000 sentences (355,000 tokens) of newspaper texts (Frankfurter Rundschau) that are annotated with parts-ofspeech and predicate-argument structures (Skut et al. , 1997)." ></td>
	<td class="line x" title="116:207	It was developed at the Saarland University in Saarbrficken 2." ></td>
	<td class="line x" title="117:207	Part of it was tagged at the IMS Stuttgart." ></td>
	<td class="line x" title="118:207	This evaluation only uses the partof-speech annotation and ignores structural annotations." ></td>
	<td class="line x" title="119:207	Tagging accuracies for the NEGRA corpus are shown in table 2." ></td>
	<td class="line x" title="120:207	Figure 3 shows the learning curve of the tagger, i.e., the accuracy depending on the amount of training data." ></td>
	<td class="line x" title="121:207	Training length is the nmnber of tokens used for training." ></td>
	<td class="line x" title="122:207	Each training length was tested ten times, training and test sets were randomly chosen and disjoint, results were averaged." ></td>
	<td class="line x" title="123:207	The training length is given on a logarithmic scale." ></td>
	<td class="line x" title="124:207	It is remarkable that tagging accuracy for known words is very high even for very small training cotpora." ></td>
	<td class="line x" title="125:207	This means that we have a good chance of getting the right tag if a word is seen at least once during training." ></td>
	<td class="line x" title="126:207	Average percentages of unknown tokens are shown in the bottom line of each diagram." ></td>
	<td class="line x" title="127:207	We exploit the fact that the tagger not only determines tags, but also assigns probabilities." ></td>
	<td class="line x" title="128:207	If there is an alternative that has a probability 'close to' that of the best assignment, this alternative can be viewed as almost equally well suited." ></td>
	<td class="line x" title="129:207	The notion of 'close to' is expressed by the distance of probabilities, and this in turn is expressed by the quotient of probabilities." ></td>
	<td class="line x" title="130:207	So, the distance of the probabilities of a best tag tbest and an alternative tag tart is expressed by P(tbest)/p(tau), which is some value greater or equal to 1 since the best tag assignment has the highest probability." ></td>
	<td class="line x" title="131:207	Figure 4 shows the accuracy when separating assignments with quotients larger and smaller than the threshold (hence reliable and unreliable assignments)." ></td>
	<td class="line x" title="132:207	As expected, we find that accuracies for 2For availability, please check h~tp ://w~." ></td>
	<td class="line x" title="133:207	col i. uni-sb, de/s fb378/negra-corpus 777 227 Table 2: Part-of-speech tagging accuracy for the NEGRA corpus, averaged over 10 test runs, training and test set are disjoint." ></td>
	<td class="line x" title="134:207	The table shows the percentage of unknown tokens, separate accuracies and standard deviations for known and unknown tokens, as well as the overall accuracy." ></td>
	<td class="line x" title="135:207	percentage unknowns NEGRA corpus 11.9% known ace." ></td>
	<td class="line x" title="136:207	97.7% 0.23 unknown acc." ></td>
	<td class="line x" title="137:207	(x 89.0% 0.72 overall aCE." ></td>
	<td class="line x" title="138:207	o' 96.7% 0.29 NEGRA Corpus: POS Learning Curve 100 9O 80 70 /S 6O 50, i 1 2 5 50.8 46.4 41.4 I i i I I I i 10 20 50 100 200 320 500 36.0 30.7 23.0 18.3 14.3 11.9 11).3 Overall min =78.1% max=96.7% e Known rain =95.7% max=97.7% ---a--Unknown rain =61.2% max=89.0% I 1000 x 1000 Training Length St avg." ></td>
	<td class="line x" title="139:207	percentage unknown Figure 3: Learning curve for tagging the NEGRA corpus." ></td>
	<td class="line x" title="140:207	The training sets of variable sizes as well as test sets of 30,000 tokens were randomly chosen." ></td>
	<td class="line x" title="141:207	Training and test sets were disjoint, the procedure was repeated 10 times and results were averaged." ></td>
	<td class="line x" title="142:207	Percentages of unknowns for 500k and 1000k training are determined from an untagged extension." ></td>
	<td class="line x" title="143:207	NEGRA Corpus: Accuracy of reliable assignments 100 99 98 97 A f ' / :." ></td>
	<td class="line x" title="144:207	Reliable rain =96.7% max=99.4% 96 I i i i i i i i i i i 2 5 10 20 50 100 500 2000 10000 threshold 0 100 97.9 95.1 92.7 90.3 86.8 84.1 81.0 76.1 71.9 68.3 64.1 62.0 % cases reliable 53.5 62.9 69.6 74.5 79.8 82.7 85.2 88.0 89.6 90.8 91.8 92.2 acc." ></td>
	<td class="line x" title="145:207	of complement Figure 4: Tagging accuracy for the NEGRA corpus when separating reliable and unreliable assignments." ></td>
	<td class="line x" title="146:207	The curve shows accuracies for reliable assignments." ></td>
	<td class="line x" title="147:207	The numbers at the bottom line indicate the percentage of reliable assignments and the accuracy of the complement set (i.e. , unreliable assignments)." ></td>
	<td class="line x" title="148:207	228 Table 5: Part-of-speech tagging accuracy for the Penn Treebank." ></td>
	<td class="line x" title="149:207	The table shows the percentage of unknown tokens, separate accuracies and standard deviations for known and unknown tokens, as well as the overall accuracy." ></td>
	<td class="line x" title="150:207	I percentage known unknowns acc." ></td>
	<td class="line x" title="151:207	a Penn Treebank 2.9% 97.0% 0.15 unknown aCC." ></td>
	<td class="line x" title="152:207	O' 85.5% 0.69 overall aCE." ></td>
	<td class="line x" title="153:207	O' 96.7% 0.15 100 9O 80 70 < 60 Penn Treebank: POS Learning Curve / 50 I I ~ i I I I i I 1 2 5 10 20 50 100 200 500 50.3 42.8 33.4 26.8 20.2 13.2 9.8 7.0 4.4 Overall rain =78.6% max=96.7% Known rain =95.2% max=97.0% Unknown min =62.2% max=85.5% I 1000  1000 Training Length 2.9 avg." ></td>
	<td class="line x" title="154:207	percentage unknown Figure 6: Learning curve for tagging the Penn Treebank." ></td>
	<td class="line x" title="155:207	The training sets of variable sizes as well as test sets of 100,000 tokens were randomly chosen." ></td>
	<td class="line x" title="156:207	Training and test sets were disjoint, the procedure was repeated 10 times and results were averaged." ></td>
	<td class="line x" title="157:207	Penn Treebank: Accuracy of reliable assignments 100 99 98 97 Overall rain =96.6% max=99.4% 96 i i i i t i i i i i i i 2 5 10 20 50 100 500 2000 10000 threshold 0 100 97.7 94.6 92.2 89.8 86.3 83.5 80.4 76.6 73.8 71.0 67.2 64.5 % cases reliable 53.5 62.8 68.9 73.9 79.3 82.6 85.2 87.5 88.8 89.8 91.0 91.6 acc." ></td>
	<td class="line x" title="158:207	of complement Figure 7: Tagging accuracy for the Penn Treebank when separating reliable and unreliable assignments." ></td>
	<td class="line x" title="159:207	The curve shows accuracies for reliable assignments." ></td>
	<td class="line x" title="160:207	The numbers at the bottom line indicate the percentage of reliable assignments and the accuracy of the complement set." ></td>
	<td class="line x" title="161:207	229 reliable assignments are much higher than for unreliable assignments." ></td>
	<td class="line x" title="162:207	This distinction is, e.g., useful for annotation projects during the cleaning process, or during pre-processing, so the tagger can emit multiple tags if the best tag is classified as unreliable." ></td>
	<td class="line x" title="163:207	3.2 Tagging the Penn Treebank We use the Wall Street Journal as contained in the Penn Treebank for our experiments." ></td>
	<td class="line x" title="164:207	The annotation consists of four parts: 1) a context-free structure augmented with traces to mark movement and discontinuous constituents, 2) phrasal categories that are annotated as node labels, 3) a small set of grammatical functions that are annotated as extensions to the node labels, and 4) part-of-speech tags (Marcus et al. , 1993)." ></td>
	<td class="line x" title="165:207	This evaluation only uses the part-ofspeech annotation." ></td>
	<td class="line x" title="166:207	The Wall Street Journal part of the Penn Treebank consists of approx." ></td>
	<td class="line x" title="167:207	50,000 sentences (1.2 million tokens)." ></td>
	<td class="line x" title="168:207	Tagging accuracies for the Penn Treebank are shown in table 5." ></td>
	<td class="line x" title="169:207	Figure 6 shows the learning curve of the tagger, i.e., the accuracy depending on the amount of training data." ></td>
	<td class="line x" title="170:207	Training length is the number of tokens used for training." ></td>
	<td class="line x" title="171:207	Each training length was tested ten times." ></td>
	<td class="line x" title="172:207	Training and test sets were disjoint, results are averaged." ></td>
	<td class="line x" title="173:207	The training length is given on a logarithmic scale." ></td>
	<td class="line x" title="174:207	As for the NEGRA corpus, tagging accuracy is very high for known tokens even with small amounts of training data." ></td>
	<td class="line x" title="175:207	We exploit the fact that the tagger not only determines tags, but also assigns probabilities." ></td>
	<td class="line x" title="176:207	Figure 7 shows the accuracy when separating assignments with quotients larger and smaller than the threshold (hence reliable and unreliable assignments)." ></td>
	<td class="line x" title="177:207	Again, we find that accuracies for reliable assignments are much higher than for unreliable assignments." ></td>
	<td class="line x" title="178:207	3.3 Summary of Part-of-Speech Tagging Results Average part-of-speech tagging accuracy is between 96% and 97%, depending on language and tagset, which is at least on a par with state-of-the-art results found in the literature, possibly better." ></td>
	<td class="line x" title="179:207	For the Penn Treebank, (Ratnaparkhi, 1996) reports an accuracy of 96.6% using the Maximum Entropy approach, our much simpler and therefore faster HMM approach delivers 96.7%." ></td>
	<td class="line x" title="180:207	This comparison needs to be re-examined, since we use a ten-fold crossvalidation and averaging of results while Ratnaparkhi only makes one test run." ></td>
	<td class="line x" title="181:207	The accuracy for known tokens is significantly higher than for unknown tokens." ></td>
	<td class="line x" title="182:207	For the German newspaper data, results are 8.7% better when the word was seen before and therefore is in the lexicon, than when it was not seen before (97.7% vs. 89.0%)." ></td>
	<td class="line x" title="183:207	Accuracy for known tokens is high even with very small amounts of training data." ></td>
	<td class="line x" title="184:207	As few as 1000 tokens are sufficient to achieve 95%-96% accuracy for them." ></td>
	<td class="line x" title="185:207	It is important for the tagger to have seen a word at least once during training." ></td>
	<td class="line x" title="186:207	Stochastic taggers assign probabilities to tags." ></td>
	<td class="line x" title="187:207	We exploit the probabilities to determine reliability of assignments." ></td>
	<td class="line x" title="188:207	For a subset that is determined during processing by the tagger we achieve accuracy rates of over 99%." ></td>
	<td class="line x" title="189:207	The accuracy of the complement set is much lower." ></td>
	<td class="line x" title="190:207	This information can, e.g., be exploited in an annotation project to give an additional treatment to the unreliable assignments, or to pass selected ambiguities to a subsequent processing step." ></td>
	<td class="line x" title="191:207	4 Conclusion We have shown that a tagger based on Markov models yields state-of-the-art results, despite contrary claims found in the literature." ></td>
	<td class="line x" title="192:207	For example, the Markov model tagger used in the comparison of (van Halteren et al. , 1998) yielded worse results than all other taggers." ></td>
	<td class="line x" title="193:207	In our opinion, a reason for the wrong claim is that the basic algorithms leave several decisions to the implementor." ></td>
	<td class="line x" title="194:207	The rather large amount of freedom was not handled in detail in previous publications: handling of startand end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words." ></td>
	<td class="line x" title="195:207	Note that the decisions we made yield good results for both the German and the English Corpus." ></td>
	<td class="line x" title="196:207	They do so for several other corpora as well." ></td>
	<td class="line x" title="197:207	The architecture remains applicable to a large variety of languages." ></td>
	<td class="line x" title="198:207	According to current tagger comparisons (van Halteren et al. , 1998; Zavrel and Daelemans, 1999), and according to a comparsion of the results presented here with those in (Ratnaparkhi, 1996), the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here." ></td>
	<td class="line x" title="199:207	It is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both." ></td>
	<td class="line x" title="200:207	TnT is freely available to universities and related organizations for research purposes (see http ://www." ></td>
	<td class="line x" title="201:207	coli." ></td>
	<td class="line x" title="202:207	uni-sb, de/-thorsten/tnt)." ></td>
	<td class="line x" title="203:207	Acknowledgements Many thanks go to Hans Uszkoreit for his support during the development of TnT." ></td>
	<td class="line x" title="204:207	Most of the work on TnT was carried out while the author received a grant of the Deutsche Forschungsgemeinschaft in the Graduiertenkolleg Kognitionswissenschaft Saarbriicken." ></td>
	<td class="line x" title="205:207	Large annotated corpora are the pre-requisite for developing and testing part-ofspeech taggers, and they enable the generation of high-quality language models." ></td>
	<td class="line x" title="206:207	Therefore, I would 230 like to thank all the people who took the effort to annotate the Penn Treebank, the Susanne Corpus, the Stuttgarter Referenzkorpus, the NEGRA Corpus, the Verbmobil Corpora, and several others." ></td>
	<td class="line x" title="207:207	And, last but not least, I would like to thank the users of TnT who provided me with bug reports and valuable suggestions for improvements." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C00-1004
Extended Models And Tools For High-Performance Part-Of-Speech
Asahara, Masayuki;Matsumoto, Yuji;"></td>
	<td class="line x" title="1:213	Extended Models and Tools for High-performance Part-of-speech Tagger Masayuki Asahara and Yuji Matsumoto Graduate School of Information Science, Nara Institute of Science and Technology 8916-5, Taka.yama-cho, Ikoma-shi, Nara, 630-0101, Japan {masayu-a,matsu}@is. aist-nara, ac." ></td>
	<td class="line x" title="2:213	jp Abstract Statistical part-of-st)eeeh(POS) taggers achieve high accuracy and robustness when based oil large, scale maimally tagged eorl)ora." ></td>
	<td class="line x" title="3:213	Ilowever, enhancements of the learning models are necessary to achieve better 1)erforma.nce." ></td>
	<td class="line x" title="4:213	We are develol)ing a learning tool for a Jalmnese morphological analyzer called Ch, aScn." ></td>
	<td class="line x" title="5:213	Currently we use a fine-grained POS tag set with about 500 tags." ></td>
	<td class="line x" title="6:213	To al)l)ly a normal trigram model on the tag set, we need unrealistic size of eorl)ora." ></td>
	<td class="line x" title="7:213	Even, for a hi-gram model, we eanno~, 1)ret)are a llloderate size of an mmotated corpus, when we take all the tags as distinct." ></td>
	<td class="line x" title="8:213	A usual technique to Col)e with such fine-grained tags is to reduce the size of the tag set 1)y grouping the set of tags into equivalence classes." ></td>
	<td class="line x" title="9:213	We introduce the concept of position-wise 9rouping where the tag set is t)artitioned into dill'el'lint equivalence classes at each t)osition in the." ></td>
	<td class="line x" title="10:213	conditional 1)rohabilities in the Markov Model." ></td>
	<td class="line x" title="11:213	Moreover, to eoi)e with the data Sl)arsen(?ss prot)lem caused 1) 3, exceptional t)henon> ena, we introduce several other techniques such as word-level statistics, smoothing of word-level an(l P()S-level statistics and a selective tri-gram model." ></td>
	<td class="line x" title="12:213	To help users determine probabilistic 1)arameters, we introduce an error-driven method for the pm'mneter selection." ></td>
	<td class="line x" title="13:213	We then give results of exl)eriments to see the effect of the tools applied to an existing Jat)anese morphological analyzer." ></td>
	<td class="line x" title="14:213	1 Introduction Along with the increasing awfilability of mmotated eorl)ora, a number of statistic P()S tatters have been developed which achieve high accuracy and robustness." ></td>
	<td class="line x" title="15:213	On the other hand, there is still continuing demand for the iinprovement of learning lnodels when sufficient quantity of annotated corpora are not available in the users domains or languages." ></td>
	<td class="line x" title="16:213	Flexible tools for easy tuning of leanfing models are in demand." ></td>
	<td class="line x" title="17:213	We present such tools in this paper." ></td>
	<td class="line x" title="18:213	Our tools are originally intended for use with the Japanese morphological analyzer, ChaSen (Matsumoto et al. , 1999), which at present is a statistical tagger based on the w~riable memory length Marker Model (lion et al. , 1.994)." ></td>
	<td class="line x" title="19:213	We first give a brief overview of the features of the learning tools." ></td>
	<td class="line x" title="20:213	The t)art-of-speech tag set we use is a slightly modified version of tile IPA POS tag set (RWCP, 2000) with about 500 distinct POS tags." ></td>
	<td class="line x" title="21:213	The real tag set is even larger since some words are treated as distim't P()S tags." ></td>
	<td class="line x" title="22:213	The size of the tag set is unrealistic for buihting tri-grmn rules and even bi-gram rules which take all the tags as distinct." ></td>
	<td class="line x" title="23:213	The usual technique for coping with such fine-grained tags is to reduce the size of the tag set by groul)ing the set of tags into equivalence classes (Jelinek, 1.998)." ></td>
	<td class="line x" title="24:213	We introduce the concept of position-wise grouping where the tag set is partitioned into different equivalence (;lasses at each position in the conditiolml probabililies in the Marker Model." ></td>
	<td class="line x" title="25:213	This feature is especially useflfl for,lapanese language analysis since Jal)anese is a highly (:onjugated language, where conjugation fOl'lllS have a great etfeet on the succeeding mor1)homes, trot have little to do with the t)receding nlorphemes." ></td>
	<td class="line x" title="26:213	Moreover, in colloquial language, a number of eolltrael;ed expressio11s are eoinmon, where two or more morphemes are central:ted into a single word." ></td>
	<td class="line x" title="27:213	The contracted word behaves as belonging to different t)arts-of-st)eech by connecting to the previous word or to the next word." ></td>
	<td class="line x" title="28:213	Position-wise grouping enables users to grouI) such words differently according to the positions in which they appear." ></td>
	<td class="line x" title="29:213	Data sparseness is always a serious problem when dealing with a large tag set." ></td>
	<td class="line x" title="30:213	Since it is unrealistic to adopt a simple POS tri-gram model to our tag set, we base our model on a hi-gram model and augment it with selective tri-grams." ></td>
	<td class="line x" title="31:213	By selective tri-gram, we mean that only special contexts are conditioned by tri-gram model and are mixed with the ordinary bigrmn model." ></td>
	<td class="line x" title="32:213	We also incorporate some smoothing techniques for coping with the data sparseness problelll." ></td>
	<td class="line x" title="33:213	By eolnbining these methods, we constructed the learning tools for a high-lmrformance statistical morphological analyzer that are able to learn the probability i)arameters with only a moderate size tagged ('orl)us." ></td>
	<td class="line x" title="34:213	The rest of this paper is structured as follows." ></td>
	<td class="line x" title="35:213	21 Section 2 discusses the basic concet)ts of tile statistical morphological analysis and some problems of the statistical approach." ></td>
	<td class="line x" title="36:213	Section 3 presents the characteristics of the our learning tools." ></td>
	<td class="line x" title="37:213	Section 4 reports the result of some experiments and tile accuracy of the tagger in several settings." ></td>
	<td class="line x" title="38:213	Section 5 discusses related works." ></td>
	<td class="line x" title="39:213	Finally, section 6 gives conclusions and discusses future works." ></td>
	<td class="line x" title="40:213	Throughout this paper, we use morphological analysis instead of part-of-speecll tagging since Japanese is an agglutinative language." ></td>
	<td class="line x" title="41:213	This is the standard ternfinology in Japanese literatures." ></td>
	<td class="line x" title="42:213	2 Preliminaries 2.1 Statistical morphological analysis The POS tagging problem or the Japanese morphological analysis problem must do tokenization and find the sequence of POS tags T = tl,." ></td>
	<td class="line x" title="43:213	., t:,~ tot the word sequence W = wl,, w,~ in the int)ut string S. Tile target is to find T that maxinfizes tile following probability: Using the Bayes' rule of probability theory, P(W,T) can be decomposed as a sequence of tile products of tag probabilities and word probabilities." ></td>
	<td class="line x" title="44:213	P (TIW) P(T, W) = argmax P(W) = argu~}.xP(T,W) =.,': uv/xP(WlT)F(T) We assumed that tile word probability is constrained only by its trig, and that the tag probability is constrained only by its preceding tags, either with the t)i-grmn or the tri-gram model: P(WIT) = HP(wilti) i=1 P(T) = fl P(tilti_l) i=1 P(T) = fl P(ti\[ti-2,i-1) ) i=1 The values are estimated from tile frequencies in tagged corpora using maximum likelihood estimation: p(w lti) F(w'L) r(t ) F(ti_,,td P(t lt -l) F(t;i-2,1,i-1, ti) = F(ti-2, ti-1) Using these parameters, tim most probable tag sequence is determined using the Viterbi algorithm." ></td>
	<td class="line x" title="45:213	2.2 Hierarchical Tag Set We use the IPA POS tag set (RWCP, 2000)." ></td>
	<td class="line x" title="46:213	This tag set consist of three eleinents: tile part-of speech, the type of conjugation and tile form of conjugation (the latter two elements are necessary only for words that conjugate)." ></td>
	<td class="line x" title="47:213	Tile POS tag set has a hierarchical structure: The top POE level consists of 15 categories(e.g. , Noun, Verb,  )." ></td>
	<td class="line x" title="48:213	The second and lower levels are th, e subdivision level." ></td>
	<td class="line x" title="49:213	For example, Noun is fllrther subdivided into common nouns(general), llroper nomls, numerals, and so on." ></td>
	<td class="line x" title="50:213	Proper Noun is sul)divided into General, Person, Organization and Place." ></td>
	<td class="line x" title="51:213	Person and Place are subdivided again." ></td>
	<td class="line x" title="52:213	The bottom level of tile subdivision level is th.c word level, which is conceptually regarded as a part of the subdivision level." ></td>
	<td class="line x" title="53:213	In the Japanese language, verbs, adjectives and auxiliary verbs have conjugation." ></td>
	<td class="line x" title="54:213	These are categorized into a fixed set of conjugation types(CTYPE), each of which has a fxed set of conjugal;ion forms(CFORM)." ></td>
	<td class="line x" title="55:213	It is known that in Japanese that the CFORM varies according to the words appearing in the succeeding position." ></td>
	<td class="line x" title="56:213	Thus, at tile conditional position of the estimated tag probabilities, the CFORM plays an important role, while in the case of other positions, they need not be distinguished." ></td>
	<td class="line x" title="57:213	Figure 1 illustrates tile structure of the tag set." ></td>
	<td class="line x" title="58:213	2.3 Problems in statistical models On the one hand, most of the i)rol)lems in statistical natural language processing stem fi'om the sparsehess of training data." ></td>
	<td class="line x" title="59:213	In our case, tile nuinber of the most fine-grained tags (disregarding the word level) is about 500." ></td>
	<td class="line x" title="60:213	Even when we use the bi-gram model, we suffer from the data sparseness problem." ></td>
	<td class="line x" title="61:213	The situation is nmch worse in the case of the tri-grmn model." ></td>
	<td class="line x" title="62:213	This may be remedied by reducing tile tag set by grouping the tags into a smaller tag set." ></td>
	<td class="line x" title="63:213	On the other hand, there are various kinds of exceptions in language phenomena." ></td>
	<td class="line x" title="64:213	Some words have different contextual features fi'om others in the same tag." ></td>
	<td class="line x" title="65:213	Such exceptions require a word or some group of words to be taken itself as a distinct part-of-speech or its statistics to be taken in distinct contexts." ></td>
	<td class="line x" title="66:213	In our statistical learning tools, those exceptions are handled by position-wise grouping, word-level statistics, smoothing of word-level and POS-level, and selective tri-gram model, which are described in turn ill the next section." ></td>
	<td class="line x" title="67:213	These features enable users to 22 POS .,o 17571 'rile subdivision level ~I I I I lhe word level GL~ WI~yO\] NL~  (;TYPE Godul>'K' Gdan'lS' Illl No Conjugation No Conjugation Type Type tbrm form  No Conjugation No Conjugation Figure 1: The examples of the hierarchical tag set adjust tile balance between fine and coarse grained model settings." ></td>
	<td class="line x" title="68:213	3 Features of the tools This section overviews characteristic timtures of tim learning tools for coping with the above, mentioned prolflems." ></td>
	<td class="line x" title="69:213	a.:t Position-wise grouping of POS tags Since we use a very fine-grained tag set, it is important to classit'y them into some equiva.lence classes l;o reduce the, size." ></td>
	<td class="line x" title="70:213	of i)rotmbilistic lmramelers." ></td>
	<td class="line x" title="71:213	Moreover, as is discussed in the 1)revious section, some words or P()S bo, lmves ditli;rently according to Ihe l)osition they at)pear." ></td>
	<td class="line x" title="72:213	In 3at)aneso, tbr instance, the CF()I/M play an iml)ortmlt role only to dismnbiguate the words at their succeeding position." ></td>
	<td class="line x" title="73:213	In other words, the CFORM should be taken into account only when they appear at tim position of 1,i-1 in either bi-gram or tri-grain model (ti-i in I'(t~lt~__~ ) and P(t,\]t~_. ,,t~_l))." ></td>
	<td class="line x" title="74:213	This means that when the statistics of verbs are, taken, they should be grouped diflbrently according to the positions." ></td>
	<td class="line x" title="75:213	Not(', that, we named the positions; The current position means the position of ti in the hi-gram statistics P(tiIti-1) or the tri-grmn statistics P(till, i_.,, ti-~)." ></td>
	<td class="line x" title="77:213	The preceding position means the position of ti-1." ></td>
	<td class="line x" title="78:213	The second preceding position means the position of ti-.2." ></td>
	<td class="line x" title="79:213	There are quite a few contracted t~rms ill colloquial expressions." ></td>
	<td class="line x" title="80:213	For example, auxiliary verb 'chau' is a contracted tbrms consisting of two words 'te(particle) + simau(auxiliary verb)' and behaves quite differently from other words." ></td>
	<td class="line x" title="81:213	One way to learn its statistical behavior is to collect various us~ges of the word and add the data to the training data after correctly mmotating them." ></td>
	<td class="line x" title="82:213	In contrast, the idea of point-wise grouping provides a nice alternative sohltion to this problem." ></td>
	<td class="line x" title="83:213	By simply group this word into the same equivalence class of 'te' for the currelfl; 1)osition I,i and grou I) it into the same equivale, nt class of 'simau' for the t)rece(ling position ti-1 in P(ti\]ti-." ></td>
	<td class="line x" title="84:213	), it learns the statistical behavior from these classes." ></td>
	<td class="line x" title="85:213	We now describe the point-wise grouping ill a nlore precise way." ></td>
	<td class="line x" title="86:213	l?or simplicity, we assume, bigram model." ></td>
	<td class="line x" title="87:213	Let 7= {A,/3,---} be ttw, original tag set." ></td>
	<td class="line x" title="88:213	\e introduce two partitions of the tag set, one is fin' the current position T ~ = {A (',/)~,-}, mid the other is for the preceding 1)osition T v = {AV,13v,  .}." ></td>
	<td class="line x" title="89:213	We define the equivalence mal>l)ing of the current position: I('(T -~ T'), and another mapping of the t)rece(ling position: U'('\]~ -4 'yv)." ></td>
	<td class="line x" title="90:213	lqgure, 2 shows an exalnple of the lmrtitiollS by those, mapl)ings, where the equivalence mappings I c = {el --> A c,L? -9 A',C ~ A':,\]) -+ B',E -5 W,} fv = {A --+ AV, \]\] -4 AJ', C -4 B v, D --+ B v, E -~ Cl,} Supl)ose we express the equivalence class to which the tag t, belongs as It\]' for the current position and \[t,\]v for the preceding position, then: = F(w, \[td \[W) 1)(till, i_l) = 3.2 Word-level statistics Seine words behave ditt'erently flom other words even in tile same POS." ></td>
	<td class="line x" title="91:213	Especially Japanese particles, auxiliary verbs a.nd some affixes are known to have different e(mtextual behavior." ></td>
	<td class="line x" title="92:213	The tools can define 23 03 g ku o -b (D The Preceding Positon Tag Set ALB C~Dlc  FIG\[ H---~-B' / D' -~  ',  i  i   ~  ~  i  i  03 Fo = < n 'E B O IThe Precedin( Position Tag Set Wb 1 B iilll,ll.q Wbr, B,~ Figure 2: Position-wise grouping of tags some words as distinct POS and their statistics are taken individually." ></td>
	<td class="line x" title="93:213	The tag set T extends to a new tag set T ~xt that defines some words as individual POSs (the word level)." ></td>
	<td class="line x" title="94:213	Modification to the probability formulas for such word level tags is straightforward." ></td>
	<td class="line x" title="95:213	Note that the statistics for POS level should be modified when some words in the same group are individuated." ></td>
	<td class="line x" title="96:213	Suppose that the tags A and B are defined in tile T and some words l,l~,, l'l~,~ E A and l, tZb~, . . ., I:F~,,~ C 17 arc individuated in 7' ~xt." ></td>
	<td class="line x" title="97:213	\Ve define tags Ae,~,l, Bext~ C T ~:t as follows: Figure 3: the word extended tag set We define two smoothing coetIicients: A~ is the smoothing ratio for the current position and /~j, is the smoothing ratio of the preceding position." ></td>
	<td class="line x" title="98:213	Those values can be defined for each word." ></td>
	<td class="line x" title="99:213	Suppose the word wi is individuated and its POS is ti." ></td>
	<td class="line x" title="100:213	If the current position is smoothed, then the tag probability is defined as follows (note that wi itself is an individuated tag): /5(wilti_l ) = ((1 A~)P(tilti_\]) + A~P(wilti_l)) If the word at the preceding positions is smoothed (assume ti-\] is the POS of wi-1): .ao~ =,4 \ 0v,~,,~,,~,,} Bcxt = /) \ {wv~,,wt,,,~} To estimate the probability for tile comlection A-B, tile frequency F(Aea:t, Bext) is used rather than the total frequency F(A, B)." ></td>
	<td class="line x" title="101:213	Figure 3 illustrate the tag set extension of this situation." ></td>
	<td class="line x" title="102:213	These tag set extension is actually a special case of position-wise grouping." ></td>
	<td class="line x" title="103:213	The equivalence mappings are fl'om all word level tags to T ~t. The mapping I ~ maps all the words ill A~xt into A~t and maps each of {W~,, 14(~., } into itself." ></td>
	<td class="line x" title="105:213	In the same way, I p maps all the words in B~.~,t into B~:~t and maps each of {Wb,,, Wb,, } into itself." ></td>
	<td class="line x" title="106:213	3.3 Smoothing of word and POS level statistics When a word is individuated while its occurrence frequency is not high, x~e have to accumulate instances to obtain enough statistics." ></td>
	<td class="line x" title="107:213	Another solution is to smooth the word level statistics with POS level statistics." ></td>
	<td class="line x" title="108:213	In order to back-off the st)arseness of the words, we use the statistics of the POS to which the words belong." ></td>
	<td class="line x" title="109:213	P(ti\]wi-1) = (1 ~Xp)P(tilti-l) + A~,F(tilwi-~) If the both words of the positions is extend: ~/~p((1 -~c)\]~(ti\[~Oi_l ) ~)tcr('ll)i\]'ll)i_ 1 )) +(1 Av)((1 A~)P(tilti_\] ) + A~P(wilti-t)) 3.4 Selective tri-gram model Simple tri-gram models are not feasible for a large tag set." ></td>
	<td class="line x" title="110:213	As a matter of fact, only limited eases require as long contexts as tri-grams." ></td>
	<td class="line x" title="111:213	We 1)rot)ose to take into account Olfly limited tri-gram instances, which we call sclcctive tri-flrams." ></td>
	<td class="line x" title="112:213	Our model is a mixture of such tri-gram statistics with bi-gram ones." ></td>
	<td class="line x" title="113:213	The idea of mixture of different context length is not new." ></td>
	<td class="line x" title="114:213	Markov Models with varial)le memory length are proposed by Ron(Ron et al. , 1994), in whictl a mixtm'e model of n-grams with various value of n is presented as well as its learning algorithms." ></td>
	<td class="line x" title="115:213	In such a model, the set of contexts (the set of states of the automata) should be mutually disjoint for the automata to be deterministic and well-defined." ></td>
	<td class="line x" title="116:213	We give a little different interpretation to tri-grmn statistics." ></td>
	<td class="line x" title="117:213	We consider a tri-grmn as an exceptional 24,, ,o,,,o. AIA~ C The preceding position A I B FB hIE CA,;,." ></td>
	<td class="line x" title="118:213	A B I(  Figure 4: Selective tri-gram context." ></td>
	<td class="line x" title="119:213	Wheii a bi-grani context and a tri-grani context have sonie intersection, the tri-gram context is regarded as an exception within the, l)i-graui context." ></td>
	<td class="line x" title="120:213	In this sense, all tim cont, exts are, mutually disjoint as well in our niodel, and it is possii)h, to convert our model into Ron's tormulal;ion." ></td>
	<td class="line x" title="121:213	Iowevei, we think that oul' %rnmlation is iilore straighforward if the longer COlltex(;s ~-/1'(!" ></td>
	<td class="line x" title="122:213	interln'eted as exc, el)i;ions to (;lie shorter (;onte, xts." ></td>
	<td class="line x" title="123:213	\Ve, assume that th(; grouping at the current 1)osit;ion ('7 -~) share the same grouping of the t)i-grain case." ></td>
	<td class="line x" title="124:213	But for the l)re(:eding l)osition and (;lie s(!(:ond 1)receding 1)osition, we can deline ditl'erent groupings of tag sets fl'om those of the bi-gram case." ></td>
	<td class="line x" title="125:213	We introduce the two new tag sets tbr the preceding positions: The tag sol; of the preceding position: 'P/{W', S/,} The tag set of the s(!(',()ii(l pre(:(!ding l>().~ition: .T),, ' _ { A I',', H~,#} We define the equiv~tlence mal)l)ing for the 1)re ceding position: I p' (74 'Y p'), and tile nml)ping for tile second 1)receding position: I s';/ (7-4 'Y Jm' )." ></td>
	<td class="line x" title="126:213	Asstoning that an equivalence classes for 1, detined by the mapping I pp' is expressed as \[t\] pj/, the, tri-grani t)robal)ility is defined naturally as fl)llows: P(tilt~-',, ti-i ) -s'(\[I,d'lb,<-._,\]''',\[l,~-,\]'') F(\[t.~_.4''', \[l.~_, \],', It,;\]') F(\[ti_~\]m', rrl.,1,.>'~ L 't--J J \] Figure 4 shows an image of fl'equency counts for tri-gram model." ></td>
	<td class="line x" title="128:213	In case some hi-gram COlltext overlaps with a trigrmn context, the bi-graln statistics are taken by excluding the tri-gram statistics." ></td>
	<td class="line x" title="129:213	For (:xmnl)h:, if we inchide (.lie tri-grmn context A C \]7 in our model, then the slat,)sties of the higrail) COilteX(; C13 is taken as folh>ws (F stands for true, frequency in training corpora while F' stands for estimated frequency to lie used for 1)robat.>ility calculation): s,'(c, )." ></td>
	<td class="line x" title="130:213	= s~'(c, J3) F(A, C, ix) Since selection of tii-gram contexts is not easy task, the tools supports the selection based on ml error-driven method." ></td>
	<td class="line x" title="131:213	We omit the detail because of the sl)a(:e limitation." ></td>
	<td class="line x" title="132:213	3.5 Estiinatioli for unseen words in eori)us Since not all the words in (;lie dictionary appear in the training corpus, |;lie occurrence probability i>f miseen words should 1)e allocated ill $Olil(: way." ></td>
	<td class="line x" title="133:213	There are a number of method for estimating unse, en events." ></td>
	<td class="line x" title="134:213	Our era'rent tool adopts Lidstone's law of succession, wlfieh add ;~ fixed (:omit to each observat)Oil." ></td>
	<td class="line x" title="135:213	~'('.,10 = F(,.,, t) + ~ E,,~ F(,., t) + ~: . Itl At 1)resent, l;he de, fault frequency (:omit, (t is set (o 0.5." ></td>
	<td class="line x" title="138:213	4 Experiments and Evaluation For evaluating how the 1)rol)osed extension lint)roves a normal t)i-glain model, we condllcted several experiments." ></td>
	<td class="line x" title="139:213	We group verl)s according (o the conjugation forms at the preceding i/osition, take word level statistics for all l/articles, auxiliary verbs and synll)ols, each of which is smoothed with the illlliie,dial:ely higher P()S level." ></td>
	<td class="line x" title="140:213	Selective, tri-grani contexts are defined for dist:riniinating a few notoriously ;lil/hi~uous particle 'no' and auxiliary ve, i'l)s 'nai' and 'aruY This is a very simple extension but suffices tbr evaluating the ett'ect of the learning tools." ></td>
	<td class="line x" title="141:213	We use 5-tbld cross ewfluation over (;he RWCP tagged corpus (RXVCP, 2000)." ></td>
	<td class="line x" title="142:213	The corpus (:late size is 37490 sentences(958678 words)." ></td>
	<td class="line x" title="143:213	The errors of the corpus are manually rood)tied." ></td>
	<td class="line x" title="144:213	The annotated corpus is divided into the traiifing data set(29992 sentences, 80%) and the test data set(7498 se, ntence, s, 20%)." ></td>
	<td class="line x" title="145:213	Experinients were repeated 5 times, and the reslllts \v(;r(; averaged." ></td>
	<td class="line x" title="146:213	The, evaluation is done at the following 3 levels:  le, vell: only word segmentation (tokenizati(m) is ewduated  level2: word segmentation mid (;lie toI) level part-of-speech are ewfluated  level3: all infornmtion is taken into at, count for evaluation Using the tools, we create the following six models: D: hernial bi-granl model D,,,: D + word level statistics for particles, etc. 25 Table 1: Results for test data (F-value %) dataset D 98.69 98.12 96.91 Dw 98.75 98.24 97.22 Dw.~t 98.80 98.26 97.20 Dws 98.76 98.27 97.23 Dwqt 98.78 98.35 97.27 Table 2: Results for learning data (F-value %) dataset D 98.84 98.36 92.36 D,o 98.96 98.58 97.81 Dwq 98.92 98.46 97.6t Dw~ 98.96 98.58 97.80 D,vgt 98.92 98.55 97.70 Dwo: Dw + groupilLg Dw,: D,o + smoothing of word level with POS level D,~,at: Dwo + selective tri-grmn The smoothing rate between the part-of-sl)eech and the words is fixed to 0.9 for each word." ></td>
	<td class="line x" title="147:213	To evahmte the results, we use tile F-value defined by the tbllowing formulae: number of correct words 12ccall = number of words in corpus number of correct words Precision = number of words by system output (/32 + 1) l~,ecall." ></td>
	<td class="line x" title="148:213	Precision F~ = /32  (Precision + 12ccall) For each model, we evaluate the F-value (with ~ = 1) for tim learlfing data and test data at; each level." ></td>
	<td class="line x" title="149:213	The results are given in the Tables 1 and 2." ></td>
	<td class="line x" title="150:213	From the results the tbllowing observation is possible: Smoothing improve on grouping dataset in test data slightly." ></td>
	<td class="line x" title="151:213	But in tile other enviromnents the accuracy isn't improved." ></td>
	<td class="line x" title="152:213	In this experiment, the smoothing rate for all words is fixed." ></td>
	<td class="line x" title="153:213	We need to make the different rate for each word in the future work." ></td>
	<td class="line x" title="154:213	The grouping performs good result for the test dataset." ></td>
	<td class="line x" title="155:213	It is natural that the grouping is not good for learning dataset since all the word level statistics are learned in the case of learning dataset." ></td>
	<td class="line x" title="156:213	Finally, the selective tri-gram (only 25 rules added) achieves non-negligible improvement at level2 and level3." ></td>
	<td class="line x" title="157:213	Compared with the normal bigram Inodel, it improves about 0.35% on level3 and about 0.2% on level2." ></td>
	<td class="line oc" title="158:213	5 Related work Cutting introduced grouping of words into equiva.lence classes based on the set of possible tags to reduce the number of the parameters (Cutting et al. , 1992) . Schmid used tile equivaleuce classes for smoothing." ></td>
	<td class="line x" title="159:213	Their classes define not a partition of POS tags, but mixtures of some POS tags (Schmid, 1995)." ></td>
	<td class="line x" title="160:213	Brill proposed a transfbrmation-based method." ></td>
	<td class="line x" title="161:213	In the selection of tri-gram contexts we will use a similar technique (Brill, 1995) . Haruno constructed variable length models based on the mistake-driven methods, and mixed these tag models." ></td>
	<td class="line x" title="162:213	They do not have grouping or smoothing facilities (Haruno and Matsumoto, 1997)." ></td>
	<td class="line x" title="163:213	Kitauchi presented a method to determine refinemeat of the tag set by a mistake-driven technique." ></td>
	<td class="line x" title="164:213	Their inethod determines the tag set according to the hierarchical definition of tags." ></td>
	<td class="line x" title="165:213	Word level discrimination and grouping beyond the hierarchical tag structure are out of scope of their method (Kitauchi el; al. , 1999)." ></td>
	<td class="line x" title="166:213	6 Conclusion and Future works We proposed several extensions to the statistical model for Japanese morphological analysis." ></td>
	<td class="line x" title="167:213	We also gave preliminary experiments and showed tile effects of the extensions." ></td>
	<td class="line x" title="168:213	Counting some words individually and smoothing them with POS level statistics alleviate the data sparseness problem." ></td>
	<td class="line x" title="169:213	Position-wise grouping enables an eflk~ctive refiimment of the probability parameter settings." ></td>
	<td class="line x" title="170:213	Using selective tri-grain provides an easy description of exceptional language phenomena." ></td>
	<td class="line x" title="171:213	In our future work, we will develol) a method to refine the models automatically or semi-automatically." ></td>
	<td class="line x" title="172:213	For example, error-driven methods will be applicable to the selection of the words to be individuated and the useflfl tri-gram contexts." ></td>
	<td class="line x" title="173:213	For the morphological analyzer Ch, aSen, we are using the mixture modeh Position-wise grouping used for conjugation." ></td>
	<td class="line x" title="174:213	Smoothing of tile word level and the POS level used tbr particles." ></td>
	<td class="line x" title="175:213	The analyzer and the learning tools are available publicly i . References E. Brill." ></td>
	<td class="line x" title="176:213	1995." ></td>
	<td class="line x" title="177:213	Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part-of-Speech Tagging." ></td>
	<td class="line x" title="178:213	Computational Linguistics, 21(4):543 565." ></td>
	<td class="line x" title="179:213	D. Cutting, J. Kupiec, J. Pedersen, and P. Sibun." ></td>
	<td class="line x" title="180:213	1992." ></td>
	<td class="line x" title="181:213	A practical part-of-speech tagger." ></td>
	<td class="line x" title="182:213	In Proceedings of the Third Conference on Applied Natural Language Processing." ></td>
	<td class="line x" title="183:213	M. Haruno and Y. Matsumoto." ></td>
	<td class="line x" title="184:213	1997." ></td>
	<td class="line x" title="185:213	Mistake Driven Mixture of Hierarchical Tag Context Trees." ></td>
	<td class="line x" title="186:213	In 35th, Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics, pages 230-237, July." ></td>
	<td class="line x" title="187:213	F. Jelinek." ></td>
	<td class="line x" title="188:213	1998." ></td>
	<td class="line x" title="189:213	Statistical Methods for @cech Recognition." ></td>
	<td class="line x" title="190:213	MIT Press." ></td>
	<td class="line x" title="191:213	A. Kitauchi, T. Utsm'o, and Y. Matsmnoto." ></td>
	<td class="line x" title="192:213	1999." ></td>
	<td class="line x" title="193:213	Probabilistic Model Le~arning tbr JatmlmSC' Mor1)hological Analysis 1)3' lgrror-driven Feat;,rc Selection (in .lal)mmse)." ></td>
	<td class="line x" title="194:213	'J}'(t~t.sa(:l/io'l~, of \]'nfi)rmatio'n l'roccssi'ng Sci('ty of,\]apa'n, 40(5):2325 2337, 5." ></td>
	<td class="line x" title="195:213	Y. Matsmnoto, A. Kitau('hi, T. Ymmtshita, Y. 1\]imno, H. M~tsuda, and 54." ></td>
	<td class="line x" title="196:213	Asahm'a." ></td>
	<td class="line x" title="197:213	1999." ></td>
	<td class="line x" title="198:213	Japanese MorphologicM Analyzer ChaSen Users Ma.mml version 2.0." ></td>
	<td class="line x" title="199:213	Technical l/el)oft NAIST-IST1~99012, Nma Institute of Science mM ~ibx:lmology ~l~(:lmicM lR,eport." ></td>
	<td class="line x" title="200:213	l)." ></td>
	<td class="line x" title="201:213	I~.on, Y. Singer, and N. Tishby." ></td>
	<td class="line x" title="202:213	1994." ></td>
	<td class="line x" title="203:213	\]A~A/I'll ing Prolml)ilistic Automal a with Vm'iM)lc Memory Length." ></td>
	<td class="line x" title="204:213	In COLT-g4, tinges 35 ~16." ></td>
	<td class="line x" title="205:213	\]/,WCP." ></td>
	<td class="line x" title="206:213	2000." ></td>
	<td class="line x" title="207:213	I\YC Tt~xt l)atabas(~." ></td>
	<td class="line x" title="208:213	http://www, rwcp." ></td>
	<td class="line x" title="209:213	or." ></td>
	<td class="line x" title="210:213	j p/wswg/rwcdb/text/." ></td>
	<td class="line x" title="211:213	H. Schmid." ></td>
	<td class="line x" title="212:213	1995." ></td>
	<td class="line x" title="213:213	Improvements In part-of-speech Tagging With an Application To German In EACL SIGDAT workshop, pages 17-50 ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N01-1023
Applying Co-Training Methods To Statistical Parsing
Sarkar, Anoop;"></td>
	<td class="line x" title="1:209	Applying Co-Training methods to Statistical Parsing #03 Anoop Sarkar Dept. of Computer and Information Science University of Pennsylvania 200 South 33rd Street, Philadelphia, PA 19104-6389 USA anoop@linc.cis.upenn.edu Abstract We propose a novel Co-Training method for statistical parsing." ></td>
	<td class="line x" title="2:209	The algorithm takes as input a small corpus (9695 sentences) annotated with parse trees, a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text." ></td>
	<td class="line x" title="3:209	The algorithm iteratively labels the entire data set with parse trees." ></td>
	<td class="line x" title="4:209	Using empirical results based on parsing the Wall Street Journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly outperforms training only on the labeled data." ></td>
	<td class="line x" title="5:209	1 Introduction The current crop of statistical parsers share a similar training methodology." ></td>
	<td class="line x" title="6:209	They train from the Penn Treebank (Marcus et al. , 1993); a collection of 40,000 sentences that are labeled with corrected parse trees (approximately a million word tokens)." ></td>
	<td class="line x" title="7:209	In this paper, we explore methods for statistical parsing that can be used to combine small amounts of labeled data with unlimited amounts of unlabeled data." ></td>
	<td class="line x" title="8:209	In the experiment reported here, we use 9695 sentences of bracketed data (234467 word tokens)." ></td>
	<td class="line x" title="9:209	Such methods are attractive for the following reasons: #0F Bracketing sentences is an expensive process." ></td>
	<td class="line x" title="10:209	A parser that can be trained on a small amount of labeled data will reduce this annotation cost." ></td>
	<td class="line x" title="11:209	#0F Creating statistical parsers for novel domains and new languages will become easier." ></td>
	<td class="line x" title="12:209	#0F Combining labeled data with unlabeled data allows exploration of unsupervised methods which can now be tested using evaluations compatible with supervised statistical parsing." ></td>
	<td class="line x" title="13:209	In this paper we introduce a new approach that combines unlabeled data with a small amount of labeled (bracketed) data to train a statistical parser." ></td>
	<td class="line x" title="14:209	We use a CoTraining method (Yarowsky, 1995; Blum and Mitchell, #03 I would like to thank Aravind Joshi, Mitch Marcus, Mark Liberman, B. Srinivas, David Chiang and the anonymous reviewers for helpful comments on this work." ></td>
	<td class="line x" title="15:209	This work was partially supported by NSF Grant SBR8920230, ARO Grant DAAH0404-94-G-0426, and DARPA Grant N66001-00-1-8915." ></td>
	<td class="line x" title="16:209	1998; Goldman and Zhou, 2000) that has been used previously to train classifiers in applications like word-sense disambiguation (Yarowsky, 1995), document classification (Blum and Mitchell, 1998) and named-entity recognition (Collins and Singer, 1999) and apply this method to the more complex domain of statistical parsing." ></td>
	<td class="line x" title="17:209	2 Unsupervised techniques in language processing While machine learning techniques that exploit annotated data have been very successful in attacking problems in NLP, there are still some aspects which are considered to be open issues: #0F Adapting to new domains: training on one domain, testing (using) on another." ></td>
	<td class="line x" title="18:209	#0F Higher performance when using limited amounts of annotated data." ></td>
	<td class="line x" title="19:209	#0F Separating structural (robust) aspects of the problem from lexical (sparse) ones to improve performance on unseen data." ></td>
	<td class="line x" title="20:209	In the particular domain of statistical parsing there has been limited success in moving towards unsupervised machine learning techniques (see Section 7 for more discussion)." ></td>
	<td class="line x" title="21:209	A more promising approach is that of combining small amounts of seed labeled data with unlimited amounts of unlabeled data to bootstrap statistical parsers." ></td>
	<td class="line x" title="22:209	In this paper, we use one such machine learning technique: Co-Training, which has been used successfully in several classification tasks like web page classification, word sense disambiguation and named-entity recognition." ></td>
	<td class="line x" title="23:209	Early work in combining labeled and unlabeled data for NLP tasks was done in the area of unsupervised part of speech (POS) tagging." ></td>
	<td class="line pc" title="24:209	(Cutting et al. , 1992) reported very high results (96% on the Brown corpus) for unsupervised POS tagging using Hidden Markov Models (HMMs) by exploiting hand-built tag dictionaries and equivalence classes." ></td>
	<td class="line x" title="25:209	Tag dictionaries are predefined assignments of all possible POS tags to words in the test data." ></td>
	<td class="line p" title="26:209	This impressive result triggered several follow-up studies in which the effect of hand tuning the tag dictionary was quantified as a combination of labeled and unlaPierre/NNP Vinken/NNP NP will/MD join/VB the/DT board/NN NP as/IN a/DT nonexecutive/JJ director/NN NP PP VP VP S Figure 1: An example of the kind of output expected from a statistical parser." ></td>
	<td class="line o" title="27:209	beled data." ></td>
	<td class="line x" title="28:209	The experiments in (Merialdo, 1994; Elworthy, 1994) showed that only in very specific cases HMMs were effective in combining labeled and unlabeled data." ></td>
	<td class="line x" title="29:209	However, (Brill, 1997) showed that aggressively using tag dictionaries extracted from labeled data could be used to bootstrap an unsupervised POS tagger with high accuracy (approx 95% on WSJ data)." ></td>
	<td class="line x" title="30:209	We exploit this approach of using tag dictionaries in our method as well (see Section 3.2 for more details)." ></td>
	<td class="line x" title="31:209	It is important to point out that, before attacking the problem of parsing using similar machine learning techniques, we face a representational problem which makes it difficult to define the notion of tag dictionary for a statistical parser." ></td>
	<td class="line x" title="32:209	The problem we face in parsing is more complex than assigning a small fixed set of labels to examples." ></td>
	<td class="line x" title="33:209	If the parser is to be generally applicable, it has to produce a fairly complex label given an input sentence." ></td>
	<td class="line x" title="34:209	For example, given the sentence Pierre Vinken will join the board as a non-executive director, the parser is expected to produce an output as shown in Figure 1." ></td>
	<td class="line x" title="35:209	Since the entire parse cannot be reasonably considered as a monolithic label, the usual method in parsing is to decompose the structure assigned in the following way: S(join) ! NP(Vinken) VP(join) NP(Vinken) ! Pierre Vinken VP(join) ! will VP(join) VP(join) ! join NP(board) PP(as) ::: However, such a recursive decomposition of structure does not allow a simple notion of a tag dictionary." ></td>
	<td class="line x" title="36:209	We solve this problem by decomposing the structure in an approach that is different from that shown above which uses context-free rules." ></td>
	<td class="line x" title="37:209	The approach uses the notion of tree rewriting as defined in the Lexicalized Tree Adjoining Grammar (LTAG) formalism (Joshi and Schabes, 1992) 1 which re1 This is a lexicalized version of Tree Adjoining Grammar (Joshi et al. , 1975; Joshi, 1985)." ></td>
	<td class="line x" title="38:209	tains the notion of lexicalization that is crucial in the success of a statistical parser while permitting a simple definition of tag dictionary." ></td>
	<td class="line x" title="39:209	For example, the parse in Figure 1 can be generated by assigning the structured labels shown in Figure 2 to each word in the sentence (for simplicity, we assume that the noun phrases are generated here as a single word)." ></td>
	<td class="line x" title="40:209	We use a tool described in (Xia et al. , 2000) to convert the Penn Treebank into this representation." ></td>
	<td class="line x" title="41:209	Pierre Vinken NP will VP VP NP join NP VP S the board NP VP as NP PP VP a nonexecutive director NP Figure 2: Parsing as tree classification and attachment." ></td>
	<td class="line x" title="42:209	Combining the trees together by rewriting nodes as trees (explained in Section 2.1) gives us the parse tree in Figure 1." ></td>
	<td class="line x" title="43:209	A history of the bi-lexical dependencies that define the probability model used to construct the parse is shown in Figure 3." ></td>
	<td class="line x" title="44:209	This history is called the derivation tree." ></td>
	<td class="line x" title="45:209	In addition, as a byproduct of this kind of representation we obtain more than the phrase structure of each sentence." ></td>
	<td class="line x" title="46:209	We also produce a more embellished parse in which phenomena such as predicate-argument structure, subcategorization and movement are given a probabilistic treatment." ></td>
	<td class="line x" title="47:209	Pierre_Vinken will the_board a_nonexecutive_director as join Figure 3: A derivation indicating all the attachments between trees that have occurred during the parse of the sentence." ></td>
	<td class="line x" title="48:209	2.1 The Generative Model A stochastic LTAG derivation proceeds as follows (Schabes, 1992; Resnik, 1992)." ></td>
	<td class="line x" title="49:209	An initial tree is selected with probability P init and other trees selected by words in the sentence are combined using the operations of substitution and adjoining." ></td>
	<td class="line x" title="50:209	These operations are explained below with examples." ></td>
	<td class="line x" title="51:209	Each of these operations is performed with probability P attach." ></td>
	<td class="line x" title="52:209	For each #1C that can be valid start of a derivation: X #1C P init #28#1C#29=1 Substitution is defined as rewriting a node in the frontier of a tree with probability P attach which is said to be proper if: X #1C 0 P attach #28#1C;#11 ! #1C 0 #29=1 where #1C;#11 ! #1C 0 indicates that tree #1C 0 is substituting into node #11 in tree #1C." ></td>
	<td class="line x" title="53:209	An example of the operation of substitution is shown in Figure 4." ></td>
	<td class="line x" title="54:209	Adjoining is defined as rewriting any internal node of a tree by another tree." ></td>
	<td class="line x" title="55:209	This is a recursive rule and each adjoining operation is performed with probability P attach which is proper if: P attach #28#1C;#11 ! NA#29+ X #1C 0 P attach #28#1C;#11 ! #1C 0 #29=1 P attach here is the probability that #1C 0 rewrites an internal node #11 in tree #1C or that no adjoining (NA) occurs at node #11 in #1C." ></td>
	<td class="line x" title="56:209	The additional factor that accounts for no adjoining at a node is required for the probability to be well-formed." ></td>
	<td class="line x" title="57:209	An example of the operation of adjoining isshowninFigure5." ></td>
	<td class="line x" title="58:209	Each LTAG derivationD which was built starting from tree #0B with n subsequent attachments has the probability: Pr#28D#29=P init #28#0B#29 Y 1#14i#14n P attach #28#1C;#11 ! #1C 0 i #29 Pierre Vinken NP join NP VP S NP join NP VP S Pierre Vinken NP Figure 4: Example substitution of the tree for Pierre Vinken into the tree for join: #1C#28join#29;NP ! #1C 0 #28Pierre Vinken#29." ></td>
	<td class="line x" title="59:209	will VP VP NP join NP VP S NP will join NP VP VP S Figure 5: Example adjoining of the tree for will into the tree for join: #1C#28join#29;VP ! #1C 0 #28will#29." ></td>
	<td class="line x" title="60:209	Note that assuming each tree is lexicalized by one word the derivation D corresponds to a sentence of n+1 words." ></td>
	<td class="line x" title="61:209	In the next section we show how to exploit this notion of tag dictionary to the problem of statistical parsing." ></td>
	<td class="line x" title="62:209	3 Co-Training methods for parsing Many supervised methods of learning from a Treebank have been studied." ></td>
	<td class="line x" title="63:209	The question we want to pursue in this paper is whether unlabeled data can be used to improve the performance of a statistical parser and at the same time reduce the amount of labeled training data necessary for good performance." ></td>
	<td class="line x" title="64:209	We will assume the data that is input to our method will have the following characteristics: 1." ></td>
	<td class="line x" title="65:209	A small set of sentences labeled with corrected parse trees and large set of unlabeled data." ></td>
	<td class="line x" title="66:209	2." ></td>
	<td class="line x" title="67:209	A pair of probabilistic models that form parts of a statistical parser." ></td>
	<td class="line x" title="68:209	This pair of models must be able to mutually constrain each other." ></td>
	<td class="line x" title="69:209	3." ></td>
	<td class="line x" title="70:209	A tag dictionary (used within a backoff smoothing strategy) for labels are not covered in the labeled set." ></td>
	<td class="line x" title="71:209	The pair of probabilistic models can be exploited to bootstrap new information from unlabeled data." ></td>
	<td class="line x" title="72:209	Since both of these steps ultimately have to agree with each other, we can utilize an iterative method called CoTraining that attempts to increase agreement between a pair of statistical models by exploiting mutual constraints between their output." ></td>
	<td class="line x" title="73:209	Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and namedentity identification (Collins and Singer, 1999)." ></td>
	<td class="line x" title="74:209	In all of these cases, using unlabeled data has resulted in performance that rivals training solely from labeled data." ></td>
	<td class="line x" title="75:209	However, these previous approaches were on tasks that involved identifying the right label from a small set of labels (typically 23), and in a relatively small parameter space." ></td>
	<td class="line x" title="76:209	Compared to these earlier models, a statistical parser has a very large parameter space and the labels that are expected as output are parse trees which have to be built up recursively." ></td>
	<td class="line x" title="77:209	We discuss previous work in combining labeled and unlabeled data in more detail in Section 7." ></td>
	<td class="line x" title="78:209	Co-training (Blum and Mitchell, 1998; Yarowsky, 1995) can be informally described in the following manner: #0F Pick two (or more) views of a classification problem." ></td>
	<td class="line x" title="79:209	#0F Build separate models for each of these views and train each model on a small set of labeled data." ></td>
	<td class="line x" title="80:209	#0F Sample an unlabeled data set and to find examples that each model independently labels with high confidence." ></td>
	<td class="line x" title="81:209	(Nigam and Ghani, 2000) #0F Confidently labeled examples can be picked in various ways." ></td>
	<td class="line x" title="82:209	(Collins and Singer, 1999; Goldman and Zhou, 2000) #0F Take these examples as being valuable as training examples and iterate this procedure until the unlabeled data is exhausted." ></td>
	<td class="line x" title="83:209	Effectively, by picking confidently labeled data from each model to add to the training data, one model is labeling data for the other model." ></td>
	<td class="line x" title="84:209	3.1 Lexicalized Grammars and Mutual Constraints In the representation we use, parsing using a lexicalized grammar is done in two steps: 1." ></td>
	<td class="line x" title="85:209	Assigning a set of lexicalized structures to each word in the input sentence (as shown in Figure 2)." ></td>
	<td class="line x" title="86:209	2." ></td>
	<td class="line x" title="87:209	Finding the correct attachments between these structures to get the best parse (as shown in Figure 1)." ></td>
	<td class="line x" title="88:209	Each of these two steps involves ambiguity which can be resolved using a statistical model." ></td>
	<td class="line x" title="89:209	By explicitly representing these two steps independently, we can pursue independent statistical models for each step: 1." ></td>
	<td class="line x" title="90:209	Each word in the sentence can take many different lexicalized structures." ></td>
	<td class="line x" title="91:209	We can introduce a statistical model that disambiguates the lexicalized structure assigned to a word depending on the local context." ></td>
	<td class="line x" title="92:209	2." ></td>
	<td class="line x" title="93:209	After each word is assigned a certain set of lexicalized structures, finding the right parse tree involves computing the correct attachments between these lexicalized structures." ></td>
	<td class="line x" title="94:209	Disambiguating attachments correctly using an appropriate statistical model is essential to finding the right parse tree." ></td>
	<td class="line x" title="95:209	These two models have to agree with each other on the trees assigned to each word in the sentence." ></td>
	<td class="line x" title="96:209	Not only do the right trees have to be assigned as predicted by the first model, but they also have to fit together to cover the entire sentence as predicted by the second model 2 .This represents the mutual constraint that each model places on the other." ></td>
	<td class="line x" title="97:209	3.2 Tag Dictionaries For the words that appear in the (unlabeled) training data, we collect a list of part-of-speech labels and trees that each word is known to select in the training data." ></td>
	<td class="line x" title="98:209	This information is stored in a POS tag dictionary and a tree dictionary." ></td>
	<td class="line x" title="99:209	It is important to note that no frequency or any other distributional information is stored." ></td>
	<td class="line x" title="100:209	The only information stored in the dictionary is which tags or trees can be selected by each word in the training data." ></td>
	<td class="line x" title="101:209	We use a count cutoff for trees in the labeled data and combine observed counts into an unobserved tree count." ></td>
	<td class="line x" title="102:209	This is similar to the usual technique of assigning the token unknown to infrequent word tokens." ></td>
	<td class="line x" title="103:209	In this way, trees unseen in the labeled data but in the tag dictionary are assigned a probability in the parser." ></td>
	<td class="line x" title="104:209	The problem of lexical coverage is a severe one for unsupervised approaches." ></td>
	<td class="line x" title="105:209	The use of tag dictionaries is a way around this problem." ></td>
	<td class="line x" title="106:209	Such an approach has already been used for unsupervised part-of-speech tagging in (Brill, 1997) where seed data of which POS tags can be selected by each word is given as input to the unsupervised tagger." ></td>
	<td class="line x" title="107:209	2 See x7 for a discussion of the relation of this approach to that of SuperTagging (Srinivas, 1997) In future work, it would be interesting to extend models for unknown-word handling or other machine learning techniques in clustering or the learning of subcategorization frames to the creation of such tag dictionaries." ></td>
	<td class="line x" title="108:209	4 Models As described before, we treat parsing as a two-step process." ></td>
	<td class="line x" title="109:209	The two models that we use are: 1." ></td>
	<td class="line x" title="110:209	H1: selects trees based on previous context (tagging probability model) 2." ></td>
	<td class="line x" title="111:209	H2: computes attachments between trees and returns best parse (parsing probability model) 4.1 H1: Tagging probability model We select the most likely trees for each word by examining the local context." ></td>
	<td class="line x" title="112:209	The statistical model we use to decide this is the trigram model that was used by B. Srinivas in his SuperTagging model (Srinivas, 1997)." ></td>
	<td class="line x" title="113:209	The model assigns an n-best lattice of tree assignments associated with the input sentence with each path corresponding to an assignment of an elementary tree for each word in the sentence." ></td>
	<td class="line x" title="114:209	(for further details, see (Srinivas, 1997))." ></td>
	<td class="line x" title="115:209	P#28TjW#29 = P#28T 0 :::T n jW 0 :::W n #29 (1) = P#28T 0 :::T n #29#02P#28W 0 :::W n jT 0 :::T n #29 P#28W 0 :::W n #29 (2) #19 P#28T i jT i,2 T i,1 #29#02P#28W i jT i #29 (3) where T 0 :::T n is a sequence of elementary trees assigned to the sentence W 0 :::W n . We get (2) by using Bayes theorem and we obtain (3) from (2) by ignore the denominator and by applying the usual Markov assumptions." ></td>
	<td class="line x" title="116:209	The output of this model is a probabilistic ranking of trees for the input sentence which is sensitive to a small local context window." ></td>
	<td class="line x" title="117:209	4.2 H2: Parsing probability model Once the words in a sentence have selected a set of elementary trees, parsing is the process of attaching these trees together to give us a consistent bracketing of the sentences." ></td>
	<td class="line x" title="118:209	Notation: Let #1C stand for an elementary tree which is lexicalized by a word: w and a part of speech tag: p. Let P init (introduced earlier in 2.1) stand for the probability of being root of a derivation tree defined as follows: X #1C P init #28#1C#29=1 including lexical information, this is written as: Pr#28#1C;w;pjtop =1#29= Pr#28#1Cjtop =1#29#02 (4) Pr#28pj#1C;top =1#29#02 (5) Pr#28wj#1C;p;top = 1#29; (6) where the variable top indicates that #1C is the tree that begins the current derivation." ></td>
	<td class="line x" title="119:209	There is a useful approximation for P init : Pr#28#1C;w;pjtop =1#29#19 Pr#28labeljtop =1#29 where label is the label of the root node of #1C." ></td>
	<td class="line x" title="120:209	^ Pr#28labeljtop =1#29= Count#28top =1;label#29+#0B Count#28top =1#29+N#0B (7) where N is the number of bracketing labels and #0B is a constant used to smooth zero counts." ></td>
	<td class="line x" title="121:209	Let P attach (introduced earlier in 2.1) stand for the probability of attachment of #1C 0 into another #1C: P attach #28#1C;#11 ! NA#29+ X #1C 0 P attach #28#1C;#11 ! #1C 0 #29=1 including lexical information, this is written as: Pr#28#1C 0 ;p 0 ;w 0 jNode;#1C;w;p#29 (8) Pr#28NAjNode;#1C;w;p#29 (9) We decompose (8) into the following components: Pr#28#1C 0 ;p 0 ;w 0 jNode;#1C;w;p#29= Pr#28#1C 0 jNode;#1C;w;p#29#02 (10) Pr#28p 0 j#1C 0 ;Node;#1C;w;p#29#02 (11) Pr#28w 0 jp 0 ;#1C 0 ;Node;#1C;w;p#29; (12) We do a similar decomposition for (9)." ></td>
	<td class="line x" title="122:209	For each of the equations above, we use a backoff model which is used to handle sparse data problems." ></td>
	<td class="line x" title="123:209	We compute a backoff model as follows: Let e 1 stand for the original lexicalized model and e 2 be the backoff level which only uses part of speech information: e 1 : Node;#1C;w;p e 2 : Node;#1C;p For both P init and P attach,letc = Count#28e 1 #29.Then the backoff model is computed as follows: #15#28c#29e 1 +#281,#15#28c#29#29e 2 where #15#28c#29= c #28c+D#29 and D is the diversity of e 1 (i.e. the number of distinct counts for e 1 )." ></td>
	<td class="line x" title="124:209	For P attach we further smooth probabilities (10), (11) and (12)." ></td>
	<td class="line x" title="125:209	We use (10) as an example, the other two are handled in the same way." ></td>
	<td class="line x" title="126:209	^ Pr#28#1C 0 jNode;#1C;w;p#29= #28Count#28Node;#1C;w;p;#1C 0 #29+#0B#29 #28Count#28Node;#1C;w;p#29+k#0B#29 (13) Count#28Node;#1C;w;p#29= X y2T 0 Count#28Node;#1C;w;p;y#29 (14) where k is the diversity of adjunction, that is: the number of different trees that can attach at that node." ></td>
	<td class="line x" title="127:209	T 0 is the set of all trees #1C 0 that can possibly attach at Node in tree #1C." ></td>
	<td class="line x" title="128:209	For our experiments, the value of #0B is set to 1 100;000 . 5 Co-Training algorithm We are now in the position to describe the Co-Training algorithm, which combines the models described in Section 4.1 and in Section 4.2 in order to iteratively label a large pool of unlabeled data." ></td>
	<td class="line x" title="129:209	We use the following datasets in the algorithm: labeled a set of sentences bracketed with the correct parse trees." ></td>
	<td class="line x" title="130:209	cache a small pool of sentences which is the focus of each iteration of the Co-Training algorithm." ></td>
	<td class="line x" title="131:209	unlabeled a large set of unlabeled sentences." ></td>
	<td class="line x" title="132:209	The only information we collect from this set of sentences is a tree-dictionary: tree-dict and part-of-speech dictionary: pos-dict." ></td>
	<td class="line x" title="133:209	Construction of these dictionaries is covered in Section 3.2." ></td>
	<td class="line x" title="134:209	In addition to the above datasets, we also use the usual development test set (termed dev in this paper), and a test set (called test) which is used to evaluate the bracketing accuracy of the parser." ></td>
	<td class="line x" title="135:209	The Co-Training algorithm consists of the following steps which are repeated iteratively until all the sentences in the set unlabeled are exhausted." ></td>
	<td class="line x" title="136:209	1." ></td>
	<td class="line x" title="137:209	Input: labeled and unlabeled 2." ></td>
	<td class="line x" title="138:209	Update cache #0F Randomly select sentences from unlabeled and refill cache #0F If cache is empty; exit 3." ></td>
	<td class="line x" title="139:209	Train models H1 and H2 using labeled 4." ></td>
	<td class="line x" title="140:209	Apply H1 and H2 to cache." ></td>
	<td class="line x" title="141:209	5." ></td>
	<td class="line x" title="142:209	Pick most probablen from H1 (run through H2) and add to labeled." ></td>
	<td class="line x" title="143:209	6." ></td>
	<td class="line x" title="144:209	Pick most probable n fromH2andaddtolabeled 7." ></td>
	<td class="line x" title="145:209	n = n + k;GotoStep2 For the experiment reported here, n =10,andk was set to be n in each iteration." ></td>
	<td class="line x" title="146:209	We ran the algorithm for 12 iterations (covering 20480 of the sentences in unlabeled) and then added the best parses for all the remaining sentences." ></td>
	<td class="line x" title="147:209	6 Experiment 6.1 Setup The experiments we report were done on the Penn Treebank WSJ Corpus (Marcus et al. , 1993)." ></td>
	<td class="line x" title="148:209	The various settings for the Co-Training algorithm (from Section 5) are as follows: #0F labeled was set to Sections 02-06 of the Penn Treebank WSJ (9625 sentences) #0F unlabeled was 30137 sentences (Section 07-21 of the Treebank stripped of all annotations)." ></td>
	<td class="line x" title="149:209	#0F A tag dictionary of all lexicalized trees from labeled and unlabeled." ></td>
	<td class="line x" title="150:209	#0F Novel trees were treated as unknown tree tokens." ></td>
	<td class="line x" title="151:209	#0F The cache size was 3000 sentences." ></td>
	<td class="line x" title="152:209	While it might seem expensive to run the parser over the cache multiple times, we use the pruning capabilities of the parser to good use here." ></td>
	<td class="line x" title="153:209	During the iterations we set the beam size to a value which is likely to prune out all derivations for a large portion of the cache except the most likely ones." ></td>
	<td class="line x" title="154:209	This allows the parser to run faster, hence avoiding the usual problem with running an iterative algorithm over thousands of sentences." ></td>
	<td class="line x" title="155:209	In the initial runs we also limit the length of the sentences entered into the cache because shorter sentences are more likely to beat out the longer sentences in any case." ></td>
	<td class="line x" title="156:209	The beam size is reset when running the parser on the test data to allow the parser a better chance at finding the most likely parse." ></td>
	<td class="line x" title="157:209	6.2 Results We scored the output of the parser on Section 23 of the Wall Street Journal Penn Treebank." ></td>
	<td class="line x" title="158:209	The following are some aspects of the scoring that might be useful for comparision with other results: No punctuations are scored, including sentence final punctuation." ></td>
	<td class="line x" title="159:209	Empty elements are not scored." ></td>
	<td class="line x" title="160:209	We used EVALB (written by Satoshi Sekine and Michael Collins) which scores based on PARSEVAL (Black et al. , 1991); with the standard parameter file (as per standard practice, part of speech brackets were not part of the evaluation)." ></td>
	<td class="line x" title="161:209	Also, we used Adwait Ratnaparkhis part-of-speech tagger (Ratnaparkhi, 1996) to tag unknown words in the test data." ></td>
	<td class="line x" title="162:209	We obtained 80.02% and 79.64% labeled bracketing precision and recall respectively (as defined in (Black et al. , 1991))." ></td>
	<td class="line x" title="163:209	The baseline model which was only trained on the 9695 sentences of labeled data performed at 72.23% and 69.12% precision and recall." ></td>
	<td class="line x" title="164:209	These results show that training a statistical parser using our Cotraining method to combine labeled and unlabeled data strongly outperforms training only on the labeled data." ></td>
	<td class="line x" title="165:209	It is important to note that unlike previous studies, our method of moving towards unsupervised parsing are directly compared to the output of supervised parsers." ></td>
	<td class="line x" title="166:209	Certain differences in the applicability of the usual methods of smoothing to our parser cause the lower accuracy as compared to other state of the art statistical parsers." ></td>
	<td class="line x" title="167:209	However, we have consistently seen increase in performance when using the Co-Training method over the baseline across several trials." ></td>
	<td class="line x" title="168:209	It should be emphasised that this is a result based on less than 20% of data that is usually used by other parsers." ></td>
	<td class="line x" title="169:209	We are experimenting with the use of an even smaller set of labeled data to investigate the learning curve." ></td>
	<td class="line x" title="170:209	7 Previous Work: Combining Labeled and Unlabeled Data The two-step procedure used in our Co-Training method for statistical parsing was incipient in the SuperTagger (Srinivas, 1997) which is a statistical model for tagging sentences with elementary lexicalized structures." ></td>
	<td class="line x" title="171:209	This was particularly so in the Lightweight Dependency Analyzer (LDA), which used shortest attachment heuristics after an initial SuperTagging stage to find syntactic dependencies between words in a sentence." ></td>
	<td class="line x" title="172:209	However, there was no statistical model for attachments and the notion of mutual constraints between these two steps was not exploited in this work." ></td>
	<td class="line x" title="173:209	Previous studies in unsupervised methods for parsing have concentrated on the use of inside-outside algorithm (Lari and Young, 1990; Carroll and Rooth, 1998)." ></td>
	<td class="line x" title="174:209	However, there are several limitations of the inside-outside algorithm for unsupervised parsing, see (Marcken, 1995) for some experiments that draw out the mismatch between minimizing error rate and iteratively increasing the likelihood of the corpus." ></td>
	<td class="line x" title="175:209	Other approaches have tried to move away from phrase structural representations into dependency style parsing (Lafferty et al. , 1992; Fong and Wu, 1996)." ></td>
	<td class="line x" title="176:209	However, there are still inherent computational limitations due to the vast search space (see (Pietra et al. , 1994) for discussion)." ></td>
	<td class="line x" title="177:209	None of these approaches can even be realistically compared to supervised parsers that are trained and tested on the kind of representations and the complexity of sentences that are found in the Penn Treebank." ></td>
	<td class="line x" title="178:209	(Chelba and Jelinek, 1998) combine unlabeled and labeled data for parsing with a view towards language modeling applications." ></td>
	<td class="line x" title="179:209	The goal in their work is not to get the right bracketing or dependencies but to reduce the word error rate in a speech recognizer." ></td>
	<td class="line x" title="180:209	Our approach is closely related to previous CoTraining methods (Yarowsky, 1995; Blum and Mitchell, 1998; Goldman and Zhou, 2000; Collins and Singer, 1999)." ></td>
	<td class="line x" title="181:209	(Yarowsky, 1995) first introduced an iterative method for increasing a small set of seed data used to disambiguate dual word senses by exploiting the constraint that in a segment of discourse only one sense of a word is used." ></td>
	<td class="line x" title="182:209	This use of unlabeled data improved performance of the disambiguator above that of purely supervised methods." ></td>
	<td class="line x" title="183:209	(Blum and Mitchell, 1998) further embellish this approach and gave it the name of CoTraining." ></td>
	<td class="line x" title="184:209	Their definition of Co-Training includes the notion (exploited in this paper) that different models can constrain each other by exploiting different views of the data." ></td>
	<td class="line x" title="185:209	They also prove some PAC results on learnability." ></td>
	<td class="line x" title="186:209	They also discuss an application of classifying web pages by using their method of mutually constrained models." ></td>
	<td class="line x" title="187:209	(Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree (called CoBoosting)." ></td>
	<td class="line x" title="188:209	(Goldman and Zhou, 2000) provide a variant of Co-Training which is suited to the learning of decision trees where the data is split up into different equivalence classes for each of the models and they use hypothesis testing to determine the agreement between the models." ></td>
	<td class="line x" title="189:209	In future work we would like to experiment whether some of these ideas could be incorporated into our model." ></td>
	<td class="line x" title="190:209	In future work we would like to explore use of the entire 1M words of the WSJ Penn Treebank as our labeled data and to use a larger set of unbracketed WSJ data as input to the Co-Training algorithm." ></td>
	<td class="line x" title="191:209	In addition, we plan to explore the following points that bear on understanding the nature of the Co-Training learning algorithm: #0F The contribution of the dictionary of trees extracted from the unlabeled set is an issue that we would like to explore in future experiments." ></td>
	<td class="line x" title="192:209	Ideally, we wish to design a co-training method where no such information is used from the unlabeled set." ></td>
	<td class="line x" title="193:209	#0F The relationship between co-training and EM bears investigation." ></td>
	<td class="line x" title="194:209	(Nigam and Ghani, 2000) is a study which tries to separate two factors: (1) The gradient descent aspect of EM vs. the iterative nature of co-training and (2) The generative model used in EM vs. the conditional independence between the features used by the two models that is exploited in co-training." ></td>
	<td class="line x" title="195:209	Also, EM has been used successfully in text classification in combination of labeled and unlabeled data (see (Nigam et al. , 1999))." ></td>
	<td class="line x" title="196:209	#0F In our experiments, unlike (Blum and Mitchell, 1998) we do not balance the label priors when picking new labeled examples for addition to the training data." ></td>
	<td class="line x" title="197:209	One way to incorporate this into our algorithm would be to incorporate some form of sample selection (or active learning) into the selection of examples that are considered as labeled with high confidence (Hwa, 2000)." ></td>
	<td class="line x" title="198:209	8 Conclusion In this paper, we proposed a new approach for training a statistical parser that combines labeled with unlabeled data." ></td>
	<td class="line x" title="199:209	It uses a Co-Training method where a pair of models attempt to increase their agreement on labeling the data." ></td>
	<td class="line x" title="200:209	The algorithm takes as input a small corpus of 9695 sentences (234467 word tokens) of bracketed data, a large pool of unlabeled text and a tag dictionary of lexicalized structures for each word in this training set (based on the LTAG formalism)." ></td>
	<td class="line x" title="201:209	The algorithm presented iteratively labels the unlabeled data set with parse trees." ></td>
	<td class="line x" title="202:209	We then train a statistical parser on the combined set of labeled and unlabeled data." ></td>
	<td class="line x" title="203:209	We obtained 80.02% and 79.64% labeled bracketing precision and recall respectively." ></td>
	<td class="line x" title="204:209	The baseline model which was only trained on the 9695 sentences of labeled data performed at 72.23% and 69.12% precision and recall." ></td>
	<td class="line x" title="205:209	These results show that training a statistical parser using our Co-training method to combine labeled and unlabeled data strongly outperforms training only on the labeled data." ></td>
	<td class="line x" title="206:209	It is important to note that unlike previous studies, our method of moving towards unsupervised parsing can be directly compared to the output of supervised parsers." ></td>
	<td class="line x" title="207:209	Unlike previous approaches to unsupervised parsing our method can be trained and tested on the kind of representations and the complexity of sentences that are found in the Penn Treebank." ></td>
	<td class="line x" title="208:209	In addition, as a byproduct of our representation we obtain more than the phrase structure of each sentence." ></td>
	<td class="line x" title="209:209	We also produce a more embellished parse in which phenomena such as predicate-argument structure, subcategorization and movement are given a probabilistic treatment." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="J02-1004
Syllable-Pattern-Based Unknown-Morpheme Segmentation And Estimation For Hybrid Part-Of-Speech Tagging Of Korean
Lee, Gary Geunbae;Cha, Jeongwon;Lee, Jong-Hyeok;"></td>
	<td class="line x" title="1:289	Syllable-Pattern-Based UnknownMorpheme Segmentation and Estimation for Hybrid Part-of-Speech Tagging of Korean Gary Geunbae Lee Jeongwon Cha y Pohang University of Science and Technology Pohang University of Science and Technology Jong-Hyeok Lee z Pohang University of Science and Technology Most errors in Korean morphological analysis and part-of-speech (POS) tagging are caused by unknown morphemes." ></td>
	<td class="line x" title="2:289	This paper presents a syllable-pattern-based generalized unknownmorpheme-estimation method with POSTAG (POStech TAGger), 1 which is a statistical and rule-based hybrid POS tagging system." ></td>
	<td class="line x" title="3:289	This method of guessing unknown morphemes is based on a combination of a morpheme pattern dictionary that encodes general lexical patterns of Korean morphemes with a posteriori syllable trigram estimation." ></td>
	<td class="line x" title="4:289	The syllable trigrams help to calculate lexical probabilities of the unknown morphemes and are utilized to search for the best tagging result." ></td>
	<td class="line x" title="5:289	This method can guess the POS tags of unknown morphemes regardless of their numbers and/or positions in an eojeol (a Korean spacing unit similar to an English word), which is not possible with other systems for tagging Korean." ></td>
	<td class="line x" title="6:289	In a series of experiments using three different domain corpora, the system achieved a 97% tagging accuracy even though 10% of the morphemes in the test corpora were unknown." ></td>
	<td class="line x" title="7:289	It also achieved very high coverage and accuracy of estimation for all classes of unknown morphemes." ></td>
	<td class="line x" title="8:289	1." ></td>
	<td class="line x" title="9:289	Introduction Part-of-speech (POS) tagging involves many difficult problems, such as insufficient amounts of training data, inherent POS ambiguities, and (most seriously) many types of unknown words." ></td>
	<td class="line x" title="10:289	Unknown words are ubiquitous in any application and cause major tagging failures in many cases." ></td>
	<td class="line x" title="11:289	Since Korean is an agglutinative language, it presents more serious problems with unknown morphemes than with unknown words because more than one morpheme can be unknown in a single word and morpheme segmentation is usually very difficult." ></td>
	<td class="line x" title="12:289	NLP Laboratory, Electrical and Computer Engineering Division, Pohang University of Science and Technology (POSTECH), Pohang, 790-784, Korea." ></td>
	<td class="line x" title="13:289	E-mail: gblee@postech.ac.kr." ></td>
	<td class="line x" title="14:289	y NLP Laboratory, Electrical and Computer Engineering Division, Pohang University of Science and Technology (POSTECH), Pohang, 790-784, Korea." ></td>
	<td class="line x" title="15:289	E-mail: himen@postech.ac.kr." ></td>
	<td class="line x" title="16:289	z NLP Laboratory, Electrical and Computer Engineering Division, Pohang University of Science and Technology (POSTECH), Pohang, 790-784, Korea." ></td>
	<td class="line x" title="17:289	E-mail: jhlee@postech.ac.kr." ></td>
	<td class="line x" title="18:289	1 The binary code of POSTAG is open to the public for research and evaluation purposes at http://nlp.postech.ac.kr/." ></td>
	<td class="line x" title="19:289	Follow the link OpenResources!DownLoad." ></td>
	<td class="line x" title="20:289	c 2002 Association for Computational Linguistics Computational Linguistics Volume 28, Number 1 Previous techniques for guessing unknown words mostly utilize the guessing rules to analyze the word features by looking at leading and trailing characters." ></td>
	<td class="line x" title="21:289	Most of them employ the analysis of trailing characters and other features such as capitalization and hyphenation (Kupiec 1992; Weischedel et al. 1993)." ></td>
	<td class="line x" title="22:289	Some of them use more morphologically oriented word features such as suffixes, prefixes, and character lengths (Brill 1995; Voutilainen 1995)." ></td>
	<td class="line x" title="23:289	The guessing rules are usually handcrafted using knowledge of morphology but sometimes are acquired automatically using lexicons and corpora (Brill 1995; Mikheev 1996; Oflazer and T ur 1996)." ></td>
	<td class="line x" title="24:289	Previously developed methods for guessing unknown morphemes in Korean are not much different from the methods used for English." ></td>
	<td class="line x" title="25:289	Basically, they rely on the rules that reflect knowledge of Korean morphology and word formation." ></td>
	<td class="line x" title="26:289	The usual way of handling unknown morphemes is to guess all the possible POS tags for an unknown morpheme by checking connectable functional morphemes in the same eojeol (Kang 1993)." ></td>
	<td class="line x" title="27:289	2 However, in this way, it is only possible to guess probable POS tags for a single unknown morpheme when it occurs at the beginning of an eojeol." ></td>
	<td class="line x" title="28:289	Unlike in English, in Korean, more than one unknown morpheme can appear in a single eojeol because an eojeol can include complex components such as Chinese characters, Japanese words, and other foreign words." ></td>
	<td class="line x" title="29:289	If an eojeol contains more than one unknown morpheme or if the unknown morphemes appear in other than first position in the eojeol, all previous methods fail to efficiently estimate them." ></td>
	<td class="line x" title="30:289	This is the reason why we try to avoid conventional guessing rules using word morphology features such as those proposed in Mikheev (1996) and Oflazer and T ur (1996)." ></td>
	<td class="line x" title="31:289	3 In this paper, we propose a syllable-pattern-based generalized unknown-morpheme estimation method using a morpheme pattern dictionary that enables us to treat unknown morphemes in the same way as registered known morphemes, and thereby to guess them regardless of their numbers or positions in an eojeol." ></td>
	<td class="line x" title="32:289	The method for estimating unknown morphemes using the morpheme pattern dictionary in Korean needs to be tightly integrated into morphological analysis and POS disambiguation systems." ></td>
	<td class="line oc" title="33:289	POS disambiguation has usually been performed by statistical approaches, mainly using the hidden Markov model (HMM) in English research communities (Cutting et al. 1992; Kupiec 1992; Weischedel et al. 1993)." ></td>
	<td class="line o" title="34:289	These approaches are also dominant for Korean, with slight improvements to accommodate the agglutinative nature of Korean." ></td>
	<td class="line x" title="35:289	For Korean, early HMM tagging was based on eojeols." ></td>
	<td class="line x" title="36:289	The eojeol-based tagging model calculates lexical and transition probabilities with eojeols as a unit; it suffers from severe data sparseness problems since a single eojeol consists of many different morphemes (Lee, Choi, and Kim 1993)." ></td>
	<td class="line x" title="37:289	Later, morpheme-based HMM tagging was tried; such models assign a single tag to a morpheme regardless of the space in a sentence." ></td>
	<td class="line x" title="38:289	Morpheme-based tagging can reduce data sparseness problems but incurs multiple observation sequences in Viterbi decoding since an eojeol can be segmented in many different ways." ></td>
	<td class="line x" title="39:289	Researchers then tried many ways of reducing computation due to multiple observation sequences, such as shared word sequences and virtual words (Kim, Lim, and Seo 1995) and two-ply HMM for morpheme unit computation but restricted within an eojeol (Kim, Im, and Im 1996)." ></td>
	<td class="line x" title="40:289	However, since statistical approaches take neighboring tags into account only within a limited win2Aneojeol is a Korean spacing unit (similar to an English word), which usually consists of one or more stem morphemes and a series of functional morphemes." ></td>
	<td class="line x" title="41:289	3 Even though Turkish and Finnish are in the same class of agglutinative languages and German also has very complex morphological structures, in our view word formation is more diverse and complex in Korean than in these Western languages because of its mix of Oriental and Western culture." ></td>
	<td class="line x" title="42:289	54 Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation dow (usually two or three), sometimes the decision fails to cover important linguistic contexts necessary for POS disambiguation." ></td>
	<td class="line x" title="43:289	Also, approaches using only statistical methods are inappropriate for idiomatic expressions, for which lexical terms need to be directly referenced." ></td>
	<td class="line x" title="44:289	And especially, statistical approaches alone do not suffice for agglutinative languages, which usually have complex morphological structures." ></td>
	<td class="line x" title="45:289	In agglutinative languages, a word usually consists of one or more stem morphemes plus a series of functional morphemes; therefore, each morpheme should receive a POS tag appropriate to its functional role to cope with the complex morphological phenomena in such languages." ></td>
	<td class="line x" title="46:289	Recently, rule-based approaches, which learn symbolic tagging rules automatically from a corpus, have been reconsidered, to overcome the limitations of statistical approaches (Brill 1995)." ></td>
	<td class="line x" title="47:289	Some systems even perform POS tagging as part of a syntactic analysis process (Voutilainen 1995)." ></td>
	<td class="line x" title="48:289	Following the success of transformation-based approaches, attempts have been made to use transformation rules in systems for tagging Korean (Im, Kim, and Im 1996)." ></td>
	<td class="line x" title="49:289	However, in general, rule-based approaches alone are not very robust and are not portable enough to be adjusted to new tagsets or new languages." ></td>
	<td class="line x" title="50:289	Also, they usually perform no better than their statistical counterparts (Brill 1995)." ></td>
	<td class="line x" title="51:289	To gain portability and robustness and also to overcome the limited coverage of statistical approaches, we need to somehow combine the two approaches to gain the advantages of each." ></td>
	<td class="line x" title="52:289	In this paper, we propose a hybrid method that combines statistical and rule-based approaches to POS disambiguation and can be tightly coupled with generalized unknown-morpheme-guessing techniques." ></td>
	<td class="line x" title="53:289	2." ></td>
	<td class="line x" title="54:289	Linguistic Characteristics of Korean Korean is classified as an agglutinative language." ></td>
	<td class="line x" title="55:289	In Korean, an eojeol consists of several morphemes that have clear-cut morpheme boundaries." ></td>
	<td class="line x" title="56:289	For example, na-neun gamgi-e geol-lyeoss-dda I caught a cold consists of 3 eojeols and 7 morphemes: 4 na(I)/T + neun(auxiliary particle)/jS, gam-gi(cold)/MC + e(adverb and conjunctive particle)/jO, geol-li(catch)/DR + eoss(past tense)/eGS + dda(final ending)/eGE." ></td>
	<td class="line x" title="57:289	Below are the characteristics of Korean that must be considered for morphological-level natural language processing and POS tagging." ></td>
	<td class="line x" title="58:289	POS tagging of Korean is usually performed on a morpheme basis rather than on an eojeol basis." ></td>
	<td class="line x" title="59:289	Accordingly, morphological analysis is essential to POS tagging because morpheme segmentation is much more important and difficult than POS assignment." ></td>
	<td class="line x" title="60:289	Moreover, morphological analysis should segment eojeols that contain unknown morphemes as well as known morphemes." ></td>
	<td class="line x" title="61:289	Hence, unknown-morpheme handling should be integrated into the morphological analysis process." ></td>
	<td class="line x" title="62:289	Because a single eojeol can have many possible analyses (e.g. , na-neun: na(I)/T + neun(topic marker)/jS, na(sprout)/DR + neun(adnominal)/eCNMG, nal(fly)/DI + neun(adnominal)/eCNMG, morpheme segmentation is inherently ambiguous." ></td>
	<td class="line x" title="63:289	Korean is a postpositional language with many kinds of noun endings (particles), verb endings, and prefinal verb endings." ></td>
	<td class="line x" title="64:289	It is these functional morphemes, rather than the order of eojeols, that determine grammatical 4 Here, + represents a morpheme boundary in an eojeol and / introduces the POS tag symbols (see Table 2)." ></td>
	<td class="line x" title="65:289	55 Computational Linguistics Volume 28, Number 1 Table 1 Sample distribution of unknown morphemes in Korean." ></td>
	<td class="line x" title="66:289	Tag # morphemes Tag # morphemes MC 2,888 (29.7%) S 1,358 (14.0%) MPN 650 (6.7%) B 603 (6.2%) MPP 235 (2.4%) T 50 (0.5%) MPC 56 (0.6%) Symbol 10 (0.1%) MPO 728 (7.5%) Foreign word 3,140 (32.3%) relations such as a nouns syntactic function, a verbs tense, aspect, modals, and even modifying relations between eojeols." ></td>
	<td class="line x" title="67:289	For example, ga/jC is a case particle, so the eojeol uri(we)-ga has a subject role due to the particle ga/jC." ></td>
	<td class="line x" title="68:289	Korean has a clear syllable structure within the morpheme; most nominal content morphemes keep their surface form when they are combined with functional morphemes." ></td>
	<td class="line x" title="69:289	Korean is basically an SOV language but has relatively free word order compared with English." ></td>
	<td class="line x" title="70:289	The weight, in Equation (1) (Section 4.1) reflects the fact that transition probability is less important in Korean than in English." ></td>
	<td class="line x" title="71:289	However, Korean does have some word order constraints: verbs must appear in sentence-final position, and modifiers must be placed before the element they modify." ></td>
	<td class="line x" title="72:289	So some order constraints must be selectively utilized as contextual information in the POS tagging process, which is taken well into account in the design of error correction rules (Section 4.3)." ></td>
	<td class="line x" title="73:289	Complex spelling changes (irregular conjugations) frequently occur between morphemes when two morphemes combine to form an eojeol." ></td>
	<td class="line x" title="74:289	These spelling changes make it difficult to segment the original morphemes before the POS tag symbols are assigned." ></td>
	<td class="line x" title="75:289	The unknown-morpheme problem in Korean differs in some ways from the unknown-word problem in English." ></td>
	<td class="line x" title="76:289	In English, it is easy to identify unknown words because they occur between spaces." ></td>
	<td class="line x" title="77:289	However, in Korean, since unknown morphemes are hidden in an eojeol, we only know that morphological analysis failed in that eojeol; pinpointing the exact unknown morphemes is usually difficult." ></td>
	<td class="line x" title="78:289	This is why, unlike in English, it is not possible to fully guess an unknown morpheme using only affixes." ></td>
	<td class="line x" title="79:289	The distribution of POS tags for unknown morphemes extracted from a 130,000-morpheme training corpus (9,718 unknown morphemes) is shown in Table 1." ></td>
	<td class="line x" title="80:289	The distribution from even a small corpus shows that we need to estimate various parts of speech for unknown morphemes rather than simply guess them as nouns." ></td>
	<td class="line x" title="81:289	Table 2 shows the tagset that was used in the experiments reported in Section 5." ></td>
	<td class="line x" title="82:289	The tagset was selected from hierarchically organized POS tags for Korean." ></td>
	<td class="line x" title="83:289	We defined about 100 different POS tags, which can be used in morphological analysis as well as in POS tagging." ></td>
	<td class="line x" title="84:289	We also designed over 300 morphotactic adjacency symbols to be used in morpheme connectivity checks for correct morpheme segmentation (to be explained in the next section)." ></td>
	<td class="line x" title="85:289	The POS tags are hierarchically organized symbols 56 Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation Table 2 A tagset with 41 tags." ></td>
	<td class="line x" title="86:289	Major category Tag Description Nominal MC common noun MPN person name MPC country name MPP place name MPO other proper noun MD bound noun T pronoun S numeral Predicate DR regular verb DI irregular verb HR regular adjective HI irregular adjective I i-predicative particle E existential predicate b auxiliary verb Modifier G adnoun B adverb Particle y predicative particle jC case particle jS auxiliary particle jO adverb and conjunctive particle Ending eGE final ending eGS prefinal ending eCNDI aux conj ending eCNDC quote conj ending eCNMM nominal ending eCNMG adnominal ending eCNB adverbial ending eCC conjunctive ending Affix + prefix  suffix Special symbol su unit symbol s left parenthesis s right parenthesis s. sentence closer ssentence connection s, sentence comma sf foreign word sh Chinese character so other symbol Interjection K interjection 57 Computational Linguistics Volume 28, Number 1 that were iteratively refined from the eight major grammatical categories of Korean: nominal, predicate, modifier, particle, ending, affix, special symbol, and interjection." ></td>
	<td class="line x" title="87:289	For a given morpheme, the acronym of a path name in the symbol hierarchy up to a certain level is assigned as a POS tag." ></td>
	<td class="line x" title="88:289	5 The rest of the detailed hierarchies, which are related only to morpheme connectivity, are independently assigned as morphotactic adjacency symbols." ></td>
	<td class="line x" title="89:289	Therefore, we can use either full or partial path names as POS tags in order to adjust the total number of tags." ></td>
	<td class="line x" title="90:289	The size of the tagset can thus be adapted by refining grammatical categories that are more pertinent to a given application." ></td>
	<td class="line x" title="91:289	For example, for text-indexing applications, we refine nominals more than predicates since index terms are usually nominals in these applications." ></td>
	<td class="line x" title="92:289	3." ></td>
	<td class="line x" title="93:289	Unknown-Morpheme Segmentation during Morphological Analysis The agglutinative nature of Korean inevitably requires doing morphological analysis before POS tagging." ></td>
	<td class="line x" title="94:289	Morphological analysis, which segments input texts into morphotactically connectable morphemes and assigns all possible POS tags to each morpheme by looking them up in a morpheme dictionary, is a basic step in natural language processing." ></td>
	<td class="line x" title="95:289	Our morphological analysis follows three general steps (Sproat 1992): morpheme segmentation, recovering original morphemes from spelling changes, and morphotactic modeling." ></td>
	<td class="line x" title="96:289	Input texts are scanned from left to right, character by character, 6 to be matched with morphemes in a morpheme dictionary." ></td>
	<td class="line x" title="97:289	The morpheme dictionary has a trie structured index for fast matching." ></td>
	<td class="line x" title="98:289	It also has an independent entry for each variant surface form (called allomorph) of the original morpheme so the original morphemes can easily be reconstructed from spelling changes (see Table 3)." ></td>
	<td class="line x" title="99:289	For morphotactic modeling, we used the POS tags and the morphotactic adjacency symbols in the dictionary." ></td>
	<td class="line x" title="100:289	The POS tags provide information about morpheme class, while the morphotactic adjacency symbols provide information about grammatical connectivity between morphemes needed to form an eojeol." ></td>
	<td class="line x" title="101:289	The full hierarchy of POS tags and morphotactic adjacency symbols is encoded in the morpheme dictionary for each morpheme." ></td>
	<td class="line x" title="102:289	Besides the morpheme dictionary, to model morphemes connectability to one another the system uses an independent morpheme connectivity table that encodes all the connectable pairs of morpheme groups using the morphemes tags and morphotactic adjacency symbol patterns." ></td>
	<td class="line x" title="103:289	After an input eojeol is segmented by trie indexed dictionary searches, the morphological analysis checks whether each segmentation is grammatically connectable by looking in the morpheme connectivity table." ></td>
	<td class="line x" title="104:289	For unknown-morpheme segmentation, we developed a generalized method for estimating unknown morphemes regardless of their position and number." ></td>
	<td class="line x" title="105:289	Using a morpheme pattern dictionary, our system can look up unknown morphemes exactly the same way it looks up known registered morphemes." ></td>
	<td class="line x" title="106:289	The morpheme pattern dictionary covers all the necessary syllable patterns for unknown morphemes, including common nouns, proper nouns, adverbs, regular and irregular verbs, regular and irregular adjectives, and special symbols for foreign words." ></td>
	<td class="line x" title="107:289	The lexical patterns for morphemes are collected from previous studies (Kang 1993) where the constraints on Korean syllable patterns regarding morpheme connectivity are well described." ></td>
	<td class="line x" title="108:289	Table 4 shows some sample entries in the morpheme pattern dictionary, where Z, V, * are 5 For example, nominal(M):proper-noun(P):person-name(N) is a three-level path name." ></td>
	<td class="line x" title="109:289	6 The character sequence in na-neun is n, a, n, eu, n. 58 Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation Table 3 Examples of morpheme dictionary entries." ></td>
	<td class="line x" title="110:289	MCC is a full POS tag that identifies a common noun consisting of Chinese characters." ></td>
	<td class="line x" title="111:289	MCK identifies a common noun consisting only of Korean characters." ></td>
	<td class="line x" title="112:289	DIgeo-la represents a geo-la irregular verb, and HIl represents an l irregular adjective." ></td>
	<td class="line x" title="113:289	Yu represents that ga-gong has a final consonant (ng)." ></td>
	<td class="line x" title="114:289	D-ha, H-ha, and D-doe are morphotactic adjacency symbols for predicate particles." ></td>
	<td class="line x" title="115:289	Nominals that have a D-ha as a morphotactic adjacency symbol can be connected with predicate particles, and they play the role of a verb or adjective." ></td>
	<td class="line x" title="116:289	In verb or adjective, gyu represents a regular form of an irregular conjugation, bul represents an irregular form of an irregular conjugation." ></td>
	<td class="line x" title="117:289	Eo is a morphotactic adjacency symbol for vowel harmony when connecting with endings." ></td>
	<td class="line x" title="118:289	Chug-yag represents that a particular verb (or adjective) contains the special contracted ending." ></td>
	<td class="line x" title="119:289	> is a special symbol for adjacent direction (>= right connection; <= left connection)." ></td>
	<td class="line x" title="120:289	POS-tag<original form> (Allomorph) [Morphotactic adjacency symbols] MCC<ga-gong> (ga-gong) [yu>D-ha>H-ha>D-doe>] MCK<geo-leum> (geo-leum) [yu>D-ha>] DIgeo-la<geon-neo-ga> (geon-neo-ga) [gyu>chug-yag>] DId<al-a-deud> (al-a-deud) [gyu>] DId<al-a-deud> (al-a-deul) [bul>eo>] DIs<heu-li-jeos> (heu-li-jeo) [bul>eo>] DIs<heu-li-jeos> (heu-li-jeos) [gyu>] HIl<ga-neul> (ga-neu) [bul>] HIl<ga-neul> (ga-neul) [gyu>eo>] Table 4 Sample entries in the morpheme pattern dictionary." ></td>
	<td class="line x" title="121:289	Symbol meanings are explained in Table 3." ></td>
	<td class="line x" title="122:289	POS-tag<original form> (Allomorph) [Morphotactic adjacency symbols] HIl<ZV*gal> (ZV*gal) [gyu>eo>] HIl<ZV*gal> (ZV*ga) [bul>] HIb<ZV*ZVb> (ZV*u) [bul>] HIb<ZV*ZVb> (ZV*weo) [chug-yag>] HIb<ZV*ZVb> (ZV*wa) [chug-yag>] DIs<ZV*jeos> (ZV*jeos) [gyu>] DIs<ZV*jeos> (ZV*jeo) [bul>eo>] DId<ZV*deud> (ZV*deud) [gyu>] DId<ZV*deud> (ZV*deul) [bul>eo>] metacharacters that indicate a consonant, a vowel, and any number of Korean characters, respectively." ></td>
	<td class="line x" title="123:289	For example, go-ma-weo thanks, which is a morpheme and an eojeol at the same time, is matched to (ZV*weo) (shown in Table 4, where it is b, irregular adjective (HIb)) in the morpheme pattern dictionary, which allows the system to recover its original morpheme form go-mab." ></td>
	<td class="line x" title="124:289	Once the unknown morphemes are identified and recovered using the pattern dictionary, when checking the unknown morphemes to see if they are connectable, the system can use the same information about adjacent morphemes in the unknown morphemes eojeol that it would use if they were known morphemes." ></td>
	<td class="line x" title="125:289	This is the reason why our method can be called generalized and can identify unknown morphemes regardless of their position and number in an eojeol." ></td>
	<td class="line x" title="126:289	The actual POS estimation is integrated into the POS tagging process that will be described in Section 4.2." ></td>
	<td class="line x" title="127:289	The essential 59 Computational Linguistics Volume 28, Number 1 morphological analyzer statistical POS tagger post errorcorrector morpheme dictionary morpheme pattern dictionary morpheme connectivity table Input sentence lexical / transition probabilities syllable trigram morpheme graph morpheme graph tagged sentence errorcorrecting rules Figure 1 Statistical and rule-based hybrid architecture for POS tagging of Korean." ></td>
	<td class="line x" title="128:289	idea of the morpheme pattern dictionary is to pre-collect all the possible general lexical patterns of Korean morphemes and encode each lexical syllable pattern with all the candidate POS tags." ></td>
	<td class="line x" title="129:289	Therefore, the system can assign initial POS tags to each unknown morpheme simply by matching the syllable patterns in the pattern dictionary." ></td>
	<td class="line x" title="130:289	In this way, unlike previous approaches, ours does not need to incorporate a special rulebased unknown-morpheme-handling module into its morphological analyzer, and all the possible POS tags can be assigned to unknown morphemes just as they are to known morphemes." ></td>
	<td class="line x" title="131:289	4." ></td>
	<td class="line x" title="132:289	A Statistical and Rule-Based Hybrid Tagging Model Figure 1 shows a proposed hybrid architecture for POS tagging of Korean with syllablepattern-based generalized unknown-morpheme guessing." ></td>
	<td class="line x" title="133:289	It has three major components: the morphological analyzer with unknown-morpheme handler, the statistical POS tagger, and the rule-based error corrector." ></td>
	<td class="line x" title="134:289	The morphological analyzer segments the morphemes from input eojeols and reconstructs the original morphemes from spelling changes by recovering the irregular conjugations." ></td>
	<td class="line x" title="135:289	It also assigns all possible POS tags to each morpheme by consulting a morpheme dictionary." ></td>
	<td class="line x" title="136:289	The unknownmorpheme handler, which is tightly integrated into the morphological analyzer, assigns initial POS tags to morphemes that are not registered in the dictionary, as explained in the previous section." ></td>
	<td class="line x" title="137:289	The statistical POS tagger runs the Viterbi algorithm (Forney 1973) on the morpheme graph to search for the optimal tag sequence for POS disambiguation." ></td>
	<td class="line x" title="138:289	To remedy the defects of a statistical POS tagger, we developed an a posteriori error correction mechanism." ></td>
	<td class="line x" title="139:289	The error corrector is a rule-based transformer 60 Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation (Brill 1995), and it corrects mistagged morphemes by consulting lexical patterns and necessary contextual information." ></td>
	<td class="line x" title="140:289	4.1 The Statistical POS Tagger The statistical POS tagging model takes the morpheme graph (output of the morphological analyzer) and selects the best morpheme and POS tag sequence 7 for sentences represented in the morpheme graph." ></td>
	<td class="line x" title="141:289	The morpheme graph is a compact way of representing multiple morpheme sequences for a sentence." ></td>
	<td class="line x" title="142:289	Each morphemes tag is a node in the graph and its morpheme connectivity is a link." ></td>
	<td class="line oc" title="143:289	Our statistical tagging model is modified from the standard bigrams (Cutting et al. 1992) using Viterbi search plus onthe-fly extra computing of lexical probabilities for unknown morphemes." ></td>
	<td class="line x" title="144:289	The equation used for the statistical tagging model is a modified bigram model with left-to-right search, T = argmax T n Y i=1 Pr(t i jt i1 ) Pr(t i jm i ) Pr(t i ) (1) where T is an optimal tag sequence that maximizes the forward Viterbi scores." ></td>
	<td class="line x" title="145:289	Pr(t i jt i1 ) is a bigram tag transition probability and Pr(t i jm i ) Pr(t i ) is a modified morpheme lexical probability." ></td>
	<td class="line x" title="146:289	and are weights and are set at 0.4 and 0.6, respectively, which means that lexical probability is more important than transition probability given the relatively free word order of Korean." ></td>
	<td class="line x" title="147:289	This equation was finally selected after extensive experiments using the following six equations: T = argmax T n Y i=1 Pr(t i jt i1 )Pr(m i jt i ) (2) T = argmax T n Y i=1 Pr(t i jt i1 ) Pr(m i jt i ) (3) T = argmax T n Y i=1 Pr(t i jt i1 )Pr(t i jm i ) (4) T = argmax T n Y i=1 Pr(t i jt i1 ) Pr(t i jm i ) (5) T = argmax T n Y i=1 Pr(t i jt i1 ) Pr(t i jm i ) Pr(t i ) (6) T = argmax T n Y i=1 Pr(t i jt i1 ) Pr(t i jm i ) Pr(t i ) (7) In the experiments, we used the 10,204-morpheme training corpus from the Kemong Encyclopedia." ></td>
	<td class="line x" title="148:289	8 Table 5 shows the tagging performance of each equation." ></td>
	<td class="line x" title="149:289	Training of the statistical tagging model requires a parameter estimation process for two different parameters, that is, morpheme lexical probabilities and bigram tag transition probabilities." ></td>
	<td class="line x" title="150:289	Several studies show that using as much tagged material as 7 Because a Korean eojeol can be segmented in many different ways, selecting the best morpheme segmentation sequence is as important as selecting the best POS sequence in POS tagging." ></td>
	<td class="line x" title="151:289	8 Provided by the Electronics and Telecommunications Research Institute (ETRI)." ></td>
	<td class="line x" title="152:289	61 Computational Linguistics Volume 28, Number 1 Table 5 Tagging performance (all in %) of each equation." ></td>
	<td class="line x" title="153:289	The eojeol row shows eojeol-unit tagging performance, and the morpheme row shows morpheme-unit performance." ></td>
	<td class="line x" title="154:289	Eq." ></td>
	<td class="line x" title="155:289	(2) Eq." ></td>
	<td class="line x" title="156:289	(3) Eq." ></td>
	<td class="line x" title="157:289	(4) Eq." ></td>
	<td class="line x" title="158:289	(5) Eq." ></td>
	<td class="line x" title="159:289	(6) Eq." ></td>
	<td class="line x" title="160:289	(7) (Eq." ></td>
	<td class="line x" title="161:289	(1)) Eojeol 86.80 90.48 89.40 89.62 91.73 92.48 Morpheme 91.32 94.93 94.40 94.48 95.77 96.12 possible for training gives much better performance than unsupervised training using the Baum-Welch reestimation algorithm (Merialdo 1994)." ></td>
	<td class="line x" title="162:289	We therefore decided to use supervised training using tagged corpora with relative frequency counts." ></td>
	<td class="line x" title="163:289	The three necessary probabilities can be estimated as in Equations (8)(10), Pr(t i jm i ) f (t i jm i )= N(m i, t i ) N(m i ) (8) Pr(t i ) f (t i )= N(t i ) P N ts n=1 N(t n ) (9) Pr(t i jt i1 ) f (t i jt i1 )= N(t i1, t i ) N(t i1 ) (10) where N(m i, t i ) indicates the total number of occurrences of the morpheme m i together with the specific tag t i, while N(m i ) indicates the total number of occurrences of the morpheme m i in the tagged training corpus." ></td>
	<td class="line x" title="164:289	N ts indicates the total number of POS tags in the tagset." ></td>
	<td class="line x" title="165:289	N(t i1, t i ) and N(t i1 ) can be interpreted similarly for two consecutive tags t i1 and t i. A beam search strategy is utilized for high-speed tagging." ></td>
	<td class="line x" title="166:289	For each morpheme in the sentence, the highest probability, P h, of the tag is recorded." ></td>
	<td class="line x" title="167:289	All other tags associated with the same morpheme must have probabilities greater than P h  for some constant beam size ; otherwise, they are discarded." ></td>
	<td class="line x" title="168:289	The beam may introduce search errors, but, in practice, search efficiency can be greatly improved with virtually no loss of accuracy." ></td>
	<td class="line x" title="169:289	4.2 Lexical Probability Estimation for Unknown-Morpheme Guessing The lexical probabilities for unknown morphemes cannot be precalculated using Equation (8) since we assume the unknown morphemes do not appear in the training corpus, so a special on-the-fly estimation method must be applied." ></td>
	<td class="line x" title="170:289	We suggest using syllable trigrams since Korean syllables can play an important role in restricting units for guessing the POS of a morpheme." ></td>
	<td class="line x" title="171:289	The lexical probability Pr(t i jm i ) Pr(t i ) for unknown morphemes can be estimated using the frequency of syllable trigram products according to the formula in (11)(13) (Nagata 1994), m = e 1 e 2 :::e n (11) Pr(tjm) Pr(t) Pr t (e 1 j#, #)Pr t (e 2 j#, e 1 ) n Y i=3 Pr t (e i je i2, e i1 ) 62 Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation Pr(# je n1, e n ) (12) Pr t (e i je i2, e i1 ) f t (e i je i2, e i1 ) + f t (e i je i1 ) + f t (e i ) (13) where m is a morpheme, e is a syllable, t is a POS tag, # is a morpheme boundary symbol, and f t (e i je i2, e i1 ) is a frequency datum for tag t with co-occurrence syllables e i2, e i1, and e i . Trigram probabilities are smoothed by Equation (13) to cope with the data sparseness problem." ></td>
	<td class="line x" title="172:289	For example, Park-jong-man is the name of a person, so it is an unknown morpheme." ></td>
	<td class="line x" title="173:289	The lexical probability that Park-jong-man should be assigned the tag MPN (person name) is estimated using the following formula: Pr(MPN jPark jongman) Pr(MPN ) Pr MPN (Park j#, #) Pr MPN (jong j#, Park) Pr MPN (man jPark, jong) Pr MPN (# jjong, man) (14) In Park-jong-man, Park is usually a family name." ></td>
	<td class="line x" title="174:289	If the first position of this morpheme is a family name, the probability that MPN is the correct tag becomes higher than the probability that the other tags are correct." ></td>
	<td class="line x" title="175:289	Table 6 shows the distribution of Pr(Park j#, #) for each possible tag." ></td>
	<td class="line x" title="176:289	In Equation (14), Pr MPN (Park j#, #) represents the popularity of the tag MPN for the morpheme Park-jong-man. All the trigrams for Korean syllables were precalculated and stored in the database and are applied with the candidate tags during the unknown-morpheme POS guessing and smoothing portion of the statistical tagging process." ></td>
	<td class="line x" title="177:289	4.3 A Posteriori Error Correction Rules Statistical morpheme tagging is widely known to cover only a limited range of contextual information." ></td>
	<td class="line x" title="178:289	Moreover, it cannot refer to lexical patterns as a context for POS disambiguation." ></td>
	<td class="line x" title="179:289	As mentioned earlier, because Korean eojeols have very complex morphological structures, it is necessary to look at the functional morphemes selectively to determine the grammatical relations between eojeols." ></td>
	<td class="line x" title="180:289	For these reasons, we designed error correction rules for eojeols to compensate for the estimation and modeling errors Table 6 The distribution of Pr(Parkj#, #) for each tag." ></td>
	<td class="line x" title="181:289	MC MPN MPC MPP MPO T No." ></td>
	<td class="line x" title="182:289	of ##Park 125 2081 0 0 8 0 No." ></td>
	<td class="line x" title="183:289	of ## 115,841 25,915 589 1,209 50,671 4,255 Pr(Parkj#, #) 0.001 0.080 0.000 0.000 0.000 0.000 BDRDIHRHI No." ></td>
	<td class="line x" title="184:289	of ##Park 5 17 2 0 9 No." ></td>
	<td class="line x" title="185:289	of ## 9,169 21,119 13,555 2,243 5,217 Pr(Parkj#, #) 0.000 0.000 0.000 0.000 0.001 63 Computational Linguistics Volume 28, Number 1 Table 7 Examples of rule schemata used to extract the error correction rules automatically from the tagged corpus." ></td>
	<td class="line x" title="186:289	The POSTAG system has about 24 rule schemata of this form." ></td>
	<td class="line x" title="187:289	Rule schema Acronym description N1FT the tag of the first morpheme (FT) of the next eojeol (N1) P1LT the tag of the last morpheme (LT) of the previous eojeol (P1) N2FT the tag of the first morpheme (FT) of the eojeol after the next one (N2) N3FT the tag of the first morpheme (FT) of the second eojeol after the next one (N3) P1LM the lexical form of the last morpheme (LM) of the previous eojeol (P1) P1FM the lexical form of the first morpheme (FM) of the previous eojeol (P1) N1FM the lexical form of the first morpheme (FM) of the next eojeol (N1) [current eojeol or morpheme] [rule schemata, referenced morpheme or tag] ! [corrected eojeol or morpheme] Figure 2 Error correction rule format." ></td>
	<td class="line x" title="188:289	of the statistical morpheme tagger." ></td>
	<td class="line x" title="189:289	However, designing the error correction rules with knowledge engineering is tedious and error prone." ></td>
	<td class="line x" title="190:289	Instead, we adopted Brills approach (Brill 1995) whereby the error correction rules are learned automatically from a small amount of tagged corpus." ></td>
	<td class="line x" title="191:289	Fortunately, Brill showed that one does not normally need a large amount of tagged corpus to extract the symbolic tagging rules compared with statistical tagging." ></td>
	<td class="line x" title="192:289	Table 7 shows examples of carefully designed rule schemata used to extract the error correction rules for Korean, where a rule schema designates the context of rule applications (i.e. , the morpheme position and the lexical/tag decision in a context eojeol)." ></td>
	<td class="line x" title="193:289	The form of the rules that can be automatically learned using the schemata in Table 7 is shown in Figure 2, where [current eojeol or morpheme] consists of the morpheme (with current tag) sequence in an eojeol, and [corrected eojeol or morpheme] consists of the morpheme (with corrected tag) sequence in the same eojeol." ></td>
	<td class="line x" title="194:289	For example, the rule [meog (Chinese ink 0 )=MC + eun=jS][N1FT, MC ]![meog (to eat 0 )=DR + eun=eCNMG] says that the current eojeol was statistically tagged as a common noun (MC) plus auxiliary particle (jS), but if the next eojeols (N1) first-position morpheme tag (FT) is also MC, the eojeol should be tagged as a regular verb (DR) plus adnominal ending (eCNMG)." ></td>
	<td class="line x" title="195:289	This statistical error is caused by the ambiguity of the morpheme meog, which has two meanings: Chinese ink (noun) and to eat (verb)." ></td>
	<td class="line x" title="196:289	Since morpheme segmentation is very difficult in Korean, many tagging errors also arise from the morpheme segmentation errors." ></td>
	<td class="line x" title="197:289	Our error correction rules can also cope with these morpheme segmentation errors by correcting the errors in the whole eojeol at once." ></td>
	<td class="line x" title="198:289	For example, the following rule can correct morpheme segmentation errors: [jul=MC + igo=jO][P1LM, ] ! [jul i=DR + go=eCC ]." ></td>
	<td class="line x" title="199:289	This rule says that the eojeol jul-i-go is usually segmented as a common noun, jul string, rope, plus the adverb and conjunctive particle i-go, but when the morpheme eul appears before the eojeol, it should be segmented as a regular verb, jul-i shrink, plus the conjunctive ending go." ></td>
	<td class="line x" title="200:289	This kind of segmentation error correction can greatly enhance the tagging performance." ></td>
	<td class="line x" title="201:289	The rules are automatically learned by comparing the correctly tagged corpus with the output of the statistical tagger." ></td>
	<td class="line x" title="202:289	The training is leveraged so the error correc64 Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation Table 8 Performance of the statistical tagger (all in %) on three document sets, using three progressively degraded versions of the tagger." ></td>
	<td class="line x" title="203:289	Document set Version 1 Version 2 Version 3 Set 1 96.4 89.5 87.1 Set 2 96.0 92.8 89.0 Set 3 96.7 88.7 84.8 Total 96.4 90.3 87.0 tion rules are gradually learned as the statistically tagged texts are corrected by the rules learned so far." ></td>
	<td class="line x" title="204:289	5." ></td>
	<td class="line x" title="205:289	Experimental Results 5.1 Embedded Performance with Hybrid POS Tagging For morphological analysis and POS tagging experiments, we used a 130,000-morpheme balanced training corpus for statistical parameter estimation and a 50,000-morpheme corpus for learning the a posteriori error correction rules." ></td>
	<td class="line x" title="206:289	The training corpus was collected from various sources such as Internet documents, encyclopedias, newspapers, and school textbooks." ></td>
	<td class="line x" title="207:289	For test sets, we carefully selected three different document sets, aiming for broad coverage." ></td>
	<td class="line x" title="208:289	The first document set (Set 1: 25,299 morphemes, 1,338 sentences), which was collected from the Kemong Encyclopedia, 9 a hotel reservation dialogue corpus, 10 and assorted Internet documents, contains about 10% unknown morphemes." ></td>
	<td class="line x" title="209:289	The second document set (Set 2: 15,250 morphemes, 574 sentences), which consists solely of Internet documents from assorted domains, such as broadcasting scripts and newspapers, contains about 8.5% unknown morphemes." ></td>
	<td class="line x" title="210:289	The third document set (Set 3: 20,919 morphemes, 555 sentences), which comes from a standard Korean document set called KTSET 2.0 11 including academic articles and electronic newspapers, contains about 14% unknown morphemes (mainly technical jargon)." ></td>
	<td class="line x" title="211:289	Table 8 shows our systems statistical tagging performance for these three document sets, using three progressively degraded versions of the tagging mechanism." ></td>
	<td class="line x" title="212:289	Version 1 is a full version using the statistical method." ></td>
	<td class="line x" title="213:289	Version 2 is a somewhat degraded version that does not use the systems unknown-morpheme guessing capability but treats all the segmented unknown morphemes as nouns (the typical method of estimation)." ></td>
	<td class="line x" title="214:289	Version 3 is an even more degraded version that rejects all unknown morphemes as tagging failures; this version does not even perform unknown-morpheme segmentation during morphological analysis." ></td>
	<td class="line x" title="215:289	This experiment verifies the effectiveness of our unknown-morpheme segmentation and guessing techniques, as shown by the sharp performance drops between Versions 1, 2, and 3." ></td>
	<td class="line x" title="216:289	As another experiment showed, the automatically acquired a posteriori error correction rules also proved to be useful." ></td>
	<td class="line x" title="217:289	In this experiment, two versions of the hybrid tagger were tested on the three document sets." ></td>
	<td class="line x" title="218:289	Version 1 was the full POSTAG system with unknown-morpheme segmentation, guessing, and 9 From the Electronics and Telecommunications Research Institute (ETRI)." ></td>
	<td class="line x" title="219:289	10 From Sogang University, Seoul, Korea." ></td>
	<td class="line x" title="220:289	11 From KT (Korea Telecom)." ></td>
	<td class="line x" title="221:289	65 Computational Linguistics Volume 28, Number 1 Table 9 Performance of the hybrid tagger (all in %) on three document sets, using two versions of the tagger." ></td>
	<td class="line x" title="222:289	Document set Version 1 Version 2 Set 1 97.2 96.4 Set 2 96.9 96.0 Set 3 97.4 96.7 Total 97.2 96.4 Table 10 Unknown-morpheme estimation performance (all in %)." ></td>
	<td class="line x" title="223:289	Experiments were performed on three different document sets as before." ></td>
	<td class="line x" title="224:289	#UKM designates the number of unknown morphemes in each document set and their percentage." ></td>
	<td class="line x" title="225:289	Recall (Rec)." ></td>
	<td class="line x" title="226:289	measures the coverage of the estimation and precision (Pre)." ></td>
	<td class="line x" title="227:289	demonstrates its accuracy." ></td>
	<td class="line x" title="228:289	Document set #UKM Rec." ></td>
	<td class="line x" title="229:289	Pre." ></td>
	<td class="line x" title="230:289	Set 1 2,531 (10.0%) 93.9 94.8 Set 2 1,303 (8.5%) 92.9 88.9 Set 3 2,889 (13.8%) 98.0 85.5 Total 6,723 (10.8%) 94.9 89.7 rule-based error correction." ></td>
	<td class="line x" title="231:289	Version 2 did not employ a posteriori error correction rules (the same system as Version 1 in the first experiment)." ></td>
	<td class="line x" title="232:289	Performance dropped between Version 1 and Version 2 (see Table 9); however, the drop rates were mild due to the performance saturation at Version 1, which means that our statistical tagger alone already achieves state-of-the-art performance for tagging of Korean morphemes." ></td>
	<td class="line x" title="233:289	5.2 Unknown-Morpheme Segmentation and Guessing Performance To see the independent performance of unknown-morpheme handling more precisely (explained in Sections 3 and 4.2), we separated the unknown-morpheme performance from hybrid tagging experiments." ></td>
	<td class="line x" title="234:289	Using the same test corpus, we measured the coverage and correctness of our unknown-morpheme estimation techniques." ></td>
	<td class="line x" title="235:289	Table 10 shows the results, which were evaluated by the metrics defined as follows: Recall = #unknown morphemes detected #unknown morphemes (segmentation performance) Precision = #unknown morphemes correctly estimated #unknown morphemes detected (guessing performance) When the morphological analyzer meets an unknown morpheme, it is important to detect first whether it is unknown or not, because sometimes, due to incorrect segmentation, an unknown morpheme can be incorrectly processed as a known one." ></td>
	<td class="line x" title="236:289	Our system reached an average recall level of 94.9%." ></td>
	<td class="line x" title="237:289	Once the unknown morphemes are detected, the correct POS needs to be estimated." ></td>
	<td class="line x" title="238:289	Our system tries to guess the POS 66 Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation Table 11 Unknown-morpheme estimation performance (all in %) for each POS tag." ></td>
	<td class="line x" title="239:289	N/A means the morpheme with the corresponding tag does not appear in the corpus." ></td>
	<td class="line x" title="240:289	Recall (Rec)." ></td>
	<td class="line x" title="241:289	measures the coverage of the estimation, and precision (Pre)." ></td>
	<td class="line x" title="242:289	demonstrates its accuracy." ></td>
	<td class="line x" title="243:289	Set 1 Set 2 Set 3 Total POS tag Rec." ></td>
	<td class="line x" title="244:289	Pre." ></td>
	<td class="line x" title="245:289	Rec." ></td>
	<td class="line x" title="246:289	Pre." ></td>
	<td class="line x" title="247:289	Rec." ></td>
	<td class="line x" title="248:289	Pre." ></td>
	<td class="line x" title="249:289	Rec." ></td>
	<td class="line x" title="250:289	Pre." ></td>
	<td class="line x" title="251:289	MC 96.9 95.4 94.5 91.7 93.9 72.5 95.1 86.5 MPN 80.0 86.7 87.4 95.0 100.0 100.0 89.1 93.9 MPC 54.3 73.7 72.7 37.5 N/A N/A 42.3 37.1 MPP 75.2 63.3 86.9 75.0 100.0 100.0 87.4 79.4 MPO 79.4 79.4 94.8 68.3 100.0 93.8 91.4 79.7 B 87.9 100.0 42.9 66.7 100.0 100.0 76.9 88.9 T N/A N/A 100.0 100.0 N/A N/A 100.0 100.0 S 99.8 100.0 99.0 100.0 100.0 100.0 99.6 100.0 Foreign word 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 Special symbol 100.0 100.0 N/A N/A 100.0 100.0 100.0 100.0 of every open class item including common nouns, proper nouns, pronouns, numbers, adverbs, and others." ></td>
	<td class="line x" title="252:289	12 The average precision of 89.7% reflects very accurate guessing considering the range of POSs that need to be estimated." ></td>
	<td class="line x" title="253:289	Table 11 shows the systems unknown-morpheme guessing performance for each POS tag." ></td>
	<td class="line x" title="254:289	To show the pattern dictionarys utility, we conducted another experiment in which we gradually reduced the morpheme dictionary size to see the smooth hybrid tagging performance (same as in Table 9) drops." ></td>
	<td class="line x" title="255:289	As the morpheme dictionary gets smaller, POSTAG becomes more dependent on the morpheme pattern dictionary and also on the unknown-morpheme estimation process." ></td>
	<td class="line x" title="256:289	From the full dictionary (with 65,000 nouns), we randomly deleted 5,000 nouns step by step for this series of experiments." ></td>
	<td class="line x" title="257:289	(We deleted only nouns because noun estimation is the best arena for showing the systems unknown-morpheme estimation power)." ></td>
	<td class="line x" title="258:289	Figure 3 shows the results." ></td>
	<td class="line x" title="259:289	Even if the POSTAG system relies heavily on unknown-morpheme estimation instead of on more accurate dictionary lookups, the performance drop is very slow." ></td>
	<td class="line x" title="260:289	This result explains why POSTAG can be used on open domain materials such as Internet documents even when only a small morpheme dictionary is available." ></td>
	<td class="line x" title="261:289	6." ></td>
	<td class="line x" title="262:289	Conclusion This paper presents a pattern-dictionary-based unknown-morpheme estimation method for generalized and powerful unknown-morpheme segmentation and guessing for a hybrid POS tagging system." ></td>
	<td class="line x" title="263:289	Generalized unknown-morpheme handling is a new and powerful idea that adopts a morpheme pattern dictionary and syllable-based lexical probability estimation." ></td>
	<td class="line x" title="264:289	The morpheme pattern dictionary enables the system to segment unknown morphemes in the same way as registered morphemes without any separate rules for Korean, and thereby to handle them regardless of their numbers or positions in an eojeol." ></td>
	<td class="line x" title="265:289	The paper also presents an error-corrective statistical and 12 Pronouns, numbers, and adverbs may be considered as closed classes." ></td>
	<td class="line x" title="266:289	However, in real-world corpora, we frequently find unexpectedly coined terms in these classes since Korean word formation is affected by very diverse sources such as foreign words, old Chinese words, archaic pure-Korean words, and so on." ></td>
	<td class="line x" title="267:289	67 Computational Linguistics Volume 28, Number 1 90 92 94 96 98 100 0 2 4 6 8 10 12 set1 set2 set3 total Figure 3 Hybrid tagging performance change (all in %), showing the utility of the pattern dictionary." ></td>
	<td class="line x" title="268:289	Experiments were performed on three different document sets as before." ></td>
	<td class="line x" title="269:289	The x-axis designates the number of deletion steps whereby the morpheme dictionary was decreased (by 5,000s) from its full size of 65,000 nouns (Step 0) to 5,000 nouns (Step 12)." ></td>
	<td class="line x" title="270:289	rule-based hybrid POS tagging method that exhibits many novel features such as an experiment-based statistical model for Korean, rule-based error correction, and hierarchically expandable tagsets." ></td>
	<td class="line x" title="271:289	The POSTAG system was developed to test these novel ideas, especially for agglutinative languages such as Korean." ></td>
	<td class="line x" title="272:289	(Japanese, being similar to Korean in linguistic characteristics, will be a good target for testing these ideas.)" ></td>
	<td class="line x" title="273:289	Unlike previous systems, POSTAG is a hybrid tagging system; such a system has never been tried before, but it turns out to be most suitable for agglutinative languages such as Korean." ></td>
	<td class="line x" title="274:289	POSTAG mainly applies a state-of-the-art HMM tagger for morphemes but considers multiple observations in the Viterbi score calculation." ></td>
	<td class="line x" title="275:289	Because of the complexity of the morpheme sequence in a Korean eojeol, a morpheme-based HMMs tagging accuracy is relatively low for Korean, compared with its accuracy for English." ></td>
	<td class="line x" title="276:289	POSTAG compensates extremely well for the limitations of HMMs by rule-based error correction." ></td>
	<td class="line x" title="277:289	The error correction rules are automatically learned to selectively correct HMM tagging errors." ></td>
	<td class="line x" title="278:289	Similar hybrid methods have been tried for English, but they integrate HMM tagging and rule-based tagging at the same level (Tapanainen and Voutilainen 1994)." ></td>
	<td class="line x" title="279:289	POSTAG integrates morphological analysis with the generalized 68 Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation unknown-morpheme segmentation so that unknown morphemes can be processed in the same manner as registered morphemes during tagging." ></td>
	<td class="line x" title="280:289	POSTAG also employs hierarchical tagsets that are flexible enough to expand/shrink according to the given application." ></td>
	<td class="line x" title="281:289	The hierarchical tagset is a novel idea." ></td>
	<td class="line x" title="282:289	Most tagging systems for Korean have applied flat, fixed tagsets and have suffered from using varying tagsets in various applications." ></td>
	<td class="line x" title="283:289	However, POSTAGs tagsets, based on the over 100 finely differentiated POS symbols for Korean are hierarchically organized and are flexibly reorganizable according to the application at hand." ></td>
	<td class="line x" title="284:289	The hierarchical tagsets can be mapped to any other existing tagset as long as they are fairly well classified and therefore can encourage corpus sharing in the Korean-tagging community." ></td>
	<td class="line x" title="285:289	POSTAG is constantly being improved through expansion of its morpheme dictionary, pattern dictionary, and tagged corpus for statistical and rule-based learning." ></td>
	<td class="line x" title="286:289	Since the generalized unknown-morpheme handling is integrated into the system, POSTAG proves to be a good tagger for open domain applications such as Internet indexing, filtering, and summarization, and we are now developing a Web indexer using POSTAG technology." ></td>
	<td class="line x" title="287:289	Acknowledgments This project was partly supported by KOSEF (teukjeongkicho #970-1020-301-3, 1997.9-2000.8) and a Ministry of Education BK21 program awarded to the Electrical and Computer Engineering Division of POSTECH." ></td>
	<td class="line x" title="288:289	We would like to thank JunHyeok Shim for coding the unknown-morpheme estimation experiments." ></td>
	<td class="line x" title="289:289	An earlier version of this paper was presented at the 6th Workshop on Very Large Corpora in Montreal, 1516 August 1998." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W03-1314
Exploring Adjectival Modification In Biomedical Discourse Across Two Genres
Bodenreider, Olivier;Pakhomov, Serguei V.;"></td>
	<td class="line x" title="1:184	Exploring adjectival modification in biomedical discourse across two genres Olivier Bodenreider Serguei V. Pakhomov Lister Hill National Center for Biomedical Communications National Library of Medicine Bethesda, Maryland, 20894  USA Division of Medical Informatics Research Department of Health Sciences Research Mayo Clinic Rochester, Minnesota, 55905  USA olivier@nlm.nih.gov Pakhomov.Serguei@mayo.edu Abstract Objectives: To explore the phenomenon of adjectival modification in biomedical discourse across two genres: the biomedical literature and patient records." ></td>
	<td class="line x" title="2:184	Methods: Adjectival modifiers are removed from phrases extracted from two corpora (three million noun phrases extracted from MEDLINE, on the one hand, and clinical notes from the Mayo Clinic, on the other)." ></td>
	<td class="line x" title="3:184	The original phrases, the adjectives extracted, and the resulting demodified phrases are compared across the two corpora after normalization." ></td>
	<td class="line x" title="4:184	Quantitative comparisons (frequency of occurrence) are performed on the whole domain." ></td>
	<td class="line x" title="5:184	Qualitative comparisons are performed on the two subdomains (disorders and procedures)." ></td>
	<td class="line x" title="6:184	Results: Although the average number of adjectives per phrase is equivalent in the two corpora (1.4), there are more adjective types in MAYO than in MEDLINE for disorders and procedures." ></td>
	<td class="line x" title="7:184	For disorder phrases, the 38% of adjective types common to the two corpora account for 85% of the occurrences." ></td>
	<td class="line x" title="8:184	The predominance of adjectives in one corpus is analyzed." ></td>
	<td class="line x" title="9:184	Discussion: Potential applications of this approach are discussed, namely terminology acquisition, information retrieval, and genre characterization." ></td>
	<td class="line x" title="10:184	1 Introduction In previous studies, we demonstrated the feasibility of using NLP techniques such as shallow parsing of adjectival modification for identifying hierarchical relations among biomedical terms (Bodenreider et al. , 2001) and for extending an existing biomedical terminology (Bodenreider et al. , 2002)." ></td>
	<td class="line x" title="11:184	In these studies, the corpus was biomedical terminology or phrases extracted from the biomedical literature." ></td>
	<td class="line x" title="12:184	Other authors have explored adjectival modification in a clinical corpus." ></td>
	<td class="line x" title="13:184	Chute and Elkin (1997) note, based on empirical observation of clinical data, that many clinical terms are accompanied by modifiers, including adjectives." ></td>
	<td class="line x" title="14:184	The authors make a distinction between clinical modifiers (such as chronic, severe, and acute) and operational or administrative qualifiers (such as no evidence of, history of, and status post)." ></td>
	<td class="line x" title="15:184	It appears that the class of clinical modifiers consists primarily of adjectives that provide specific information regarding condition and are distributed on a scale." ></td>
	<td class="line x" title="16:184	They suggest that operational modifiers be kept separate from the terms themselves in order to avoid combinatorial explosion." ></td>
	<td class="line x" title="17:184	Taking this idea one step further, we believe that, besides operational modifiers, other adjectives encountered in clinical phrases could receive a special treatment in applications such as information retrieval." ></td>
	<td class="line x" title="18:184	For example, adjectives expressing nuances useful only in the context of clinical care could be removed from the phrase when searching the biomedical literature." ></td>
	<td class="line x" title="19:184	This is the case of adjectives expressing degree of certainty (e.g. , probable)." ></td>
	<td class="line x" title="20:184	In other cases, adjectives specific to clinical phrases can be mapped to synonyms or closely related modifiers (e.g. , greenish sputum, green sputum)." ></td>
	<td class="line x" title="21:184	The ability to map stylistic variations of the same adjective becomes especially important to establishing links between clinical records and scientific literature, which actually has significant implications for improving patient care in clinical practice as well as health science research." ></td>
	<td class="line x" title="22:184	Finally, adjectives absent from the biomedical literature or terminologies may denote recent phenomena, not yet integrated in terminologies." ></td>
	<td class="line x" title="23:184	Knowledge about these classes of adjectives may help map across genres." ></td>
	<td class="line x" title="24:184	Conversely, studying adjectival modification across genres may help identify adjectives whose representation varies across genres, possibly denoting one of these phenomena." ></td>
	<td class="line x" title="25:184	In the present paper, we explore the phenomenon of adjectival modification across two genres: the biomedical literature and patient records." ></td>
	<td class="line x" title="26:184	The expected outcome of this study is to obtain a better characterization of adjectival modification in biomedical phrases of various origins, in order to fully take advantage of this phenomenon in applications such as the automatic construction of terminology and ontology resources and the retrieval of clinical documents." ></td>
	<td class="line x" title="27:184	2 Background Adjectival modification as well as lexical semantics of adjectives has been studied extensively in the linguistic and NLP literature." ></td>
	<td class="line x" title="28:184	Most approaches have been directed at creating adjective taxonomies and other ways of classifying and representing adjectives according to their properties and function." ></td>
	<td class="line x" title="29:184	Raskin and Niernburg (1995) provide a comprehensive overview of the various approaches that have been taken to description, classification and representation of adjectives." ></td>
	<td class="line x" title="30:184	From the NLP standpoint, Fellbaum (1993) partitions adjectives in WordNet 1 into two large classes: descriptive and relational." ></td>
	<td class="line x" title="31:184	Descriptive adjectives ascribe a value of an attribute to a noun (p.27) (i.e. , big child) while relational adjectives are usually derived from and are somehow associated with a noun (i.e. , musical child)." ></td>
	<td class="line x" title="32:184	Another prominent distinction has to do with whether an adjective can express continuous (scalar) or discrete (non-scalar) values." ></td>
	<td class="line x" title="33:184	Raskin and Niernburg (1996) point out that for text meaning representa1 www.cogsci.princeton.edu/~wn/ tion for computational semantics, the most important distinction to make is between scalar and nonscalar." ></td>
	<td class="line x" title="34:184	They also present a method for incorporating the semantics of the modifier adjective into the semantics of the modified noun by representing nouns as frames with elements such as ATTRIBUTE_SIZE than can be filled in by the semantic content of the modifying adjectives." ></td>
	<td class="line x" title="35:184	The major contribution of this study is to explore adjectival modification across two genres in the biomedical domain." ></td>
	<td class="line x" title="36:184	Our approach is essentially practical and oriented towards applied perspectives." ></td>
	<td class="line x" title="37:184	3 Resources The two genres compared in this study are the biomedical literature and patient records." ></td>
	<td class="line x" title="38:184	More precisely, we use MEDLINE as our bibliographic corpus and clinical notes recorded at the Mayo Clinic as our clinical corpus." ></td>
	<td class="line x" title="39:184	MEDLINE 2, the U.S. National Library of Medicines (NLM) premier bibliographic database, contains over twelve million references to articles from more than 4,600 worldwide journals in life sciences with a concentration on biomedicine." ></td>
	<td class="line x" title="40:184	Srinivasan et al.(2002) performed a shallow syntactic analysis on the entire MEDLINE collection, using only titles and abstracts in English." ></td>
	<td class="line x" title="42:184	From the 175 million noun phrase types identified in their study, we selected the subset of simple phrases, i.e., noun phrases excluding prepositional modification or any other complex feature." ></td>
	<td class="line x" title="43:184	In this study, a randomly selected subset of three million of these simple noun phrases constitutes our bibliographic corpus." ></td>
	<td class="line x" title="44:184	The Mayo Clinic is a group medical practice in the United States and spans all recognized medical care settings and specialties." ></td>
	<td class="line x" title="45:184	Currently over 50,000 patient visits occur each week that generate 40,000 medical documentation entries in Mayo electronic record that principally consists of text narratives." ></td>
	<td class="line x" title="46:184	The current size of the collection is approaching fifteen million notes and each note has on average 200 to 250 words of text." ></td>
	<td class="line x" title="47:184	For this study we considered only the most current sample of the clinical notes collection  1,783,377 documents recorded in 2002." ></td>
	<td class="line x" title="48:184	Only simple noun phrases of the same type extracted from MEDLINE were extracted 2 www.ncbi.nlm.nih.gov/entrez/query.fcgi from this corpus, resulting in a set of 9,665,942 phrases." ></td>
	<td class="line x" title="49:184	A randomly selected subset of three million of these simple noun phrases constitutes our clinical corpus." ></td>
	<td class="line x" title="50:184	In both cases, the noun phrases were first normalized for case, so that the two subsets studied represent three million noun phrase types each." ></td>
	<td class="line x" title="51:184	Another resource used in this study is the Unified Medical Language System 3 (UMLS) Metathesaurus." ></td>
	<td class="line x" title="52:184	The Metathesaurus, also developed by NLM, is organized by concept or meaning." ></td>
	<td class="line x" title="53:184	A concept is defined as a cluster of terms representing the same meaning (synonyms, lexical variants, acronyms, translations)." ></td>
	<td class="line x" title="54:184	The 14th edition (2003AA) of the UMLS Metathesaurus contains over 1.75 million unique English terms drawn from more than sixty families of medical vocabularies, and organized in some 875,000 concepts." ></td>
	<td class="line x" title="55:184	In the UMLS, each concept is categorized by semantic types from the Semantic Network." ></td>
	<td class="line x" title="56:184	McCray et al.(2001) designed groupings of semantic types that provide a partition the Metathesaurus and, therefore, can be used to extract consistent sets of concepts corresponding to a subdomain, such as disorders or procedures." ></td>
	<td class="line x" title="58:184	4 Methods In order to compare the linguistic phenomenon of adjectival modification across two corpora of noun phrases, we first extracted the adjectives after submitting the phrases to a shallow syntactic analysis and normalizing the head noun of the phrase for inflectional variation." ></td>
	<td class="line x" title="59:184	Then, we compared across corpora the adjectives on the one hand and the demodified noun phrases4 (i.e. , noun phrases from which the adjectives have been removed) on the other." ></td>
	<td class="line x" title="60:184	In order to address the size of these corpora, we limited the focus of our study to a significant subdomain of clinical medicine: disorders and procedures." ></td>
	<td class="line x" title="61:184	4.1 Extracting adjectives Figure 1 illustrates the sequence of methods used for extracting adjectives from the original noun phrases." ></td>
	<td class="line x" title="62:184	It also presents the number of phrases present before and after each of the four steps detailed below." ></td>
	<td class="line x" title="63:184	3 umlsinfo.nlm.nih.gov 4 also referred to as nested terms in the literature Step 1." ></td>
	<td class="line oc" title="64:184	Syntactic analysis The phrases in our bibliographic and clinical samples were then submitted to an underspecified syntactic analysis described by Rindflesch et al.(2000) that draws on a stochastic tagger (see (Cutting et al. , 1992) for details) as well as the SPECIALIST Lexicon5, a large syntactic lexicon of both general and medical English that is distributed with the UMLS." ></td>
	<td class="line n" title="66:184	Although not perfect, this combination of resources effectively addresses the phenomenon of part-of-speech ambiguity in English." ></td>
	<td class="line x" title="67:184	The resulting syntactic structure identifies the head and modifiers for the noun phrase analyzed." ></td>
	<td class="line x" title="68:184	Each modifier is also labeled as being adjectival, adverbial, or nominal." ></td>
	<td class="line x" title="69:184	Although all types of modification in the simple English noun phrase were labeled, only adjectives and nouns were selected for further analysis in this study." ></td>
	<td class="line x" title="70:184	For example, the phrase abnormal esophageal motility study was analyzed as: [[mod([abnormal,adj]), mod([esophageal,adj]), mod([motility,noun]), head([study,noun])]] The result of the syntactic analysis was used to select the noun phrases suitable for studying the adjectival modification phenomenon, i.e., phrases having the following structure: (adj+, noun*, head)." ></td>
	<td class="line x" title="71:184	The phrase is required to start with an adjectival modifier, possibly followed by other adjectives and end with a head noun, possibly preceded by other nouns." ></td>
	<td class="line x" title="72:184	This specification excludes both simple phrases (e.g. , one isolated noun) and complex phrases, not suitable for our analysis." ></td>
	<td class="line x" title="73:184	Step 2." ></td>
	<td class="line x" title="74:184	Normalizing the head noun In order to compare phrases across corpora, we normalized the head noun for inflectional variation in each noun phrase." ></td>
	<td class="line x" title="75:184	As a result, the two noun phrases cerebrovascular accident (in MAYO) and cerebrovascular accidents (in MEDLINE) are considered equivalent." ></td>
	<td class="line x" title="76:184	When both the singular and the plural form of a phrase appear in the same corpus, only the singular form is considered for further processing." ></td>
	<td class="line x" title="77:184	In practice, to normalize head nouns, we used the program lvg6, developed at NLM and distributed with the UMLS." ></td>
	<td class="line x" title="78:184	5 umlslex.nlm.nih.gov 6 umlslex.nlm.nih.gov (lvg parameters used: -f:b -CR:oc)" ></td>
	<td class="line x" title="79:184	   1,329,225 (adj+, noun*, head) phrases 3,000,000 randomly selected simple phrases syntactic analysis 1,322,403 normalized phrases normalize head noun remove adjectives select subdomain" ></td>
	<td class="line x" title="80:184	 2,826,395 demodified phrases 72,324 adjective types Disorders 18,370 adjectives 279,182 dem." ></td>
	<td class="line x" title="81:184	terms Procedures 16,098 adjectives 160,207 dem." ></td>
	<td class="line x" title="82:184	terms 1,641,350 (adj+, noun*, head) phrases 3,000,000 randomly selected simple phrases syntactic analysis 1,575,478 normalized phrases normalize head noun remove adjectives 3,092,340 demodified phrases 44,268 adjective types" ></td>
	<td class="line x" title="83:184	 select subdomain Disorders 16,486 adjectives 714,257 dem." ></td>
	<td class="line x" title="84:184	terms Procedures 11,630 adjectives 242,326 dem." ></td>
	<td class="line x" title="85:184	terms Figure 1." ></td>
	<td class="line x" title="86:184	Summary of the methods." ></td>
	<td class="line x" title="87:184	Step 3." ></td>
	<td class="line x" title="88:184	Creating demodified phrases When adjectives are identified in a phrase O, a set of demodified phrases {T1, T2,,Tn} is created by removing from phrase O any combinations of adjectival modifiers found in it." ></td>
	<td class="line x" title="89:184	While the structure of the demodified phrases remains syntactically correct, the semantics of some phrases may be anomalous, especially when adjectives other than the leftmost are removed." ></td>
	<td class="line x" title="90:184	Since most of them are semantically valid, we found it convenient to keep all demodified phrases for further analysis." ></td>
	<td class="line x" title="91:184	Demodified phrases with incorrect semantics will be filtered out later in the experiment, since they will appear with a lower frequency." ></td>
	<td class="line x" title="92:184	The number of demodified phrases derived from a given phrase is 2m  1, m being the number of adjectives in the phrase." ></td>
	<td class="line x" title="93:184	For example, the phrase acute respiratory infection syndrome starts with the two adjectival modifiers acute and respiratory, so that the following three demodified phrases are generated respiratory infection syndrome, acute infection syndrome, and infection syndrome." ></td>
	<td class="line x" title="94:184	Step 4." ></td>
	<td class="line x" title="95:184	Restricting to disorders and procedures Because of the large size of the two corpora, we only performed a quantitative analysis of adjectival modification for the whole biomedical domain." ></td>
	<td class="line x" title="96:184	We restricted the qualitative study to disorders and procedures." ></td>
	<td class="line x" title="97:184	These represent a significant subdomain of clinical medicine, yet are small enough to be able to perform at least a somewhat detailed analysis." ></td>
	<td class="line x" title="98:184	All phrases, original and demodified, were mapped to the UMLS Metathesaurus by first attempting an exact match between phrases and Metathesaurus concepts." ></td>
	<td class="line x" title="99:184	If an exact match failed, normalization was then attempted." ></td>
	<td class="line x" title="100:184	This process makes the input and target terms potentially compatible by eliminating such inessential differences as inflection, case and hyphen variation, as well as word order variation." ></td>
	<td class="line x" title="101:184	From the phrases mapping to some concept in the UMLS, we selected those for which the semantic category of the concept mapped to corresponded to the subdomains of interest." ></td>
	<td class="line x" title="102:184	In practice, for a phrase to be considered a procedure, it had to map to a UMLS concept and the semantic type of this concept had to belong to the semantic group Procedures." ></td>
	<td class="line x" title="103:184	The same principle was used for selecting disorders, using the semantic group Disorders." ></td>
	<td class="line x" title="104:184	For example, the demodified phrase arthroscopic surgery (derived from decompressive arthroscopic surgery) is considered a procedure because it maps, as a synonym, to the concept Surgical Procedures, Arthroscopic, whose semantic group is Procedures." ></td>
	<td class="line x" title="105:184	Exceptionally (32 UMLS concepts), a term may name both a disorder and a procedure." ></td>
	<td class="line x" title="106:184	These terms are simply counted twice, once with Disorders and once with Procedures." ></td>
	<td class="line x" title="107:184	4.2 Comparing corpora In order to investigate the characteristics of each corpus (noun phrases extracted from the biomedical literature and from patient records), we used two kinds of comparisons: quantitative and qualitative." ></td>
	<td class="line x" title="108:184	The quantitative part consists of comparing frequencies of adjectives and demodified phrases across corpora, for the whole corpus as well as on specific subsets (Disorders and Procedures)." ></td>
	<td class="line x" title="109:184	In the qualitative part, we examined only phrases form the subdomains of Disorders and Procedures." ></td>
	<td class="line x" title="110:184	Quantitative comparisons As mentioned earlier, the head noun of each phrase was normalized for inflectional variation (see Step 2 above)." ></td>
	<td class="line x" title="111:184	The purpose of normalizing the head noun is two-fold." ></td>
	<td class="line x" title="112:184	First, it contributes to identifying phrase variants within each corpus, resulting in accurate counts of phrase types after duplicates had been removed." ></td>
	<td class="line x" title="113:184	Second, it provides a simple means (string match) for identifying equivalent phrases across corpora." ></td>
	<td class="line x" title="114:184	We computed the number of original phrases, adjectives, and demodified phrases in each corpus, counting tokens and types in each category." ></td>
	<td class="line x" title="115:184	Additionally, we explored similarities between the two genres by computing the number of phrases and adjectives common to the two corpora (intersection)." ></td>
	<td class="line x" title="116:184	Finally, we computed the number of phrase and adjective types for the two corpora taken together (union) in order to better characterize the whole domain." ></td>
	<td class="line x" title="117:184	From these frequencies, we derived additional parameters such as the ratio of the number of adjectives to the number of original phrases." ></td>
	<td class="line x" title="118:184	Qualitative comparisons We first extracted adjectives from the original phrases corresponding to Disorders and Procedures and computed their frequency of occurrence." ></td>
	<td class="line x" title="119:184	Because phrases must map to a UMLS term in order to be identified as members of a subdomain, only the adjectives present in biomedical terms can be analyzed." ></td>
	<td class="line x" title="120:184	For this reason, their rank will be studied rather than their frequency7." ></td>
	<td class="line x" title="121:184	In order to better represent the whole spectrum of adjectives present in the two corpora, we then turned to the demodified phrases instead of the original phrases." ></td>
	<td class="line x" title="122:184	In this second part, the condition 7 rank n simply corresponds to the nth highest frequency for a phrase to be considered a member of a subdomain was that the demodified phrase (not the entire phrase) map to a UMLS term." ></td>
	<td class="line x" title="123:184	However, some adjectives may be overrepresented when several demodified phrases map to a UMLS term in the subdomains considered." ></td>
	<td class="line x" title="124:184	For example, the phrase abdominal vascular reconstructive surgery, once demodified, maps to both vascular surgery (with modifiers abdominal and reconstructive) and reconstructive surgery (with modifiers abdominal and vascular)." ></td>
	<td class="line x" title="125:184	In this case, the adjective abdominal was counted twice." ></td>
	<td class="line x" title="126:184	For each adjective, we determined the corpus in which it was predominantly used." ></td>
	<td class="line x" title="127:184	If more than half of the occurrences appear in one corpus, the adjective is considered predominant in this corpus." ></td>
	<td class="line x" title="128:184	When more than half of the occurrences appear in both corpora, the adjective is considered common to the two corpora." ></td>
	<td class="line x" title="129:184	5 Results 5.1 Extracting adjectives Out of the 3 million simple noun phrases randomly selected from MEDLINE, 1,322,403 phrase types were selected for further processing." ></td>
	<td class="line x" title="130:184	Out of these, 72,324 adjective types (1,916,530 tokens) were extracted and 2,826,395 demodified phrases were generated." ></td>
	<td class="line x" title="131:184	1,575,478 phrase types were selected from the 3 million noun phrases in the MAYO corpus." ></td>
	<td class="line x" title="132:184	Out of these, 44,268 adjective types (2,209,778 tokens) were extracted and 3,092,340 demodified phrases were generated." ></td>
	<td class="line x" title="133:184	Details about the number of phrases selected at each step of the processing are given in Figure 1." ></td>
	<td class="line x" title="134:184	5.2 Comparing corpora Quantitative results The number of original phrases (Table 1), adjectives (Table 2), and demodified phrases (Table 3) are presented below in tabular format." ></td>
	<td class="line x" title="135:184	Counts are broken down by corpus (MEDLINE and MAYO), on the one hand, and by subdomain (Disorders and Prodedures), on the other." ></td>
	<td class="line x" title="136:184	Tables also include results obtained on the whole corpus (All), i.e., without subsetting, and on the union of the two corpora (Together)." ></td>
	<td class="line x" title="137:184	Except for original phrases (Table 1), which, by design, are phrase types, Table 2 and Table 3 contain the numbers of types (upper left) and tokens (lower right)." ></td>
	<td class="line x" title="138:184	The number of adjectives per phrase ranges from 1 to 16 in MEDLINE and from 1 to 7 for MAYO when the whole corpus is considered." ></td>
	<td class="line x" title="139:184	The maximum number of adjectives per phrase is 6 or 7 for the various subsets." ></td>
	<td class="line x" title="140:184	Phrases containing so many adjectives may look syntactically and semantically suspicious." ></td>
	<td class="line x" title="141:184	While some of them denote extraction errors (often due to inappropriate part-of-speech tagging), most correspond to valid phrases and reflect the complexity of the biomedical domain (e.g. , diastolic systolic mean middle cerebral artery blood flow velocity and combined enteral parenteral synthetic hypercaloric nutrition)." ></td>
	<td class="line x" title="142:184	The distribution of the number of adjectives per phrase is plotted in Figure 2." ></td>
	<td class="line x" title="143:184	Although the number of phrases processed is slightly more important for MAYO (1,575,476) than for MEDLINE (1,322,403), and although the ratio of the number of adjective tokens extracted to the number of original phrases is roughly similar in the two corpora (1.45 for MEDLINE and 1.40 for MAYO), there are significantly more adjective types in MEDLINE (72,324) than in MAYO (44,268)." ></td>
	<td class="line x" title="144:184	A difference in the opposite direction is observed in the Disorders and Procedures subsets, where the number of adjective types is higher in MAYO than in MEDLINE, while the average number of adjectives per phrase is still slightly higher in MEDLINE (1.27 vs. 1.21 for Disorders and 1.21 vs. 1.14 for Procedures)." ></td>
	<td class="line x" title="145:184	This finding requires further investigation." ></td>
	<td class="line x" title="146:184	Despite reducing the variation by normalizing head nouns for inflection, less than 3% of the original phrases are common to the two corpora." ></td>
	<td class="line x" title="147:184	This proportion is significantly higher for the subset of disorder and procedure phrases where up to one third of MEDLINE phrases can be found in the MAYO corpus." ></td>
	<td class="line x" title="148:184	Not surprisingly, the proportion of adjectives in common is higher." ></td>
	<td class="line x" title="149:184	Overall, 44% of the adjectives in MAYO are also found in MEDLINE and up to 75% of the adjectives in MEDLINE are also found in MAYO (for disorders)." ></td>
	<td class="line x" title="150:184	Interestingly, the adjectives common to both corpora are also the most frequent." ></td>
	<td class="line x" title="151:184	For example, as shown in Table 2, the 1,584 adjective types in common in the subset Disorders account for 38% of all adjectives for Disorders (4,148), but the corresponding 25,557 adjective tokens account for 85% of all tokens (30,046)." ></td>
	<td class="line x" title="152:184	Table 1  Number of original phrases (types), for Disorders (Di) and Procedures (Pr) MEDLINE MAYO Together Common Di 4,941 19,641 22,774 1,808 Pr 1,534 4,959 6,028 465 All 1,322,403 1,575,476 2,857,848 40,031 Table 2  Number of adjectives (types [top] and tokens [bottom]), for Disorders (Di) and Procedures (Pr) MEDLINE MAYO Together Common 2,048 3,684 4,148 1,584 Di 6,299 23,747 30,046 25,557 902 1,499 1,790 611 Pr 1,852 5,667 7,519 5,683 72,324 44,268 97,762 18,830 All 1,916,530 2,209,778 4,126,308 3,885,852 Table 3  Number of demodified phrases (types [top] and tokens [bottom]), for Disorders (Di) and Procedures (Pr) MEDLINE MAYO Together Common 22,031 24,719 34,302 12,448 Di 174,548 463,097 637,645 571,041 9,850 8,595 13,691 4,754 Pr 101,323 166,180 267,503 241,790 1,487,889 1,047,772 2,403,504 132,157 All 2,826,395 3,092,340 5,918,735 2,709,100 0% 10% 20% 30% 40% 50% 60% 70% 1 2 3 4 1 2 3 4 1 2 3 4 Number of adjectives per phrase MEDLINE MAYO all disorders procedures Figure 2." ></td>
	<td class="line x" title="153:184	Distribution of the number of adjectives per phrase Qualitative results The list of the most frequent adjectives found in the original phrases corresponding to Disorders and Procedures in the UMLS is given in Table 4, with their rank in each corpus." ></td>
	<td class="line x" title="154:184	Interestingly, most high-ranking adjectives are found in both corpora." ></td>
	<td class="line x" title="155:184	Table 4  Rank of the most frequent adjectives in MEDLINE (ME) and MAYO (Ma) Disorders ME Ma Procedures ME Ma chronic 2 2 total 1 2 normal 3 1 surgical 2 3 acute 4 3 partial 5 1 congenital 1 8 serum 4 5 increased 6 5 patient 13 4 abnormal 8 4 percutaneous 3 15 neonatal 17 >100 renal 12 7 decreased 11 7 pulmonary 10 12 pulmonary 10 9 ultrasound >100 22 benign 7 13 general >100 23 renal 9 11 cardiac 16 8 recurrent 15 6 spinal 11 14 multiple 12 10 radical 14 13 increasing 14 12 evoked 29 >100 malignant 5 27 coronary 8 24 fetal 33 >100 femoral >100 33 nasal >100 33 studied 33 >100 joint 18 18 aortic >100 34 intracranial 40 >100 fluid 7 27 positive 24 17 abdominal 24 11 Considering not the original phrases, but demodified phrases corresponding to disorders and procedures, most adjectives with a frequency greater than 10 are found in the two corpora (86% for disorder and 80% for procedures)." ></td>
	<td class="line x" title="156:184	However, their representation may differ largely across corpora." ></td>
	<td class="line x" title="157:184	Examining the contexts of adjectives for Disorders (4978 adjectives with a frequency greater than 10), we found that 40% of the adjectives appear predominantly in MAYO (e.g. , mild, possible, recent, probable, questionable, greenish), 20% predominantly in MEDLINE (e.g. , experimental, human, neonatal, canine, intracellular), while 40% share most of their contexts across the two corpora (e.g. , acute, chronic, recurrent)." ></td>
	<td class="line x" title="158:184	The repartition of the demodified phrases for Disorders (8263 phrases with a frequency greater than 10) is somewhat different." ></td>
	<td class="line x" title="159:184	65% of the demodified phrases appear predominantly in MAYO (e.g. , discomfort, tenderness, low back pain, chest pain, diarrhea), 15% predominantly in MEDLINE (e.g. , resistance, strain, vesicle, hyperthermia), while 20% share most of their contexts across the two corpora (e.g. , disease, lesion, pain, symptom, abnormality)." ></td>
	<td class="line x" title="160:184	6 Applications In this section, we briefly examine some of the applications that may benefit from a better knowledge of adjectival modification in biomedical discourse: genre characterization, terminology and ontology acquisition, and information retrieval." ></td>
	<td class="line x" title="161:184	Genre characterization Knowledge about adjectives and demodified phrases predominantly associated with one corpus may be useful to characterize corpora, and in this experiment, genres." ></td>
	<td class="line x" title="162:184	Although limited, this study suggests, for example, that a clinical corpus contains markers for uncertainty (e.g. , possible, probable, questionable) and non-specific symptoms (e.g. , discomfort, low back pain)." ></td>
	<td class="line x" title="163:184	On the other hand, in a broad bibliographic corpus, precisions about organism or age groups must be given (e.g. , human, canine, neonatal)." ></td>
	<td class="line x" title="164:184	Interestingly, while the term fever is found with no predominance in either corpus, its more scientific synonyms hyperthermia and pyrexia are used predominantly in MEDLINE." ></td>
	<td class="line x" title="165:184	If corroborated, this finding may suggest that, although both scientific publications and medical records are geared toward peers, the language used in scientific publications tends to be more specialized." ></td>
	<td class="line x" title="166:184	Terminology and ontology acquisition The method described in this paper constitutes a useful technique for adapting existing terminologies and ontologies with empirically derived terms from a new subdomain." ></td>
	<td class="line x" title="167:184	First, demodified phrases are more likely to be mapped to another corpus." ></td>
	<td class="line x" title="168:184	And second, because adjectival modification often denotes a hyponymic relation between a phrase without modifier and a modified phrase, the modified phrase can be linked as a candidate hyponym to the phrase without modifier (Bodenreider et al. , 2002)." ></td>
	<td class="line x" title="169:184	This approach could be used, for example, for adapting biomedical terminologies to subtle clinical nuances." ></td>
	<td class="line x" title="170:184	When used with exactly the same subdomain the existing terminology comes from, this technique could enable regular updates of the terminology provided that current textual data is used for phrase extraction." ></td>
	<td class="line x" title="171:184	The approach is currently limited to simple adjectival modification; however, this is a selfimposed limitation." ></td>
	<td class="line x" title="172:184	Theoretically, the same methodology can be adapted to work on nominal, prepositional phrase and other types of modification." ></td>
	<td class="line x" title="173:184	Information retrieval Terminologies as well as ontologies are frequently used for information or document retrieval in the domains for which such terminologies or ontologies are available." ></td>
	<td class="line x" title="174:184	Medicine is one such domain where there are numerous terminological resources." ></td>
	<td class="line x" title="175:184	Integrated in a system such as the UMLS, these resources provide, for example, many synonyms for each concept, increasing the chances of retrieving documents from a given term." ></td>
	<td class="line x" title="176:184	However, most terms in these resources are pre-coordinated and may not include all the variants needed in various contexts." ></td>
	<td class="line x" title="177:184	Moreover, most terms are noun phrases and, while synonyms are often given for nouns, it may not be the case for their modifiers." ></td>
	<td class="line x" title="178:184	For example, while the various synonyms for fever (e.g. , hyperthermia and pyrexia) are present in the UMLS, there is no greenish variant for green sputum." ></td>
	<td class="line x" title="179:184	Nor can there systematically be a variant denoting uncertainty." ></td>
	<td class="line x" title="180:184	Therefore, identifying classes of adjectives that can be either ignored (e.g. , uncertainty markers) or mapped to other adjectives (e.g. , greenish to green) would increase the performance of information retrieval systems operating on clinical corpora." ></td>
	<td class="line x" title="181:184	In light of these findings, existing terminologies and ontologies can provide a core of medical concepts common to most subdomains; whereas the methodology described here can be used to tailor the general-purpose terminological resources to accommodate subdomain-specific terminology services." ></td>
	<td class="line x" title="182:184	7 Conclusions In conclusion, adjectival modification plays an important role in biomedical texts, and knowledge about this phenomenon can be exploited in applications such as the retrieval of biomedical documents and for developing terminology services in the biomedical domain." ></td>
	<td class="line x" title="183:184	In the future, we would like to identify patterns in biomedical terms and phrases based, in part, on classes of adjectival modifiers." ></td>
	<td class="line x" title="184:184	Creating such a model for terms would constitute a generative approach to biomedical terminology, contrasting with the lists of precoordinated terms populating most terminology systems in the biomedical domain." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W04-1211
Creating A Test Corpus Of Clinical Notes Manually Tagged For Part-Of-Speech Information
Pakhomov, Serguei V.;Coden, Anni;Chute, Christopher G.;"></td>
	<td class="line x" title="1:98	Creating a Test Corpus of Clinical Notes Manually Tagged for Part-of-Speech Information Serguei PAKHOMOV Division of Medical Informatics Research, Mayo Clinic Rochester, MN Pakhomov.Serguei@mayo.edu Anni CODEN IBM, T.J. Watson Research Center, Hawthorne, NY 10532 anni@us.ibm.com Christopher CHUTE Division of Medical Informatics Research, Mayo Clinic Rochester, MN Chute@mayo.edu Abstract This paper presents a project whose main goal is to construct a corpus of clinical text manually annotated for part-of-speech information." ></td>
	<td class="line x" title="2:98	We describe and discuss the process of training three domain experts to perform linguistic annotation." ></td>
	<td class="line x" title="3:98	We list some of the challenges as well as encouraging results pertaining to inter-rater agreement and consistency of annotation." ></td>
	<td class="line x" title="4:98	We also present preliminary experimental results indicating the necessity for adapting state-of-the-art POS taggers to the sublanguage domain of medical text." ></td>
	<td class="line x" title="5:98	1 Introduction Having reliable part-of-speech (POS) information is critical to successful implementation of Natural Language Processing (NLP) techniques for processing unrestricted text in the biomedical domain." ></td>
	<td class="line x" title="6:98	State-of-the-art automated POS taggers achieve accuracy of 93% 98% and the most successful implementations are based on statistical approaches to POS tagging." ></td>
	<td class="line p" title="7:98	Taggers based on Hidden Markoff Model (HMM) technology currently appear to be in the lead." ></td>
	<td class="line oc" title="8:98	The prime public domain examples of such implementations include the TrigramsnTags tagger (Brandts 2000), Xerox tagger (Cutting et al. 1992) and LT POS tagger (Mikheev 1997)." ></td>
	<td class="line x" title="9:98	Maximum Entropy (MaxEnt) based taggers also seem to perform very well (Ratnaparkhi 1996, Jason Baldridge, Tom Morton, and Gann Bierner http://maxent.sourceforge.net )." ></td>
	<td class="line x" title="10:98	One of the issues with statistical POS taggers is that most of them need a representative amount of hand-labeled training data either in the form of a comprehensive lexicon and a corpus of untagged data or a large corpus of text annotated for POS or a combination of the two." ></td>
	<td class="line x" title="11:98	Currently, most of the POS tagger accuracy reports are based on the experiments involving Penn Treebank data (Marcus, 1993)." ></td>
	<td class="line x" title="12:98	The texts in Treebank represent the general English domain." ></td>
	<td class="line x" title="13:98	It is not entirely clear how representative the general English language vocabulary and structure are of a specialized subdomain such as clinical reports." ></td>
	<td class="line x" title="14:98	A well-recognized problem is that the accuracy of all current POS taggers drops dramatically on unknown words." ></td>
	<td class="line x" title="15:98	For example, while the TnT tagger performs at 97% accuracy on known words in the Treebank, the accuracy drops to 89% on unknown words (Brandts, 2000)." ></td>
	<td class="line x" title="16:98	The LT POS tagger is reported to perform at 93.6-94.3% accuracy on known words and at 87.7-88.7% on unknown words using a cascading unknown word guesser (Mikheev, 1997)." ></td>
	<td class="line x" title="17:98	The overall results for both of these taggers are much closer to the high end of the spectrum because the rate of the unknown words in the tests performed on the Penn Treebank corpus is generally relatively low  2.9% (Brandts, 2000)." ></td>
	<td class="line x" title="18:98	From these results, we can conclude that the higher the rate of unknown vocabulary, the lower the overall accuracy will be, necessitating the adaptation of the taggers trained on Penn Treebank to sublanguage domains with vocabulary that is substantially different from the one represented by the Penn Treebank corpus." ></td>
	<td class="line x" title="19:98	Based on the observable differences between the clinical and the general English discourse and POS tagging accuracy results on unknown vocabulary, it is reasonable to assume that a tagger trained on general English may not perform as well on clinical notes, where the percentage of unknown words will increase." ></td>
	<td class="line x" title="20:98	To test this assumption, a gold standard corpus of clinical notes needs to be manually annotated for POS information." ></td>
	<td class="line x" title="21:98	The issues with the annotation process constitute the primary focus of this paper." ></td>
	<td class="line x" title="22:98	We describe an effort to train three medical coding experts to mark the text of clinical notes for part-of-speech information." ></td>
	<td class="line x" title="23:98	The motivation for using medical coders rather than trained linguists is threefold." ></td>
	<td class="line x" title="24:98	First of all, due to confidentiality restrictions, in order to develop a corpus of hand labeled data from clinical notes one can only use personnel authorized to access patient information." ></td>
	<td class="line x" title="25:98	The only way to avoid it, is to anonymize the notes prior to POS tagging which in itself is a difficult and expensive process (Ruch et al. 2000)." ></td>
	<td class="line x" title="26:98	Second, medical coding experts are well familiar with 62 clinical discourse, which helps especially with annotating medicine specific vocabulary." ></td>
	<td class="line x" title="27:98	Third, the fact that POS tagging can be viewed as a classification task makes the medical coding experts highly suitable because their primary occupation and expertise is in classifying patient records for subsequent retrieval." ></td>
	<td class="line x" title="28:98	We show that, given a good set of guidelines, medical coding experts can be trained in a limited amount of time to perform a linguistic task such as POS annotation at a high level of agreement on both clinical notes and Penn Treebank data." ></td>
	<td class="line x" title="29:98	Finally, we report on a set of training experiments performed with the TnT tagger (Brandts, 2000) using the Penn Treebank as well as the newly developed medical corpus 2 Annotation Prior to this study, the three annotators who participated in it had a substantial experience in coding clinical diagnoses but virtually no experience in POS markup." ></td>
	<td class="line x" title="30:98	The training process consisted of a general and rather superficial introduction to the issues in linguistics as well as some formal training using the POS tagging guidelines developed by Santoriny (1991) for tagging Penn Treebank data." ></td>
	<td class="line x" title="31:98	The formal training was followed by informal discussions of the data and difficult cases pertinent to the clinical notes domain which often resulted in slight modifications to the Penn Treebank guidelines." ></td>
	<td class="line x" title="32:98	The annotation process consisted of preprocessing and editing." ></td>
	<td class="line x" title="33:98	The pre-processing includes sentence boundary detection, tokenization and priming with part-of-speech tags generated by a MaxEnt tagger (Maxent 1.2.4 package (Baldridge et al))." ></td>
	<td class="line x" title="34:98	trained on Penn Treebank data." ></td>
	<td class="line x" title="35:98	Automatically annotated notes were then presented to the domain experts for editing." ></td>
	<td class="line x" title="36:98	3 Annotator agreement In order to establish reliability of the data, we need to ensure internal as well as external consistency of the annotation." ></td>
	<td class="line x" title="37:98	First of all, we need to make sure that the annotators agree amongst themselves (internal consistency) on how they mark up text for part-of-speech information." ></td>
	<td class="line x" title="38:98	Second, we need to find out how closely the annotators generating data for this study agree with the annotators of an established project such as Penn Treebank (external consistency)." ></td>
	<td class="line x" title="39:98	If both tests show relatively high levels of agreement, then we can safely assume that the annotators in this study are able to generate part-of-speech tags for biomedical data that will be consistent with a widely recognized standard and can work independently of each other thus tripling the amount of manually annotated data." ></td>
	<td class="line x" title="40:98	3.1 Methods Two types of measures of consistency were computed  absolute agreement and Kappa coefficient." ></td>
	<td class="line x" title="41:98	The absolute agreement (Abs Agr) was calculated by dividing the total number of times all annotators agreed on a tag over the total number of tags." ></td>
	<td class="line x" title="42:98	Kappa coefficient is given in (1) (Carletta 1996) (1) )(1 )()( EP EPAP Kappa   = where P(A) is the proportion of times the annotators actually agree and P(E) is the proportion of times the annotators are expected to agree due to chance 3." ></td>
	<td class="line x" title="43:98	The Absolute Agreement is most informative when computed over several sets of labels and where one of the sets represents the authoritative set." ></td>
	<td class="line x" title="44:98	In this case, the ratio of matches among all the sets including the authoritative set to the total number of labels shows how close the other sets are to the authoritative one." ></td>
	<td class="line x" title="45:98	The Kappa statistic is useful in measuring how consistent the annotators are compared to each other as opposed to an authority standard." ></td>
	<td class="line x" title="46:98	3.2 Annotator consistency In order to test for internal consistency, we analyzed inter-annotator agreement where the three annotators tagged the same small corpus of clinical dictations." ></td>
	<td class="line x" title="47:98	File ID Abs agr." ></td>
	<td class="line x" title="48:98	Kappa N Samples 1137689 93.24% 0.9527 755 1165875 94.59% 0.9622 795 1283904 89.79% 0.9302 392 1284881 90.42% 0.9328 397 1307526 84.43% 0.8943 347 Total 2686 Average 90.49% 0.9344 Table 1." ></td>
	<td class="line x" title="49:98	Annotator agreement results based on 5 clinical notes 3 A very detailed explanation of the terms used in the formula for Kappa computation as well as concrete examples of how it is computed are provided in Poessio and Vieira (1988)." ></td>
	<td class="line x" title="50:98	63 The results were compared and the Kappastatistic was used to calculate the inter-annotator agreement." ></td>
	<td class="line x" title="51:98	The results of this experiment are summarized in Table 1." ></td>
	<td class="line x" title="52:98	For the absolute agreement, we computed the ratio of how many times all three annotators agreed on a tag for a given token to the total number of tags." ></td>
	<td class="line x" title="53:98	Based on the small pilot sample of 5 clinical notes (2686 words), the Kappa test showed a very high agreement coefficient  0.93." ></td>
	<td class="line x" title="54:98	An acceptable agreement for most NLP classification tasks lies between 0.7 and 0.8 (Carletta 1996, Poessio and Vieira 1988)." ></td>
	<td class="line x" title="55:98	Absolute agreement numbers are consistent with high Kappa as they show an average of 90% of all tags in the test documents assigned exactly the same way by all three annotators." ></td>
	<td class="line x" title="56:98	The external consistency with the Penn Treebank annotation was computed using a small random sample of 939 words from the Penn Treebank Corpus annotated for POS information." ></td>
	<td class="line x" title="57:98	Annotator Abs agr A1 88.17% A2 87.85% A3 87.85% Average 87.95% Table 2." ></td>
	<td class="line x" title="58:98	Absolute agreement results based on 5 clinical notes with an authority label set." ></td>
	<td class="line x" title="59:98	The results in Table 2 show that the three annotators are on average 88% consistent with the annotators of the Penn Treebank corpus." ></td>
	<td class="line x" title="60:98	3.3 Descriptive statistics for the corpus of clinical notes The annotation process resulted in a corpus of 273 clinical notes annotated with POS tags." ></td>
	<td class="line x" title="61:98	The corpus contains 100650 tokens from 8702 types distributed across 7299 sentences." ></td>
	<td class="line x" title="62:98	Table 3 displays frequency counts for the top most frequent syntactic categories." ></td>
	<td class="line x" title="63:98	Category Count % total NN 18372 18% IN 8963 9% JJ 8851 9% DT 6796 7% NNP 4794 5% Table 3 Syntactic category distribution in the corpus of clinical notes." ></td>
	<td class="line x" title="64:98	The distribution of syntactic categories suggests the predominance of nominal categories, which is consistent with the nature of clinical notes reporting on various patient characteristics such as disorders, signs and symptoms." ></td>
	<td class="line x" title="65:98	Another important descriptive characteristic of this corpus is that the average sentence length is 13.79 tokens per sentence, which is relatively short as compared to the Treebank corpus where the average sentence length is 24.16 tokens per sentence." ></td>
	<td class="line x" title="66:98	This supports our informal observation of the clinical notes data containing multiple sentence fragments and short diagnostic statements." ></td>
	<td class="line x" title="67:98	Shorter sentence length implies greater number of inter-sentential transitions and therefore is likely to present a challenge for a stochastic process." ></td>
	<td class="line x" title="68:98	4 Training a POS tagger on medical data In order to test some of our assumptions regarding how the differences between general English language and the language of clinical notes may affect POS tagging, we have trained the HMM-based TnT tagger (Brandts, 2000) with default parameters at the tri-gram level both on Penn Treebank and the clinical notes data." ></td>
	<td class="line x" title="69:98	We should also note that the tagger relies on a sophisticated unknown word guessing algorithm which computes the likelihood of a tag based on the N last letters of the word, which is meant to leverage the words morphology in a purely statistical manner." ></td>
	<td class="line x" title="70:98	The clinical notes data was split at random 10 times in 80/20 fashion where 80% of the sentences were used for training and 20% were used for testing." ></td>
	<td class="line x" title="71:98	This technique is a variation on the classic 10-fold validation and appears to be more suitable for smaller amounts of data." ></td>
	<td class="line x" title="72:98	We conducted two experiments." ></td>
	<td class="line x" title="73:98	First, we computed the correctness of the Treebank model on each fold of the clinical notes data." ></td>
	<td class="line x" title="74:98	We tested the Treebank model on the 10 folds rather than the whole corpus of clinical notes in order to produce correctness results on exactly the same test data as would be used for validation tests of models build from the clinical notes data." ></td>
	<td class="line x" title="75:98	Then, we computed the correctness of each of the 10 models trained on each training fold of the clinical notes data using the corresponding testing fold of the same data for testing." ></td>
	<td class="line x" title="76:98	Table 4 Correctness results for the Treebank model." ></td>
	<td class="line x" title="77:98	Correctness was computed simply as the percentage of correct tag assignments of the POS tagger (hits) to the total number of tokens in the test set." ></td>
	<td class="line x" title="78:98	Table 4 summarizes the results of testing the Treebank model, while Table 5 summarizes the Split Hits Total Correctness Average 21826.3 24309 89.79% 64 testing results for the models trained on the clinical notes." ></td>
	<td class="line x" title="79:98	The average correctness of the Treebank model tested on clinical notes is ~88%, which is considerably lower than the state-of-the-art performance of the TnT tagger ~96%." ></td>
	<td class="line x" title="80:98	Training the tagger on a relatively small amount of clinical notes data brings the performance much closer to the state-of-the-art  ~95%." ></td>
	<td class="line x" title="81:98	Table 5 Correctness results for the clinical notes model." ></td>
	<td class="line x" title="82:98	5 Discussion The results of this pilot project are encouraging." ></td>
	<td class="line x" title="83:98	It is clear that with appropriate supervision, people who are well familiar with medical content can be reliably trained to carry out some of the tasks traditionally done by trained linguists." ></td>
	<td class="line x" title="84:98	This study also indicates that an automatic POS tagger trained on data that does not include clinical documents may not perform as well as a tagger trained on data from the same domain." ></td>
	<td class="line x" title="85:98	A comparison between the Treebank and the clinical notes data shows that the clinical notes corpus contains 3,239 lexical items that are not found in Treebank." ></td>
	<td class="line x" title="86:98	The Treebank corpus contains over 40,000 lexical items that are not found in the corpus of clinical notes." ></td>
	<td class="line x" title="87:98	5,463 lexical items are found in both corpora." ></td>
	<td class="line x" title="88:98	In addition to this 37% outof-vocabulary rate (words in clinical notes but not the Treebank corpus), the picture is further complicated by the differences between the n-gram tag transitions within the two corpora." ></td>
	<td class="line x" title="89:98	For example, the likelihood of a DT  NN bigram is 1 in Treebank and 0.75 in the clinical notes corpus." ></td>
	<td class="line x" title="90:98	On the other hand, JJ  NN transition in the clinical notes is 1 but in the Treebank corpus it has a likelihood of 0.73." ></td>
	<td class="line x" title="91:98	This is just to illustrate the fact that not only the unknown out-of-vocabulary items may be responsible for the decreased accuracy of POS taggers trained on general English domain and tested on the clinical notes domain, but the actual n-gram statistics may be a major contributing factor." ></td>
	<td class="line x" title="92:98	6 Conclusion Several questions remain unresolved." ></td>
	<td class="line x" title="93:98	First of all, it is unclear how much domain specific data is enough to achieve state-of-the-art performance on POS tagging." ></td>
	<td class="line x" title="94:98	Second, given that it is somewhat easier to develop lexicons for POS tagging than to annotate corpora, we need to find out how important the corpus statistics are as opposed to a domain specific lexicon." ></td>
	<td class="line x" title="95:98	In other words, can we achieve state-of-the-art performance in a specialized domain by simply adding the vocabulary from the domain to the POS taggers lexicon?" ></td>
	<td class="line x" title="96:98	We intend to address both of these questions with further experimentation." ></td>
	<td class="line x" title="97:98	7 Acknowledgements Our thanks go to Barbara Abbot, Pauline Funk and Debora Albrecht for their persistent efforts in the difficult task of corpus annotation." ></td>
	<td class="line x" title="98:98	This work has been carried out under the NLM Training Grant # T15 LM07041-19." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W04-2010
Robust Ending Guessing Rules With Application To Slavonic Languages
Nakov, Preslav I.;Paskaleva, Elena;"></td>
	<td class="line x" title="1:241	Robust Ending Guessing Rules with Application to Slavonic Languages Preslav NAKOV EECS, CS Division, University of California, Berkeley Berkeley, CA, 94720 USA nakov@cs.berkeley.edu Elena PASKALEVA Linguistic Modelling Department, IPP Bulgarian Academy of Sciences 25A, Acad." ></td>
	<td class="line x" title="2:241	G. Bontchev St Sofia, Bulgaria, 1113 hellen@lml.bas.bg Abstract The paper studies the automatic extraction of diagnostic word endings for Slavonic languages aimed to determine some grammatical, morphological and semantic properties of the underlying word." ></td>
	<td class="line x" title="3:241	In particular, ending guessing rules are being learned from a large morphological dictionary of Bulgarian in order to predict POS, gender, number, article and semantics." ></td>
	<td class="line x" title="4:241	A simple exact high accuracy algorithm is developed and compared to an approximate one, which uses a scoring function previously proposed by Mikheev for POS guessing." ></td>
	<td class="line x" title="5:241	It is shown how the number of rules of the latter can be reduced by a factor of up to 35, without sacrificing performance." ></td>
	<td class="line x" title="6:241	The evaluation demonstrates coverage close to 100%, and precision of 97-99% for the approximate algorithm." ></td>
	<td class="line x" title="7:241	1 Introduction An important property of the Slavonic languages is the rich morphology, which determines the specifics of their representation and processing in NLP applications." ></td>
	<td class="line x" title="8:241	This variety is arranged not only linearly along the paradigmatic axe, i.e. abundance of wordforms for a given lemma (up to 52 forms for the Bulgarian verb), but also in the derivational tree (up to 30 members per word formation)." ></td>
	<td class="line x" title="9:241	The grammatical system of the Slavonic languages and their descriptions differentiate these two mechanisms as word formation and word derivation." ></td>
	<td class="line x" title="10:241	The word formation building blocks define the so called inflectional classes, which represent sequential letter strings associated with word classes as well as with individual words, also known as isuffixes in Porter-like stemmers (Porter,1980)." ></td>
	<td class="line x" title="11:241	The derivational building blocks represent derivational suffixes listed in grammars (d-suffixes in Porterlike stemmers)." ></td>
	<td class="line x" title="12:241	A considerable part of the Slavonic d-suffixes change not only the part of speech (POS) but also the semantics of the newly formed word." ></td>
	<td class="line x" title="13:241	When multiple d-suffixes are concatenated, the word formation chain yields also a semantic derivation." ></td>
	<td class="line x" title="14:241	For example, the chain (observe ; observer ; observing ; observability): !'#$%&-(=>=)?" ></td>
	<td class="line x" title="15:241	@=ABCD-''($ ; @=ABCD=EFB-(!" ></td>
	<td class="line x" title="16:241	@=ABCD=EFB@-)*' represents the derivation: verb ; noun ; adjective ; noun but also the following semantic transformation: action ; actor ; feature ; abstract feature The combination of grammatical and semantic functions of the Slavonic d-suffixes, together with their frequent usage (at least for some of them) and the high productivity, make very attractive the idea to study the regularities and the predictive power of ending letter combinations in a large text set." ></td>
	<td class="line x" title="17:241	We believe the results obtained over a representative collection can be used in a variety of robust analysis applications." ></td>
	<td class="line x" title="18:241	Linguistically, we interpret the last term as operations over a large text set with insufficient linguistic support, typically given by a lexical database, grammatical rules, parsing rules etc. We target applications like POS tagging, text categorisation, information extraction, word sense disambiguation, question answering etc. Below we concentrate on the automatic extraction of a set of diagnostic word endings for Bulgarian that can determine the POS as well as some grammatical, morphological and semantic properties of the underlying word." ></td>
	<td class="line x" title="19:241	This is a two-step process including endings identification & learning and application & evaluation." ></td>
	<td class="line x" title="20:241	The paper is organised as follows." ></td>
	<td class="line x" title="21:241	Section 2 discusses the related work on POS guessing and general morphology." ></td>
	<td class="line x" title="22:241	Section 3 introduces our basic resource: the Large Grammatical Dictionary of Bulgarian." ></td>
	<td class="line x" title="23:241	Section 4 describes two algorithms for ending guessing rules induction (an exact and an approximate one) and how to reduce the number of rules by a factor of up to 35." ></td>
	<td class="line x" title="24:241	Section 5 contains the experimental setup and evaluation trying to predict POS, gender, number, article and semantics." ></td>
	<td class="line x" title="25:241	Section 6 discusses the results and Section 7 points to direction for future work." ></td>
	<td class="line x" title="26:241	2 Related Work POS guessing." ></td>
	<td class="line x" title="27:241	Kupiec (1992) uses pre-specified suffixes and performs statistical learning for POS guessing." ></td>
	<td class="line oc" title="28:241	The XEROX tagger comes with a list of built-in ending guessing rules (Cutting et al. ,1992)." ></td>
	<td class="line x" title="29:241	In addition to the ending, Weischedel et al.(1993) exploit capitalisation." ></td>
	<td class="line x" title="31:241	Thede and Harper (1997) consider contextual information, word endings, entropy and open-class smoothing." ></td>
	<td class="line x" title="32:241	A similar approach is presented in (Schmid,1995)." ></td>
	<td class="line x" title="33:241	Ruch et al.(2000) combine POS guessing, contextual rules and Markov models to build a POS tagger for biomedical text." ></td>
	<td class="line x" title="35:241	A very influential is the work of Brill (1997), who induces more linguistically motivated rules exploiting both a tagged corpus and a lexicon." ></td>
	<td class="line x" title="36:241	He does not look at the affixes only, but also checks their POS class in a lexicon." ></td>
	<td class="line x" title="37:241	Mikheev (1997) proposes a similar approach, but learns the rules from raw as opposed to tagged text." ></td>
	<td class="line x" title="38:241	Daciuk (1999) speeds up the process by means of finite state transducers." ></td>
	<td class="line x" title="39:241	General morphology." ></td>
	<td class="line x" title="40:241	Nakov et al.(2003) use ending guessing rules to predict the morphological class of unknown German nouns." ></td>
	<td class="line x" title="42:241	Schone and Jurafsky (2000) apply latent semantic analysis for a knowledge-free morphology induction." ></td>
	<td class="line x" title="43:241	DeJean (1998), Hafer and Weiss (1974) follow a successor variety approach: the word is cut, if the number of distinct letters after a pre-specified sequence surpasses a threshold." ></td>
	<td class="line x" title="44:241	Goldsmith (2001) performs a minimum description length analysis of the morphology of several European languages using corpora." ></td>
	<td class="line x" title="45:241	Gaussier (1999) induces derivational morphology from a lexicon by means of p-similarity based splitting." ></td>
	<td class="line x" title="46:241	Jacquemin (1997) focuses on the morphological processes." ></td>
	<td class="line x" title="47:241	Van den Bosch and Daelemans (1999) propose a memory-based approach, which maps directly from letters in context to categories that encode morphological boundaries, syntactic class labels and spelling changes." ></td>
	<td class="line x" title="48:241	Yarowsky and Wicentowski (2000) present a corpus-based approach for morphological analysis of both regular and irregular forms based on four models including: relative corpus frequency, context similarity, weighted string similarity and incremental retraining of inflectional transduction probabilities." ></td>
	<td class="line x" title="49:241	Another interesting work, exploiting capitalisation and fixed/variable suffixes, is presented in Cucerzan and Yarowsky (2000)." ></td>
	<td class="line x" title="50:241	3 Source Data As the related work above shows, a large lexical database is often needed for the automatic identification of good diagnostic word endings." ></td>
	<td class="line x" title="51:241	In particular, in our experiments we used the Large Grammatical Dictionary of Bulgarian (Paskaleva,2003), created at the Linguistic Modelling Department of the Bulgarian Academy of Sciences (CLPP-BAS) and comprising approximately 995,000 wordforms (about 65,000 lemmas), encoded in DELAF format (Silberztein,1993)." ></td>
	<td class="line x" title="52:241	The following information is listed for each wordform: 1) lemma; 2) lemma properties (POS, additional grammatical features related to the word formation: gender, e.g. for the nouns; degree, e.g. for the adjectives; transitivity, for verbs; kind for pronouns/numerals, etc.); and 3) properties of the wordform as a member of the lemma paradigm." ></td>
	<td class="line x" title="53:241	The first group of properties represent our primary learning resource, as we focus on the extraction of ending rules for whole word classes and not for individual words." ></td>
	<td class="line x" title="54:241	4 Ending Guessing Rules Extraction Our learning algorithms produce lists of endings of various length (up to 8 letters), predicting different kinds of linguistic information (see below for details): POS: adjective/adverb/noun/numeral/verb article: definite/indefinite/none gender: feminine/masculine/neutre/none number: singular/plural/none semantics: human/animate/none We use two different algorithms, inducing exact and approximate ending rules, accordingly." ></td>
	<td class="line x" title="55:241	4.1 Exact Rules A study of the ending letter sequences of the dictionary entries and their properties shows the well known inverse correlation between the length of a word ending and its ambiguity: the shorter the string, the more likely to be ambiguous." ></td>
	<td class="line x" title="56:241	This raises the idea of a simple algorithm producing 100% correct rules 1." ></td>
	<td class="line x" title="57:241	Suppose we want to predict POS and let us consider all wordforms in the dictionary that end on -=." ></td>
	<td class="line x" title="58:241	There are 203,420 of them, distributed as follows 2 : V=128,162(63.00%), A=42,262(20.78%), N=32,597(16.02%), NU=240(0.12%), ADV=99(0.05%), PRO=38(0.02%), INTJ=7(0.00%), CONJ=7(0.00%), PC=6(0.00%), PREP=2(0.00%)." ></td>
	<td class="line x" title="59:241	Let us now consider a sequence with an additional starting letter, e.g. -E=." ></td>
	<td class="line x" title="60:241	There are 83,375 wordforms with this ending, distributed in POS as follows: V=42,843(51.39%), A=22,225(26.66%), N=18,092(21.70%), NU=157(0.19%), ADV=48(0.06%), PRO=9(0.01%), CONJ=1(0.00%)." ></td>
	<td class="line x" title="61:241	When a further letter is included, 1 As it is 100% precise it risks over fitting and thus a low coverage." ></td>
	<td class="line x" title="62:241	We will return to this issue later." ></td>
	<td class="line x" title="63:241	2 We use the following abbreviations for the ten POS: A (adjective), ADV (adverb), CONJ (conjunction), INTJ (interjunction), N (noun), NU (numeral), PC (particle), PREP (preposition), PRO (pronoun) and V (verb)." ></td>
	<td class="line x" title="64:241	we obtain e.g. -=E= with a total frequency of 72,235 and a POS distribution: V=42249(58.49%), A=21415(29.65%), N=8399(11.63%), NU=119(0.16%), ADV=47(0.07%), PRO=6(0.01%)." ></td>
	<td class="line x" title="65:241	Next, for -W=E= we have a frequency of only 799 and an even lower ambiguity: N=793(99.25%), A=6(0.75%)." ></td>
	<td class="line x" title="66:241	Finally, there is a single POS tag for -XW=E=: N=726(100.00%)." ></td>
	<td class="line x" title="67:241	Note how the most likely tag (shown in italic for each ending above) and the degree of certainty about it change." ></td>
	<td class="line x" title="68:241	At the beginning, the most likely tag was V, but later it changed to N. In addition, the uncertainty does not necessarily decrease monotonically as the most likely tag changes from V(63.00%) to V(51.39%) to V(58.49%) to N(99.25%) and to N(100.00%)." ></td>
	<td class="line x" title="69:241	Generalizing this example, we obtain the following Exact Algorithm: 1." ></td>
	<td class="line x" title="70:241	S =  E = {all possible endings of dictionary wordforms, up to k letters long}; 2." ></td>
	<td class="line x" title="71:241	While E [ 2.1." ></td>
	<td class="line x" title="72:241	Take a random ending e from E of minimum length." ></td>
	<td class="line x" title="73:241	2.2." ></td>
	<td class="line x" title="74:241	If all wordforms in the dictionary that end on e have the same POS then S  e. 3." ></td>
	<td class="line x" title="75:241	Output S. Wordforms Number of Different POS count % 1 936,409 97.37% 2 24,913 2.59% 3 356 0.03% 4 1 0.00% Table 1: Dictionary ambiguity with respect to POS." ></td>
	<td class="line x" title="76:241	While it is clear that this approach produces only 100% correct rules (and also the shortest possible ones), its coverage is not guaranteed to be 100% due to homography, i.e. the same graphemic wordform can be met in the dictionary multiple times with different annotations." ></td>
	<td class="line x" title="77:241	For example, \EA]=@= is annotated as 3 : \EA]=@=,\EAF]=.V+F+T:Psf \EA]=@=,\EA]=@.ADJ:sf \EA]=@=,\EA]=@=.N+F:s The first one denotes the inflected wordform selected of the finite transitive verb select, the second 3 The format used is as follows inflected_form, lemma . lemma_properties : wordform_properties one stands for the feminine adjective selected, and the last one, for the feminine noun defence." ></td>
	<td class="line x" title="78:241	In fact, the level of ambiguity is relatively low: 97.37% of the wordforms in the dictionary are unambiguous, so ignoring the ambiguity on training is not unreasonable." ></td>
	<td class="line x" title="79:241	See Table 1 for a detailed dictionary ambiguity distribution with respect to POS." ></td>
	<td class="line x" title="80:241	4.2 Approximate Rules Our approximate rules are similar to the ones proposed by Mikheev (1997), who uses a dictionary to build POS prediction rules with four parts: deletion (), addition (+), checking against the dictionary ()?" ></td>
	<td class="line x" title="81:241	and POS assignment (;)." ></td>
	<td class="line x" title="82:241	Generally speaking, each rule operates either on the beginning or the ending of the target wordform." ></td>
	<td class="line x" title="83:241	For example, the following rule says that if an unknown word ends on -ied, this ending should be stripped, -y should be appended, a check should be performed of whether the newly created word is in the dictionary and annotated as (VB VBP) there, and if so, (JJ VBD VBN) for the original word should be predicted: e[ied +y ?(VP VBP) ; (JJ VBD VBN)] All rule elements are optional, except for the POS assignment." ></td>
	<td class="line x" title="84:241	This means that a rule can just add and/or remove letters, without looking in the dictionary (although it could potentially benefit from doing so)." ></td>
	<td class="line x" title="85:241	When both removal and addition are used, one can account for mutations in the word stem." ></td>
	<td class="line x" title="86:241	In fact, Mikheev uses the following restricted types of rules: Prefix (prefix deletion and dictionary lookup), Suffix 0 (suffix deletion and dictionary lookup), Suffix 1 (suffix deletion with mutation in the last letter and dictionary lookup), Ending (suffix deletion)." ></td>
	<td class="line x" title="87:241	There are separate ending guessing rules for hyphenated, capitalised and all other words." ></td>
	<td class="line x" title="88:241	Given a dictionary, a scan through the wordforms is performed, during which all possible rules are collected and scored, and those above some threshold are selected." ></td>
	<td class="line x" title="89:241	Finally, rule merging is applied to rules with identical preconditions but different predictions: the new rule predicts the union of the predictions of the original rules, which results in higher ambiguity but possibly allows the new rule to pass above the threshold after being rescored." ></td>
	<td class="line x" title="90:241	We do not use the full power of the Mikheevlike rules and we limit ourselves to ending rules without dictionary lookup and single class predictions." ></td>
	<td class="line x" title="91:241	Further, at present we do not treat the hyphenated or capitalised wordforms in any special way." ></td>
	<td class="line x" title="92:241	The intuition behind the Mikheevs rule score is that a good guessing rule should be unambiguous (predicts a particular class without or with only very few exceptions), frequent (must be based on a large number of occurrences) and long (the longer the rule the lower the probability that it will happen by chance and thus the better its prediction)." ></td>
	<td class="line x" title="93:241	These criteria are combined in the following formula: )log(1 )1( )1( 2/)1( l n pp t pscore n +  =   where: l is the rule length; x is the number of successful rule guesses; n is the total number of training instances compatible with the rule; p is a smoothed version of the maximum likelihood estimation p, which ensures that neither p nor (1p) could be zero: p = (x+0.5)/(n+1); n pp )1 (  is an estimation of the dispersion; )1( 2/)1(   n t  is a coefficient of the t-distribution with n1 degrees of freedom and confidence level ." ></td>
	<td class="line x" title="94:241	It is important to note that Mikheev weights the rule frequencies with the frequencies of the wordforms they match as estimated from raw text." ></td>
	<td class="line x" title="95:241	We performed experiments both with and without such weighting." ></td>
	<td class="line x" title="96:241	Cleaned Threshold Original 100% only everything 0.00 738,446 115,474 20,846 0.50 597,238 89,324 18,663 0.80 122,439 27,477 4,881 0.90 55,144 15,071 2,459 0.95 22,015 7,673 1,402 Table 2: Mikheev-like rules for POS guessing count: original and cleaned (100% correct and all)." ></td>
	<td class="line x" title="97:241	Column 2 of Table 2 gives an idea about the number of selected ending guessing rules for POS prediction when different thresholds are used (and when the training was performed on a subset of the dictionary, containing 894,915 wordforms, as described below)." ></td>
	<td class="line x" title="98:241	We were unhappy with such a large number of rules, especially after we observed that they were highly redundant." ></td>
	<td class="line x" title="99:241	For example, if the threshold is set to 0.95, all the rules listed in Table 3 (and many more) are selected." ></td>
	<td class="line x" title="100:241	In fact, all these are covered by the ending -dEF, which is 100% correct, and they all predict that the POS should be verb." ></td>
	<td class="line x" title="101:241	So, all we need is to keep -dEF, while dropping all other longer endings that have additional starting letters 4 . This reduces the number of rules by a factor of 3 to 7 (see column 3 of Table 2)." ></td>
	<td class="line x" title="102:241	Thinking again, we can see that we can reduce the number of rules even further." ></td>
	<td class="line x" title="103:241	For example, there is a rule ->=d=, which is scored 0.99967073 and was met 6,593 times as a verb and only once as a noun (i.e. it is 99.98% correct)." ></td>
	<td class="line x" title="104:241	There is another one -e>=d=, which is scored 0.99943267, and was met 1,498 times, always as a verb." ></td>
	<td class="line x" title="105:241	There are also rules like -=>=d=, -f>=d=, etc. Obviously, all they, and any other ending on ->=d=, will make the same prediction, so we do not need to keep them." ></td>
	<td class="line x" title="106:241	Removing the redundancies of this kind leads to another dramatic drop in the number of rules by a similar factor (see column 4 of Table 2)." ></td>
	<td class="line x" title="107:241	In the experiments below we always applied this kind of cleaning." ></td>
	<td class="line x" title="108:241	5 Ending Score Frequency -F@e>=dEF 0.98336703 47 -@e>=dEF 0.99666399 241 -e>=dEF 0.99944650 1,489 ->=dEF 0.99987014 6,546 -=dEF 0.99992176 11,346 -dEF 0.99995697 22,074 Table 3: Some redundant selection for -dEF." ></td>
	<td class="line x" title="109:241	5 Evaluation We ran two different general types of experiments: using the dictionary only and using additional raw text to estimate the frequencies of the dictionary words." ></td>
	<td class="line x" title="110:241	We split the dictionary into two parts at random: 894,915 wordforms for training (about 90%) and the remaining 99,624 wordforms for testing." ></td>
	<td class="line x" title="111:241	In the dictionary-only experiments we extracted the ending guessing rules by observing the endings of all wordforms from the training part of the dictionary 6 . We then applied the rules thus learned (each time preferring the longest one that is compatible with the target word) to the testing part of the dictionary and we measured the precision P (what % of the cases the predicted class matched the hypothesised one) and the coverage C (what % of the cases there was at least one rule that was compatible with the target wordform)." ></td>
	<td class="line x" title="112:241	We also calculated a kind of F-measure, which is normally 4 Table 3 does not list all of them and there are several dozens additional highly scored ones, e.g. -FdEF." ></td>
	<td class="line x" title="113:241	5 It looks like Mikheev (1997) did not observe that kind of redundancy." ></td>
	<td class="line x" title="114:241	6 For a given word, we extracted all the corresponding endings up to 8 letters long." ></td>
	<td class="line x" title="115:241	This can possibly be the whole word." ></td>
	<td class="line x" title="116:241	defined as 2PR/(P+R), where R is the recall (proportion of proposed instances out of all that have to be found)." ></td>
	<td class="line x" title="117:241	Precision, recall and F-measure are defined in the information retrieval community in terms of positive and negative documents for a given query, i.e. with respect to a single class, but here we have multiple of them." ></td>
	<td class="line x" title="118:241	While we can define both an overall and a class-specific precision, it makes sense to talk about recall with respect to a particular class, but about coverage, when this is a measure for all classes." ></td>
	<td class="line x" title="119:241	So, we redefined the F-measure as 2PC/(P+C)." ></td>
	<td class="line x" title="120:241	In the dictionary+text experiments, we use the same training and testing parts of the dictionary, and in addition, the frequencies for the corresponding words in the training and testing text sets, accordingly." ></td>
	<td class="line x" title="121:241	I.e. a wordform in the text that is not in the dictionary is ignored and the rest are treated as if they have been repeated in its training/testing part the same number of times as they were met in the training/testing raw text." ></td>
	<td class="line x" title="122:241	We used a collection of 23.5 MB of various genres of Bulgarian texts as follows: legal: 742 KB poetry: 236 KB prose: 1,032 KB religion: 393 KB newspapers: 21,118 KB We used 2,211 KB of the newspaper texts for testing (about 10% of the collection) and the rest for training." ></td>
	<td class="line x" title="123:241	As we already mentioned above, the same graphemic wordform can be met in the dictionary multiple times with different annotations." ></td>
	<td class="line x" title="124:241	In such cases, we treated them as equally likely both on training and testing." ></td>
	<td class="line x" title="125:241	This resulted in 1,751,963 wordforms tokens on training and 18,832 on testing." ></td>
	<td class="line x" title="126:241	The huge difference is due to the fact that on testing we have both 10 times smaller dictionary and 10 times smaller text set to estimate the wordform frequencies from, which multiply and result in 100 fold drop." ></td>
	<td class="line x" title="127:241	For all experiments, we excluded the wordforms from a stoplist composed of the closed class words, i.e. the ones with the following POS (counts in parentheses): auxiliary verbs (91), conjunctions (31), interjections (28), particles (41), prepositions (69) and pronouns (286)." ></td>
	<td class="line x" title="128:241	We have been hesitating also about the numerals but there were 582 of them in the dictionary and one can produce more, so they do not represent a closed class and we did not include them." ></td>
	<td class="line x" title="129:241	A potential problem with the stopwords removal is that many of them can also be non-stop ones depending on their POS, e.g.: while g\D/preposition (under), EFhX/pronoun (these) and AXB/auxiliary (has been) are stop-words, g\D/noun (floor), EFhX/noun (theses) and iXB/person (Bill) are not." ></td>
	<td class="line x" title="130:241	We did not try to address this problem (which would have required POS tagging and possibly morphological analysis, which is unacceptable, given our task) and we simply removed all homographs that matched a stoplist wordform." ></td>
	<td class="line x" title="131:241	We performed several experiments trying to assess the performance of the ending guessing rules as predictors for POS, article, gender, number and semantics." ></td>
	<td class="line x" title="132:241	The details follow." ></td>
	<td class="line x" title="133:241	5.1 POS We do a major distinction, between the following five open POS classes: A (adjective), ADV (adverb), N (noun), NU (numeral) and V (verb)." ></td>
	<td class="line x" title="134:241	Remember that we already excluded the auxiliary verbs, conjunctions, interjections, particles, prepositions and pronouns (and all their homographs)." ></td>
	<td class="line x" title="135:241	Some statistics are shown in Table 4 and the results of the evaluation are presented on Figure 1." ></td>
	<td class="line x" title="136:241	Note the differences in the distribution of the dictionary vs. text ending frequency estimations." ></td>
	<td class="line x" title="137:241	Note also how the results for training and testing using raw text lead to consistently lower performance." ></td>
	<td class="line x" title="138:241	The same observation can be made for the other kinds of predictions, see Figures 2-7." ></td>
	<td class="line x" title="139:241	Class Dictionary Text A 129,828 17.34% 217,035 17.33% ADV 661 0.09% 62,996 5.03% N 84,303 11.26% 646,890 51.65% NU 408 0.05% 12,112 0.97% V 533,453 71.26% 313,327 25.02% Table 4: Prior (training) distribution of POS." ></td>
	<td class="line x" title="140:241	55% 60% 65% 70% 75% 80% 85% 90% 95% 100% mik-0.50 mik-0.80 mik-0.90 mik-0.95 exact precision (dictionary) coverage (dictionary) F measure (dictionary) precision (text) coverage (text) F measure (text) Figure 1: Results for POS." ></td>
	<td class="line x" title="141:241	5.2 Article We learn rules to distinguish between three classes of articles: definite, indefinite and none." ></td>
	<td class="line x" title="142:241	Unlike English, the articles in Bulgarian 7 appear augmented at the end of one of the words in a noun phrase, typically the first one." ></td>
	<td class="line x" title="143:241	The feminine and neutre no7 Bulgarian and Macedonian are the only Slavonic languages with definite articles of this kind." ></td>
	<td class="line x" title="144:241	uns, adjectives, numerals and some verb forms (e.g. participles) have the same form for both definite and indefinite articles (e.g. defence: \EA]=@= ; \EA]=@=Ea/(in)def), while for masculine these are distinct (e.g. man: j\>Ff ; j\>Ff=/indef, j\>FfkE/def)." ></td>
	<td class="line x" title="145:241	We ran two experiments: with (see Table 5 and Figure 2) and without POS (see Table 6 and Figure 3)." ></td>
	<td class="line x" title="146:241	Note that we certainly need the none class in a real system so we had to include it." ></td>
	<td class="line x" title="147:241	Class Dictionary Text def 324,253 39.31% 393,658 28.01% indef 250,345 30.35% 703,116 50.02% none 250,345 30.35% 308,802 21.97% Table 5: Prior (training) distribution for article (no POS)." ></td>
	<td class="line x" title="148:241	Class Dictionary Text A+def 73,866 10.00% 96,909 7.89% A+indef 48,327 6.54% 116,282 9.47% N+def 43,426 5.88% 230,506 18.78% N+indef 40,343 5.46% 390,609 31.82% NU+def 229 0.03% 4,927 0.40% NU+indef 179 0.02% 7,185 0.59% V+def 142,500 19.28% 5,635 0.46% V+indef 137,692 18.63% 66,798 5.44% none 252,407 34.16% 308,802 25.15% Table 6: Prior (training) distribution of article (with POS)." ></td>
	<td class="line x" title="149:241	60% 65% 70% 75% 80% 85% 90% 95% 100% mik-0.50 mik-0.80 mik-0.90 mik-0.95 exact precision (dictionary) coverage (dictionary) F measure (dictionary) precision (text) coverage (text) F measure (text) Figure 2: Results for article (no POS)." ></td>
	<td class="line x" title="150:241	50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 100% mik-0.50 mik-0.80 mik-0.90 mik-0.95 exact precision (dictionary) coverage (dictionary) F measure (dictionary) precision (text) coverage (text) F measure (text) Figure 3: Results for article (with POS)." ></td>
	<td class="line x" title="151:241	5.3 Gender There are three genders in Bulgarian: masculine, feminine and neuter." ></td>
	<td class="line x" title="152:241	Only some of the word classes can have gender, namely: adjectives, nouns, numerals and some verb forms (e.g. participles)." ></td>
	<td class="line x" title="153:241	The results of the gender guessing experiments are shown in Tables 7, 8 and Figures 4, 5." ></td>
	<td class="line x" title="154:241	Class Dictionary Text Fem 112,201 13.65% 87,856 5.76% Mas 150,426 18.30% 110,361 7.24% Neu 121,134 14.73% 87,608 5.74% none 438,386 53.32% 1,239,183 81.26% Table 7: Prior (training) distribution of gender (no POS)." ></td>
	<td class="line x" title="155:241	Class Dictionary Text A+fem 30,465 3.96% 56,414 3.89% A+mas 38,490 5.00% 59,306 4.09% A+neu 25,258 3.28% 30,556 2.11% N+neu 276 0.04% 4,592 0.32% NU+fem 68 0.01% 2,862 0.20% NU+mas 161 0.02% 6,582 0.45% NU+neu 65 0.01% 1,139 0.08% V+fem 67,385 8.76% 12,326 0.85% V+mas 96,529 12.55% 28,660 1.97% V+neu 72,040 9.37% 9,671 0.67% none 438,386 57.00% 1,239,183 85.38% Table 8: Prior (training) distribution of gender (with POS)." ></td>
	<td class="line x" title="156:241	80% 85% 90% 95% 100% mik-0.50 mik-0.80 mik-0.90 mik-0.95 exact precision (dictionary) coverage (dictionary) F measure (dictionary) precision (text) coverage (text) F measure (text) Figure 4: Results for gender (no POS)." ></td>
	<td class="line x" title="157:241	75% 80% 85% 90% 95% 100% mik-0.50 mik-0.80 mik-0.90 mik-0.95 exact precision (dictionary) coverage (dictionary) F measure (dictionary) precision (text) coverage (text) F measure (text) Figure 5: Results for gender (with POS)." ></td>
	<td class="line x" title="158:241	5.4 Number There are two grammatical numbers in todays Bulgarian: singular and plural 8 . Again, only some of the word classes can have number, namely: adjectives, nouns, numerals and some verb forms (e.g. participles)." ></td>
	<td class="line x" title="159:241	Tables 9, 10 and Figures 6, 7 for the results of the number guessing experiments." ></td>
	<td class="line x" title="160:241	Class Dictionary Text Plural 146,592 17.49% 299,134 20.84% Singular 455,186 54.32% 827,131 57.62% none 236,260 28.19% 309,276 21.54% Table 9: Prior (training) distribution of number (no POS)." ></td>
	<td class="line x" title="161:241	Class Dictionary Text A+pl 29,281 3.92% 68,144 5.48% A+sg 100,535 13.44% 148,845 11.97% N+pl 31,766 4.25% 163,403 13.14% N+sg 46,317 6.19% 468,577 37.69% NU+pl 57 0.01% 69 0.01% NU+sg 197 0.03% 8,218 0.66% V+pl 67,557 9.03% 25,939 2.09% V+sg 235,896 31.54% 50,657 4.07% none 236,260 31.59% 309,276 24.88% Table 10: Prior (training) distribution of number (with POS)." ></td>
	<td class="line x" title="162:241	70% 75% 80% 85% 90% 95% 100% mik-0.50 mik-0.80 mik-0.90 mik-0.95 exact precision (dictionary) coverage (dictionary) F measure (dictionary) precision (text) coverage (text) F measure (text) Figure 6: Results for number (no POS)." ></td>
	<td class="line x" title="163:241	50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 100% mik-0.50 mik-0.80 mik-0.90 mik-0.95 exact precision (dictionary) coverage (dictionary) F measure (dictionary) precision (text) coverage (text) F measure (text) Figure 7: Results for number (with POS)." ></td>
	<td class="line x" title="164:241	8 The Old Bulgarian language used to have also a dual number." ></td>
	<td class="line x" title="165:241	The only Slavonic language this grammatical number has been preserved in is Slovenian." ></td>
	<td class="line x" title="166:241	5.5 Semantics The last kind of experiments we performed was recognising some kind of semantics." ></td>
	<td class="line x" title="167:241	We tried to guess whether a wordform is a human, animate or neither, as we had such information in our dictionary." ></td>
	<td class="line x" title="168:241	These are always limited to nouns (at least in our dictionary annotations), so we did not have separate experiments with and without POS (they would have produced almost the same result, except for some potential problems caused by homographs with a non-noun POS)." ></td>
	<td class="line x" title="169:241	The results are shown in Table 11 and Figure 8." ></td>
	<td class="line x" title="170:241	Class Dictionary Text Animate 1,765 0.21% 4,536 0.28% Human 26,918 3.14% 121,299 7.39% none 828,887 96.66% 1,516,053 92.34% Table 11: Training (prior) distribution of semantics." ></td>
	<td class="line x" title="171:241	80% 85% 90% 95% 100% mik-0.50 mik-0.80 mik-0.90 mik-0.95 exact precision (dictionary) coverage (dictionary) F measure (dictionary) precision (text) coverage (text) F measure (text) Figure 8: Results for semantics: human/animate." ></td>
	<td class="line x" title="172:241	75% 80% 85% 90% 95% 100% freq 1 freq 2 freq 3 freq 4 freq 5 freq 10 freq 20 precision (dictionary) coverage (dictionary) F measure (dictionary) Figure 9: Results for article: the exact algorithm for different thresholds." ></td>
	<td class="line x" title="173:241	Figures 1-8 show that the approximate rules with confidence score of 0.50 perform consistently better than the exact ones, where we keep every single rule, even the ones met only once." ></td>
	<td class="line x" title="174:241	So, we are very likely to over fit." ></td>
	<td class="line x" title="175:241	One way to prevent this is to ignore some of the least reliable rules." ></td>
	<td class="line x" title="176:241	The simplest criterion for this is the minimum frequency." ></td>
	<td class="line x" title="177:241	We performed some experiments, setting it to 1, 2, 3, 4, 5, 10 and 20." ></td>
	<td class="line x" title="178:241	The results are shown on Figure 9, where we can see that while gaining a little bit on recall, we lose a lot on precision." ></td>
	<td class="line x" title="179:241	Thus, if we stick to the exact rules, we apparently cannot gain by removing some of the rules based on frequency." ></td>
	<td class="line x" title="180:241	In fact, this is not necessarily true, as it could be possible when using more complex criterion that takes into account more than just frequency, e.g. rule length." ></td>
	<td class="line x" title="181:241	6 Discussion Table 12 contains summary results for the experiments with the exact and the approximate rules (with a threshold of 0.50, since, as Figures 1-8 show, it had the highest F-measure)." ></td>
	<td class="line x" title="182:241	The first two columns describe the kind of experiment and the method, followed by the precision, coverage and F-measure." ></td>
	<td class="line x" title="183:241	Finally, the last two columns show the corresponding number of rules used and the number of target classes." ></td>
	<td class="line x" title="184:241	Experiment Method P C F # rules # class article exact 98.61% 94.00% 96.25% 53,216 3 article mik-.50 97.02% 99.97% 98.47% 10,745 3 article+POS exact 97.01% 83.09% 89.51% 85,061 9 article+POS mik-.50 92.33% 99.84% 95.94% 27,263 9 gender exact 99.04% 93.88% 96.39% 39,309 4 gender mik-.50 97.43% 100.00% 98.70% 7,263 4 gender+POS exact 98.35% 87.45% 92.58% 53,385 11 gender+POS mik-.50 94.79% 99.81% 97.24% 12,473 11 number exact 99.21% 95.49% 97.31% 40,856 3 number mik-.50 97.90% 100.00% 98.94% 7,493 3 number+POS exact 97.60% 84.07% 90.33% 81,154 9 number+POS mik-.50 93.08% 99.92% 96.38% 20,144 9 POS exact 97.70% 84.23% 90.47% 79,609 5 POS mik-.50 93.23% 100.00% 96.50% 18,663 5 semantics exact 99.10% 97.13% 98.11% 43,902 3 semantics mik-.50 98.33% 99.99% 99.15% 9,971 3 Table 12: Experiments summary (dictionary)." ></td>
	<td class="line x" title="185:241	There are several interesting observations about Table 12 (and Figures 1-8)." ></td>
	<td class="line x" title="186:241	First, the precision of the exact rules is consistently higher than that of the approximate ones with a threshold of 0.50." ></td>
	<td class="line x" title="187:241	This is not surprising as the ending guessing rules produced by the exact method are guaranteed to be 100% correct on the training set (but not necessarily on the testing one, as we explained above)." ></td>
	<td class="line x" title="188:241	Figures 1-8 show that this observation holds for all other score thresholds considered, even for 0.95 (remember that the score reflects not only the rule accuracy but also its length and smoothed frequency)." ></td>
	<td class="line x" title="189:241	The situation is reversed with respect to the coverage: the exact rules have a lower coverage, which more than compensates for their higher precision." ></td>
	<td class="line x" title="190:241	As a result, the F-measure is consistently lower for the exact algorithm as compared to the approximate one with a threshold of 0.50." ></td>
	<td class="line x" title="191:241	Figures 1-8 show this is not the case for higher thresholds (especially 0.95) when the coverage becomes lower and the F-measure gets worse as compared to that of the exact method." ></td>
	<td class="line x" title="192:241	Comparing article, gender and number to article+POS, gender+POS and number+POS, accordingly, where the number of classes is increased by a factor of 3, we can see that the exact algorithm remains robust with respect to precision: there is a decrease of about 1-1.5% only." ></td>
	<td class="line x" title="193:241	The precision of the approximate rules is decreased by 3-4%." ></td>
	<td class="line x" title="194:241	On the other hand, the coverage of the approximate rules is virtually unaffected and stays very close to 100% (decreased by less than 0.2%), while for the exact rules it drops significantly: by 6-9%." ></td>
	<td class="line x" title="195:241	As a result, the approximate algorithm has a more robust F-measure, which drops by 1-2.5% only, while for the exact algorithm this is 4-7%." ></td>
	<td class="line x" title="196:241	The approximate method is also more robust with respect to the number of rules, as it produces about five times less of them as compared to the exact one." ></td>
	<td class="line x" title="197:241	When article, gender and number are combined with POS, the number of rules is increased by a factor of 2 to 3." ></td>
	<td class="line x" title="198:241	Overall, the approximate rules with a threshold of 0.50 exhibit a very high coverage (100% or very close) and precision/F-measure of about 97-99%." ></td>
	<td class="line x" title="199:241	Finally, the tasks are not equally hard." ></td>
	<td class="line x" title="200:241	The easiest one is semantics, and the hardest one is POS." ></td>
	<td class="line x" title="201:241	Class P R F A+fem 91.67% 87.58% 89.58% A+mas 92.21% 85.83% 88.91% A+neu 83.78% 85.44% 84.60% N+neu 16.24% 3.82% 6.18% NU+fem 44.44% 80.00% 57.14% NU+mas 85.71% 81.82% 83.72% NU+neu 85.71% 60.00% 70.59% V+fem 93.88% 97.73% 95.77% V+mas 93.10% 96.94% 94.98% V+neu 88.10% 96.79% 92.24% None 98.66% 96.49% 97.56% Table 13: Testing performance per class for gender+POS approximate rules 0.50 (dictionary)." ></td>
	<td class="line x" title="202:241	Class P R F A+fem 96.79% 96.96% 96.88% A+mas 97.04% 95.74% 96.39% A+neu 94.83% 95.03% 94.93% N+neu 21.74% 18.29% 19.87% NU+fem 80.00% 100.00% 88.89% NU+mas 94.74% 81.82% 87.80% NU+neu 85.71% 66.67% 75.00% V+fem 98.36% 99.12% 98.74% V+mas 98.41% 98.49% 98.45% V+neu 95.92% 97.42% 96.67% None 99.27% 99.01% 99.14% Table 14: Testing performance per class for gender+POS exact rules (dictionary)." ></td>
	<td class="line x" title="203:241	It is interesting to observe the performance of the different classes in a particular experiment, e.g. gender+POS." ></td>
	<td class="line x" title="204:241	Note that now we can calculate a true recall as opposed to coverage, as we can work with a particular class." ></td>
	<td class="line x" title="205:241	The results for the gender+POS, dictionary trained, experiments are shown in Tables 13 and 14." ></td>
	<td class="line x" title="206:241	We can see that the precision, the recall and the F-measure of the exact rules are consistently better for each class as compared to the ones obtained using approximate rules (with threshold of 0.50)." ></td>
	<td class="line x" title="207:241	Note however that the recall here is calculated only for the part for which there was a prediction." ></td>
	<td class="line x" title="208:241	The exact rules covered 84,512 out of all 96,643 wordforms (coverage: 87.45%) and 83,120 of them were correct (precision: 98.35%)." ></td>
	<td class="line x" title="209:241	The per-class P, R and F are calculated only for those 84,512 wordforms for which a prediction has been made." ></td>
	<td class="line x" title="210:241	I.e. we did not assign the non-covered wordforms the class none by default, although probably we should, as it is the most frequent one." ></td>
	<td class="line x" title="211:241	The approximate rules made predictions for 96,458 wordforms (coverage: 99.81%) 91,430 of which were correct (precision: 94.79%)." ></td>
	<td class="line x" title="212:241	Table 15 shows the performance for the approximate rules as evaluated on the training set 9 . Out of the 867,567 wordforms, 866,786 have been covered (coverage 99.91%), 835,330 of which correctly (precision 96.37%)." ></td>
	<td class="line x" title="213:241	We see that the class N+neu was hard to predict not only on testing but also on training." ></td>
	<td class="line x" title="214:241	Class P R F A+fem 95.60% 91.35% 93.43% A+mas 96.36% 90.64% 93.41% A+neu 87.26% 90.41% 88.81% N+neu 83.67% 8.50% 15.44% NU+fem 98.53% 83.75% 90.54% NU+mas 90.96% 89.44% 90.20% NU+neu 98.48% 89.04% 93.53% V+fem 95.91% 98.59% 97.23% V+mas 94.58% 98.49% 96.50% V+neu 89.21% 99.10% 93.90% none 99.53% 97.29% 98.40% Table 15: Training performance per class for gender+POS approximate rules 0.50 (dictionary)." ></td>
	<td class="line x" title="215:241	Something that Table 12 does not show, but one can see on Figures 1-8, is the consistently worse performance of training & testing on the dictionary vs. training & testing only on these dictionary words that are met in the raw text, using the corresponding frequencies." ></td>
	<td class="line x" title="216:241	The major reason for this is 9 We do not show a corresponding training accuracy table for the exact rules as every cell there is replaced with 100%, i.e. there is a perfect fit." ></td>
	<td class="line x" title="217:241	the insufficient amount of training text." ></td>
	<td class="line x" title="218:241	While the number of word tokens is high, the number of word types is much less than that of the dictionary." ></td>
	<td class="line x" title="219:241	So, the significantly lower variability of wordforms more than compensates any gains of having real word frequencies." ></td>
	<td class="line x" title="220:241	We believe weighting through real text is important and we plan to re-run these experiments with word frequencies estimated from orders of magnitude more textual data (it is cheap and freely available on the Web)." ></td>
	<td class="line x" title="221:241	Another, less attractive alternative could be to add the dictionary as a text." ></td>
	<td class="line x" title="222:241	That way we would have incorrect frequency estimations for some of the words, but also the learning algorithm would have access to the rich word variability of the dictionary words." ></td>
	<td class="line x" title="223:241	7 Future Work There are several possible extensions to the work presented above." ></td>
	<td class="line x" title="224:241	First, the exact algorithm can be extended with non-exact rules." ></td>
	<td class="line x" title="225:241	Second, the Mikheev-like ending guessing rules construction could be augmented with a merging phase as originally proposed." ></td>
	<td class="line x" title="226:241	It would be interesting to consider using the dictionary not only during rules generation but also during their application: e.g. try to add/remove suffixes/prefixes and check whether the newly obtained word is listed in the dictionary (e.g. we might have the word @=ABCD=EFB/noun (observer) but not @=ABCD=EFB(!/adj (observing), generated following a standard derivational rule)." ></td>
	<td class="line x" title="227:241	There are prefixes, mostly foreign, that can attach to any open class word, but the resulting words are unlikely to be in our dictionary: =@EX- (anti-), lBE]=- (ultra-), mlgF]- (super-), f\@E]=- (contra-)." ></td>
	<td class="line x" title="228:241	Furthermore, there are some important prefixes, specific to Bulgarian, that can limit the possible POS: e.g. g\- and @=n- (- is part of the prefix) are used to construct a comparative and a superlative form, accordingly, and can be used only with adjectives, adverbs and some verb forms (e.g. participle)." ></td>
	<td class="line x" title="229:241	We believe in the potential of the combined evidence from both prefixes and suffixes." ></td>
	<td class="line x" title="230:241	In addition, it seems important to allow for mutations in the word stem as these are common in Slavonic languages." ></td>
	<td class="line x" title="231:241	Finally, it might be beneficial to learn separate rules for capitalised and dashed words (but maybe it is not that important as their usage is less frequent, especially the capitalisation)." ></td>
	<td class="line x" title="232:241	We would like to try other scoring and smoothing approaches." ></td>
	<td class="line x" title="233:241	We did not address the problem of selecting the best threshold (although it is clear that it should be low, maybe around 0.50)." ></td>
	<td class="line x" title="234:241	One way to do this is to split the training set into rulestraining and threshold-training sets." ></td>
	<td class="line x" title="235:241	Next, it looks promising to try to estimate the dictionary word frequencies using a search engine instead of text corpus, as proposed by Lapata and Keller (2004)." ></td>
	<td class="line x" title="236:241	While the exact algorithm performed worse due to insufficient coverage 10, we believe it has a potential, e.g. if extended with some approximate rules." ></td>
	<td class="line x" title="237:241	Note that the way the exact rules were built is very similar to the standard algorithm for decision tree construction." ></td>
	<td class="line x" title="238:241	Thus the corresponding tree cutting criteria used to prevent over fitting can help decide when to go further and look at longer endings and when to stop." ></td>
	<td class="line x" title="239:241	It is interesting to see how the proposed rules perform for other Slavonic languages." ></td>
	<td class="line x" title="240:241	In particular, we plan similar experiments for Russian as a comparable morphological dictionary with the same kind of linguistic annotations is already available." ></td>
	<td class="line x" title="241:241	Finally, we would like to explore the machine learning potential offered by morphological dictionaries with application to other related tasks such as stemming (Nakov, 2003), lemmatisation and POS tagging." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W04-2602
Towards Full Automation Of Lexicon Construction
Rohwer, Richard;Freitag, Dayne;"></td>
	<td class="line x" title="1:214	Towards Full Automation of Lexicon Construction Richard Rohwer Fair Isaac Corporation RichardRohwer@fairisaac.com Dayne Freitag Fair Isaac Corporation DayneFreitag@fairisaac.com Abstract We describe work in progress aimed at developing methods for automatically constructing a lexicon using only statistical data derived from analysis of corpora, a problem we call lexical optimization." ></td>
	<td class="line x" title="2:214	Speci cally, we use statistical methods alone to obtain information equivalent to syntactic categories, and to discover the semantically meaningful units of text, which may be multi-word units or polysemous terms-incontext." ></td>
	<td class="line x" title="3:214	Our guiding principle is to employ a notion of meaningfulness that can be quantied information-theoretically, so that plausible variants of a lexicon can be judged relative to each other." ></td>
	<td class="line x" title="4:214	We describe a technique of this nature called information theoretic co-clustering and give results of a series of experiments built around it that demonstrate the main ingredients of lexical optimization." ></td>
	<td class="line x" title="5:214	We conclude by describing our plans for further improvements, and for applying the same mathematical principles to other problems in natural language processing." ></td>
	<td class="line x" title="6:214	1 Introduction A lexicon is a key resource for natural language processing, providing the link between the terms of a language and the semantic and syntactic properties with which they are associated." ></td>
	<td class="line x" title="7:214	Like most resources of considerable value, a good lexicon can be dif cult or expensive to obtain." ></td>
	<td class="line x" title="8:214	This is particularly true if the lexicon needs to be specialized to a technical subject, an obscure language or dialect, or a highly idiomatic writing style." ></td>
	<td class="line x" title="9:214	Motivated by the practical importance of these cases as well as the theoretical interest inherent to the problem, we have set out to develop methods for building a lexicon automatically, given only a corpus of text representative of the domain of interest." ></td>
	<td class="line x" title="10:214	We represent the semantics of a term by an associated probability distribution over what we call a grounding space, which we de ne in various relatively conventional ways involving terms that occur in text in the vicinity of the term in question." ></td>
	<td class="line x" title="11:214	It is well-known that such distributions can represent meaning reasonably well, at least for meaning-comparison purposes (Landauer and Dumais, 1997)." ></td>
	<td class="line x" title="12:214	We add to this framework the notion that the more information such a distributional lexicon can capture, the more useful it is. This provides us with a mathematical concept of lexical optimization." ></td>
	<td class="line x" title="13:214	We begin the lexicon construction process by applying a distributional clustering technique called information theoretic co-clustering to make a rst pass at grouping the most frequent terms in the corpus according to their most common syntactic part of speech category, as described in Section 2 along with illustrative results." ></td>
	<td class="line x" title="14:214	We brie y describe the co-clustering algorithm in Section 2.1." ></td>
	<td class="line x" title="15:214	In Section 3.1, we show that novel terms can be sensibly assigned to previously de ned clusters using the same information theoretic criterion that the co-clustering uses." ></td>
	<td class="line x" title="16:214	Even though term clustering crudely ignores the fact that a terms part of speech generally varies with its context, it is clear from inspection that the clusters themselves correspond to corpus-adapted part-of-speech categories, and can be used as such." ></td>
	<td class="line x" title="17:214	In Section 3.2, we examine two approaches to incorporating context information." ></td>
	<td class="line x" title="18:214	The most direct is to partition the contexts in which a term occurs into classes according to the informatic criterion used in co-clustering, creating sense-disambiguated word-with-context-class pseudo-terms." ></td>
	<td class="line x" title="19:214	We also discuss the use of Hidden Markov Models (HMMs) to capture contextual information." ></td>
	<td class="line x" title="20:214	In Section 3.3 we apply the same principle in reverse to nd multi-word units." ></td>
	<td class="line x" title="21:214	We conclude in Section 3.5 with a discussion of possible improvements to our approach, and possible extensions of it." ></td>
	<td class="line x" title="22:214	2 Co-clustering to de ne surrogate syntactic tags Many applications of text processing rely on or bene t from information regarding the parts of speech of individual terms." ></td>
	<td class="line x" title="23:214	While part of speech is a somewhat uid notion, the computational linguistics community has converged on a handful of standard tag sets, and taggers are now available in a number of languages." ></td>
	<td class="line x" title="24:214	Since some high-quality taggers are in the public domain, any application that could bene t from part-of-speech information should have access to it." ></td>
	<td class="line x" title="25:214	However, using a speci c tagger and its tag set entails adopting the assumptions it embodies, which may not be appropriate for the target application." ></td>
	<td class="line x" title="26:214	In the worst case, the domain of interest may include text in a language not covered by available taggers." ></td>
	<td class="line x" title="27:214	Even when a tagger is available, the domain may involve usages substantially different from those in the corpus for which the tagger was developed." ></td>
	<td class="line x" title="28:214	Many current taggers are tuned to relatively formal corpora, such as newswire, while many interesting domains, such as email, netnews, or physicians notes, are replete with elisions, jargon, and neologisms." ></td>
	<td class="line x" title="29:214	Fortunately, using distributional characteristics of term contexts, it is feasible to induce part-of-speech categories directly from a corpus of suf cient size, as several papers have made clear (Brown et al. , 1992; Schcurrency1utze, 1993; Clark, 2000)." ></td>
	<td class="line x" title="30:214	Distributional information has uses beyond part of speech induction." ></td>
	<td class="line x" title="31:214	For example, it is possible to augment a xed syntactic or semantic taxonomy with such information to good effect (Hearst and Schcurrency1utze, 1993)." ></td>
	<td class="line x" title="32:214	Our objective is, where possible, to work directly with the inferred syntactic categories and their underlying distributions." ></td>
	<td class="line x" title="33:214	There are many applications of computational linguistics, particularly those involving shallow processing, such as information extraction, which can bene t from such automatically derived information, especially as research into acquisition of grammar matures (e.g. , (Clark, 2001))." ></td>
	<td class="line x" title="34:214	2.1 The Co-clustering Algorithm." ></td>
	<td class="line x" title="35:214	Our approach to inducing syntactic clusters is closely related to that described in Brown, et al, (1992) which is one of the earliest papers on the subject." ></td>
	<td class="line x" title="36:214	We seek to nd a partition of the vocabulary that maximizes the mutual information between term categories and their contexts." ></td>
	<td class="line x" title="37:214	We achieve this in the framework of information theoretic co-clustering (Dhillon et al. , 2003), in which a space of entities, on the one hand, and their contexts, on the other, are alternately clustered in a way that maximizes mutual information between the two spaces." ></td>
	<td class="line x" title="38:214	By treating the space of terms and the space of contexts as separate, we part ways with Brown, et al. This allows us to experiment with the notion of context, as well as to investigate whether pooling contexts is useful, as has been assumed." ></td>
	<td class="line x" title="39:214	2.2 De nitions Given a corpus, and some notion of term and context, we derive co-occurrence statistics." ></td>
	<td class="line x" title="40:214	More formally, the input to our algorithm is two nite sets of symbols, say a0a2a1a4a3a6a5a8a7a10a9a11a5a13a12a14a9a16a15a17a15a17a15a18a9a19a5a21a20a23a22a25a24 and a26 a1a27a3a29a28a30a7a30a9a19a28a31a12a32a9a29a15a18a15a17a15a18a9a19a28a30a20a23a33a34a24, together with a set of co-occurrence count data consisting of a non-negative integer a35a37a36a29a38a40a39a42a41 for every pair of symbols a43 a5a21a44a19a9a11a28a46a45a6a47, that can be drawn from a0 and a26 . The output is two sets of sets: a0a49a48a50a1a51a3a6a5a13a48a7 a9a16a15a17a15a18a15a17a9a11a5a52a48a20 a22a54a53 a24 and a26 a48 a1a55a3a29a28 a48 a7 a9a29a15a18a15a17a15a17a9a11a28 a48 a20 a33a8a53 a24, where each a5 a48 a44 is a subset of a0 (a cluster ), none of the a5 a48a44 intersect each other, the union of all the a5 a48a44 is a0 (similar remarks apply to the a28 a48a45 and a26 )." ></td>
	<td class="line x" title="41:214	The co-clustering algorithm chooses the partitions a0 a48 and a26 a48 to (locally) maximize the expected mutual information between them." ></td>
	<td class="line x" title="42:214	The multinomial parameters a56a13a36a29a39 of a joint distribution over a0 and a26 may be estimated from this co-occurrence data as a56a21a36a6a39 a1 a35a54a36a6a39a30a57a34a58 a36a30a59a39 a35a54a36a29a39, using the naive maximum likelihood method." ></td>
	<td class="line x" title="43:214	We follow a more fully Bayesian procedure to obtain pseudo-counts a60a54a36a6a39 that are added to the counts a35a54a36a6a39 to obtain smoothed estimates." ></td>
	<td class="line x" title="44:214	Due to space limitations, we de ne but do not fully discuss our procedure here." ></td>
	<td class="line x" title="45:214	We apply the Evidence method in the Dice Factory setting of MacKay and Peto (1994), to obtain a pseudo-count a60 a36a30a61 for every symbol a5a63a62a64a0 by treating each a28a4a62 a26 as a sample of (not from) a random process a65a66a43 a5a68a67a28a69a47, in a Multinomial/Dirichlet setting." ></td>
	<td class="line x" title="46:214	By a symmetric procedure, we also obtain pseudocounts a60a37a61a39 for each a28a70a62 a26 . These are combined according to a60a13a36a6a39 a1a72a71 a73 a43a74a60a76a75a78a77a79a60a13a80 a47 a43a74a60a76a36a30a61a81a60a82a61a39 a47 a57a69a43a40a60a13a75a25a60a52a80 a47, and then the totals a35a54a36a6a39a66a77a79a60a13a36a29a39 are rescaled by a35a37a57a83a43a74a60a84a77a64a35 a47, where a60a13a75 a1 a58 a36 a60a13a36a14a61, a60a52a80 a1 a58 a39 a60a82a61a39, a60 a1 a58 a36a29a39 a60a76a36a6a39, and a35 a1 a58 a36a29a39 a35a54a36a6a39 . The entropy or Shannon information of a discrete distribution is: a85 a75 a1a87a86a89a88 a36a91a90 a43 a5a76a47a93a92a17a94 a90 a43 a5a76a47a95a15 (1) This quanti es average improvement in ones knowledge upon learning the speci c value of an event drawn from a0 . It is large or small depending on whether a0 has many or few probable values." ></td>
	<td class="line x" title="47:214	The mutual information between random variables a0 and a26 can be written: a96 a75a97a80 a1a98a88 a36a29a39 a90 a43 a5a82a9a11a28a69a47a93a92a17a94 a90 a43 a5a37a9a11a28a69a47 a90 a43 a5a76a47 a90 a43 a28a69a47 (2) This quanti es the amount that one expects to learn indirectly about a0 upon learning the value of a26, or vice versa." ></td>
	<td class="line x" title="48:214	The following relationship holds between the information of a joint distribution and the information of the marginals and mutual information: a85 a75a97a80 a1a99a85 a75a100a77 a85 a80 a86a101a96 a75a102a80 (3) From this we see that the expected amount one can learn upon hearing of a joint event a43 a5a37a9a11a28a69a47 is bounded by what one can learn about a0 and a26 separately." ></td>
	<td class="line x" title="49:214	Combined with another elementary result, a103 a75a78a104a106a105a107a75a97a80a108a104a108a109 and symmetrically a103a16a80 a104a64a105 a75a102a80 a104a79a109, we see that a joint event a43 a5a82a9a11a28a69a47 yields at least as much information as either event alone, and that one cannot learn more about an event a28 from a26 by hearing about an event a5 from a0 than one would know by hearing about a28 explicitly." ></td>
	<td class="line x" title="50:214	2.3 The Algorithm The co-clustering algorithm seeks partitions a0 a48 of a0 and a26 a48 of a26 with maximal mutual information a105 a75 a53 a80 a53, under a constraint limiting the total number of clusters in each partition." ></td>
	<td class="line x" title="51:214	The mutual information is computed from the distributions estimated as discussed in Section 2.2, by summing a65a66a43 a5a37a9a11a28a69a47 over the elements within each cluster to obtain a65a66a43 a5 a48 a9a19a28 a48 a47 . We perform an approximate maximization of a105a110a75 a53 a80 a53 using a simulated annealing procedure in which each trial move takes a symbol a5 or a28 out of the cluster to which it is tentatively assigned and places it into another." ></td>
	<td class="line x" title="52:214	It is straightforward to obtain a formula for the change in a105 a75 a53 a80 a53 under this operation that does not involve its complete re-computation." ></td>
	<td class="line x" title="53:214	We use an ad hoc adaptive cooling schedule that seeks to continuously reduce the rejection rate of trial moves from an initial level near 50%, staying at each target rejection rate long enough to visit a xed fraction of the possible moves with high probability." ></td>
	<td class="line x" title="54:214	After achieving one rejection rate target for the required number of moves, the target is lowered." ></td>
	<td class="line x" title="55:214	The temperature is also lowered, but will be raised again to an intermediate value if the resulting rejection rate is below the next target, or lowered further if the rejection rate remains above the next target." ></td>
	<td class="line x" title="56:214	Candidate moves are chosen by selecting a non-empty cluster uniformly at random, randomly selecting one of its members, then randomly selecting a destination cluster other than the source cluster." ></td>
	<td class="line x" title="57:214	When temperature 0 is reached, all possible moves are repeatedly attempted until no move leads to an increase in the objective function." ></td>
	<td class="line x" title="58:214	2.4 Co-clustering for Term Categorization Applying co-clustering to the problem of part of speech induction is straightforward." ></td>
	<td class="line x" title="59:214	We de ne a0 to be the space of terms under some tokenization of the corpus, and a26 to be the space of contexts of those terms, which are a function of the close neighborhood of occurrences from a0 . Members of a26 are also typically terms, but we have also Experiment Time No Conj." ></td>
	<td class="line x" title="60:214	Clusters 74:17:31 Conj." ></td>
	<td class="line x" title="61:214	Clusters 12:07:43 Table 1: Time to complete clustering, with and without conjugate clusters in hours:minutes:seconds." ></td>
	<td class="line x" title="62:214	experimented with concatenations of terms, and more complex de nitions based on relative position." ></td>
	<td class="line x" title="63:214	The results reported here are based on the simple context de nition of one term to the left and one to the right, regarded as separate events." ></td>
	<td class="line x" title="64:214	Given a particular tokenization and method for de ning context, we can derive input for the co-clustering algorithm." ></td>
	<td class="line x" title="65:214	Sparse co-occurrence tables are created for each term of interest; each entry in such a table records a context identi er and the number of times the corresponding context occurred with the reference term." ></td>
	<td class="line x" title="66:214	For expediency, and to avoid problems with sparse statistics, we retain only the most frequent terms and contexts." ></td>
	<td class="line x" title="67:214	(We chose the top 5000 of each)." ></td>
	<td class="line x" title="68:214	In Section 3.1, we show that we can overcome this limitation through subsequent processing." ></td>
	<td class="line x" title="69:214	2.5 Experimental details and results We conducted experiments with the Reuters-21578 corpus a relatively tiny one for such experiments." ></td>
	<td class="line x" title="70:214	Clark (2000) reports results on a corpus containing 12 million terms, Schcurrency1utze (1993) on one containing 25 million terms, and Brown, et al, (1992) on one containing 365 million terms." ></td>
	<td class="line x" title="71:214	In contrast, we count approximately 2.8 million terms in Reuters-21578." ></td>
	<td class="line x" title="72:214	Only the bodies of articles in the corpus were considered." ></td>
	<td class="line x" title="73:214	Each such article was segmented into paragraphs, but not sentences." ></td>
	<td class="line x" title="74:214	Paragraphs were then converted into token arrays, with each token corresponding to one of the following: an unbroken string of alphabetic characters or hyphens, possibly terminated by an apostrophe and additional alphabetic characters; a numeric expression; or a single occurrence and unit of punctuation presumed to be syntactically signi cant (e.g. , periods, commas, and question marks)." ></td>
	<td class="line x" title="75:214	Alphabetic tokens were casenormalized, and all numeric expressions were replaced with the special token <num>." ></td>
	<td class="line x" title="76:214	For the purposes of constructing context distributions, special contexts (<bop> and <eop>) were inserted at the beginnings and endings of each such array." ></td>
	<td class="line x" title="77:214	We applied the co-clustering algorithm to the most frequent 5000 terms and most frequent 5000 contexts in the corpus, clustering each into 200 categories." ></td>
	<td class="line x" title="78:214	Co-clustering alternately clustering terms and contexts is faster than simple clustering against the full set of contexts." ></td>
	<td class="line x" title="79:214	Table 1 presents computation times for experiments with one grounding space on the same Clust." ></td>
	<td class="line x" title="80:214	Terms 37 may employs 71 because out ahead comprised consists  96 he she tzwater mulford azpurua  145 reported announced showed follows owns  159 set available used asked given paid taken  161 are were am 179 operations funds gurers results issues  180 on until upon regarding governing 186 business investment development sugar  194 to 195 of 199 the japans todays brazils canadas  Table 2: Selected clusters from experiment on the full corpus." ></td>
	<td class="line x" title="81:214	Clusters are ordered according to their impact on mutual information, least to greatest ascending." ></td>
	<td class="line x" title="82:214	Within each cluster, terms are ordered most frequent to least." ></td>
	<td class="line x" title="83:214	machine under similar loads." ></td>
	<td class="line x" title="84:214	While the exact time to completion is a function of particularities such as machine speed, cluster count, and annealing schedule, the relative durations (co-clustering nishes in 1/6 the time) are representative." ></td>
	<td class="line x" title="85:214	This may be counter-intuitive, since co-clustering involves two parallel clustering runs, instead of a single one." ></td>
	<td class="line x" title="86:214	However, the savings in the time it takes to compute the objective function (in this case, mutual information with 200 contexts, instead of 5000) typically more than compensates for the additional algorithmic steps." ></td>
	<td class="line x" title="87:214	Table 2 lists clusters that illustrate both strengths and weaknesses of our approach." ></td>
	<td class="line x" title="88:214	While many of the clusters correspond unambiguously to some part of speech, we can identify four phenomena that sometimes prevent the clusters from corresponding to unique part-of-speech tags: 1." ></td>
	<td class="line x" title="89:214	Lack of distributional evidence." ></td>
	<td class="line x" title="90:214	In several cases, the grounding space chosen provides no evidence for a distinction made by the tagger." ></td>
	<td class="line x" title="91:214	Examples of this are cluster 199, where the is equated with the possessive form of many nouns; cluster 145, where present tense and past tense verbs are both represented; and cluster 96, where personal pronouns are equated with surnames.1 2." ></td>
	<td class="line x" title="92:214	Predominant idioms and contexts." ></td>
	<td class="line x" title="93:214	If a term is used predominantly in a particular idiom, then the context supplied by that idiom may have the strongest in uence on its cluster assignment, occa1Far from a bad thing, however, this last identi cation suggests some avenues for research in unsupervised pronominal reference resolution." ></td>
	<td class="line x" title="94:214	sionally leading to counter-intuitive clusters." ></td>
	<td class="line x" title="95:214	An obvious example of this is cluster 71." ></td>
	<td class="line x" title="96:214	All of the terms in this cluster are typically followed by the context of." ></td>
	<td class="line x" title="97:214	3." ></td>
	<td class="line x" title="98:214	Lexical ambiguity." ></td>
	<td class="line x" title="99:214	If a term has two or more frequent syntactic categories, the algorithm assigns it (in the best case) to a cluster corresponding to its more frequent sense, or (in the worst case) to a junk or singleton cluster." ></td>
	<td class="line x" title="100:214	This happens with the word may (cluster 37, above) in all our experiments." ></td>
	<td class="line x" title="101:214	4." ></td>
	<td class="line x" title="102:214	Multi-token lexemes." ></td>
	<td class="line x" title="103:214	In order to tally context distributions, we must commit to an initial xed segmentation of the corpus." ></td>
	<td class="line x" title="104:214	While English orthography insures that this is not dif cult, there exist nevertheless xed collocations (commonly called multi-word units, MWUs), such as New York, which inject statistical noise under the default segmentation." ></td>
	<td class="line x" title="105:214	Of these four problems, the last two are probably more serious, since they give rise to specious distinctions." ></td>
	<td class="line x" title="106:214	Depending on the application, problems 1 and 2 may not be problems at all." ></td>
	<td class="line x" title="107:214	In this corpus, for example, the term regarding (cluster 180) may never be used in any but a quasi-prepositional sense." ></td>
	<td class="line x" title="108:214	And proper nouns in the possessive arguably do share a syntactic function with the." ></td>
	<td class="line x" title="109:214	3 Re nements Lexical categorizations, such as those provided by a part of speech tagger or a semantic resource like Wordnet, are usually a means to an end, almost never applications in their own right." ></td>
	<td class="line x" title="110:214	While it is interesting to measure how faithfully an unsupervised algorithm can reconstruct prior categories, we neither expect to achieve anything like perfect performance on this task, nor believe that it is necessary to do so." ></td>
	<td class="line x" title="111:214	In fact, adherence to a speci c tag set can be seen as an impediment, inasmuch as it introduces brittleness and susceptibility to noise in categorization." ></td>
	<td class="line x" title="112:214	It is nevertheless interesting to ignore the confounding factors enumerated in Section 2.5 and measure the agreement between term categories induced by co-clustering and the tags assigned by a tagger." ></td>
	<td class="line x" title="113:214	Using the tagger from The XTag Project (Project, 2003), we measured the agreement between our clusters and the tagger output over the terms used in clustering." ></td>
	<td class="line x" title="114:214	We found that the clusters captured 85% of the information in the tagged text (the tagged data had an entropy of 2.68, while mutual information between clusters and tags is 2.23)." ></td>
	<td class="line x" title="115:214	In a theoretical optimal classi er, this yields a ninefold increase in accuracy over the default rule of always choosing the most frequent tag." ></td>
	<td class="line x" title="116:214	In order to make our distributional lexicon useful, however, we need to extend its reach beyond the few thousand most frequent terms, on the one hand, and adjust for phenomena that lead to sub-optimal performance, on the other." ></td>
	<td class="line x" title="117:214	We call the process of expanding and adjusting the lexicon after its initial creation lexicon optimization." ></td>
	<td class="line x" title="118:214	3.1 Increasing Lexicon Coverage For tractability, the initial classes are induced using only the most frequent terms in a corpus." ></td>
	<td class="line x" title="119:214	(While we cluster using only the 5000 most frequent terms, the corpus contains approximately 41,000 distinct word-forms)." ></td>
	<td class="line x" title="120:214	This yields consistent results and broad coverage of the corpus, but leaves us unable to categorize about 5% of tokens." ></td>
	<td class="line x" title="121:214	Clearly, in order for our automatically constructed resource to be useful, we must introduce these uncovered terms into the lexicon, or better still, nd a way to apply it to individual novel tokens." ></td>
	<td class="line x" title="122:214	3.1.1 HMM tagging In light of the current state of the art in part of speech tagging, the occurrence of these unknown terms does not pose a signi cant problem." ></td>
	<td class="line pc" title="123:214	It has been known for some years that good performance can be realized with partial tagging and a hidden Markov model (Cutting et al. , 1992)." ></td>
	<td class="line o" title="124:214	Note that the notion of partial tagging described in Cutting, et al, is essentially different from what we consider here." ></td>
	<td class="line o" title="125:214	Whereas they assume a lexicon which, for every term in the vocabulary, lists its possible parts of speech, we construct a lexicon which imposes a single sense (or a few senses; see Section 3.2) on each of the few thousand most frequent terms, but provides no information about other terms." ></td>
	<td class="line o" title="126:214	As in Cutting, et al, however, we can use BaumWelch re-estimation to extract information from novel terms, and apply the Viterbi algorithm to dispose of a particular occurrence." ></td>
	<td class="line x" title="127:214	While the literature suggests that Baum-Welch training can degrade performance on the tagging task (Elworthy, 1994; Merialdo, 1994), we have found in early experiments that agreement between a tagger trained in this way and the tagger from the XTag Project consistently increases with each iteration of Baum-Welch, eventually reaching a plateau, but not decreasing." ></td>
	<td class="line x" title="128:214	We attribute this discrepancy to the different structure of our problem." ></td>
	<td class="line x" title="129:214	3.1.2 Lexicon expansion Note that a HMM is under no constraint to handle a given term in a consistent fashion." ></td>
	<td class="line x" title="130:214	A single model can and often does assign a single term to multiple classes, even in a single document." ></td>
	<td class="line x" title="131:214	When a term is suf ciently frequent, a more robust approach may be to assign it to a category using only its summary co-occurrence statistics." ></td>
	<td class="line x" title="132:214	The idea is straightforward: Create an entry in the lexicon for the novel term and measure the change in mutual information associated with assigning it to each of the Term Freq." ></td>
	<td class="line x" title="133:214	Cluster Example Terms weizsaecker 30 baker morgan shearson provoke 20 take buy make glut 10 price level volume councils 5 prices markets operations stockbuilding 3 earnings income profits ammonia 2 energy computer petroleum unwise 2 expected likely scheduled Table 3: Assigning novel terms to clusters using the mutual information objective function." ></td>
	<td class="line x" title="134:214	Each row shows a term not present in the initial clustering, its corpus frequency, and example terms from the cluster to which it is assigned." ></td>
	<td class="line x" title="135:214	available categories." ></td>
	<td class="line x" title="136:214	Assign it to the category for which this change is maximized." ></td>
	<td class="line x" title="137:214	As Table 3 demonstrates, this procedure works surprisingly well, even for words with low corpus frequencies." ></td>
	<td class="line x" title="138:214	Of course, as frequencies are reduced, the likelihood of making a sub-optimal assignment increases." ></td>
	<td class="line x" title="139:214	At some point, the decision is better made on an individual basis, by a classi er trained to account for the larger context in which a novel term occurs, such as an HMM." ></td>
	<td class="line x" title="140:214	We are currently investigating how to strike this trade-off, in a way that best exploits the two available techniques for accommodating novel tokens." ></td>
	<td class="line x" title="141:214	Lexical ambiguity (or polysemy) and xed collocations (multi-word units) are two phenomena which clearly lead to sub-optimal clusters." ></td>
	<td class="line x" title="142:214	We have achieved promising results resolving these problems while remaining within the co-clustering framework." ></td>
	<td class="line x" title="143:214	The basic idea is as follows: If by treating a term as two distinct lexemes (or, respectively, a pair of commonly adjacent terms as a distinct lexeme), we can realize an increase in mutual information, then the term is lexically ambiguous (respectively, a xed collocation)." ></td>
	<td class="line x" title="144:214	In the case of polysemy resolution, this involves factoring the context distribution into two or more clusters." ></td>
	<td class="line x" title="145:214	In the case of a xed collocation, we consider the effect of treating an n-gram as a lexical unit." ></td>
	<td class="line x" title="146:214	3.2 Polysemy Resolution To determine whether a term is polysemous we must determine whether the lexicons mutual information can be increased by treating the term as two distinct lexemes." ></td>
	<td class="line x" title="147:214	Given a particular term, we make this determination by attempting to factor its context distribution into every possible pair of distinct clusters.2 Faced with a candidate pair, we posit two senses of the target term, one in each 2In this discussion, we assume exactly two senses, but the approach is easily extended to handle more than two." ></td>
	<td class="line x" title="148:214	Term a111 MI Cluster Example Terms april march junemay 8.75e-5 would could should continue remain comeact 6.51e-5 board committee court continue remain comevote 4.32e-5 meeting report japan canada brazilfrance -1.2e-6 and willwould -0.0008 would could should Table 4: The result of polysemy resolution run on some representative terms." ></td>
	<td class="line x" title="149:214	The third column lists sample terms from the two clusters into which each term is divided." ></td>
	<td class="line x" title="150:214	cluster." ></td>
	<td class="line x" title="151:214	The probability mass associated with each event type in the terms context distribution is then assigned to one or the other hypothetical sense, always to the one that improves mutual information the most (or hurts it the least)." ></td>
	<td class="line x" title="152:214	Once the probability mass of the original term has been re-apportioned in this way, the resulting change in mutual information re ects the quality of the hypothetical sense division." ></td>
	<td class="line x" title="153:214	The maximum change in mutual information over all such cluster pairs is then taken to be the polysemy score for the target term." ></td>
	<td class="line x" title="154:214	Table 4 shows how this procedure handles selected terms from the Reuters corpus." ></td>
	<td class="line x" title="155:214	Positive changes in mutual information clearly correspond to polysemy in the target term." ></td>
	<td class="line x" title="156:214	In the Reuters corpus, there are a fair number of terms that have a noun and a verb sense, such as act and vote in the table." ></td>
	<td class="line x" title="157:214	Note, too, the result of polysemy resolution run on unambiguous terms either a nonsensical division, as with france, or division into two closely related clusters, in both cases, however, with a decrease in mutual information." ></td>
	<td class="line x" title="158:214	Note that the problem of lexical ambiguity has been studied elsewhere." ></td>
	<td class="line x" title="159:214	Schcurrency1utze (1993; 1995) proposes two distinct methods by which ambiguity may be resolved." ></td>
	<td class="line x" title="160:214	In one paper, a separate model (a neural network) is trained on the results of clustering in order to classify individual term occurrences." ></td>
	<td class="line x" title="161:214	In the other, the individual occurrences of a term are tagged according to the distributional properties of their neighbors." ></td>
	<td class="line x" title="162:214	Clark (2000) presents a framework which in principle should accommodate lexical ambiguity using mixtures, but includes no evidence that it does so." ></td>
	<td class="line x" title="163:214	Furthermore, a mixture distribution species the proportion of occurrences of a term that should be tagged one way or another, but does not prescribe what to do with every individual event." ></td>
	<td class="line x" title="164:214	In contrast to the above approaches, we derive a lexicon which succinctly lists the possible syntactic senses for a term and provides a means to disambiguate the sense of a single occurrence." ></td>
	<td class="line x" title="165:214	MorePhrase Example Cluster Terms cubic feet francs barrels ounces hong kong london tokyo texas pointed out added noted disclosed los angeles london tokyo texas merrill lynch texaco chrysler ibm we dont we i you saudi arabia japan canada brazil morgan stanley texaco chrysler ibm managing director president chairman smith barney underwriters consumers Table 5: The ten highest-scoring two-word multi-word units in Reuters, along with example terms from the cluster to which each was assigned." ></td>
	<td class="line x" title="166:214	over, a shortcoming of occurrence-based methods of polysemy resolution is that a given term may be assigned to an implausibly large number of categories." ></td>
	<td class="line x" title="167:214	By analyzing this behavior at the type level, rather than the token level, we not only can exploit the corpus-wide behavior of a term, but we can enforce the linguistically defensible constraint that it have only a few senses." ></td>
	<td class="line x" title="168:214	3.3 Multi-Word Units In English, orthography provides a convenient clue to textual word segmentation." ></td>
	<td class="line x" title="169:214	Doing little more than breaking the text on whitespace boundaries, it is possible to perform a linguistically meaningful statistical analysis of a corpus." ></td>
	<td class="line x" title="170:214	Multi-word units (MWUs) are the exception to this rule." ></td>
	<td class="line x" title="171:214	Treating terms such as York terms which in a particular corpus may not be meaningful in isolation gives rise to highly idiosyncratic context distributions, which in turn add noise to cluster statistics or lead to the production of junk clusters." ></td>
	<td class="line x" title="172:214	In order to recognize such cases, we apply a variant of our by now familiar lexicon optimization rule: We posit a lexical entry for a given candidate MWU, nd the cluster to which it is best suited, and ask whether creating the lexeme improves the situation." ></td>
	<td class="line x" title="173:214	In principle, we can conduct this process in the same way as with novel terms and polysemy." ></td>
	<td class="line x" title="174:214	Here, however, we report the results of a simple surrogate technique." ></td>
	<td class="line x" title="175:214	After assembling the context distribution of the candidate MWU (an n-gram), we compute the Hellinger distance between this distribution and that of each cluster." ></td>
	<td class="line x" title="176:214	The Hellinger distance between two distributions a65 and a112 is de ned as: a113a79a114 a65 a9 a112a116a115 a1a87a117a102a86a107a88 a44a98a118 a56 a44 a118 a119 a44 (4) The candidate MWU is then tentatively assigned to the cluster for which this quantity is minimized and its distance to this cluster is noted (call this distance Score Band % in Wordnet a120 0.5 55 0.25 0.5 37 0 0.25 21 -0.25 0 11 -0.5 -0.25 5 -1 -0.5 1.4 a121 -1 1.9 Table 6: Fraction of two-word collocations present in Wordnet in each MWU score band." ></td>
	<td class="line x" title="177:214	a113a123a122a16a124a42a125a40a126a42a127 )." ></td>
	<td class="line x" title="178:214	We then compute the distance between each of the n-grams constituent terms and its respective cluster (a113a129a128a131a130a132a125a40a127 a71a68a133a16a133a16a133 a113a129a128a131a130a132a125a40a127a135a134 )." ></td>
	<td class="line x" title="179:214	The MWU score is the difference between the maximum term distance and the n-gram distance, or a136a66a137a46a138 a44a14a113 a128a74a130a132a125a40a127 a38 a86a139a113 a122a16a124a42a125a40a126a42a127 . In other words, the score of a candidate MWU increases with its closeness of t to its cluster and the lack of t of its constituent terms." ></td>
	<td class="line x" title="180:214	Table 5 shows the ten bi-grams that score highest using this heuristic." ></td>
	<td class="line x" title="181:214	Note that they come from a number of syntactic categories." ></td>
	<td class="line x" title="182:214	In this list, the only error is the phrase we dont, which is determined to be syntactically substitutable for pronouns." ></td>
	<td class="line x" title="183:214	Note, however, that this is the only collocation in this list consisting entirely of closed-class terms." ></td>
	<td class="line x" title="184:214	To the extent that we can recognize such terms, it is easy to rule out such cases." ></td>
	<td class="line x" title="185:214	Table 6 benchmarks this technique against Wordnet." ></td>
	<td class="line x" title="186:214	Breaking the range of MWU scores into bands, we ask what fraction of n-grams in each band can be found in Wordnet." ></td>
	<td class="line x" title="187:214	The result is a monotonic decrease in Wordnet representation." ></td>
	<td class="line x" title="188:214	Investigating further, we nd that almost all of the missing n-grams that score high are absent because they are corpus-speci c concepts, such as Morgan Stanley and Smith Barney." ></td>
	<td class="line x" title="189:214	On the other end, we nd that low-scoring n-grams present in Wordnet are typically included for reasons other than their ability to serve as independent lexemes." ></td>
	<td class="line x" title="190:214	For example, on that appears to have been included in Wordnet because it is a synonym for thereon." ></td>
	<td class="line x" title="191:214	3.4 Directions We have begun research into characterizing more precisely the grammatical roles of the clusters found by our methods, with an eye to identifying the lowest-level expansions in the grammar responsible for the text." ></td>
	<td class="line x" title="192:214	Inasmuch as information extraction can rely on shallow methods, the ability to produce a shallow parser without supervision should enable rapid creation of information extraction systems for new subject domains and languages." ></td>
	<td class="line x" title="193:214	We have had some success distinguishing open-class clusters from closed-class clusters, on the one hand, and head clusters from modi er clusters, on the other." ></td>
	<td class="line x" title="194:214	Highest Lowest Term Entropy Term Entropy and 6.67 swedish 3.50,(comma) 6.31 june 3.50 to 6.27 apparel 3.50 for 6.01 giant 3.50 was 5.92 modi ed 3.50 Table 7: Five most entropic and ve least entropic terms among the 5000 most frequent terms, using the -1 a140 +1 grounding space." ></td>
	<td class="line x" title="195:214	In general, closed-class terms have higher entropy." ></td>
	<td class="line x" title="196:214	Schone and Jurafsky (2001) list several universal characteristics of language that can serve as clues in this process, some of which we exploit." ></td>
	<td class="line x" title="197:214	However, their use of perfect clusters renders some of their algorithmic suggestions problematic." ></td>
	<td class="line x" title="198:214	For example, they propose using the tendency of a cluster to admit new members as an indication that it contains closed-class (or function) terms." ></td>
	<td class="line x" title="199:214	While we do nd large clusters corresponding to open classes and small clusters to closed classes, the separation is not always clean (e.g. , cluster 199 in Table 2)." ></td>
	<td class="line x" title="200:214	Small clusters often contain open-class terms with predominant corpus-speci c idiomatic usages." ></td>
	<td class="line x" title="201:214	For example, Reuters21578 has special usages for the terms note, net, and pay, in additional to their usual usages." ></td>
	<td class="line x" title="202:214	While the size of its cluster is a useful clue to the openor closed-class status of a term, we are forced to search for other sources of evidence." ></td>
	<td class="line x" title="203:214	Once such indicator is the entropy of the terms context distribution." ></td>
	<td class="line x" title="204:214	Table 7 lists the ve most and least entropic among the 5000 most frequent terms in Reuters-21578." ></td>
	<td class="line x" title="205:214	Function terms have higher entropy not only because they are more frequent than non-function terms, but also because a function term must participate syntactically with a wide variety of content-carrying terms." ></td>
	<td class="line x" title="206:214	While entropy alone also does not yield a clean separation between function and content terms, it may be possible to use it in combination with the suggestion of Schone and Jurafsky to produce a reliable separation." ></td>
	<td class="line x" title="207:214	3.5 Conclusion It seems clear that practical constraints will necessitate the development of powerful corpus-driven methods for meaning representation, particularly when dealing with diverse languages, subject matter, and writing styles." ></td>
	<td class="line x" title="208:214	Although it remains to be fully developed and tested, the evidence assembled thus far seems suf cient to conclude that our lexical optimization approach offers this prospect." ></td>
	<td class="line x" title="209:214	The approach follows a simple information-theoretic principle: A lexicon can be judged by the amount of information it captures about a suitably chosen grounding space . The process results in a distributional lexicon suitable for semantic comparison of sense-disambiguated terms, multi-word units, and most likely, larger units of text such as short phrases." ></td>
	<td class="line x" title="210:214	One can initialize the lexical optimization process by applying a distributional clustering algorithm such as coclustering to obtain term classes that have the properties of syntactic tags, regardless of the fact that many of the terms in a typical cluster will, in many contexts, fail to exhibit the syntactic class that the cluster implicitly represents." ></td>
	<td class="line x" title="211:214	This starting point is suf cient to support incremental re nements including sense disambiguation, multi-word-unit detection, and the incorporation of novel terms into the lexicon." ></td>
	<td class="line x" title="212:214	The preliminary evidence also suggests that this approach can be extended to capture shallow parsing information." ></td>
	<td class="line x" title="213:214	Although we have yet to conduct such experiments, it also seems clear that given a set of re nements based on one co-clustering run, it becomes possible to re-analyze the corpus in terms of the improved lexicon and generate an improved coclustering, etc. It remains to be seen how far such an approach can be productively pursued." ></td>
	<td class="line x" title="214:214	Acknowledgements This work was supported in full by the Advanced Research and Development Activity (ARDA) under contract number 2003-H265500-000." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W04-2611
Abstraction Summarization For Managing The Biomedical Research Literature
Fiszman, Marcelo;Rindflesch, Thomas C.;Kilicoglu, Halil;"></td>
	<td class="line x" title="1:165	Abstraction Summarization for Managing the Biomedical Research Literature Marcelo Fiszman Thomas C. Rindflesch Halil Kilicoglu Lister Hill National Center for Biomedical Communications National Library of Medicine Bethesda, MD 20894 {fiszman|tcr|halil}@nlm.nih.gov Abstract We explore a semantic abstraction approach to automatic summarization in the biomedical domain." ></td>
	<td class="line x" title="2:165	The approach relies on a semantic processor that functions as the source interpreter and produces a list of predications." ></td>
	<td class="line x" title="3:165	A transformation stage then generalizes and condenses this list, ultimately generating a conceptual condensate for a disorder input topic." ></td>
	<td class="line x" title="4:165	The final condensate is displayed in graphical form." ></td>
	<td class="line x" title="5:165	We provide a set of principles for the transformation stage and describe the application of this approach to multidocument input." ></td>
	<td class="line x" title="6:165	Finally, we examine the characteristics and quality of the condensates produced." ></td>
	<td class="line x" title="7:165	1 Introduction Several approaches to text-based information management applications are being pursued, including wordbased statistical processing and those depending on string matching, syntax, or semantics." ></td>
	<td class="line x" title="8:165	Statistical systems have enjoyed considerable success for information retrieval, especially using the vector space model (Salton et al. , 1975)." ></td>
	<td class="line x" title="9:165	Since the SIR system (Raphael, 1968), some have felt that automatic information management could best be addressed using semantic information." ></td>
	<td class="line x" title="10:165	Subsequent research (Schank, 1975; Wilks, 1976) expanded this paradigm." ></td>
	<td class="line x" title="11:165	More recently, a number of examples of knowledge-based applications show considerable promise." ></td>
	<td class="line x" title="12:165	These include systems for machine translation (Viegas et al. , 1998), question answering, (Harabagiu et al. , 2001; Clark et al. , 2003), and information retrieval (Mihalcea and Moldovan, 2000)." ></td>
	<td class="line x" title="13:165	In the biomedical domain, the MEDLINE  bibliographic database provides opportunities for keeping abreast of the research literature." ></td>
	<td class="line x" title="14:165	However, the large size of this online resource presents potential challenges to the user." ></td>
	<td class="line x" title="15:165	Query results often include hundreds or thousands of citations (including title and abstract)." ></td>
	<td class="line x" title="16:165	Automatic summarization offers potential help in managing such results; however, the most popular approach, extraction, faces challenges when applied to multidocument summarization (McKeown et al. , 2001)." ></td>
	<td class="line x" title="17:165	Abstraction summarization offers an attractive alternative for managing citations resulting from MEDLINE searches." ></td>
	<td class="line x" title="18:165	We present a knowledge-rich abstraction approach that depends on underspecified semantic interpretation of biomedical text." ></td>
	<td class="line x" title="19:165	As an example, a graphical representation (Batagelj, 2003) of the semantic predications serving as a summary (or conceptual condensate) from our system is shown in Figure 1." ></td>
	<td class="line x" title="20:165	The input text was a MEDLINE citation with title Gastrointestinal tolerability and effectiveness of rofecoxib versus naproxen in the treatment of osteoarthritis: a randomized, controlled trial. Figure 1." ></td>
	<td class="line x" title="21:165	Semantic abstraction summarization Our semantic interpreter and the abstraction summarizer based on it both draw on semantic information from the Unified Medical Language System  (UMLS),  a resource for structured knowledge in the biomedical domain." ></td>
	<td class="line x" title="22:165	After introducing the semantic interpreter, we describe the transformation phase of our paradigm, discussing principles that depend on semantic notions in order to condense the semantic predications representing the content of text." ></td>
	<td class="line x" title="23:165	Initially, this process was applied to summarizing single documents." ></td>
	<td class="line x" title="24:165	We discuss its adaptation to multidocument input, specifically to the set of citations resulting from a query to the MEDLINE database." ></td>
	<td class="line x" title="25:165	Although we have not yet formally evaluated the effectiveness of the resulting condensate, we discuss its characteristics and possibilities as both an indicative and informative summary." ></td>
	<td class="line x" title="26:165	2 2.1 2.2 Background Lexical Semantics Research in lexical semantics (Cruse, 1986) provides insight into the interaction of reference and linguistic structure." ></td>
	<td class="line x" title="27:165	In addition to paradigmatic lexical phenomena such as synonymy, hypernymy, and meronymy, diathesis alternation (Levin and Rappaport Hovav, 1996), deep case (Fillmore, 1968), and the interaction of predicational structure and events (Tenny and Pustejovsky, 2000) are being investigated." ></td>
	<td class="line x" title="28:165	Some of the consequences of research in lexical semantics, with particular attention to natural language processing, are discussed by Pustejovsky et al.(1993) and Nirenburg and Raskin (1996)." ></td>
	<td class="line x" title="30:165	Implemented systems often draw on the information contained in WordNet (Fellbaum, 1998)." ></td>
	<td class="line x" title="31:165	In the biomedical domain, UMLS knowledge provides considerable support for text-based systems." ></td>
	<td class="line x" title="32:165	(Burgun and Bodenreider (2001) compare the UMLS to WordNet)." ></td>
	<td class="line x" title="33:165	The UMLS (Humphreys et al. , 1998) consists of three components: the Metathesaurus,  Semantic Network (McCray, 1993), and SPECIALIST Lexicon (McCray et al. , 1994)." ></td>
	<td class="line x" title="34:165	The Metathesaurus is at the core and contains more than 900,000 concepts compiled from more than sixty controlled vocabularies." ></td>
	<td class="line x" title="35:165	Many of these have hierarchical structure, and some contain meronymic information in addition to hypernymy." ></td>
	<td class="line x" title="36:165	Editors combine terms in the constituent vocabularies into a set of synonyms (cf.WordNets synsets), which constitutes a concept." ></td>
	<td class="line x" title="38:165	One term in this set is called the preferred name and is used as the concept name, as shown in (1)." ></td>
	<td class="line x" title="39:165	(1) Concept: Dyspnea Synonyms: Breathlessness, Shortness of breath, Breathless, Difficulty breathing, Respiration difficulty, etc. In addition, each concept in the Metathesaurus is assigned at least one semantic type (such as Sign or Symptom for (1)), which categorizes the concept in the biomedical domain." ></td>
	<td class="line x" title="40:165	The semantic types available are drawn from the Semantic Network, in which they are organized hierarchically in two single-inheritance trees, one under the root Entity and another under Event." ></td>
	<td class="line x" title="41:165	The Semantic Network also contains semantic predications with semantic types as arguments." ></td>
	<td class="line x" title="42:165	The predicates are semantic relations relevant to the biomedical domain and are organized as subtypes of five classes, such as TEMPORALLY_RELATED_TO and FUNCTIONALLY_RELATED_TO." ></td>
	<td class="line x" title="43:165	Examples are shown in (2)." ></td>
	<td class="line x" title="44:165	(2) Pharmacologic Substance TREATS Disease or Syndrome, Virus CAUSES Disease or Syndrome Lexical semantic information in the UMLS is distributed between the Metathesaurus and the Semantic Network." ></td>
	<td class="line x" title="45:165	The Semantic Network stipulates permissible argument categories for classes of semantic predications, although it does not refer to deep case relations." ></td>
	<td class="line x" title="46:165	The Metathesaurus encodes synonymy, hypernymy, and meronymy (especially for human anatomy)." ></td>
	<td class="line x" title="47:165	Synonymy is represented by including synonymous terms under a single concept." ></td>
	<td class="line x" title="48:165	Word sense ambiguity is represented to some extent in the Metathesaurus." ></td>
	<td class="line x" title="49:165	For example discharge is represented by the two concepts in (3), with different semantic types." ></td>
	<td class="line x" title="50:165	(3) Discharge, Body Substance: Body Substance Patient Discharge: Health Care Activity The SPECIALIST Lexicon contains orthographic information (such as spelling variants) and syntactic information, including inflections for nouns and verbs and sub-categorization for verbs." ></td>
	<td class="line x" title="51:165	A suite of lexical access tools accommodate other phenomena, including derivational variation." ></td>
	<td class="line x" title="52:165	SemRep Our summarization system relies on semantic predications provided by SemRep (Rindflesch and Fiszman, 2003), a program that draws on UMLS information to provide underspecified semantic interpretation in the biomedical domain (Srinivasan and Rindflesch, 2002; Rindflesch et al. , 2000)." ></td>
	<td class="line x" title="53:165	Semantic interpretation is based on a categorical analysis that is underspecified in that it is a partial parse (cf.McDonald, 1992)." ></td>
	<td class="line oc" title="55:165	This analysis depends on the SPECIALIST Lexicon and the Xerox part-of-speech tagger (Cutting et al. , 1992) and provides simple noun phrases that are mapped to concepts in the UMLS Metathesaurus using MetaMap (Aronson, 2001)." ></td>
	<td class="line x" title="56:165	The categorial analysis enhanced with Metathesaurus concepts and associated semantic types provides the basis for semantic interpretation, which relies on two components: a set of indicator rules and an (underspecified) dependency grammar." ></td>
	<td class="line x" title="57:165	Indicator rules map between syntactic phenomena (such as verbs, nominalizations, and prepositions) and predicates in the Semantic Network." ></td>
	<td class="line x" title="58:165	For example, such rules stipulate that the preposition for indicates the semantic predicate TREATS in sumatriptan for migraine." ></td>
	<td class="line x" title="59:165	The application of an indicator rule satisfies the first of several necessary conditions for the interpretation of a semantic predication." ></td>
	<td class="line x" title="60:165	Argument identification is controlled by a partial dependency grammar." ></td>
	<td class="line x" title="61:165	As is common in such grammars, a general principle disallows intercalated dependencies (crossing lines)." ></td>
	<td class="line x" title="62:165	Further, a noun phrase may not be used as an argument in the interpretation of more than one semantic predication, without license." ></td>
	<td class="line x" title="63:165	(Coordination and relativization license noun phrase reuse)." ></td>
	<td class="line x" title="64:165	A final principle states that if a rule can apply it must apply." ></td>
	<td class="line x" title="65:165	Semantic interpretation in SemRep is not based on the real syntactic structure of the sentence; however linear order of the components of the partial parse is crucial." ></td>
	<td class="line x" title="66:165	Argument identification rules are articulated for each indicator in terms of surface subject and object." ></td>
	<td class="line x" title="67:165	For example, subjects of verbs are to the left and objects are to the right." ></td>
	<td class="line x" title="68:165	(Passivization is accommodated before final interpretation)." ></td>
	<td class="line x" title="69:165	There are also rules for prepositions and several rules for arguments of nominalizations." ></td>
	<td class="line x" title="70:165	The final condition on the interpretation of an associative semantic predication is that it must conform to the appropriate relationship in the Semantic Network." ></td>
	<td class="line x" title="71:165	For example, if a predication is being constructed on the basis of an indicator rule for TREATS, the syntactic arguments identified by the dependency grammar must have been mapped to Metathesaurus concepts with semantic types that conform to the semantic arguments of TREATS in the Semantic Network, such as Pharmacologic Substance and Disease or Syndrome." ></td>
	<td class="line x" title="72:165	Hypernymic propositions are further controlled by hierarchical information in the Metathesaurus (Rindflesch and Fiszman, 2003)." ></td>
	<td class="line x" title="73:165	In processing the sentence in (4), SemRep first constructs the partial categorical representation given schematically in (5)." ></td>
	<td class="line x" title="74:165	This is enhanced with semantic information from the Metathesaurus as shown in (6), where the corresponding concept for each relevant noun phrase is shown, along with its semantic type." ></td>
	<td class="line x" title="75:165	The final semantic interpretation for (4) is given in (7)." ></td>
	<td class="line x" title="76:165	(4) Mycoplasma pneumonia is an infection of the lung caused by Mycoplasma pneumoniae (5) [[Mycoplasma pneumonia] [is] [an infection] [of the lung] [caused] [by Mycoplasma pneumoniae]] (6) Mycoplasma pneumoniaDisease or Syndrome InfectionDisease or Syndrome LungBody Part, Organ, or Organ Component Mycoplasma pneumoniaeBacterium (7) Mycoplasma Pneumonia ISA Infection Lung LOCATION_OF Infection Lung LOCATION_OF Mycoplasma Pneumonia Mycoplasma pneumoniae CAUSES Infection Mycoplasma pneumoniae CAUSES Mycoplasma Pneumonia 3 3.1 Automatic Summarization Automatic summarization is a reductive transformation of source text to summary text through content reduction, selection, and/or generalization on what is important in the source (Sparck Jones, 1999)." ></td>
	<td class="line x" title="77:165	Two paradigms are being pursued: extraction and abstraction (Hahn and Mani, 2000)." ></td>
	<td class="line x" title="78:165	Extraction concentrates on creating a summary from the actual text occurring in the source document, relying on notions such as frequency of occurrence and cue phrases to identify important information." ></td>
	<td class="line x" title="79:165	Abstraction, on the other hand, relies either on linguistic processing followed by structural compaction (Mani et al. , 1999) or on interpretation of the source text into a semantic representation, which is then condensed to retain only the most important information asserted in the source." ></td>
	<td class="line x" title="80:165	The semantic abstraction paradigm is attractive due to its ability to manipulate information that may not have been explicitly articulated in the source document." ></td>
	<td class="line x" title="81:165	However, due to the challenges in providing semantic representation, semantic abstraction has not been widely pursued, although the TOPIC system (Hahn and Reimer, 1999) is a notable exception." ></td>
	<td class="line x" title="82:165	Semantic Abstraction Summarization We are devising an approach to automatic summarization in the semantic abstraction paradigm, relying on SemRep for semantic interpretation of source text." ></td>
	<td class="line x" title="83:165	The transformation stage that condenses these predications is guided by principles articulated in terms of frequency of occurrence as well as lexical semantic phenomena." ></td>
	<td class="line x" title="84:165	We do not produce a textual summary; instead, we present the disorder condensates in graphical format." ></td>
	<td class="line x" title="85:165	We first discuss the application of this approach to summarizing single documents (full text research articles on treatment of disease) and then consider its extension to multidocument input in the form of biomedical scientific abstracts directed at clinical researchers." ></td>
	<td class="line x" title="86:165	The transformation stage takes as input a list of Sem 3.2 Transformation In the semantic abstraction paradigm the transformation b. Connectivity: Also include useful additional c. Novelty: Do not include predications that the d. Saliency: Only include the most frequently ocAlth urrence (saliency) plays a rol (relevance), a condensation process, identifies ders} {Disorders} isorders} ders} s of sem a generalization process and ovelty) provides further condensation by elim tion phase and n these principles are applied to the semantic pre Rep predications and a seed disorder concept." ></td>
	<td class="line x" title="87:165	The output is a conceptual condensate for the input concept." ></td>
	<td class="line x" title="88:165	Before transformation begins, predications are subjected to a focused word sense disambiguation filter." ></td>
	<td class="line x" title="89:165	Branded drug names such as Advantage (Advantage brand of Imidacloprid) and Direct (Direct type of resin cement), which are ambiguous with the more common meaning of their names, are resolved to their non-pharmaceutical sense." ></td>
	<td class="line x" title="90:165	stage condenses and generalizes, and in our approach these processes are based on four general principles: a. Relevance: Include predications on the topic of the summary predications user already knows curring predications ough frequency of occ e in determining predications to be included in the summary, the other three principles depend crucially on lexical semantic information from the UMLS." ></td>
	<td class="line x" title="91:165	These four principles guide the phases involved in creating a summary." ></td>
	<td class="line x" title="92:165	Phase 1 predications on a given topic (in this study, disorders) and is controlled by a semantic schema (Jacquelinet et al. , 2003) for that topic." ></td>
	<td class="line x" title="93:165	The schema is represented as a set of predications in which the predicate is drawn from a relation in the UMLS Semantic Network and the arguments are represented as a domain covering a class of concepts in the Metathesaurus (Disorders, for example)." ></td>
	<td class="line x" title="94:165	{Disorders} ISA {Disor {Etiological process} CAUSES {Treatment} TREATS {Disorders} {Body location} LOCATION_OF {D {Disorders} OCCURS_IN {Disorders} {Disorders} CO-OCCURS_WITH {Disor Each domain for the schema is defined in term antic categorization in the Semantic Network." ></td>
	<td class="line x" title="95:165	For example {Disorders} is a subset of the semantic group Disorders (McCray et al. , 2001) and contains the following semantic types: Disease or Syndrome, Neoplastic Process, Mental or Behavioral Dysfunction, and Sign or Symptom." ></td>
	<td class="line x" title="96:165	Although the schema is not complete, it represents a substantial amount of what can be said about disorders." ></td>
	<td class="line x" title="97:165	Predications produced by SemRep must conform to this schema in order to be included in the conceptual condensate; such predications are called core predications. Phase 2 (connectivity) is identifies predications occurring in neighboring semantic space of the core." ></td>
	<td class="line x" title="98:165	This is accomplished by retrieving all the predications that share an argument with one of the core predications." ></td>
	<td class="line x" title="99:165	For example, from Naproxen TREATS Osteoarthritis, non-core predications such as Naproxen ISA NSAID are included in the condensate." ></td>
	<td class="line x" title="100:165	Phase 3 (n inating predications that have a generic argument, as determined by hierarchical depth in the Metathesaurus." ></td>
	<td class="line x" title="101:165	Arguments occurring less than an empirically determined distance from the root are considered too general to be useful, and predications containing them are eliminated." ></td>
	<td class="line x" title="102:165	For example Pharmaceutical Preparations TREATS Migraine is not included in the condensate for migraine because Pharmaceutical Preparations was determined to be generic." ></td>
	<td class="line x" title="103:165	Phase 4 (saliency) is the final transforma its operations are adapted from TOPICs (Hahn and Reimer, 1999) saliency operators." ></td>
	<td class="line x" title="104:165	Frequency of occurrence for arguments, predicates, and predications are calculated, and those occurring more frequently than the average are kept in the condensate; others are eliminated." ></td>
	<td class="line x" title="105:165	Whe dications produced by SemRep for a full-text article with 214 sentences (Lisse et al. , 2003) concerned with comparing naproxen and rofecoxib for treating osteoarthritis, with respect to effectiveness and gastrointestinal tolerability, the resulting condensate is given in Figure 2." ></td>
	<td class="line x" title="106:165	(The abstract for this article was summarized in Figure 1)." ></td>
	<td class="line x" title="107:165	Figure 2." ></td>
	<td class="line x" title="108:165	Semantic abstraction summarization of a journal article on osteoarthritis 4 Multidocument Summarization The MEDLINE database, developed and maintained by the N than 12 million citations (dating from the 1960s to the present) d at the same time retaining differences that ramework for determ Th results for Eval ation, especially for mult ev et al. , 2003)." ></td>
	<td class="line x" title="109:165	It is usually classified as intrinsic (measures the quality nd marked the predication h set ational Library of Medicine, contains more drawn from nearly 4,600 journals in the biomedical domain." ></td>
	<td class="line x" title="110:165	Access is provided by a statistical information retrieval system." ></td>
	<td class="line x" title="111:165	Due to the size of the database, searches often retrieve large numbers of items." ></td>
	<td class="line x" title="112:165	For example, the query diabetes returns 207,997 citations." ></td>
	<td class="line x" title="113:165	Although users can restrict searches by language, date and publication type (as well as specific journals), results can still be large." ></td>
	<td class="line x" title="114:165	For example, a query for treatment (only) for diabetes, limited to articles published in 2003 and having an abstract in English finds 3,621 items; limiting this further to articles describing clinical trials still returns 390 citations." ></td>
	<td class="line x" title="115:165	We describe the adaptation of our abstraction summarization process to multidocument input for managing the results of searches in MEDLINE." ></td>
	<td class="line x" title="116:165	Extending summarization to multidocument input presents challenges in removing redundancies across documents an might be important." ></td>
	<td class="line x" title="117:165	One issue is devising a framework on which to compute similarities and differences across documents." ></td>
	<td class="line x" title="118:165	Radev (2000) defines twenty-four relationships (such as equivalence, subsumption, and contradiction) that might apply at various structural levels across documents." ></td>
	<td class="line x" title="119:165	Sub-events (Daniel et al. , 2003) and sub-topics (Saggion and Lapalme, 2002) also contribute to the framework used for comparing documents in multidocument summarization." ></td>
	<td class="line x" title="120:165	A particular challenge to multidocument summarization in the extraction paradigm is determining what parts of documents conform to the f ining similarities and differences." ></td>
	<td class="line x" title="121:165	A recent study (Kan et al. , 2001) uses topic composition from text headers, but other studies in the extraction paradigm (Goldstein et al. , 1999), extraction coupled with rhetorical structural identification (Teufel and Moens, 2002), and syntactic abstraction paradigms use different methodologies (Barzilay et al. , 1999; McKeown et al. , 1999)." ></td>
	<td class="line x" title="122:165	Our semantic abstraction summarization system naturally extends to multidocument input with no modification from the system designed for single documents." ></td>
	<td class="line x" title="123:165	e disorder schema serves as the framework for identifying sub-topics, and predications retrieved across several documents must conform to its structure." ></td>
	<td class="line x" title="124:165	Informational equivalence (and redundancy) is computed on this basis." ></td>
	<td class="line x" title="125:165	For example, all predications that conform to the schema line {Treatment} TREATS {Disorders} constitute a representation of a subtopic in the disorder domain." ></td>
	<td class="line x" title="126:165	Exact matches in this set constitute redundant information, and other types of relationships can be computed on the basis of partial matches." ></td>
	<td class="line x" title="127:165	Although we concentrate on similarities across documents, differences could be computed by examining predications that are not shared among citations." ></td>
	<td class="line x" title="128:165	We have begun testing our system applied to the results of MEDLINE searches on disorders, concentrating on the most recent 300 citations retrieved." ></td>
	<td class="line x" title="129:165	The migraine are represented graphically in Figure 3." ></td>
	<td class="line x" title="130:165	Traversing the predicates (arcs) in this condensate provides an informative summary of these citations." ></td>
	<td class="line x" title="131:165	5 Evaluation and Results uation in automatic summariz idocument input, is daunting (Rad of the summary as related to the source documents) or extrinsic (how the summary affects some other task)." ></td>
	<td class="line x" title="132:165	Since we do not have a gold standard to compare the final condensates against, we performed a linguistic evaluation on the quality of the condensates generated for four diseases: migraine, angina pectoris, Crohns disease, and pneumonia." ></td>
	<td class="line x" title="133:165	The input for each summary was 300 MEDLINE citations." ></td>
	<td class="line x" title="134:165	Table 1 presents evaluation results." ></td>
	<td class="line x" title="135:165	The first author (MF) examined the source sentence that SemRep used to generate each predication a s as either correct or incorrect." ></td>
	<td class="line x" title="136:165	Precision was calculated as the total number of correct predications divided by the total number of predications in the condensate." ></td>
	<td class="line x" title="137:165	We also measured the reduction (compression) for each of the four disorder concepts." ></td>
	<td class="line x" title="138:165	In Table 1, Base is the number of predications SemRep produced from eac of 300 citations." ></td>
	<td class="line x" title="139:165	Final is the number of predications left after the final transformation." ></td>
	<td class="line x" title="140:165	Therefore, this is a compression ratio on the semantic space of predications, and is different from text compression in the traditional sense." ></td>
	<td class="line x" title="141:165	Concept Base Final C I Precision Migraine 2485 102 72 30 71% A 3 8 ia ngina 2989 41 3 80% Crohns 3077 135 71 64 53% Pneumon 2694 28 27 1 96% Total 11245 306 203 103 66% Table 1." ></td>
	<td class="line x" title="142:165	ts the f di se r = r In Crohns disease (with lowest precision) a single Sem for 52% of th ocessing the sen Resul for our sea concepts C = Co rect, I Incor ect Rep error type in argument identification accounts e mistakes." ></td>
	<td class="line x" title="143:165	For example in pr tence 36 patients with inflammatory bowel disease (11 with ulcerative colitis and 25 with Crohns disease), the parenthesized material caused SemRep to incorrectly returned Inflammatory Bowel Diseases COOCCURS_WITH Ulcerative Colitis and Ulcerative Colitis predicate CO-OCCURS_WITH Crohns Disease. Word sense ambiguity also contributed to a large number of errors." ></td>
	<td class="line x" title="144:165	6 Content Characterization We sformation stage has and predications during the summarization process." ></td>
	<td class="line x" title="145:165	SemRep produced S_IN; and pes in the final con he fin nitially parsed, only 63 are represented in the o far do not accommodate." ></td>
	<td class="line x" title="146:165	Some of the lusion and Future Directions We raction summarization that produces conceptual condensates for condensate to the text that produced them." ></td>
	<td class="line x" title="147:165	We examined the effect that the tran on the distribution of predicates 2,485 predications from 300 citations retrieved for migraine." ></td>
	<td class="line x" title="148:165	Of these, 1,638 are distributed over four predicates in the disorder schema (327TREATS; 148ISA; 180LOCATION_OF; 54CAUSES; 720 OCCURS_IN; and 209CO-OCCURS_WITH)." ></td>
	<td class="line x" title="149:165	After phases 1, 2, and 3 of the transformation process, 311 predications remain (134TREATS; 41ISA; 12LOCATION_OF; 5CAUSES; 68OCCUR 51CO-OCCURS_WITH)." ></td>
	<td class="line x" title="150:165	This reduction is largely due to hierarchical pruning in phase 3." ></td>
	<td class="line x" title="151:165	Phase 4 operations, based on frequency of occurrence pruning (saliency), further condensed the list, and the top three TREATS predication ty densate are (13Sumatriptan TREATS Migraine; 6 Botulinum Toxins TREATS Migraine; and 6feverfew extract TREATS Migraine)." ></td>
	<td class="line x" title="152:165	This list represents the fact that Sumatriptan is a popular treatment for migraine." ></td>
	<td class="line x" title="153:165	Besides frequency, another way of looking at the predications is typicality (Kan et al. , 2001), or distribution of predications across citations." ></td>
	<td class="line x" title="154:165	Looking at t al condensate for migraine and focusing on TREATS, the most widely distributed predications are Sumatriptan TREATS Migraine, which occurs in ten citations; Botulinum Toxins TREATS Migraine (three citations); and feverfew extract TREATS Migraine (two citations)." ></td>
	<td class="line x" title="155:165	One can also view the final condensate from the perspective of citations, rather than predications." ></td>
	<td class="line x" title="156:165	Of the 300 citations i final condensate, one with six predications, one with five predications, three with four predications, and so on." ></td>
	<td class="line x" title="157:165	It is tempting to hypothesize that more highly relevant citations will have produced more predications, but this must be formally tested in the context of the users retrieval objective." ></td>
	<td class="line x" title="158:165	An informal examination of the citations that contributed to the final condensate for migraine revealed differences that we s se, such as publication and study type, could be addressed outside of natural language processing with MEDLINE metadata." ></td>
	<td class="line x" title="159:165	Others, including medication delivery system and target population of the disorder topic, are amenable to current processing either through extension of the disease schema or enhancements to SemRep." ></td>
	<td class="line x" title="160:165	7 Conc propose a framework based on semantic abst disorder topics that are both indicative and informative." ></td>
	<td class="line x" title="161:165	The approach uses a biomedical semantic processor as the source interpreter." ></td>
	<td class="line x" title="162:165	After semantic interpretation, a series of transformations condense the predications produced, and a final condensate is displayed in graphical form." ></td>
	<td class="line x" title="163:165	In the future, we would like to link the predications in the o plan to evaluate the effectiveness of this approach in retrieving useful articles for clinical researchers." ></td>
	<td class="line x" title="164:165	Finally, we would like to investigate additional ways of visualizing the condensates." ></td>
	<td class="line x" title="165:165	Acknowledgements The first author was supported by an appointment to the Nat search Participation Program administered by the Oak Ridge Institute for Science and Education through an inter-agency agreement between the U.S. Department of Energy and the National Library of Medicine." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W04-3112
Using Natural Language Processing, LocusLink And The Gene Ontology To Compare OMIM To MEDLINE
Libbus, Bisharah;Kilicoglu, Halil;Rindflesch, Thomas C.;Mork, James G.;Aronson, Alan R.;"></td>
	<td class="line x" title="1:156	Using Natural Language Processing, Locus Link, and the Gene Ontology to Compare OMIM to MEDLINE Bisharah Libbus Halil Kilicoglu Thomas C. Rindflesch James G. Mork Alan R. Aronson Lister Hill National Center for Biomedical Communications National Library of Medicine Bethesda, Maryland, 20894 {libbus|halil|tcr|mork|alan}@nlm.nih.gov Abstract Researchers in the biomedical and molecular biology fields are faced with a wide variety of information sources." ></td>
	<td class="line x" title="2:156	These are presented in the form of images, free text, and structured data files that include medical records, gene and protein sequence data, and whole genome microarray data, all gathered from a variety of experimental organisms and clinical subjects." ></td>
	<td class="line x" title="3:156	The need to organize and relate this information, particularly concerning genes, has motivated the development of resources, such as the Unified Medical Language System, Gene Ontology, LocusLink, and the Online Inheritance In Man (OMIM) database." ></td>
	<td class="line x" title="4:156	We describe a natural language processing application to extract information on genes from unstructured text and discuss ways to integrate this information with some of the available online resources." ></td>
	<td class="line x" title="5:156	1 Introduction The current knowledge explosion in genetics and genomics poses a challenge to both researchers and medical practitioners." ></td>
	<td class="line x" title="6:156	Traditionally, scientific reviews, which summarize and evaluate the literature, have been indispensable in addressing this challenge." ></td>
	<td class="line x" title="7:156	OMIM (Online Mendelian Inheritance in Man) (OMIM 2000), for example, is a clinical and biomedical information resource on human genes and genetic disorders." ></td>
	<td class="line x" title="8:156	It has close to 15,000 entries detailing clinical phenotypes and disorders as well as information on nearly 9,000 genes." ></td>
	<td class="line x" title="9:156	The database can be searched by gene symbol, chromosomal location, or disorder." ></td>
	<td class="line x" title="10:156	More recently, automated techniques for information and knowledge extraction from the literature are being developed to complement scientific reviews." ></td>
	<td class="line x" title="11:156	These methods address the need to condense and efficiently present large amounts of data to the user." ></td>
	<td class="line x" title="12:156	The feasibility of applying natural language processing techniques to the biomedical literature (Friedman and Hripcsak 1999; de Bruijn and Martin 2002) and to the wealth of genomics data now available (Jenssen et al. 2001; Yandell and Majoros 2002) is increasingly being recognized." ></td>
	<td class="line x" title="13:156	Efforts to develop systems that work toward this goal focus on the identification of such items as gene and protein names (Tanabe and Wilbur 2002) or groups of genes with similar function (Jenssen et al. 2001; Masys et al. 2001)." ></td>
	<td class="line x" title="14:156	Other groups are interested in identifying protein-protein (Blaschke et al. 1999; Temkin and Gilder 2003) or gene-gene interactions (Stephens et al. 2001; Tao et al. 2002), inhibit relations (Pustejovsky et al. 2002), protein structure (Gaizauskas et al. 2003), and pathways (Ng and Wong 1999; Friedman et al. 2001)." ></td>
	<td class="line x" title="15:156	We discuss the modification of an existing natural language processing system, SemGen (Rindflesch et al. 2003), that has broad applicability to biomedical text and that takes advantage of online resources such as LocusLink and the Gene Ontology." ></td>
	<td class="line x" title="16:156	We are pursuing research that identifies gene-gene interactions in text on genetic diseases." ></td>
	<td class="line x" title="17:156	For example the system extracts (2) from (1)." ></td>
	<td class="line x" title="18:156	1) Here, we report that TSLC1 directly associates with MPP3, one of the human homologues of a Drosophila tumor suppressor gene, Discs large (Dlg)." ></td>
	<td class="line x" title="19:156	2) TSLC1|INTERACT_WITH|MPP3 Association for Computational Linguistics." ></td>
	<td class="line x" title="20:156	Linking Biological Literature, Ontologies and Databases, pp." ></td>
	<td class="line x" title="21:156	69-76." ></td>
	<td class="line x" title="22:156	HLT-NAACL 2004 Workshop: Biolink 2004, Due to the complexity of the language involved, the extraction of such predications is currently not accurate enough to support practical application." ></td>
	<td class="line x" title="23:156	However, we suggest its potential in the context of an application that combines traditional, human-curated resources such as OMIM and emerging information extraction applications." ></td>
	<td class="line x" title="24:156	2 3 SemGen Molecular Biology Resources To support and supplement the information extracted by SemGen from biomedical text, we draw on two resources, LocusLink and the Gene Ontology." ></td>
	<td class="line x" title="25:156	LocusLink (Wheeler et al. 2004) provides a single query interface to curated genomic sequences and genetic loci." ></td>
	<td class="line x" title="26:156	It presents information on official nomenclature, aliases, sequence accessions, phenotypes, OMIM numbers, homology, map locations, and related Web sites, among others." ></td>
	<td class="line x" title="27:156	Of particular interest is the Reference Sequence (RefSeq) collection, which provides a comprehensive, curated, integrated, non-redundant set of sequences, including genomic DNA, transcript (RNA), and protein products for major research organisms." ></td>
	<td class="line x" title="28:156	Currently, SemGen uses LocusLink to obtain normalized gene names and Gene Ontology annotations." ></td>
	<td class="line x" title="29:156	The Gene Ontology (GO) (The Gene Ontology Consortium 2000, 2001, 2004) aims to provide a dynamic controlled vocabulary that can be applied to all organisms, even while knowledge of gene and protein function is incomplete or unfolding." ></td>
	<td class="line x" title="30:156	The GO consists of three separate ontologies: molecular function, biological process, and cellular component." ></td>
	<td class="line x" title="31:156	These three branches are used to characterize gene function and products and provide a comprehensive structure that permits the annotation of molecular attributes of genes in various organisms." ></td>
	<td class="line x" title="32:156	We use GO annotations to examine whether there are identifiable patterns, or concordance, in the function of gene pairs identified by SemGen." ></td>
	<td class="line x" title="33:156	SemGen identifies gene interaction predications based on semantic interpretation adapted from SemRep (Srinivasan and Rindflesch 2002; Rindflesch and Fiszman 2003), a general natural language processing system being developed for the biomedical domain." ></td>
	<td class="line x" title="34:156	After the application of a statistically-based labeled categorizer (Humphrey 1999) that limits input text to the molecular biology domain, SemGen processing proceeds in three major phases: categorial analysis, identification of concepts, and identification of relations." ></td>
	<td class="line oc" title="35:156	The initial phase relies on a parser that draws on the SPECIALIST Lexicon (McCray et al. 1994) and the Xerox Part-of-Speech Tagger (Cutting et al. 1992) to produce an underspecified categorial analysis." ></td>
	<td class="line x" title="36:156	In the phase for identifying concepts, disorders as well as genes and proteins are isolated by mapping simple noun phrases from the previous phase to concepts in the Unified Medical Language System  (UMLS)  Metathesaurus  (Humphreys et al. 1998), using MetaMap (Aronson 2001)." ></td>
	<td class="line x" title="37:156	ABGene, a program that identifies genes and proteins using several statistical and empirical methods (Tanabe and Wilbur 2002) is also consulted during this phase." ></td>
	<td class="line x" title="38:156	In addition, a small list of signal words (such as gene, codon, and exon) helps identify genetic phenomena." ></td>
	<td class="line x" title="39:156	For example, the genetic phenomena in (4) are identified from the sentence in (3)." ></td>
	<td class="line x" title="40:156	Concepts isolated in this phase serve as potential arguments in the next phase." ></td>
	<td class="line x" title="41:156	3) WIF1 was down-regulated in 64% of primary prostate cancers, while SFRP4 was up-regulated in 81% of the patients." ></td>
	<td class="line x" title="42:156	4) genphenom|WIF1 genphenom|SFRP4 During the final phase, in which relations are identified, the predicates of semantic propositions are based on indicator rules." ></td>
	<td class="line x" title="43:156	These stipulate verbs, nominalizations, and prepositions that indicate semantic predicates." ></td>
	<td class="line x" title="44:156	During this phase, argument identification is constrained by an underspecified dependency grammar, which also attempts to accommodate coordinated arguments as well as predicates." ></td>
	<td class="line x" title="45:156	SemGen originally had twenty rules indicating one of three etiology relations between genetic phenomena and diseases, namely CAUSE, PREDISPOSE, and ASSOCIATED_WITH." ></td>
	<td class="line x" title="46:156	In this project, we extended SemGen to cover gene-gene interaction relations: INHIBIT, STIMULATE, AND INTERACT_WITH." ></td>
	<td class="line x" title="47:156	About 20 indicator rules were taken from MedMiner (Tanabe et al. 1999)." ></td>
	<td class="line x" title="48:156	We supplemented this list by taking advantage of the verbs identified in syntactic predications by GeneScene (Leroy et al. 2003)." ></td>
	<td class="line x" title="49:156	SemGen has 46 gene-gene interaction indicator rules (mostly verbs), including 16 for INHIBIT (such as block, deplete, down-regulate); 12 for INTERACT_WITH (bind, implicate, influence, mediate); and 18 for STIMULATE (amplify, activate, induce, upregulate)." ></td>
	<td class="line x" title="50:156	An overview of the SemGen system is given in Figure 1, and an example is provided below." ></td>
	<td class="line x" title="51:156	SemGen processing on input text (5) produces the underspecified syntactic structure (represented schematically) in (6)." ></td>
	<td class="line x" title="52:156	(7) illustrates genetic phenomena identified, and (8) shows the final semantic interpretation." ></td>
	<td class="line x" title="53:156	Figure 1." ></td>
	<td class="line x" title="54:156	SemGen system 5) We show here that EGR1 binds to the AR in prostate carcinoma cells, and an EGR1-AR complex can be detected by chromatin immunoprecipitation at the enhancer of an endogenous AR target gene." ></td>
	<td class="line x" title="55:156	6) [We] [show] [here] [that] [EGR1] [binds] [to the AR] [in prostate carcinoma cells,] [and] [an EGR1-AR complex] [can] [be] [detected] [by chromatin immunoprecipitation] [at the enhancer] [of an endogenous AR target gene] 7) genphenom|egr1 genphenom|ar genphenom|enhancer endogenous ar target gene 8) egr1|INTERACT_WITH|ar During processing, SemGen normalizes gene symbols using the preferred symbol from LocusLink." ></td>
	<td class="line x" title="56:156	The final interpretation with LocusLink gene symbol is shown in (9)." ></td>
	<td class="line x" title="57:156	9) EGR1|INTERACT_WITH|AR As we retrieve the LocusLink symbol for a gene, we also get the GO terms associated with that gene." ></td>
	<td class="line x" title="58:156	We are interested in extending the application of our textual analysis and knowledge extraction methodology and relating it to other biomedical and genomic resources." ></td>
	<td class="line x" title="59:156	Gene Ontology is one such important resource, and below we discuss the possibility that GO might shed additional light on the biological relationship between genes that are paired functionally based on textual analysis." ></td>
	<td class="line x" title="60:156	The GO terms for the genes in (9) are given in (10) and (11)." ></td>
	<td class="line x" title="61:156	10) EGR1|[transcription factor activity; regulation of transcription, DNA-dependent; nucleus] 11) AR|[androgen receptor activity; steroid binding; receptor activity; transcription factor activity; transport; sex differentiation; regulation of transcription, DNA-dependent; signal transduction; cell-cell signaling; nucleus] 4 SemGen Evaluation and Error Analysis Before suggesting an application using SemGen output, we discuss the results of error analysis performed on 344 sentences from MEDLINE citations related to six genetic diseases: Alzheimer's disease, Crohns disease, lung cancer, ovarian cancer, prostate cancer and sickle cell anemia." ></td>
	<td class="line x" title="62:156	Out of 442 predications identified by SemGen, 181 were correct, for 41% precision." ></td>
	<td class="line x" title="63:156	This is not yet accurate enough to support a production system; however, the majority of the errors are focused in two syntactic areas, and we believe that with further development it is possible to provide output effective for supporting practical applications." ></td>
	<td class="line x" title="64:156	The majority of the errors fall into one of two major syntactic classes, relativization and coordination." ></td>
	<td class="line x" title="65:156	A further source of error is the fact that we have not yet addressed interaction relations that involve a process in addition to a gene." ></td>
	<td class="line x" title="66:156	Reduced relative clauses, such as mediated by Tip60 in (12), are a rich source of argument identification errors." ></td>
	<td class="line x" title="67:156	12) LRPICD dramatically inhibits APP-derived intracellular domain/Fe65 transactivation mediated by Tip60." ></td>
	<td class="line x" title="68:156	SemGen wrongly interpreted this sentence as asserting that LRPICD inhibits Tip60." ></td>
	<td class="line x" title="69:156	The rules of the underspecified dependency grammar that identify arguments essentially look to the left and right of a verb for a noun phrase that has been marked as referring to a genetic phenomenon." ></td>
	<td class="line x" title="70:156	Arguments are not allowed to be used in more than one predication (unless licensed by coordination or as the head of a relative clause)." ></td>
	<td class="line x" title="71:156	A number of phenomena conspire in (12) to wrongly allow TIP60 to be analyzed as the object of inhibits." ></td>
	<td class="line x" title="72:156	The actual object, transactivation, was not recognized because we have not yet addressed processes as arguments of gene interaction predications." ></td>
	<td class="line x" title="73:156	Further, the predication on transactivation, with argument TIP60, was not interpreted, and hence TIP60 was available (incorrectly) for the object of inhibits." ></td>
	<td class="line x" title="74:156	If we had recognized the relative clause in (12), TIP60 would not have been reused as an argument of inhibits, since only heads of relative clauses can be reused." ></td>
	<td class="line x" title="75:156	The underspecified analysis on which SemGen is based is not always effective in identifying verb phrase coordination, as in (13), leading to the incorrect interpretation that WIF1 interacts with SFRP4." ></td>
	<td class="line x" title="76:156	13) WIF1 was down-regulated in 64% of primary prostate cancers, while SFRP4 was up-regulated in 81% of the patients." ></td>
	<td class="line n" title="77:156	A further source of error in this sentence is that down-regulated was analyzed by the tagger as a past tense rather than past participle, thus causing the argument identification phase to look for an object to the right of this verb form." ></td>
	<td class="line x" title="78:156	A further issue here is that we have not yet addressed truncated passives." ></td>
	<td class="line x" title="79:156	5 Using SemGen to Compare OMIM and MEDLINE SemGen errors notwithstanding, we are investigating possibilities for exploiting automatically extracted gene interaction predications." ></td>
	<td class="line x" title="80:156	We discuss an application which compares MEDLINE text to OMIM documents, for specified diseases." ></td>
	<td class="line x" title="81:156	LocusLink preferred gene symbols and GO terms are an integral part of this processing." ></td>
	<td class="line x" title="82:156	We feel it is instructive to investigate the consequences of this comparison, anticipating results that are effective enough for practical application." ></td>
	<td class="line x" title="83:156	We selected five diseases with a genetic component (Alzheimers disease, Crohns disease, lung cancer, prostate cancer, and sickle cell anemia), and retrieved the corresponding OMIM report for each disease, automatically discarding sections such as references, headings, and edit history." ></td>
	<td class="line x" title="84:156	We also queried PubMed for each disease and retrieved all MEDLINE citations that were more recent than the corresponding OMIM report." ></td>
	<td class="line x" title="85:156	Both OMIM and MEDLINE files were then submitted to SemGen." ></td>
	<td class="line x" title="86:156	For each disease, the MEDLINE file was larger than the corresponding OMIM file, and the categorizer eliminated some parts of each file as not being in the molecular biology domain." ></td>
	<td class="line x" title="87:156	Table 1 shows the number of sentences in the original input files and the number processed after the categorizer eliminated sentences not in the molecular biology domain." ></td>
	<td class="line x" title="88:156	OMIM Orig." ></td>
	<td class="line x" title="89:156	OMIM Proc." ></td>
	<td class="line x" title="90:156	MEDLINE Orig." ></td>
	<td class="line x" title="91:156	MEDLINE Proc." ></td>
	<td class="line x" title="92:156	Alz 408 264 1639 862 Crohn 188 124 4871 1236 LungCa 55 34 9058 2966 ProstCa 121 69 6989 2964 SCA 184 79 4383 1057 Table 1." ></td>
	<td class="line x" title="93:156	Input sentences processed by SemGen A paragraph in the OMIM file for Alzheimers disease beginning with the sentence Alzheimer disease is by far the most common cause of dementia, for example, was eliminated, while a MEDLINE citation with the title Semantic decision making in early probable AD: A PET activation study was removed." ></td>
	<td class="line x" title="94:156	An overview of predication types retrieved by SemGen is given in Table 2 for the files on Alzheimers disease." ></td>
	<td class="line x" title="95:156	Of the gene-disease predications, the majority had predicate ASSOCIATED_WITH (15 from OMIM and 25 from MEDLINE)." ></td>
	<td class="line x" title="96:156	For gene-gene relations, INTERACT_WITH predominated (3 from OMIM and 12 from MEDLINE)." ></td>
	<td class="line x" title="97:156	Alzheimer disease OMIM MEDLINE Gene-Disease 16 31 Gene-Gene 3 22 Total 19 53 Table 2." ></td>
	<td class="line x" title="98:156	Gene interaction predication types We developed a program that compares semantic predications found in MEDLINE abstracts to those found in an OMIM report associated with a particular disease and classifies the comparison between two predications as either an exact match, partial match, or no match." ></td>
	<td class="line x" title="99:156	The category of a comparison is determined by examining the argument and predicate fields of the predications." ></td>
	<td class="line x" title="100:156	If all three fields match, the comparison is an exact match; if any two fields match it is a partial match." ></td>
	<td class="line x" title="101:156	All other cases are considered as no match." ></td>
	<td class="line x" title="102:156	Although fewer than half of the predications extracted by SemGen are likely to be correct, we provide some examples from the files on Alzheimers disease." ></td>
	<td class="line x" title="103:156	(The system retains the document IDs, which are suppressed here for clarity)." ></td>
	<td class="line x" title="104:156	Examples of partial matches between gene-disease predications extracted from OMIM and MEDLINE are shown in (14) and (15)." ></td>
	<td class="line x" title="105:156	14) OM: APP | ASSOCIATED_WITH | Alzheimers Disease ML: CD14 | ASSOCIATED_WITH | Alzheimers Disease 15) OM: amyloid beta peptide | ASSOCIATED_WITH | Alzheimers Disease ML: amyloid beta peptide | ASSOCIATED_WITH | Senile Plaques Some of the gene-disease predications that only occurred in OMIM are given in (16), and a few of those occurring exclusively in MEDLINE are given in (17)." ></td>
	<td class="line x" title="106:156	16) TGFB1 | ASSOCIATED_WITH | Amyloid deposition PRNP | ASSOCIATED_WITH | Amyloid deposition Mutation 4 gene | CAUSE | Alzheimers Disease 17) MOG | ASSOCIATED_WITH | Nervous System Diseases Acetylcholinesterase | PREDISPOSE | Alzheimers Disease In (18) are listed some of the gene-gene interaction predications found in MEDLINE but not in OMIM." ></td>
	<td class="line x" title="107:156	18) LAMR1 | STIMULATE | HTATIP MAPT|INTERACT_WITH | HSPA8 CD14 | STIMULATE | amyloid peptide 6 Using the GO Terms As noted above, for each gene argument in the predications identified by SemGen, we retrieved from LocusLink the GO terms associated with that gene." ></td>
	<td class="line x" title="108:156	We have begun to investigate ways in which these terms might be used to compare genes by looking at the gene-gene interaction predications extracted from MEDLINE that did not occur in OMIM." ></td>
	<td class="line x" title="109:156	To support this work, we developed a program that sorts gene-gene interaction predications by the GO terms of their arguments." ></td>
	<td class="line x" title="110:156	For each gene function, the predications in which both arguments share the same function are listed first." ></td>
	<td class="line x" title="111:156	These are followed by the predications in which only the first argument has that gene function, and then the predications in which only the second argument has the relevant gene function." ></td>
	<td class="line x" title="112:156	A typical output file of this process is shown in (19): 19) RECEPTOR ACTIVITY ----------------Both Arguments: DTR|STIMULATE|EGFR First Argument: AR|STIMULATE|TRXR3 EPHB2|STIMULATE|ENO2 Second Argument: EGR1|INTERACT_WITH|AR PSMC6|STIMULATE|AR The three branches of the Gene Ontology provide a uniform system for relating genes by function." ></td>
	<td class="line x" title="113:156	The terms in the molecular function and biological process branches are perhaps most useful for this purpose; however, we have begun by considering all three branches (including the cellular component branch)." ></td>
	<td class="line x" title="114:156	The most effective method of exploiting GO annotations remains a matter of research." ></td>
	<td class="line x" title="115:156	It is important to recognize that GO mapping is not precise; different annotators may make different GO assignments for the same gene." ></td>
	<td class="line x" title="116:156	Nevertheless, GO annotations provide considerable potential for relating the molecular functions and biological processes of genes." ></td>
	<td class="line x" title="117:156	We consider one of the predications extracted from the MEDLINE file for prostate cancer that did not occur in OMIM: 19) EGR1|INTERACT_WITH|AR Both genes EGR1 and AR in LocusLink elicit the same human gene set (367 Hs AR; 1026 Hs CDKN1A; 1958 Hs EGR1; 3949 Hs LDLR; 4664 Hs NAB1; 4665 Hs NAB2; 5734 Hs PTGER4; 114034 Hs TOE1)." ></td>
	<td class="line x" title="118:156	This suggests a high degree of sequence homology and functional similarity." ></td>
	<td class="line x" title="119:156	In addition, LocusLink provides the following GO terms for the two genes: 20) EGR1: early growth response 1; LocusID: 1958 Gene Ontology: transcription factor activity; regulation of transcription, DNA-dependent; nucleus 21) AR: androgen receptor (dihydrotestosterone receptor; testicular feminization; spinal and bulb ar muscular atrophy; Kennedy disease) ; LocusID: 367 Gene Ontology: androgen receptor activity; steroid binding; receptor activity; transcription factor activity; transport; sex differentiation; regulation of transcription, DNAdependent; signal transduction; cell-cell signaling; nucleus (The GO provides additional, hierarchical information for terms, which we have not yet exploited)." ></td>
	<td class="line x" title="120:156	Thirty percent of the predications examined had some degree of overlap in their GO terms." ></td>
	<td class="line x" title="121:156	For example, the terms for EGR1 (transcription factor activity; regulation of transcription, DNAdependent; and nucleus) are identical to three of the GO terms for the AR gene." ></td>
	<td class="line x" title="122:156	This concordance may not be typical of the majority of paired genes in our sample." ></td>
	<td class="line x" title="123:156	However, in the case of genes that do not exhibit such complete overlap, concordance might be obtained at higher nodes in the classification scheme." ></td>
	<td class="line x" title="124:156	An alternate approach for assessing distance between GO annotations has been suggested by Lord et al.(2003a, 2003b)." ></td>
	<td class="line x" title="126:156	They propose a semantic similarity measure using ontologies to explore the relationships between genes that may have associated interaction or function." ></td>
	<td class="line x" title="127:156	The authors consider the information content of each GO term, defined as the number of times each term, or any child term, occurs." ></td>
	<td class="line x" title="128:156	The fact that any one gene has a number of GO annotations indicates that a particular gene may perform more than one function or its function may be classified under a number of molecular activities." ></td>
	<td class="line x" title="129:156	Some of these activities may be part of, i.e. extending to a variable degree down, the same GO structure." ></td>
	<td class="line x" title="130:156	For example, for gene AR, receptor activity (GO 4872) partially overlaps with androgen receptor activity (GO 4882), as does steroid binding (GO 5496) with transcription factor activity (GO 3700), and signal transduction (GO 7165) and cell-cell signaling (GO 7267)." ></td>
	<td class="line x" title="131:156	This indicates that in assessing similarity one needs to examine the ontology structure and not rely solely on the GO terms." ></td>
	<td class="line x" title="132:156	While we have no experimental evidence, we would like to speculate about the functional or biological significance indicated by similarity in GO annotation." ></td>
	<td class="line x" title="133:156	There are three orthogonal aspects to GO: molecular function, biological process, and cellular component." ></td>
	<td class="line x" title="134:156	If two genes map more closely in one of the taxonomies, then their function is necessarily more closely related." ></td>
	<td class="line x" title="135:156	The majority of GO terms are in the molecular function taxonomy." ></td>
	<td class="line x" title="136:156	It is conceivable that genes that map more closely could be involved in the same cascade or participate in the same genetic regulatory network." ></td>
	<td class="line x" title="137:156	There is increasing interest in genetic networks (e.g. www.genome.ad.jp/kegg/ kegg2.html; http://ecocyc.org; http://us.expasy.org/tools/pathways; www.biocarta.com) and combining the ability to search and extract information from the literature with GO mapping could prove effective in elucidating the functional interactions of genes." ></td>
	<td class="line x" title="138:156	7 8 Conclusion Potential Knowledge Discovery To determine whether our automatic comparison of MEDLINE to OMIM based on SemGen predications might throw new light on gene-gene interactions, we examined predications found in the MEDLINE file that had no match in the OMIM file." ></td>
	<td class="line x" title="139:156	We searched the OMIM reports for information on the genes found in such predications to confirm that they were absent from the OMIM reports." ></td>
	<td class="line x" title="140:156	For example, while the OMIM report on colon cancer did not mention BARD1, the SemGen output for MEDLINE had 22) BARD1|INTERACT_WITH|hmsh2 The abstract containing this predication (PMID 11498787) asserts that the BARD1 gene (LocusID 580) interacts with the breast cancer gene BRCA1 as well as with hMSH2, a mismatch repair gene associated with colon cancer." ></td>
	<td class="line x" title="141:156	BARD1 shares homology with the two conserved regions of BRCA1 and also interacts with the N-terminal region of BRCA1." ></td>
	<td class="line x" title="142:156	Interaction of BARD1 with BRCA1 could be essential for the function of BRCA1 in tumor suppression." ></td>
	<td class="line x" title="143:156	Conversely, disruption of this interaction may possibly contribute to the process of oncogenesis." ></td>
	<td class="line x" title="144:156	It has been reported that the BRCA1/BARD1 complex is responsible for many of the tumor suppression activities of BRCA1 (Baer and Ludwig 2002)." ></td>
	<td class="line x" title="145:156	The gene hMSH2 (LocusID 4436) is one of a number of genes that, when mutated, predisposes to colon cancer type 1." ></td>
	<td class="line x" title="146:156	It is the human homolog of the bacterial mismatch repair gene mutS." ></td>
	<td class="line x" title="147:156	We hypothesize that the interaction of BARD1 with hMSH2, in a similar fashion to BRCA1, may be necessary for tumor suppression." ></td>
	<td class="line x" title="148:156	Disruption of this interaction may increase the likelihood of developing colon cancer." ></td>
	<td class="line x" title="149:156	Furthermore, this observation serves to point toward a possible link between BRCA1 and colon cancer." ></td>
	<td class="line x" title="150:156	We have extended earlier work with SemGen (Rindflesch et al. 2003) and are now able to extract from text, in addition to names of gene and disorders, genedisorder and gene-gene relations." ></td>
	<td class="line x" title="151:156	Although SemGen is not at a stage where it can be used indiscriminately and without selective review and evaluation, it may nevertheless prove useful for reviewers by providing an efficient means of scanning a large number of references and extracting relations involving genes and diseases." ></td>
	<td class="line x" title="152:156	The process of curation and review is time consuming." ></td>
	<td class="line x" title="153:156	Given the rate at which new publications are added to the scientific literature, the availability of tools for accelerating the review process would meet a real need." ></td>
	<td class="line x" title="154:156	As demonstrated by our pilot study on six disorders, SemGen could prove useful, even at this prototype stage, in extracting relevant information from the literature concerning genes and diseases." ></td>
	<td class="line x" title="155:156	Additionally, the ability to scan and extract information from diverse scientific domains could play an important role in identifying new relationships between genes and diseases that would promote hypothesis-generation and advance scientific research." ></td>
	<td class="line x" title="156:156	Even with the present limitations, SemGen could assist in making the scientific literature more accessible and reduce the time it takes for researchers to update their knowledge and expertise." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="H05-1052
Unsupervised Large-Vocabulary Word Sense Disambiguation With Graph-Based Algorithms For Sequence Data Labeling
Mihalcea, Rada;"></td>
	<td class="line x" title="1:160	Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 411418, Vancouver, October 2005." ></td>
	<td class="line x" title="2:160	c2005 Association for Computational Linguistics Unsupervised Large-Vocabulary Word Sense Disambiguation with Graph-based Algorithms for Sequence Data Labeling Rada Mihalcea Department of Computer Science University of North Texas rada@cs.unt.edu Abstract This paper introduces a graph-based algorithm for sequence data labeling, using random walks on graphs encoding label dependencies." ></td>
	<td class="line x" title="3:160	The algorithm is illustrated and tested in the context of an unsupervised word sense disambiguation problem, and shown to significantly outperform the accuracy achieved through individual label assignment, as measured on standard senseannotated data sets." ></td>
	<td class="line x" title="4:160	1 Introduction Many natural language processing tasks consist of labeling sequences of words with linguistic annotations, e.g. word sense disambiguation, part-of-speech tagging, named entity recognition, and others." ></td>
	<td class="line x" title="5:160	Typical labeling algorithms attempt to formulate the annotation task as a traditional learning problem, where the correct label is individually determined for each word in the sequence using a learning process, usually conducted independent of the labels assigned to the other words in the sequence." ></td>
	<td class="line x" title="6:160	Such algorithms do not have the ability to encode and thereby exploit dependencies across labels corresponding to the words in the sequence, which potentially limits their performance in applications where such dependencies can influence the selection of the correct set of labels." ></td>
	<td class="line x" title="7:160	In this paper, we introduce a graph-based sequence data labeling algorithm well suited for such natural language annotation tasks." ></td>
	<td class="line x" title="8:160	The algorithm simultaneously annotates all the words in a sequence by exploiting relations identified among word labels, using random walks on graphs encoding label dependencies." ></td>
	<td class="line x" title="9:160	The random walks are mathematically modeled through iterative graph-based algorithms, which are applied on the label graph associated with the given sequence of words, resulting in a stationary distribution over label probabilities." ></td>
	<td class="line x" title="10:160	These probabilities are then used to simultaneously select the most probable set of labels for the words in the input sequence." ></td>
	<td class="line x" title="11:160	The annotation method is illustrated and tested on an unsupervised word sense disambiguation problem, targeting the annotation of all open-class words in unrestricted text using information derived exclusively from dictionary definitions." ></td>
	<td class="line x" title="12:160	The graph-based sequence data labeling algorithm significantly outperforms the accuracy achieved through individual data labeling, resulting in an error reduction of 10.7%, as measured on standard sense-annotated data sets." ></td>
	<td class="line x" title="13:160	The method is also shown to exceed the performance of other previously proposed unsupervised word sense disambiguation algorithms." ></td>
	<td class="line x" title="14:160	2 Iterative Graphical Algorithms for Sequence Data Labeling In this section, we introduce the iterative graphical algorithm for sequence data labeling." ></td>
	<td class="line x" title="15:160	The algorithm is succinctly illustrated using a sample sequence for a generic annotation problem, with a more extensive illustration and evaluation provided in Section 3." ></td>
	<td class="line x" title="16:160	Given a sequence of words W = {w1,w2,,wn}, each word wi with corresponding admissible labels Lwi = {l1wi,l2wi,,lNwiwi }, we define a label graph G = (V,E) such that there is a vertex v  V for every possible label ljwi, i = 1n, j = 1Nwi." ></td>
	<td class="line x" title="17:160	Dependencies between pairs of labels are represented as directed or indirected edges e  E, defined over the set of vertex pairs V  V. Such label dependencies can be learned from annotated data, or derived by other means, as illustrated later." ></td>
	<td class="line x" title="18:160	Figure 1 shows an example of a graph411 1w 1 2 3 2 1 1 1 2 4 w 2 w 3 w 4 w1 w1 3 w1l l l w2 w2 l l w3l w4 w4 w4 w4l l l l 1.1 0.4 0.2 0.5 0.2 0.1 1.3 0.9 0.6 0.7 1.6 [1.12] [1.39] [0.86] [1.13] [1.38] [1.56] [0.40] [1.05] [0.58] [0.48] Figure 1: Sample graph built on the set of possible labels (shaded nodes) for a sequence of four words (unshaded nodes)." ></td>
	<td class="line x" title="19:160	Label dependencies are indicated as edge weights." ></td>
	<td class="line x" title="20:160	Scores computed by the graph-based algorithm are shown in brackets, next to each label." ></td>
	<td class="line x" title="21:160	ical structure derived over the set of labels for a sequence of four words." ></td>
	<td class="line x" title="22:160	Note that the graph does not have to be fully connected, as not all label pairs can be related by a dependency." ></td>
	<td class="line x" title="23:160	Given such a label graph associated with a sequence of words, the likelihood of each label can be recursively determined using an iterative graph-based ranking algorithm, which runs over the graph of labels and identifies the importance of each label (vertex) in the graph." ></td>
	<td class="line x" title="24:160	The iterative graphical algorithm is modeling a random walk, leading to a stationary distribution over label probabilities, represented as scores attached to vertices in the graph." ></td>
	<td class="line x" title="25:160	These scores are then used to identify the most probable label for each word, resulting in the annotation of all the words in the input sequence." ></td>
	<td class="line x" title="26:160	For instance, for the graph drawn in Figure 1, the word w1 will be assigned with label l1w1, since the score associated with this label (1.39) is the maximum among the scores assigned to all admissible labels associated with this word." ></td>
	<td class="line x" title="27:160	A remarkable property that makes these iterative graphical algorithms appealing for sequence data labeling is the fact that they take into account global information recursively drawn from the entire graph, rather than relying on local vertex-specific information." ></td>
	<td class="line x" title="28:160	Through the random walk performed on the label graph, these iterative algorithms attempt to collectively exploit the dependencies drawn between all labels in the graph, which makes them superior to other approaches that rely only on local information, individually derived for each word in the sequence." ></td>
	<td class="line x" title="29:160	2.1 Graph-based Ranking The basic idea implemented by an iterative graphbased ranking algorithm is that of voting or recommendation." ></td>
	<td class="line x" title="30:160	When one vertex links to another one, it is basically casting a vote for that other vertex." ></td>
	<td class="line x" title="31:160	The higher the number of votes that are cast for a vertex, the higher the importance of the vertex." ></td>
	<td class="line x" title="32:160	Moreover, the importance of the vertex casting a vote determines how important the vote itself is, and this information is also taken into account by the ranking algorithm." ></td>
	<td class="line x" title="33:160	While there are several graph-based ranking algorithms previously proposed in the literature, we focus on only one such algorithm, namely PageRank (Brin and Page, 1998), as it was previously found successful in a number of applications, including Web link analysis, social networks, citation analysis, and more recently in several text processing applications." ></td>
	<td class="line x" title="34:160	Given a graph G = (V,E), let In(Va) be the set of vertices that point to vertex Va (predecessors), and let Out(Va) be the set of vertices that vertex Va points to (successors)." ></td>
	<td class="line x" title="35:160	The PageRank score associated with the vertex Va is then defined using a recursive function that integrates the scores of its predecessors: P(Va) = (1  d) + d  summationdisplay VbIn(Va) P(Vb) |Out(Vb)| (1) where d is a parameter that is set between 0 and 11." ></td>
	<td class="line x" title="36:160	This vertex scoring scheme is based on a random walk model, where a walker takes random steps on the graph G, with the walk being modeled as a Markov process  that is, the decision on what edge to follow is solely based on the vertex where the walker is currently located." ></td>
	<td class="line x" title="37:160	Under certain conditions, this model converges to a stationary distribution of probabilities, associated with vertices in the graph." ></td>
	<td class="line x" title="38:160	Based on the Ergodic theorem for Markov chains (Grimmett and Stirzaker, 1989), the algorithm is guaranteed to converge if the graph is both aperiodic and irreducible." ></td>
	<td class="line x" title="39:160	The first condition is achieved for any graph that is a non-bipartite graph, while the second condition holds for any strongly connected graph  property achieved by PageRank through the random jumps introduced by the (1  d) factor." ></td>
	<td class="line x" title="40:160	In matrix notation, the PageRank vector of stationary probabilities is the principal eigenvector for the matrix Arow, which is obtained from the adjacency matrix A representing the graph, with all rows normalized to sum to 1: (P = ATrowP)." ></td>
	<td class="line x" title="41:160	Intuitively, the stationary probability associated with a vertex in the graph represents the probability 1The typical value for d is 0.85 (Brin and Page, 1998), and this is the value we are also using in our implementation." ></td>
	<td class="line x" title="42:160	412 of finding the walker at that vertex during the random walk, and thus it represents the importance of the vertex within the graph." ></td>
	<td class="line x" title="43:160	In the context of sequence data labeling, the random walk is performed on the label graph associated with a sequence of words, and thus the resulting stationary distribution of probabilities can be used to decide on the most probable set of labels for the given sequence." ></td>
	<td class="line x" title="44:160	2.2 Ranking on Weighted Graphs In a weighted graph, the decision on what edge to follow during a random walk is also taking into account the weights of outgoing edges, with a higher likelihood of following an edge that has a larger weight." ></td>
	<td class="line x" title="45:160	The weighted version of the ranking algorithm is particularly useful for sequence data labeling, since the dependencies between pairs of labels are more naturally modeled through weights indicating their strength, rather than binary 0/1 values." ></td>
	<td class="line x" title="46:160	Given a set of weights wab associated with edges connecting vertices Va and Vb, the weighted PageRank score is determined as: WP(Va) = (1d)+d summationdisplay VbIn(Va) wbasummationtext VcOut(Vb) wbc WP(Vb) (2) 2.3 Algorithm for Sequence Data Labeling Given a sequence of words with their corresponding admissible labels, the algorithm for sequence data labeling seeks to identify a graph of label dependencies on which a random walk can be performed, resulting in a set of scores that can be used for label assignment." ></td>
	<td class="line x" title="47:160	Algorithm 1 shows the pseudocode for the labeling process." ></td>
	<td class="line x" title="48:160	The algorithm consists of three main steps: (1) construction of label dependencies graph; (2) label scoring using graph-based ranking algorithms; (3) label assignment." ></td>
	<td class="line x" title="49:160	First, a weighted graph of label dependencies is built by adding a vertex for each admissible label, and an edge for each pair of labels for which a dependency is identified." ></td>
	<td class="line x" title="50:160	A maximum allowable distance can be set (MaxDist), indicating a constraint over the distance between words for which a label dependency is sought." ></td>
	<td class="line x" title="51:160	For instance, if MaxDist is set to 3, no edges will be drawn between labels corresponding to words that are more than three words apart, counting all running words." ></td>
	<td class="line x" title="52:160	Label dependencies are determined through the Dependency function, whose definition depends on the application and type of resources available (see Section 2.4)." ></td>
	<td class="line x" title="53:160	Next, scores are assigned to vertices using a graphbased ranking algorithm." ></td>
	<td class="line x" title="54:160	Current experiments are Algorithm 1 Graph-based Sequence Data Labeling Input: Sequence W ={wi|i = 1N} Input: Admissible labels Lwi ={ltwi|t = 1Nwi},i = 1N Output: Sequence of labels L ={lwi|i = 1N}, with label lwi corresponding to word wi from the input sequence." ></td>
	<td class="line x" title="55:160	Build graph G of label dependencies 1: for i = 1 to N do 2: for j = i + 1 to N do 3: if ji > MaxDist then 4: break 5: end if 6: for t = 1 to Nwi do 7: for s = 1 to Nwj do 8: weightDependency(ltwi,lswj,wi,wj) 9: if weight > 0 then 10: AddEdge(G,ltwi,lswj,weight) 11: end if 12: end for 13: end for 14: end for 15: end for Score vertices in G 1: repeat 2: for all Va V ertices(G) do 3: WP(Va) = (1d) + dsummationtext VbIn(Va) wbaWP(Vb)/ summationtext VcOut(Vb) wbc 4: end for 5: until convergence of scores WP(Va) Label assignment 1: for i = 1 to N do 2: lwi argmaxWP(ltwi)|t = 1Nwi} 3: end for based on PageRank, but other ranking algorithms can be used as well." ></td>
	<td class="line x" title="56:160	Finally, the most likely set of labels is determined by identifying for each word the label that has the highest score." ></td>
	<td class="line x" title="57:160	Note that all admissible labels corresponding to the words in the input sequence are assigned with a score, and thus the selection of two or more most likely labels for a word is also possible." ></td>
	<td class="line x" title="58:160	2.4 Label Dependencies Label dependencies can be defined in various ways, depending on the application at hand and on the knowledge sources that are available." ></td>
	<td class="line x" title="59:160	If an annotated corpus is available, dependencies can be defined as label co-occurrence probabilities approximated with frequency counts P(ltwi,lswj ), or as conditional probabilities P(ltwi|lswj )." ></td>
	<td class="line x" title="60:160	Optionally, these dependencies can be lexicalized by taking into account the corresponding words in the sequence, e.g. P(ltwi|lswj )  P(wi|ltwi)." ></td>
	<td class="line oc" title="61:160	In the absence of an annotated corpus, dependencies can be derived by other means, e.g. part413 of-speech probabilities can be approximated from a raw corpus as in (Cutting et al. , 1992), word-sense dependencies can be derived as definition-based similarities, etc. Label dependencies are set as weights on the arcs drawn between corresponding labels." ></td>
	<td class="line x" title="62:160	Arcs can be directed or undirected for joint probabilities or similarity measures, and are usually directed for conditional probabilities." ></td>
	<td class="line x" title="63:160	2.5 Labeling Example Consider again the example from Figure 1, consisting of a sequence of four words, and their possible corresponding labels." ></td>
	<td class="line x" title="64:160	In the first step of the algorithm, label dependencies are determined, and let us assume that the values for these dependencies are as indicated through the edge weights in Figure 1." ></td>
	<td class="line x" title="65:160	Next, vertices in the graph are scored using an iterative ranking algorithm, resulting in a score attached to each label, shown in brackets next to each vertex." ></td>
	<td class="line x" title="66:160	Finally, the most probable label for each word is selected." ></td>
	<td class="line x" title="67:160	Word w1 is thus assigned with label l1w1, since the score of this label (1.39) is the maximum among the scores associated with all its possible labels (1.39, 1.12, 0.86)." ></td>
	<td class="line x" title="68:160	Similarly, word w2 is assigned with label l2w2, w3 with label l1w3, and w4 receives label l2w4." ></td>
	<td class="line x" title="69:160	2.6 Efficiency Considerations For a sequence of words W = {w1,w2,,wn}, each word wi with Nwi admissible labels, the running time of the graph-based sequence data labeling algorithm is proportional with O(C nsummationtext i=1 i+MaxDistsummationtext j=i+1 (Nwi  Nwj)) (the time spent in building the label graph and iterating the algorithm for a constant number of times C)." ></td>
	<td class="line x" title="70:160	This is order of magnitudes better than the running time of O( nproducttext i=1 Nwi) for algorithms that attempt to select the best sequence of labels by searching through the entire space of possible label combinations, although it can be significantly higher than the running time of O( nsummationtext i=1 Nwi) for individual data labeling." ></td>
	<td class="line x" title="71:160	2.7 Other Algorithms for Sequence Data Labeling It is interesting to contrast our algorithm with previously proposed models for sequence data labeling, e.g. Hidden Markov Models, Maximum Entropy Markov Models, or Conditional Random Fields." ></td>
	<td class="line x" title="72:160	Although they differ in the model used (generative, discriminative, or dual), and the type of probabilities involved (joint or conditional), these previous algorithms are all parameterized algorithms that typically require parameter training through maximization of likelihood on training examples." ></td>
	<td class="line x" title="73:160	In these models, parameters that maximize sequence probabilities are learned from a corpus during a training phase, and then applied to the annotation of new unseen data." ></td>
	<td class="line x" title="74:160	Instead, in the algorithm proposed in this paper, the likelihood of a sequence of labels is determined during test phase, through random walks performed on the label graph built for the data to be annotated." ></td>
	<td class="line x" title="75:160	While current evaluations of our algorithm are performed on an unsupervised labeling task, future work will consider the evaluation of the algorithm in the presence of an annotated corpus, which will allow for direct comparison with these previously proposed models for sequence data labeling." ></td>
	<td class="line x" title="76:160	3 Experiments in Word Sense Disambiguation The algorithm for sequence data labeling is illustrated and tested on an all-words word sense disambiguation problem." ></td>
	<td class="line x" title="77:160	Word sense disambiguation is a labeling task consisting of assigning the correct meaning to each open-class word in a sequence (usually a sentence)." ></td>
	<td class="line x" title="78:160	Most of the efforts for solving this problem were concentrated so far toward targeted supervised learning, where each sense tagged occurrence of a particular word is transformed into a feature vector used in an automatic learning process." ></td>
	<td class="line x" title="79:160	The applicability of such supervised algorithms is however limited to those few words for which sense tagged data is available, and their accuracy is strongly connected to the amount of labeled data available at hand." ></td>
	<td class="line x" title="80:160	Instead, algorithms that attempt to disambiguate all-words in unrestricted text have received significantly less attention, as the development and success of such algorithms has been hindered by both (a) lack of resources (training data), and (b) efficiency aspects resulting from the large size of the problem." ></td>
	<td class="line x" title="81:160	3.1 Graph-based Sequence Data Labeling for Unsupervised Word Sense Disambiguation To apply the graph-based sequence data labeling algorithm to the disambiguation of an input text, we need information on labels (word senses) and dependencies (word sense dependencies)." ></td>
	<td class="line x" title="82:160	Word senses can be easily obtained from any sense inventory, e.g. WordNet or LDOCE." ></td>
	<td class="line x" title="83:160	Sense dependencies can be derived in various ways, depending on the type of resources available for the language and/or domain at hand." ></td>
	<td class="line x" title="84:160	In this paper, we explore the unsupervised derivation of sense 414 dependencies using information drawn from machine readable dictionaries, which is general and can be applied to any language or domain for which a sense inventory is available." ></td>
	<td class="line x" title="85:160	Relying exclusively on a machine readable dictionary, a sense dependency can be defined as a measure of similarity between word senses." ></td>
	<td class="line x" title="86:160	There are several metrics that can be used for this purpose, see for instance (Budanitsky and Hirst, 2001) for an overview." ></td>
	<td class="line x" title="87:160	However, most of them rely on measures of semantic distance computed on semantic networks, and thus they are limited by the availability of explicitly encoded semantic relations (e.g. is-a, part-of)." ></td>
	<td class="line x" title="88:160	To maintain the unsupervised aspect of the algorithm, we chose instead to use a measure of similarity based on sense definitions, which can be computed on any dictionary, and can be evaluated across different parts-ofspeech." ></td>
	<td class="line x" title="89:160	Given two word senses and their corresponding definitions, the sense similarity is determined as a function of definition overlap, measured as the number of common tokens between the two definitions, after running them through a simple filter that eliminates all stop-words." ></td>
	<td class="line x" title="90:160	To avoid promoting long definitions, we also use a normalization factor, and divide the content overlap of the two definitions with the length of each definition." ></td>
	<td class="line x" title="91:160	This sense similarity measure is inspired by the definition of the Lesk algorithm (Lesk, 1986)." ></td>
	<td class="line x" title="92:160	Starting with a sense inventory and a function for computing sense dependencies, the application of the sequence data labeling algorithm to the unsupervised disambiguation of a new text proceeds as follows." ></td>
	<td class="line x" title="93:160	First, for the given text, a label graph is built by adding a vertex for each possible sense for all openclass words in the text." ></td>
	<td class="line x" title="94:160	Next, weighted edges are drawn using the definition-based semantic similarity measure, computed for all pairs of senses for words found within a certain distance (MaxDist, as defined in Algorithm 1)." ></td>
	<td class="line x" title="95:160	Once the graph is constructed, the graph-based ranking algorithm is applied, and a score is determined for all word senses in the graph." ></td>
	<td class="line x" title="96:160	Finally, for each open-class word in the text, we select the vertex in the label graph which has the highest score, and label the word with the corresponding word sense." ></td>
	<td class="line x" title="97:160	3.2 An Example Consider the task of assigning senses to the words in the text The church bells no longer rung on Sundays2." ></td>
	<td class="line x" title="98:160	For the purpose of illustration, we assume at 2Example drawn from the data set provided during the SENSEVAL-2 English all-words task." ></td>
	<td class="line x" title="99:160	Manual sense annotations The church bells no longer rung on Sundays." ></td>
	<td class="line x" title="100:160	church 1: one of the groups of Christians who have their own beliefs and forms of worship 2: a place for public (especially Christian) worship 3: a service conducted in a church bell 1: a hollow device made of metal that makes a ringing sound when struck 2: a push button at an outer door that gives a ringing or buzzing signal when pushed 3: the sound of a bell ring 1: make a ringing sound 2: ring or echo with sound 3: make (bells) ring, often for the purposes of musical edification Sunday 1: first day of the week; observed as a day of rest and worship by most Christians bell ring [1.46] [0.99] [0.96] [2.56] [0.63] [0.58] [0.42] [0.67] Sundaychurch S2 S1 s3 s2 s3 s2 S3 s1 S1s1 0.35 0.501.06 0.40 0.19 0.34 1.01 0.55 [0.73] 0.30 [0.93] 0.35 0.31 0.80 0.85 0.23 Figure 2: The label graph for assigning senses to words in the sentence The church bells no longer rung on Sundays." ></td>
	<td class="line x" title="101:160	most three senses for each word, which are shown in Figure 2." ></td>
	<td class="line x" title="102:160	Word senses and definitions are obtained from the WordNet sense inventory (Miller, 1995)." ></td>
	<td class="line x" title="103:160	All word senses are added as vertices in the label graph, and weighted edges are drawn as dependencies among word senses, derived using the definition-based similarity measure (no edges are drawn between word senses with a similarity of zero)." ></td>
	<td class="line x" title="104:160	The resulting label graph is an undirected weighted graph, as shown in Figure 2." ></td>
	<td class="line x" title="105:160	After running the ranking algorithm, scores are identified for each word-sense in the graph, indicated between brackets next to each node." ></td>
	<td class="line x" title="106:160	Selecting for each word the sense with the largest score results in the following sense assignment: The church#2 bells#1 were also made available for this data." ></td>
	<td class="line x" title="107:160	415 no longer rung#3 on Sundays#1, which is correct according to annotations performed by professional lexicographers." ></td>
	<td class="line x" title="108:160	3.3 Results and Discussion The algorithm was primarily evaluated on the SENSEVAL-2 English all-words data set, consisting of three documents from Penn Treebank, with 2,456 open-class words (Palmer et al. , 2001)." ></td>
	<td class="line x" title="109:160	Unlike other sense-annotated data sets, e.g. SENSEVAL-3 or SemCor, SENSEVAL-2 is the only testbed for all-words word sense disambiguation that includes a sense map, which allows for additional coarse-grained sense evaluations." ></td>
	<td class="line x" title="110:160	Moreover, there is a larger body of previous work that was evaluated on this data set, which can be used as a base of comparison." ></td>
	<td class="line x" title="111:160	The performance of our algorithm is compared with the disambiguation accuracy obtained with a variation of the Lesk algorithm3 (Lesk, 1986), which selects the meaning of an open-class word by finding the word sense that leads to the highest overlap between the corresponding dictionary definition and the current context." ></td>
	<td class="line x" title="112:160	Similar to the definition similarity function used in the graph-based disambiguation algorithm (Section 3.1), the overlap measure used in the Lesk implementation does not take into account stop-words, and it is normalized with the length of each definition to avoid promoting longer definitions." ></td>
	<td class="line x" title="113:160	We are thus comparing the performance of sequence data labeling, which takes into account label dependencies, with individual data labeling, where a label is selected independent of the other labels in the text." ></td>
	<td class="line x" title="114:160	Note that both algorithms rely on the same knowledge source, i.e. dictionary definitions, and thus they are directly comparable." ></td>
	<td class="line x" title="115:160	Moreover, none of the algorithms take into account the dictionary sense order (e.g. the most frequent sense provided by WordNet), and therefore they are both fully unsupervised." ></td>
	<td class="line x" title="116:160	Table 1 shows precision and recall figures4 for a 3Given a sequence of words, the original Lesk algorithm attempts to identify the combination of word senses that maximizes the redundancy (overlap) across all corresponding definitions." ></td>
	<td class="line x" title="117:160	The algorithm was later improved through a method for simulated annealing (Cowie et al. , 1992), which solved the combinatorial explosion of word senses, while still finding an optimal solution." ></td>
	<td class="line x" title="118:160	However, recent comparative evaluations of different variants of the Lesk algorithm have shown that the performance of the original algorithm is significantly exceeded by an algorithm variation that relies on the overlap between word senses and current context (Vasilescu et al. , 2004)." ></td>
	<td class="line x" title="119:160	We are thus using this latter Lesk variant in our implementation." ></td>
	<td class="line x" title="120:160	4Recall is particularly low for each individual part-of-speech because it is calculated with respect to the entire data set." ></td>
	<td class="line x" title="121:160	The overall precision and recall figures coincide, reflecting the 100% coverage of the algorithm." ></td>
	<td class="line x" title="122:160	context size (MaxDist) equal to the length of each sentence, using: (a) sequence data labeling with iterative graph-based algorithms; (b) individual data labeling with a version of the Lesk algorithm; (c) random baseline." ></td>
	<td class="line x" title="123:160	Evaluations are run for both fine-grained and coarse-grained sense distinctions, to determine the algorithm performance under different classification granularities." ></td>
	<td class="line x" title="124:160	The accuracy of the graph-based sequence data labeling algorithm exceeds by a large margin the individual data labeling algorithm, resulting in 10.7% error rate reduction for fine-grained sense distinctions, which is statistically significant (p < 0.0001, paired t-test)." ></td>
	<td class="line x" title="125:160	Performance improvements are equally distributed across all parts-of-speech, with comparable improvements obtained for nouns, verbs, and adjectives." ></td>
	<td class="line x" title="126:160	A similar error rate reduction of 11.0% is obtained for coarse-grained sense distinctions, which suggests that the performance of the graph-based sequence data labeling algorithm does not depend on classification granularity, and similar improvements over individual data labeling can be obtained regardless of the average number of labels per word." ></td>
	<td class="line x" title="127:160	We also measured the variation of performance with context size, and evaluated the disambiguation accuracy for both algorithms for a window size ranging from two words to an entire sentence." ></td>
	<td class="line x" title="128:160	The window size parameter limits the number of surrounding words considered when seeking label dependencies (sequence data labeling), or the words counted in the measure of definitioncontext overlap (individual data labeling)." ></td>
	<td class="line x" title="129:160	Figure 3 plots the disambiguation accuracy of the two algorithms as a function of context size." ></td>
	<td class="line x" title="130:160	As seen in the figure, both algorithms benefit from larger contexts, with a steady increase in performance observed for increasingly larger window sizes." ></td>
	<td class="line x" title="131:160	Although the initial growth observed for the sequence data labeling algorithm is somewhat sharper, the gap between the two curves stabilizes for window sizes larger than five words, which suggests that the improvement in performance achieved with sequence data labeling over individual data labeling does not depend on the size of available context." ></td>
	<td class="line x" title="132:160	The algorithm was also evaluated on two other data sets, SENSEVAL-3 English all-words data (Snyder and Palmer, 2004) and a subset of SemCor (Miller et al. , 1993), although only fine-grained sense evaluations could be conducted on these test sets." ></td>
	<td class="line x" title="133:160	The disambiguation precision on the SENSEVAL-3 data was measured at 52.2% using sequence data labeling, compared to 48.1% obtained with individual 416 Fine-grained sense distinctions Coarse-grained sense distinctions Random Individual Sequence Random Individual Sequence Part-of baseline (Lesk) (graph-based) baseline (Lesk) (graph-based) speech P R P R P R P R P R P R Noun 41.4% 19.4% 50.3% 23.6% 57.5% 27.0% 42.7% 20.0% 51.4% 24.1% 58.8% 27.5% Verb 20.7% 3.9% 30.5% 5.7% 36.5% 6.9% 22.8% 4.3% 31.9% 6.0% 37.9% 7.1% Adjective 41.3% 9.3% 49.1% 11.0% 56.7% 12.7% 42.6% 42.6% 49.8% 11.2% 57.6% 12.9% Adverb 44.6% 5.2% 64.6% 7.6% 70.9% 8.3% 40.7% 4.8% 65.3% 7.7% 71.9% 8.5% ALL 37.9% 37.9% 48.7% 48.7% 54.2% 54.2% 38.7% 38.7% 49.8% 49.8% 55.3% 55.3% Table 1: Precision and recall for graph-based sequence data labeling, individual data labeling, and random baseline, for fine-grained and coarse-grained sense distinctions." ></td>
	<td class="line x" title="134:160	35 40 45 50 55 60 0 5 10 15 20 25 30 Disambiguation precision (%) Window size sequence individual random Figure 3: Disambiguation results using sequence data labeling, individual labeling, and random baseline, for various context sizes." ></td>
	<td class="line x" title="135:160	data labeling, and 34.3% achieved through random sense assignment." ></td>
	<td class="line x" title="136:160	The average disambiguation figure obtained on all the words in a random subset of 10 SemCor documents, covering different domains, was 56.5% for sequence data labeling, 47.4% for individual labeling, and 35.3% for the random baseline." ></td>
	<td class="line x" title="137:160	Comparison with Related Work For a given sequence of ambiguous words, the original definition of the Lesk algorithm (Lesk, 1986), and more recent improvements based on simulated annealing (Cowie et al. , 1992), seek to identify the combination of senses that maximizes the overlap among their dictionary definitions." ></td>
	<td class="line x" title="138:160	Tests performed with this algorithm on the SENSEVAL-2 data set resulted in a disambiguation accuracy of 39.5%." ></td>
	<td class="line x" title="139:160	This precision is exceeded by the Lesk algorithm variation used in the experiments reported in this paper, which measures the overlap between sense definitions and the current context, for a precision of 48.7% on the same data set (see Table 1)." ></td>
	<td class="line x" title="140:160	In the SENSEVAL-2 evaluations, the best performing fully unsupervised algorithm5 was developed by (Litkowski, 2001), who combines analysis of multiword units and contextual clues based on collocations and content words from dictionary definitions and examples, for an overall precision and recall of 45.1%." ></td>
	<td class="line x" title="141:160	More recently, (McCarthy et al. , 2004) reports one of the best results on the SENSEVAL-2 data set, using an algorithm that automatically derives the most frequent sense for a word using distributional similarities learned from a large raw corpus, for a disambiguation precision of 53.0% and a recall of 49.0%." ></td>
	<td class="line x" title="142:160	Another related line of work consists of the disambiguation algorithms based on lexical chains (Morris and Hirst, 1991), and the more recent improvements reported in (Galley and McKeown, 2003)  where threads of meaning are identified throughout a text." ></td>
	<td class="line x" title="143:160	Lexical chains however only take into account connections between concepts identified in a static way, without considering the importance of the concepts that participate in a relation, which is recursively determined in our algorithm." ></td>
	<td class="line x" title="144:160	Moreover, the construction of lexical chains requires structured dictionaries such as WordNet, with explicitly defined semantic relations between word senses, whereas our algorithm can also work with simple unstructured dictionaries that provide only word sense definitions." ></td>
	<td class="line x" title="145:160	(Galley and McKeown, 2003) evaluated their algorithm on the nouns from a subset of SEMCOR, reporting 62.09% disambiguation precision." ></td>
	<td class="line x" title="146:160	The performance of our algorithm on the same subset of SEMCOR nouns was measured at 64.2%6." ></td>
	<td class="line x" title="147:160	Finally, another disambiguation method relying on graph algorithms that exploit the 5Algorithms that integrate the most frequent sense in WordNet are not considered here, since this represents a supervised knowledge source (WordNet sense frequencies are derived from a sense-annotated corpus)." ></td>
	<td class="line x" title="148:160	6Note that the results are not directly comparable, since (Galley and McKeown, 2003) used the WordNet sense order to break the ties, whereas we assume that such sense order frequency is not available, and thus we break the ties through random choice." ></td>
	<td class="line x" title="149:160	417 structure of semantic networks was proposed in (Mihalcea et al. , 2004), with a disambiguation accuracy of 50.9% measured on all the words in the SENSEVAL-2 data set." ></td>
	<td class="line x" title="150:160	Although it relies exclusively on dictionary definitions, the graph-based sequence data labeling algorithm proposed in this paper, with its overall performance of 54.2%, exceeds significantly the accuracy of all these previously proposed unsupervised word sense disambiguation methods, proving the benefits of taking into account label dependencies when annotating sequence data." ></td>
	<td class="line x" title="151:160	An additional interesting benefit of the algorithm is that it provides a ranking over word senses, and thus the selection of two or more most probable senses for each word is also possible." ></td>
	<td class="line x" title="152:160	4 Conclusions We proposed a graphical algorithm for sequence data labeling that relies on random walks on graphs encoding label dependencies." ></td>
	<td class="line x" title="153:160	Through the label graphs it builds for a given sequence of words, the algorithm exploits relations between word labels, and implements a concept of recommendation." ></td>
	<td class="line x" title="154:160	A label recommends other related labels, and the strength of the recommendation is recursively computed based on the importance of the labels making the recommendation." ></td>
	<td class="line x" title="155:160	In this way, the algorithm simultaneously annotates all the words in an input sequence, by identifying the most probable (most recommended) set of labels." ></td>
	<td class="line x" title="156:160	The algorithm was illustrated and tested on an unsupervised word sense disambiguation problem, targeting the annotation of all words in unrestricted texts." ></td>
	<td class="line x" title="157:160	Through experiments performed on standard senseannotated data sets, the graph-based sequence data labeling algorithm was shown to significantly outperform the accuracy achieved through individual data labeling, resulting in a statistically significant error rate reduction of 10.7%." ></td>
	<td class="line x" title="158:160	The disambiguation method was also shown to exceed the performance of previously proposed unsupervised word sense disambiguation algorithms." ></td>
	<td class="line x" title="159:160	Moreover, comparative results obtained under various experimental settings have shown that the algorithm is robust to changes in classification granularity and context size." ></td>
	<td class="line x" title="160:160	Acknowledgments This work was partially supported by a National Science Foundation grant IIS-0336793." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W05-0708
POS Tagging Of Dialectal Arabic: A Minimally Supervised Approach
Duh, Kevin;Kirchhoff, Katrin;"></td>
	<td class="line x" title="1:204	Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 5562, Ann Arbor, June 2005." ></td>
	<td class="line x" title="2:204	c2005 Association for Computational Linguistics POS Tagging of Dialectal Arabic: A Minimally Supervised Approach Kevin Duh and Katrin Kirchhoff Department of Electrical Engineering University of Washington, Seattle, WA, 98195 {duh,katrin}@ee.washington.edu Abstract Natural language processing technology for the dialects of Arabic is still in its infancy, due to the problem of obtaining large amounts of text data for spoken Arabic." ></td>
	<td class="line x" title="3:204	In this paper we describe the development of a part-of-speech (POS) tagger for Egyptian Colloquial Arabic." ></td>
	<td class="line x" title="4:204	We adopt a minimally supervised approach that only requires raw text data from several varieties of Arabic and a morphological analyzer for Modern Standard Arabic." ></td>
	<td class="line x" title="5:204	No dialect-specific tools are used." ></td>
	<td class="line x" title="6:204	We present several statistical modeling and cross-dialectal data sharing techniques to enhance the performance of the baseline tagger and compare the results to those obtained by a supervised tagger trained on hand-annotated data and, by a state-ofthe-art Modern Standard Arabic tagger applied to Egyptian Arabic." ></td>
	<td class="line x" title="7:204	1 Introduction Part-of-speech (POS) tagging is a core natural language processing task that can benefit a wide range of downstream processing applications." ></td>
	<td class="line x" title="8:204	Tagging is often the first step towards parsing or chunking (Osborne, 2000; Koeling, 2000), and knowledge of POS tags can benefit statistical language models for speech recognition or machine translation (Heeman, 1998; Vergyri et al. , 2004)." ></td>
	<td class="line nc" title="9:204	Many approaches for POS tagging have been developed in the past, including rule-based tagging (Brill, 1995), HMM taggers (Brants, 2000; Cutting and others, 1992), maximum-entropy models (Rathnaparki, 1996), cyclic dependency networks (Toutanova et al. , 2003), memory-based learning (Daelemans et al. , 1996), etc. All of these approaches require either a large amount of annotated training data (for supervised tagging) or a lexicon listing all possible tags for each word (for unsupervised tagging)." ></td>
	<td class="line x" title="10:204	Taggers have been developed for a variety of languages, including Modern Standard Arabic (MSA) (Khoja, 2001; Diab et al. , 2004)." ></td>
	<td class="line x" title="11:204	Since large amount of text material as well as standard lexicons can be obtained in these cases, POS tagging is a straightforward task." ></td>
	<td class="line x" title="12:204	The dialects of Arabic, by contrast, are spoken rather than written languages." ></td>
	<td class="line x" title="13:204	Apart from small amounts of written dialectal material in e.g. plays, novels, chat rooms, etc. , data can only be obtained by recording and manually transcribing actual conversations." ></td>
	<td class="line x" title="14:204	Moreover, there is no universally agreed upon writing standard for dialects (though several standardization efforts are underway); any largescale data collection and transcription effort therefore requires extensive training of annotators to ensure consistency." ></td>
	<td class="line x" title="15:204	Due to this data acquisition bottleneck, the development of NLP technology for dialectal Arabic is still in its infancy." ></td>
	<td class="line x" title="16:204	In addition to the problems of sparse training data and lack of writing standards, tagging of dialectal Arabic is difficult for the following reasons:  Resources such as lexicons, morphological analyzers, tokenizers, etc. are scarce or nonexistent for dialectal Arabic." ></td>
	<td class="line x" title="17:204	 Dialectal Arabic is a spoken language." ></td>
	<td class="line x" title="18:204	Tagging spoken language is typically harder than tag55 ging written language, due to the effect of disfluencies, incomplete sentences, varied word order, etc.  The rich morphology of Arabic leads to a large number of possible word forms, which increases the number of out-of-vocabulary (OOV) words." ></td>
	<td class="line x" title="19:204	 The lack of short vowel information results in high lexical ambiguity." ></td>
	<td class="line x" title="20:204	In this paper we present an attempt at developing a POS tagger for dialectal Arabic in a minimally supervised way." ></td>
	<td class="line x" title="21:204	Our goal is to utilize existing resources and data for several varieties of Arabic in combination with unsupervised learning algorithms, rather than developing dialect-specific tools." ></td>
	<td class="line x" title="22:204	The resources available to us are the CallHome Egyptian Colloquial Arabic (ECA) corpus, the LDC Levantine Arabic (LCA) corpus, the LDC MSA Treebank corpus, and a generally available morphological analysis tool (the LDC-distributed Buckwalter stemmer) for MSA." ></td>
	<td class="line x" title="23:204	The target dialect is ECA, since this is the only dialectal corpus for which POS annotations are available." ></td>
	<td class="line x" title="24:204	Our general approach is to bootstrap the tagger in an unsupervised way using POS information from the morphological analyzer, and to subsequently improve it by integrating additional data from other dialects and by general machine learning techniques." ></td>
	<td class="line x" title="25:204	We compare the result against the performance of a tagger trained in a supervised way and an unsupervised tagger with a hand-developed ECA lexicon." ></td>
	<td class="line x" title="26:204	2 Data The ECA corpus consists of telephone conversations between family members or close friends, with one speaker being located in the U.S. and the other in Egypt." ></td>
	<td class="line x" title="27:204	We use the combined train, eval96 and hub5 sections of the corpus for training, the dev set for development and the eval97 set for evaluation." ></td>
	<td class="line x" title="28:204	The LCA data consists of telephone conversations on pre-defined topics between Levantine speakers previously unknown to each other; all of the available data was used." ></td>
	<td class="line x" title="29:204	The Treebank corpus is a collection of MSA newswire text from Agence France Press, An Nahar News, and Unmah Press." ></td>
	<td class="line x" title="30:204	We use parts 1 (v3.0), 2 (v2.0) and 3 (v1.0)." ></td>
	<td class="line x" title="31:204	The sizes of the various corpora are shown in Table 1." ></td>
	<td class="line x" title="32:204	The ECA corpus was originally transcribed in a romanized form; a script representation was then derived automatically from the romanized transcripts." ></td>
	<td class="line x" title="33:204	The script, therefore, does not entirely conform to the MSA standard: romanized forms may represent actual pronunciations and contain such MSA  ECA changes as //  /s/ or /t/ and //  /z/ or /d/." ></td>
	<td class="line x" title="34:204	The resulting representation cannot be unambiguously mapped back to MSA script; the variants /s/ or /t/, for instance, are represented by a0 or a1a2, rather than a3 a2." ></td>
	<td class="line x" title="35:204	This introduces additional noise into the data, but it also mimics the real-world situation of variable spelling standards that need to be handled by a robust NLP system." ></td>
	<td class="line x" title="36:204	We use the script representation of this corpus for all our experiments." ></td>
	<td class="line x" title="37:204	The ECA corpus is accompanied by a lexicon containing the morphological analysis of all words, i.e. an analysis in terms of stem and morphological characteristics such as person, number, gender, POS, etc. Since the analysis is based on the romanized form, a single tag can be assigned to the majority of words (75% of all tokens) in the corpus." ></td>
	<td class="line x" title="38:204	We use this assignment as the reference annotation for our experiments to evaluate the output of our tagger." ></td>
	<td class="line x" title="39:204	The remaining 25% tokens (ambiguous words) have 2 or more tags in the lexicon and are thus ignored during evaluation since the correct reference tag cannot be determined." ></td>
	<td class="line x" title="40:204	Both the LCA and the MSA Treebank data sets were transcribed in standard MSA script." ></td>
	<td class="line x" title="41:204	The LCA corpus only consists of raw orthographic transcriptions; no further annotations exist for this data set." ></td>
	<td class="line x" title="42:204	Each word in the Treebank corpus is associated with all of its possible POS tags; the correct tag has been marked manually." ></td>
	<td class="line x" title="43:204	We use the undecomposed word forms rather than the forms resulting from splitting off conjunctions, prepositions, and other clitics." ></td>
	<td class="line x" title="44:204	Although improved tokenization can be expected to result in better tagging performance, tokenization tools for dialectal Arabic are currently not available, and our goal was to create comparable conditions for tagging across all of our data sets." ></td>
	<td class="line x" title="45:204	Preprocessing of the data thus only included removing punctuation from the MSA data and removing word fragments from the spoken language corpora." ></td>
	<td class="line x" title="46:204	Other disfluencies (fillers and repetitions) were retained since they are likely to have predictive value." ></td>
	<td class="line x" title="47:204	Finally, singleton words (e.g. inconsistent spellings) were removed 56 from the LCA data." ></td>
	<td class="line x" title="48:204	The properties of the different data sets (number of words, n-grams, and ambiguous words) are displayed in Table 1." ></td>
	<td class="line x" title="49:204	ECA LCA MSA train dev test sentences 25k 6k 2.7k 114k 20k # tokens 156k 31k 12k 476k 552k # types 15k 5k 1.5k 16k 65k # bigrams 81k 20k 7k 180k 336k # trigrams 125k 26k 10k 320k 458k % ambig." ></td>
	<td class="line x" title="50:204	24.4 27.8 28.2   Table 1: Corpus statistics for ECA, LCA and MSA." ></td>
	<td class="line x" title="51:204	The only resource we utilize in addition to raw data is the LDC-distributed Buckwalter stemmer." ></td>
	<td class="line x" title="52:204	The stemmer analyzes MSA script forms and outputs all possible morphological analyses (stems and POS tags, as well as diacritizations) for the word." ></td>
	<td class="line x" title="53:204	The analysis is based on an internal stem lexicon combined with rules for affixation." ></td>
	<td class="line x" title="54:204	Although the stemmer was developed primarily for MSA, it can accommodate a certain percentage of dialectal words." ></td>
	<td class="line x" title="55:204	Table 2 shows the percentages of word types and tokens in the ECA and LCA corpora that received an analysis from the Buckwalter stemmer." ></td>
	<td class="line x" title="56:204	Since both sets contain dialectal as well as standard MSA forms, it is not possible to determine precisely how many of the unanalyzable forms are dialectal forms vs. words that were rejected for other reasons, such as misspellings." ></td>
	<td class="line x" title="57:204	The higher percentage of rejected word types in the ECA corpus is most likely due to the non-standard script forms described above." ></td>
	<td class="line x" title="58:204	Type Token N ECA LCA ECA LCA 0 37.6 23.3 18.2 28.2 1 34.0 52.5 33.6 40.4 2 19.4 17.7 26.4 19.9 3 7.2 5.2 16.2 10.5 4 1.4 1.0 5.0 2.3 5 0.1 0.1 0.4 0.6 Table 2: Percentage of word types/tokens with N possible tags, as determined by the Buckwalter stemmer." ></td>
	<td class="line x" title="59:204	Words with 0 tags are unanalyzable." ></td>
	<td class="line x" title="60:204	The POS tags used in the LDC ECA annotation and in the Buckwalter stemmer are rather finegrained; furthermore, they are not identical." ></td>
	<td class="line x" title="61:204	We therefore mapped both sets of tags to a unified, simpler tagset consisting only of the major POS categories listed in Table 2." ></td>
	<td class="line x" title="62:204	The mapping from the original Buckwalter tag to the simplified set was based on the conversion scheme suggested in (Bies, 2003)." ></td>
	<td class="line x" title="63:204	The same underlying conversion rules were applicable to most of the LDC tags; those cases that could not be determined automatically were converted by hand." ></td>
	<td class="line x" title="64:204	Symbol Gloss (%) CC coordinating conjunction 7.15 DT determiner 2.23 FOR foreign word 1.18 IN preposition 7.46 JJ adjective 6.02 NN noun 19.95 NNP proper noun 3.55 NNS non-singular noun 3.04 NOTAG non-word 0.05 PRP pronoun 5.85 RB adverb 4.13 RP particle 9.71 UH disfluency, interjection 9.55 VBD perfect verb 6.53 VBN passive verb 1.88 VBP imperfect verb 10.62 WP relative pronoun 1.08 Table 3: Collapsed tagset and percentage of occurrence of each tag in the ECA corpus." ></td>
	<td class="line x" title="65:204	Prior to the development of our tagger we computed the cross-corpus coverage of word n-grams in the ECA development set, in order to verify our assumption that utilizing data from other dialects might be helpful." ></td>
	<td class="line x" title="66:204	Table 4 demonstrates that the n-gram coverage of the ECA development set increases slightly by adding LCA and MSA data." ></td>
	<td class="line x" title="67:204	Types Tokens 1gr 2gr 3gr 1gr 2gr 3gr ECA 64 33 12 94 58 22 LCA 31 9 1.4 69 20 3.7 ECA + LCA 68 35 13 95 60 23 MSA 32 3.7 0.2 66 8.6 0.3 ECA + MSA 71 34 12 95 60 22 Table 4: Percentages of n-gram types and tokens in ECA dev set that are covered by the ECA training set, the LCA set, combined ECA training + LCA set, and MSA sets." ></td>
	<td class="line x" title="68:204	Note that adding the LCA or MSA improves the coverage slightly." ></td>
	<td class="line x" title="69:204	57 3 Baseline Tagger We use a statistical trigram tagger in the form of a hidden Markov model (HMM)." ></td>
	<td class="line x" title="70:204	Let w0:M be a sequence of words (w0,w1,,,wM ) and t0:M be the corresponding sequence of tags." ></td>
	<td class="line x" title="71:204	The trigram HMM computes the conditional probability of the word and tag sequence p(w0:M,t0:M) as: P(t0:M|w0:M) = Mproductdisplay i=0 p(wi|ti)p(ti|ti1,ti2) (1) The lexical model p(wi|ti) characterizes the distribution of words for a specific tag; the contextual model p(ti|ti1,ti2) is trigram model over the tag sequence." ></td>
	<td class="line x" title="72:204	For notational simplicity, in subsequent sections we will denote p(ti|ti1,ti2) as p(ti|hi), where hi represents the tag history." ></td>
	<td class="line x" title="73:204	The HMM is trained to maximize the likelihood of the training data." ></td>
	<td class="line x" title="74:204	In supervised training, both tag and word sequences are observed, so the maximum likelihood estimate is obtained by relative frequency counting, and, possibly, smoothing." ></td>
	<td class="line x" title="75:204	During unsupervised training, the tag sequences are hidden, and the Expectation-Maximization Algorithm is used to iteratively update probabilities based on expected counts." ></td>
	<td class="line x" title="76:204	Unsupervised tagging requires a lexicon specifying the set of possible tags for each word." ></td>
	<td class="line x" title="77:204	Given a test sentence wprime0:M, the Viterbi algorithm is used to find the tag sequence maximizing the probability of tags given words: t0:M = argmaxt0:Mp(t0:M|wprime0:M)." ></td>
	<td class="line x" title="78:204	Our taggers are implemented using the Graphical Models Toolkit (GMTK) (Bilmes and Zweig, 2002), which allows training a wide range of probabilistic models with both hidden and observed variables." ></td>
	<td class="line x" title="79:204	As a first step, we compare the performance of four different baseline systems on our ECA dev set." ></td>
	<td class="line x" title="80:204	First, we trained a supervised tagger on the MSA treebank corpus (System I), in order to gauge how a standard system trained on written Arabic performs on dialectal speech." ></td>
	<td class="line x" title="81:204	The second system (System II) is a supervised tagger trained on the manual ECA POS annotations." ></td>
	<td class="line x" title="82:204	System III is an unsupervised tagger on the ECA training set." ></td>
	<td class="line x" title="83:204	The lexicon for this system is derived from the reference annotations of the training set  thus, the correct tag is not known during training, but the lexicon is constrained by expert information." ></td>
	<td class="line x" title="84:204	The difference in accuracy between Systems II and III indicates the loss due to the unsupervised training method." ></td>
	<td class="line x" title="85:204	Finally, we trained a system using only the unannotated ECA data and a lexicon generated by applying the MSA analyzer to the training corpus and collecting all resulting tags for each word." ></td>
	<td class="line x" title="86:204	In this case, the lexicon is much less constrained; moreover, many words do not receive an output from the stemmer at all." ></td>
	<td class="line x" title="87:204	This is the training method with the least amount of supervision and therefore the method we are interested in most." ></td>
	<td class="line x" title="88:204	Table 5 shows the accuracies of the four systems on the ECA development set." ></td>
	<td class="line x" title="89:204	Also included is a breakdown of accuracy by analyzable (AW), unanalyzable (UW), and out-of-vocabulary (OOV) words." ></td>
	<td class="line x" title="90:204	Analyzable words are those for which the stemmer outputs possible analyses; unanalyzable words cannot be processed by the stemmer." ></td>
	<td class="line x" title="91:204	The percentage of unanalyzable word tokens in the dev set is 18.8%." ></td>
	<td class="line x" title="92:204	The MSA-trained tagger (System I) achieves an accuracy of 97% on a held-out set (117k words) of MSA data, but performs poorly on ECA due to a high OOV rate (43%)." ></td>
	<td class="line x" title="93:204	By contrast, the OOV rate for taggers trained on ECA data is 9.5%." ></td>
	<td class="line x" title="94:204	The minimally-supervised tagger (System IV) achieves a baseline accuracy of 62.76%." ></td>
	<td class="line x" title="95:204	In the following sections, we present several methods to improve this system, in order to approximate as closely as possible the performance of the supervised systems." ></td>
	<td class="line x" title="96:204	1 System Total AW UW OOV I 39.84 55.98 21.05 19.21 II 92.53 98.64 99.09 32.20 III 84.88 90.17 99.11 32.64 IV 62.76 67.07 20.74 21.84 Table 5: Tagging accuracy (%) for the 4 baseline systems." ></td>
	<td class="line x" title="97:204	AW = analyzable words, UW unanalyzable words, OOV = out-of-vocabulary words." ></td>
	<td class="line x" title="98:204	4 System Improvements 4.1 Adding Affix Features The low accuracy of unanalyzable and OOV words may significantly impact downstream applications." ></td>
	<td class="line x" title="99:204	1The accuracy of a naive tagger which labels all words with the most likely tag (NN) achieves an accuracy of 20%." ></td>
	<td class="line x" title="100:204	A tagger which labels words with the most likely tag amongst its possible tags achieves an accuracy of 52%." ></td>
	<td class="line x" title="101:204	58 One common way to improve accuracy is to add word features." ></td>
	<td class="line x" title="102:204	In particular, we are interested in features that can be derived automatically from the script form, such as affixes." ></td>
	<td class="line x" title="103:204	Affix features are added in a Naive Bayes fashion to the basic HMM model defined in Eq.1." ></td>
	<td class="line x" title="104:204	In addition to the lexical model p(wi|ti) we now have prefix and suffix models p(ai|ti) and p(bi|ti), where a and b are the prefix and suffix variables, respectively." ></td>
	<td class="line x" title="105:204	The affixes used are: a0a2a1 -, a3 a1 -, a4a5 -, a6a8a7 -, a9a10a7 -, a11a12a14a13 -, a11a12a16a15a17 -, a1a18 -, -a7a20a19, -a7a21a3a22a15a23, -a7a20a19a24, -a25a26a7, -a7a21a3 a27a28 . Affixes are derived for each word by simple substring matching." ></td>
	<td class="line x" title="106:204	More elaborate techniques are not used due to the philosophy of staying within a minimally supervised approach that does not require dialect-specific knowledge." ></td>
	<td class="line x" title="107:204	4.2 Constraining the Lexicon The quality of the lexicon has a major impact on unsupervised HMM training." ></td>
	<td class="line x" title="108:204	Banko et." ></td>
	<td class="line x" title="109:204	al." ></td>
	<td class="line x" title="110:204	(2004) demonstrated that tagging accuracy improves when the number of possible tags per word in a noisy lexicon can be restricted based on corpus frequency." ></td>
	<td class="line x" title="111:204	In the current setup, words that are not analyzable by the MSA stemmer are initally assigned all possible tags, with the exception of obvious restricted tags like the begin and end-of-sentence tags, NOTAG, etc. Our goal is to constrain the set of tags for these unanalyzable words." ></td>
	<td class="line x" title="112:204	To this end, we cluster both analyzable and unanalyzable words, and reduce the set of possible tags for unanalyzable words based on its cluster membership." ></td>
	<td class="line x" title="113:204	Several different clustering algorithms could in principle be used; here we utilize Browns clustering algorithm (Brown and others, 1992), which iteratively merges word clusters with high mutual information based on distributional criteria." ></td>
	<td class="line x" title="114:204	The tagger lexicon is then derived as follows: 1." ></td>
	<td class="line x" title="115:204	Generate K clusters of words from data." ></td>
	<td class="line x" title="116:204	2." ></td>
	<td class="line x" title="117:204	For each cluster C, calculate P(t|C) =summationtext wA,C P(t|w)P(w|C) where t and w are the word and tag, and A is the set of analyzable words." ></td>
	<td class="line x" title="118:204	3." ></td>
	<td class="line x" title="119:204	The clusters tagset is determined by choosing all tags tprime with P(tprime|C) above a certain threshold ." ></td>
	<td class="line x" title="120:204	4." ></td>
	<td class="line x" title="121:204	All unanalyzable words within this cluster are given these possible tags." ></td>
	<td class="line x" title="122:204	The number of clusters K and the threshold  are variables that affect the final tagset for unanalyzable words." ></td>
	<td class="line x" title="123:204	Using K = 200 and  = 0.05 for instance, the number of tags per unanalyzable word reduces to an average of four and ranges from one to eight tags." ></td>
	<td class="line x" title="124:204	There is a tradeoff regarding the degree of tagset reduction: choosing fewer tags results in less confusability but may also involve the removal of the correct tag from a words lexicon entry." ></td>
	<td class="line x" title="125:204	We did not optimize for K and  since an annotated development set for calculating accuracy is not available in a minimally supervised approach in practice." ></td>
	<td class="line x" title="126:204	Nevertheless, we have observed that tagset reduction generally leads to improvements compared to the baseline system with an unconstrained lexicon." ></td>
	<td class="line x" title="127:204	The improvements gained from adding affix features to System IV and constraining the lexicon are shown in Table 6." ></td>
	<td class="line x" title="128:204	We notice that adding affix features yields improvements in OOV accuracy." ></td>
	<td class="line x" title="129:204	The relationship between the constrained lexicon and unanalyzable word accuracy is less straighforward." ></td>
	<td class="line x" title="130:204	In this case, the degradation of unanalyzable word accuracy is due to the fact that the constrained lexicon over-restricts the tagsets of some frequent unanalyzable words." ></td>
	<td class="line x" title="131:204	However, the constrained lexicon generally improves the overall accuracy and is thus a viable technique." ></td>
	<td class="line x" title="132:204	Finally, the combination of affix features and constrained lexicon results in a tagger with 69.83% accuracy, which is a 7% absolute improvement over System IV." ></td>
	<td class="line x" title="133:204	System Total AW UW OOV System IV 62.76 67.07 20.74 21.84 +affixes 67.48 71.30 22.82 29.82 +constrained lex 66.25 70.29 21.28 26.32 +both 69.83 74.10 24.65 27.68 Table 6: Improvements in tagging accuracy from adding affix features and constraining lexicon." ></td>
	<td class="line x" title="134:204	5 Cross-Dialectal Data Sharing Next we examine whether unannotated corpora in other dialects (LCA) can be used to further improve the ECA tagger." ></td>
	<td class="line x" title="135:204	The problem of data sparseness for Arabic dialects would be less severe if we were able to exploit the commonalities between similar dialects." ></td>
	<td class="line x" title="136:204	In natural language processing, Kim & Khu59 danpur (2004) have explored techniques for using parallel Chinese/English corpora for language modeling." ></td>
	<td class="line x" title="137:204	Parallel corpora have also been used to infer morphological analyzers, POS taggers, and noun phrase bracketers by projections via word alignments (Yarowsky et al. , 2001)." ></td>
	<td class="line x" title="138:204	In (Hana et al. , 2004), Czech data is used to develop a morphological analyzer for Russian." ></td>
	<td class="line x" title="139:204	In contrast to these works, we do not require parallel/comparable corpora or a bilingual dictionary, which may be difficult to obtain." ></td>
	<td class="line x" title="140:204	Our goal is to develop general algorithms for utilizing the commonalities across dialects for developing a tool for a specific dialect." ></td>
	<td class="line x" title="141:204	Although dialects can differ very strongly, they are similar in that they exhibit morphological simplifications and a different word order compared to MSA (e.g. SVO rather than VSO order), and close dialects share some vocabulary." ></td>
	<td class="line x" title="142:204	Each of the tagger components (i.e. contextual model p(ti|hi), lexical model p(wi|ti), and affix model p(ai|ti)p(bi|ti)) can be shared during training." ></td>
	<td class="line x" title="143:204	In the following, we present two approaches for sharing data between dialects, each derived from following different assumptions about the underlying data generation process." ></td>
	<td class="line x" title="144:204	5.1 Contextual Model Interpolation Contextual model interpolation is a widely-used data-sharing technique which assumes that models trained on data from different sources can be mixed in order to provide the most appropriate probability distribution for the target data." ></td>
	<td class="line x" title="145:204	In our case, we have LCA as an out-of-domain data source, and ECA as the in-domain data source, with the former being about 4 times larger than the latter." ></td>
	<td class="line x" title="146:204	If properly combined, the larger amount of out-ofdomain data might improve the robustness of the in-domain tagger." ></td>
	<td class="line x" title="147:204	We therefore use a linear interpolation of in-domain and out-of-domain contextual models." ></td>
	<td class="line x" title="148:204	The joint probability p(w0:M,t0:M) becomes: Mproductdisplay i=0 pE(wi|ti)(pE(ti|hi) + (1  )pL(ti|hi)) (2) Here  defines the interpolation weights for the ECA contextual model pE(ti|hi) and the LCA contextual model pL(ti|hi)." ></td>
	<td class="line x" title="149:204	pE(wn|tn) is the ECA lexical model." ></td>
	<td class="line x" title="150:204	The interpolation weight  is estimated by maximizing the likelihood of a held-out data set given the combined model." ></td>
	<td class="line x" title="151:204	As an extension, we allow the interpolation weights to be a function of the current tag: (ti), since class-dependent interpolation has shown improvements over basic interpolation in applications such as language modeling (Bulyko et al. , 2003)." ></td>
	<td class="line x" title="152:204	5.2 Joint Training of Contextual Model As an alternative to model interpolation, we consider training a single model jointly from the two different data sets." ></td>
	<td class="line x" title="153:204	The underlying assumption of this technique is that tag sequences in LCA and ECA are generated by the same process, whereas the observations (the words) are generated from the tag by two different processes in the two different dialects." ></td>
	<td class="line x" title="154:204	The HMM model for joint training is expressed as: Mproductdisplay i=0 (ipE(wi|ti) + (1  i)pL(wi|ti))pE+L(ti|hi) (3) where i= braceleftbigg 1 if word w i is ECA 0 otherwise A single conditional probability table is used for the contextual model, whereas the lexical model switches between two different parameter tables, one for LCA observations and another for ECA observations." ></td>
	<td class="line x" title="155:204	During training, the contextual model is trained jointly from both ECA and LCA data; however, the data is divided into ECA and LCA portions when updating the lexical models." ></td>
	<td class="line x" title="156:204	Both the contextual and lexical models are trained within the same training pass." ></td>
	<td class="line x" title="157:204	A graphical model representation of this system is shown in Figure 1." ></td>
	<td class="line x" title="158:204	This model can be implemented using the functionality of switching parents (Bilmes, 2000) provided by GMTK." ></td>
	<td class="line x" title="159:204	During decoding, the tagger can in principle switch its lexical model to ECA or LCA, depending on the input; this system thus is essentially a multidialect tagger." ></td>
	<td class="line x" title="160:204	In the experiments reported below, however, we exclusively test on ECA, and the LCA lexical model is not used." ></td>
	<td class="line x" title="161:204	Due to the larger amount of data available for contextual model, joint training can be expected to improve the performance on the target dialect." ></td>
	<td class="line x" title="162:204	The affix models can be trained jointly in a similar fashion." ></td>
	<td class="line x" title="163:204	60 5.3 Data sharing results The results for data sharing are shown in Table 7." ></td>
	<td class="line x" title="164:204	The systems Interpolate- and Interpolate-(ti) are taggers built by interpolation and class-dependent interpolation, respectively." ></td>
	<td class="line x" title="165:204	For joint training, we present results for two systems: JointTrain(1:4) is trained on the existing collection ECA and LCA data, which has a 1:4 ratio in terms of corpus size; JointTrain(2:1) weights the ECA data twice, in order to bias the training process more towards ECAs distribution." ></td>
	<td class="line x" title="166:204	We also provide results for two more taggers: the first (CombineData) is trained naively on the pooled data from both ECA and LCA, without any weighting, interpolation, or changes to the probabilistic model." ></td>
	<td class="line x" title="167:204	The second (CombineLex) uses a contextual model trained on ECA and a lexical model estimated from both ECA and LCA data." ></td>
	<td class="line x" title="168:204	The latter was trained in order to assess the potential for improvement due to the reduction in OOV rate on the dev set when adding the LCA data (cf.Table 4)." ></td>
	<td class="line x" title="170:204	All the above systems utilize the constrained lexicon, as it consistently gives improvements." ></td>
	<td class="line x" title="171:204	Table 7 shows that, as expected, the brute-force combination of training data is not helpful and degrades performance." ></td>
	<td class="line x" title="172:204	CombineLex results in higher accuracy but does not outperform models in Table 6." ></td>
	<td class="line x" title="173:204	The same is true of the taggers using model interpolation." ></td>
	<td class="line x" title="174:204	The best performance is obtained by the system using the joint contextual model with separate lexical models, with 2:1 weighting of ECA vs. LCA data." ></td>
	<td class="line x" title="175:204	Finally, we added word affix information to the best shared-data system, which resulted in an accuracy of 70.88%." ></td>
	<td class="line x" title="176:204	In contrast, adding affix to CombineData achieves 61.78%, suggesting that improvements in JointTrain comes from the joint training technique rather than simple addition of new data." ></td>
	<td class="line x" title="177:204	This result is directly comparable to the best system in Section 4 (last row of Table 6)2." ></td>
	<td class="line x" title="178:204	The analysis of tagging errors revealed that the most frequent confusions are between VBD/NNS, 2We also experimented with joint training of ECA+MSA." ></td>
	<td class="line x" title="179:204	This gave good OOV accuracy, but overall it did not improve over the best system in Section 4." ></td>
	<td class="line x" title="180:204	Also, note that all accuracies are calculated by ignoring the scoring of ambiguous words, which have several possible tags as the correct reference." ></td>
	<td class="line x" title="181:204	If we score the ambiguous words as correct when the hypothesized tag is within this set, the accuracy of ECA+LCA+affix JointTrain rises to 77.18%, which is an optimistic upper-bound on the total accuracy." ></td>
	<td class="line x" title="182:204	System Total AW UW OOV CombineData 60.79 64.21 20.27 26.10 CombineLex 65.13 69.47 18.81 22.34 Interpolate- 62.82 67.42 16.98 17.44 Interpolate-(ti) 63.49 67.96 17.19 19.33 JointTrain(1:4) 62.53 66.18 27.78 26.52 JointTrain(2:1) 66.95 71.02 31.72 26.81 JointTrain(2:1)+affix w/ ECA+LCA 70.88 75.20 28.17 34.06 w/ ECA+MSA 67.85 71.50 17.76 31.76 Table 7: Tagging accuracy for various data sharing methods." ></td>
	<td class="line x" title="183:204	Figure 1: Graphical Model of Joint Training: switching between different lexical models while sharing the underlying contextual model." ></td>
	<td class="line x" title="184:204	The variable s represents the  term in Eq." ></td>
	<td class="line x" title="185:204	3 and chooses the lexical model depending on the origin of the word." ></td>
	<td class="line x" title="186:204	VBP/VBD, and JJ/NN." ></td>
	<td class="line x" title="187:204	Commonly mistagged words include cases like a11a12 a15 a0a2a1 a27 a12 (means-3rd.sg), which is labeled as a particle in the reference but is most often tagged as a verb, which is also a reasonable tag." ></td>
	<td class="line x" title="188:204	6 Discussion and Future Work Table 8 highlights the performance of the various taggers on the ECA evaluation set." ></td>
	<td class="line x" title="189:204	The accuracy of the unsupervised HMM tagger (System IV) improves from 58.47% to 66.61% via the affix features and constrained lexicon, and to a 68.48% by including joint training." ></td>
	<td class="line x" title="190:204	These improvements are statistical significant at the 0.005 level according to a difference-of-proportions test." ></td>
	<td class="line x" title="191:204	Several of the methods proposed here deserve further work: first, additional ways of constraining the lexicon can be explored, which may include imposing probability distributions on the possible tags for unanalyzable words." ></td>
	<td class="line x" title="192:204	Other clustering algorithms (e.g. root-based clustering of Arabic (De Roeck and 61 Al-Fares, 2000)), may be used instead of, or in addition to, distribution-based clustering." ></td>
	<td class="line x" title="193:204	Cross-dialectal data sharing for tagging also deserves more research." ></td>
	<td class="line x" title="194:204	For instance, the performance of the contextual model interpolation might be increased if one trains interpolation weights dependent on the classes based on previous two tags." ></td>
	<td class="line x" title="195:204	Joint training of contextual model and data sharing for lexical models can be combined; other dialectal data may also be added into the same joint training framework." ></td>
	<td class="line x" title="196:204	It would also be useful to extend these methods to create a more fine-grained part-ofspeech tagger with case, person, number, etc. information." ></td>
	<td class="line x" title="197:204	Stems, POS, and fine-grained POS can be combined into a factorial hidden Markov model, so that relationships between the stems and POS as well as data sharing between dialects can be simultaneously exploited to build a better system." ></td>
	<td class="line x" title="198:204	In conclusion, we have presented the first steps towards developing a dialectal Arabic tagger with minimal supervision." ></td>
	<td class="line x" title="199:204	We have shown that adding affix features and constraining the lexicon for unanalyzable words are simple resource-light methods to improve tagging accuracy." ></td>
	<td class="line x" title="200:204	We also explore the possibility of improving an ECA tagger using LCA data and present two data sharing methods." ></td>
	<td class="line x" title="201:204	The combination of these techniques yield a 10% improvement over the baseline." ></td>
	<td class="line x" title="202:204	System Total AW UW OOV System IV 58.47 64.71 22.34 17.50 +affix+lexicon 66.61 72.87 20.17 25.49 Interpolate II 60.07 66.56 20.55 17.61 JointTr.+affix 68.48 76.20 48.44 17.76 CombineLex 61.35 68.12 16.02 16.87 Table 8: Tagging accuracy on ECA evaluation set Acknowledgements This material is based on work funded by the NSF and the CIA under NSF Grant No." ></td>
	<td class="line x" title="203:204	IIS-0326276." ></td>
	<td class="line x" title="204:204	Any opinions, findings, and conclusions expressed in this material are those of the authors and do not necessarily reflect the views of these agencies." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E06-1034
From Detecting Errors To Automatically Correcting Them
Dickinson, Markus;"></td>
	<td class="line x" title="1:197	From detecting errors to automatically correcting them Markus Dickinson Department of Linguistics Georgetown University mad87@georgetown.edu Abstract Faced with the problem of annotation errors in part-of-speech (POS) annotated corpora, we develop a method for automatically correcting such errors." ></td>
	<td class="line x" title="2:197	Building on top of a successful error detection method, wefirsttry correcting acorpus using two off-the-shelf POS taggers, based on the idea that they enforce consistency; with this, we find some improvement." ></td>
	<td class="line x" title="3:197	After some discussion of the tagging process, we alter the tagging model to better account for problematic tagging distinctions." ></td>
	<td class="line x" title="4:197	This modification results in significantly improved performance, reducing the error rate of the corpus." ></td>
	<td class="line x" title="5:197	1 Introduction Annotated corpora serve as training material and as gold standard testing material for the development of tools in computational linguistics, and as a source of data for theoretical linguists searching for relevant language patterns." ></td>
	<td class="line x" title="6:197	However, they contain annotation errors, and such errors provide unreliable training andevaluation data, ashasbeen previously shown (see ch." ></td>
	<td class="line x" title="7:197	1 of Dickinson (2005) and references therein)." ></td>
	<td class="line x" title="8:197	Improving the quality of linguistic annotation where possible is thus a key issue for the use of annotated corpora in computational and theoretical linguistics." ></td>
	<td class="line x" title="9:197	Research has gone into automatically detecting annotation errors for part-of-speech annotation (van Halteren, 2000; Kveton and Oliva, 2002; Dickinson and Meurers, 2003), yet there has been virtually no work on automatically or semiautomatically correcting such annotation errors.1 1Oliva (2001) specifies hand-written rules to detect and Automatic correction can speed up corpus improvement efforts and provide new data for NLP technology training on the corpus." ></td>
	<td class="line x" title="10:197	Additionally, an investigation into automatic correction forces us to re-evaluate the technology using the corpus, providing new insights into such technology." ></td>
	<td class="line x" title="11:197	We propose in this paper to automatically correct part-of-speech (POS) annotation errors in corpora, by adapting existing technology for POSdisambiguation." ></td>
	<td class="line x" title="12:197	We build the correction work on top of a POS error detection phase, described in section 2." ></td>
	<td class="line x" title="13:197	In section 3 we discuss how to evaluate corpus correction work, given that we have no benchmark corpus to compare with." ></td>
	<td class="line x" title="14:197	We turn to the actual work of correction in section 4, using two different POS taggers as automatic correctors and using the Wall Street Journal (WSJ) corpus as our data." ></td>
	<td class="line x" title="15:197	After more thoroughly investigating how problematic tagging distinctions affect thePOSdisambiguation task, insection 5wemodify the tagging model in order to better account for these distinctions, and we show this to significantly reduce the error rate of a corpus." ></td>
	<td class="line x" title="16:197	It might be objected that automatic correction of annotation errors will cause information to be lost or will make the corpus worse than it was, but the construction of a large corpus generally requires semi-automated methods of annotation, and automatic tools must be used sensibly at every stage in the corpus building process." ></td>
	<td class="line x" title="17:197	Automated annotation methods are not perfect, but humans also add errors, from biases and inconsistent judgments." ></td>
	<td class="line x" title="18:197	Thus, automatic corpus correction methods canbeused semi-automatically, just astheoriginal corpus creation methods were used." ></td>
	<td class="line x" title="19:197	then correct errors, but there is no general correction scheme." ></td>
	<td class="line x" title="20:197	265 2 Detecting POS Annotation Errors To correct part-of-speech (POS) annotation errors, one has to first detect such errors." ></td>
	<td class="line x" title="21:197	Although there are POS error detection approaches, using, e.g., anomaly detection (Eskin, 2000), our approach builds on the variation n-gram algorithm introduced in Dickinson and Meurers (2003) and Dickinson (2005)." ></td>
	<td class="line x" title="22:197	As we will show in section 5, such a method is useful for correction because it highlights recurring problematic tag distinctions in the corpus." ></td>
	<td class="line x" title="23:197	The idea behind the variation n-gram approach is that a string occurring more than once can occur with different labels in a corpus, which is referred to as variation." ></td>
	<td class="line x" title="24:197	Variation is caused by one of two reasons: i) ambiguity: there is a type of string with multiple possible labels and different corpus occurrences of that string realize the different options, or ii) error: the tagging of a string is inconsistent across comparable occurrences." ></td>
	<td class="line x" title="25:197	The more similar the context of a variation, the more likely the variation is an error." ></td>
	<td class="line x" title="26:197	In Dickinson and Meurers (2003), contexts are composed of words, and identity of the context is required." ></td>
	<td class="line x" title="27:197	The term variation n-gram refers to an n-gram (of words) in a corpus that contains a string annotated differently in another occurrence of the same ngram in the corpus." ></td>
	<td class="line x" title="28:197	The string exhibiting the variation is referred to as the variation nucleus." ></td>
	<td class="line x" title="29:197	For example, in the WSJ corpus, part of the Penn Treebank 3 release (Marcus et al. , 1993), the string in (1) is a variation 12-gram since off is a variation nucleus that in one corpus occurrence is tagged as a preposition (IN), while in another it is tagged as a particle (RP)." ></td>
	<td class="line x" title="30:197	(1) to ward off a hostile takeover attempt by two European shipping concerns Once the variation n-grams for a corpus have been computed, heuristics are employed to classify the variations into errors and ambiguities." ></td>
	<td class="line x" title="31:197	The most effective heuristic takes into account the fact that natural languages favor the use of local dependencies over non-local ones: nuclei found at the fringe of an n-gram are more likely to be genuine ambiguities than those occurring with at least one word of surrounding context." ></td>
	<td class="line x" title="32:197	Running the variation n-gram error detection method on the WSJ turns up 7141 distinct2 non2Being distinct means each corpus position is only taken into account for the longest variation n-gram it occurs in." ></td>
	<td class="line x" title="33:197	fringe nuclei, of which an estimated 92.8%, or 6627, are erroneous.3 Since a variation nucleus refers to multiple corpus positions, this precision is a precision on types; we, however, are correcting tokens." ></td>
	<td class="line x" title="34:197	Still, this precision is high enough to experiment with error correction." ></td>
	<td class="line x" title="35:197	3 Methodology Since we intend to correct a corpus with POS annotation errors, we have no true benchmark by which to gauge the accuracy of the corrected corpus, and we thus created a hand-checked subcorpus." ></td>
	<td class="line x" title="36:197	Using the variation n-gram output, we flagged every non-fringe variation nucleus (token) as a potential error, giving us 21,575 flagged positions in the WSJ." ></td>
	<td class="line x" title="37:197	From this set, we sampled 300 positions, removed the tag for each position, and hand-marked what the correct tag should be, based solely on the tagset definitions given in the WSJ tagging manual (Santorini, 1990), i.e., blind to the original data." ></td>
	<td class="line x" title="38:197	Because some of the tagset distinctions werenot defined clearly enough in the guidelines, in 20 cases we could not decide what the exact tag should be." ></td>
	<td class="line x" title="39:197	For the purposes of comparison, we score a match with either tag as correct since a human could not disambiguate such cases." ></td>
	<td class="line x" title="40:197	For the benchmark, we find that 201 positions in our sample set of 300 are correct, giving us a precision of 67%." ></td>
	<td class="line x" title="41:197	A correction method must then surpass this precision figure in order to be useful." ></td>
	<td class="line x" title="42:197	4 Approach to correction Since our error detection phase relies on variation in annotation, i.e., the inconsistent application of POS labels across the corpus, we propose to correct such errors by enforcing consistency in the text." ></td>
	<td class="line x" title="43:197	As van Halteren (2000) points out, POS taggers can be used to enforce consistency, and so we employ off-the-shelf supervised POS taggers for error correction." ></td>
	<td class="line x" title="44:197	The procedure is as follows: 1." ></td>
	<td class="line x" title="45:197	Train the tagger on the entire corpus." ></td>
	<td class="line x" title="46:197	2." ></td>
	<td class="line x" title="47:197	Run the trained tagger over the same corpus." ></td>
	<td class="line x" title="48:197	3." ></td>
	<td class="line x" title="49:197	For the positions the variation n-gram detection method flags as potentially erroneous, choose the label obtained in step 2." ></td>
	<td class="line x" title="50:197	We do not split training data from testing data because we want to apply the patterns found in the 3The recall cannot easily be estimated, but this is still a significant number of errors." ></td>
	<td class="line x" title="51:197	266 whole corpus to the corpus we want to correct, which happens to be the same corpus.4 If the tagger has learned the consistent patterns in the corpus, it will then generalize these patterns to the problematic parts of the corpus." ></td>
	<td class="line x" title="52:197	This approach hinges on high-quality error detection since in general we cannot assume that discrepancies between a POS tagger and the benchmark are errors in the benchmark." ></td>
	<td class="line x" title="53:197	Van Halteren (2000), for example, found that his tagger was correct in only 20% of disagreements with the benchmark." ></td>
	<td class="line x" title="54:197	By focusing only on the variationflagged positions, we expect the tagger decisions to be more often correct than incorrect." ></td>
	<td class="line x" title="55:197	We use two off-the-shelf taggers for correction, the Markov model tagger TnT (Brants, 2000) and the Decision Tree Tagger (Schmid, 1997), which we will abbreviate as DTT." ></td>
	<td class="line x" title="56:197	Both taggers use probabilistic contextual and lexical information to disambiguate a tag at a particular corpus position." ></td>
	<td class="line x" title="57:197	The difference is that TnT obtains contextual probabilities from maximum likelihood counts, whereas DTT constructs binary-branching decision trees to obtain contextual probabilities." ></td>
	<td class="line x" title="58:197	In both cases, instead of looking at n-grams of words, the taggers use n-grams of tags." ></td>
	<td class="line x" title="59:197	This generalization is desirable, as the variation n-gram method shows that the corpus has conflicting labels for the exact same sequence of n words." ></td>
	<td class="line x" title="60:197	Results For the TnT tagger, we obtain an overall precision of 71.67% (215/300) on the 300 handannotated samples." ></td>
	<td class="line x" title="61:197	For the DTT tagger, we get a higher precision, that of 76.33% (229/300)." ></td>
	<td class="line x" title="62:197	The DTT results are a significant improvement over the original corpus precision of 67% (p =.0045), while the TnT results are not." ></td>
	<td class="line x" title="63:197	As mentioned, tagger-benchmark disagreements are more commonly tagger errors, but we find the opposite for variation-flagged positions." ></td>
	<td class="line x" title="64:197	Narrowing in on the positions which the tagger changed, we find a precision of 58.56% (65/111) for TnTand 65.59% (69/107) for DTT.Asthegoal of correction is to change tags with 100% accuracy, we place a priority in improving these figures." ></td>
	<td class="line x" title="65:197	One likely reason that DTT outperforms TnT is 4Note, then, that some typical tagging issues, such as dealing with unknown words, are not an issue for us." ></td>
	<td class="line x" title="66:197	5All p-values in this paper are from McNemars Test (McNemar, 1947) for analyzing matched dichotomous data (i.e. , acorrect orincorrect scorefor each corpus position fromboth models)." ></td>
	<td class="line x" title="67:197	its more flexible context." ></td>
	<td class="line x" title="68:197	For instance, in example (2)which DTT correctly changes and TnT does not to know that such should be changed from adjective (JJ) to pre-determiner (PDT), one only need look at the following determiner (DT) an, and that provides enough context to disambiguate." ></td>
	<td class="line x" title="69:197	TnT uses a fixed context of trigrams, and so can be swayed by irrelevant tagshere, the previous tagswhich DTT can in principle ignore.6 (2) Mr. Bush was nt interested in such/JJ an informal get-together . 5 Modifying the tagging model The errors detected by the variation n-gram method arise from variation in the corpus, often reflecting decisions difficult for annotators to maintain over the entire corpus, for example, the distinction between preposition (IN) and particle (RP) (as in (1))." ></td>
	<td class="line x" title="70:197	Although these distinctions are listed in the tagging guidelines (Santorini, 1990), nowhere are they encoded in the tags themselves; thus, atagger has nodirect wayof knowing that IN and RPare easily confusable but IN and NN(common noun) are not." ></td>
	<td class="line x" title="71:197	In order to improve automatic correction, we can add information about these recurring distinctions to the tagging model, making the tagger aware of the difficult distinctions." ></td>
	<td class="line x" title="72:197	But how do we make a tagger aware of a relevant problematic distinction?" ></td>
	<td class="line x" title="73:197	Consider the domain of POS tagging." ></td>
	<td class="line x" title="74:197	Every word patterns uniquely, yet there are generalizations about words which we capture by grouping them into POS classes." ></td>
	<td class="line x" title="75:197	By grouping words into the same class, there is often a claim that these words share distributional properties." ></td>
	<td class="line x" title="76:197	But how true this is depends on ones tagset (see, e.g., Dejean (2000))." ></td>
	<td class="line x" title="77:197	If we can alter the tagset to better match the distributional facts, we can improve correction." ></td>
	<td class="line x" title="78:197	To see how problematic distinctions can assist in altering the tagset, consider the words away and aboard, both of which can be adverbs (RB) in the Penn Treebank, as shown in (3a) and (4a)." ></td>
	<td class="line x" title="79:197	In example (3b), we find that away can also be a particle (RP), thus making it a part of the ambiguity class RB/RP." ></td>
	<td class="line x" title="80:197	On the other hand, as shown in (4b), aboard can be a preposition (IN), but not a particle, putting it in the ambiguity class IN/RB." ></td>
	<td class="line x" title="81:197	Crucially, not only do away and aboard belong 6As DTT does not provide a way of viewing output trees, we cannot confirm that this is the reason for improvement." ></td>
	<td class="line x" title="82:197	267 to different ambiguity classes, but their adverbial uses are also distinguished." ></td>
	<td class="line x" title="83:197	The adverbial away is followed by from, a construction forbidden for aboard." ></td>
	<td class="line x" title="84:197	When we examine the RB/RP words, we find that they form a natural class: apart, aside, and away, all of which can be followed by from." ></td>
	<td class="line x" title="85:197	(3) a. the Cray-3 machine is at least another year away/RB from a  prototype b. A lot of people think 0 I will give away/RP the store (4) a. Saturday s crash  that *T* killed 132 of the 146 people aboard/RB b. These are used * aboard/IN military helicopters Although not every ambiguity class is so cleanly delineated, this example demonstrates that such classes can be used to redefine a tagging model with more unified groupings." ></td>
	<td class="line x" title="86:197	5.1 Using complex ambiguity tags We thus propose splitting a class such as RB into subclasses, using these ambiguity classesJJ/RB, NN/RB, IN/RB, etc.akin to previous work on splitting labels in order to obtain better statistics (e.g. , Brants (1996); Ule (2003)) for situations with the same label but different usage (Ule, 2003, p. 181)." ></td>
	<td class="line x" title="87:197	By taking this approach, we are narrowing in on what annotators were instructed to focus on, namely difficult tagging decisions, (Santorini, 1990, p. 7)." ></td>
	<td class="line x" title="88:197	We implement this idea by assigning words a new, complex tag composed of its ambiguity class and the benchmark tag for that position." ></td>
	<td class="line x" title="89:197	For example, ago has the ambiguity class IN/RB, and in example (5a), it resolves to RB." ></td>
	<td class="line x" title="90:197	Thus, following the notation in Pla and Molina (2004), we assign ago the complex ambiguity tag <IN/RB,RB> in the training data, as shown in (5b)." ></td>
	<td class="line x" title="91:197	(5) a. ago/RB b. ago/<IN/RB,RB> Complex ambiguity tags can provide better distinctions than the unaltered tags." ></td>
	<td class="line x" title="92:197	For example, words which vary between IN and RB and tagged as IN (e.g. , ago, tagged <IN/RB,IN>) can ignore the contextual information that words varying between DT (determiner) and IN (e.g. , that, tagged <DT/IN,IN>) provide." ></td>
	<td class="line x" title="93:197	This proposal is in the spirit of a tagger like that described in Marquez et al (2000), which breaks the POS tagging problem into one problem for each ambiguity class, but because we alter the tagset here, different underlying tagging algorithms can be used." ></td>
	<td class="line x" title="94:197	To take an example, consider the 5-gram revenue of about $ 370 as it is tagged by TnT." ></td>
	<td class="line x" title="95:197	The 5-gram (at position 1344) in the WSJ is annotated as in (6)." ></td>
	<td class="line x" title="96:197	The tag for about is incorrect since about whenused to meanapproximately should be tagged as an adverb (RB), rather than a preposition (IN) (Santorini, 1990, p. 22)." ></td>
	<td class="line x" title="97:197	(6) revenue/NN of/IN about/IN $/$ 370/CD Between of and $, the word about varies between preposition (IN) and adverb (RB): it is IN 67 times and RB 65 times." ></td>
	<td class="line x" title="98:197	After training TnT on the original corpus, we find that RB is a slightly better predictor of the following $ tag, as shown in (7), but, due to the surrounding probabilities, IN is the tag TnT assigns." ></td>
	<td class="line x" title="99:197	(7) a. p($|IN,RB) = .0859 b. p($|IN,IN) = .0635 The difference between probabilities is more pronounced in the model with complex ambiguity tags." ></td>
	<td class="line x" title="100:197	The word about generally varies between three tags: IN,RB,and RP(particle), receiving the ambiguity class IN/RB/RP (as of also does)." ></td>
	<td class="line x" title="101:197	For IN/RB/RP words, RB is significantly more probable in this context than IN, as shown in (8)." ></td>
	<td class="line x" title="102:197	(8) a. p($|<IN/RB/RP,IN>,<IN/RB/RP,RB>) = .6016 b. p($|<IN/RB/RP,IN>,<IN/RB/RP,IN>) = .1256 Comparing (7) and (8), we see that RB for the ambiguity class of IN/RB/RP behaves differently than the general class of RB words." ></td>
	<td class="line x" title="103:197	We have just shown that the contextual probabilities of an n-gram tagger are affected when using complex ambiguity tags; lexical probabilities are also dramatically changed." ></td>
	<td class="line x" title="104:197	The relevant probabilities were originally as in (9), but for the modified corpus, we have the probabilities in (10)." ></td>
	<td class="line x" title="105:197	(9) a. p(about|IN) = 2074/134926 = .0154 b. p(about|RB) = 785/42207 = .0186 (10) a. p(about|<IN/RB/RP,IN>) = 2074/64046 = .0324 b. p(about|<IN/RB/RP,RB>) = 785/2045 = .3839 268 These altered probabilities provide information similar to that found in a lexicalized tagger i.e., about behaves differently than the rest of its classbut the altered contextual probabilities, unlike a lexicalized tagger, bring general IN/RB/RP class information to bear on this tagging situation." ></td>
	<td class="line x" title="106:197	Combining the two, we get the correct tag RB at this position." ></td>
	<td class="line x" title="107:197	Since variation errors are errors for words with prominent ambiguity classes, zeroing in on these ambiguity classes should provide more accurate probabilities." ></td>
	<td class="line x" title="108:197	For this to work, however, we have to ensure that we have the most effective ambiguity class for every word." ></td>
	<td class="line oc" title="109:197	5.2 Assigning complex ambiguity tags In the tagging literature (e.g. , Cutting et al (1992)) an ambiguity class is often composed of the set of every possible tag for a word." ></td>
	<td class="line x" title="110:197	For correction, using every possible tag for an ambiguity class will result in too many classes, for two reasons: 1) there are erroneous tags which should not be part of the ambiguity class, and 2) some classes are irrelevant for disambiguating variation positions." ></td>
	<td class="line x" title="111:197	Guided by these considerations, we use the procedure below to assign complex ambiguity tags to all wordsinthe corpus, based onwhether awordis a non-fringe variation nucleus and thus flagged as a potential error by the variation n-gram method (choice 1), or is not a nucleus (choice 2)." ></td>
	<td class="line x" title="112:197	1." ></td>
	<td class="line x" title="113:197	Every word which is a variation word (nucleus of a non-fringe variation) or typeidentical to a variation word is assigned: (a) a complex tag reflecting the ambiguity class of all relevant ambiguities in the non-fringe variation nuclei; or (b) a simple tag reflecting no ambiguity, if the tag is irrelevant." ></td>
	<td class="line x" title="114:197	2." ></td>
	<td class="line x" title="115:197	Based on their relevant unigram tags, nonvariation words are assigned: (a) a complex tag, if the words ambiguity tag also appears as a variation ambiguity; or (b) a simple tag, otherwise." ></td>
	<td class="line x" title="116:197	Variation words (choice 1) We start with variation nuclei because these are the potential errors we wish to correct." ></td>
	<td class="line x" title="117:197	An example of choice 1a is ago, which varies between IN and RB as a nucleus, and so receives the tag <IN/RB,IN> when it resolves to IN and <IN/RB,RB> when it resolves to RB." ></td>
	<td class="line x" title="118:197	The choices are based on relevance, though; instead of simply assigning all tags occurring in an ambiguity to an ambiguity class, we filter out ambiguities which we deem irrelevant." ></td>
	<td class="line x" title="119:197	Similar to Brill and Pop (1999) and Schmid (1997), we do this by examining the variation unigrams and removing tags which occur less than 0.01 of the time for a word and less than 10 times overall." ></td>
	<td class="line x" title="120:197	This eliminates variations like,/DT where DT appears 4210 times for an, but the comma tag appears only once." ></td>
	<td class="line x" title="121:197	Doing this means that an can now be grouped with other unambiguous determiners (DT)." ></td>
	<td class="line x" title="122:197	In addition to removing some erroneous classes, we gain generality and avoid data sparseness by using fewer ambiguity classes." ></td>
	<td class="line x" title="123:197	This pruning also means that some variation words will receive tags which are not part of a variation, which is when choice 1b is selected." ></td>
	<td class="line x" title="124:197	For instance, if the class is IN/RB and the current tag is JJ, it gets JJ instead of <IN/RB,JJ> because a word varying between IN and RB should not resolve to JJ." ></td>
	<td class="line x" title="125:197	This situation also arises because we are deriving the ambiguity tags only from the nonfringe nuclei but are additionally assigning them to type-identical words in the corpus." ></td>
	<td class="line x" title="126:197	Words involved in a variation may elsewhere have tags never involved in a variation." ></td>
	<td class="line x" title="127:197	For example, Advertisers occurs as a non-fringe nucleus varying between NNP (proper noun) and NNPS (plural proper noun)." ></td>
	<td class="line x" title="128:197	In non-variation positions, it appears as a plural common noun (NNS), which we tag as NNS because NNS is not relevant to the variation (NNP/NNPS) we wish to distinguish." ></td>
	<td class="line x" title="129:197	Onemorenoteisneeded toexplain howwehandled the vertical slashes used in the Penn Treebank annotation." ></td>
	<td class="line x" title="130:197	Vertical slashes represent uncertainty between two tagse.g. , JJ|VBN means the annotator could not decide between JJ and VBN (past participle)." ></td>
	<td class="line x" title="131:197	Variation between JJ, VBN, and JJ|VBN is simply variation between JJ and VBN, and we represent it by the class JJ/VBN, thereby ensuring that JJ/VBN has more data." ></td>
	<td class="line x" title="132:197	In short, we assign complex ambiguity tags to variation words whenever possible (choice 1a), but because of pruning and because of non-variation tags for a word, we have to assign simple tags to some corpus positions (choice 1b)." ></td>
	<td class="line x" title="133:197	Non-variation words (choice 2) In order to have more data for a tag, non-variation words also 269 take complex ambiguity tags." ></td>
	<td class="line x" title="134:197	For words which are not a part of a variation nucleus, we similarly determine relevance and then assign a complex ambiguity tag if the ambiguity is elsewhere involved in a non-fringe nucleus (choice 2a)." ></td>
	<td class="line x" title="135:197	For instance, even though join is never a non-fringe variation nucleus, it gets the tag <VB/VBP,VB> inthe first sentence of the treebank because its ambiguity class VB/VBP is represented in the nonfringe nuclei." ></td>
	<td class="line x" title="136:197	On the other hand, we ignore ambiguity classes which have no bearing on correction (choice 2b)." ></td>
	<td class="line x" title="137:197	Forexample, ours varies between JJand PRP(personal pronoun), but no non-fringe variation nuclei have this same ambiguity class, so no complex ambiguity tag is assigned." ></td>
	<td class="line x" title="138:197	Our treatment of nonvariation words increases the amount of relevant data (choice 2a) and still puts all non-varying data together (choice 2b)." ></td>
	<td class="line x" title="139:197	Uniform assignment of tags Why do we allow only one ambiguity class per word over the whole corpus?" ></td>
	<td class="line x" title="140:197	Consider the variation nucleus traded: in publicly traded investments, traded varies between JJ and VBN, but in contracts traded on, it varies between VBNand VBD(past tense verb)." ></td>
	<td class="line x" title="141:197	It seems like it would be useful to keep the JJ/VBN cases separate from the VBD/VBN ones, so that a tagger can learnone set of patterns for JJ/VBNand a different set for VBD/VBN." ></td>
	<td class="line x" title="142:197	While that might have its benefits, there are several reasons why restricting words to a single ambiguity class is desirable, i.e., why we assign traded the ambiguity class JJ/VBD/VBN in this case." ></td>
	<td class="line x" title="143:197	First, we want to group as many of the word occurrences as possible together into a single class." ></td>
	<td class="line x" title="144:197	UsingJJ/VBNandVBD/VBNastwoseparate ambiguity classes would mean that traded as VBN lacks a pattern of its own." ></td>
	<td class="line x" title="145:197	Secondly, multiple ambiguity classes for a word can increase the number of possible tags for a word." ></td>
	<td class="line x" title="146:197	For example, instead of having only the tag <JJ/VBD/VBN,VBN> for traded as VBN, we would have both <JJ/VBN,VBN> and <VBD/VBN,VBN>." ></td>
	<td class="line x" title="147:197	With such an increase in the number of tags, data sparseness becomes a problem." ></td>
	<td class="line x" title="148:197	Finally, although we know what the exact ambiguity in question is for a non-fringe nucleus, it is too difficult to go through position by position to guess the correct ambiguity for every other spot." ></td>
	<td class="line x" title="149:197	If we encounter a JJ/VBD/VBN word like followed tagged as VBN, for example, we cannot know for sure whether this isan instance where JJ/VBNwas thedecision whichhadtobemadeorifVBD/VBN wasthe difficult choice; keeping only one ambiguity class per word allows us to avoid guessing." ></td>
	<td class="line x" title="150:197	5.3 Results with complex ambiguity tags Using complex ambiguity tags increases the size of the tagset from 80 tags in the original corpus 7 to 418 tags in the altered tagset, 53 of which are simple (e.g. IN) and 365 of which are complex (e.g. <IN/RB,IN>)." ></td>
	<td class="line x" title="151:197	TnT Examining the 300 samples of variation positions from the WSJ corpus for the TnT tagger with complex ambiguity tags, we find that 234 spots are correctly tagged, for a precision of 78.00%." ></td>
	<td class="line x" title="152:197	Additionally, we find 73.86% (65/88) precision for tags which have been changed from the original corpus." ></td>
	<td class="line x" title="153:197	The 78% precision is a significant improvement both over the original TnT precision of 71.67% (p = .008) and the benchmark of 67%(p = .001)." ></td>
	<td class="line x" title="154:197	Perhaps morerevealing istheimprovement in the precision of the changed tokens, from 58.56% to 73.86%." ></td>
	<td class="line x" title="155:197	With 73.86% precision for changed positions, this means that we expect approximately 3968 of the 5373 changes that the tagger makes, out of 21,575 flagged positions, to be correct changes." ></td>
	<td class="line x" title="156:197	Thus, the error rate of the corpus will be reduced." ></td>
	<td class="line x" title="157:197	Decision Tree Tagger (DTT) Using complex ambiguity tags with DTT results in an overall precision of 78.33% (235/300) and a precision of 73.56% (64/87) for the changed positions." ></td>
	<td class="line x" title="158:197	We improve the overall error correction precision, from 76.33% to78.33%, and the tagging of changed positions, going from 65.59% to 73.56%." ></td>
	<td class="line x" title="159:197	The results for all four models, plus the baseline, are summarized in figure 1." ></td>
	<td class="line x" title="160:197	From these figures, it seems that the solution for error correction lies less in what tagging method is used and more in the information we give each method." ></td>
	<td class="line x" title="161:197	The improvement in changed positions for both TnT and DTT is partly attributable to the fact that both tagging models are making fewer changes." ></td>
	<td class="line x" title="162:197	Indeed, training TnT on the original corpus and thentesting onthesamecorpus results ina97.37% similarity, but a TnT model trained on complex ambiguity tags results in 98.49% similarity with 7The number of tags here counts tags with vertical slashes separately." ></td>
	<td class="line x" title="163:197	270 Total Changed Baseline 67.00% N/A TnT 71.67% 58.56% (65/111) C.A. TnT 78.00% 73.86% (65/88) DTT 76.33% 65.59% (69/107) C.A. DTT 78.33% 73.56% (64/87) Figure 1: Summary of results the original." ></td>
	<td class="line x" title="164:197	DTT sees a parallel overall improvement, from 97.47% to 98.33%." ></td>
	<td class="line x" title="165:197	Clearly, then, each complexambiguity model isacloser fittotheoriginal corpus." ></td>
	<td class="line x" title="166:197	Whether this means it is an overall better POS tagging model is an open question." ></td>
	<td class="line x" title="167:197	Remaining issues We have shown that we can improve the annotation of a corpus by using tagging models with complex ambiguity tags, but can we improve even further?" ></td>
	<td class="line x" title="168:197	To do so, there are several obstacles to overcome." ></td>
	<td class="line x" title="169:197	First, some distinctions cannot be handled by an automated system without semantic or non-local information." ></td>
	<td class="line x" title="170:197	As Marquez and Padro (1997) point out, distinctions such as that between JJ and VBN are essentially semantic distinctions without any structural basis." ></td>
	<td class="line x" title="171:197	For example, in the phrase proposed offering, the reason that proposed should be VBN is that it indicates a specific event." ></td>
	<td class="line x" title="172:197	Since our method uses no external semantic information, we have no way to know how to correct this.8 Other distinctions, such as the one between VBD and VBN, require some form of non-local knowledge in order to disambiguate because it depends on the presence or absence of an auxiliary verb, which can be arbitrarily far away." ></td>
	<td class="line x" title="173:197	Secondly, sometimes the corpus was more often wrong than right for a particular pattern." ></td>
	<td class="line x" title="174:197	This can be illustrated by looking at the word later in example (11), from the WSJ corpus." ></td>
	<td class="line x" title="175:197	In the tagging manual (Santorini, 1990, p. 25), we find the description of later as in (12)." ></td>
	<td class="line x" title="176:197	(11) Now, 13 years later, Mr. Lane has revived his Artist  (12) later should be tagged as a simple adverb (RB) rather than as a comparative adverb (RBR), unless its meaning is clearly comparative." ></td>
	<td class="line x" title="177:197	A 8Note that it could be argued that this lack of a structural distinction contributed to the inconsistency among annotators in the first place and thus made error detection successful." ></td>
	<td class="line x" title="178:197	useful diagnostic is that the comparative later can be preceded by even or still." ></td>
	<td class="line x" title="179:197	In example (11), along with the fact that this is 13 years later as compared to now (i.e. , comparative), one can say Now, (even) 13 years later, Mr. Lane has revived his Artist  , favoring RBR as a tag." ></td>
	<td class="line x" title="180:197	But the trigram years later, occurs 16 times, 12 as RB and 4 as RBR." ></td>
	<td class="line x" title="181:197	Assuming RBR is correct, we clearly have a lot of wrong annotation in the corpus, even though here the corpus is correctly annotated as RBR." ></td>
	<td class="line x" title="182:197	As seen in (13), in the context of following CD and NNS, RBR is much less likely for TnT than either RB or JJ." ></td>
	<td class="line x" title="183:197	(13) a. p(JJ|CD,NNS) = .0366 b. p(RB|CD,NNS) = .0531 c. p(RBR|CD,NNS) = .0044 As shown in (14), even when we use complex ambiguity tags, we still find this favoritism for RB because of the overwhelmingly wrong data in the corpus." ></td>
	<td class="line x" title="184:197	However, we note that although RB is favored, its next closest competitor is now RBR not JJand RB is no longer favored by as much as it was over RBR." ></td>
	<td class="line x" title="185:197	We have more appropriately narrowed down the list of proper tags for this position by using complex ambiguity tags, but because of too much incorrect annotation, we still generate the wrong tag." ></td>
	<td class="line x" title="186:197	(14) a. p(<JJ/RB/RBR,JJ>|CD,NNS) = .0002 b. p(<JJ/RB/RBR,RB>|CD,NNS)= .0054 c. p(<JJ/RB/RBR,RBR>|CD,NNS)=.0017 These issues show that automatic correction must beused withcare, but they alsohighlight particular aspects of this tagset that any POS tagging method will have difficulty overcoming, and the effect of wrong data again serves to illustrate the problem of annotation errors in training data." ></td>
	<td class="line x" title="187:197	6 Summary and Outlook We have demonstrated the effectiveness of using POS tagging technology to correct a corpus, once an error detection method has identified potentially erroneous corpus positions." ></td>
	<td class="line x" title="188:197	We first showed that using a tagger as is provides moderate results, but adapting atagger toaccount forproblematic tag distinctions in the datai.e. , using complex ambiguity tagsperforms much better and 271 reduces the true error rate of a corpus." ></td>
	<td class="line x" title="189:197	Thedistinctions in the tagging model have more of an impact on the precision of correction than the underlying tagging algorithm." ></td>
	<td class="line x" title="190:197	Despite the gain in accuracy, we pointed out that there are still several residual problems which are difficult for any tagging system." ></td>
	<td class="line x" title="191:197	Future work will go into automatically sorting the tags so that the difficult disambiguation decisions can be dealt with differently from the easily disambiguated corpus positions." ></td>
	<td class="line x" title="192:197	Additionally, we will want to test the method on a variety of corpora and tagging schemes and gauge the impact of correction on POS tagger training and evaluation." ></td>
	<td class="line x" title="193:197	We hypothesize that this method will work for any tagset with potentially confusing distinctions between tags, but this is yet to be tested." ></td>
	<td class="line x" title="194:197	The method of adapting a tagging model by using complex ambiguity tags originated from an understanding that the POS tagging process is crucially dependent upon the tagset distinctions." ></td>
	<td class="line x" title="195:197	Based on this, the correction work described in this paper can be extended to the general task of POStagging, as atagger using complex ambiguity classes is attempting to tackle the difficult distinctions in a corpus." ></td>
	<td class="line x" title="196:197	To pursue this line of research, work has to go into defining ambiguity classes for all words in the corpus, instead of focusing on words involved in variations." ></td>
	<td class="line x" title="197:197	Acknowledgments I would like to thank Detmar Meurers for helpful discussion, Stephanie Dickinson for her statistical assistance, and the three anonymous reviewers for their comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N06-1042
Learning Morphological Disambiguation Rules For Turkish
Yuret, Deniz;Ture, Ferhan;"></td>
	<td class="line x" title="1:255	Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 328334, New York, June 2006." ></td>
	<td class="line x" title="2:255	c2006 Association for Computational Linguistics Learning Morphological Disambiguation Rules for Turkish Deniz Yuret Dept. of Computer Engineering Koc University Istanbul, Turkey dyuret@ku.edu.tr Ferhan Tcurrency1ure Dept. of Computer Engineering Koc University Istanbul, Turkey fture@ku.edu.tr Abstract In this paper, we present a rule based model for morphological disambiguation of Turkish." ></td>
	<td class="line x" title="3:255	The rules are generated by a novel decision list learning algorithm using supervised training." ></td>
	<td class="line x" title="4:255	Morphological ambiguity (e.g. lives = live+s or life+s) is a challenging problem for agglutinative languages like Turkish where close to half of the words in running text are morphologically ambiguous." ></td>
	<td class="line x" title="5:255	Furthermore, it is possible for a word to take an unlimited number of suf xes, therefore the number of possible morphological tags is unlimited." ></td>
	<td class="line x" title="6:255	We attempted to cope with these problems by training a separate model for each of the 126 morphological features recognized by the morphological analyzer." ></td>
	<td class="line x" title="7:255	The resulting decision lists independently vote on each of the potential parses of a word and the nal parse is selected based on our con dence on these votes." ></td>
	<td class="line x" title="8:255	The accuracy of our model (96%) is slightly above the best previously reported results which use statistical models." ></td>
	<td class="line x" title="9:255	For comparison, when we train a single decision list on full tags instead of using separate models on each feature we get 91% accuracy." ></td>
	<td class="line x" title="10:255	1 Introduction Morphological disambiguation is the task of selecting the correct morphological parse for a given word in a given context." ></td>
	<td class="line x" title="11:255	The possible parses of a word are generated by a morphological analyzer." ></td>
	<td class="line x" title="12:255	In Turkish, close to half the words in running text are morphologically ambiguous." ></td>
	<td class="line x" title="13:255	Below is a typical word masal with three possible parses." ></td>
	<td class="line x" title="14:255	masal+Noun+A3sg+Pnon+Acc(= the story) masal+Noun+A3sg+P3sg+Nom(= his story) masa+Noun+A3sg+Pnon+Nom DB+Adj+With (= with tables) Table 1: Three parses of the word masal The rst two parses start with the same root, masal (= story, fable), but the interpretation of the following + suf x is the Accusative marker in one case, and third person possessive agreement in the other." ></td>
	<td class="line x" title="15:255	The third parse starts with a different root, masa (= table) followed by a derivational suf x +l (= with) which turns the noun into an adjective." ></td>
	<td class="line x" title="16:255	The symbol DB represents a derivational boundary and splits the parse into chunks called in ectional groups (IGs).1 We will use the term feature to refer to individual morphological features like +Acc and +With; the term IG to refer to groups of features split by derivational boundaries ( DB), and the term tag to refer to the sequence of IGs following the root." ></td>
	<td class="line x" title="17:255	Morphological disambiguation is a useful rst step for higher level analysis of any language but it is especially critical for agglutinative languages like Turkish, Czech, Hungarian, and Finnish." ></td>
	<td class="line x" title="18:255	These languages have a relatively free constituent order, and 1See (Oflazer et al. , 1999) for a detailed description of the morphological features used in this paper." ></td>
	<td class="line x" title="19:255	328 syntactic relations are partly determined by morphological features." ></td>
	<td class="line x" title="20:255	Many applications including syntactic parsing, word sense disambiguation, text to speech synthesis and spelling correction depend on accurate analyses of words." ></td>
	<td class="line x" title="21:255	An important qualitative difference between part of speech tagging in English and morphological disambiguation in an agglutinative language like Turkish is the number of possible tags that can be assigned to a word." ></td>
	<td class="line x" title="22:255	Typical English tag sets include less than a hundred tag types representing syntactic and morphological information." ></td>
	<td class="line x" title="23:255	The number of potential morphological tags in Turkish is theoretically unlimited." ></td>
	<td class="line x" title="24:255	We have observed more than ten thousand tag types in our training corpus of a million words." ></td>
	<td class="line x" title="25:255	The high number of possible tags poses a data sparseness challenge for the typical machine learning approach, somewhat akin to what we observe in word sense disambiguation." ></td>
	<td class="line x" title="26:255	One way out of this dilemma could be to ignore the detailed morphological structure of the word and focus on determining only the major and minor parts of speech." ></td>
	<td class="line x" title="27:255	However (Oflazer et al. , 1999) observes that the modi er words in Turkish can have dependencies to any one of the in ectional groups of a derived word." ></td>
	<td class="line x" title="28:255	For example, in mavi masal oda (= the room with a blue table) the adjective mavi (= blue) modi es the noun root masa (= table) even though the nal part of speech of masal is an adjective." ></td>
	<td class="line x" title="29:255	Therefore, the nal part of speech and inection of a word do not carry suf cient information for the identi cation of the syntactic dependencies it is involved in." ></td>
	<td class="line x" title="30:255	One needs the full morphological analysis." ></td>
	<td class="line x" title="31:255	Our approach to the data sparseness problem is to consider each morphological feature separately." ></td>
	<td class="line x" title="32:255	Even though the number of potential tags is unlimited, the number of morphological features is small: The Turkish morphological analyzer we use (Oflazer, 1994) produces tags that consist of 126 unique features." ></td>
	<td class="line x" title="33:255	For each unique feature f, we take the subset of the training data in which one of the parses for each instance contain f. We then split this subset into positive and negative examples depending on whether the correct parse contains the feature f. These examples are used to learn rules using the Greedy Prepend Algorithm (GPA), a novel decision list learner." ></td>
	<td class="line x" title="34:255	To predict the tag of an unknown word, rst the morphological analyzer is used to generate all its possible parses." ></td>
	<td class="line x" title="35:255	The decision lists are then used to predict the presence or absence of each of the features contained in the candidate parses." ></td>
	<td class="line x" title="36:255	The results are probabilistically combined taking into account the accuracy of each decision list to select the best parse." ></td>
	<td class="line x" title="37:255	The resulting tagging accuracy is 96% on a hand tagged test set." ></td>
	<td class="line x" title="38:255	A more direct approach would be to train a single decision list using the full tags as the target classi cation." ></td>
	<td class="line x" title="39:255	Given a word in context, such a decision list assigns a complete morphological tag instead of predicting individual morphological features." ></td>
	<td class="line x" title="40:255	As such, it does not need the output of a morphological analyzer and should be considered a tagger rather than a disambiguator." ></td>
	<td class="line x" title="41:255	For comparison, such a decision list was built, and its accuracy was determined to be 91% on the same test set." ></td>
	<td class="line x" title="42:255	The main reason we chose to work with decision lists and the GPA algorithm is their robustness to irrelevant or redundant features." ></td>
	<td class="line x" title="43:255	The input to the decision lists include the suf xes of all possible lengths and character type information within a ve word window." ></td>
	<td class="line x" title="44:255	Each instance ends up with 40 attributes on average which are highly redundant and mostly irrelevant." ></td>
	<td class="line x" title="45:255	GPA is able to sort out the relevant features automatically and build a fairly accurate model." ></td>
	<td class="line x" title="46:255	Our experiments with Naive Bayes resulted in a significantly worse performance." ></td>
	<td class="line x" title="47:255	Typical statistical approaches include the tags of the previous words as inputs in the model." ></td>
	<td class="line x" title="48:255	GPA was able to deliver good performance without using the previous tags as inputs, because it was able to extract equivalent information implicit in the surface attributes." ></td>
	<td class="line x" title="49:255	Finally, unlike most statistical approaches, the resulting models of GPA are human readable and open to interpretation as Section 3.1 illustrates." ></td>
	<td class="line x" title="50:255	The next section will review related work." ></td>
	<td class="line x" title="51:255	Section 3 introduces decision lists and the GPA training algorithm." ></td>
	<td class="line x" title="52:255	Section 4 presents the experiments and the results." ></td>
	<td class="line x" title="53:255	2 Related Work There is a large body of work on morphological disambiguation and part of speech tagging using a variety of rule-based and statistical approaches." ></td>
	<td class="line x" title="54:255	In the 329 rule-based approach a large number of hand crafted rules are used to select the correct morphological parse or POS tag of a given word in a given context (Karlsson et al. , 1995; Oflazer and Tcurrency1ur, 1997)." ></td>
	<td class="line x" title="55:255	In the statistical approach a hand tagged corpus is used to train a probabilistic model which is then used to select the best tags in unseen text (Church, 1988; Hakkani-Tcurrency1ur et al. , 2002)." ></td>
	<td class="line x" title="56:255	Examples of statistical and machine learning approaches that have been used for tagging include transformation based learning (Brill, 1995), memory based learning (Daelemans et al. , 1996), and maximum entropy models (Ratnaparkhi, 1996)." ></td>
	<td class="line oc" title="57:255	It is also possible to train statistical models using unlabeled data with the expectation maximization algorithm (Cutting et al. , 1992)." ></td>
	<td class="line x" title="58:255	Van Halteren (1999) gives a comprehensive overview of syntactic word-class tagging." ></td>
	<td class="line x" title="59:255	Previous work on morphological disambiguation of in ectional or agglutinative languages include unsupervised learning for of Hebrew (Levinger et al. , 1995), maximum entropy modeling for Czech (Haji c and Hladka, 1998), combination of statistical and rule-based disambiguation methods for Basque (Ezeiza et al. , 1998), transformation based tagging for Hungarian (Megyesi, 1999)." ></td>
	<td class="line x" title="60:255	Early work on Turkish used a constraint-based approach with hand crafted rules (Oflazer and Kurucurrency1oz, 1994)." ></td>
	<td class="line x" title="61:255	A purely statistical morphological disambiguation model was recently introduced (HakkaniTcurrency1ur et al. , 2002)." ></td>
	<td class="line x" title="62:255	To counter the data sparseness problem the morphological parses are split across their derivational boundaries and certain independence assumptions are made in the prediction of each in ectional group." ></td>
	<td class="line x" title="63:255	A combination of three ideas makes our approach unique in the eld: (1) the use of decision lists and a novel learning algorithm that combine the statistical and rule based techniques, (2) the treatment of each individual feature separately to address the data sparseness problem, and (3) the lack of dependence on previous tags and relying on surface attributes alone." ></td>
	<td class="line x" title="64:255	3 Decision Lists We introduce a new method for morphological disambiguation based on decision lists." ></td>
	<td class="line x" title="65:255	A decision list is an ordered list of rules where each rule consists of a pattern and a classi cation (Rivest, 1987)." ></td>
	<td class="line x" title="66:255	In our application the pattern speci es the surface attributes of the words surrounding the target such as suf xes and character types (e.g. upper vs. lower case, use of punctuation, digits)." ></td>
	<td class="line x" title="67:255	The classi cation indicates the presence or absence of a morphological feature for the center word." ></td>
	<td class="line x" title="68:255	3.1 A Sample Decision List We will explain the rules and their patterns using the sample decision list in Table 2 trained to identify the feature +Det (determiner)." ></td>
	<td class="line x" title="69:255	Rule Class Pattern 1 1 W= cok R1=+DA 2 1 L1= pek 3 0 W=+AzI 4 0 W= cok 5 1 Table 2: A ve rule decision list for +Det The value in the class column is 1 if word W should have a +Det feature and 0 otherwise." ></td>
	<td class="line x" title="70:255	The pattern column describes the required attributes of the words surrounding the target word for the rule to match." ></td>
	<td class="line x" title="71:255	The last (default) rule has no pattern, matches every instance, and assigns them +Det. This default rule captures the behavior of the majority of the training instances which had +Det in their correct parse." ></td>
	<td class="line x" title="72:255	Rule 4 indicates a common exception: the frequently used word cok (meaning very) should not be assigned +Det by default: cok can be also used as an adjective, an adverb, or a postposition." ></td>
	<td class="line x" title="73:255	Rule 1 introduces an exception to rule 4: if the right neighbor R1 ends with the suf x +DA (the locative suf x) then cok should receive +Det. The meanings of various symbols in the patterns are described below." ></td>
	<td class="line x" title="74:255	When the decision list is applied to a window of words, the rules are tried in the order from the most speci c (rule 1) to the most general (rule 5)." ></td>
	<td class="line x" title="75:255	The rst rule that matches is used to predict the classi cation of the center word." ></td>
	<td class="line x" title="76:255	The last rule acts as a catch-all; if none of the other rules have matched, this rule assigns the instance a default classi cation." ></td>
	<td class="line x" title="77:255	For example, the ve rule decision list given above classi es the middle word in pek cok alanda (matches rule 330 W target word A [ae] L1, L2 left neighbors I [ iucurrency1u] R1, R2 right neighbors D [dt] == exact match B [bp] = case insensitive match C [cc] =+ is a suf x of K [kg g] Table 3: Symbols used in the rule patterns." ></td>
	<td class="line x" title="78:255	Capital letters on the right represent character groups useful in identifying phonetic variations of certain suf xes, e.g. the locative suf x +DA can surface as +de, +da, +te, or +ta depending on the root word ending." ></td>
	<td class="line x" title="79:255	1) and pek cok insan (matches rule 2) as +Det, but insan cok daha (matches rule 4) as not +Det. One way to interpret a decision list is as a sequence of if-then-else constructs familiar from programming languages." ></td>
	<td class="line x" title="80:255	Another way is to see the last rule as the default classi cation, the previous rule as specifying a set of exceptions to the default, the rule before that as specifying exceptions to these exceptions and so on." ></td>
	<td class="line x" title="81:255	3.2 The Greedy Prepend Algorithm (GPA) To learn a decision list from a given set of training examples the general approach is to start with a default rule or an empty decision list and keep adding the best rule to cover the unclassi ed or misclassied examples." ></td>
	<td class="line x" title="82:255	The new rules can be added to the end of the list (Clark and Niblett, 1989), the front of the list (Webb and Brkic, 1993), or other positions (Newlands and Webb, 2004)." ></td>
	<td class="line x" title="83:255	Other design decisions include the criteria used to select the best rule and how to search for it." ></td>
	<td class="line x" title="84:255	The Greedy Prepend Algorithm (GPA) is a variant of the PREPEND algorithm (Webb and Brkic, 1993)." ></td>
	<td class="line x" title="85:255	It starts with a default rule that matches all instances and classi es them using the most common class in the training data." ></td>
	<td class="line x" title="86:255	Then it keeps prepending the rule with the maximum gain to the front of the growing decision list until no further improvement can be made." ></td>
	<td class="line x" title="87:255	The algorithm can be described as follows: GPA(data) 1 dlist NIL 2 default-class MOST-COMMON-CLASS(data) 3 rule [if TRUE then default-class] 4 while GAIN(rule, dlist, data) > 0 5 do dlist prepend(rule, dlist) 6 rule MAX-GAIN-RULE(dlist, data) 7 return dlist The gain of a candidate rule in GPA is de ned as the increase in the number of correctly classi ed instances in the training set as a result of prepending the rule to the existing decision list." ></td>
	<td class="line x" title="88:255	This is in contrast with the original PREPEND algorithm which uses the less direct Laplace preference function (Webb and Brkic, 1993; Clark and Boswell, 1991)." ></td>
	<td class="line x" title="89:255	To nd the next rule with the maximum gain, GPA uses a heuristic search algorithm." ></td>
	<td class="line x" title="90:255	Candidate rules are generated by adding a single new attribute to the pattern of each rule already in the decision list." ></td>
	<td class="line x" title="91:255	The candidate with the maximum gain is prepended to the decision list and the process is repeated until no more positive gain rules can be found." ></td>
	<td class="line x" title="92:255	Note that if the best possible rule has more than one extra attribute compared to the existing rules in the decision list, a suboptimal rule will be selected." ></td>
	<td class="line x" title="93:255	The original PREPEND uses an admissible search algorithm, OPUS, which is guaranteed to nd the best possible candidate (Webb, 1995), but we found OPUS to be too slow to be practical for a problem of this scale." ></td>
	<td class="line x" title="94:255	We picked GPA for the morphological disambiguation problem because we nd it to be fast and fairly robust to the existence of irrelevant or redundant attributes." ></td>
	<td class="line x" title="95:255	The average training instance has 40 attributes describing the suf xes of all possible lengths and character type information in a ve word window." ></td>
	<td class="line x" title="96:255	Most of this information is redundant or irrelevant to the problem at hand." ></td>
	<td class="line x" title="97:255	The number of distinct attributes is on the order of the number of distinct word-forms in the training set." ></td>
	<td class="line x" title="98:255	Nevertheless GPA is able to process a million training instances for each of the 126 unique morphological features and produce a model with state of the art accuracy in about two hours on a regular desktop PC.2 2Pentium 4 CPU 2.40GHz 331 4 Experiments and Results In this section we present the details of the data, the training and testing procedures, the surface attributes used, and the accuracy results." ></td>
	<td class="line x" title="99:255	4.1 Training Data documents 2383 sentences 50673 tokens 948404 parses 1.76 per token IGs 1.33 per parse features 3.29 per IG unique tokens 111467 unique tags 11084 unique IGs 2440 unique features 126 ambiguous tokens 399223 (42.1%) Table 4: Statistics for the training data Our training data consists of about 1 million words of semi-automatically disambiguated Turkish news text." ></td>
	<td class="line x" title="100:255	For each one of the 126 unique morphological features, we used the subset of the training data in which instances have the given feature in at least one of their generated parses." ></td>
	<td class="line x" title="101:255	We then split this subset into positive and negative examples depending on whether the correct parse contains the given feature." ></td>
	<td class="line x" title="102:255	A decision list speci c to that feature is created using GPA based on these examples." ></td>
	<td class="line x" title="103:255	Some relevant statistics for the training data are given in Table 4." ></td>
	<td class="line x" title="104:255	4.2 Input Attributes Once the training data is selected for a particular morphological feature, each instance is represented by surface attributes of ve words centered around the target word." ></td>
	<td class="line x" title="105:255	We have tried larger window sizes but no signi cant improvement was observed." ></td>
	<td class="line x" title="106:255	The attributes computed for each word in the window consist of the following: 1." ></td>
	<td class="line x" title="107:255	The exact word string (e.g. W==Alinin) 2." ></td>
	<td class="line x" title="108:255	The lowercase version (e.g. W= alinin) Note: all digits are replaced by 0s at this stage." ></td>
	<td class="line x" title="109:255	3." ></td>
	<td class="line x" title="110:255	All suf xes of the lowercase version (e.g. W=+n, W=+In, W=+nIn, W=+nIn, etc)." ></td>
	<td class="line x" title="111:255	Note: certain characters are replaced with capital letters representing character groups mentioned in Table 3." ></td>
	<td class="line x" title="112:255	These groups help the algorithm recognize different forms of a suf x created by the phonetic rules of Turkish: for example the locative suf x +DA can surface as +de, +da, +te, or +ta depending on the ending of the root word." ></td>
	<td class="line x" title="113:255	4." ></td>
	<td class="line x" title="114:255	Attributes indicating the types of characters at various positions of the word (e.g. Alinin would be described with W=UPPER-FIRST, W=LOWER-MID, W=APOS-MID, W=LOWERLAST) Each training instance is represented by 40 attributes on average." ></td>
	<td class="line x" title="115:255	The GPA procedure is responsible for picking the attributes that are relevant to the decision." ></td>
	<td class="line x" title="116:255	No dictionary information is required or used, therefore the models are fairly robust to unknown words." ></td>
	<td class="line x" title="117:255	One potentially useful source of attributes is the tags assigned to previous words which we plan to experiment with in future work." ></td>
	<td class="line x" title="118:255	4.3 The Decision Lists At the conclusion of the training, 126 decision lists are produced of the form given in Table 2." ></td>
	<td class="line x" title="119:255	The number of rules in each decision list range from 1 to 6145." ></td>
	<td class="line x" title="120:255	The longer decision lists are typically for part of speech features, e.g. distinguishing nouns from adjectives, and contain rules speci c to lexical items." ></td>
	<td class="line x" title="121:255	The average number of rules is 266." ></td>
	<td class="line x" title="122:255	To get an estimate on the accuracy of each decision list, we split the one million word data into training, validation, and test portions using the ratio 4:1:1." ></td>
	<td class="line x" title="123:255	The training set accuracy of the decision lists is consistently above 98%." ></td>
	<td class="line x" title="124:255	The test set accuracies of the 126 decision lists range from 80% to 100% with the average at 95%." ></td>
	<td class="line x" title="125:255	Table 5 gives the six worst features with test set accuracy below 89%; these are the most dif cult to disambiguate." ></td>
	<td class="line x" title="126:255	4.4 Correct Tag Selection To evaluate the candidate tags, we need to combine the results of the decision lists." ></td>
	<td class="line x" title="127:255	We assume that the presence or absence of each feature is an independent event with a probability determined by the test set accuracy of the corresponding decision list." ></td>
	<td class="line x" title="128:255	For example, if the +P3pl decision list predicts YES, we assume that the +P3pl feature is present with 332 87.89% +Acquire To acquire (noun) 86.18% +PCIns Postposition subcat." ></td>
	<td class="line x" title="129:255	85.11% +Fut Future tense 84.08% +P3pl 3." ></td>
	<td class="line x" title="130:255	plural possessive 80.79% +Neces Must 79.81% +Become To become (noun) Table 5: The six features with the worst test set accuracy." ></td>
	<td class="line x" title="131:255	probability 0.8408 (See Table 5)." ></td>
	<td class="line x" title="132:255	If the +Fut decision list predicts NO, we assume the +Futfeature is present with probability 10.8511 = 0.1489." ></td>
	<td class="line x" title="133:255	To avoid zero probabilities we cap the test set accuracies at 99%." ></td>
	<td class="line x" title="134:255	Each candidate tag indicates the presence of certain features and the absence of others." ></td>
	<td class="line x" title="135:255	The probability of the tag being correct under our independence assumption is the product of the probabilities for the presence and absence of each of the 126 features as determined by our decision lists." ></td>
	<td class="line x" title="136:255	For ef ciency, one can neglect the features that are absent from all the candidate tags because their contribution will not effect the comparison." ></td>
	<td class="line x" title="137:255	4.5 Results The nal evaluation of the model was performed on a test data set of 958 instances." ></td>
	<td class="line x" title="138:255	The possible parses for each instance were generated by the morphological analyzer and the correct one was picked manually." ></td>
	<td class="line x" title="139:255	40% of the instances were ambiguous, which on the average had 3.9 parses." ></td>
	<td class="line x" title="140:255	The disambiguation accuracy of our model was 95.82%." ></td>
	<td class="line x" title="141:255	The 95% con dence interval for the accuracy is [0.9457, 0.9708]." ></td>
	<td class="line x" title="142:255	An analysis of the mistakes in the test data show that at least some of them are due to incorrect tags in our training data." ></td>
	<td class="line x" title="143:255	The training data was semiautomatically generated and thus contained some errors." ></td>
	<td class="line x" title="144:255	Based on hand evaluation of the differences between the training data tags and the GPA generated tags, we estimate the accuracy of the training data to be below 95%." ></td>
	<td class="line x" title="145:255	We ran two further experiments to see if we could improve on the initial results." ></td>
	<td class="line x" title="146:255	In our rst experiment we used our original model to re-tag the training data." ></td>
	<td class="line x" title="147:255	The re-tagged training data was used to construct a new model." ></td>
	<td class="line x" title="148:255	The resulting accuracy on the test set increased to 96.03%, not a statistically signi cant improvement." ></td>
	<td class="line x" title="149:255	In our second experiment we used only unambiguous instances for training." ></td>
	<td class="line x" title="150:255	Decision list training requires negative examples, so we selected random unambiguous instances for positive and negative examples for each feature." ></td>
	<td class="line x" title="151:255	The accuracy of the resulting model on the test set was 82.57%." ></td>
	<td class="line x" title="152:255	The problem with selecting unambiguous instances is that certain common disambiguation decisions are never represented during training." ></td>
	<td class="line x" title="153:255	More careful selection of negative examples and a sophisticated bootstrapping mechanism may still make this approach workable." ></td>
	<td class="line x" title="154:255	Finally, we decided to see if our decision lists could be used for tagging rather than disambiguation, i.e. given a word in a context decide on the full tag without the help of a morphological analyzer." ></td>
	<td class="line x" title="155:255	Even though the number of possible tags is unlimited, the most frequent 1000 tags cover about 99% of the instances." ></td>
	<td class="line x" title="156:255	A single decision list trained with the full tags was able to achieve 91.23% accuracy using 10000 rules." ></td>
	<td class="line x" title="157:255	This is a promising result and will be explored further in future work." ></td>
	<td class="line x" title="158:255	5 Contributions We have presented an automated approach to learn morphological disambiguation rules for Turkish using a novel decision list induction algorithm, GPA." ></td>
	<td class="line x" title="159:255	The only input to the rules are the surface attributes of a ve word window." ></td>
	<td class="line x" title="160:255	The approach can be generalized to other agglutinative languages which share the common challenge of a large number of potential tags." ></td>
	<td class="line x" title="161:255	Our approach for resolving the data sparseness problem caused by the large number of tags is to generate a separate model for each morphological feature." ></td>
	<td class="line x" title="162:255	The predictions for individual features are probabilistically combined based on the accuracy of each model to select the best tag." ></td>
	<td class="line x" title="163:255	We were able to achieve an accuracy around 96% using this approach." ></td>
	<td class="line x" title="164:255	Acknowledgments We would like to thank Kemal O azer of Sabanc University for providing us with the Turkish morphological analyzer, training and testing data for disambiguation, and valuable feedback." ></td>
	<td class="line x" title="165:255	333 References Brill, E." ></td>
	<td class="line x" title="166:255	(1995)." ></td>
	<td class="line x" title="167:255	Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging." ></td>
	<td class="line x" title="168:255	Computational Linguistics, 21(4):543 565." ></td>
	<td class="line x" title="169:255	Church, K. W." ></td>
	<td class="line x" title="170:255	(1988)." ></td>
	<td class="line x" title="171:255	A stochastic parts program and noun phrase parser for unrestricted text." ></td>
	<td class="line x" title="172:255	In Proceedings of the Second Conference on Applied Natural Language Processing, pages 136 143." ></td>
	<td class="line x" title="173:255	Clark, P. and Boswell, R." ></td>
	<td class="line x" title="174:255	(1991)." ></td>
	<td class="line x" title="175:255	Rule induction with CN2: Some recent improvements." ></td>
	<td class="line x" title="176:255	In Kodratoff, Y. , editor, Machine Learning Proceedings of the Fifth European Conference (EWSL-91), pages 151 163, Berlin." ></td>
	<td class="line x" title="177:255	Springer-Verlag." ></td>
	<td class="line x" title="178:255	Clark, P. and Niblett, T." ></td>
	<td class="line x" title="179:255	(1989)." ></td>
	<td class="line x" title="180:255	The CN2 induction algorithm." ></td>
	<td class="line x" title="181:255	Machine Learning, 3:261 283." ></td>
	<td class="line x" title="182:255	Cutting, D. , Kupiec, J. , Pedersen, J. , and Sibun, P." ></td>
	<td class="line x" title="183:255	(1992)." ></td>
	<td class="line x" title="184:255	A practical part-of-speech tagger." ></td>
	<td class="line x" title="185:255	In Proceedings of the 3rd Conference on Applied Language Processing, pages 133 140." ></td>
	<td class="line x" title="186:255	Daelemans, W. et al.(1996)." ></td>
	<td class="line x" title="188:255	MBT: A memory-based part of speech tagger-generator." ></td>
	<td class="line x" title="189:255	In Ejerhead, E. and Dagan, I. , editors, Proceedings of the Fourth Workshop on Very Large Corpora, pages 14 27." ></td>
	<td class="line x" title="190:255	Ezeiza, N. et al.(1998)." ></td>
	<td class="line x" title="192:255	Combining stochastic and rulebased methods for disambiguation in agglutinative languages." ></td>
	<td class="line x" title="193:255	In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics (COLING/ACL98), pages 379 384." ></td>
	<td class="line x" title="194:255	Haji c, J. and Hladka, B." ></td>
	<td class="line x" title="195:255	(1998)." ></td>
	<td class="line x" title="196:255	Tagging in ective languages: Prediction of morphological categories for a rich, structured tagset." ></td>
	<td class="line x" title="197:255	In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics (COLING/ACL98), pages 483 490, Montreal, Canada." ></td>
	<td class="line x" title="198:255	Hakkani-Tcurrency1ur, D. Z., Oflazer, K. , and Tcurrency1ur, G." ></td>
	<td class="line x" title="199:255	(2002)." ></td>
	<td class="line x" title="200:255	Statistical morphological disambiguation for agglutinative languages." ></td>
	<td class="line x" title="201:255	Computers and the Humanities, 36:381 410." ></td>
	<td class="line x" title="202:255	Karlsson, F. , Voutialinen, A. , Heikkilcurrency1a, J. , and Anttila, A." ></td>
	<td class="line x" title="203:255	(1995)." ></td>
	<td class="line x" title="204:255	Constraint Grammar A Language Independent System for Parsing Unrestricted Text." ></td>
	<td class="line x" title="205:255	Mouton de Gruyter." ></td>
	<td class="line x" title="206:255	Levinger, M. , Ornan, U. , and Itai, A." ></td>
	<td class="line x" title="207:255	(1995)." ></td>
	<td class="line x" title="208:255	Learning morpho-lexical probabilities from an untagged corpus with an application to hebrew." ></td>
	<td class="line x" title="209:255	Computational Linguistics, 21(3):383 404." ></td>
	<td class="line x" title="210:255	Megyesi, B." ></td>
	<td class="line x" title="211:255	(1999)." ></td>
	<td class="line x" title="212:255	Improving brills pos tagger for an agglutinative language." ></td>
	<td class="line x" title="213:255	In Pascale, F. and Joe, Z. , editors, Proceedings of the Joing SIGDAT Conference on Empirical Methods in Natural Language and Very Large Corpora, pages 275 284, College Park, Maryland, USA." ></td>
	<td class="line x" title="214:255	Newlands, D. and Webb, G. I." ></td>
	<td class="line x" title="215:255	(2004)." ></td>
	<td class="line x" title="216:255	Alternative strategies for decision list construction." ></td>
	<td class="line x" title="217:255	In Proceedings of the Fourth Data Mining Conference (DM IV 03), pages 265 273." ></td>
	<td class="line x" title="218:255	Oflazer, K." ></td>
	<td class="line x" title="219:255	(1994)." ></td>
	<td class="line x" title="220:255	Two-level description of turkish morphology." ></td>
	<td class="line x" title="221:255	Literary and Linguistic Computing, 9(2):137 148." ></td>
	<td class="line x" title="222:255	Oflazer, K. , Hakkani-Tcurrency1ur, D. Z., and Tcurrency1ur, G." ></td>
	<td class="line x" title="223:255	(1999)." ></td>
	<td class="line x" title="224:255	Design for a turkish treebank." ></td>
	<td class="line x" title="225:255	In Proceedings of the Workshop on Linguistically Interpreted Corpora, EACL 99, Bergen, Norway." ></td>
	<td class="line x" title="226:255	Oflazer, K. and Kurucurrency1oz, I." ></td>
	<td class="line x" title="227:255	(1994)." ></td>
	<td class="line x" title="228:255	Tagging and morphological disambiguation of turkish text." ></td>
	<td class="line x" title="229:255	In Proceedings of the 4th Applied Natural Language Processing Conference, pages 144 149." ></td>
	<td class="line x" title="230:255	ACL." ></td>
	<td class="line x" title="231:255	Oflazer, K. and Tcurrency1ur, G." ></td>
	<td class="line x" title="232:255	(1997)." ></td>
	<td class="line x" title="233:255	Morphological disambiguation by voting constraints." ></td>
	<td class="line x" title="234:255	In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL97, EACL97), Madrid, Spain." ></td>
	<td class="line x" title="235:255	Ratnaparkhi, A." ></td>
	<td class="line x" title="236:255	(1996)." ></td>
	<td class="line x" title="237:255	A maximum entropy model for part-of-speech tagging." ></td>
	<td class="line x" title="238:255	In Proceedings of the Conference on Empirical Methods in Natural Language Processing." ></td>
	<td class="line x" title="239:255	Rivest, R. L." ></td>
	<td class="line x" title="240:255	(1987)." ></td>
	<td class="line x" title="241:255	Learning decision lists." ></td>
	<td class="line x" title="242:255	Machine Learning, 2:229 246." ></td>
	<td class="line x" title="243:255	van Halteren, H. , editor (1999)." ></td>
	<td class="line x" title="244:255	Syntactic Wordclass Tagging." ></td>
	<td class="line x" title="245:255	Text, Speech and Language Technology." ></td>
	<td class="line x" title="246:255	Kluwer Academic Publishers." ></td>
	<td class="line x" title="247:255	Webb, G. I." ></td>
	<td class="line x" title="248:255	(1995)." ></td>
	<td class="line x" title="249:255	Opus: An ef cient admissible algorithm for unordered search." ></td>
	<td class="line x" title="250:255	JAIR, 3:431 465." ></td>
	<td class="line x" title="251:255	Webb, G. I. and Brkic, N." ></td>
	<td class="line x" title="252:255	(1993)." ></td>
	<td class="line x" title="253:255	Learning decision lists by prepending inferred rules." ></td>
	<td class="line x" title="254:255	In Proceedings of the AI 93 Workshop on Machine Learning and Hybrid Systems, pages 6 10, Melbourne." ></td>
	<td class="line x" title="255:255	334" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-2100
Morphological Richness Offsets Resource Demand - Experiences In Constructing A POS Tagger For Hindi
Singh, Smriti;Gupta, Kuhoo;Shrivastava, Manish;Bhattacharyya, Pushpak;"></td>
	<td class="line x" title="1:183	Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 779786, Sydney, July 2006." ></td>
	<td class="line x" title="2:183	c2006 Association for Computational Linguistics Morphological Richness Offsets Resource DemandExperiences in Constructing a POS Tagger for Hindi Smriti Singh Kuhoo Gupta Department of Computer Science and Engineering Indian Institute of Technology, Bombay Powai, Mumbai 400076 Maharashtra, India {smriti,kuhoo,manshri,pb}@cse.iitb.ac.in Manish Shrivastava Pushpak Bhattacharyya Abstract In this paper we report our work on building a POS tagger for a morphologically rich languageHindi." ></td>
	<td class="line x" title="3:183	The theme of the research is to vindicate the stand thatif morphology is strong and harnessable, then lack of training corpora is not debilitating." ></td>
	<td class="line x" title="4:183	We establish a methodology of POS tagging which the resource disadvantaged (lacking annotated corpora) languages can make use of." ></td>
	<td class="line x" title="5:183	The methodology makes use of locally annotated modestly-sized corpora (15,562 words), exhaustive morpohological analysis backed by high-coverage lexicon and a decision tree based learning algorithm (CN2)." ></td>
	<td class="line x" title="6:183	The evaluation of the system was done with 4-fold cross validation of the corpora in the news domain (www.bbc.co.uk/hindi)." ></td>
	<td class="line x" title="7:183	The current accuracy of POS tagging is 93.45% and can be further improved." ></td>
	<td class="line x" title="8:183	1 Motivation and Problem De nition Part-Of-Speech (POS) tagging is a complex task fraught with challenges like ambiguity of parts of speech and handling of lexical absence (proper nouns, foreign words, derivationally morphed words, spelling variations and other unknown words) (Manning and Schutze, 2002)." ></td>
	<td class="line oc" title="9:183	For English there are many POS taggers, employing machine learning techniques like transformation-based error-driven learning (Brill, 1995), decision trees (Black et al. , 1992), markov model (Cutting et al. 1992), maximum entropy methods (Ratnaparkhi, 1996) etc. There are also taggers which are hybrid using both stochastic and rule-based approaches, such as CLAWS (Garside and Smith, 1997)." ></td>
	<td class="line o" title="10:183	The accuracy of these taggers ranges from 93-98% approximately." ></td>
	<td class="line x" title="11:183	English has annotated corpora in abundance, enabling usage of powerful data driven machine learning methods." ></td>
	<td class="line x" title="12:183	But, very few languages in the world have the resource advantage that English enjoys." ></td>
	<td class="line x" title="13:183	In this scenario, POS tagging of highly inectional languages presents an interesting case study." ></td>
	<td class="line x" title="14:183	Morphologically rich languages are characterized by a large number of morphemes in a single word, where morpheme boundaries are dif cult to detect because they are fused together." ></td>
	<td class="line x" title="15:183	They are typically free-word ordered, which causes xed-context systems to be hardly adequate for statistical approaches (Samuelsson and Voutilainen, 1997)." ></td>
	<td class="line x" title="16:183	Morphology-based POS tagging of some languages like Turkish (O azer and Kuruoz, 1994), Arabic (Guiassa, 2006), Czech (Hajic et al. , 2001), Modern Greek (Orphanos et al. , 1999) and Hungarian (Megyesi, 1999) has been tried out using a combination of hand-crafted rules and statistical learning." ></td>
	<td class="line x" title="17:183	These systems use large amount of corpora along with morphological analysis to POS tag the texts." ></td>
	<td class="line x" title="18:183	It may be noted that a purely rule-based or a purely stochastic approach will not be effective for such 779 languages, since the former demands subtle linguistic expertise and the latter variously permuted corpora." ></td>
	<td class="line x" title="19:183	1.1 Previous Work on Hindi POS Tagging There is some amount of work done on morphology-based disambiguation in Hindi POS tagging." ></td>
	<td class="line x" title="20:183	Bharati et al.(1995) in their work on computational Paninian parser, describe a technique where POS tagging is implicit and is merged with the parsing phase." ></td>
	<td class="line x" title="22:183	Ray et al.(2003) proposed an algorithm that identi es Hindi word groups on the basis of the lexical tags of the individual words." ></td>
	<td class="line x" title="24:183	Their partial POS tagger (as they call it) reduces the number of possible tags for a given sentence by imposing some constraints on the sequence of lexical categories that are possible in a Hindi sentence." ></td>
	<td class="line x" title="25:183	UPENN also has an online Hindi morphological tagger1 but there exists no literature discussing the performance of the tagger." ></td>
	<td class="line x" title="26:183	1.2 Our Approach We present in this paper a POS tagger for Hindithe national language of India, spoken by 500 million people and ranking 4th in the world." ></td>
	<td class="line x" title="27:183	We establish a methodology of POS tagging which the resource disadvantaged (lacking annotated corpora) languages can make use of." ></td>
	<td class="line x" title="28:183	This methodology uses locally annotated modestly sized corpora (15,562 words), exhaustive morphological analysis backed by highcoverage lexicon and a decision tree based learning algorithmCN2 (Clark and Niblett, 1989)." ></td>
	<td class="line x" title="29:183	To the best of our knowledge, such an approach has never been tried out for Hindi." ></td>
	<td class="line x" title="30:183	The heart of the system is the detailed linguistic analysis of morphosyntactic phenomena, adroit handling of suf xes, accurate verb group identi cation and learning of disambiguation rules." ></td>
	<td class="line x" title="31:183	The approach can be used for other in ectional languages by providing the language speci c resources in the form of suf x replacement rules (SRRs), lexicon, group identi cation and morpheme analysis rules etc. and keeping the 1http://ccat.sas.upenn.edu/plc/tamilweb/hindi.html processes the same as shown in Figure 1." ></td>
	<td class="line x" title="32:183	The similar kind of work exploiting morphological information to assign POS tags is under progress for Marathi which is also an Indian language." ></td>
	<td class="line x" title="33:183	In what follows, we discuss in section 2 the challenges in Hindi POS tagging followed by a section on morphological structure of Hindi." ></td>
	<td class="line x" title="34:183	Section 4 presents the design of Hindi POS tagger." ></td>
	<td class="line x" title="35:183	The experimental setup and results are given in sections 5 and 6." ></td>
	<td class="line x" title="36:183	Section 7 concludes the paper." ></td>
	<td class="line x" title="37:183	2 Challenges of POS Tagging in Hindi The inter-POS ambiguity surfaces when a word or a morpheme displays an ambiguity across POS categories." ></td>
	<td class="line x" title="38:183	Such a word has multiple entries in the lexicon (one for each category)." ></td>
	<td class="line x" title="39:183	After stemming, the word would be assigned all possible POS tags based on the number of entries it has in the lexicon." ></td>
	<td class="line x" title="40:183	The complexity of the task can be understood looking at the following English sentence where the word back falls into three different POS categoriesI get back to the back seat to give rest to my back." ></td>
	<td class="line x" title="41:183	The complexity further increases when it comes to tagging a free-word order language like Hindi where almost all the permutations of words in a clause are possible (Shrivastava et al. , 2005)." ></td>
	<td class="line x" title="42:183	This phenomenon in the language, makes the task of a stochastic tagger dif cult." ></td>
	<td class="line x" title="43:183	Intra-POS ambiguity arises when a word has one POS with different feature values, e.g., the word a0a2a1a4a3a5  {laDke} (boys/boy) in Hindi is a noun but can be analyzed in two ways in terms of its feature values: 1." ></td>
	<td class="line x" title="44:183	POS: Noun, Number: Sg, Case: Oblique a6a7a9a8a11a10a5a12a0a2a1a13a3a5a14a3a16a15a14a17a4a3a19a18a21a20a22a6a24a23a26a25a28a27a29a20." ></td>
	<td class="line x" title="45:183	maine laDke ko ek aam diyaa." ></td>
	<td class="line x" title="46:183	I-erg boy to one mango gave." ></td>
	<td class="line x" title="47:183	I gave a mango to the boy." ></td>
	<td class="line x" title="48:183	2." ></td>
	<td class="line x" title="49:183	POS: Noun, Number: Pl, Case: Direct a0a30a1a4a3a5a31a18a21a20a22a6a33a32a2a20a22a34a5a12a35a7a36a8 . laDke aam khaate hain." ></td>
	<td class="line x" title="50:183	Boys mangoes eat." ></td>
	<td class="line x" title="51:183	Boys eat mangoes." ></td>
	<td class="line x" title="52:183	780 One of the dif cult tasks here is to choose the appropriate tag based on the morphology of the word and the context used." ></td>
	<td class="line x" title="53:183	Also, new words appear all the time in the texts." ></td>
	<td class="line x" title="54:183	Thus, a method for determining the tag of a new word is needed when it is not present in the lexicon." ></td>
	<td class="line x" title="55:183	This is done using context information and the information coded in the af xes, as af xes in Hindi (especially in nouns and verbs) are strong indicators of a words POS category." ></td>
	<td class="line x" title="56:183	For example, it is possible to determine that the word a37 a20a22a17a4a38a39a20  {jaaegaa} (will go) is a verb, based on the environment in which it appears and the knowledge that it carries the in ectional suf x -a17a4a38a39a20 {egaa} that attaches to the base verb a37 a20  {jaa}." ></td>
	<td class="line x" title="57:183	2.1 Ambiguity Schemes The criterion to decide whether the tag of a word is a Noun or a Verb is entirely different from that of whether a word is an Adjective or an Adverb." ></td>
	<td class="line x" title="58:183	For example, the word a40a42a41  can occur as conjunction, post-position or a noun (as shown previously), hence it falls in an Ambiguity Scheme Conjunction-Noun-Postposition." ></td>
	<td class="line x" title="59:183	We grouped all the ambiguous words into sets according to the Ambiguity Schemes that are possible in Hindi, e.g., Adjective-Noun, Adjective-Adverb, NounVerb, etc. This idea was rst proposed by Orphanos et al.(1999) for Modern Greek POS tagging." ></td>
	<td class="line x" title="61:183	3 Morphological Structure Of Hindi In Hindi, Nouns in ect for number and case." ></td>
	<td class="line x" title="62:183	To capture their morphological variations, they can be categorized into various paradigms2 (Narayana, 1994) based on their vowel ending, gender, number and case information." ></td>
	<td class="line x" title="63:183	We have a list of around 29,000 Hindi nouns that are categorized into such paradigms3." ></td>
	<td class="line x" title="64:183	Looking at the morphological patterns of the words in a paradigm, suf x-replacement rules have been developed." ></td>
	<td class="line x" title="65:183	These rules help in separating out a valid suf x 2A paradigm systematically arranges and identi es the unin ected forms of the words that share similar in ectional patterns." ></td>
	<td class="line x" title="66:183	3Anusaaraka system developed at IIT Kanpur (INDIA) uses similar noun sets in the form of paradigms from an in ected word to output the correct stem and consequently, get the correct root." ></td>
	<td class="line x" title="67:183	Hindi Adjectives may be in ected or uninected, e.g., a43 a6a29a3a45a44a46a0a30a20  {chamkiilaa} (shiny), a18a21a47a49a48a50a20  {acchaa} (nice), a0a51a8a53a52a42a20  {lambaa} (long) in ect based on the number and case values of their head nouns while a54a55 a8a56a25 a41  {sundar} (beautiful), a57 a20 a41 a44  {bhaarii} (heavy) etc. do not in ect." ></td>
	<td class="line x" title="68:183	Hindi Verbs in ect for the following grammatical properties (GNPTAM): 1." ></td>
	<td class="line x" title="69:183	Gender: Masculine, Feminine, Nonspeci c 2." ></td>
	<td class="line x" title="70:183	Number: Singular, Plural, Non-speci c 3." ></td>
	<td class="line x" title="71:183	Person: 1st, 2nd and 3rd 4." ></td>
	<td class="line x" title="72:183	Tense: Past, Present, Future 5." ></td>
	<td class="line x" title="73:183	Aspect: Perfective, Completive, Frequentative, Habitual, Durative, Inceptive, Stative 6." ></td>
	<td class="line x" title="74:183	Modality: Imperative, Probabilitive, Subjunctive, Conditional, Deontic, Abilitive, Permissive The morphemes attached to a verb along with their corresponding analyses help identify values for GNPTAM features for a given verb form." ></td>
	<td class="line x" title="75:183	Division of Information Load in Hindi Verb Groups A Verb Group (VG) primarily comprises main verb and auxiliaries." ></td>
	<td class="line x" title="76:183	Constituents like particles, negation markers, conjunction, etc. can also occur within a VG." ></td>
	<td class="line x" title="77:183	It is important to know how much of GNPTAM feature information is stored in VG constituents individually and what is the load division in the absence or presence of auxiliaries." ></td>
	<td class="line x" title="78:183	In a Hindi VG, when there is no auxiliary present, the complete information load falls on the main verb which carries information for GNPTAM features." ></td>
	<td class="line x" title="79:183	In presence of auxiliaries, the load gets shared between the main verb and auxiliaries, and is represented in the form of different morphemes (in ected or unin ected), e.g., in the sentence 781 a6a7a9a8a29a52a26a15a42a0 a40 a20 a41 a35a58a20a59a35a60a61 main bol paa rahaa hoon I am able to speak 1." ></td>
	<td class="line x" title="80:183	Main verb a52a22a15a42a0  {bol} is unin ected and does not carry any information for any of the GNPTAM features." ></td>
	<td class="line x" title="81:183	2." ></td>
	<td class="line x" title="82:183	a40 a20  {paa} is unin ected and gives modality information, i.e., Abilitive." ></td>
	<td class="line x" title="83:183	3." ></td>
	<td class="line x" title="84:183	a41 a35a58a20  {rahaa} gives Number (Sg), Gender (Masculine), Aspect (Durative) 4." ></td>
	<td class="line x" title="85:183	a35a60a61  {hoon} gives Number (Sg), Person (1st), Tense (Present) Gerund Identi cation In Hindi, the attachment of verbal suf xes like a10a4a20  {naa} and a10a5  {ne} to a verb root results either in a gerund like a34a7 a41 a10a13a20  {tairnaa} (swimming) or in an in nitival verb form like a34a7 a41 a10a4a20  {tairnaa} (to swim)." ></td>
	<td class="line x" title="86:183	We observed that it is easy to detect a gerund if it is followed by a casemarker or by any other in nitival verb form." ></td>
	<td class="line x" title="87:183	4 Design of Hindi POS Tagger 4.1 Morphology Driven Tagger Morphology driven tagger makes use of the af x information stored in a word and assigns a POS tag using no contextual information." ></td>
	<td class="line x" title="88:183	Though, it does take into account the previous and the next word in a VG to correctly identify the main verb and the auxiliaries, other POS categories are identi ed through lexicon lookup of the root form." ></td>
	<td class="line x" title="89:183	The current lexicon4 has around 42,000 entries belonging to the major categories as mentioned in Figure 3." ></td>
	<td class="line x" title="90:183	The format of each entry is word,paradigm,category." ></td>
	<td class="line x" title="91:183	The process does not involve learning or disambiguation of any sort and is completely driven by hand-crafted morphology rules." ></td>
	<td class="line x" title="92:183	The architecture of the tagger is shown in Figure 1." ></td>
	<td class="line x" title="93:183	The work progresses at two levels: 4The lexicon was developed using the wordlist from Hindi Wordnet (http://www.c lt.iitb.ac.in/wordnet/webhwn/) and partial noun list from Anusaraka." ></td>
	<td class="line x" title="94:183	It is being enhanced by adding new words from the corpus and removing the inconsistencies." ></td>
	<td class="line x" title="95:183	1." ></td>
	<td class="line x" title="96:183	At Word Level: A stemmer is used in conjunction with lexicon and Suf x Replacement Rules (SRRs) to output all possible root-suf x pairs along with POS category label for a word." ></td>
	<td class="line x" title="97:183	There is a possibility that the input word is not found in the lexicon and does not carry any in ectional suf x. In such a case, derivational morphology rules are applied." ></td>
	<td class="line x" title="98:183	2." ></td>
	<td class="line x" title="99:183	At Group Level: At this level a Morphological Analyzer (MA) uses the information encoded in the extracted suf x to add morphological information to the word." ></td>
	<td class="line x" title="100:183	For nouns, the information provided by the sufxes is restricted only to Number." ></td>
	<td class="line x" title="101:183	Case can be inferred later by looking at the neighbouring words." ></td>
	<td class="line x" title="102:183	For verbs, GNP values are found at the word level, while TAM values are identi ed during the VG Identi cation phase, described later." ></td>
	<td class="line x" title="103:183	The analysis of the suf x is done in a discrete manner, i.e., each component of the suf x is analyzed separately." ></td>
	<td class="line x" title="104:183	A morpheme analysis table comprising individual morphemes with their paradigm information and analyses is used for this purpose." ></td>
	<td class="line x" title="105:183	MAs output for the word a32a62a20a22a63a28a8a56a38a64a44 {khaaoongii} (will eat) looks like Stem: a32a2a20 (eat) Suf x: a63a28a8a53a38a26a44 Category: Verb Morpheme 1: a63a28a8 Analysis: 1 Per, Sg Morpheme 2: a38 Analysis: Future Morpheme 3: a65 Analysis: Feminine 4.1.1 Verb Group Identi cation The structure of a Hindi VG is relatively rigid and can be captured well using simple syntactic rules." ></td>
	<td class="line x" title="106:183	In Hindi, certain auxiliaries like a41 a35  {rah}, a40 a20  {paa}, a54 a3 , {sak} or a40 a1  {paD} can also occur as main verbs in some contexts." ></td>
	<td class="line x" title="107:183	VG identi cation deals with identifying the main verb and the auxiliaries of a VG while discounting for particles, conjunctions and negation markers." ></td>
	<td class="line x" title="108:183	The VG identi cation goes left to right by marking the rst constituent as the main verb or copula verb and making every other verb con782 Figure 1: Overall Architecture of the Tagger Table 1: Average Accuracy(%) Comparison of Various Approaches LLB LLBD MD BL LB 61.19 86.77 73.62 82.63 93.45 struct an auxiliary till a non-VG constituent is encountered." ></td>
	<td class="line x" title="109:183	Main verb and copula verb can take the head position of a VG and can occur with or without auxiliary verbs." ></td>
	<td class="line x" title="110:183	Auxiliary verbs, on the other hand, always come along with a main verb or a copula verb." ></td>
	<td class="line x" title="111:183	This results in a very high accuracy of 99.5% for verb auxiliaries." ></td>
	<td class="line x" title="112:183	Ambiguity between a main verb and a copula verb remains unresolved at this level and asks for disambiguation rules." ></td>
	<td class="line x" title="113:183	4.2 Need for Disambiguation The accuracy obtained by simple lexicon lookup based approach (LLB) comes out to be 61.19%." ></td>
	<td class="line x" title="114:183	The morphology-driven tagger, on the other hand, performs better than just lexicon lookup but still results in considerable ambiguity." ></td>
	<td class="line x" title="115:183	These results are signi cant as they present a strong case in favor of using detailed morphological analysis." ></td>
	<td class="line x" title="116:183	Similar observation has been presented by Uchimoto et al.(2001) for Japanese language." ></td>
	<td class="line x" title="118:183	According to the tagging performed by SRRs and the lexicon, a word receives n tags if it belongs to n POSs." ></td>
	<td class="line x" title="119:183	If we consider multiple tags for a word as an error of the tagger (even when the options contain the correct tag for a word), then the accuracy of the tagger comes to be 73.62% (as shown in Table 1)." ></td>
	<td class="line x" title="120:183	The goal is to keep the contextually appropriate tag and eliminate others which can be achieved by devising a disambiguation technique." ></td>
	<td class="line x" title="121:183	The disambiguation task can be naively addressed by choosing the most frequent tag for a word." ></td>
	<td class="line x" title="122:183	This approach is also known as baseline (BL) tagging." ></td>
	<td class="line x" title="123:183	The baseline accuracy turns out to be 82.63% which is still higher than that of the morphology-driven tagger5." ></td>
	<td class="line x" title="124:183	The drawback with baseline tagging is that its accuracy cannot be further improved." ></td>
	<td class="line x" title="125:183	On the other hand, there is enough room for improving upon the accuracy of morphology-driven (MD) tagger." ></td>
	<td class="line x" title="126:183	It is quite evident that though the MD tagger works well for VG and many close categories, around 30% of the words are either ambiguous or unknown." ></td>
	<td class="line x" title="127:183	Hence, a disambiguation stage is needed to shoot up the accuracy." ></td>
	<td class="line x" title="128:183	The common choice for disambiguation rule learning in POS tagging task is usually machine learning techniques mainly focussing on decision tree based algorithms (Orphanos and Christodoulalds, 1999), neural networks (Schmid, 1994), etc. Among the various decision tree based algorithms like ID3, AQR, ASSISTANT and CN2, CN2 is known to perform better than the rest (Clark and Niblett, 1989)." ></td>
	<td class="line x" title="129:183	Since no such machine learning technique has been used for Hindi language, we thought of choosing CN2 as it performs well on noisy data6." ></td>
	<td class="line x" title="130:183	5These numbers may change if we experiment on a different dataset 6The training annotated corpora becomes noisy by virtue of intuitions of different annotators (trained native Hindi speakers) 783 4.2.1 Training Corpora We set up a corpus, collecting sentences from BBC news site7 and let the morphology-driven tagger assign morphosyntactic tags to all the words." ></td>
	<td class="line x" title="131:183	For an ambiguous word, the contextually appropriate POS tag is manually chosen." ></td>
	<td class="line x" title="132:183	Unknown words are assigned a correct tag based on their context and usage." ></td>
	<td class="line x" title="133:183	4.2.2 Learning Out of the completely manually corrected corpora of 15,562 tokens, we created training instances for each Ambiguity Scheme and for Unknown words." ></td>
	<td class="line x" title="134:183	These training instances take into account the POS categories of the neighbouring words and not the feature values8." ></td>
	<td class="line x" title="135:183	The experiments were carried out for different context window sizes ranging from 2 to 20 to nd the best con guration." ></td>
	<td class="line x" title="136:183	4.2.3 Rule Generation The rules are generated from the training corpora by extracting the ambiguity scheme (AS) of each word." ></td>
	<td class="line x" title="137:183	If the word is not present in the lexicon then its AS is set as unknown." ></td>
	<td class="line x" title="138:183	Once the AS is identi ed, a training instance is formed." ></td>
	<td class="line x" title="139:183	This training instance contains the neighbouring correct POS categories as attributes." ></td>
	<td class="line x" title="140:183	The number of neighbours included in the training instance is the window size for CN2." ></td>
	<td class="line x" title="141:183	After all the ambiguous words are processed and training instances for all seen ASs are created, the CN2 algorithm is applied over the training instances to generate actual rule-sets for each AS." ></td>
	<td class="line x" title="142:183	The CN2 algorithm gives one set of If-Then rules (either ordered or unordered) for each AS including unknown9." ></td>
	<td class="line x" title="143:183	The AS of every ambiguous word is formed while tagging." ></td>
	<td class="line x" title="144:183	A corresponding rule-set for that AS is then identi ed and traversed to get the contextually appropriate rule." ></td>
	<td class="line x" title="145:183	The resultant 7http://www.bbc.co.uk/hindi/ 8Considering that a tag encodes 0 to 6 morphosyntactic features and each feature takes either one or a disjunction of 2 to 7 values, the total number of different tags can count up to several hundreds 9We used the CN2 algorithm implementation (1990) by Robin Boswell." ></td>
	<td class="line x" title="146:183	The software is available at ftp://ftp.cs.utexas.edu/pub/pclark/cn2.tar.Z category outputted by this rule is then assigned to the ambiguous word." ></td>
	<td class="line x" title="147:183	The traversal rule differs for ordered and unordered implementation." ></td>
	<td class="line x" title="148:183	The POS of an unknown word is guessed by traversing the rule-set for unknown words10 and assigning it the resultant tag." ></td>
	<td class="line x" title="149:183	5 Experimental Setup The experimentation involved, rst, identifying the best parameter values for the CN2 algorithm and second, evaluating the performance of the disambiguation rules generated by CN2 for the POS tagging task." ></td>
	<td class="line x" title="150:183	5.1 CN2 Parameters The various parameters in CN2 algorithm are: rule type (ordered or unordered), star size, signi cance threshold and size of the training instances (window size)." ></td>
	<td class="line x" title="151:183	The best results are empirically achieved with ordered rules, star size as 1, signi cance threshold as 10 and window size 4, i.e., two neighbours on either side are used to generate the training instances." ></td>
	<td class="line x" title="152:183	5.2 Evaluation The tests are performed on contiguous partitions of the corpora (15,562 words) that are 75% training set and 25% testing set." ></td>
	<td class="line x" title="153:183	Accuracy = no." ></td>
	<td class="line x" title="154:183	of single correct tagstotal no." ></td>
	<td class="line x" title="155:183	of tokens The results are obtained by performing a 4fold cross validation over the corpora." ></td>
	<td class="line x" title="156:183	Figure 2 gives the learning curve of the disambiguation module for varying corpora sizes starting from 1000 to the complete training corpora size." ></td>
	<td class="line x" title="157:183	The accuracy for known and unknown words is also measured separately." ></td>
	<td class="line x" title="158:183	6 Results and Discussion The average accuracy of the learning based (LB) tagger after 4-fold cross validation is 93.45%." ></td>
	<td class="line x" title="159:183	To 10Most of the unknown words are proper nouns, which cannot be stored in the lexicon extensively." ></td>
	<td class="line x" title="160:183	So, it also helps in named-entity detection." ></td>
	<td class="line x" title="161:183	784 90 90.5 91 91.5 92 92.5 93 93.5 94 94.5 0 2000 4000 6000 8000 10000 12000 Accuracy Number of Words in Training Corpus Overall Accuracy Known Words Accuracy Unknown Words Accuracy Figure 2: POS Learning Curve the best of our knowledge no comparable results have been reported so far for Hindi." ></td>
	<td class="line x" title="162:183	From Table 1, we can see that the disambiguation module brings up the accuracy of simple lexicon lookup based approach by around 25% (LLBD)." ></td>
	<td class="line x" title="163:183	The overall average accuracy is also brought up by around 20% by augmenting the morphology-driven (MD) tagger by a disambiguation module; hence justifying our belief that a disambiguation module over a morphology driven approach yields better results." ></td>
	<td class="line x" title="164:183	One interesting observation is the performance of the tagger on individual POS categories." ></td>
	<td class="line x" title="165:183	Figure 3 shows clearly that the per POS accuracies of the LB tagger highly exceeds those of the MD and BL tagger for most categories." ></td>
	<td class="line x" title="166:183	This means that the disambiguation module correctly disambiguates and correctly identi es the unknown words too." ></td>
	<td class="line x" title="167:183	The accuracy on unknown words, as earlier shown in Figure 2, is very high at 92.08%." ></td>
	<td class="line x" title="168:183	The percentage of unknown words in the test corpora is 0.013." ></td>
	<td class="line x" title="169:183	It seems independent of the size of training corpus because the corpora is unbalanced having most of the unknowns as proper nouns." ></td>
	<td class="line x" title="170:183	The rules are formed on this bias, and hence the application of these rules assigns PPN tag to an unknown which is mostly the case." ></td>
	<td class="line x" title="171:183	From Figure 3 again we see that the accuracy on some categories remains very low even after disambiguation." ></td>
	<td class="line x" title="172:183	This calls for some detailed failure analysis." ></td>
	<td class="line x" title="173:183	By looking at the categories having low accuracy, such as pronoun, intensi er, demonstratives and verb copula, we nd that all of them are highly ambiguous and, almost invariably, very rare in the corpus." ></td>
	<td class="line x" title="174:183	Also, most of them are hard to disambiguate without any semantic information." ></td>
	<td class="line x" title="175:183	7 Conclusions & Future Work We have described in this paper a POS tagger for Hindi which can overcome the handicap of annotated corpora scarcity by exploiting the rich morphology of the language and the relatively rigid word-order within a VG." ></td>
	<td class="line x" title="176:183	The whole work was driven by hunting down the factors that lower the accuracy of Verbs and weeding them out." ></td>
	<td class="line x" title="177:183	A detailed study of accuracy distribution across the POS tags pointed out the cases calling for elaborate disambiguation rules." ></td>
	<td class="line x" title="178:183	A major strength of the work is the learning of disambiguation rules, which otherwise would have been hand-coded, thus demanding exhaustive analysis of language phenomena." ></td>
	<td class="line x" title="179:183	Attaining an accuracy of close to 94%, from a corpora of just about 15,562 words lends credence to the belief that morphological richness can offset resource scarcity . The work could lead such efforts of POS tag building for all those languages which have rich morphology, but cannot afford to invest a lot in creating large annotated corpora." ></td>
	<td class="line x" title="180:183	Several interesting future directions suggest themselves." ></td>
	<td class="line x" title="181:183	It will be worthwhile to investigate a statistical approach like Conditional Random Fields in which the feature functions would be constructed from morphology." ></td>
	<td class="line x" title="182:183	The next logical step from the POS tagger is a chunker for Hindi." ></td>
	<td class="line x" title="183:183	In fact a start on this has already been made through VG identi cation." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-2056
Automatic Part-of-Speech Tagging for Bengali: An Approach for Morphologically Rich Languages in a Poor Resource Scenario
Dandapat, Sandipan;Sarkar, Sudeshna;Basu, Anupam;"></td>
	<td class="line x" title="1:107	Proceedings of the ACL 2007 Demo and Poster Sessions, pages 221??24, Prague, June 2007." ></td>
	<td class="line x" title="2:107	c2007 Association for Computational Linguistics Automatic Part-of-Speech Tagging for Bengali: An Approach for Morphologically Rich Languages in a Poor Resource Scenario Sandipan Dandapat, Sudeshna Sarkar, Anupam Basu Department of Computer Science and Engineering Indian Institute of Technology Kharagpur India 721302 {sandipan,sudeshna,anupam.basu}@cse.iitkgp.ernet.in Abstract This paper describes our work on building Part-of-Speech (POS) tagger for Bengali." ></td>
	<td class="line x" title="3:107	We have use Hidden Markov Model (HMM) and Maximum Entropy (ME) based stochastic taggers." ></td>
	<td class="line x" title="4:107	Bengali is a morphologically rich language and our taggers make use of morphological and contextual information of the words." ></td>
	<td class="line x" title="5:107	Since only a small labeled training set is available (45,000 words), simple stochastic approach does not yield very good results." ></td>
	<td class="line x" title="6:107	In this work, we have studied the effect of using a morphological analyzer to improve the performance of the tagger." ></td>
	<td class="line x" title="7:107	We find that the use of morphology helps improve the accuracy of the tagger especially when less amount of tagged corpora are available." ></td>
	<td class="line x" title="8:107	1 Introduction Part-of-Speech (POS) taggers for natural language texts have been developed using linguistic rules, stochastic models as well as a combination of both (hybrid taggers)." ></td>
	<td class="line oc" title="9:107	Stochastic models (Cutting et al. , 1992; Dermatas et al. , 1995; Brants, 2000) have been widely used in POS tagging for simplicity and language independence of the models." ></td>
	<td class="line x" title="10:107	Among stochastic models, bi-gram and tri-gram Hidden Markov Model (HMM) are quite popular." ></td>
	<td class="line x" title="11:107	Development of a high accuracy stochastic tagger requires a large amount of annotated text." ></td>
	<td class="line x" title="12:107	Stochastic taggers with more than 95% word-level accuracy have been developed for English, German and other European Languages, for which large labeled data is available." ></td>
	<td class="line x" title="13:107	Our aim here is to develop a stochastic POS tagger for Bengali but we are limited by lack of a large annotated corpus for Bengali." ></td>
	<td class="line x" title="14:107	Simple HMM models do not achieve high accuracy when the training set is small." ></td>
	<td class="line oc" title="15:107	In such cases, additional information may be coded into the HMM model to achieve higher accuracy (Cutting et al. , 1992)." ></td>
	<td class="line oc" title="16:107	The semi-supervised model described in Cutting et al.(1992), makes use of both labeled training text and some amount of unlabeled text." ></td>
	<td class="line n" title="18:107	Incorporating a diverse set of overlapping features in a HMM-based tagger is difficult and complicates the smoothing typically used for such taggers." ></td>
	<td class="line x" title="19:107	In contrast, methods based on Maximum Entropy (Ratnaparkhi, 1996), Conditional Random Field (Shrivastav, 2006) etc. can deal with diverse, overlapping features." ></td>
	<td class="line x" title="20:107	1.1 Previous Work on Indian Language POS Tagging Although some work has been done on POS tagging of different Indian languages, the systems are still in their infancy due to resource poverty." ></td>
	<td class="line x" title="21:107	Very little work has been done previously on POS tagging of Bengali." ></td>
	<td class="line x" title="22:107	Bengali is the main language spoken in Bangladesh, the second most commonly spoken language in India, and the fourth most commonly spoken language in the world." ></td>
	<td class="line x" title="23:107	Ray et al.(2003) describes a morphologybased disambiguation for Hindi POS tagging." ></td>
	<td class="line x" title="25:107	System using a decision tree based learning algorithm (CN2) has been developed for statistical Hindi POS tagging (Singh et al. , 2006)." ></td>
	<td class="line x" title="26:107	A reasonably good accuracy POS tagger for Hindi has been developed using Maximum Entropy Markov Model (Dalal et al. , 2007)." ></td>
	<td class="line x" title="27:107	The system uses linguistic suffix and POS categories of a word along with other contextual features." ></td>
	<td class="line x" title="28:107	2 Our Approach The problem of POS tagging can be formally stated as follows." ></td>
	<td class="line x" title="29:107	Given a sequence of words w 1 ??w n, we want to find the corresponding sequence of tags t 1 ??t n, drawn from a set of tags T. We use a tagset of 40 tags 1." ></td>
	<td class="line x" title="30:107	In this work, we explore supervised and semi-supervised bi-gram 1 http://www.mla.iitkgp.ernet.in/Tag.html 221 HMM and a ME based model." ></td>
	<td class="line x" title="31:107	The bi-gram assumption states that the POS-tag of a word depends on the current word and the POS tag of the previous word." ></td>
	<td class="line x" title="32:107	An ME model estimates the probabilities based on the imposed constraints." ></td>
	<td class="line x" title="33:107	Such constraints are derived from the training data, maintaining some relationship between features and outcomes." ></td>
	<td class="line x" title="34:107	The most probable tag sequence for a given word sequence satisfies equation (1) and (2) respectively for HMM and ME model: 1 1  1, (|)(| )arg max ii ii ttn in SPwtPt??" ></td>
	<td class="line x" title="35:107	(1) 11 1, (  |  ) ( | )nn ii in p ttww pth = = ??" ></td>
	<td class="line x" title="36:107	(2) Here, h i is the context for word w i . Since the basic bigram model of HMM as well as the equivalent ME models do not yield satisfactory accuracy, we wish to explore whether other available resources like a morphological analyzer can be used appropriately for better accuracy." ></td>
	<td class="line x" title="37:107	2.1 HMM and ME based Taggers Three taggers have been implemented based on bigram HMM and ME model." ></td>
	<td class="line x" title="38:107	The first tagger (we shall call it HMM-S) makes use of the supervised HMM model parameters, whereas the second tagger (we shall call it HMM-SS) uses the semi supervised model parameters." ></td>
	<td class="line x" title="39:107	The third tagger uses ME based model to find the most probable tag sequence for a given sequence of words." ></td>
	<td class="line x" title="40:107	In order to further improve the tagging accuracy, we use a Morphological Analyzer (MA) and integrate morphological information with the models." ></td>
	<td class="line x" title="41:107	We assume that the POS-tag of a word w can take values from the set T MA (w), where T MA (w) is computed by the Morphological Analyzer." ></td>
	<td class="line x" title="42:107	Note that the size of T MA (w) is much smaller than T. Thus, we have a restricted choice of tags as well as tag sequences for a given sentence." ></td>
	<td class="line x" title="43:107	Since the correct tag t for w is always in T MA (w) (assuming that the morphological analyzer is complete), it is always possible to find out the correct tag sequence for a sentence even after applying the morphological restriction." ></td>
	<td class="line x" title="44:107	Due to a much reduced set of possibilities, this model is expected to perform better for both the HMM (HMM-S and HMM-SS) and ME models even when only a small amount of labeled training text is available." ></td>
	<td class="line x" title="45:107	We shall call these new models HMM-S+MA, HMM-SS+ MA and ME+MA." ></td>
	<td class="line x" title="46:107	Our MA has high accuracy and coverage but it still has some missing words and a few errors." ></td>
	<td class="line x" title="47:107	For the purpose of these experiments we have made sure that all words of the test set are present in the root dictionary that an MA uses." ></td>
	<td class="line x" title="48:107	While MA helps us to restrict the possible choice of tags for a given word, one can also use suffix information (i.e. , the sequence of last few characters of a word) to further improve the models." ></td>
	<td class="line x" title="49:107	For HMM models, suffix information has been used during smoothing of emission probabilities, whereas for ME models, suffix information is used as another type of feature." ></td>
	<td class="line x" title="50:107	We shall denote the models with suffix information with a ??suf??" ></td>
	<td class="line x" title="51:107	marker." ></td>
	<td class="line x" title="52:107	Thus, we have ??HMM-S+suf, HMMS+suf+MA, HMM-SS+suf etc. 2.1.1 Unknown Word Hypothesis in HMM The transition probabilities are estimated by linear interpolation of unigrams and bigrams." ></td>
	<td class="line x" title="53:107	For the estimation of emission probabilities add-one smoothing or suffix information is used for the unknown words." ></td>
	<td class="line x" title="54:107	If the word is unknown to the morphological analyzer, we assume that the POS-tag of that word belongs to any of the open class grammatical categories (all classes of Noun, Verb, Adjective, Adverb and Interjection)." ></td>
	<td class="line x" title="55:107	2.1.2 Features of the ME Model Experiments were carried out to find out the most suitable binary valued features for the POS tagging in the ME model." ></td>
	<td class="line x" title="56:107	The main features for the POS tagging task have been identified based on the different possible combination of the available word and tag context." ></td>
	<td class="line x" title="57:107	The features also include prefix and suffix up to length four." ></td>
	<td class="line x" title="58:107	We considered different combinations from the following set for obtaining the best feature set for the POS tagging task with the data we have." ></td>
	<td class="line x" title="59:107	{ }112 212,,,,,,, 4, 4iii i i i iFwwwwwtt pre suf+? +?= ? Forty different experiments were conducted taking several combinations from set ?F??to identify the best suited feature set for the POS tagging task." ></td>
	<td class="line x" title="60:107	From our empirical analysis we found that the combination of contextual features (current word and previous tag), prefixes and suffixes of length ??4 gives the best performance for the ME model." ></td>
	<td class="line x" title="61:107	It is interesting to note that the inclusion of prefix and suffix for all words gives better result instead of using only for rare words as is described in Ratnaparkhi (1996)." ></td>
	<td class="line x" title="62:107	This can be explained by the fact that due to small amount of annotated data, a significant number of instances 222 are not found for most of the word of the language vocabulary." ></td>
	<td class="line x" title="63:107	3 Experiments We have a total of 12 models as described in subsection 2.1 under different stochastic tagging schemes." ></td>
	<td class="line x" title="64:107	The same training text has been used to estimate the parameters for all the models." ></td>
	<td class="line x" title="65:107	The model parameters for supervised HMM and ME models are estimated from the annotated text corpus." ></td>
	<td class="line x" title="66:107	For semi-supervised learning, the HMM learned through supervised training is considered as the initial model." ></td>
	<td class="line x" title="67:107	Further, a larger unlabelled training data has been used to re-estimate the model parameters of the semi-supervised HMM." ></td>
	<td class="line x" title="68:107	The experiments were conducted with three different sizes (10K, 20K and 40K words) of the training data to understand the relative performance of the models as we keep on increasing the size of the annotated data." ></td>
	<td class="line x" title="69:107	3.1 Training Data The training data includes manually annotated 3625 sentences (approximately 40,000 words) for both supervised HMM and ME model." ></td>
	<td class="line x" title="70:107	A fixed set of 11,000 unlabeled sentences (approximately 100,000 words) taken from CIIL corpus 2 are used to re-estimate the model parameter during semi-supervised learning." ></td>
	<td class="line x" title="71:107	It has been observed that the corpus ambiguity (mean number of possible tags for each word) in the training text is 1.77 which is much larger compared to the European languages (Dermatas et al. , 1995)." ></td>
	<td class="line x" title="72:107	3.2 Test Data All the models have been tested on a set of randomly drawn 400 sentences (5000 words) disjoint from the training corpus." ></td>
	<td class="line x" title="73:107	It has been noted that 14% words in the open testing text are unknown with respect to the training set, which is also a little higher compared to the European languages (Dermatas et al. , 1995) 3.3 Results We define the tagging accuracy as the ratio of the correctly tagged words to the total number of words." ></td>
	<td class="line x" title="74:107	Table 1 summarizes the final accuracies achieved by different learning methods with the varying size of the training data." ></td>
	<td class="line x" title="75:107	Note that the baseline model (i.e. , the tag probabilities depends 2 A part of the EMILE/CIIL corpus developed at Central Institute of Indian Languages (CIIL), Mysore." ></td>
	<td class="line x" title="76:107	only on the current word) has an accuracy of 76.8%." ></td>
	<td class="line x" title="77:107	Accuracy Method 10K 20K 40K HMM-S 57.53 70.61 77.29 HMM-S+suf 75.12 79.76 83.85 HMM-S+MA 82.39 84.06 86.64 HMM-S+suf+MA 84.73 87.35 88.75 HMM-SS 63.40 70.67 77.16 HMM-SS+suf 75.08 79.31 83.76 HMM-SS+MA 83.04 84.47 86.41 HMM-SS+suf+MA 84.41 87.16 87.95 ME 74.37 79.50 84.56 ME+suf 77.38 82.63 86.78 ME+MA 82.34 84.97 87.38 ME+suf+MA 84.13 87.07 88.41 Table 1: Tagging accuracies (in %) of different models with 10K, 20K and 40K training data." ></td>
	<td class="line x" title="78:107	3.4 Observations We find that in both the HMM based models (HMM-S and HMM-SS), the use of suffix information as well as the use of a morphological analyzer improves the accuracy of POS tagging with respect to the base models." ></td>
	<td class="line x" title="79:107	The use of MA gives better results than the use of suffix information." ></td>
	<td class="line x" title="80:107	When we use both suffix information as well as MA, the results is even better." ></td>
	<td class="line x" title="81:107	HMM-SS does better than HMM-S when very little tagged data is available, for example, when we use 10K training corpus." ></td>
	<td class="line x" title="82:107	However, the accuracy of the semi-supervised HMM models are slightly poorer than that of the supervised HMM models for moderate size training data and use of suffix information." ></td>
	<td class="line x" title="83:107	This discrepancy arises due to the over-fitting of the supervised models in the case of small training data; the problem is alleviated with the increase in the annotated data." ></td>
	<td class="line x" title="84:107	As we have noted already the use of MA and/or suffix information improves the accuracy of the POS tagger." ></td>
	<td class="line x" title="85:107	But what is significant to note is that the percentage of improvement is higher when the amount of training data is less." ></td>
	<td class="line x" title="86:107	The HMMS+suf model gives an improvement of around 18%, 9% and 6% over the HMM-S model for 10K, 20K and 40K training data respectively." ></td>
	<td class="line x" title="87:107	Similar trends are observed in the case of the semi-supervised HMM and the ME models." ></td>
	<td class="line x" title="88:107	The use of morphological restriction (HMM-S+MA) gives an improvement of 25%, 14% and 9% respectively over the HMM-S in case of 10K, 20K 223 and 40K training data." ></td>
	<td class="line x" title="89:107	As the improvement due to MA decreases with increasing data, it might be concluded that the use of morphological restriction may not improve the accuracy when a large amount of training data is available." ></td>
	<td class="line x" title="90:107	From our empirical observations we found that both suffix and morphological restriction (HMMS+suf+MA) gives an improvement of 27%, 17% and 12% over the HMM-S model respectively for the three different sizes of training data." ></td>
	<td class="line x" title="91:107	The Maximum Entropy model does better than the HMM models for smaller training data." ></td>
	<td class="line x" title="92:107	But with higher amount of training data the performance of the HMM and ME model are comparable." ></td>
	<td class="line x" title="93:107	Here also we observe that suffix information and MA have positive effect, and the effect is higher with poor resources." ></td>
	<td class="line x" title="94:107	Furthermore, in order to estimate the relative performance of the models, experiments were carried out with two existing taggers: TnT (Brants, 2000) and ACOPOST 3 . The accuracy achieved using TnT are 87.44% and 87.36% respectively with bigram and trigram model for 40K training data." ></td>
	<td class="line x" title="95:107	The accuracy with ACOPOST is 86.3%." ></td>
	<td class="line x" title="96:107	This reflects that the higher order Markov models do not work well under the current experimental setup." ></td>
	<td class="line x" title="97:107	3.5 Assessment of Error Types Table 2 shows the top five confusion classes for HMM-S+MA model." ></td>
	<td class="line x" title="98:107	The most common types of errors are the confusion between proper noun and common noun and the confusion between adjective and common noun." ></td>
	<td class="line x" title="99:107	This results from the fact that most of the proper nouns can be used as common nouns and most of the adjectives can be used as common nouns in Bengali." ></td>
	<td class="line x" title="100:107	Actual Class (frequency) Predicted Class % of total errors % of class errors NP(251) NN 21.03 43.82 JJ(311) NN 5.16 8.68 NN(1483) JJ 4.78 1.68 DTA(100) PP 2.87 15.0 NN(1483) VN 2.29 0.81 Table 2: Five most common types of errors Almost all the confusions are wrong assignment due to less number of instances in the training corpora, including errors due to long distance phenomena." ></td>
	<td class="line x" title="101:107	3 http://maxent.sourceforge.net 4 Conclusion In this paper we have described an approach for automatic stochastic tagging of natural language text for Bengali." ></td>
	<td class="line x" title="102:107	The models described here are very simple and efficient for automatic tagging even when the amount of available annotated text is small." ></td>
	<td class="line x" title="103:107	The models have a much higher accuracy than the nave baseline model." ></td>
	<td class="line x" title="104:107	However, the performance of the current system is not as good as that of the contemporary POStaggers available for English and other European languages." ></td>
	<td class="line x" title="105:107	The best performance is achieved for the supervised learning model along with suffix information and morphological restriction on the possible grammatical categories of a word." ></td>
	<td class="line x" title="106:107	In fact, the use of MA in any of the models discussed above enhances the performance of the POS tagger significantly." ></td>
	<td class="line x" title="107:107	We conclude that the use of morphological features is especially helpful to develop a reasonable POS tagger when tagged resources are limited." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1026
Representations for category disambiguation
Dickinson, Markus;"></td>
	<td class="line x" title="1:203	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 201208 Manchester, August 2008 Representations for category disambiguation Markus Dickinson Indiana University Bloomington, IN 47405 md7@indiana.edu Abstract AsitservesasabasisforPOStagging, category induction, and human category acquisition, we investigate the information needed to disambiguate a word in a local context, when using corpus categories." ></td>
	<td class="line x" title="2:203	Specifically, we increase the recall of an error detection method by abstracting the word to be disambiguated to a representation containing information about some of its inherent properties, namely the set of categories it can potentially have." ></td>
	<td class="line x" title="3:203	This work thus provides insights into the relation of corpus categories to categories derived from local contexts." ></td>
	<td class="line x" title="4:203	1 Introduction and Motivation Category induction techniques generally rely on local contexts, i.e., surrounding words, to cluster word types together (e.g., Clark, 2003; Schutze, 1995), using information of a kind also found in human category acquisition tasks (e.g., Mintz, 2002, 2003)." ></td>
	<td class="line x" title="5:203	Such information is also at the core of standard part-of-speech (POS) tagging, or disambiguation, methods (see, e.g., Manning and Schutze, 1999, ch." ></td>
	<td class="line x" title="6:203	10), with the contexts generally abstracted to POS tags." ></td>
	<td class="line x" title="7:203	The contextual information is similar in both tasks because induction is founded in part upon the notion that local contexts are useful for disambiguation: one morphosyntactically clusters words which should have the same category in the same contexts." ></td>
	<td class="line x" title="8:203	But which contexts count as being the same?" ></td>
	<td class="line x" title="9:203	And to what extent do categories based on context distributions resemble c2008." ></td>
	<td class="line x" title="10:203	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="11:203	Some rights reserved." ></td>
	<td class="line x" title="12:203	corpus annotation categories?" ></td>
	<td class="line x" title="13:203	Since disambiguation is in some sense more primary, to begin to answer these questions we investigate which representations are effective for category disambiguation." ></td>
	<td class="line x" title="14:203	Disambiguating a words category in context has of course been explored in other situations, especially POS tagging." ></td>
	<td class="line x" title="15:203	Rarely, however, has it been shown as to which information is the most accurate at disambiguation and which information is absolutely necessary, without mixing these issues with other tagging issues, such as smoothing and unknown word tagging." ></td>
	<td class="line x" title="16:203	We need techniques which isolate disambiguation, placing less emphasis on generalizing contexts to new data." ></td>
	<td class="line x" title="17:203	To determine the essential information needed for accurate disambiguation, we start with a precise model and generalize it." ></td>
	<td class="line x" title="18:203	Changing the model in small ways and evaluating the resulting precision will indicate how particular aspects of the representation are contributing to successful disambiguation." ></td>
	<td class="line x" title="19:203	The central question of this paper is: which representation (of a word and its context) indicates that two situations should be categorized the same?" ></td>
	<td class="line x" title="20:203	In this context, POS annotation error detection provides an ideal setting to explore representations for disambiguation." ></td>
	<td class="line x" title="21:203	Error detection relies on the assumption that words should be annotated consistentlyinotherwords, contextsaregrouped which accuratelyidentify thecategory of aword as being consistentand it does this with an emphasis on high precision." ></td>
	<td class="line x" title="22:203	In essence, error detection already investigates where disambiguation can be done, often using local contexts (e.g., Dickinson, 2005)." ></td>
	<td class="line x" title="23:203	With an emphasis on high precision, however, many corpus instances are essentially uncategorized and are thus in need of generalization." ></td>
	<td class="line x" title="24:203	To get at the central question of an appropriate 201 representation for disambiguation, then, our task is to generalize error detection and increase the recall of errors foundin a corpus byexploiting more general properties of a corpus." ></td>
	<td class="line x" title="25:203	Given that annotation errors can have a profound impact on the quality of training and testing on such data (see Dickinson, 2005, ch." ></td>
	<td class="line x" title="26:203	1), this task also serves an immense practical need in its own right." ></td>
	<td class="line x" title="27:203	In exploring error detection recall, we can connect the task to another with much of the same emphasis." ></td>
	<td class="line x" title="28:203	Human category acquisition experiments have also focused on precision: instead of asking how every word is categorized, they examine how some words are categorized, from which others can be bootstrapped." ></td>
	<td class="line x" title="29:203	As outlined in sections 2 and 3, we can use such studies as a starting point for generalizing error detection." ></td>
	<td class="line x" title="30:203	2 Background 2.1 The variation n-gram method The error detection method we build from is the variation n-gram method (Dickinson and Meurers, 2003; Dickinson, 2005)." ></td>
	<td class="line x" title="31:203	The approach detects items which occur multiple times in the corpus with varying annotation, the so-called variation nuclei." ></td>
	<td class="line x" title="32:203	A nucleus with its repeated surrounding context is referred to as a variation n-gram." ></td>
	<td class="line x" title="33:203	Every detected variation in the annotation of a nucleus is classified as an error or a genuine ambiguity using a basic heuristic requiring at least one word of context on each side of the nucleus." ></td>
	<td class="line x" title="34:203	For example, in the WSJ corpus, part of the Penn Treebank 3 release (Marcus et al., 1993), the string in (1) is a variation 12-gram since off is a variation nucleus that is tagged preposition (IN) in one corpus occurrence and particle (RP) in another.1 Dickinson (2005) shows that examining those cases with identical local contextin this case, lookingat ward off aresultsinanestimated error detection precision of 92.5%." ></td>
	<td class="line x" title="35:203	(1) toward off ahostiletakeoverattemptbytwo European shipping concerns This method can be applied to syntactic annotation, and for this annotation, one can increase the recall of errors found by abstracting the nuclei to POS tags (Boyd et al., 2007)." ></td>
	<td class="line x" title="36:203	Clearly, this is not a feasible abstraction here, given that we are attempting to detect errors in POS annotation." ></td>
	<td class="line x" title="37:203	1To distinguish variation nuclei, we shade them in gray and underline the immediately surrounding context." ></td>
	<td class="line x" title="38:203	2.2 Frames for language acquisition Research on language acquisition has addressed thequestionofhowhumansdiscoverandlearncategories of words, using virtually the same contexts as in the variation n-gram method." ></td>
	<td class="line x" title="39:203	Mintz (2002) shows that local context, in the form of a frame of two words surrounding a target word, leads to categorization in adults of the target, and Mintz (2003) shows that frequent frames supply category information, consistent across child language corpora." ></td>
	<td class="line x" title="40:203	A frame is defined as two jointly occurring words with one word intervening (Mintz, 2003), e.g., you it." ></td>
	<td class="line x" title="41:203	Theframeisnotdecomposedintoits left side and right side (cf., e.g., Redington et al., 1998; Clark, 2003) , but instead is taken as the occurrence of both sides." ></td>
	<td class="line x" title="42:203	The target word is the intervening word, but it is not included in the frame (unlike variation nuclei)." ></td>
	<td class="line x" title="43:203	For category acquisition, only frequent frames are used, those with a frequency above a certain threshold." ></td>
	<td class="line x" title="44:203	Frequent frames predict category membership: the set of words appearing in a given frame should represent a single category." ></td>
	<td class="line x" title="45:203	The frequent frame you it, for example, largelyidentifiesverbs,asshownin(2),takenfrom the CHILDES database of child-directed speech (MacWhinney, 2000)." ></td>
	<td class="line x" title="46:203	Analyzing the frequent frames in six subcorpora of CHILDES, Mintz (2003) obtains both high type and high token accuracy in grouping words into the same categories." ></td>
	<td class="line x" title="47:203	(2) a. you put it b. you see it To take this work as a basis for investigating disambiguation, somepointsareinorderabouttheresults." ></td>
	<td class="line x" title="48:203	First, accuracies slightly degrade when moving from the Standard Labeling category set2 to the more fine-grained Expanded Labeling category set,3 i.e., a .98 to .91 drop in token accuracy and .93 to .91 drop in type accuracy." ></td>
	<td class="line x" title="49:203	It is not clear what happens with even more fine-grained corpus tagsets." ></td>
	<td class="line x" title="50:203	Secondly, Mintz (2003) assumes that, at least for his experiments, each word has only one class (see also Redington et al., 1998, p. 439-440)." ></td>
	<td class="line x" title="51:203	The tasks of category induction and category disambiguation are thus conflated into a single step." ></td>
	<td class="line x" title="52:203	We do not know for sure whether frames induce 2Categories = noun, verb, adjective, preposition, adverb, determiner, wh-word, not, conjunction, and interjection." ></td>
	<td class="line x" title="53:203	3Nouns split into nouns and pronouns; verbs split into verbs, auxiliaries, and copula 202 coherent sets of words or whether they accurately disambiguate a word, or both." ></td>
	<td class="line x" title="54:203	In other words, can frames be used to group the target words (induction) or to group the contexts (disambiguation)?" ></td>
	<td class="line x" title="55:203	While we investigate using frames for disambiguation in English (and somewhat in German), the concept of a frame has been shown to be crosslinguistically viable (Chemla et al., in press), and inprinciplecouldextendtolanguagesencodingrelations through morphology instead of linear order (see the discussion in Mintz, 2003)." ></td>
	<td class="line x" title="56:203	3 Generalizing error detection via frames Both strands of research employ local contexts for identifying categories, but the variation n-gram method relies on identical words to serve as variation nuclei, or target words to be disambiguated." ></td>
	<td class="line x" title="57:203	To increase the recall of the method in a way relating to acquisition, the nucleus should be abstracted to something more general than a word." ></td>
	<td class="line x" title="58:203	As a (frequent)framedoesnotincludethetarget, predicting that the category within that context is always the same, a first step in abstracting the nucleus is to require no similarity between nuclei." ></td>
	<td class="line x" title="59:203	We thus search for all identical nuclei with frame contextor what we will call framed variation nucleisuch that there is variation in labeling for the nucleus, but we require no identity of the nucleus." ></td>
	<td class="line x" title="60:203	We investigate the WSJ portion of the Penn Treebank, and, to provide more robust evaluation, also compare the TIGER corpus of German, version 2 (Brants et al., 2002) where appropriate." ></td>
	<td class="line x" title="61:203	Given that punctuation is less informative for determining a category, we remove from consideration frames containing punctuation as one of the context words, and obtain 48,717 variations in the WSJ and 22,613 in TIGER." ></td>
	<td class="line x" title="62:203	Although basic hand-examination reveals some errors, a majority of cases contain acceptable variations." ></td>
	<td class="line x" title="63:203	As one example, in the WSJ the frame the of occurs as the most frequent frame with variation in labeling for the target (5737 instances)." ></td>
	<td class="line x" title="64:203	This is a nominal position, and thus we find variationbetweenavarietyofcorrectnominaltags: cardinal number (CD), adjective (JJ, JJR, JJS), common noun (NN, NNS), and proper noun (NNP, NNPS), in addition to the erroneous verbal tags VBD (past tense verb) and VBG (verb, -ing form)." ></td>
	<td class="line x" title="65:203	Restricting our attention to the frequent frames, as in Mintz (2003), is not helpful: the problem occurs irrespective of frequency." ></td>
	<td class="line x" title="66:203	Indeed, there is an average of 2.56 categories per variation, with one variation (and in) having 21 categories." ></td>
	<td class="line x" title="67:203	This is consistent in TIGER, which has 2.57 categories per variation and 22 categories for und in." ></td>
	<td class="line x" title="68:203	While more context could help, the real issue is the definition of a nucleus." ></td>
	<td class="line x" title="69:203	In the example above, which nominal tag is used depends upon inherent properties of the word involved." ></td>
	<td class="line x" title="70:203	Consider the frame that the." ></td>
	<td class="line x" title="71:203	Among the 18 possible tags, there is variation between NN (common noun) for words like afternoon and VBZ (present tense verb, 3rd person singular) for words like says." ></td>
	<td class="line x" title="72:203	Both are legitimate, and the primary way to tell is by examining information about the target word." ></td>
	<td class="line x" title="73:203	In generalizing the nucleus, instead of abstracting it to nothing, we need to abstract it to something indicating broad characteristics of the word." ></td>
	<td class="line x" title="74:203	4 An appropriate level of abstraction On the one hand, the variation n-gram method has highprecision; ontheother, usingframesresultsin high recall, but too low a precision to sort through." ></td>
	<td class="line x" title="75:203	Both methods rely on the same identical contexts; theissueisinfindingwhichwordsarecomparable." ></td>
	<td class="line x" title="76:203	Consider the frame nt that." ></td>
	<td class="line x" title="77:203	Some words are inherentlysimilarandshouldhavethesametags: the correct nt help/VB that and the erroneous nt matter/NN that, for instance, are comparable." ></td>
	<td class="line x" title="78:203	Other cases are not: one/CD and shown/VBN can never have the same category." ></td>
	<td class="line x" title="79:203	We need to find classes of words that, within the same context, should not varyintheirannotation,anditmakessensetocompare words in context if they have the same category possibilities." ></td>
	<td class="line pc" title="80:203	4.1 Complete ambiguity classes Ambiguity classes capture the relevant property we are interested in: words with the same category possibilities are grouped together.4 And ambiguity classes have been shown to be successfully employed, in a variety of ways, to improve POS tagging (e.g., Cutting et al., 1992; Daelemans et al., 1996; Dickinson, 2007; Goldberg et al., 2008; Tseng et al., 2005)." ></td>
	<td class="line x" title="81:203	Only certain words can take one of two (or more) tags, and these should be disambiguated in the same way in context." ></td>
	<td class="line x" title="82:203	As an example of how using ambiguity classes as variation nuclei can increase recall, consider the frame being by in example (3)." ></td>
	<td class="line x" title="83:203	There are at least 27 4One could group affixes by ambiguity class for languages like Chinese (cf.CTBMorph features in Tseng et al., 2005)." ></td>
	<td class="line x" title="85:203	203 differentVBN(pastparticiple)verbsappearingbetween being and by (3a), but none of these verbs ever appear as VBD here, even though all of them could be VBD." ></td>
	<td class="line x" title="86:203	Two other VBD/VBN verbs, rejected (3b) and played (3c), erroneously appear as VBD here, but never as VBN." ></td>
	<td class="line x" title="87:203	With the nucleus VBD/VBN, we can find this erroneous variation." ></td>
	<td class="line x" title="88:203	(3) a. being { raised/VBN , infringed/VBN , supported/VBN ,  } by b. as probable as being rejected/VBD by the Book-of-the-Month Club c. the  role in takeover financing being played/VBD by Japanese banks Thus, to define complete ambiguity class variation nuclei, we make a first pass through the corpus to calculate every words ambiguity class." ></td>
	<td class="line x" title="89:203	On a second pass, the ambiguity class serves as the (framed) variation nucleus, e.g., being VBD/VBN by." ></td>
	<td class="line x" title="90:203	Ambiguity class nuclei with more than one tag in a frame context are flagged as a potential error." ></td>
	<td class="line x" title="91:203	4.2 Pairwise ambiguity classes While abstracting to a words possible classes can increase the number of errors found, potentially erroneous classes prevent further increased recall." ></td>
	<td class="line x" title="92:203	For example, the class for plans is erroneously classified as NNS/VBP/VBZ, even though its one instance of VBP (present tense verb, non-3rd person singular) in the corpus is erroneous." ></td>
	<td class="line x" title="93:203	Without that case, we would have NNS/VBZ and more directly comparable words." ></td>
	<td class="line x" title="94:203	As a second experiment, then, we define pairwise ambiguity class variation nuclei, using subsets of ambiguity classes to define a nucleus." ></td>
	<td class="line x" title="95:203	If the variation is only between NNS and VBZ, we need to allow all words with NNS/VBZ variation to count as comparable nuclei." ></td>
	<td class="line x" title="96:203	As above, we calculated a words ambiguity class during a first pass." ></td>
	<td class="line x" title="97:203	In the second pass through the corpus, we break the ambiguity class down into its pairs, and each relevant pair is stored as a variation nucleus." ></td>
	<td class="line x" title="98:203	The relevant pairs of tags are those which contain the tag at that position since classes without that tag can never have meaningful variation." ></td>
	<td class="line x" title="99:203	Taking the example of company plans to, with the ambiguity class NNS/VBP/VBZ for plans, if the current corpus position marks plans as NNS, then we store the two trigrams in (4)." ></td>
	<td class="line x" title="100:203	(4) a. company NNS/VBZ to b. company NNS/VBP to Looking over the whole corpus, we find variation between NNS and VBZ, but none between NNS and VBP." ></td>
	<td class="line x" title="101:203	In principle, this instance of plans/NNS could be in both an NNS/VBZ and an NNS/VBP variation; this is necessary since we do not a priori know which variations will be problematic." ></td>
	<td class="line x" title="102:203	5 Results and Insights 5.1 Complete ambiguity classes Using complete ambiguity class variation nuclei, we find 4131 framed variation nuclei in the WSJ." ></td>
	<td class="line x" title="103:203	Almost all variations involve only two or three tags, with 2.03 tags per variation." ></td>
	<td class="line x" title="104:203	TIGER has 626 framed variation nuclei, with 2.01 tags per variation." ></td>
	<td class="line x" title="105:203	Fromthe4131variations, werandomlysampled 100casesandhand-evaluatedwhethertheycontain an error, and whether its detection is attributable to the generalization to complete ambiguity classes." ></td>
	<td class="line x" title="106:203	Of the 100, 79 of the cases contain at least one error, and 15 of these cases are new examples, i.e., cases without identical words." ></td>
	<td class="line x" title="107:203	With a point estimate of .79, we estimate 3263 errors and obtain a 95% confidence interval of (0.7102, 0.8698), meaning that we predict between 2933 and 3593 of the 4131 cases contain errors." ></td>
	<td class="line x" title="108:203	The 79 erroneous cases point to 134 token errors, of which 23 are new." ></td>
	<td class="line x" title="109:203	In addition to increasing the recall of the method, the cases are arguably more thoroughly grouped than before." ></td>
	<td class="line x" title="110:203	For instance, we see in (5) that both pretax and third-quarter vary between JJ and NN in the variation said JJ/NN profit, with first-half additionally appearing only as JJ." ></td>
	<td class="line x" title="111:203	Since JJ is the correct tag for all instances, the two NN errors are detected with word nuclei, but here all the relevant examples are together." ></td>
	<td class="line x" title="112:203	This provides evidence for the claim that an ambiguity class is a level of abstraction supporting identical disambiguation in the same context." ></td>
	<td class="line x" title="113:203	(5) said { first-half/JJ , third-quarter/JJ , pretax/JJ , third-quarter/NN , pretax/NN } profit The recall has increased, but 79% is below the 92.5% precision previously obtained for the variation n-gram method with word nuclei (Dickinson, 2005)." ></td>
	<td class="line x" title="114:203	However, that result used distinct variation 204 nuclei, meaning that the longest contexts were examined before working down to shorter contexts." ></td>
	<td class="line x" title="115:203	Furthermore, it is not clear how well the original word nuclei method scales up to larger corpora." ></td>
	<td class="line x" title="116:203	Some of the new false positives we observe would likely be false positives for word nuclei, given more data." ></td>
	<td class="line x" title="117:203	For example, the new method turns up generally VBD/VBN the as a false positive, as in (6), because of the non-local tagset distinction and short context." ></td>
	<td class="line x" title="118:203	With more data, we are more likely to see an acceptable use of, e.g., generally favored/VBD the, a false positive for word nuclei." ></td>
	<td class="line x" title="119:203	In some sense, then, this 79% precision might be a more general indication of the methods precision for this tagset and genre." ></td>
	<td class="line x" title="120:203	(6) a. TV news coverage has generally favored/VBN the government b. Members generally received/VBD the regional officials Finally, ofthe21falsepositives(20ofwhichare new), five of them stem from an error in the ambiguity class, corresponding to five token errors." ></td>
	<td class="line x" title="121:203	For example, there is variation for JJ/NN words in the frame of pills, as in (7)." ></td>
	<td class="line x" title="122:203	However, poison should never be JJ: its ambiguity class should be NN, not theincorrectJJ/NN.Forerrordetection,thismeans 84 of the 100 samples lead to some kind of POS error; for investigating disambiguation contexts, this means that 83% (79/95) of the cases support complete disambiguation." ></td>
	<td class="line x" title="123:203	Thus, when abstracting to ambiguity class nuclei, local context generally provides sufficient information for disambiguation (see also section 6)." ></td>
	<td class="line x" title="124:203	(7) of { birth-control/JJ , poison/NN } pills Onelimitationofthevariationn-grammethodis the fact that some distinctions often need non-local information (cf.(6))." ></td>
	<td class="line x" title="126:203	A bigger problem for grouping words by ambiguity classes is the fact that annotation can be semantically-based." ></td>
	<td class="line x" title="127:203	For example, the variation of JJ/NN bank is a legitimate ambiguity because the distinction between JJ and NN is semantic." ></td>
	<td class="line x" title="128:203	Compare a sort of merchant/NN bank with an extension of senior/JJ bank debt: both nuclei are clearly in a noun modifier position, but the tags are different based on what they denote." ></td>
	<td class="line x" title="129:203	This shows the limitations of local distributional information without lexical information, for making these tagset distinctions." ></td>
	<td class="line x" title="130:203	5.2 Pairwise ambiguity classes With pairwise ambiguity classes serving as variation nuclei, we find 6235 variation frames in the WSJ and 874 in TIGER, significant increases over using complete ambiguity class nuclei." ></td>
	<td class="line x" title="131:203	To evaluate the method, we want to know: a) how many total errors we detect, b) how many of these were detected by using either complete or pairwise ambiguity classes, and c) how many were detected specifically with pairwise ambiguity classes." ></td>
	<td class="line x" title="132:203	A sample of 100 of the WSJ cases reveals (a) 59 total errors, (b) 18 of which involve ambiguity class nuclei that would not have been found with word nuclei." ></td>
	<td class="line x" title="133:203	Of these 18, (c) 8 cases can only be found by extending the method to pairwise classes." ></td>
	<td class="line x" title="134:203	For the point estimate of .59, we estimate approximately 3679 variations to be errors (95% CI: 3078 to 4280 errors)." ></td>
	<td class="line x" title="135:203	The 59 erroneous variations point to a total of (a) 134 token errors, (b) 30 of which were detected by ambiguity classes; (c) 17 of these were detected by pairwise ambiguity classes." ></td>
	<td class="line x" title="136:203	Clearly, using pairwise ambiguity classes increases the number of errors found." ></td>
	<td class="line x" title="137:203	As an example, consider (8), centering on the frame came for." ></td>
	<td class="line x" title="138:203	The original variation n-gram method turns up no variation here, but neither does the complete ambiguity class extension: in has the ambiguity class FW/IN/NN/RB/RBR/RP, and out the class IN/JJ/NN/RB/RP." ></td>
	<td class="line x" title="139:203	Since the only relevant variation is between IN and RP, the pairwise nuclei methodturnsupsuchcaseswiththevariationcame IN/RP for, pointing to an error in the two cases of out." ></td>
	<td class="line x" title="140:203	(8) a. accounts came in/RP for some blocks b. numbers came out/IN for September c. he again came out/IN for an amendment But what of the 41 false positives, 22 of which are due to the pairwise classes?" ></td>
	<td class="line x" title="141:203	We have increased recall, but there is also a 20% absolute drop in precision." ></td>
	<td class="line x" title="142:203	Is this tradeoff worth it?" ></td>
	<td class="line x" title="143:203	To answer this, it is important to note that 15 of the false positives are due to faulty ambiguity classes, as discussed above, and 10 of those 15 are from pairwise classes." ></td>
	<td class="line x" title="144:203	For error detection, this means 74 of the 100 samples lead to some POS error; for investigating disambiguation contexts, this means 69% (59/85) of the cases support disambiguation." ></td>
	<td class="line x" title="145:203	Additionally, the 15 cases point to 53 token errors, much more than in the previous experiment, due to 44 token errors from the new pair205 wise ambiguity classes." ></td>
	<td class="line x" title="146:203	For example, in the variation frame as DT/JJ sales, the words which vary are a (tagged DT (determiner), with a complete ambiguity class of DT/FW/IN/JJ/LS/NNP/SYM) and many (tagged JJ, with an ambiguity class of DT/JJ/NNS/PDT/RB/VB)." ></td>
	<td class="line x" title="147:203	Unsurprisingly, a should never have been tagged JJ in the corpus, i.e., its ambiguity class is wrong." ></td>
	<td class="line x" title="148:203	In addition to the issue of erroneous tags in an ambiguity class, atypical tags also pose a problem." ></td>
	<td class="line x" title="149:203	Consider the frame that JJ/RB in, as illustrated in (9), with acceptable variation." ></td>
	<td class="line x" title="150:203	It might appear that sometime has a problem with its ambiguity class, but the use of JJ is actually correct, as shown in (10), where sometime is atypically modifying a noun." ></td>
	<td class="line x" title="151:203	To counter atypical uses, one could use only typical ambiguity classes (cf.Dickinson, 2007) or define ambiguity classes according to order of frequency (cf.Daelemans et al., 1996), e.g., JJ/RB vs. RB/JJ." ></td>
	<td class="line x" title="154:203	(9) a. a departure from the past that many/JJ in the industry  b. hope that sometime/RB in the near future (10) real estate magnate and sometime/JJ raider Donald Trump This illustrates that the selection of an abstracted classforanucleusdefinitionisnon-trivial,andambiguity classes are simply an approximation." ></td>
	<td class="line x" title="155:203	POS contexts One problem for our method is that word contexts are not always truly comparable; identical context words can be used differently." ></td>
	<td class="line x" title="156:203	Forinstance, withthevariationthat NN/VBP along in (11), the uses of that are clearly distinct and are marked as such by their tags." ></td>
	<td class="line x" title="157:203	(11) a. gifts that/WDT go/VBP along with purchases b. We are considering that/DT offer/NN along with all other alternatives But do tagset categories actually aid in local disambiguation?" ></td>
	<td class="line x" title="158:203	To quickly gauge this, we take the previous sample of 100 variations and recover the POS information for the context." ></td>
	<td class="line x" title="159:203	Isolating those cases with non-identical POS tags for the same word contexts, we find 10 examples and hypothesize that these will more likely be acceptable variations." ></td>
	<td class="line x" title="160:203	Interestingly, however, of those ten, six successfully identified errors; it turns out that the POS of the word is often irrelevant for disambiguation." ></td>
	<td class="line x" title="161:203	For the variation paid JJR/RBR than in (12), for example, the tag of the context word paid is different in these cases, but that does not matter for the tag of more, which should be consistent." ></td>
	<td class="line x" title="162:203	(12) a. they paid/VBD more/JJR than $ 1 million b. he has paid/VBN more/RBR than $ 70,000 More problematically, four of the erroneous variation nuclei also contained POS errors in the context, as in example (13)." ></td>
	<td class="line x" title="163:203	The variation all CC/RB disappeared points to an error in the word but, yet there is also a noticeable inconsistency in the word all." ></td>
	<td class="line x" title="164:203	(13) a. have all/DT but/CC disappeared b. have all/RB but/RB disappeared In other words, it is often the case that we should ignore the POS of the context words, due to the fact that erroneous contexts exist and, more importantly, that not all categories aid in disambiguation." ></td>
	<td class="line x" title="165:203	Exploring which contextual categories aid in target category disambiguation (cf., e.g., Brants, 1997) could aid in developing better disambiguation models, and perhaps also a better sense of what categories are useful to induce (e.g., a broader category Verb in (12) for paid)." ></td>
	<td class="line x" title="166:203	6 Representations for disambiguation We have shown that local lexical context provides a generally unambiguous context for corpus tags, given sufficient information about the word to be disambiguated." ></td>
	<td class="line x" title="167:203	The information need not be very abstract, either: frames using ambiguity class nuclei only require a words category possibilities." ></td>
	<td class="line x" title="168:203	Even for many unsupervised situations, this is available from a lexicon (e.g., Banko and Moore, 2004; Goldberg et al., 2008)." ></td>
	<td class="line x" title="169:203	We have only looked at cases with variation in tagging; fully gauging the accuracy of such a data representation for disambiguation requires more of the framed nuclei from the corpus, including those without variation." ></td>
	<td class="line x" title="170:203	For this, we could take all framed nuclei from a corpus and compare the level of ambiguity for differing abstractions." ></td>
	<td class="line x" title="171:203	However, most framed nuclei occur only once, and it is not clear how meaningful it is to say that these are unambiguous." ></td>
	<td class="line x" title="172:203	Thus, we examine framed nuclei which occur at least twice and report in table 1 206 for the WSJ how unambiguous a particular level of nucleus abstraction is.5 Abstraction Unamb." ></td>
	<td class="line x" title="173:203	Total Accuracy Word 84,784 87,390 97.02% Complete AC 90,341 94,472 95.63% No info." ></td>
	<td class="line x" title="174:203	51,945 100,662 51.60% Table 1: Disambiguation accuracy for the WSJ While abstracting to the case where the nucleus contains no information (No info.)" ></td>
	<td class="line x" title="175:203	creates more cases which are classifiableover 100,000the accuracy of disambiguation drops from the upper 90% range to 52%." ></td>
	<td class="line x" title="176:203	Note, however, that the abstraction to complete ambiguity class (AC) nuclei has minimal degradation in accuracy, yet increases the number of accurately classified cases." ></td>
	<td class="line x" title="177:203	When we recall that approximately 79% of of the 4131 variation frames should have a single tag, i.e., 3263 cases, this means that the overall disambiguation accuracy is estimated to be 99.08% (93,604/94,472)." ></td>
	<td class="line x" title="178:203	In addition to the disambiguation accuracy of frames, wecanlookattheaccuracyofwordtokens identified by frames." ></td>
	<td class="line x" title="179:203	To gauge this, we identify the most likely tag of each framed variation nucleus and assign it to all instances of the nucleus." ></td>
	<td class="line x" title="180:203	In the case of ties, one tag is randomly selected; since we are only calculating overall word token accuracy, the exact tag selected is unimportant." ></td>
	<td class="line x" title="181:203	The results of comparing to the benchmark tags are given in table 2." ></td>
	<td class="line x" title="182:203	Even though the abstraction to no information identifies more word tokens, the ambiguity class abstraction correctly categorizes nearly as many words." ></td>
	<td class="line x" title="183:203	Abstraction Correct Total Accuracy Word 340,860 345,139 98.76% Complete AC 441,603 448,402 98.74% No info." ></td>
	<td class="line x" title="184:203	444,635 582,601 76.32% Table 2: Word token accuracy for the WSJ With the smaller and likely more accurately tagged TIGER corpus, we find exactly the same trends, as shown in table 3." ></td>
	<td class="line x" title="185:203	This supports the claim across corpora that local context is often sufficienttodisambiguateaword,ifsomeinformation from the wordhere, the category possibilities is present in the nucleus." ></td>
	<td class="line x" title="186:203	5As pairwise ambiguity classes involve more than one nucleus per corpus position, we use complete ambiguity classes." ></td>
	<td class="line x" title="187:203	Abstraction Unamb." ></td>
	<td class="line x" title="188:203	Total Accuracy Word 37,038 37,324 99.23% Complete AC 47,832 48,458 98.71% No info." ></td>
	<td class="line x" title="189:203	33,881 56,494 59.97% Table 3: Disambiguation accuracy for TIGER The poor accuracy for framed nuclei with no information indicates that methods which intend to match corpus annotation categories could face difficulties in obtaining a single category without using more information." ></td>
	<td class="line x" title="190:203	There is still much space to explore, however, between using ambiguity class nuclei and no information, in order to further increase the number of comparable cases without losing accuracy and in order to be more knowledge-free." ></td>
	<td class="line x" title="191:203	7 Summary and Outlook Motivated by work on category acquisition, we have shown that local contextsi.e., immediately surrounding words, or framescan delineate corpus categories when the level of abstraction for the word to be disambiguated indicates some inherent propertiesoftheword, namelythecategoriesitcan have." ></td>
	<td class="line x" title="192:203	By abstracting away from lexical items to broader classes of words, we have been able to increase the recall of an error detection method without much drop in its precision." ></td>
	<td class="line x" title="193:203	Having successfully defined a representation for disambiguation, the next step is to make the representation more general, in order to include more comparableinstances." ></td>
	<td class="line x" title="194:203	Aswhatwehavedoneisessentially a form of nearest neighbor classification, one could in the future explore more sophisticated techniques to cluster contexts." ></td>
	<td class="line x" title="195:203	At the same time, we wish to use as little annotated knowledge as possible." ></td>
	<td class="line x" title="196:203	Thus, an orthogonal line of research can involve inducing classes for words which are more general than single categories, i.e., something akin to ambiguity classes (see, e.g., the discussion of ambiguity class guessers in Goldberg et al., 2008)." ></td>
	<td class="line x" title="197:203	This could make error detection completely independent of the annotation and, more importantly, lead to an improved understanding of the best knowledgefree representation for disambiguation." ></td>
	<td class="line x" title="198:203	Since induction is founded to some extent upon disambiguating contexts, this work has some bearing on the evaluation of induced categories with corpus annotation; not only is there more than 207 one tagset in existence (see discussion in Clark, 2003), but annotation schemes make distinctions that morphosyntactic contexts cannot readily capture." ></td>
	<td class="line x" title="199:203	For example, there is an implicit notion of inherencyinthedistinctionbetweenJJandNNinthe Penn Treebank (Santorini, 1990, p. 12-13)." ></td>
	<td class="line x" title="200:203	Fully outlining these inherent properties could provide insights into induction and its evaluation." ></td>
	<td class="line x" title="201:203	Acknowledgments Thanks to the three anonymous reviewers for their usefulcommentsandtoCharlesJochimforhelpful discussion." ></td>
	<td class="line x" title="202:203	This material is based upon work supported by the National Science Foundation under Grant No." ></td>
	<td class="line x" title="203:203	IIS-0623837." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="I08-3015
Morphology Driven Manipuri POS Tagger
Singh, Thoudam Doren;Bandyopadhyay, Sivaji;"></td>
	<td class="line x" title="1:172	Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 9198, Hyderabad, India, January 2008." ></td>
	<td class="line x" title="2:172	c2008 Asian Federation of Natural Language Processing Morphology Driven Manipuri POS Tagger   Thoudam Doren Singh             Sivaji Bandyopadhyay       Computer Science Department       Computer Science & Engineering Department   St. Anthonys College                 Jadavpur University               Shillong-793001, Meghalaya, India                     Kolkata  700 032, India         thoudam_doren@rediffmail.com                     sivaji_cse_ju@yahoo.com   Abstract  A good POS tagger is a critical component of a machine translation system and other related NLP applications where an appropriate POS tag will be assigned to individual words in a collection of texts." ></td>
	<td class="line x" title="3:172	There is not enough POS tagged corpus available in Manipuri language ruling out machine learning approaches for a POS tagger in the language." ></td>
	<td class="line x" title="4:172	A morphology driven Manipuri POS tagger that uses three dictionaries containing root words, prefixes and suffixes has been designed and implemented using the affix information irrespective of the context of the words." ></td>
	<td class="line x" title="5:172	We have tested the current POS tagger on 3784 sentences containing 10917 unique words." ></td>
	<td class="line x" title="6:172	The POS tagger demonstrated an accuracy of 69%." ></td>
	<td class="line x" title="7:172	Among the incorrectly tagged 31% words, 23% were unknown words (includes 9% named entities) and 8% known words were wrongly tagged." ></td>
	<td class="line x" title="8:172	1 Introduction  Manipuri (Meiteilon or Meiteiron) belongs to the Tibeto-Burman language family and is highly agglutinative in behavior, monosyllabic, influenced and enriched by the Indo-Aryan languages of Sanskrit origin and English." ></td>
	<td class="line x" title="9:172	The affixes play the most important role in the structure of the language." ></td>
	<td class="line x" title="10:172	A clear -cut demarcation between morphology and syntax is not possible." ></td>
	<td class="line x" title="11:172	In Manipuri, words are formed in three processes called affixation, derivation and compounding (Thoudam, 2006)." ></td>
	<td class="line x" title="12:172	The majority of the roots found in the language are bound and the affixes are the determining factor of the class of the words in the language." ></td>
	<td class="line x" title="13:172	Classification of words using the role of affix helps to implement the tagger for a resource poor language like Manipuri with high performance." ></td>
	<td class="line oc" title="14:172	There are many POS taggers developed using different techniques for many major languages such as transformation-based error-driven learning (Brill, 1995), decision trees (Black et al., 1992), Markov model (Cutting et al., 1992), maximum entropy methods (Ratnaparkhi, 1996) etc for English." ></td>
	<td class="line x" title="15:172	Decision trees are used to estimate marginal probabilities in a maximum entropy model for predicting the parts-of-speech of a word given the context in which it appears (Black et al., 1992)." ></td>
	<td class="line x" title="16:172	The rules in a rule-based system are usually difficult to construct and typically are not very robust (Brill, 1992)." ></td>
	<td class="line x" title="17:172	Large tables of statistics are not needed for the rule-based tagger." ></td>
	<td class="line x" title="18:172	In a stochastic tagger, tens of thousands of lines of statistical information are needed to capture the contextual information (Brill, 1992)." ></td>
	<td class="line oc" title="19:172	For a tagger to function as a practical component in a language processing system, a tagger must be robust, efficient, accurate, tunable and reusable (Cutting, 1992)." ></td>
	<td class="line x" title="20:172	2 Previous work on Manipuri POS tagger  Morphology based POS tagging of some languages like Turkish (Oflazer and Kuruoz, 1994), Czech (Hajic, et al., 2001) has been tried out using a combination of hand-crafted rules and statistical learning." ></td>
	<td class="line x" title="21:172	A Marathi rule based POS tagger used a technique called SRR (suffix replacement rule) (Burange et al., 2006) with considerable accuracy." ></td>
	<td class="line x" title="22:172	A POS tagger for Hindi overcomes the handicap of annotated corpora scarcity by exploiting the rich morphology of the language (Singh et al., 2006)." ></td>
	<td class="line x" title="23:172	To the best of our knowledge, there is no record available of work done on a Manipuri POS tagger." ></td>
	<td class="line x" title="24:172	A related work of word class and sentence type identification in a Manipuri Morphological Analyzer 91 is found in (Thoudam and Bandyopadhyay, 2006) where the classification of few word categories and sentence type identification are discussed based on affix rules." ></td>
	<td class="line x" title="25:172	3 Manipuri Morphemes  There are free and bound roots in Manipuri." ></td>
	<td class="line x" title="26:172	All the verb roots are bound roots." ></td>
	<td class="line x" title="27:172	There are also a few bound noun roots, the interrogative and demonstrative pronoun roots." ></td>
	<td class="line x" title="28:172	They cannot occur without some particle prefixed or suffixed to it." ></td>
	<td class="line x" title="29:172	The bound root may form a compound by the addition of another root." ></td>
	<td class="line x" title="30:172	The free roots are pure nouns, pronouns, time adverbials and some numerals." ></td>
	<td class="line x" title="31:172	The bound roots are mostly verb roots although there are a few noun and other roots." ></td>
	<td class="line x" title="32:172	The suffixes, which are attached to the nouns, derived nouns, to the adjectives in noun phrases including numerals, the case markers and the bound coordinators are the nominal suffixes." ></td>
	<td class="line x" title="33:172	In Manipuri, the nominal suffixes are always attached to the numeral in a noun phrase and the noun cannot take the suffixes." ></td>
	<td class="line x" title="34:172	Since numerals are considered as adjectives, the position occupied by the numerals in Manipuri may be regarded adjective position (Thoudam, 2006)." ></td>
	<td class="line x" title="35:172	There are a few prefixes in Manipuri." ></td>
	<td class="line x" title="36:172	These prefixes are mostly attached to the verb roots." ></td>
	<td class="line x" title="37:172	They can also be attached to the derived nouns and bound noun roots." ></td>
	<td class="line x" title="38:172	There are also a few prefixes derived from the personal pronouns." ></td>
	<td class="line x" title="39:172	In this agglutinative language the numbers of verbal suffixes are more than that of the nominal suffixes (Singh, 2000)." ></td>
	<td class="line x" title="40:172	New words are easily formed in Manipuri using morphological rules." ></td>
	<td class="line x" title="41:172	Inflectional morphology is more productive than derivative morphology (Chelliah, 1997)." ></td>
	<td class="line x" title="42:172	There are 8 inflectional (INFL) suffixes and 23 enclitics (ENC)." ></td>
	<td class="line x" title="43:172	There are 5 derivational prefixes out of which 2 are category changing and 3 are non-category changing." ></td>
	<td class="line x" title="44:172	There are 31 non-category changing derivational suffixes and 2 category changing suffixes." ></td>
	<td class="line x" title="45:172	The noncategory changing derivational suffixes may be divided into first level derivatives (1 st  LD) of 8 suffixes, second level derivatives (2 nd  LD) of 16 suffixes and third level derivatives (3 rd  LD) of 7 suffixes." ></td>
	<td class="line x" title="46:172	Enclitics in Manipuri fall in six categories: determiners, case markers, the copula, mood markers, inclusive/exclusive and pragmatic peak markers and attitude markers." ></td>
	<td class="line x" title="47:172	The categories are determined on the basis of position in the word (category 1 occurs before category 2, category 2 occurs before category 3 and so on)." ></td>
	<td class="line x" title="48:172	4 Dictionaries  Three different dictionaries namely prefix which contains prefix information, suffix which contains suffix information and root containing 2051 entries are used for the system." ></td>
	<td class="line x" title="49:172	The format of root is <root><category>." ></td>
	<td class="line x" title="50:172	A bilingual dictionary consisting of Manipuri word and its corresponding pronunciation, POS, 1 st  English (Eng1) word meaning, 2 nd  English (Eng2) word meaning (if any), 3 rd  English (Eng3) word meaning (if any), a Manipuri sentence or phrase using the word and corresponding English meaning has been developed based on the work of Manipuri to English Dictionary (Imoba, 2004)." ></td>
	<td class="line x" title="51:172	The bilingual parallel dictionary is used for testing POS tagger and later on will be used for EBMT system." ></td>
	<td class="line x" title="52:172	The Manipuri sentences/phrases using a particular word are used as the input to the POS tagger thus enabling to sort out words with multiple meaning." ></td>
	<td class="line x" title="53:172	5 Morphological analysis of Major Lexical categories  The lexical categories in Manipuri can be of two types  major and minor (Chelliah, 1997)." ></td>
	<td class="line x" title="54:172	Major lexical categories can be of two types, namely actual and potential." ></td>
	<td class="line x" title="55:172	The lexicon of actual lexical categories i.e., actual lexicon consists of an unordered list of roots and affixes and lexicalized forms." ></td>
	<td class="line x" title="56:172	Each lexical entry in the actual lexicon consists of what lexical category it belongs to and what its meaning is. On the other hand, the output of the potential lexicon consists of words created through productive morphological processes." ></td>
	<td class="line x" title="57:172	In the actual lexicon, roots may be bound or free." ></td>
	<td class="line x" title="58:172	Nouns and verbs from the actual lexicon can be distinguished on formal grounds in that bound roots are verbs and free roots are nouns." ></td>
	<td class="line x" title="59:172	In the potential lexicon, adjectives, adverbs and nominal forms can be derived from verb roots and stative verbs can be derived from noun roots." ></td>
	<td class="line x" title="60:172	There are several instances where the words belonging to some class or category plays the role of some other category sometimes based on its position in the sentences (P.C. Thoudam, 92 2006) Some of the generalized handcrafted rules to identify the lexical are given as below." ></td>
	<td class="line x" title="61:172	5.1 Nouns  Nouns can be distinguished from other lexical categories on morphological grounds." ></td>
	<td class="line x" title="62:172	Unlike verbs, nouns can be suffixed by gender, number or case markers." ></td>
	<td class="line x" title="63:172	Proper nouns and common nouns are free standing forms." ></td>
	<td class="line x" title="64:172	The following is the list of word structure rules for nouns (Chelliah, 1997) N  Root INFL (ENC) Root  Root (2 nd  LD) Root  Root (1 st  LD) Root  (prefix) root (root) Figure 1 shows the general form of noun morphology in Manipuri." ></td>
	<td class="line x" title="65:172	Examples of some singular/plural noun forms are listed in Table 1." ></td>
	<td class="line x" title="66:172	Prono minal prefix Root gende r number Quant ifier Cas e Figure 1." ></td>
	<td class="line x" title="67:172	General form of Noun Morphology  Singular Form Plural Form =JE  -Uchek (bird) =JE`e -Ucheksing(birds) ]   -Ma (He/She) ]F^  -Makhoy (they) ]  -Mi (man) ]^] -Mi-yaam (men) Table 1: Singular/Plural forms  Although case markers are functionally inflectional, they exhibit the clitic like characteristic of docking at the edge of a phrase." ></td>
	<td class="line x" title="68:172	The word structure of rules of verbs and nouns are identical except for the category of the word level node, the possible terminal elements of the derivational and inflectional categories and the lack of the third level nominal derivation." ></td>
	<td class="line x" title="69:172	Two examples to demonstrate the noun morphology are given below:]JXY`eX (m-ca-nu-pi-si -n ) by his/her daughters ]JXY`eX (m-ca-nu-pa-si n ) by his/her sons  The ] -m his/her is the pronominal suffix and J -ca child is the noun root." ></td>
	<td class="line x" title="70:172	The X -nu human is suffixed by Y -pi to indicate a female human and Ypa to indicate a male human." ></td>
	<td class="line x" title="71:172	`e si or F+ -khoy or ^] yaam can be used to indicate plurality." ></td>
	<td class="line x" title="72:172	-si cannot be used with pronouns or proper nouns and -khoy cannot be used with nonhuman nouns." ></td>
	<td class="line x" title="73:172	X -n meaning by the is the instrumental case marker." ></td>
	<td class="line x" title="74:172	5.2 Pronouns  The singular personal pronouns are B -y  I, Xe -n you and ] -ma he/she." ></td>
	<td class="line x" title="75:172	Possessive pronouns are formed through the suffixation of E -ki genitive on these personal pronouns." ></td>
	<td class="line x" title="76:172	Indefinite pronouns are also lexicalized forms that consists of a question word which may be followed by a -su also or the sequence E -kumb composed of  E] kum, like, kind of and [ b nominalizer . The strategy for creating relative clause in Manipuri is to place the relativized noun directly after a normalized clause; there is no relative pronoun to mark the relative clause." ></td>
	<td class="line x" title="77:172	The determiner may occur either as an independent pronoun or encliticized on the noun phrase with no difference in meaning." ></td>
	<td class="line x" title="78:172	The determiners a si proximate and T  tu distal are stems that function as enclitics." ></td>
	<td class="line x" title="79:172	a si indicated that the object or person being spoken of is near or currently seen or known to be near., even if not viewable by the speaker, or is currently the topic of conversation; T -tu signifies something or someone not present at the time of speech or newly introduced in the conversation." ></td>
	<td class="line x" title="80:172	Possessive pronominal prefix may be affixed to the root ` sa body to form pronouns emphasizing that the subject of the verb is a particular person or thing and no one or nothing else: +`X isan by myself  X`X nsan  by yourself and ]`X msan  by him/her/itself/." ></td>
	<td class="line x" title="81:172	The set of Manipuri Pronominal prefixes differ for different persons (+  {I} for 1 st  person, X {Na} for 2 nd  person and ]  {Ma} for 3 rd  person) while the set of pronominal suffixes differ only on gender (Y pa for masculine gender,  Y -Pi for feminine gender)." ></td>
	<td class="line x" title="82:172	5.3 Verbs  Verbs roots are in the actual lexicon and are bound forms." ></td>
	<td class="line x" title="83:172	A verb may be free standing word if it is minimally suffixed by an inflectional marker." ></td>
	<td class="line x" title="84:172	The verb root may also be followed by one of the enclitics." ></td>
	<td class="line x" title="85:172	Three derivational categories may optionally precede the final inflectional suffix." ></td>
	<td class="line x" title="86:172	The 1 st  LD suffixes signal adverbial meanings, the 2 nd  LD suffixes indicate evidentiality, the deitic reference of 93 a verb, or the number of persons performing the action and the 3 rd  LD suffixes signal aspect and mood." ></td>
	<td class="line x" title="87:172	Verb roots may also be used to form verbal nouns, adjectives and adverbs." ></td>
	<td class="line x" title="88:172	Verbal nouns are formed through the suffixation of the nominalizer Y p to the verb root." ></td>
	<td class="line x" title="89:172	The following is the list of word structure rules for verbs (Chelliah, 1997)  a. Verb  Root INFL b. Root  Root (3 rd  LD) c. Root  Root (2 nd  LD) d. Root  Root (1 st  LD) e. Root  root (root) f. 3 rd  LD  (mood1)(mood2)(aspect) g. 2 nd  LD  (2 nd  LD1),(2 nd  LD2),(2 nd  LD3) h. 1 st  LD  1 st  LD  Derivat ional Prefixa tion Ro ot 1 st  Level derivati on 2 nd level derivati on 3 rd  level deriv ation Infle ction Figure 2." ></td>
	<td class="line x" title="90:172	General form of Verb Morphology  There are 3 categories (mood1, mood2, and aspect) belonging to the third level derivational (3 rd  LD) markers." ></td>
	<td class="line x" title="91:172	The general form of verb morphology is shown in figure 2." ></td>
	<td class="line x" title="92:172	The sub-categorization frames of affixes will restrict that only nominal affixes occur with a noun and verbal affixes occur with a verb root." ></td>
	<td class="line x" title="93:172	The derivational suffix order of the word JEF+[hX is given below: JE         F+            [E        E            X cek        khay                -rk          -k               -ni crack   -totally affect    -distal      -potential   copula  (1 st  LD)          (2 nd  LD)     (3 rd  LD) The [E -rk  has allomorph _E-lk.   [E -rk occurs after vowels while _E-lk occurs after consonants." ></td>
	<td class="line x" title="94:172	Such allomorph is an example of orthographic change and it is taken care by the system by making individual entries into the dictionary." ></td>
	<td class="line x" title="95:172	J[EA ca-rk-y (ate there and came here) JEA  cam-lk-y (washed there and came here)  The formation of verb can be of the form  Verb stem + aspect/mood  verb UE -thk  (drink)  + _ -le UE_ thkle  (has drunk)  The verbal noun is formed with the rule as given as  Verb Stem + Nominalizer  Verbal noun Ue -thong (cook)+ [ -ba   Ue[ thongba  (to cook)  5.4 Adjectives  An adjective is derived through the affixation of the attributive, derivational prefix % to a verbal noun." ></td>
	<td class="line x" title="96:172	e.g. % - + Verbal noun  Adjective % - + a -si (die) + [ -ba>%a[ siba (something dead) Adjectives may appear before or after the nouns they modify." ></td>
	<td class="line x" title="97:172	Possessive adjectives are formed through the suffixation of the genitive marker E ki to the possessor of a noun." ></td>
	<td class="line x" title="98:172	5.5 Adverbs  Manner adverbs are formed through suffixation of X n adverbial to a verb root." ></td>
	<td class="line x" title="99:172	e.g. _^X  loyn completely, all from loy complete,finish." ></td>
	<td class="line x" title="100:172	e.g.,  Stem  +   X na    Adverb EY -K p (cry)+ X na  > EYX -k p-na (cryingly)  Locative adverbs are derived through the prefixation of ] m noun marker to a noun or verb roots." ></td>
	<td class="line x" title="101:172	e.g. ]F mkha  below, underneath from  F kha south   6 Morphological analyses of some minor lexical categories  The three minor lexical categories of Manipuri are quantifiers, numerals and interjections." ></td>
	<td class="line x" title="102:172	These are considered minor categories because these lexical items are closed sets which express meanings most often encoded by affixal morphology." ></td>
	<td class="line x" title="103:172	The lexical items in interjection is defined on the semantic similarity of its members, all express strong emotion." ></td>
	<td class="line x" title="104:172	94 6.1 Quantifiers  Most quantifiers in Manipuri are lexicalized forms consisting of the unproductive prefix khV(where the vowel can be a, i, u)." ></td>
	<td class="line x" title="105:172	These are F[ -khra  some which indicates an indeterminate amount; FTe khit  ever so little, a particle of some tangible material." ></td>
	<td class="line x" title="106:172	These quantifiers can be combined as in  <`e   F[     FTe     Y[E=." ></td>
	<td class="line x" title="107:172	Ishi    khra    khit   purk-u  Bring me just a little bit of water." ></td>
	<td class="line x" title="108:172	6.2 Numerals  The numerals are nouns." ></td>
	<td class="line x" title="109:172	Ordinal numerals are adjectives, derived through the affixation of the attributive prefix %  and the nominalizer  [ b to any numeral with  su also: thus %X[ nisub  second one." ></td>
	<td class="line x" title="110:172	6.3 Interjections  The lexical items of this category which is defined on the semantic similarity of its members, all express strong emotion." ></td>
	<td class="line x" title="111:172	Some of these are composite forms where one syllable is identifiable as the exasperative enclitic c he and the second syllable is not identifiable as a productive affix or stem." ></td>
	<td class="line x" title="112:172	7 Manipuri Tagset  The basic Manipuri POS tag set used in the POS tagger is listed below." ></td>
	<td class="line x" title="113:172	Ey Ey kukru kukru (a pigeons cry) is ideophone." ></td>
	<td class="line x" title="114:172	T tu that is a determiner." ></td>
	<td class="line x" title="115:172	c^[` haybasi is a determiner complementizer." ></td>
	<td class="line x" title="116:172	Sl." ></td>
	<td class="line x" title="117:172	No. Category name Tag 1 adjective ADJ 2 adverb ADV 3 conjunction CONJ 4 complementizer CMP 5 determiner DET 6 ideophone IDEO 7 interjection INTJ 8 noun N 9 pronoun PN 10 quantifier QU 11 verb VB 12 Verbal noun VN 13 Unknown  UNK Table 2." ></td>
	<td class="line x" title="118:172	Manipuri POS tagset  8 Design of Manipuri POS tagger  In Manipuri, the basic POS tags are assigned to the words on the basis of morphological rules." ></td>
	<td class="line x" title="119:172	Figure 3 shows the system diagram of Manipuri POS tagger." ></td>
	<td class="line x" title="120:172	Input sentence               Lexical Rules                Tagged Output Sentence  Figure 3." ></td>
	<td class="line x" title="121:172	System Diagram  The different parts involved in the system are:a. Tokenizer: Words are separated based on the space given between consecutive words." ></td>
	<td class="line x" title="122:172	b. Stemmer: It separates the prefixes and suffixes from the words." ></td>
	<td class="line x" title="123:172	c. Engine: Different analysis and treatment of different words are performed based on the category." ></td>
	<td class="line x" title="124:172	d. Tag Generator: Tags are assigned to the words in the sentence input based on the tagset and morphology rules." ></td>
	<td class="line x" title="125:172	e. Dictionaries:  Prefix, suffix and word dictionary along with sentences using the words are maintained." ></td>
	<td class="line x" title="126:172	Tokenizer Stemmer   Engine Major Lexical Category Module Minor Lexical Category module Dictionaries    Tag Generator 95 8.1 Algorithm of POS tagging  Algorithm used for tagging is as follows: 1." ></td>
	<td class="line x" title="127:172	Input the Manipuri input texts to the Tokenizer." ></td>
	<td class="line x" title="128:172	2." ></td>
	<td class="line x" title="129:172	Repeat steps 3 to 6 until the end of the texts for each token." ></td>
	<td class="line x" title="130:172	3." ></td>
	<td class="line x" title="131:172	Feed the tokens to the stemmer." ></td>
	<td class="line x" title="132:172	4." ></td>
	<td class="line x" title="133:172	Check the patterns and order of the different morphemes by looking at the stem category." ></td>
	<td class="line x" title="134:172	5." ></td>
	<td class="line x" title="135:172	Apply the handcrafted morphological rules for identifying the category using the engine." ></td>
	<td class="line x" title="136:172	6." ></td>
	<td class="line x" title="137:172	Generate the POS tags using Tag generator." ></td>
	<td class="line x" title="138:172	7." ></td>
	<td class="line x" title="139:172	End." ></td>
	<td class="line x" title="140:172	The Visual C++, MsAccess and GIST SDK are used to develop the system." ></td>
	<td class="line x" title="141:172	The Manipuri words are entered into the dictionary using Bengali script (BN1 TTBidisha font)." ></td>
	<td class="line x" title="142:172	9 Evaluation  In Manipuri, word category is not so distinct except Noun." ></td>
	<td class="line x" title="143:172	The verbs are also under bound category." ></td>
	<td class="line x" title="144:172	Another problem is to classify basic root forms according to word class although the distinction between the noun class and verb classes is relatively clear, the distinction between nouns and adjectives is often vague." ></td>
	<td class="line x" title="145:172	Distinction between a noun and an adverb becomes unclear because structurally a word may be a noun but contextually it is adverb." ></td>
	<td class="line x" title="146:172	Thus, the assumption made for word categories are depending upon the root category and affix information available from the dictionaries." ></td>
	<td class="line x" title="147:172	At the moment, we use a sequential search of a stem from the root dictionary in alphabetical order." ></td>
	<td class="line x" title="148:172	It is found to be suitable for small size dictionary." ></td>
	<td class="line x" title="149:172	Further a part of root may also be a prefix which leads to wrong tagging." ></td>
	<td class="line x" title="150:172	The verb morphology is more complex than that of noun." ></td>
	<td class="line x" title="151:172	A comparative study on the number of words tagged by the system and manually tagged had been carried out." ></td>
	<td class="line x" title="152:172	The inputs of 3784 Manipuri sentences of 10917 unique words as input to the tagger engine." ></td>
	<td class="line x" title="153:172	Sometimes two words get fused to form a complete word." ></td>
	<td class="line x" title="154:172	Handling such collocations is difficult." ></td>
	<td class="line x" title="155:172	Conjuncts require a separate dealing using a table." ></td>
	<td class="line x" title="156:172	Verbs, nouns and noun phrases, subordinate sentences, and root sentences can be affixed by enclitics." ></td>
	<td class="line x" title="157:172	Table 4 shows the percentage statistics of tagging output based on the actual and correctly tagged words." ></td>
	<td class="line x" title="158:172	The accuracy of tagging can be further improved by populating more root morphemes to the root dictionary." ></td>
	<td class="line x" title="159:172	No." ></td>
	<td class="line x" title="160:172	of single correct tags Accuracy percentage=            X 100     Total no." ></td>
	<td class="line x" title="161:172	of tokens  Group Types Percentage Single tagged correct words 65% Multiple tagged correct words  4% Unknown words 23% ( 9% Named Entities) Wrong tagged words 8% Table 4." ></td>
	<td class="line x" title="162:172	Tagger output statistics  The unknown words are the words which could not be tagged based on the linguistic rules and unavailability of entries mainly in root dictionary." ></td>
	<td class="line x" title="163:172	In the process of word formation, only affixation: prefixing, suffixing or compounding takes the role of formation of new words in this language." ></td>
	<td class="line x" title="164:172	Due to the fact that new words are easily formed in Manipuri, thus the number of unknown words (out of vocabulary) is relatively large (Sirajul et al., 2004)." ></td>
	<td class="line x" title="165:172	10 Challenges for future work  The noun group words handling are not incorporated." ></td>
	<td class="line x" title="166:172	For example %FE %[C (pronounced as khak raw ) meaning thunderbolt, %I] %[+ FIV[ (pronounced as  am ray kh  dba) meaning wanton are noun group words and are not tagged by the POS tagger correctly." ></td>
	<td class="line x" title="167:172	The Noun-Adjective ambiguity disambiguation scheme is required as a separate module and implementations are to be included in the future work." ></td>
	<td class="line x" title="168:172	The Manipuri tagging is very much dependent on the morphological analysis and lexical rules of each category." ></td>
	<td class="line x" title="169:172	There is a cleaning process of all word and morphemes specially the spelling to ensure that the lexical rules are implemented." ></td>
	<td class="line x" title="170:172	This has not yet been implemented." ></td>
	<td class="line x" title="171:172	Collocations handling and more disambiguation rules will be developed in further phases of the work." ></td>
	<td class="line x" title="172:172	The output of the POS tagger will be used in a Manipuri-English machine translation system." ></td>
</tr></table>
</div
</body></html>
